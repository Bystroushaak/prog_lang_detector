/* 
 * File:   ServerContainer.cpp
 * Author: Bystroushaak (bystrousak[at]kitakitsune.org)
 * 
 * This work is licensed under a Creative Commons Attribution-Noncommercial-Share
 * Alike 3.0 Unported Licence (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/)
*/

#include <cstdlib>
#include <string>
#include "ServerContainer.h"

using namespace std;

// Konstruktor
ServerContainer::ServerContainer() {
    this->len = STARTSIZE;
    this->array = new Server *[this->len];
    this->act = 0;
}

// Kopirovaci konstruktor
ServerContainer::ServerContainer(const ServerContainer& b) {
    this->act = b.act;
    this->len = b.len;
    this->array = new Server *[this->len];

    for (int i = 0; i < this->act; i++)
        this->array[i] = new Server(*b.array[i]);
}

/*= append =====================================================================
  Name         :  append
  Description  :  Rozsiri velikost pole na dvojnasobek.
  Argument(s)  :  None.
  Return value :  None.
 */
void ServerContainer::append() {
    Server **tmp = new Server *[this->len * 2];

    for (int i = 0; i < this->act; i++) {
        tmp[i] = new Server(*this->array[i]);
    }

    delete [] this->array;

    this->array = tmp;
    this->len *= 2;
}
//==============================================================================

/*= add ========================================================================
  Name         :  add
  Description  :  Tato metoda prida do this->array objekt ktery ji byl predany
                  parametrem. Pokud je pole plne, zavola privatni metodu
                  append(), ktera se postara o rozsireni.
  Argument(s)  :  Server *s - Pointer na objekt ktery ma byt pridan do pole.
  Return value :  None.
 */
void ServerContainer::add(Server *s) {
    if ((this->len - 1) <= this->act)
        this->append();

    this->array[this->act++] = s;
}
//==============================================================================

/*= get ========================================================================
  Name         :  get
  Description  :  Z this->array vybere prvek na indexu _index_.
  Argument(s)  :  int index - index na kterem se nachazi prvek ktery chceme vratit.
  Return value :  Prvek na indexu _index_, nebo NULL v pripade ze nebyl nalezen.
 */
Server *ServerContainer::get(int index) {
    if (index > this->act || index < 0)
        return NULL;

    return this->array[index];
}
//==============================================================================

/*= del ========================================================================
  Name         :  del
  Description  :  Odstrani na prvek na indexu _index_. Pole zustane stejne velke.
  Argument(s)  :  int index - prvek ktery ma byt odstranen.
  Return value :  Void.
 */
void ServerContainer::del(int index) {
    if (index > this->act || index < 0)
        return;

    Server **tmp = new Server *[this->len];

    for (int i = 0; i < index; i++) {
        tmp[i] = this->array[i];
    }

    for (int i = index; i < this->act; i++) {
        tmp[i] = this->array[i + 1];
    }

    delete [] this->array;
    this->array = tmp;
    this->act--;
}
//==============================================================================

/*= length =====================================================================
  Name         :  length
  Description  :  Metoda vraci aktualni pocet prvku pole, nikoli jeho velikost!
  Argument(s)  :  None.
  Return value :  Pocet prvku pole.
 */
int ServerContainer::length() {
    return this->act;
}
//==============================================================================

/*= convertToSSH ===============================================================
  Name         :  convertToSSH
  Description  :  Postupne projde vsechny prvky ktere ServerContainer obsahuje
                  a vygeneruje z nich retezec obsahujici prikazy pro vytvoreni
                  tunelu skrz vsechny servery.
                  V zavislosti na argumentu _raw_ je vystup bud vice nebo mene
                  ukecany.
  Argument(s)  :  int port - specifikuje port na kterem ma byt vytvoren posledni
                             tunel
                  bool raw - specifikuje ukecanost vystupu (false = ukecany)
  Return value :  Retezec obsahujici prikazy pro pripojeni.
 */
string ServerContainer::convertToSSH(int port, bool raw) {
    if (this->act == 0) // pokud neobsahuje zadne zaznamy, vrati prazdny string
        return string();
    else {
        string out;
        char *buffer = new char[BUFFSIZE];

        // pokud neni zapnutej raw mod, vykecavej se
        if (!raw)
            out = string("\nAdd this into console;\n\n");

        for (int i = 0; i < this->act - 1; i++) {
            if (i == 0) { // syntaxe prvniho prikazu smeruje rovnou na server skrz kterej povedou vsechny ostatni tunely
                Server &s = *this->array[i];
                Server &n = *this->array[i + 1];

                snprintf(buffer, BUFFSIZE, "ssh -L %d:%s:%s %s@%s -p %s\n",
                        (this->act - 1) + port,
                        n.getServer().c_str(),
                        n.getPort().c_str(),
                        s.getUser().c_str(),
                        s.getServer().c_str(),
                        s.getPort().c_str());
            } else { // dalsi tunely pak lezou do portu na localhostu ktere vytvorily predchozi prikazy
                Server &s = *this->array[i];
                Server &n = *this->array[i + 1];
                snprintf(buffer, BUFFSIZE, "ssh -L %d:%s:%s %s@localhost -p %d\n",
                        (this->act - 1) + port - i,
                        n.getServer().c_str(),
                        n.getPort().c_str(),
                        s.getUser().c_str(),
                        this->act + port - i);
            }

            out += string(buffer);
        }

        // nazaver skrz to prostrci -D
        Server &s = *this->array[this->act - 1];
        if (this->act == 1) // pokud se jednalo jen o jeden server, syntaxe je trochu rozdilna (smeruje primo na dany server)
            snprintf(buffer, BUFFSIZE, "ssh -D %d %s@%s:%s",
                port,
                s.getUser().c_str(),
                s.getServer().c_str(),
                s.getPort().c_str());
        else // pokud jsou zde predchozi porty ktere naslouchaji na localhostu, prostrc -D skrz ne
            snprintf(buffer, BUFFSIZE, "ssh -D %d %s@localhost -p %d",
                port,
                s.getUser().c_str(),
                port + 1);

        out += string(buffer);

        // pokud neni zapnutej raw mod, vykecavej se
        if (!raw) {
            snprintf(buffer, BUFFSIZE, "\n\n---\n\nYou can connect on:\n\tlocalhost:%d\n\n", port);
            out += string(buffer);
        }

        return out;
    }
}
//==============================================================================


// Destruktor
ServerContainer::~ServerContainer() {
    for (int i = 0; i < this->act; i++) {
        delete this->array[i];
    }

    delete [] this->array;
    this->act = 0;
    this->len = 0;
}

#include <iostream>

#ifdef _WIN32
#include <windows.h>
#else
    #include <netdb.h>      // gethostbyname()
    #include <netinet/in.h> // in_addr
    #include <arpa/inet.h>  // inet_ntoa()
#endif

using namespace std;

void printHelp() {
    cout << "Preklad domeny v1.0.0 (09.02.2010) by Bystroushaak (bystrousak@kitakitsune.org)" << endl;
    cout << endl << "Pouziti;" << endl;
    cout << "\t./PrekladDomen jmenodomeny.tld" << endl << endl;
}

int main(int argc, char** argv) {
    // Kontrola poctu parametru
    if (argc <= 1 || argc > 2) {
        cout << endl << ">> Spatny pocet parametru! <<" << endl << endl;
        printHelp();
        return 1;
    }

    #ifdef _WIN32
        WORD wVersionRequested = MAKEWORD(1,1);
        WSADATA data;
        if (WSAStartup(wVersionRequested, &data) != 0){
            std::cout << "Nepodařilo se inicializovat sokety" << std::endl;
            // Podle všeho, zde se WSACleanup volat nemusí.
            return -1;
        }
    #endif

    // Vytvoreni struktury z url
    hostent *h = gethostbyname(argv[1]);

    if (h == NULL){
        cout << endl << ">> Nepodarilo se zjistit adresu! <<" << endl << endl;
        return 1;
    }

    cout << "---" << endl;
    cout << "Oficialni jmeno; " << h->h_name << endl;

    // Vypsani alternativnich nazvu
    char **alternativy = h->h_aliases;
    if (*alternativy != NULL){
        cout << "Alternativni nazvy:" << endl;
        for (int i = 0; *alternativy != NULL; i++){
            cout << i << ") " << *(alternativy++) << endl;
        }
    }

    // Vypsani jednotlivych adres
    cout << "Adresy:" << endl;
    for (int i = 0; h->h_addr_list[i] != NULL; i++){
        cout << "\t" << i + 1 << ") " << inet_ntoa(*(in_addr *) h->h_addr_list[i]) << endl;
    }

    cout << "Velikost adres: " << h->h_length << "B" << endl;

    cout << "---" << endl;

    #ifdef _WIN32
        WSACleanup();
    #endif

    return 0;

}
/*
 * File:   server.cpp
 * Author: Bystroushaak (bystrousak[at]kitakitsune.org)
 *
 * This work is licensed under a Creative Commons Attribution-Noncommercial-Share
 * Alike 3.0 Unported Licence (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/)
*/

#include "Server.h"

using namespace std;

// Konstruktor
Server::Server() {
}

// Kopirovaci konstruktor
Server::Server(const Server& b) {
    this->pass = b.pass;
    this->port = b.port;
    this->server = b.server;
    this->user = b.user;
}

// Konstruktor s parametrem char *
Server::Server(char *str) {
    parseStr(string(str));
}

// Konstruktor s parametrem string
Server::Server(string str) {
    parseStr(str);
}

// Getery
string Server::getUser() {
    return this->user;
}

string Server::getServer() {
    return this->server;
}

string Server::getPort() {
    return this->port;
}

string Server::getPass() {
    return this->pass;
}

/*= parseStr ===================================================================
 * Name         :  parseStr
 * Description  :  Rozparsuje string v parametru _str_ na jednotlive atributy.
 *
 *                 Tato metoda se da pouzit i jako setter (heh, stejne se jmenuje
 *                 jeden druh psa).
 * Argument(s)  :  string str - vstupni retezec ktery bude parsovan
 * Return value :  None.
 */
void Server::parseStr(string str) {
    // uznavam, ze tahle metoda neni nic moc, ale hey! tohle je moje prvni prace se stringem v cpp (zlatej python..)

    // parsovani hesla
    if (str.find("-p") != string::npos) {
        int i = ((str.find("-p") + 3) < str.length()) ? (str.find("-p") + 3) : (str.length());
        while (i < str.length()) { // popr. pridat !isspace(str[i]) && pro parsovani pouze do neblizsi mezery
            this->pass += str[i++];
        }
    }

    int i = 0;
    string buffer;
    bool portlck = false;

    // parsovani vseho ostatniho
    while (!isspace(str[i]) && i < str.length()) { // parsovani username
        if (!portlck) {
            if (str[i] == '@') {
                this->user = buffer;

                buffer = "";
                i++;
                continue;
            } else if (str[i] == ':') { // parsovani serveru
                this->server = buffer;

                i++;
                buffer = "";
                portlck = true;
                continue;
            }

            buffer += str[i++];

        } else { // pokud narazi na :, sepne portlck a zbytek uklada jen do this->port
            this->port += str[i++];
        }
    }
    // pokud zustalo neco v bufferu, jedna se o servername
    if (buffer != "") {
        this->server = buffer;
    }

    // pokud nebyl zadan port, nastavi se na default (22)
    if (this->port == "") {
        this->port = "22";
    }

    // pokud nebylo servername nalezeno..
    if (this->server == "") {
        throw ParseException("Servername not found!");
    }

    // overi jestli v servername nejsou kraviny
    for (int i = 0; i < this->server.length(); i++) {
        if (!isalnum(this->server[i]) && this->server[i] != '.' && this->server[i] != '-')
            throw ParseException("Bad server name");
    }

    // test zdali bylo zadano username
    if (this->user == "")
        throw ParseException("Username required!");
    // testovani validity username
    for (int i = 0; i < this->user.length(); i++) {
        if (!isalnum(this->user[i]) && this->user[i] != '_' && this->user[i] != '-')
            throw ParseException("Bad username!");
    }

    // pokud jsou v portu kraviny, nastavi ho na default
    for (int i = 0; i < this->port.length(); i++) {
        if (!isdigit(this->port[i])) {
            port = DEFAULTPORT;
            break;
        }
    }
}
//==============================================================================


/*= testPrint ==================================================================
 * Name         :  testPrint
 * Description  :  Testovaci metoda urcena k overeni ze vse funguje jak ma.
 * Argument(s)  :  None.
 * Return value :  None.
 */
void Server::testPrint() {
    if (this->user != "")
        cout << "Username: " << this->user << endl;

    cout << "Server: " << this->server << endl;

    if (this->port != "")
        cout << "Port: " << this->port << endl;

    if (this->pass != "")
        cout << "Password: " << this->pass << endl;
}
//==============================================================================
#include <iostream>

using namespace std;

class go{
    int i;
public:
    virtual void vykresli() = 0;
};

class a: public go{
public:
    virtual void vykresli();
};

void a::vykresli(){
    cout << "Vykresluji a" << endl;
}

class b: public go{
public:
    virtual void vykresli();
};

void b::vykresli(){
    cout << "Vykresluji b" << endl;
}

int main(int argc, char *argv[])
{
    go *pole[2];
    pole[0] = new a();
    pole[1] = new b();
    
    for (int i = 0; i < 2; i++)
        pole[i]->vykresli();

    return 0;
}
#include <iostream>
#include <string>

using namespace std;

class Zivocich{
protected:
    string nazev;
    int vek;
public:
    Zivocich(){};
    Zivocich(string nazev, int vek);
    virtual void pohyb() = 0;
};

Zivocich::Zivocich(string nazev, int vek){
    this->nazev = nazev;
    this->vek = vek;

    cout << "Narodil se novy zivocich, ";
}

// ---

class Savec : public Zivocich{
public:
    Savec(string nazev, int vek):Zivocich(nazev, vek){cout << "savec, ";};
    void leze();
};

void Savec::leze(){
    cout << nazev << " je savec, takze umi lezt!" << endl;
}

// ---

class Ptak : public Zivocich{
public:
    Ptak(){};
    Ptak(string nazev, int vek):Zivocich(nazev, vek){cout << "ptak, ";};
    void leta();

};

void Ptak::leta(){
    cout << nazev << " je ptak, takze umi letat!" << endl;
}

// ---

class Pes : public Savec{
public:
    Pes(string nazev, int vek):Savec(nazev, vek){cout << "pes " << nazev << endl;};
    void steka();
};

void Pes::steka(){
    cout << "Pes " << nazev << " steka!" << endl;
}

// ---

class Ptakopysk : public Savec, public Ptak{
public:
    Ptakopysk(string nazev, int vek):Savec(nazev, vek),Ptak(nazev, vek){cout << "ptakopysk " << nazev << endl;};
    void plave();
};

void Ptakopysk::plave(){
    cout << "Ptakopysk " << Savec::nazev << " plave!" << endl;
}

// ---

class Kos : public Ptak{
public:
    Kos(string nazev, int vek):Ptak(nazev, vek){cout << "kos " << nazev << endl;};
    void zpiva();
};

void Kos::zpiva(){
    cout << "Kos " << nazev << " zpiva!" << endl;
}

int main(int argc, char** argv) {
    Kos Kosak(string("Kosak"), 12);
    Kosak.leta();
    Kosak.zpiva();

    Ptakopysk Petr(string("Petr"), 12);
    Petr.leta();
    Petr.leze();
    Petr.plave();

    return 0;
}

/*
 * File:   main.cpp
 * Author: Bystroushaak (bystrousak[at]kitakitsune.org)
 *
 * This work is licensed under a Creative Commons Attribution-Noncommercial-Share
 * Alike 3.0 Unported Licence (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/)
*/

/* TODO:
 *  upravit nejak do normalu makefile
 */

#include <iostream>
#include <string>
#include <cstdlib>
#include <cctype>
#include "Server.h"
#include "ServerContainer.h"

using namespace std;

// Funkce slouzici k vypsani napovedy.
void printHelp() {
    cout << "About:" << endl;
    cout << " This program reads list of SHH accounts from stdin and returns a list of ssh" << endl;
    cout << " commands which will create a SSH tunnel through all servers." << endl << endl;

    cout << "usage:\tSSHTunnelTool [options]" << endl << endl;

    cout << "options:" << endl;
    cout << "\t\t-r or --raw" << endl;
    cout << "\t\t\tRaw output." << endl << endl;

    cout << "\t\t-p or --port" << endl;
    cout << "\t\t\tPort where will be end of tunnel. Default is 2222." << endl << endl;

    cout << "\t\t-h or --help" << endl;
    cout << "\t\t\tShow this help." << endl << endl;

    cout << "\t\t-v or --version" << endl;
    cout << "\t\t\tShow version." << endl << endl;
}

// Funkce slouzici k vypsani verze
void printVersion() {
    cout << "\n SSHTunnelTool v1.0.0 (27.12.2009)\n" << endl;
    cout << "Licence:" << endl;
    cout << "   This work is licensed under a Creative Commons Attribution Noncommercial" << endl;
    cout << "   Share Alike 3.0 Unported License." << endl;
    cout << "   (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/)\n\n" << endl;
    cout << "                __.----_         \\`.,~/  Bystroushaak" << endl;
    cout << "                \\ ;     \\,------'/_  _\\  (bystrousak [at]" << endl;
    cout << "                 `--.__-;; ,____. '\\/    kitakitsune.org)" << endl;
    cout << "                        ((/     \\/       http://kitakitsune.org\n" << endl << endl;
}


// Funkce parsuje argumenty prikazove radky a nacita data od uzivatele.
int main(int argc, char** argv) {
    string arg;
    int port = DEFPORT;
    bool raw = false;

    // parsovani argumentu
    for (int i = 0; i < argc; i++) {
        arg = string(argv[i]);

        if (arg == "-v" || arg == "--version") {
            printVersion();
            return 0;
        } else if (arg == "-h" || arg == "--help") {
            printHelp();
            return 0;
        } else if (arg == "-r" || arg == "--raw") {
            raw = true;
        } else if ((arg == "-p" || arg == "--port") && argc > i) {
            string p = string(argv[i + 1]);

            for (int j = 0; j < p.length(); j++) {
                if (!isdigit(p[j])) {
                    cerr << "Bad port!" << endl;
                    return 1;
                }
            }
            port = atoi(p.c_str());

            // pokud neni port ve spravnem rozmezi ..
            if (port < 0 || port > 65535) {
                cerr << "Bad port range! (allowed is 0 .. (65535 - count_of_server_lines)" << endl;
                return 1;
            }
        }
    }
    
    string line;
    ServerContainer s;

    // cte radky, dokud nenarazi na EOF
    while (getline(cin, line)) {
        try {
            s.add(new Server(line)); // z kazdeho radku se pokusi vyparsovat username/servername
        } catch (ParseException e) { // pokud se to nepovede, vypise chybovou hlasku
            cerr << e.getErrorMessage() << endl;
        }
    }

    cout << s.convertToSSH(port, raw) << endl; // nakonec vypise jak vytvorit tunel

    return 0;
}

/* 
 * File:   ParseException.cpp
 * Author: Bystroushaak (bystrousak[at]kitakitsune.org)
 *
 * This work is licensed under a Creative Commons Attribution-Noncommercial-Share
 * Alike 3.0 Unported Licence (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/)
*/

#include "ParseException.h"

ParseException::ParseException() {
}

ParseException::ParseException(char *s){
    this->str = string(s);
}

ParseException::ParseException(string s){
    this->str = s;
}

string ParseException::getErrorMessage(){
    return this->str;
}
#include <iostream>

using namespace std;

class go{
    int i;
public:
    virtual void vykresli() = 0;
};

class a: public go{
public:
    virtual void vykresli();
};

void a::vykresli(){
    cout << "Vykresluji a" << endl;
}

class b: public go{
public:
    virtual void vykresli();
};

void b::vykresli(){
    cout << "Vykresluji b" << endl;
}

int main(int argc, char *argv[])
{
    go *pole[2];
    pole[0] = new a();
    pole[1] = new b();
    
    for (int i = 0; i < 2; i++)
        pole[i]->vykresli();

    return 0;
}
// $Revision: 30.11 $
// 
// Copyright 1993-2006 Sun Microsystems, Inc. 901 San Antonio Road,
// Palo Alto, California, 94303, U.S.A.  All Rights Reserved.
// 
// This software is the confidential and proprietary information of Sun
// Microsystems, Inc. ("Confidential Information").  You shall not
// disclose such Confidential Information and shall use it only in
// accordance with the terms of the license agreement you entered into
// with Sun.
// 
// CopyrightVersion 1.2


/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */
   

// This program reads an include file database.
// The database should cover each self .cpp and .hh file,
//   but not files in /usr/include
// The database consists of pairs of nonblank words, where the first word is
//   the filename that needs to include the file named by the second word.
// For each .cpp file, this program generates a fooIncludes.hh file that 
//  the .cpp file may include to include all the needed files in the right order.
// It also generates a foo.dep file to include in the makefile.
// Finally it detects cycles, and can work with two files, an old and a new one.
// To incrementally write out only needed files after a small change.
//
// Based on a suggestion by Roland Conybeare, algorithm suggested by Craig
//  Chambers, written by David Ungar, 3/1/89.
//  Added PREFIX, {DEP/INC}_DIR, smaller dep output  10/92  -Urs

// Add something for precompiled headers


/* The following platform-file functionality is gone as of 07/2012 
 * in favor of an external preconfiguration of includeDB.

// To handle different platforms, I am introducing a platform file.
// The platform file contains lines like:
// os = svr4
//
// Then, when processing the includeDB file, a token such as <os>
// gets replaced by svr4. -- dmu 3/25/97

 */

// Modified to centralize Dependencies to speed up make -- dmu 5/97

// a line like: "filename no_precompiled_headers" means that a .cpp file
// does not use a precompiled header.

// Changed to support new filename extensions: .cpp and .hh -- dmu 2/01

// Changed to support preprocessed Assembler filess: .S -- topa 6/12

// Changed to support Mac OS X specific Objective-C extensions: .m and .mm -- topa 6/12

# ifdef __linux__
#   include <new>
# endif


# include <stdio.h>
# include <stdlib.h>
# include <string.h>
# include <ctype.h>


# define TARGET_IS_SELF


typedef int Bool;
const Bool False = 0;
const Bool True = 1;

static const int maxMatches = 10;

class FileName { 
protected:
  char dir[BUFSIZ], prefix[BUFSIZ], stem[BUFSIZ], suffix[BUFSIZ], 
       altSuffix[BUFSIZ], inverseDir[BUFSIZ];

  char dpss[BUFSIZ], psa[BUFSIZ], dpsa[BUFSIZ], pss[BUFSIZ];

public:
  void check_length();

  FileName(const char* d, const char* p, const char* s, const char* x, const char* i, const char* a) {
    strcpy( dir, d);
    strcpy( prefix, p);
    strcpy( stem, s);
    strcpy( suffix, x);
    strcpy( inverseDir, i);
    strcpy( altSuffix, a);

    sprintf(  pss, "%s%s%s",        prefix, stem, suffix);
    sprintf( dpss, "%s%s%s%s", dir, prefix, stem, suffix);
    sprintf(  psa, "%s%s%s",        prefix, stem, altSuffix);
    sprintf( dpsa, "%s%s%s%s", dir, prefix, stem, altSuffix);

    check_length();
  }

  inline const char* dir_pre_stem_suff()    { return dpss; }
  inline const char* pre_stem_altSuff()     { return psa; }
  inline const char* dir_pre_stem_altSuff() { return dpsa; }
  inline const char* pre_stem_suff()        { return pss; }

  FileName* copy_stem(const char* newStem) {
    return new FileName(dir, prefix, newStem, suffix, inverseDir, altSuffix);
  }

  Bool suffix_matches(const char* s);

  const char* nameOfList() { return stem; }

  const char* getInvDir() { return inverseDir; }

};


// file name templates capture naming conventions

FileName * DummyFileTemplate = new FileName( "", "", "", "", "", "");

FileName * InclFileTemplate;  // an incl file is produced per .cpp file and contains all the includes it needs
FileName *   GIFileTemplate;  // a GI (grand-include) file has any file used more than N times for precompiled headers
FileName*    GDFileTemplate;  // a GD (grand-dependencies) file that tells Unix make all the .o's needed for linking and the include dependencies




// I define what must be specified for each platform
class AbstractPlatform {
 public:
  // an incl file is the file included by each.cpp file that includes all needed header files

    
  void setupFileTemplates() {
    abort(); // should not run me; copy me for each platform
    InclFileTemplate = new FileName( "", "_", "",                     ".incl", "", "");
      GIFileTemplate = new FileName( "", "",  "", "",      "", "");
      GDFileTemplate = new FileName( "", "",  "Dependencies.hh",       "",      "", "");
  }

  const char** outer_suffixes() { 
    abort(); // should not run me; copy me for each platform
    /* Compiler bug:
      static char *suffs[] = { ".cpp", 0 };
      return suffs;
    */
    return NULL;
  }

  // empty file name -> no grand include file
  inline Bool haveGrandInclude() { return GIFileTemplate->nameOfList()[0]; }

  inline Bool writeDeps() const { return GDFileTemplate->nameOfList()[0]; }

  // a gi file is the grand-include file. It includes in one file any file that
  // is included more than a certain number of times.
  // It is used for precompiled header files.
  // It has a source name, that is the file that this program generates,
  // and a compiled name; that is the file that is included by other files.
  // (Non-animorphic platforms have this program actually explictly include
  // the preprocessed gi file-- see includeGIInEachIncl().)
  // Also, some platforms need a pragma in the GI file.
  
  Bool includeGIInEachIncl() { return False; }

  // For some platforms, e.g. Solaris, include the grand-include dependencies
  // in the makefile. For others, e.g. Windows, do not.

  Bool includeGIDependencies() { return False; }

  void writeGIPragma(FILE* /*giFile*/) {}

  // Use writeGIInclude to write out the actual include statement in an indiv. incl file
  // that pulls in the Grand Include file.
 
  void writeGIInclude(FILE* /* inclFile */) {
    fatal("writeGIInclude unimplemented for this platform");
  }
  
  
  // The comment prefix indicates a comment line in includeDB
  
  inline const char* commentPrefix()  const { return "//"; }


  // A line with a filename and the noGrandInclude string means that
  // this file cannot use the precompiled header.
  
  inline const char* noGrandInclude()  const { return "no_precompiled_headers"; }
  
  // A line with a filename and the generatePlatformDependentInclude means that
  // an include file for the header file must be generate.
  // Does not effect the dependency computation.

  // For example, the line foo_<arch>.hh generate_platform_dependent_include
  // means that this program will create a file called "foo_pd.hh"
  // containing # include "foo_sparc.hh" (assuming arch = sparc).
    
  inline const char* generatePlatformDependentInclude()  const { return "generate_platform_dependent_include"; }

  // Format strings for emitting Makefile rules
  virtual const char* obj_file_format()  = 0;
  virtual const char* asm_file_format()  = 0;
  virtual const char* dependent_format() = 0;

  // platform-dependent exit routines:
  // abort means an internal error
   
  virtual void abort() = 0;
   
  void set_args(int& /*argc*/, const char**& /*argv*/) {} // per-platform argument setting

  Bool fileNameStringEquality(const char* a, const char* b) { return strcmp(a, b) == 0; }

  void fileNamePortabilityCheck(const char* name );
  void fileNamePortabilityCheck(const char* name, const char* matchingName);

  int fileNameLengthLimit() { return 31; } // max is 31 on mac, so warn

  int  defaultGrandIncludeThreshold() { return 30; }
  
  virtual const char* GIFileForDependency() { return GIFileTemplate->pre_stem_suff(); }

  virtual void fatal(const char* msg = "See console window") { fprintf(stderr, "%s\n", msg); exit(1); }
};

# if defined(__GNUC__) && defined(__APPLE__)

// For Apple Project Builder under OS X (MACHO) or just gcc on OS X

class MACOSX_Platform: public AbstractPlatform {
 public:
  void setupFileTemplates() {
    InclFileTemplate = new FileName( "incls/", "_", "",  ".incl", "", "");
      GIFileTemplate = new FileName( "incls/", "",  "_precompiled", ".hh", "", ".hh.gch");
      GDFileTemplate = new FileName( "", "",  "Dependencies.hh",      "",      "", "");
  }
      
  const char** outer_suffixes() { 
      static const char *suffs[] = { ".cpp", ".c", ".s", ".S", ".m", ".mm", 0 };
      return suffs;
  }

  const char* obj_file_format()  { return "%s.o"; }
  const char* asm_file_format()  { return "%s.i"; }
  const char* dependent_format() { return "%s";   }

  void abort()     { ::abort(); }

  int defaultGrandIncludeThreshold() { return 10000; }

  Bool includeGIDependencies() { return True; }
 
  Bool includeGIInEachIncl() { return False; }

  void writeGIInclude(FILE* inclFile ) {
    fprintf(inclFile, "# include \"%s\"\n", GIFileTemplate->pre_stem_suff());
  }
 
  void set_args(int& argc, const char**& argv) {
#   ifdef TARGET_IS_SELF
      if (argc == 1) {
          static const char *default_args[] = {
              "makeDeps", "-100000",  "includeDB", "../../src/includeDB"};
          argc = sizeof(default_args) / sizeof(default_args[0]);
          argv = default_args;
      }
#   endif
  }

} Plat;

# elif defined(_WIN32)

#include <windows.h> // for MessageBox

class WinGammaPlatform: public AbstractPlatform {
 public:
  void setupFileTemplates() {
    InclFileTemplate = new FileName( "incls\\", "_", "",                      ".incl", "", "");
      GIFileTemplate = new FileName( "incls\\", "",  "_precompiled", ".incl", "", "");
      GDFileTemplate = new FileName( "", "",  "Dependencies",         "",      "", "");
  }

  const char** outer_suffixes() { 
   static const char* suffs[] = { ".cpp", ".c", 0 };
   return suffs;
  }

  const char* obj_file_format()  { return "%s.obj";       }
  const char* asm_file_format()  { return "%s.i";         }
  const char* dependent_format() { return "$(VM_PATH)%s"; }

  Bool includeGIInEachIncl() { return False;}

  void abort() { ::abort(); }
     
  // I think windows file names are case-insensitive -- dmu
  virtual Bool fileNameStringEquality(const char* a, const char* b) { return stricmp(a, b) == 0; }

  void fatal(const char* msg = "See console window") {
    MessageBox(NULL, msg, "makeDeps error:", MB_OK);
    exit(1);
  }

} Plat;

# elif defined(__linux__)

class LinuxPlatform: public AbstractPlatform {
 public:
  void setupFileTemplates() {
    InclFileTemplate = new FileName( "incls/", "_", "",                     ".incl", "", "");
      GIFileTemplate = new FileName( "incls/", "",  "_precompiled", ".hh", "", ".hh.gch");
      GDFileTemplate = new FileName( "", "",  "Dependencies.hh",      "",      "", "");
  }
      
  const char** outer_suffixes() { 
    static const char* suffs[] = { ".cpp", ".c", ".s", ".S", 0 };
    return suffs;
  }

  const char* obj_file_format()  { return "%s.o"; }
  const char* asm_file_format()  { return "%s.i"; }
  const char* dependent_format() { return "%s";   }

  void abort()     { ::abort(); }
  int defaultGrandIncludeThreshold() { return 100; } // does not speed things up

  // For Unix make, include the dependencies for precompiled header files.
  Bool includeGIDependencies() { return True; }

  Bool includeGIInEachIncl() { return False; }

  void writeGIInclude(FILE* inclFile ) {
    fprintf(inclFile, "# include \"%s\"\n", GIFileTemplate->pre_stem_suff());
  }
  
  const char* GIFileForDependency() { return GIFileTemplate->pre_stem_altSuff(); }
 
} Plat;


# else

class UnixPlatform: public AbstractPlatform {
 public:
  void setupFileTemplates() {
    InclFileTemplate = new FileName( "incls/", "_", "",                     ".incl", "", "");
      GIFileTemplate = new FileName( "incls/", "",  "_precompiled", ".hh", "", ".hh.gch");
      GDFileTemplate = new FileName( "", "",  "Dependencies.hh",      "",      "", "");
  }
      
  const char** outer_suffixes() { 
    static const char *suffs[] = { ".cpp", ".c", ".s", ".S", 0 };
    return suffs;
  }

  const char* obj_file_format()  { return "%s.o"; }
  const char* asm_file_format()  { return "%s.i"; }
  const char* dependent_format() { return "%s";   }

  void abort()     { ::abort(); }
  // Do not change this; unless you fix things so precompiled header files
  // get translated into make dependencies. - Ungar
  int defaultGrandIncludeThreshold() { return 1 << 30; }

  // For Unix make, include the dependencies for precompiled header files.
  Bool includeGIDependencies() { return True; }

} Plat;


# endif


void FileName::check_length() {
  const char* s = strlen(suffix) >= strlen(altSuffix)  ?  suffix  :  altSuffix;
  int len = strlen(prefix) + strlen(stem) + strlen(s);
  int lim = Plat.fileNameLengthLimit();
  if ( len  >  lim ) {
    char buf[BUFSIZ];
    sprintf( buf, "%s%s%s is too long; %d >= %d\n",
             prefix, stem, s, len, lim);
    Plat.fatal(buf);
  }
}


Bool suffix_matches(const char* s, const char** suffixes) {
  register int len = strlen(s);
  for ( char** suffp = (char**) suffixes;  *suffp;  ++suffp ) {
    register int suffLen = strlen(*suffp);
    if ( len >= suffLen  
    &&  Plat.fileNameStringEquality(&s[len - suffLen], *suffp))
         return True;
  } 
  return False;
}

Bool is_outer_file(const char* s) { return suffix_matches(s, Plat.outer_suffixes()); }



int exitCode = 0;



class item;
class database;

const item* NullItem = 0;

// a list (of files)

class list {
 public:
  item *first;
  item *last;
  char *name;

  Bool beenHere;
  Bool mayBeCycle;
  Bool isCycle;
  Bool useGrandInclude; // put in list because a file can refuse to
  const char* platformDependentInclude; // e.g. if this is sig_unix.hh this field contains sig_pd.hh
  list* platformDependentIncludees; // e.g. if this is sig.hh this field contains sig_unix.hh
  int  count;

  list(const char* n) {
    first = last = (item*)NullItem;
    beenHere =  mayBeCycle = isCycle = False;
    platformDependentInclude = NULL;
    platformDependentIncludees = NULL;
    name = new char[(int)strlen(n) + 1];
    count = 0;
    strcpy(name, n);
    useGrandInclude = Plat.haveGrandInclude();
  }

  list* listForFile(const char*);
  Bool  isEmpty() { return first == 0; }
  void  add(list*);
  void  addFirst(list*);
  void  addIfAbsent(list*);
  void  doFiles(list* s);
  list* doCFile();
  void  doHFile(list* s);
  void  traceCycle(list* s);
  Bool  compareLists(list* s, database* cur = NULL, database* prev = NULL);

  void  put_incl_file( database* );
};


// a list item

class item {
 public:
  item* next;
  list *contents;
  item(list* c) {next = (item*)NullItem;  contents = c;}
};

// the whole database

class database {
  list*   allFiles;
  list* outerFiles;
  list* indiv_includes;
  list* grand_include; // the results for the grand include file
  long int   threshold;
  int   nOuterFiles;
  int   nPrecompiledFiles;
  Bool  missing_ok;
  
 public:
  
  void absolute_generation(const char* new_db_fn);
  void relative_generation(database* prev, const char* old_db_fn, const char* new_db_fn);
  void copy_file(const char* src, const char* dst);
  void can_be_missing() { missing_ok = True; }
  void get(const char * db_file);
  void compute();
  void put();
  void verify();
  void putDiffs(database*);

   database(long int t) {
      allFiles      = new list("allFiles");
    outerFiles      = new list("outerFiles");
    indiv_includes  = new list("indiv_includes");
     grand_include  = new list(GIFileTemplate->nameOfList());
     
    threshold = t;
    nOuterFiles = 0;
    nPrecompiledFiles = 0;
    missing_ok = False;
  };
  
  Bool hfile_is_in_grand_include(list* hfile, list* cfile) {
    return  hfile->count >= threshold  &&  cfile->useGrandInclude;
  }

  protected:
   void create_file_if_absent( const char* );
   void write_grand_include();
   void write_grand_unix_makefile();
   void write_individual_includes();
   void write_individual_includes(database* previous);
   
   void get_pair(const char* includer, const char* includee,
                 const char* db_fileName, const char* line, int lineNo // for errors
                 );
   void get_quad(const char* plat_dep_inc, 
                 const char* foo_hh, const char* foo_pd_hh, const char* foo_arch_hh,
                 const char* db_fileName, const char* line, int lineNo // for errors
                 );
};



void AbstractPlatform::fileNamePortabilityCheck(const char* name, const char* matchingName) {
  if (strcmp(name, matchingName) != 0) {
    char err[BUFSIZ];
    sprintf(err, 
      "Error: file %s also appears as %s.  "
      "Case must be consistent for portability.\n",
      matchingName, name);
    Plat.fatal(err);
  }
}


void AbstractPlatform::fileNamePortabilityCheck(const char* name) {
  if ('A' <= name[0]  &&  name[0] <= 'Z') {
    char err[BUFSIZ];
    sprintf(err, 
      "Error: for the sake of portability\n"
            " we have chosen to avoid files starting with an uppercase letter.\n"
            " Please rename %s.",
            name);
    Plat.fatal(err);
  }
}


     
list* list::listForFile(const char* namea) {
  for ( register item* p = first;  p;  p = p->next)
    if ( Plat.fileNameStringEquality(p->contents->name, namea) ) {
      Plat.fileNamePortabilityCheck( namea, p->contents->name );
      return p->contents;
    }

  Plat.fileNamePortabilityCheck( namea ); 
  register list *s = new list(namea);
  add(s);
  return s;
}


Bool list::compareLists(register list* s, database* cur, database* prev) {
  if (    platformDependentInclude != NULL
  &&   s->platformDependentInclude != NULL
  &&   strcmp(    platformDependentInclude, 
               s->platformDependentInclude ) != 0 )
    return False; // difference in platformDependentIncludes
    
  register item *mine, *his;
  for ( mine = first,  his = s->first;
        ;
        mine = mine->next,  his = his->next) {

    if ( (mine == NullItem)  !=  (his == NullItem) )
            return False; // one ends first
    if ( mine == NullItem )
            return True; // equal
    if ( !Plat.fileNameStringEquality( mine->contents->name, his->contents->name))
            return False; // crude: order dependent
    if ( cur == NULL )
      ;
    else if ( cur ->hfile_is_in_grand_include( mine->contents, this )
         !=   prev->hfile_is_in_grand_include( his->contents,  s    ))
            return False; // one is in grand, other is not
  }
}


void list::add(list* s) {
  register item* i = new item(s);
  // next two statements are for debugging
  if (  i->next     )    { printf("next %p\n", i->next);      Plat.abort();}
  if ( !i->contents )    { printf("c = %p\n",  i->contents);  Plat.abort();}
  
  if (last)   last->next = i;
  else             first = i;
  
  last = i;
}


void list::addFirst(list* s) {
  register item* i = new item(s);
  if (first) {
      i->next = first;
      first = i->next;
  }
  else {
      first = last = i;
  }
}


void list::addIfAbsent(list* s) {
  for ( register item* p = first;  p;  p = p->next)
    if (p->contents == s)
      return;
  add(s);
}


void list::doFiles(list* s) {
  for ( register item* p = first;  p;  p = p->next) {
    list* h = p->contents;
    if (h->platformDependentInclude != NULL) {
      fprintf(stderr, "Error: The source for %s is %s.\n\tIt shouldn't be included directly by %s.\n",
        h->platformDependentInclude,
        h->name,
        name);
      h->platformDependentInclude = NULL; // report once per file
      exitCode = 1;
    }
    h->doHFile(s);
  }
}


void list::traceCycle(list* s) {
  if (isCycle) // already traced
    return;
  isCycle = True;
  fprintf(stderr, "\ttracing cycle for %s\n", name);
  exitCode = 1;
  for (register item* p = first; p; p = p->next) {
    register list* q = p->contents;
    if (q->mayBeCycle) {
      if (s == q) {
        char err[BUFSIZ];
        sprintf(err, "\tend of cycle for %s\n", s->name);
        Plat.fatal(err);
      }
      else {
        q->traceCycle(s);
      }
    }
  }
}


void list::doHFile(list* s) {
  if (beenHere) {
    if (mayBeCycle) 
      traceCycle(this);
    return;
  }
  beenHere = True;
  mayBeCycle = True;
  doFiles(s);
  mayBeCycle = False;
  s->add(this);
}


list* list::doCFile() {
  list* s = new list(name);
  s->useGrandInclude = useGrandInclude; // propagate this
  doFiles(s);
  for ( register item *si = s->first;  si;  si = si->next)
    si->contents->beenHere = False;
  return s;
}


FILE* fileFor(const char* fname) {
  if (fname == NULL  ||  fname[0] == '\0')
    Plat.fatal("fileFor: empty or null name");
  FILE *r = fopen(fname, "w");
  if (r == NULL) {
    perror(fname);
    Plat.fatal();
  }
  return r;
}


char* remove_suffix_from(const char* s) {
  char* r = new char[BUFSIZ];
  strcpy( r, s);
  char* p;
  for ( p = &r[strlen(r)];  *p != '.';  --p)
    if (p <= r)  abort();
  *p = '\0';
  return r;
} 


// if .hh file is included thresh times, put it in the grand include file
void list::put_incl_file( database* db ) {

  FileName* inclName = InclFileTemplate->copy_stem(name);
  FILE*     inclFile = fileFor( inclName->dir_pre_stem_suff() );
  

  if ( Plat.haveGrandInclude()  &&  Plat.includeGIInEachIncl()  &&  useGrandInclude ) {
    Plat.writeGIInclude(inclFile);
  }
  
  for (register item *si = first;  si;  si = si->next) {
    if ( !db->hfile_is_in_grand_include( si->contents, this ) )
      fprintf( inclFile, 
               "# include \"%s%s\"\n", 
               InclFileTemplate->getInvDir(),
               si->contents->name);
  }
  fclose(inclFile);
}


void database::get(const char* db_fileName) {
  FILE* f = fopen(db_fileName, "r");
  if (!f && missing_ok)  return;
  if (!f) perror(db_fileName), Plat.fatal();
  printf("\treading database: %s\n", db_fileName);

  int lineNo = 0;
  while (!feof(f)) {
#   define LEN 1024
    char line[LEN];
    
    
    // read line into line[]
    int i, c;
    for ( i = 0,  c = fgetc(f);
          i < LEN-1  &&  c != EOF  &&  (char)c != '\n'  &&  (char)c != '\r';
          ++i,  c = fgetc(f) )
      line[i] = (char)c;

    line[i] = '\0';
    lineNo++;
    
    // check for comment
    if ( strncmp( line,  Plat.commentPrefix(),  strlen(Plat.commentPrefix()))
         == 0)
      continue;

    static char token1[BUFSIZ];
    static char token2[BUFSIZ];
    static char token3[BUFSIZ];
    static char token4[BUFSIZ];
    
    static bool cannot_recurse = 0;
    if (cannot_recurse)
      Plat.fatal("cannot_recurse");
    cannot_recurse = true;

    token1[0] = token2[0] = token3[0] = token4[0] = '\0';
    int n = sscanf(line, " %s %s %s %s", token1, token2, token3, token4);
    if (n <= 0) {
      // empty line?
      char *c;
      for (c = line; *c && isspace(*c); c++) {}
      if (*c == '\0')
        n = 0; // flag empty line, error
      else if (n == 0)
        n = -1; // flag error
    }
    
    if ( n == 0 ) {  // empty line
    } 
    else if (n == 2)
      get_pair(token1, token2,                 db_fileName, line, lineNo - 1);   
    else if (n == 4) 
      get_quad(token1, token2, token3, token4, db_fileName, line, lineNo - 1);
    else {
      char err[BUFSIZ];
      sprintf(err, "invalid line: \"%s\"\nerror position: line %ld\n", line, (long)lineNo);
      Plat.fatal(err);
    }
    cannot_recurse = false;  
  }   
}




void database::get_pair(const char* includer, const char* includee,
                        const char* dbf, const char* line, int lineNo // for errors
                        ) {
  
  list *p = allFiles->listForFile(includer);
  
    if (is_outer_file(includer))
    outerFiles->addIfAbsent(p);
  
  if (strcmp( includee, Plat.generatePlatformDependentInclude() ) == 0 ) {      
    char err[BUFSIZ];
    sprintf(err, "invalid line: \"%s\"\nerror position: line %ld."
            " Old-style %s. Use \"%s foo.hh foo_pd.hh foo_<arch>.hh\".\n",
            line, (long)lineNo, Plat.generatePlatformDependentInclude(), 
            Plat.generatePlatformDependentInclude());
    Plat.fatal(err);
  } 
  else if ( strcmp( includee, Plat.noGrandInclude() ) == 0 ) {
    p->useGrandInclude = False;
  }
  else {            
    list *q = allFiles->listForFile(includee);
    p->addIfAbsent(q);
  }
}


void database::get_quad(const char* plat_dep_inc, 
                        const char* foo_hh, const char* foo_pd_hh, const char* foo_arch_hh,
                        const char* dbf, const char* line, int lineNo // for errors
                       ) {
  if (strcmp( plat_dep_inc, Plat.generatePlatformDependentInclude()) != 0) {
      char err[BUFSIZ];
      sprintf(err, "invalid line: \"%s\"\nerror position: line %ld."
              " First token of four-word line should be %s.\n", line, (long)lineNo, Plat.generatePlatformDependentInclude());
      Plat.fatal(err);
  }
  
  FileName* foo_pd_hh_name = InclFileTemplate->copy_stem(foo_pd_hh);
  const char* incls_foo_pd_hh = foo_pd_hh_name->dir_pre_stem_suff();
  FILE* foo_pd_file = fileFor( incls_foo_pd_hh );
  fprintf(foo_pd_file, "# include \"%s\"\n", foo_arch_hh);
  fclose(foo_pd_file);
  
  list *foo_pd_list = allFiles->listForFile(foo_pd_hh);
  foo_pd_list->platformDependentInclude = incls_foo_pd_hh; // mark foo_unix.hh for error checking    
  
  list* foo_list = allFiles->listForFile(foo_hh);
  if (foo_list->platformDependentIncludees == NULL)
    foo_list->platformDependentIncludees = new list("my platformDependentIncludees");
  foo_list->platformDependentIncludees->add(allFiles->listForFile(foo_arch_hh));
}


void database::compute() {
  printf("\tcomputing closures\n");
  register item* si;
  for ( si = outerFiles->first;  si;  si = si->next) {
    // build both indiv and grand results
    indiv_includes->add(si->contents->doCFile());
    ++nOuterFiles;
  }
  if (!Plat.haveGrandInclude())
    return; // nothing in grand include
          
  // count how many times each include is included in grand-includable context
  // & add em to grand
  for ( register item* cf = indiv_includes->first;  cf;  cf = cf->next) {
    list *indiv_include = cf->contents;

    if ( !indiv_include->useGrandInclude )
      continue; // do not bump count if my files cannot be in grand include

    indiv_include->doFiles(grand_include); // put em on grand_include list


    for ( register item* incListItem = indiv_include->first;
          incListItem;
          incListItem = incListItem->next ) {
      ++incListItem->contents->count;
    }
  }
}


void database::verify() {
  for (register item *si = indiv_includes->first;  si;  si = si->next) {
    if ( !si->contents) Plat.abort();
  }
}

void database::write_individual_includes() {
  printf("\twriting individual include files\n");
  
  register item* si;
  for ( si = indiv_includes->first;  si;  si = si->next) {
    printf("\tcreating %s\n", si->contents->name);
    si->contents->put_incl_file( this );
  }
}

void database::write_individual_includes(database* previous) {
  register item *ci, *pi;
  for ( ci = indiv_includes->first,  pi = previous->indiv_includes->first;
        ci;
        ci = ci->next,  pi = pi->next) {

    register list  *newCFileList = ci->contents;
    register list *prevCFileList = pi->contents;
    if (!newCFileList->compareLists(prevCFileList, this, previous)) {
        printf("\tupdating %s\n", newCFileList->name);
        newCFileList->put_incl_file( this );
    }
  }
}



void database::write_grand_include() {
  printf("\twriting grand include file\n");
  FILE* inc = fileFor(GIFileTemplate->dir_pre_stem_suff());
  Plat.writeGIPragma(inc);
  // For xcode on MacOSX, only way to get compiler to not process grand include file
  // is to go to the .cpp file that does not want precompiled headers
  //  (i.e. grep for no_precompiled_headers in includeDB)
  // open the .cpp file in xcode, hit command-I, and put -D NO_PRECOMPILED_HEADERS
  // in additional compile flags field of inspector. -- dmu 11/03
  
  fprintf( inc,  "# ifdef __cplusplus\n\n");

  for ( item* si = grand_include->first;  si;  si = si->next) {
    //printf("  %4d  %s\n", si->contents->count, si->contents->name);
    if (si->contents->count >= threshold) {
      fprintf( inc, "# include \"%s%s\"\n", 
               GIFileTemplate->getInvDir(), si->contents->name);
      nPrecompiledFiles += 1;
    }
  }
  fprintf( inc, "\n");
  fprintf( inc, "\n# endif // __cplusplus\n");
  fclose( inc );
}


static void print_dependent_on(FILE* gd, const char* name) {
  fprintf( gd, " ");
  fprintf( gd, Plat.dependent_format(), name);
}

void database::write_grand_unix_makefile() {
  if (!Plat.writeDeps()) 
    return;

  // 2012-05-02 topa: newer cpp do not like the multi-line style
  // hence, we generate the file with one-line things directly
  printf("\twriting dependencies file\n");
  FILE* gd = fileFor(GDFileTemplate->dir_pre_stem_suff());
  
  { // write Obj_Files = ...

    fprintf(gd, "Obj_Files = ");
    for ( item* si = outerFiles->first;  si;  si = si->next) {
      list* anOuterFile = si->contents;
      char* stemName = remove_suffix_from(anOuterFile->name);
      fprintf(gd, Plat.obj_file_format(), stemName);
      fprintf(gd, " ");
    }
    fprintf(gd, "\n\n");
  }

  if ( Plat.includeGIDependencies()  &&  nPrecompiledFiles > 0 ) {
      // write Precompiled_Files = ...
      fprintf(gd, "Precompiled_Files = ");
      for ( item* si = grand_include->first;  si;  si = si->next) {
        if (si->contents->count < threshold) 
          continue;
        fprintf(gd, "%s ", si->contents->name);
        // if .hh has plat dep incls, we also depend on them:
        if (si->contents->platformDependentIncludees != NULL)
          for ( item* pdi = si->contents->platformDependentIncludees->first;  pdi;  pdi = pdi->next)
            fprintf(gd, "%s ", pdi->contents->name);
      }
      fprintf(gd, "\n\n");
  }

  { // write each dependency
    for ( item* si = indiv_includes->first;  si;  si = si->next) {
      list* anII = si->contents;

      const char*     stemName = remove_suffix_from(anII->name);
      const char* inclFileName = InclFileTemplate->copy_stem(anII->name)->pre_stem_suff();

      fprintf(gd, Plat.obj_file_format(), stemName);
      fprintf(gd, " ");
      fprintf(gd, Plat.asm_file_format(), stemName);
      fprintf(gd, ": ");

      print_dependent_on(gd, anII->name);
      print_dependent_on(gd, inclFileName );

      if ( Plat.haveGrandInclude() ) {
        print_dependent_on(gd, GIFileTemplate->pre_stem_altSuff());
      }

      for ( item* hi = anII->first;  hi;  hi = hi->next ) {
        if ( hfile_is_in_grand_include( hi->contents, anII ) ) 
          continue;
         print_dependent_on( gd, hi->contents->name);
         // need to add dependency for e.g., sig_unix.hh
        if (hi->contents->platformDependentIncludees != NULL) 
          for ( item* pdi = hi->contents->platformDependentIncludees->first;  pdi;  pdi = pdi->next)
            print_dependent_on(gd, pdi->contents->name);
      }
      fprintf( gd, "\n\n");
    }
  }
  
  // write deps for grand include file:
  if ( Plat.includeGIDependencies()  
  &&   nPrecompiledFiles > 0 ) {
      fprintf( gd, "%s: ", Plat.GIFileForDependency());
      fprintf( gd, "$(Precompiled_Files) \n" );
  }

  fclose( gd );
}


void database::put() {
  write_individual_includes();
  
  if (Plat.haveGrandInclude())
    write_grand_include();
   
  write_grand_unix_makefile();
}

void database::putDiffs(database* previous) {  
  printf("\tupdating output files\n");

  if ( !indiv_includes->compareLists(previous->indiv_includes)
  ||   !grand_include ->compareLists(previous->grand_include )) {
    printf(
      "The order of .cpp or .s has changed, or the grand include file has changed.\n");
    put();
    return;
  }

  write_individual_includes(previous);
   
  write_grand_unix_makefile();
}


int main(int argc, const char **argv) {
  Plat.setupFileTemplates();

  long int t = Plat.defaultGrandIncludeThreshold();
  Plat.set_args(argc, argv);
  
  if (argc > 1  &&  argv[1][0] == '-') {
    t = atoi(&argv[1][1]);
    --argc; ++argv;
  }
  database* previous = new database( t );
  database* current  = new database( t );

  previous->can_be_missing();
  
  switch (argc) {
   case 2:
    current->absolute_generation(argv[1]);
    break;
    
   case 3:
    current->relative_generation(previous, argv[1], argv[2]);
    break;
    
   default:
    Plat.fatal( "usage:\n  makeDeps [-#]     database-file\nor\n"
                        "  makeDeps [-#] old-database-file new-database-file\n");
  }
  if (exitCode != 0) {
    Plat.fatal( "Error: an error occured earlier\n");
  }
  exit(exitCode);
  return exitCode;
}

void database::absolute_generation(const char* new_db_fn) {
  printf("New database:\n");
  get(new_db_fn);
  compute();
  put();
}


void database::create_file_if_absent(const char* fn) {
  FILE* f = fopen(fn, "r");
  if ( f == NULL ) {
    printf("Cannot open old file %s; will create then anew\n", fn);
    f = fopen(fn, "w");
    if (f == NULL) { perror(fn); Plat.fatal(); }
  }
  fclose(f);
}
  
                                   
void database::relative_generation(database* prev,
                                   const char* old_db_fn,
                                   const char* new_db_fn) {

  printf("Old database:\n");  prev->get(old_db_fn);  prev->compute();
  printf("New database:\n");        get(new_db_fn);        compute();

  printf("Deltas:\n");        putDiffs(prev);
  
  printf("Copying new to old files:\n");
  copy_file(new_db_fn,   old_db_fn);
  printf("Copying finished\n");
}

void database::copy_file(const char* src,const char* dst) {
  static char buf[16 * 1024];
  printf("Copying %s -> %s...\n", src, dst);
  FILE* s = fopen(src, "r");  if (s == NULL) { perror(src); Plat.fatal(); }
  FILE* d = fopen(dst, "w");  if (s == NULL) { perror(dst); Plat.fatal(); }
  for (;;) {
    size_t n = fread( buf, 1, sizeof(buf), s );
         if (n == 0)   break;
    else if (ferror(s) != 0) { perror(src); Plat.fatal(); }
    size_t nn = 0;
    for (;;) {
      nn += fwrite( buf, 1, n, d);
           if (nn == n)   break;
      else if (nn >  n) { fprintf(stdout, "what??\n"); Plat.fatal(); }
    }
  } 
  if (fclose(s)) { perror(src); Plat.fatal(); }   
  if (fclose(d)) { perror(dst); Plat.fatal(); }   
  printf("Copy finished\n");
}
/* Sun-$Revision: 30.6 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// replaces first occurrence of "from" pattern with "to" pattern
// used to patch object files

# include <stdio.h>
# include <stdlib.h>
# include <ctype.h>
# include <sys/types.h>
# include <sys/stat.h>
# include <memory.h>
# include <string.h>

void usage(char* program_name) {
  printf("usage: %s [ -DTARGET_OS_VERSION=SOLARIS_VERSION | -DTARGET_OS_VERSION=SUNOS_VERSION ] [ -D<ignored> ] patch_file from-label to-label\n",
   program_name);
}

enum {unknown, solaris, bsd};
int os = unknown;

char* entryPoint(char* name) {
  if (os == solaris) return name;
  // prepend an "_"
  char* new_name = (char*) malloc(strlen(name)+2);
  sprintf(new_name, "_%s", name);
  return new_name;
}


int is_match(char* ident, int len,char* pos,char* buf) {
  if (! pos                              ) return 0;
  if (memcmp(ident, pos, len)            ) return 0;
  if (os == solaris) {
    if (! (pos > buf && *(pos-1) == '\0')) return 0;
  } else {
    if (! (pos > buf && *(pos-1) != '_') ) return 0;
  }
  return 1;
}

int main(int argc, char* argv[]) {
  int index = 1;

  while(index < argc && *argv[index] == '-') {
    if (strcmp("-DTARGET_OS_VERSION=SOLARIS_VERSION", argv[1]) == 0) {
      os = solaris;
    } else if (strcmp("-DTARGET_OS_VERSION=SUNOS_VERSION", argv[1]) == 0) {
      os = bsd;
    }
    index++;
  }

  if (os == unknown || (argc - index) != 3) {
    usage(argv[0]);
    exit(1);
  }
    
  char* fname = argv[index++];
  char* from  = entryPoint(argv[index++]);
  char* to    = entryPoint(argv[index++]);
  int   len   = strlen(from);

  if (len != strlen(to)) {
    printf("error: from- and to-patterns have different length\n");
    exit(1);
  }
    
  struct stat sbuf;
  if (!stat(fname, &sbuf)) {
    int   size = int(sbuf.st_size);
    char* buf = (char*) malloc(size);
    char* end = buf + size - len + 1;
    FILE* f = fopen(fname, "r+");
    if (1 !=  fread(buf, size, 1, f)) {
      perror("error");
      exit(1);
    }
    char* fromPos = buf;
    while(1) {
      fromPos = (char*)memchr(fromPos, from[0], end - fromPos);
      if (is_match(from,len+1,fromPos, buf)) break;
      if (fromPos == NULL) {
  printf("error: from pattern (%s) not found\n", from);
  exit(1);
      }
      fromPos++;
    }
    memcpy(fromPos, to, len);
    rewind(f);
    fwrite(buf, size, 1, f);
    free(buf);
    fclose(f);
  } else {
    perror("error");
    exit(1);
  }
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "monitorWindow_unix.hh"
# pragma implementation "monitorWindow.hh"


# include "_monitorWindow.cpp.incl"

MonitorWindow::MonitorWindow() {
  _m = NULL;
  pw = NULL;
}


bool MonitorWindow::open_and_resize(Monitor* m) {
  _m = m;
  
  pw = 0;
# ifdef QUARTZ_LIB
  if (!SpyDisplay[0]) {
    pw = (AbstractPlatformWindow*) new QuartzWindow;
  }
# endif
  
# ifdef XLIB
  if (!pw) {
    pw = (AbstractPlatformWindow*) new XPlatformWindow;
  }
# endif
  
  if (!pw) {
    warning("no window system for spy");
    return false;
  }
  
  // open the window, and resize for the spy
  // set size hints to don't care cause we will resize window below
  if (!  pw->open( compute_display_name(),
                  100, 100, 500, 500,
                  -1,  -1,  -1,  -1,
                  compute_window_name(),
                  "Spy",
                  SpyFont[0] ? SpyFont : pw->default_fixed_font_name(),
                  pw->default_fixed_font_size()))
    return false;
  
  // Spy uses fixed-width font and sizes window accordingly.
  // On some platforms, cannot get font till window is open.
  // So, now that window is opened, can ask spy for height, and
  // can set window place/size and size limits.
  if (!pw->change_extent( initial_left(), initial_top(), initial_width(), initial_height()))  { close();  return false; }
  if (!pw->change_size_hints( -1,  initial_width(), initial_height(), initial_height()))       { close();  return false; }
  return true;
}




const char* MonitorWindow::compute_display_name() {
  const char* ev = OS::get_environment_variable("DISPLAY");
  const char* prim = SpyDisplay;
  return  prim && prim[0]  ?  prim
       :  ev               ?  ev
       :  ":0";
}

    
const char* MonitorWindow::compute_window_name() {
  const char *username= OS::get_user_name();
  const char *hostName= OS::get_host_name();
  const char* window_base_name = "Self Spy of ";
  char* window_name =
    (char*)selfs_malloc(strlen(window_base_name) + strlen(username) + 1
                        + strlen(hostName) + 1);
  sprintf(window_name, "Self Spy of %s@%s", username, hostName);
  return window_name;
}


void MonitorWindow::close() {
  if (pw->is_open())
    pw->close();
  if (my_monitor()->is_active())
    my_monitor()->deactivate(); 
}


void MonitorWindow::adjust_after_resize() {
  my_monitor()->adjust_after_resize();
 pw->adjust_after_resize();
}
                 

void MonitorWindow::full_redraw() {
  // Do not call  PlatformWindow::full_redraw();
  // It does Begin/End Updates which mess up the spy.
  my_monitor()->full_redraw();
}


// Based on screen size, foctors in insets, that is size of window frame
int MonitorWindow::initial_left()   { return  0; }
int MonitorWindow::initial_top()    { return  pw->screen_height() - initial_height(); }
int MonitorWindow::initial_width()  { return  pw->screen_width(); }
int MonitorWindow::initial_height() { return  my_monitor()->contents_height() + pw->inset_top() + pw->inset_bottom(); }

int MonitorWindow::width()  { return pw->width(); }
int MonitorWindow::height() { return pw->height(); }

int MonitorWindow::font_width()  { return pw->font_width(); }
int MonitorWindow::font_height() { return pw->font_height(); }
/* Sun-$Revision: 30.7 $ $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "timer.hh"
# pragma implementation "timer_abstract.hh"
# pragma implementation "timer_inline.hh"

# include "_timer.cpp.incl"

const long  ElapsedTimer::one_million  = 1000 * 1000;
const float ElapsedTimer::one_thousand = 1000.0;


void ElapsedTimer::init(bool start_timer) {
  reset(); 
  if (start_timer) start();
}

void ElapsedTimer::reset() { 
  is_running = false;
  reset_platform();
}


void ElapsedTimer::print() { 
  lprintf("%2.2f (ms)", millisecs()); 
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation

# include "_xlibWindow.cpp.incl"

# ifdef XLIB

// First: cross-plat fns:


// Creation, destruction:


XPlatformWindow::XPlatformWindow() : AbstractPlatformWindow() {
  _display   = NULL;
  _font_info = NULL;
}


// Open/Close:

bool XPlatformWindow::open( const char* display_name,
                            int x, int y, int w, int h,
                            int min_w, int max_w, int min_h, int max_h, // -1 for don't care
                            const char* window_name,  const char* icon_name,
                            const char* font_name,    int   /*font_size unimp X*/ ) {

  // (adapted from Spy open routine, needs font info for sizing)
  // XOpenDisplay fails silently if a signal is received during the call.
  // All signals except user interrupts are therefore blocked. 
  SignalBlocker sb(SignalBlocker::allow_user_int);
  
  if (!open_xdisplay(display_name)) { close();  return false; }
  
  bool debugMe = false;              // set to true when debugging X
  XSynchronize(_display, debugMe);  
  XSetErrorHandler(XErrorHandlers::handle_X_error);

  _screen_num     = DefaultScreen(_display);
  _display_width  = DisplayWidth (_display, _screen_num);
  _display_height = DisplayHeight(_display, _screen_num);
  
  _window_x = x;  _window_y = y;  _width = w;  _height = h;

  // create the window; will be resized and repositioned when reparented
  Window root = RootWindow(_display, _screen_num);
  
  _xwindow = XCreateSimpleWindow(  _display, root,
                                   _window_x, _window_y,
                                   width(), height(), 0,
                                   BlackPixel(_display, _screen_num),
                                   WhitePixel(_display, _screen_num));
  
  if (!set_font_info(font_name))                       {  close();  return false; }
  if (!change_size_hints(min_w, max_w, min_h, max_h))  {  close();  return false; }
  
  if (!set_name(     window_name))  {  close(); return false; }
  if (!set_icon_name(  icon_name))  {  close(); return false; }
  
  setup_events();  
  XMapWindow(_display, _xwindow);
  
 if (!setup_gcs())      {  close(); return false; }
 
  return true;
}



bool XPlatformWindow::open_xdisplay(const char *n) {
  if (n == NULL) n = "";
  _display = XOpenDisplay(n);
  if (_display == NULL) {
    warning2("cannot open X display '%s' a.k.a. '%s'! window won't work.",
             n, XDisplayName(n));
    return false;
  }
  return true;
}

  
bool XPlatformWindow::set_icon_name(const char* icon_name) {  
  XTextProperty iconName;
  if (XStringListToTextProperty((char**)&icon_name, 1, &iconName) == 0) {
    warning("X structure allocation for icon name failed--window won't work.");
    return false;
  }
  XSetWMIconName(_display, _xwindow, &iconName);
  return true;
}


bool XPlatformWindow::set_name(const char* window_name) {
  XTextProperty windowName;
  if (XStringListToTextProperty((char**)&window_name, 1, &windowName) == 0) {
    warning("X structure allocation for window name failed--window won't work.");
    return false;
  }
  XSetWMName(_display, _xwindow, &windowName);
  return true;
}


void XPlatformWindow::setup_events() {  
  // to catch the clientMessage event when user deletes
  _wmProtocolsAtom    = XInternAtom(_display, "WM_PROTOCOLS",     false);
  _wmDeleteWindowAtom = XInternAtom(_display, "WM_DELETE_WINDOW", false);
  XSetWMProtocols(_display, _xwindow, &_wmDeleteWindowAtom, 1);
  
  // choose events to receive
  long event_mask = ExposureMask | StructureNotifyMask;
  XSelectInput(_display, _xwindow, event_mask);
}


bool XPlatformWindow::tell_platform_size_hints() {
  if ( _min_w == -1)
    return true; // hack for don't care
    
  // tell window manager that we'd like our own size and position
  XSizeHints* size_hints;
  if ((size_hints = XAllocSizeHints()) == NULL) {
    warning("X structure allocation for size hints failed--spy won't work.");
    close();
    return false;
  }
  size_hints->flags = PPosition | PSize | PMinSize | PMaxSize;
  size_hints->min_width  = _min_w;
  size_hints->max_width  = _max_w;
  size_hints->min_height = _min_h;
  size_hints->max_height = _max_h;
  XSetWMNormalHints(_display, _xwindow, size_hints);
  return true;
}


bool XPlatformWindow::set_font_info(const char* font_name) {  
  // must set _font_info here so that font_height() is defined for BOTTOM 
  _font_info = XLoadQueryFont(_display, font_name);
  return _font_info != NULL;
}


bool XPlatformWindow::setup_gcs() {
  // create 3 gc's and set their attributes
  unsigned long valuemask = 0;
  XGCValues values;
  _gc = XCreateGC(_display, _xwindow, valuemask, &values);
  
  XSetFont(_display, _gc, _font_info->fid);
  XSetForeground(_display, _gc, BlackPixel(_display, _screen_num));
  XSetBackground(_display, _gc, WhitePixel(_display, _screen_num));
  
  // 16x16 grey stipple pixmap (16x16 is preferred stipple size)
  const int grey_width = 16;
  const int grey_height = 16;
  static unsigned char grey_bits[] = {
    0x55, 0x55, 0xaa, 0xaa, 0x55, 0x55, 0xaa, 0xaa, 0x55, 0x55, 0xaa, 0xaa,
    0x55, 0x55, 0xaa, 0xaa, 0x55, 0x55, 0xaa, 0xaa, 0x55, 0x55, 0xaa, 0xaa,
    0x55, 0x55, 0xaa, 0xaa, 0x55, 0x55, 0xaa, 0xaa};
  Pixmap stipple = XCreateBitmapFromData(_display, _xwindow, (char*)grey_bits,
                                         grey_width, grey_height);
  XSetStipple(_display, _gc, stipple);
  
  _black= BlackPixel(_display, _screen_num);
  _white= WhitePixel(_display, _screen_num);
  _is_mono = DefaultDepth(_display, _screen_num) == 1;
  if (_is_mono)
    _red= _yellow= _gray= _black;
  else {
    Colormap cmap= DefaultColormap(_display, _screen_num);
    XColor col1, col2;
    _red= XAllocNamedColor(_display, cmap, "red", &col1, &col2)
      ? col1.pixel : _black;
    _yellow= XAllocNamedColor(_display, cmap, "gold", &col1, &col2)
      ? col1.pixel : _black;
    _gray= XAllocNamedColor(_display, cmap, "gray", &col1, &col2)
      ? col1.pixel : _black;
  }
  return true;
}


void XPlatformWindow::close() { 
  if (_font_info != NULL) { XFreeFont(_display, _font_info); _font_info = NULL; }
  if (_gc        != NULL) { XFreeGC(  _display, _gc);               _gc = NULL; }
  if (_display   != NULL) { XCloseDisplay(_display);           _display = NULL; }
}



// Insets from outer to inner portion of window. Not sure what to do for X here, these are guesses.
int  XPlatformWindow::inset_left()    { return 0; }  // Mac needs this
int  XPlatformWindow::inset_right()   { return 0; }  // Mac needs this
int  XPlatformWindow::inset_top()     { return 0; }  // Mac needs this
int  XPlatformWindow::inset_bottom()  { return 0; }  // Mac needs this


// Accessors:

bool XPlatformWindow::is_open() { return _xwindow != None; }
bool XPlatformWindow::is_mono() { return _is_mono; }

int  XPlatformWindow::left()   { return _window_x; }
int  XPlatformWindow::top()    { return _window_y; }
int  XPlatformWindow::width()  { return _width; }
int  XPlatformWindow::height() { return _height; }

int  XPlatformWindow::screen_width()   { return _display_width; }
int  XPlatformWindow::screen_height()  { return _display_height; }
int  XPlatformWindow::menubar_height() { return 0; } // none in X
  
int  XPlatformWindow::font_width() { return _font_info->max_bounds.width; }
int  XPlatformWindow::font_height() { return _font_info->max_bounds.ascent + _font_info->max_bounds.descent; }

const char* XPlatformWindow::default_fixed_font_name() { return "fixed"; }
int   XPlatformWindow::default_fixed_font_size() { return 10; }


// Handy operations;

bool XPlatformWindow::change_extent(int left, int top, int w, int h) { 
  // left, top in global coordinates
  // don't need to adjust by insets for X
  XMoveResizeWindow(_display, _xwindow, left, top, w, h);
  _window_x = left;
  _window_y = top;
  _width = w;
  _height = h;
  return true;
}
void XPlatformWindow::adjust_after_resize() { 
  if (TheSpy != NULL)
    TheSpy->adjust_after_resize(); // in case this is the spy
}
void XPlatformWindow::full_redraw() {
  if (TheSpy != NULL)
    TheSpy->full_redraw(); // in case this is the spy
}


// Drawing:

bool XPlatformWindow:: pre_draw( bool incremental) {
  if ( get_graphics_semaphore())  return false;   //  (X isn't reentrant)
  if (!handle_polled_events())    return false;   // returns if user selects QUIT
  return true;
}


void XPlatformWindow::post_draw( bool ) { /* XFlush(_display); should be needed but was not there before */ }

void XPlatformWindow::draw_text(const char* text, int x, int y) {
  XDrawImageString(_display, _xwindow, _gc, x, y, text, strlen(text));
}

void XPlatformWindow::draw_line(int x1, int y1, int x2, int y2) {
  XDrawLine(_display, _xwindow, _gc, x1, y1, x2, y2);
}

void XPlatformWindow::draw_rectangle_black(int x, int y, int w, int h) {
  if (w > 0 && h > 0)
    XDrawRectangle(_display, _xwindow, _gc, x, y, w, h);
}
      

void XPlatformWindow::clear_rectangle(int x, int y, int w, int h) {
  if (w > 0 && h > 0)
    XClearArea(_display, _xwindow, x, y, w, h, false);
}


// X drawing functions
// the X calls do the wrong thing if w or h is 0, so suppress these calls 

void XPlatformWindow::fill_rectangle(int x, int y, int w, int h) {
  if (w > 0 && h > 0)
    XFillRectangle(_display, _xwindow, _gc, x, y, w, h);
}

void XPlatformWindow::set_color(int c)     { XSetForeground    (_display, _gc, c); }
void XPlatformWindow::set_thickness(int t) { XSetLineAttributes(_display, _gc, t, LineSolid, CapButt, JoinMiter); }
void XPlatformWindow::set_xor()            { XSetFunction      (_display, _gc, GXxor);  }
void XPlatformWindow::set_copy()           { XSetFunction      (_display, _gc, GXcopy); }


bool XPlatformWindow::get_graphics_semaphore() { 
  // if you draw while X may be drawing something else, check this
  // and don't draw if it is true
  extern bool xlib_semaphore; 
  return xlib_semaphore;
}


// Events:


bool XPlatformWindow::handle_polled_events() {
  XEvent event;
  // OS X, spy breaks this:  for(int n = XPending(_display); n > 0; --n) {
  // So use while loop:
  while ( XPending(_display) > 0 ) {
    XNextEvent(_display, &event);
    switch (event.type) {
    
     case Expose:
      if (event.xexpose.count != 0) break;
      full_redraw(); // force redraw
      break;
      
     case ConfigureNotify:
      _width  = event.xconfigure.width;
      _height = event.xconfigure.height;
      adjust_after_resize();
      break;
      
     case ReparentNotify: 
      handle_reparent_event(event);
      break;
      
     case ClientMessage:
      if ((event.xclient.message_type = _wmProtocolsAtom)
          && (event.xclient.data.l[0] = _wmDeleteWindowAtom)) {
        // user has selected Quit from window's menu
        close();
        return false;
      }
      break;
      
     default:
      break;
    }
  }
  return true;
}


void XPlatformWindow::handle_reparent_event(XEvent& event) {
  Window root;
  int x, y;
  unsigned w, h, border_width, depth;
  if (XGetGeometry(_display, event.xreparent.parent, &root, &x, &y,
                   &w, &h, &border_width, &depth) == 0) 
    return;
  int wdelta = w - width();
  // sanity check: sometimes X gives weird width
  if (wdelta < 0) {
    if (XGetGeometry(_display, event.xreparent.parent, &root, &x, &y,
                     &w, &h, &border_width, &depth) == 0) 
      return;
    wdelta = w - width();
    if (wdelta < 0) {
      return;    // just ignore if X continues to report weird width
    }
  }
  _width = width() - wdelta;
  int display_height = DisplayHeight(_display,
                                     DefaultScreen(_display));

  XMoveResizeWindow(_display, _xwindow, event.xreparent.x,
                    _display_height - h + event.xreparent.y,
                    width(), height());
  tell_platform_size_hints();
}

# endif // XLIB
/* Sun-$Revision: 30.3 $ */
// $Revision: 30.3 $

/* Copyright 2007 David Ungar
   See the LICENSE file for license information. */
   
// Mac Quartz implementation:

# pragma implementation

# if defined(QUARTZ_LIB) 

# if TARGET_OS_VERSION == MACOSX_VERSION  &&  !TARGET_API_MAC_CARBON
  #  undef ASSEMBLER
  #  undef Alloc
  #  undef Status

  #  include <Carbon/Carbon.h>
  #  include <ApplicationServices/ApplicationServices.h>
  
  // implicit by the above. 
  // #  include <CoreGraphics/CoreGraphics.h>
  
  // remove Carbon macros to avoid name collisions
  #  undef assert
  #  undef assert_type
  #  undef assert_smi
  #  undef assert_byteVector
  #  undef assert_objVector
  
  #  include "asserts.hh"
  #  undef verify
  #  undef check
  #  undef XLIB

# endif


# include "_quartzWindow.cpp.incl"




int       WindowSet::_num_windows = 0;
WindowSet_WindowPtr WindowSet::_my_windows[WindowSet::_max_windows];

void WindowSet::add_window(WindowSet_WindowPtr w) {
  if (_num_windows >= _max_windows)  fatal("too many");
  _my_windows[_num_windows++] = w;
}

void WindowSet::rm_window(WindowSet_WindowPtr w) {
  int i;
  for (i = 0;  _my_windows[i] != w;  ++i)
    if (i >= _num_windows)
      fatal("did not find  _window");
  if (i == _num_windows - 1)   --_num_windows;
  else                         _my_windows[i] = _my_windows[--_num_windows];
}

bool WindowSet::includes_window(WindowSet_WindowPtr w) {
  for (int i = 0;  i < _num_windows;  ++i)
    if (_my_windows[i] == w) {
      if (i != 0) {
        WindowSet_WindowPtr w0 = _my_windows[0];
        _my_windows[0] = w;
        _my_windows[i] = w0;
      }
      return true;
    }
  return false;
}





// see MacWindows.h


// ===========================
// First: cross-platform functions:

// Creation, destruction:

QuartzWindow::QuartzWindow() : AbstractPlatformWindow(), _evtQ() { 
  _is_open = false;
  _my_event_handler_upp = NULL;
  _my_event_handler = NULL;
  _my_spy_event_handler_upp = NULL;
  _my_spy_event_handler = NULL;
  _bounds_changed = false;
  _was_closed = false;
  _quartz_win = NULL;
  myContext = NULL;
}


// (destructor is plat-independant)

bool QuartzWindow::open( const char* /* display_name  unimp mac */,
                           int x, int y, int w, int h,
                           int min_w, int max_w, int min_h, int max_h, // -1 for don't care
                           const char* window_name,  const char* /*icon_name*/,
                           const char* font_name,    int   font_size ) {

  int options[8] = {
    kHIWindowBitCloseBox        ,
    kHIWindowBitZoomBox         ,
    kHIWindowBitCollapseBox     ,
    kHIWindowBitResizable       ,
    // kHIWindowBitToolbarButton     ,
    // kHIWindowBitUnifiedTitleAndToolbar,
    // kHIWindowBitTextured         ,
    kHIWindowBitRoundBottomBarCorners,
    // ?kHIWindowBitCompositing,
    kHIWindowBitStandardHandler,
    0
  };

  if ( !open( kDocumentWindowClass, options,
             x, y, x + w, y + h, window_name, font_name, font_size))
    return false;
  if ( !change_size_hints(min_w, max_w, min_h, max_h)) { close();  return false; }
  init_font_info();
  activate();
  return true;
}


bool QuartzWindow::open( 
                    uint32  /* WindowClass */ wc,
                    int*    /* WindowAttributes  */  attrs,
                    int   left,
                    int   top, 
                    int   right, 
                    int   bottom, 
                    const char* title,
                    const char* font_name,
                    int   font_size ) {
  HIRect bounds = (HIRect) CGRectMake(left, top, right, bottom);
  
  OSStatus err =  HIWindowCreate(wc, attrs, NULL, kHICoordSpace72DPIGlobal, &bounds, &_quartz_win);
  if (err != noErr)
    return false;
  SetWRefCon(my_window(), (int32)this);

    
  CFStringRef cftitle = CFStringCreateWithCString(NULL, title, kCFStringEncodingMacRoman);
  SetWindowTitleWithCFString(my_window(), cftitle);
  CFRelease(cftitle);

  WindowSet::add_window(my_window());
  _is_open = true;
  
  init_colors();
  init_events();
  
  
  return true;                   
} 

void QuartzWindow::init_colors() {
  float redc[]    = {1.0, 0.0, 0.0, 1.0};
  float yellowc[] = {1.0, 1.0, 0.0, 1.0};
  float blackc[]  = {0.0, 0.0, 0.0, 1.0};
  float grayc[]   = {0.5, 0.5, 0.5, 1.0};
  float whitec[]  = {1.0, 1.0, 1.0, 1.0};
  _color_space = CGColorSpaceCreateDeviceRGB();
  _red    = (int) CGColorCreate( _color_space, redc);
  _yellow = (int) CGColorCreate( _color_space, yellowc);
  _black  = (int) CGColorCreate( _color_space, blackc);
  _gray   = (int) CGColorCreate( _color_space, grayc);
  _white  = (int) CGColorCreate( _color_space, whitec);
}




void QuartzWindow::init_font_info() {
  CFStringRef desired = CFStringCreateWithCString(
    kCFAllocatorDefault, default_fixed_font_name(), kCFStringEncodingMacRoman);
    
  ATSFontIterator fi;
  OSStatus e = ATSFontIteratorCreate( kATSFontContextLocal, NULL, NULL,
                                      kATSOptionFlagsDefaultScope, &fi);
  if (e) fatal1("could not create ATSFontIterator %d", e);
  
  ATSFontRef font;
  for (;;) {
    ATSFontIteratorNext( fi, &font);
    if (!font)
      fatal("could not find font");
    CFStringRef name;
    ATSFontGetName( font, kATSOptionFlagsDefault, &name);
    if ( kCFCompareEqualTo == CFStringCompare( desired, name, kCFCompareCaseInsensitive | kCFCompareAnchored))
      break;
  }  
  _default_font = font;
  e = ATSFontGetHorizontalMetrics(font, kATSOptionFlagsDefault, &_metrics);
  if (e) fatal1("could not get metrics %d", e);
  e = ATSFontIteratorRelease(&fi);
  if (e) fatal1("could not release ATSFontIterator %d", e);
  CFRelease(desired);
}


void QuartzWindow::activate() {
  WindowPtr wp = my_window();
  if (!IsWindowVisible(wp))
    ShowWindow(wp);
  SelectWindow(wp);
  OSStatus e = SetUserFocusWindow(wp); 
  if (e) lprintf("SetUserFocus %d\n", e);
  
  e = ActivateWindow(wp, true);
  if (e) lprintf("ActivateWindow %d\n", e);
}



void QuartzWindow::close() {
  if (!is_open())
    return;
  CGrafPtr gp = GetWindowPort(my_window());
  if (gp != NULL) // already closed by std handler
    QDEndCGContext( gp, &myContext );
  CGColorRelease((CGColorRef) _red);
  CGColorRelease((CGColorRef) _yellow);
  CGColorRelease((CGColorRef) _black);
  CGColorRelease((CGColorRef) _gray);
  CGColorRelease((CGColorRef) _white);
  CGColorSpaceRelease(_color_space);
  WindowSet::rm_window(my_window());
  if (gp != NULL)
    DisposeWindow(my_window());
  _is_open = false; 
  DisposeEventHandlerUPP(_my_event_handler_upp);
  DisposeEventHandlerUPP(_my_spy_event_handler_upp);
  _my_event_handler = NULL;
  _my_spy_event_handler = NULL;
  _quartz_win = NULL;
}


static CGDirectDisplayID screen(void* w) {
  HIRect bounds;
  OSStatus err = HIWindowGetBounds((WindowRef)w, kWindowGlobalPortRgn, 
                                   kHICoordSpace72DPIGlobal, &bounds);
  if (err) {
    lprintf("HIWindowGetBounds failed: %d\n", err);
    return CGMainDisplayID();
  }
  CGDirectDisplayID display;
  CGDisplayCount displayCount;
  CGDisplayErr e = CGGetDisplaysWithRect(bounds, 1, 
                                         &display, &displayCount);
  return (displayCount && !e) ? display : CGMainDisplayID();
}


int QuartzWindow::screen_width() {
  return CGDisplayPixelsWide(screen(my_window()));
}

int QuartzWindow::screen_height() {
  return CGDisplayPixelsHigh(screen(my_window()));
}
    
int QuartzWindow::menubar_height() {
  return screen(my_window()) == CGMainDisplayID() ? GetMBarHeight() : 0;
}



// Insets:

// Compute inset from outer to inner part of windows 
// by comparing portRect against strucRgn bounding box

// Strangely, on Mac, the inner part of the window includes the scroll bar.
// So, add that to offset for bottom/right. 

// Ah, but on Carbon, we can get content area, so ixnay the ollbarscray.
int QuartzWindow::inset_left() {
  Rect cr;  get_window_region_rect( kWindowContentRgn,    &cr);
  Rect sr;  get_window_region_rect( kWindowStructureRgn,  &sr);
  int r = cr.left - sr.left;
  // warning1("inset_left = %d", r);
  return r;
}

int QuartzWindow::inset_top() {
  Rect cr;  get_window_region_rect( kWindowContentRgn,    &cr);
  Rect sr;  get_window_region_rect( kWindowStructureRgn,  &sr);
  int r = cr.top - sr.top;
  // warning1("inset_top = %d", r);
  return r;
}

int QuartzWindow::inset_right() {
  Rect cr;  get_window_region_rect( kWindowContentRgn,    &cr);
  Rect sr;  get_window_region_rect( kWindowStructureRgn,  &sr);
  int r = sr.right - cr.right;
  // warning1("inset_right = %d", r);
  return r;
}

int QuartzWindow::inset_bottom() {
  Rect cr;  get_window_region_rect( kWindowContentRgn,    &cr);
  Rect sr;  get_window_region_rect( kWindowStructureRgn,  &sr);
  int r = sr.bottom - cr.bottom;
  // warning1("inset_bottom = %d", r);
  return r;
}



void QuartzWindow::get_window_region_rect(int wh, Rect* r) {
  HIRect bounds;
  OSStatus err = HIWindowGetBounds(my_window(), wh, 
                                   kHICoordSpace72DPIGlobal, &bounds);
  if (err) {
    lprintf("HIWindowGetBounds failed: %d\n", err);
    r->left = r->top = 0; r->bottom = r->right = 1;
  } else {
    r->left   = (short) CGRectGetMinX(bounds);
    r->top    = (short) CGRectGetMinY(bounds);
    r->bottom = (short) CGRectGetMaxY(bounds);
    r->right  = (short) CGRectGetMaxX(bounds);
  }
}


int  QuartzWindow::left()   { Rect r;  get_window_region_rect( kWindowContentRgn, &r);  return  r.left; }
int  QuartzWindow::top()    { Rect r;  get_window_region_rect( kWindowContentRgn, &r);  return  r.top;  } 
int  QuartzWindow::width()  { Rect r;  get_window_region_rect( kWindowContentRgn, &r);  return  r.right  - r.left; }
int  QuartzWindow::height() { Rect r;  get_window_region_rect( kWindowContentRgn, &r);  return  r.bottom - r.top; }





int QuartzWindow::font_width()  { 
  return ceil(_metrics.maxAdvanceWidth * default_fixed_font_size()); 
}
int QuartzWindow::font_height() { 
  return ceil(_metrics.leading) * default_fixed_font_size(); 
}

const char* QuartzWindow::default_fixed_font_name() { return "Monaco"; }
int   QuartzWindow::default_fixed_font_size() { return 9; }


// Handy operations:

// Long comments in this routine are extra stuff from the Mac toolbox assistant.

bool QuartzWindow::change_extent(int left, int top, int w, int h) { 
  // Remember, left, top, w and h are for outer parts of window.  
  // convert to inner
  CGrafPtr gp = GetWindowPort(my_window());
  if (gp != NULL) // already closed by std handler
    QDEndCGContext( gp, &myContext );
  myContext = NULL;  
  MoveWindow( my_window(), left + inset_left(), top + inset_top(), false); 
  SizeWindow( my_window(), w - inset_left() - inset_right(), h - inset_top() - inset_bottom(), true);              

  adjust_after_resize();        
  return true;
 }
 

bool QuartzWindow::tell_platform_size_hints() { 
  HISize minSize, maxSize; // sizes of content region

  minSize.width  =  _min_w == -1 ?        0  :  (_min_w - inset_left() - inset_right());
  minSize.height =  _min_h == -1 ?        0  :  (_min_h - inset_top()  - inset_bottom());
  maxSize.width  =  _max_w == -1 ?  1000000  :  (_max_w - inset_left() - inset_right());
  maxSize.height =  _max_h == -1 ?  1000000  :  (_max_h - inset_top()  - inset_bottom());
  OSStatus e = SetWindowResizeLimits(  my_window(),  &minSize, &maxSize);
  return e == noErr; 
}


void QuartzWindow::setupCTM() {
  CGContextTranslateCTM(myContext, 0, height());
  CGContextScaleCTM(myContext, 1, -1);
}

// On Mac, must redraw borders yourself:
void QuartzWindow::adjust_after_resize() { 
  if (myContext) {
    CGAffineTransform x = CGContextGetCTM(myContext);
    CGContextScaleCTM(myContext, 1.0 / x.a, 1.0 / x.d);
    CGContextTranslateCTM(myContext, -x.tx, -x.ty);
    setupCTM();
  }  
  if (TheSpy != NULL)
    TheSpy->adjust_after_resize(); // might be the spy
}

 
// Drawing: 


bool QuartzWindow::pre_draw(bool incremental) {
  if ( get_graphics_semaphore())  return false;
  if (!_is_open) return false;
  if (_was_closed) {
    TheSpy->deactivate();
    _was_closed = false;
    return false;
  }
  if ( myContext == NULL ) {
    // Self does this for Self windows, so only do it for Spy windows--that's why it's here and not in open
    SetPortWindowPort(my_window());
    QDBeginCGContext( GetWindowPort(my_window()), &myContext);
    setupCTM();
    CGContextSetTextMatrix(myContext, CGAffineTransformMake( 1, 0, 0, -1, 0, 0));
    CGContextSelectFont(myContext, 
      default_fixed_font_name(), default_fixed_font_size(), kCGEncodingMacRoman);
    CGContextSetShouldAntialias(myContext, false);
    
    EventTypeSpec es[] = { 
      {kEventClassWindow, kEventWindowBoundsChanged},
      {kEventClassWindow, kEventWindowClose}
    };
    OSStatus e = AddEventTypesToHandler(_my_spy_event_handler,  sizeof(es) / sizeof(es[0]),  es);
    if (e != noErr) fatal1("could not add types to handler %d\n", e);
  }
  if (_bounds_changed) {
    _bounds_changed = false;
    adjust_after_resize();
  }
  if (!incremental) {
    Rect r;  get_window_region_rect( kWindowContentRgn, &r);
    clear_rectangle(0, 0, r.right - r.left, r.bottom - r.top);
  }
  return true;
}


void QuartzWindow::post_draw(bool incremental) {
  CGContextFlush(myContext);
}

  
 
void QuartzWindow::full_redraw() {
  if (TheSpy != NULL)
    TheSpy->full_redraw(); // might be the spy
}





void QuartzWindow::draw_text(const char* text, int x, int y)  { 
  int len = strlen(text);
  int h = font_height();
  
  clear_rectangle(x, y-h, len * font_width(), h);
  CGContextSetTextPosition(myContext, x, y);
  CGContextShowText(myContext, text, len);
} 

void QuartzWindow::draw_line(int x1, int y1, int x2, int y2) {
  CGContextBeginPath(myContext);
  CGContextMoveToPoint(myContext, x1, y1);
  CGContextAddLineToPoint(myContext, x2, y2);
  CGContextStrokePath(myContext);
}


void QuartzWindow::draw_rectangle_black(int x, int y, int w, int h) {
  set_color(black());
  CGContextStrokeRectWithWidth(myContext, CGRectMake(x, y, w, h), 1.0);
}


void QuartzWindow::clear_rectangle(int x, int y, int w, int h) {
  set_color(white());
  CGContextFillRect(myContext, CGRectMake(x-1, y-1, w+1, h+1));
  set_color(black());
}


void QuartzWindow::fill_rectangle(int x, int y, int w, int h) {
  CGContextFillRect(myContext, CGRectMake(x, y-1, w, h+1));
}

void QuartzWindow::set_color(int c) { 
  CGContextSetFillColorWithColor(   myContext, (CGColorRef)c );
  CGContextSetStrokeColorWithColor( myContext, (CGColorRef)c );
}


void QuartzWindow::set_thickness(int t) {
  CGContextSetLineWidth(myContext, max(1, t));
}

void QuartzWindow::set_xor()   { CGContextSetBlendMode(myContext, kCGBlendModeDifference);  }
void QuartzWindow::set_copy()  { CGContextSetBlendMode(myContext, kCGBlendModeNormal);  }




bool QuartzWindow::get_graphics_semaphore() {
  // Carbon under OSX runs the spy asynchronously, just like on Solaris.
  // But, when running the spy and UI2, Self locks up in a loop sometimes.
  // Maybe the graphics toolbox is not reentrant, let's try the same scheme
  // used for X to avoid drawing from the spy in the midst of a signal if
  // the self is in the midst of a Quartz operation. -- dmu 9/01
  extern bool quartz_semaphore;
  return quartz_semaphore;
}



void QuartzWindow::warp_pointer(int x, int y) {
  // lprintf("warping to: %d, %d\n", x, y);
# if TARGET_OS_VERSION == MACOSX_VERSION
    CGPoint pt;
    pt.x = x;
    pt.y = y;
    const int n = 16;
    CGDisplayCount count = 0;
    CGDirectDisplayID dspys[n];
    CGDisplayErr err = CGGetDisplaysWithPoint( pt, n, dspys, &count);
    if (err != noErr) {
      lprintf("CGGetDisplaysWithPoint failed: %d\n", err);
      return;
    }
    // lprintf("CGGetDisplaysWithPoint count = %d\n", count);
    for (int i = 0;  i < count;  ++i) {
      // MUST adj pt to be relative to TL of display -- dmu 5/03
      CGRect bounds = CGDisplayBounds(dspys[i]);
      CGPoint adjusted_pt;
      adjusted_pt.x = pt.x - bounds.origin.x;
      adjusted_pt.y = pt.y - bounds.origin.y;
      err = CGDisplayMoveCursorToPoint( dspys[i], adjusted_pt);
      // lprintf("CGDisplayMoveCursorToPoint %d returned %d\n", i, err);
    }
# else
  Unused(x); Unused(y);
# endif
}


#define kUTTypeOldMacText CFSTR("com.apple.traditional-mac-plain-text")


oop QuartzWindow::get_scrap_text() {
  // See Pasteboard Manager Programming guide
  PasteboardRef       clipboard;
  PasteboardSyncFlags syncFlags;
  CFDataRef           textData = NULL;
  ItemCount           itemCount;
  
  if ( PasteboardCreate(kPasteboardClipboard, &clipboard) != noErr
  ||  (PasteboardSynchronize(clipboard) & kPasteboardModified)
  ||   PasteboardGetItemCount(clipboard, &itemCount) != noErr  ) 
    return new_string("", 0);
  
  for( UInt32 itemIndex = 1; itemIndex <= itemCount; itemIndex++ ) {
    PasteboardItemID itemID = 0;
    CFArrayRef       flavorTypeArray = NULL;
    CFIndex          flavorCount = 0;

    if (PasteboardGetItemIdentifier(clipboard, itemIndex, &itemID) != noErr)
      continue;
  
    if (PasteboardCopyItemFlavors(clipboard, itemID, &flavorTypeArray) != noErr)
      continue;

    flavorCount = CFArrayGetCount(flavorTypeArray);
     
    for(CFIndex flavorIndex = 0; flavorIndex < flavorCount; flavorIndex++) {
      CFStringRef flavorType;
      CFDataRef   flavorData;
      CFIndex     flavorDataSize;
      char        flavorText[256];
      
      
      flavorType = (CFStringRef)CFArrayGetValueAtIndex( flavorTypeArray,// 6
                                                       flavorIndex );
      
      if (UTTypeConformsTo(flavorType, kUTTypeOldMacText)) {
        
        if (PasteboardCopyItemFlavorData(clipboard, itemID, flavorType, 
                                         &flavorData) != noErr)
          continue;
          
        flavorDataSize = CFDataGetLength(flavorData);

        // allocate new string.
        byteVectorOop r = Memory->byteVectorObj->cloneSize(flavorDataSize, CANFAIL);
        if (r->is_mark()) {
          CFRelease (flavorData);
          CFRelease (flavorTypeArray);
          return new_string("", 0);
        }
        // copy over
        CFDataGetBytes(flavorData, CFRangeMake(0,CFDataGetLength(flavorData)),
                       (UInt8 *)r->bytes());          
        CFRelease(flavorData);
        CFRelease(flavorTypeArray);
        return r;
      } // else try next      
    }
    CFRelease(flavorTypeArray);
  }  
  
}

int QuartzWindow::put_scrap_text(const char* s, int len) {
  // See Pasteboard Manager Programming guide
  PasteboardRef clipboard;
  OSStatus      err;
  CFDataRef     textData = CFDataCreate(kCFAllocatorDefault, 
                                        (const UInt8*)s, len);
  if (textData == NULL) return -1;
  if ((err = PasteboardCreate(kPasteboardClipboard, &clipboard)) != noErr) return err;
  if ((err = PasteboardClear(clipboard)) != noErr) return err;

  return PasteboardPutItemFlavor(clipboard, (PasteboardItemID)s, 
                                 kUTTypeOldMacText, textData, 
                                 kPasteboardFlavorNoFlags);
  
}

// Convert WindowPtr to QuartzWindow by using refcon
QuartzWindow* QuartzWindow::getPlatformWindow(WindowRef ww, void* FH) {
    if (!WindowSet::includes_window(ww))  { failure(FH, "not a QuartzPLatformWindow"); return NULL; }
    QuartzWindow* w = (QuartzWindow*) GetWRefCon(ww);
    assert(w->my_window() == ww, "");
    return w;
}



// Events


int  QuartzWindow::events_pending(void* FH) { 
  if (!is_open())  { failure(FH, "window is closed"); return 0; }
  return _evtQ.count(); 
}

EventRef  QuartzWindow::peek_event(void* FH) {
  if (!is_open())  { failure(FH, "window is closed"); return 0; }
  EventRef e = _evtQ.peek();
  if (e == NULL) {
    failure(FH, "no more events");
    return NULL;
  }
  return e;
}
  

EventRef  QuartzWindow::next_event(void* FH) {
  if (!is_open())  { failure(FH, "window is closed"); return 0; }
  EventRef e = _evtQ.get();
  if (e == NULL) {
    failure(FH, "no more events");
    return NULL;
  }
  return e;
}

void QuartzWindow::put_event(EventRef e) {
  _evtQ.put(e);
}


static OSStatus handle_event(EventHandlerCallRef handler_call_chain, EventRef e, void* data) {
  QuartzWindow *w = (QuartzWindow*)data;
  return w->handle_event(handler_call_chain, e);
}


static OSStatus handle_spy_event(EventHandlerCallRef handler_call_chain, EventRef e, void* data) {
  QuartzWindow *w = (QuartzWindow*)data;
    if ( GetEventClass(e) == kEventClassWindow ) 
      switch (GetEventKind(e)) {
        case kEventWindowBoundsChanged:  w->set_bounds_changed();  break;
        case kEventWindowClose:          w->set_was_closed();      break;
      }
    return noErr;
}      


void QuartzWindow::init_events() {
  _my_event_handler_upp = NewEventHandlerUPP(::handle_event);
  OSStatus e = InstallWindowEventHandler(my_window(), _my_event_handler_upp, 0, NULL, this, &_my_event_handler);
  if (e != noErr) fatal1("could not install event handler: %d\n", e);
  
  _my_spy_event_handler_upp = NewEventHandlerUPP(::handle_spy_event);
  e = InstallWindowEventHandler(my_window(), _my_spy_event_handler_upp, 0, NULL, this, &_my_spy_event_handler);
  if (e != noErr) fatal1("could not install spy event handler: %d\n", e);
}

OSStatus QuartzWindow::AddHandledEvent_wrap( uint32* eclass, uint ec_len, uint ekind, void* FH) {
  if (ec_len != 1)  { failure( FH, "class needs to have four bytes"); return NULL; }
  EventTypeSpec es;
  es.eventClass = EndianU32_BtoN(*eclass);
  es.eventKind = ekind;
  return AddEventTypesToHandler(_my_event_handler, 1, &es);
}

OSStatus QuartzWindow::RemoveHandledEvent_wrap( uint32* eclass, uint ec_len, uint ekind, void* FH) {
  if (ec_len != 1)  { failure( FH, "class needs to have four bytes"); return NULL; }
  EventTypeSpec es;
  es.eventClass = EndianU32_BtoN(*eclass);
  es.eventKind = ekind;
  return RemoveEventTypesFromHandler(_my_event_handler, 1, &es);
}
 
 
static uint16 get_short_event_parm(EventRef evt, EventParamName n, EventParamType t) {
  uint16 r = NULL;
  OSStatus e = GetEventParameter(evt, n, t, NULL, sizeof(r), NULL, &r);
  uint32 nb = EndianU32_NtoB(n);
  if (e) lprintf("scalar event parm failed: %4.4s, %d\n", (char*)&n, e);
  return r;
}
 
 
static void* get_scalar_event_parm(EventRef evt, EventParamName n, EventParamType t) {
  void* r = NULL;
  OSStatus e = GetEventParameter(evt, n, t, NULL, sizeof(r), NULL, &r);
  if (e == noErr || e == -9870) return r;
  uint32 nb = EndianU32_NtoB(n);
  lprintf("scalar event parm failed: %4.4s, %d\n", (char*)&n, e);
  return r;
}

void print_event(EventRef evt) {
  uint32 cl = EndianU32_NtoB(GetEventClass(evt));
  uint32 ki = /*EndianU32_NtoB*/(GetEventKind(evt));
  lprintf("Event class: %4.4s kind: %d  ",
    (char*)&cl, ki);
    
  lprintf("\tbtn: 0x%x  ", get_scalar_event_parm(evt, kEventParamMouseButton, typeMouseButton));
 // lprintf("\ttarg: 0x%x\n", get_scalar_event_parm(evt, kEventParamDirectObject, typeWildCard));
  lprintf("\ttarg: 0x%x  ", get_scalar_event_parm(evt, kEventParamPostTarget, typeEventTargetRef));
  lprintf("\twind: 0x%x  ", get_scalar_event_parm(evt, kEventParamWindowRef, typeWindowRef));
  lprintf("\tgrafport: 0x%x\n", get_scalar_event_parm(evt, kEventParamGrafPort, typeGrafPtr));
}


OSStatus QuartzWindow::handle_event(EventHandlerCallRef handler_call_chain, EventRef e) {
  put_event(e);
 
  // Hack: if this is a user-level window (not SPY) and if it is window-close,
  // must leave it for Self, otherwise let standard handler do it
  if ( myContext == NULL  &&  GetEventClass
  &&   GetEventClass(e) == kEventClassWindow 
  &&   GetEventKind(e)  == kEventWindowClose )
    return noErr;

  return CallNextEventHandler(handler_call_chain, e); 
  // In future, could return either eventNotHandledErr or noErr
}


void QuartzWindow::check_carbon_events() {
  BlockGlueFlag f(quartz_semaphore);

  for (;;) {
    EventRef evt;
    ReceiveNextEvent(0, NULL, kEventDurationNoWait, true, &evt);
    if (!evt) return;
    SendEventToEventTarget(evt, GetEventDispatcherTarget());
  }
}





# endif // QUARTZ_LIB
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "itimer.hh"
# pragma implementation "itimer_abstract.hh"
# pragma implementation "itimer_inline.hh"
# include "_itimer.cpp.incl"

// entries

IntervalTimer* IntervalTimer::_Real_timer = NULL;
IntervalTimer* IntervalTimer::_CPU_timer  = NULL;

bool IntervalTimer::dont_use_real_timer = false;
bool IntervalTimer::dont_use_any_timer  = false;
bool IntervalTimer::use_real_instead_of_cpu_timer = false;

// Changed by -o argument; see processArguments() in shell.cpp
int32 IntervalTimer::oversample_rate = 1;

// initialization & finalization

void IntervalTimer::common_initialization() { registered = 0;  state = disabled; }
void IntervalTimer::common_finalization()   { disable(true);      };

void IntervalTimer::exit() {
  disable_all(true); // needed to not crash mac
}


// primitive implementations:

smi IntervalTimer::setRealTimer_prim(smi ms) { return Real_timer()->setTimer_prim(ms, SignalInterface::Self_real_timer_tick); }
smi IntervalTimer:: setCPUTimer_prim(smi ms) { return  CPU_timer()->setTimer_prim(ms, SignalInterface::Self_CPU_timer_tick); }

smi IntervalTimer::setTimer_prim(smi ms, doFn tick_fn)  {
  assert(ms >= 0, "negative ms");
  withdraw(tick_fn);
  if (ms) 
    enroll_async_if_safe( 1000.0 / float(ms), tick_fn );
  return ms;
}


// ensemble operations:

void IntervalTimer::start_all() {
  Real_timer()->start();
  if (use_real_instead_of_cpu_timer)
    return;
  CPU_timer()->start();
}

void IntervalTimer::disable_all(bool skipAsserts) {
  Real_timer()->disable(skipAsserts);
  if (use_real_instead_of_cpu_timer)
    return;
  CPU_timer()->disable(skipAsserts);
}

void IntervalTimer::enable_all() {
  Real_timer()->enable();
  if (use_real_instead_of_cpu_timer)
    return;
  CPU_timer()->enable();
}

void IntervalTimer::do_all_sync_tasks() {
  Real_timer()->do_sync_tasks();
  if (use_real_instead_of_cpu_timer)
    return;
  CPU_timer()->do_sync_tasks();
}


// Enrolling, etc.

void IntervalTimer::withdraw(doFn sync_or_async_fn) {
  fint i = 0;
  for (  ;  
           i < registered  &&  !((AbstractTimerEntry*)entry_at(i))->includes_fn( sync_or_async_fn );
         ++i) {
  }
  if ( i == registered ) {
    // function wasn't registered
    // missing entry, but setCPUTimer_prim does this, so OK
    return;
  }
  
  TimerEntry* e  = entry_at(i);
  withdraw_entry(e);
  if (i + 1  ==  registered) {
    --registered;
    assert(0 <= registered  &&  registered < number_of_entries, "registered out of range");
  }
  else { // reclaim non-last entry by using it for last one
    --registered;
    TimerEntry* e1 = entry_at(registered);
    move_entry(e1, e);
  }
}


// Utilities:

TimerEntry* IntervalTimer::alloc_entry() {
  if (registered >= number_of_entries)
    fatal2("out of interval timers, increase IntervalTimer::number_of_entries and recompile Self\n"
           "(registered = %d, number_of_entries = %d)",
           registered, 
           number_of_entries);
  return entry_at(registered++);
}

bool IntervalTimer::verify_entry_address(TimerEntry* te) {
  if (entries() <= te  &&  te < entry_at(number_of_entries)) ;
  else return false; // Timer entry is out of range
  
  int   offset = (char*)entry_at(1) - (char*)entry_at(0);
  if ( ((char*)te - (char*)entry_at(0)) % offset  ==  0 ) ;
  else return false; // not aligned
  
  return true;
}


void IntervalTimer::do_sync_tasks() {
  if (state != enabled)
    return;
  // This calls sync_fn's which may recurse back in here!
  for ( fint i = 0;  i < registered;  ++i)
    ((AbstractTimerEntry*)entry_at(i))->do_sync_fn();
}


void AbstractTimerEntry::do_sync_fn() {
  if (!is_ready_to_do_sync_fn  ||  is_doing_sync_fn) 
    return;
  if (sync_fn == NULL)      fatal("interval timer error");
  
  is_doing_sync_fn = true;
  
  doFn p = sync_fn;
  if (p != (doFn)-1)
    (*p)();
    
  is_ready_to_do_sync_fn = false;
  is_doing_sync_fn = false;
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "errorCodes.hh"
# include "_errorCodes.cpp.incl"


void ErrorCodes::init() {
# define CHECK(offset, vmStringIndex) \
  if (offset  !=  sizeof(int32) * vmStringIndex) \
    lprintf("In asmErrorCodes.h: %s is wrong.  Should be %ld\n", \
           STR(offset), \
           long(sizeof(int) * vmStringIndex)); \
  else {}
  
  CHECK(primitiveFailedOffset, PRIMITIVEFAILEDERROR);
  CHECK(        badTypeOffset,         BADTYPEERROR);
  CHECK( divisionByZeroOffset,  DIVISIONBYZEROERROR);
  CHECK(       overflowOffset,        OVERFLOWERROR);
  
# undef CHECK
}
  

markOop ErrorCodes::general_prim_error(const char* err_string) {
  return  new_string(err_string)->markify();
}


markOop ErrorCodes::os_prim_error(fint error) {
  // result must have mark tag to indicate primitive failure
  return  new_string(ErrorCodes::os_error_name(error))->markify();
}


# define      EXTRACT_MESSAGE_ARGUMENT(template,s1,s2,s3)  template(s3)
# define PASS_THROUGH_MESSAGE_ARGUMENT(s)  s ,

static  const char* errorMessages[] = {
  ErrorStrings(PASS_THROUGH_MESSAGE_ARGUMENT, EXTRACT_MESSAGE_ARGUMENT)
};
# undef      EXTRACT_MESSAGE_ARGUMENT
# undef PASS_THROUGH_MESSAGE_ARGUMENT
  
  
oop ErrorCodes::error_message_prim(byteVectorOop errString) {
  // Try to match a prefix of errString with either a "prim error" name,
  // a unix error name or a dynamic linker error name. If matched, return
  // corresponding explanation. 
  char* s = errString->bytes();

  // search in VMString table
  assert( 0 == strncmp(errorMessages[0], VMString[0]->bytes(), VMString[0]->length()),
          "VMString table must start with ErrorStrings");
          
  for (int32 i = 0; i < sizeof(errorMessages) / sizeof(errorMessages[0]); i++) {
    if (!strncmp(s, VMString[i]->bytes(), VMString[i]->length()))
      return new_string(errorMessages[i]);
  }
  
  char* r = ErrorCodes::os_error_message(s);
  if (r == (char*)-1 )  return errString;
  if (r == NULL      )  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
  else                  return new_string(r); 
}


markOop ErrorCodes::vmString_prim_error(VMStringsIndex i) { 
  return  VMString[i]->markify(); 
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "os_mac.hh"
# pragma implementation "os_unix.hh"
# pragma implementation "os.hh"
# pragma implementation "os_includes.hh"


# include "_os.cpp.incl"

extern "C" {
  int uname(struct utsname *name);
}

// expand ~/asfd/wert/sd or ~user/wer/wer/sdf
// out is OS::max_path_length long
// returns NULL if successful; otherwise returns an error string
char* OS::expand_unix_dir(const char* in,  char* out) {
  static char err[max_path_length + 50];
  char* dirName = (char*) "";
  
  if (*in != '~') {
    if (strlen(in) >= max_path_length) {
      sprintf(err, "'%s' exceeds %d chars in length", in, max_path_length);
      return err;
    }
    strcpy(out, in);
    return NULL;
  }
  
  // slash is location of slash after user name
  const char* slash = strchr(in, '/');  
  if (slash == NULL)  
    slash = in + strlen(in);
  
  if (in + 1  ==  slash) {
    // ~/ use HOME
    char* p = get_environment_variable("HOME");
    if (p)
      dirName = p;
  } 
  else if (!expand_user_name(in, slash, dirName)) {
    return dirName; // really an error, here
  }
  
  if (strlen(dirName) + strlen(slash)  >=  max_path_length) {
    sprintf(err, "'%s%s' exceeds %d characters in length",
            dirName, slash, max_path_length);
    return err;
  }
  sprintf(out, "%s%s", dirName, slash);
  return NULL;
}


bool OS::expand_user_name(const char* in, const char* slash, char*& dirName) {
  // expands user name after tilde at in, up to slash
  // put result in dirName
  // return false on error

  char user[logname_max+1];
  if (slash  -  (in + 1)  >= sizeof(user)) {
    static char err[max_path_length + 50];
    static char buf[max_path_length + 50];
    lsprintf_string(buf, slash - (in + 1), in + 1);
    sprintf(err, "'%s' exceeds %ld in length",
            buf,  long(sizeof(user) - 1));
    dirName = err;
    return false;
  }
  strncpy(user, in + 1,  slash - (in + 1));
  user[slash - (in + 1)] = '\0';
  dirName = get_user_directory(user);
  if (dirName == NULL) {
    static char err[max_path_length + 50];
    sprintf(err, "unable to find a home directory for user: %s\n", user);
    dirName = err;
    return false;
  }
  return true;
}


// is f a pipe?  cache a result to speed things up
bool OS::is_pipe(FILE *f) {
  static bool lastAns;
  static FILE *last= NULL;
  static int fd= -1;
  if (f == last  &&  fileno(f) == fd) return lastAns;
  last = f;
  fd= fileno(f);
  struct stat st_buf;
  if (fstat(fd, &st_buf)) fatal("fstat failed");
  lastAns= is_fifo(st_buf.st_mode);
  return lastAns;
}


void OS::setDateTimeBuf(struct tm *tod, smi buf[]) {
  buf[0] = tod->tm_year + 1900;
  buf[1] = tod->tm_mon + 1;
  buf[2] = tod->tm_mday;
  buf[3] = tod->tm_wday;
  buf[4] = tod->tm_hour;
  buf[5] = tod->tm_min;
  buf[6] = tod->tm_sec;
  buf[7] = tod->tm_yday;
  buf[8] = tod->tm_isdst;
}


// ============================================================


// Allocate on a page boundary, multiple of page size
// Set size to amount actually allocated, 0 if none
// If desiredAddress not 0, try to get that area; assume the caller 
// knows what he wants.

char* OS::allocate_idealized_page_aligned(int32 &size, const char *name,
                                          caddr_t desiredAddress, 
                                          bool mustAllocate) {
  size= roundTo(size, idealized_page_size);
  assert(idealized_page_size % get_page_size() == 0, "page size mismatch");
  char* b = allocate_heap_aligned(desiredAddress, size, idealized_page_size,
                                    name, mustAllocate);
  if (b == NULL) size= 0;
  return b;
}


void OS::allocate_failed(const char* what) {
  lprintf("\n**** could not allocate space for %s\n", what);
  lprintf("Out of virtual memory - please add swap space to your system.\n");

  fatal("out of memory");
}


// the address of the start of the page containing p
char* OS::page_start(void *p, unsigned int pg_sz) {
  return (char*)(int((char*)p) & ~(pg_sz - 1)); 
}
  
// the address of the start of the next page after p-1
char* OS::page_end(void *p, unsigned int pg_sz)   {
  return page_start(((char*)p) - 1, pg_sz) + pg_sz; 
}


// ===========================================================


void OS::FRead(void* buffer, int32 size, FILE* stream) {
  if (size != 0  &&  fread((char*)buffer, size, 1, stream) != 1) {
    perror("cannot read from file");
    fatal("read error");
  }
}


void OS::FRead_swap(void* buffer, int32 size, FILE* stream) {
  FRead(buffer, size, stream);
  int32* end = (int32*)((char*)buffer + size);
  for (int32* p = (int32*)buffer;  p < end;  p++)
    if (Memory->is_snapshot_other_endian) 
      swap_bytes(p); 
}


// use this only to write snapshots
void OS::FWrite(const void* buffer, int32 size, FILE* stream) {
  if (size && fwrite((char*)buffer, size, 1, stream) != 1)
    universe::snapshot_failed();
}


// used to skip unusable portions of the snapshot
void OS::read_or_seek(void* ptr, int32 size, FILE* f) {
  if (okToUseCodeFromSnapshot) {
    OS::FRead(ptr, size, f);
  } else if (!is_pipe(f)) {
    // can't just let fseek fail as it alters the state of f
    if (fseek(f, size, SEEK_CUR)) fatal("seek failed");
  } else {
    while (size--) getc(f);
  }
  if (ferror(f)) fatal("read error");
}


// =====================================================


bool OS::expand_dir(const char* in,  char* out) { // returns true on success
  static char mid[max_path_length];
  if (is_non_unix_path(in)) {
     if (strlen(in) >= max_path_length)
       fatal("path too long");
     strcpy(out, in);
     return true;
  }
  char* err = expand_unix_dir(in, mid);
  convert_unix_filename(mid, out);
  if ( err != NULL ) {
    warning(err);
    return false;
  }
  return true;
}


char* OS::ExpandDir_prim(const char* in, void* FH) {
  static char mid[OS::max_path_length];
  static char out[OS::max_path_length];
  char* err = OS::expand_unix_dir(in, mid);
  if (err) {
    failure(FH, err);
    return NULL;
  }
  OS::convert_unix_filename(mid, out);
  return out;
}


// ==============================================================


unsigned int OS::real_mem_size;


// ==============================================================

smi OS::user_time() {
  ProcessInfo::update();
  return ProcessInfo::user_time().milli_secs();
}
  
smi OS::system_time() {
  ProcessInfo::update();
  return ProcessInfo::system_time().milli_secs();
}
  
smi OS::cpu_time() {
  ProcessInfo::update();
  return ProcessInfo::user_time().milli_secs() 
    + ProcessInfo::system_time().milli_secs();
}


void OS::date_time(smi day, smi msec, smi buf[]) {
  time_t l = combine_time(day, msec);
  setDateTimeBuf(localtime(&l), buf);      // Set local time info
  setDateTimeBuf(   gmtime(&l), buf + 9);  // and GMT info. All at once.
}

    
// ================================================================


const char *OS::get_operating_system() {
  struct utsname name;
  char* str = NEW_RESOURCE_ARRAY(char, sizeof(utsname));
  uname(&name);
  sprintf(str,"%s %s.%s", name.sysname, name.release, name.version);
  return str;
}
 

// ================================================================


oop get_swap_space_prim(oop rcvrIgnored, void *FH) {
  Unused(rcvrIgnored);
  int totalK, freeK;
  if (OS::get_swap_space_info(totalK, freeK)) {
    objVectorOop arr= Memory->objVectorObj->cloneSize(2);
    arr->obj_at_put(0, as_smiOop(totalK));
    arr->obj_at_put(1, as_smiOop(freeK));
    return arr;
  } else {
    failure(FH, "Couldn't get swap space info");
    return 0;
  }
}

// ================================================================
 
char*   OS::optarg;
int     OS::opterr, OS::optind = 1, OS::optopt;

// simulate Unix stdlib routine


int OS::simulated_getopt(int argc,  char* const* argv,  const char* optstring) {
  if (optind >= argc) return EOF;
  int i = optind;
  ++optind;
  if (argv[i][0] != '-') return EOF;
  char c = argv[i][1];
  if (c == '-') { // -- delims opts
    return EOF;
  }
  
  optarg = NULL;
  for ( const char* osp = optstring;  *osp;  ++osp) {
    if ( *osp == ':' ) continue;
    if ( *osp != c   ) continue;
    if ( osp[1] != ':')  return c;
    if ( argv[i][2] ) {
      optarg = &argv[i][2];
      return c;
    }
    i = optind;
    ++optind;
    if ( i < argc ) {
      optarg = &argv[i][0];
      return c;
    }
    // error: no opt arg
    c = '\0';
    break;
  }
  
  // matched none
   optopt = c;
  if (opterr)
    fprintf(stderr, "?\n");
  return '?';
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "platformWindow_unix.hh"
# pragma implementation "platformWindow.hh"


# include "_platformWindow.cpp.incl"

// Platform-independant functions:


bool AbstractPlatformWindow::change_size_hints(int min_w, int max_w, int min_h, int max_h) {
  // supposed to be for outside
  _min_w = min_w;  _max_w = max_w;  _min_h = min_h;  _max_h = max_h;
  return tell_platform_size_hints();
}
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sig.hh"
# pragma implementation "sig_abstract.hh"
# pragma implementation "sig_inline.hh"

# include "_sig.cpp.incl"


VMStringsIndex PendingSelfSignals::_name [n_real_SelfSignals];
smi            PendingSelfSignals::_count[n_real_SelfSignals];
bool           PendingSelfSignals::_are_any_pending;
bool           PendingSelfSignals::_know_if_any_pending;


void PendingSelfSignals::init() {
  assert(sigint == 0,  "code assumes that first sig is set to zero");

  _are_any_pending     = false;
  _know_if_any_pending = true;
  
  for ( int i = 0;  i < sizeof(_name)/sizeof(_name[0]);  ++i) {
    _name [i] = SIG_UNKNOWN;
    _count[i] = 0;
  }
  _name[ sigint        ] = SIG_INT;
  _name[ sigquit       ] = SIG_QUIT;
  _name[ sigio         ] = SIG_IO; 
  _name[ siguser1      ] = SIG_USER1; 
  _name[ siguser2      ] = SIG_USER2; 
  _name[ sigpipe       ] = SIG_PIPE; 
  _name[ sigterm       ] = SIG_TERM; 
  _name[ sigurg        ] = SIG_URG; 
  _name[ sigchild      ] = SIG_CHILD; 
  _name[ sighup        ] = SIG_HUP; 
  _name[ sigwinch      ] = SIG_WINCH; 
  _name[ sigrealtimer  ] = SIG_REALTIMER; 
  _name[ sigcputimer   ] = SIG_CPUTIMER; 
}
    

smi PendingSelfSignals::Self_result_size() {
  return 2 * n_real_SelfSignals  +  1;
}

void PendingSelfSignals::pass_to_Self(oop resultArg) {
  if (SignalInterface::are_self_signals_blocked())
    fatal("signals are blocked -- shouldn't transfer");
  // must block interrupts to avoid interference from signal_handler
  { SignalBlocker sb;
    fint index = 1;
    for (fint i = 0;
              i < n_real_SelfSignals;  
            ++i ) {
      smi c = count(SelfSignal(i));
      if ( c != 0) {
        resultArg->obj_at_put(index++, name(SelfSignal(i)));
        resultArg->obj_at_put(index++, as_smiOop(c));
        reset(SelfSignal(i));
      }
    }
    _are_any_pending = false;
    _know_if_any_pending = true;
    resultArg->obj_at_put(0, as_smiOop(index / 2));
  }
}


bool PendingSelfSignals::are_any_pending() {
  bool r = false;
  for (  fint i = 0;
              i < n_real_SelfSignals;
            ++i) {
    if ( count(SelfSignal(i)) != 0) {
      r = true;
      break;
    }
  }
  _are_any_pending = r;
  _know_if_any_pending = true;
  return r;
}

// ===============================================================

SelfSignal OSToSelfSignalMapper::_map[Last_OS_Signal];

void OSToSelfSignalMapper::init() {
  for ( int32 i = 0;  i < Last_OS_Signal;  i++)
    _map[i] = sigunknown;
    
  init_platform();
}

// ===============================================================


bool SignalInterface::_block_self_signals  = false; 
bool SignalInterface::_initializing        = false;
bool SignalInterface::_is_in_map_load      = false;


void SignalInterface::initialize(bool ctrlC) {
  _initializing = true;
  _is_in_map_load = false;
  PendingSelfSignals::init();
  OSToSelfSignalMapper::init(); // virtual so cannot do in constructor
  _initializing = !ctrlC;
  initialize_platform(ctrlC);
}


bool SignalInterface::BlockSignals_prim(bool b) {
  bool old = SignalInterface::are_self_signals_blocked();
  if (b) {
    SignalInterface::block_self_signals();
    if (old) warning("_BlockSignals: signals already blocked");
  } 
  else {
    SignalInterface::unblock_self_signals();
    //  LOG_EVENT("unblocking signals");
    if (PendingSelfSignals::are_any_pending()) {
      // make sure the switch happens soon
      currentProcess->setupPreemption();
    }
  }
  return old;
}  


// for timers started by Self primitives:

void SignalInterface::Self_real_timer_tick() { handle_Self_signal( sigrealtimer ); }
void SignalInterface:: Self_CPU_timer_tick() { handle_Self_signal( sigcputimer  ); }


// Called from platfom signal handler, with Unix-like arguments

void SignalInterface::handle_signal(int ossig, char* addr, int32 code) {
  if (!(0 <= ossig  &&  ossig < Last_OS_Signal))
    fatal1("handle_signal: signal %d out of bounds", ossig);
  SelfSignal ssig = OSToSelfSignalMapper::map(ossig);
  if (ssig == sigunknown)    handle_OS_signal( ossig, addr, code);
  else                       handle_Self_signal( ssig );
}


void SignalInterface::flush_input_after_ctrl_c() {
  if ( PendingSelfSignals::keyboard_signals() != 0 ) {
    abortSelf();        // to flush stdin
    PendingSelfSignals::reset_keyboard_signals();
  }
}

    
void SignalInterface::simulate_fatal_signal() {
  InterruptedContext::the_interrupted_context->set();
  AbortContext.invalidate();
  InterruptedContext::fatal_menu();
}


// After OS signal has been translated to a Self-scheduler-aware
// signal, call me.
void SignalInterface::handle_Self_signal(SelfSignal ssig) {
  const int max_ctrl_c = 5;
  FlagSettingInt fs(errno, 0);  // save errno
  safely_handle_Self_signal(ssig);
  
  smi kbs = PendingSelfSignals::keyboard_signals();
  if  ( kbs < max_ctrl_c )
    return; // OK
    
  if  ( kbs == max_ctrl_c ) {
    warning("press one more ^\\ or ^C to force abort");
    return ;
  }

  OS::handle_suspend_and_resume(true);                // set stdin to normal mode
  lprintf("\nDo you really want to force a 'hard' abort (y/n)? ");
  char c[255];
  c[0] = '\0';
  if (fgets(c, sizeof(c), stdin) == NULL) {
    lprintf("\nError while reading answer, quitting.");
    OS::terminate(1);
  }
  if (c[0] == 'y') {
    fatal("forced ^\\ abort");
  } 
  else {
    lprintf("--- not aborted.\n");
    PendingSelfSignals::reset_keyboard_signals(1);
  }
}


void SignalInterface::safely_handle_Self_signal(SelfSignal ssig) {
  // was called from Mac timer interrupt,
  // the safely referred to the fact that it is so easy to crash mac by
  //  e.g., printing
  PendingSelfSignals::increment(ssig);
  if (are_self_signals_blocked())
    return; // no more for now
    
  if (preemptCause == cNoCause) 
    preemptCause = cSignal;
    
  if ( twainsProcess != NULL
  ||   PendingSelfSignals::keyboard_signals() != 0
  ||   need_preemptor_for_timer )
    preemptor();
}


// Handle an OS signal that won't be passed to Self
void SignalInterface::handle_OS_signal(int ossig, char* addr, int32 code) {

  FlagSettingInt fs(errno, 0);  // save errno

  if (eventLog != NULL) // might not exist yet
    LOG_EVENT3("signal %ld pc %#lx npc %#lx",
               ossig, InterruptedContext::the_interrupted_context->pc(), InterruptedContext::the_interrupted_context->next_pc());
            
  # if TARGET_OS_VERSION != MACOSX_VERSION
  assert(!is_off_signal_stack(), "should be on interrupt stack");
  # endif
  // Linux???
  
  if (handle_SIC_OS_signal(ossig, addr, code))
    return;
# if TARGET_OS_VERSION == LINUX_VERSION
  lprintf("\nInternal error: signal %d code %d addr 0x%lx pc 0x%lx.\n",
         (void*)ossig, (void*)code, (void*)(long unsigned)addr,
         (void*)(long unsigned)(InterruptedContext::the_interrupted_context->pc()));
# elif TARGET_OS_VERSION == SOLARIS_VERSION
  lprintf("\nInternal error: signal %d (sig%s) code %d addr 0x%lx pc 0x%lx.\n",
          (void*)ossig, (void*)strsignal(ossig),
          (void*)code, (void*)(long unsigned)addr,
          (void*)(long unsigned)(InterruptedContext::the_interrupted_context->pc()));
# else
  lprintf("\nInternal error: signal %d (sig%s) code %d addr 0x%lx pc 0x%lx.\n",
         (void*)ossig, (void*)sys_signame[ossig],
         (void*)code, (void*)(long unsigned)addr,
         (void*)(long unsigned)(InterruptedContext::the_interrupted_context->pc()));
# endif
  error_breakpoint();
  if (WizardAbortMode) {
    // for better VM debugging - see regs and stack undisturbed, but
    // printing/traversing Self stack may break
    InterruptedContext::fatal_menu();
  } 
  else {
    // let user print the stack etc; easier to do in user context
    WizardAbortMode = true; // next bit might fail over and over
    InterruptedContext::continue_abort_at(first_inst_addr(InterruptedContext::fatal_menu), true);
    AbortContext.set(InterruptedContext::the_interrupted_context);
  }
}


bool SignalInterface::handle_SIC_OS_signal(int ossig, char* addr, int32 code) {
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  
  assert( !(ossig == SIGNonLifo && code == ST_ShouldNeverHappen),
          "SIC compiler error: should never get to this trap instruction");

  if ( FastMapTest
  &&   is_map_load_signal(ossig)
  &&   isMapLoad((int32*)InterruptedContext::the_interrupted_context->pc())) {
    FlagSetting fs2(_is_in_map_load, true);
    handleMapLoadTrap(InterruptedContext::the_interrupted_context);
    return true;
  }
  if (ossig == SIGUncommon && handleUncommonTrap())
    // was uncommon branch trap
    return true;
    
  if ( (ossig == SIGNonLifo     &&  is_uplevel_trap(code))
  ||   (ossig == SIGBadHomeRef  &&  NLRSupport::is_bad_home_reference(addr)
                                &&  Memory->code->contains(InterruptedContext::the_interrupted_context->pc()))) {
    // continue in NLRSupport::non_lifo_abort
    // This is much easier than doing it here because the stack is in a mess right now.
    InterruptedContext::continue_abort_at(first_inst_addr(NLRSupport::non_lifo_abort_from_continuePC), false);
    return true;
  } 

  return false;
}
/* Sun-$Revision: 30.18 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "prim.hh"
# include "_prim.cpp.incl"



# ifndef DYNAMIC
// causes warnings on Tiger -- dmu 5/05
// # include "include_glue.hh"
# endif



# define WHAT_GLUE PROTOTYPES
  # ifdef stat_entries
          stat_glue
  # endif
  # ifdef termcap_entries
          termcap_glue
  # endif
  # ifdef transcendental_entries
          transcendental_glue
  # endif
  # if defined(XLIB) && defined(xlib_entries)
          xlib_glue
  # endif
# undef WHAT_GLUE

  

inline void PrimDesc::verify() {
  // static consistency checks for PrimDescs
  assert(!canAbortProcess() || canScavenge(),
         "inconsistent prim table entry: canAbortProcess implies canScavenge");
  assert(!canWalkStack()    || canScavenge(),
         "inconsistent prim table entry: canWalkStack implies canScavenge");
  assert(docString(), "forgot a boolean flag?");
}

fint PrimDesc::arg_count() {
  // For internal primitives, there is no way to know the number
  // of arguments (e.g. interruptCheck has none, not even a receiver),
  // so return -1 for them (see frame::outgoing_arg_count).
  // I am assuming we never need to restart a send to an internal primitive,
  // seems like a safe bet to me.
  // -- dmu 2/03
  return _type == InternalPrimitive  ?  -1
                                     : str_arg_count(_name);
}


/* The following 3 macros define common settings for the 6 boolean flags
   describing primitives. These are the properties:
          SIDEEFFECTS                              NOSIDEEFFECTS
      ----------------------------------------------------------------
      can    fail                              can    fail
      cannot cause scavenge                    cannot cause scavenge
      cannot be constant folded                can    be constant folded
      cannot be moved or cut                   can    be moved or cut
      cannot walk stack                        cannot walk stack
      cannot abort process                     cannot abort process

   Ole, 11/23/91.
*/
# define SIDEEFFECTS               true, false, false, true,  false, false
# define SIDEEFFECTS_WALKSTACK     true, true , false, true,  true , false
# define SIDEEFFECTS_CANABORT      true, true,  false, true,  true, true
# define NOSIDEEFFECTS             true, false, true,  false, false, false
# define SAFE_SIDEEFFECTS          false,false, false, true,  false, false
# define SAFE_NONIDEMPOTENT        false,false, false, false, false, false


static PrimDesc fntable1[] = {

  // internal prims, in alphabetical order

  // CAUTION: the flags of internal prims are (mostly) for documentation
  // purposes, i.e. they are hardwired into the compiler(s)!
{
  "BlockClone", fntype(&clone_block_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone0", fntype(&clone0_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone1", fntype(&clone1_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone2", fntype(&clone2_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone3", fntype(&clone3_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone4", fntype(&clone4_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone5", fntype(&clone5_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone6", fntype(&clone6_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone7", fntype(&clone7_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone8", fntype(&clone8_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "Clone9", fntype(&clone9_prim),
  InternalPrimitive, ReceiverMapPrimType,
  SAFE_NONIDEMPOTENT,
  "Internal primitive"
},
{
  "SendDIMessage_stub", fntype(&SendDIMessage_stub),
  InternalPrimitive, UnknownPrimType,
  false, true, false, true, true, true,
  "Internal primitive"
},
{
  "DIRecompile_stub", fntype(&DIRecompile_stub),
  InternalPrimitive, UnknownPrimType,
  false, true, false, false, true, false,
  "Internal primitive"
},
{
  "InterruptCheck", fntype(&interruptCheck),
  InternalPrimitive, UnknownPrimType,
  false, true, false, true, true, true,
  "Internal primitive"
},
{
  "SendMessage_stub", fntype(&SendMessage_stub),
  InternalPrimitive, UnknownPrimType,
  false, true, false, true, true, true,
  "Internal primitive"
},
{
  "Recompile_stub", fntype(&Recompile_stub),
  InternalPrimitive, UnknownPrimType,
  false, true, false, false, true, false,
  "Internal primitive"
},
{
  "CatchInterprocessReturns", fntype(&catch_interprocess_returns),
  InternalPrimitive, UnknownPrimType,
   SAFE_NONIDEMPOTENT,
  "Internal primitive for debugging"
},
  
  // called from Self code
{  
"AbortProcess", fntype(&AbortProcess_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a process object.  The associated process is aborted; "
 "if it was the initial process, control will return to the VM prompt; "
 "otherwise, if the aborted process was the current process, the "
 "TWAINS primitive will return 'aborted'.  Otherwise (if the aborted "
 "process isn't the current process), _AbortProcess returns the "
 "receiver."
},
{
"ActivationAt:", fntype(&ActivationAt_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,     // treat as side-effecting because it can't be const
 "Return a mirror on the activation whose number is given as an "
 "argument (0 = most recent activation)."
},
{
"ActivationStack", fntype(&ActivationStack_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_WALKSTACK,     // treat as side-effecting because it can't be const, needs WALKSTACK for PPC
 "Receiver is a process.  Returns a vector containing all activations "
 "on the process stack.  Added to optimize stack tracing.  The same "
 "result can be obtained by iterating through the stack using "
 "_ActivationAt: but is roughly an order of magnitude slower due to "
 "the internal representation of activation mirrors."
},
{
"AddSlots:", fntype(&add_slots_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it includes a define
 "Add all slots of the argument to the receiver."
},
{
"AddSlotsIfAbsent:", fntype(&add_slots_if_absent_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it includes a define
 "Same as _AddSlots:, except that existing slots are never changed."
},
{
"AddressAsObject", fntype(&address_as_oop_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 NOSIDEEFFECTS,
 "Returns the receiver (an integer denoting an address) converted into "
 "an object.  This primitive is used for low-level debugging (addresses "
 "of objects change upon scavenges and garbage collections)."
},
{
"AnnotateSpyLog", fntype(&SelfMonitor::annotateLog_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"AsObject", fntype(&as_object_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 NOSIDEEFFECTS,
 "Returns the receiver (an integer) converted to an object.  The "
 "integer receiver is an object reference number displayed by _Print "
 "or a stack trace."
},
{
  // [Byte]At:[Put:] & Size not glueified for better performance   -Urs 9/92
  // (glue made them 2-3x slower)
"At:", fntype(&ov_at_prim),
 AtPrimitive, UnknownPrimType,
 SIDEEFFECTS,      // cannot be constant-folded
 "Return the element of the receiver (an object vector) indexed by the "
 "argument (an integer).  Vectors are indexed beginning with 0."
},
{
"At:Put:", fntype(&ov_at_put_prim),
 AtPutPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Store into an object vector element.  The receiver is the object "
 "vector, the first argument is the integer index, and the second "
 "argument is the object to be stored.  Returns the receiver."
},
{
"BitSize", fntype(&bitSize_glue),
  ExternalPrimitive, IntegerPrimType,
  NOSIDEEFFECTS,
 "The receiver is a string designating a type.  The primitive returns "
 "an integer, the number of bits in the representation of that "
 "type.  The following strings are valid receivers: 'self_int', "
 "'self_float', 'char', 'short', 'int', 'long', 'float', 'double' and "
 "'void *'.  If sent to any other string the primitive fails."
},
{
"BlockSignals", fntype(&BlockSignals_prim_glue),
 ExternalPrimitive, BooleanPrimType,
 SIDEEFFECTS_CANABORT,  // can do a yield on unblock
 "Sent to true or false.  Enables/disables signals."
},
{
"Breakpoint", fntype(&breakpoint_prim),
 ExternalPrimitive, ReceiverPrimType,
 false, true, false, true, true, true,
 "For debugging the VM; returns the receiver."
},
{
"ByteAt:", fntype(&byteVectorOopClass::bv_at_prim),
 ByteAtPrimitive, IntegerPrimType,
 SIDEEFFECTS,      // cannot be constant-folded (exc. for string literal)
 "Analogous to _At:, but for byte vectors.  Returns an integer in the "
 "range [0..255]."
},
{
"ByteAt:Put:", fntype(&byteVectorOopClass::bv_at_put_prim),
 ByteAtPutPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Store into a byte vector element; analogous to _At:Put:.  The value "
 "to be stored must be an integer in the range [0..255].  Fails if the "
 "receiver is a canonical string."
},
{
"ByteSize", fntype(&byteVectorOopClass::bv_size_prim),
 ByteSizePrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Analogous to _Size, but for byte vectors."
},
{
"ByteVectorCompare:", fntype(&byteVectorOopClass::bv_compare_prim),
 ExternalPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Compares two byte vectors, returning -1, 0, or 1 if the receiver is "
 "less than, equal to, or greater than the argument, respectively."
},
{
"ByteVectorConcatenate:Prototype:", fntype(&bv_concatenate_prim_glue),
 ExternalPrimitive, UnknownPrimType,  // NOT ByteVectorPrimType - depends on arg
 NOSIDEEFFECTS,
 "Clones the second argument, and copies into it a concatentation of "
 "the receiver and first arguments (which should be byte vectors).  "
 "May fail due to lack of space, or if the prototype is a canonical string."
},
{
"CopyByteRangeDstPos:Src:SrcPos:Length:", fntype(&copyByteRange_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Copies 'Length:' bytes into the receiver byte vector at position 'DstPos:' "
 "from the 'Src:' byte vector (or string) at position 'SrcPos:'. May fail "
 "with badTypeError or badIndexError. The destination (receiver) and source "
 "can be identical. Note: the receiver cannot be a canonical string since "
 "these are immutable. Returns the receiver."
},
{
"CopyRangeDstPos:Src:SrcPos:Length:", fntype(&copyRange_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Copies 'Length:' elements into the receiver vector at position 'DstPos:' "
 "from the 'Src:' vector at position 'SrcPos:'. May fail with "
 "badTypeError or badIndexError. The estination (receiver) and source "
 "can be identical. Returns the receiver."
},
{
"CFloatDouble:At:", fntype(&CFloatDouble_At_prim_glue),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "The receiver must be a byte vector, the first argument a boolean and "
 "the second argument an integer index.  The return value is a float "
 "obtained by interpreting the bytes in the byte vector starting at "
 "the given index as either a C float (if the boolean is false) or a C "
 "double (if the boolean is true).  Note that precision may be "
 "lost.  The number of bytes occupied by a C float or C double is "
 "implementation dependent.  The actual sizes can be found using the "
 "_BitSize primitive."
},
{
"CFloatDouble:At:Put:", fntype(&CFloatDouble_At_Put_prim_glue),
 ExternalPrimitive, IntegerPrimType,  /* Always returns 0. */ 
 SIDEEFFECTS,
 "Analogous to _CFloatDouble:At: but stores a floating point "
 "value at a given index in the byte vector.  The last argument is the "
 "value to be stored.  Returns (integer) zero."
},
#  ifndef NO_LONG_LONG // no long long
  {
  "CSignedIntSize:At:", fntype(&CSignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "The receiver must be a byte vector, the first argument an integer "
   "and the second argument an integer index.  The return value is an "
   "integer obtained by interpreting the bytes in the byte vector "
   "starting at the given index as a C int type.  The first argument "
   "gives the size of this integer in bits.  Not all bit sizes are "
   "supported, but the bit sizes corresponding to the C types char, "
   "short, int and long are guaranteed to be valid.  Note: may fail with "
   "overflow error."
  },
  {
  "CSignedIntSize:At:Put:", fntype(&CSignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0.  */
   SIDEEFFECTS,
   "Analogous to _CSignedIntSize:At: but stores an integer value "
   "at a given index in the byte vector.  The last argument is the value "
   "to be stored.  Returns zero."
  },
  {
  "CUnsignedIntSize:At:", fntype(&CUnsignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "Analogous to _CSignedIntSize:At: but interprets the bytes in the "
   "byte vector starting at the given index as a C unsigned int type."
  },
  {
  "CUnsignedIntSize:At:Put:", fntype(&CUnsignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0. */
   SIDEEFFECTS,
   "Analogous to _CUnsignedIntSize:At: but stores an unsigned integer "
   "value at a given index in the byte vector.  The last argument is the "
   "value to be stored.  Returns zero."
  },
  
  {
  "BigEndianSignedIntSize:At:", fntype(&BigEndianSignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "The receiver must be a byte vector, the first argument an integer "
   "and the second argument an integer index.  The return value is an "
   "integer obtained by interpreting the bytes in the byte vector "
   "starting at the given index as a big endian C int type (msbyte first).  The first argument "
   "gives the size of this integer in bits.  Not all bit sizes are "
   "supported, but the bit sizes corresponding to the C types char, "
   "short, int and long are guaranteed to be valid.  Note: may fail with "
   "overflow error."
  },
  {
  "BigEndianSignedIntSize:At:Put:", fntype(&BigEndianSignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0.  */
   SIDEEFFECTS,
   "Analogous to _BigEndianSignedIntSize:At: but stores an integer value "
   "at a given index in the byte vector.  The last argument is the value "
   "to be stored.  Returns zero."
  },
  {
  "BigEndianUnsignedIntSize:At:", fntype(&BigEndianUnsignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "Analogous to _BigEndianSignedIntSize:At: but interprets the bytes in the "
   "byte vector starting at the given index as a BigEndian unsigned int type."
  },
  {
  "BigEndianUnsignedIntSize:At:Put:", fntype(&BigEndianUnsignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0. */
   SIDEEFFECTS,
   "Analogous to _BigEndianUnsignedIntSize:At: but stores an unsigned integer "
   "value at a given index in the byte vector.  The last argument is the "
   "value to be stored.  Returns zero."
  },
  
  
  {
  "LittleEndianSignedIntSize:At:", fntype(&LittleEndianSignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "The receiver must be a byte vector, the first argument an integer "
   "and the second argument an integer index.  The return value is an "
   "integer obtained by interpreting the bytes in the byte vector "
   "starting at the given index as a little endian C int type (lsb first).  The first argument "
   "gives the size of this integer in bits.  Not all bit sizes are "
   "supported, but the bit sizes corresponding to the C types char, "
   "short, int and long are guaranteed to be valid.  Note: may fail with "
   "overflow error."
  },
  {
  "LittleEndianSignedIntSize:At:Put:", fntype(&LittleEndianSignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0.  */
   SIDEEFFECTS,
   "Analogous to _LittleEndianSignedIntSize:At: but stores an integer value "
   "at a given index in the byte vector.  The last argument is the value "
   "to be stored.  Returns zero."
  },
  {
  "LittleEndianUnsignedIntSize:At:", fntype(&LittleEndianUnsignedIntSize_At_prim_glue),
   ExternalPrimitive, IntegerPrimType,
   SIDEEFFECTS,
   "Analogous to _LittleEndianSignedIntSize:At: but interprets the bytes in the "
   "byte vector starting at the given index as a LittleEndian unsigned int type."
  },
  {
  "LittleEndianUnsignedIntSize:At:Put:", fntype(&LittleEndianUnsignedIntSize_At_Put_prim_glue),
   ExternalPrimitive, IntegerPrimType,  /* Always returns 0. */
   SIDEEFFECTS,
   "Analogous to _LittleEndianUnsignedIntSize:At: but stores an unsigned integer "
   "value at a given index in the byte vector.  The last argument is the "
   "value to be stored.  Returns zero."
  },
  
# endif // NO_LONG_LONG
{
"Call",   fntype(&call0_prim),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy.  Calls to foreign routine it "
 "represents and return its return value converted to a Self object"
},
{
"Call:",   fntype(&call1_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:",   fntype(&call2_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:",   fntype(&call3_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:",   fntype(&call4_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:",   fntype(&call5_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:With:",   fntype(&call6_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:With:With:",  
 fntype(&call7_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:With:With:With:",  
 fntype(&call8_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:With:With:With:With:",  
 fntype(&call9_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"Call:With:With:With:With:With:With:With:With:With:",  
 fntype(&call10_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "The receiver is a live fctProxy. Passes the arguments to the foreign "
 "routine and calls the foreign routine it represents, and return its "
 "return value converted to a Self object."
},
{
"CallAndConvert", fntype(&call_and_convert0_glue),
  ExternalPrimitive, ByteVectorPrimType,
  SIDEEFFECTS_CANABORT,
 "Similar to _Call, but returns a byte vector that literally contains "
 "the bit pattern that the foreign routine returned."
},
{
"CallAndConvertWith:And:", fntype(&call_and_convert1_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:", fntype(&call_and_convert2_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:",  
 fntype(&call_and_convert3_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:",  
 fntype(&call_and_convert4_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:",   
fntype(&call_and_convert5_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:With:And:",  
 fntype(&call_and_convert6_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:With:And:With:And",  
 fntype(&call_and_convert7_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And",  
 fntype(&call_and_convert8_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And",  
 fntype(&call_and_convert9_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"CallAndConvertWith:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And:With:And",  
 fntype(&call_and_convert10_glue),
 ExternalPrimitive, ByteVectorPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _CallAndConvert but passes arguments to the foreign "
 "routine. These arguments are determined by interpreting each pair of "
 "Self level arguments using the `any' conversion."
},
{
"Clone", fntype(&clone_prim_glue),
 ClonePrimitive, ReceiverMapPrimType,
 true, true, false, false, false, false, 
 "Return a clone (a shallow copy) of the receiver.  Cloning is the only "
 "way to create new objects in Self.  Returns its receiver (not a copy) "
 "when sent to integers, floats, and canonical strings.  May fail due to "
 "lack of space."
},
{
"Clone:Filler:", fntype(&ov_clone_prim_glue),
  CloneVectorPrimitive, ReceiverMapPrimType,
 true, true, false, false, false, false, 
 "Return a clone (shallow copy) of the receiver object vector, "
 "possibly resized.  The receiver must be an object vector.  The first "
 "argument (an integer) specifies the length of the new vector, and "
 "the second argument specifies the initial value of extra elements if "
 "the result vector is longer than the receiver vector.  Fails with "
 "badSizeError if the first argument is negative.  May also fail due to "
 "lack of space.\n"
 "_Clone:Filler: is "
 "identical to _Clone if the first argument is the same as the length "
 "of the receiver."
},
{
"CloneBytes:Filler:", fntype(&bv_clone_prim_glue),
  CloneVectorPrimitive, ReceiverMapPrimType,
 true, true, false, false, false, false, 
 "Analogous to _Clone:Filler, but for byte vectors.  The receiver must "
 "be a byte vector, and the second argument must be an integer in the "
 "range [0..255].  The integer is used to initialize new "
 "elements.  Fails with badTypeError if sent to a canonical string."
},
{
"CommandLine", fntype(&command_line_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "Return the command line arguments as a vector of strings.  "
 "May fail due to lack of space."
},
{
"Compact", fntype(&compact_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Compact the compiled code cache."
},
{
"CompileCounts", fntype(&get_compile_counts_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"Compilers", fntype(&get_compilers_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 NOSIDEEFFECTS,
 "For internal consumption only."
},
{
"Compilers:Limits:", fntype(&set_recompilation_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"CoreDump", fntype(&core_dump_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"CreateBlockMethodBytecodes:Literals:File:Line:Source:",  
 fntype(&create_block_method_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Creates a new block method given a byteVector of bytecodes, a vector "
 "of literals, a string for the file name, a small integer for the "
 "line number, and a string for the source code.  The literals are "
 "recursively searched for blocks, which are copied and set with the "
 "appropriate lexical parents."
},
{
"CreateOuterMethodBytecodes:Literals:File:Line:Source:",
 fntype(&create_outer_method_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Creates a new outer method given a byteVector of bytecodes, a vector "
 "of literals, a string for the file name, a small integer for the "
 "line number, and a string for the source code.  The literals are "
 "recursively searched for blocks, which are copied and set with the "
 "appropriate lexical parents."
},
{
"Credits", fntype(&credits_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Prints out the credit message."
},
{
"CurrentHash:", fntype(&set_current_hash_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Whenever an identity hash value is assigned to an object, the value "
 "is computed by incrementing a counter that is kept in the process "
 "that requests the identity hash.  This primitive allows the counter "
 "to be reset.  It's main use is to allow for reproducible debugging."
},
{
"CurrentTimeString", fntype(&current_time_string_prim_glue),
  ExternalPrimitive, StringPrimType,
  SIDEEFFECTS, 
 "Returns a human readable string containing the current time.  "
 "Receiver is ignored."
},
{
"DateTime:", fntype(&date_time_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS,
 "Receiver is an integer, number of days after Jan. 1, 1970 describing "
 "the date.  The argument is an integer, number of milliseconds "
 "describing the time of the date.  (_TimeReal can be used to obtain "
 "these numbers for the current time.)  "
 "Converts the date and time integers into two blocks of 9 integers: "
 "year, month, day, weekday (0 = Sunday), hour, minute, second, "
 "yearday (Jan. 1st = 0), and is-daylight-savings-time (positive=yes, "
 "zero=no, negative=unknown). The first block of nine integers describes "
 "local time, the second block describes UTC/GMT."
},
{
"Define:", fntype(&define_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it includes a define
 "Define slots of the receiver.  "
 "Can fail due to low memory."
},
{
"EnumerateAllLimit:", fntype(&all_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS_WALKSTACK, // needs WALKSTACK for debugging
 "Receiver is any object.  Returns a vector containing mirrors on all "
 "objects in the system up to the limit specified by the "
 "argument.  Using a limit of floating-point infinity yields a vector "
 "of mirrors on all objects in the entire Self system."
},
{
"EnumerateVectorImplementorsLimit:", fntype(&ov_implementors_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "Receiver is a vector of selectors (strings) for the messages of "
 "interest.  Returns a vector of mirrors on objects containing a slot "
 "that matches some target selector.  The argument specifies the "
 "maximum size of the result vector.  A limit of floating-point infinity "
 "specifies an unlimited enumeration."
},
{
"EnumerateVectorReferencesLimit:", fntype(&ov_references_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "Receiver is a vector of mirrors on the target objects for which "
 "references are sought.  Returns a vector of mirrors on objects that "
 "refer to any of the target objects.  The limit (either a positive "
 "integer or floating-point infinity) limits the maximum size of the "
 "result vector, to avoid running out of space inadvertently.  A limit "
 "of floating-point infinity specifies an unlimited enumeration."
},
{
"Eq:", fntype(&oop_eq_prim_glue),
 EQPrimitive, BooleanPrimType,
 false, false, true, false, false, false,
 "Identity test; return true iff the receiver and the argument are the same "
 "object."
},
{
"ErrorMessage", fntype(&error_msg_prim_glue),
 ExternalPrimitive, StringPrimType,
 NOSIDEEFFECTS,
 "Return a short description (a canonical string) of the receiver, "
 "which should be an error string returned by a primitive "
 "failure.  Especially useful for UNIX errors.\nExample: 'E2BIG' "
 "_ErrorMessage returns 'Arg list too long'."
},
{
"FindNmethodsRcvr:Selector:", fntype(&findNMethods_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
// float arithmetic/comparisons are unglued for performance reasons
{
"FloatAdd:", fntype(&float_add_prim),
 FloatArithmeticPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Floating-point add.  Returns sum of receiver and argument.  The range "
 "of floating-point numbers currently is approximately "
 "[-4.3*10^-9..4.3*10^9]; the precision is 6 decimal digits (this is "
 "IEEE standard 32-bit float, but with two fewer exponent bits).  Does "
 "not overflow or underflow (but may go to 0.0 or Inf, IEEE "
 "floating-point infinity)."
},
{
"FloatAsInt", fntype(&as_int_prim),
 ExternalPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Return the floating-point receiver rounded as in _FloatRound.  The "
 "result is an integer.  May overflow."
},
{
"FloatCeil", fntype(&float_ceil_prim),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Return the greatest integral value greater than or equal to the "
 "receiver (i.e., rounding towards positive infinity).  The result is a "
 "floating-point number."
},
{
"FloatDiv:", fntype(&float_div_prim),
 FloatArithmeticPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Floating-point division.  Returns receiver divided by argument.  May "
 "fail because of division by zero."
},
{
"FloatEQ:", fntype(&float_eq_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point equality.  Two floating point numbers may be _FloatEQ: "
 "but not _Eq: (e.g. 0.0 and -0.0)."
},
{
"FloatFloor", fntype(&float_floor_prim),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Return the greatest integral value less than or equal to the "
 "floating-point receiver (i.e. rounding towards negative "
 "infinity).  The result is a floating-point number."
},
{
"FloatGE:", fntype(&float_ge_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point greater than or equal."
},
{
"FloatGT:", fntype(&float_gt_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point greater than."
},
{
"FloatLE:", fntype(&float_le_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point less than or equal."
},
{
"FloatLT:", fntype(&float_lt_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point less than."
},
{
"FloatMod:", fntype(&float_mod_prim),
 FloatArithmeticPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Floating-point modulus.  Returns receiver modulo argument.  If r is (x "
 "_FloatMod: y), then 0 <= r < abs(y), and (x-r)/y is an integral "
 "number (even though it might not be representable as a Self "
 "integer).  May fail because of division by zero."
},
{
"FloatMul:", fntype(&float_mul_prim),
 FloatArithmeticPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Floating-point multiplication.  Returns product of receiver and "
 "argument.  Does not overflow or underflow."
},
{
"FloatNE:", fntype(&float_ne_prim),
 FloatComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Floating-point inequality."
},
{
"FloatPrintString", fntype(&print_string_prim_glue),
 ExternalPrimitive, StringPrimType,
 NOSIDEEFFECTS,
 "Return the receiver, a floating-point number, formatted into an "
 "canonical string (similar to C's sprintf(\"%g\") format)."
},
{
"FloatPrintStringPrecision:", fntype(&print_string_precision_prim_glue),
 ExternalPrimitive, StringPrimType,
 NOSIDEEFFECTS,
 "Analogous to _FloatPrintString, but takes an integer argument that "
 "specifies the number of digits after the decimal point."
},
{
"FloatRound", fntype(&float_round_prim),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Return the receiver rounded to the nearest integer. 0.5 is rounded "
 "to even, so 1.5 rounds to 2, and 2.5 also rounds to 2.  The result is "
 "a floating-point number."
},
{
"FloatSub:", fntype(&float_sub_prim),
 FloatArithmeticPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Floating-point subtraction.  Returns receiver minus argument.  Does "
 "not overflow or underflow."
},
{
"FloatTruncate", fntype(&float_truncate_prim),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Return the receiver truncated towards zero.  The result is a "
 "floating-point number."
},
{
"Flush", fntype(&flush_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Flush all compiled methods from the compiled code cache."
},
{
"FlushInlineCache", fntype(&flush_inline_cache_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Flush all inline caches.  An inline cache caches the result of "
 "message lookups at the site of a message send.  Inline caching speeds "
 "up subsequent executions of the particular send if the type of the "
 "receiver does not change.  See [DS84] for details."
},
{
"FlushUnusedCode", fntype(&flushUnused_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Flush unused methods from the compiled code cache."
},
{
"MarkCodeUnused", fntype(&markAllUnused_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Mark all code as unused in the compiled code cache."
},
{
"ForeignHash", fntype(&foreign_hash_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS,
 "The receiver is a proxy or fctProxy object.  Return an integer hash "
 "value for the proxy.  Note that this hash value is dependent on what "
 "the proxy is referring."
},
{
"ForeignIsLive", fntype(&foreign_is_live_glue),
 ExternalPrimitive, BooleanPrimType,
 SIDEEFFECTS,
 "The receiver is a proxy or fctProxy object.  Returns true iff it "
 "receiver is live."
},
{
"ForeignIsNull", fntype(&foreign_is_null_glue),
 ExternalPrimitive, BooleanPrimType,
 SIDEEFFECTS,
 "The receiver is a proxy or fctProxy object.  Returns true iff the "
 "pointer it encapsulates is NULL."
},
{
"ForeignKill", fntype(&foreign_kill_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS,
 "The receiver is a proxy or fctProxy object.  Kills this "
 "object.  Returns 0."
},
{
"GarbageCollect", fntype(&garbage_collect_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 false, true, false, true, false, false,
 "Force a full garbage collection (this may take several seconds "
 "depending on your hardware and the size of old space)."
},
{
"GotoByteCode:ExpressionStack:", fntype(&GotoByteCode_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 true, true, false, true,  false, false, // SIDEEFFECTS & CAN_CAUSE_SCAVENGE
 "Receiver is a process.  Changes the current byte code index and "
 "replaces the expression stack of the current activation."
},
{
"IdentityHash", fntype(&identity_hash_prim_glue),
  ExternalPrimitive, IntegerPrimType,
  false, false, true, false, false, false, 
 "Return an integer hash value for the receiver.  The hash for a "
 "particular object is constant, but it is not unique (several objects "
 "might have the same hash value)."
},
// int arithmetic/comparisons are unglued for performance reasons
{
"IntAdd:", fntype(&smi_add_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Integer addition.  Returns sum of receiver and argument.  The range of "
 "integers is [-229..229-1], i.e., roughly 536,000,000 (standard 30-bit "
 "two's complement).  May fail because of overflow."
},
{
"IntAnd:", fntype(&smi_and_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Returns bitwise AND of receiver and argument."
},
{
"IntArithmeticShiftLeft:", fntype(&smi_arithmetic_shift_left_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Shift receiver left by the number of bits indicated by the argument "
 "(an integer).  Will fail with overflowError if the resulting number "
 "is too large to be represented as an integer (equivalent to "
 "multiplying by a power of 2)."
},
{
"IntArithmeticShiftRight:", fntype(&smi_arithmetic_shift_right_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Arithmetic right shift of receiver by the number of bits indicated "
 "by the argument (an integer).  The sign bit is preserved."
},
{
"IntAsFloat", fntype(&as_float_prim),
 ExternalPrimitive, FloatPrimType,
 NOSIDEEFFECTS,
 "Return the integer receiver converted to a float."
},
{
// could be IntArithmeticPrimitive, but would need more work
// since a unary primitive, not a binary primitive
"IntComplement", fntype(&smi_complement_prim),
 ExternalPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Returns bitwise complement (i.e., invert all bits) of receiver."
},
{
"IntDiv:", fntype(&smi_div_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Integer division.  Returns integer part of receiver divided by "
 "argument.  May fail because of overflow or division by zero."
},
{
"IntEQ:", fntype(&smi_eq_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer equality.  If two integers are _IntEQ: then they are _Eq:."
},
{
"IntGE:", fntype(&smi_ge_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer greater than or equal."
},
{
"IntGT:", fntype(&smi_gt_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer greater than."
},
{
"IntLE:", fntype(&smi_le_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer less than or equal."
},
{
"IntLT:", fntype(&smi_lt_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer less than."
},
{
"IntLogicalShiftLeft:", fntype(&smi_logical_shift_left_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Bitwise shift receiver left by the number of bits indicated by the "
 "argument (an integer).  No overflow will occur."
},
{
"IntLogicalShiftRight:", fntype(&smi_logical_shift_right_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Logical right shift of receiver by the number of bits indicated by "
 "the argument (an integer).  0 is shifted into the most significant "
 "bit."
},
{
"IntMod:", fntype(&smi_mod_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Integer modulus.  Returns receiver modulo argument, with range 0 <= "
 "(n _IntMod: m) < abs(m).  May fail because of division by zero."
},
{
"IntMul:", fntype(&smi_mul_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Integer multiplication.  Returns product of receiver and "
 "argument.  May fail because of overflow."
},
{
"IntNE:", fntype(&smi_ne_prim),
 IntComparisonPrimitive, BooleanPrimType,
 NOSIDEEFFECTS,
 "Integer inequality."
},
{
"IntOr:", fntype(&smi_or_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Returns bitwise inclusive OR of receiver and argument."
},
{
"IntSub:", fntype(&smi_sub_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Integer subtraction.  Returns receiver minus argument.  May fail "
 "because of overflow."
},
{
"IntXor:", fntype(&smi_xor_prim),
 IntArithmeticPrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Returns bitwise exclusive OR of receiver and argument."
},
{
"KillActivationsUpTo:", fntype(&KillUpTo_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 true, true, false, true,  false, false, // SIDEEFFECTS & CAN_CAUSE_SCAVENGE
 "Receiver is a process (not the current process or the scheduler); "
 "argument is the number of activations to kill."
},
{
"Manufacturer", fntype(&manufacturer_prim_glue),
 ExternalPrimitive, StringPrimType,
 SAFE_NONIDEMPOTENT,
 "Return the name (a canonical string) of the manufacturer of the host "
 "computer."
},
{
"MemoryCurrentSpaceSizes:", fntype(&get_current_space_sizes_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Argument is a prototype which will be filled in with the current VM "
 "startup parameters.  See result of _MemoryDefaultSpaceSizes for slot "
 "names.  Receiver is ignored.  Fails if a slot is not assignable."
},
{
"MemoryCurrentState:", fntype(&get_mem_current_state_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Argument is a prototype which will be filled in with the current VM "
 "memory occupancy.\n"
 "Memory spaces:  slot names are 'eden', 'from', 'to', and 'old'.\n"
 " 'old' will be set to reference a new vector, the size being the same as "
 "the number of old space segments.  Each of these elements, together "
 "with the slots called 'eden', 'from', and 'to' will be set to reference "
 "a vector with these elements:\n"
 "\t0 is the number of bytes occupied by oops\n"
 "\t1 is the number of bytes occupied by byte strings.\n"
 "\t2 is the capacity of the space in bytes.  Note that the capacity of"
 " a space may be less than its size as reported by _MemoryCurrentSpaceSizes:,"
 " due to alignment requirements, padding, etc."
 "(Future versions may supply extra elements.)\n"
 "GC and card table:\n"
 " 'card_table_size' will be set to the size of the card table in bytes.\n"
 " 'num_scavenge' will be set to the number of scavenges since system "
 "  startup.\n"
 "Zone: slots called 'code', 'deps', 'pics' and 'debug' will be set to the "
 "number of bytes used in the corresponding spaces.\n"
 "Any slot can be omitted, in which case the corresponding information "
 "is omitted.  Extra slots are ignored.  "
 "Note that calling this primitive affects the very statistics it returns!\n"
 "Can fail due to insufficient memory, or if a slot is not assignable.  "
 "Receiver is ignored."
},
{
"MemoryDefaultSpaceSizes", fntype(&get_default_space_sizes_prim),
 ExternalPrimitive, UnknownPrimType,
 SAFE_SIDEEFFECTS,
 "Returns an object containing the default VM startup parameters.  "
 "Receiver is ignored."
},
{
"MemoryExpandHeap:", fntype(&expand_heap_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS_WALKSTACK, // Walkstack for PPC to reload byte-map-base
 "Attempt to expand the Self by heap by the (non-negative smallInt) number "
 "of bytes which is the argument.  "
 "Either expands by the desired amount (or possibly a little more due to "
 "rounding to page boundaries), or performs no expansion.  Returns "
 "the actual number of bytes added to the heap.  "
 "Receiver is ignored."
},
{
"MemoryLowSpaceThreshold", fntype(&get_memory_low_space_threshold_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SAFE_SIDEEFFECTS,
 "Returns the low space threshold for the object heap (old space).  "
 "When the amount of free space drops below this amount, the scheduler "
 "is awakened in order to reclaim or expand memory.  "
 "Receiver is ignored."
},
{
"MemoryLowSpaceThreshold:", fntype(&set_memory_low_space_threshold_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS,
 "Sets the low space threshold (see _MemoryLowSpaceThreshold) from the "
 "argument (which must be a smallInt as least as large as "
 "_MemoryVMReservedAmount; otherwise the prim fails).  "
 "Returns the previous threshold.  Receiver is ignored."
},
{
"MemorySwapSpace", fntype(&get_swap_space_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Returns an array whose first element is the total number of kilobytes in "
 "the swap area, and whose second element is the number of kilobytes free.  "
 "Receiver is ignored.  Only supported on Solaris 2.X at present."
},
{
"MemoryTenuringThreshold", fntype(&get_memory_tenuring_threshold_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SAFE_SIDEEFFECTS,
 "Returns the desired tenuring threshold (in bytes).  The scavenger will "
 "attempt to keep the occupancy of the survivor space after scavenging below "
 "this amount.  Receiver is ignored."
},
{
"MemoryTenuringThreshold:", fntype(&set_memory_tenuring_threshold_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS,
 "Sets the tenuring threshold (see _MemoryTenuringThreshold) from the "
 "argument (which must be a non-negative smallInt, less than the size of a "
 "survivor space [see _MemoryCurrentSpaceSizes:], otherwise the prim fails).  "
 "Returns the previous threshold.  Receiver is ignored."
},
{
"MemoryVMReservedAmount", fntype(&VM_reserved_mem_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SAFE_SIDEEFFECTS,
 "Return the amount of memory in old space (in bytes) reserved by the VM.  "
 "Receiver is ignored."
},
{
"MemoryWriteSnapshot:Compress:Sizes:SaveCode:", fntype(&full_write_snapshot_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Write a snapshot to the file named by the first argument, a byte vector.\n"
 "The second argument "
 "dictates whether the snapshot will be compressed or not.  "
 "It must be a slots objects.  "
 "If it contains slots named 'compression_filter' and "
 "'decompression_filter', and the values of these slots are byte vectors, "
 "they will be used to compress and decompress the binary part of the "
 "snapshot.  Each is used as a pure filter (i.e., reads from stdin, "
 "writes to stdout).  The value of the PATH environment variable will be "
 "used to locate the programs.  The decompression filter is only used at "
 "system startup -- it is _essential_ that a valid filter be named, "
 "otherwise the snapshot will be incapable of startup.  Suitable choices "
 "are 'compress' and 'zcat'.\n"
 "The third "
 "argument is used to set the default space sizes in the snapshot, and "
 "should be an object with slots like those in the object returned by "
 "_MemoryDefaultSpaceSizes.  You may omit slots to use the current "
 "values; extra slots are ignored.  Returns the "
 "first argument.  Receiver is ignored.  Fails if the snapshot cannot "
 "be written (e.g, due to the disk being full, etc.).\n"
 "The fourth argument, a boolean, says whether to save compiled code in the "
 "snapshot.  Note that setting this to true does not guarantee that the code "
 "can be used when the snapshot is read."
},
{
"MethodPointer", fntype(&ov_methodPointer_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS,
 "Receiver is an object vector.  If the object vector is a literal "
 "vector it returns a mirror on the method referring to the literal "
 "vector.  Otherwise the primitive fails."
},
{
"Mirror", fntype(&as_mirror_prim),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Returns a mirror on the receiver (any object).  A mirror gives a view "
 "of an object that looks like a vector of slots.  Mirrors are used to "
 "obtain information about aspects of objects that are not directly "
 "observable on the Self level, for example, the names of an object's "
 "slots or the source code of a method.\nThe object on which a mirror "
 "is created is called its reflectee.  The mirror answers all questions "
 "by inspecting its reflectee.  There are a different kinds of mirrors "
 "for different kinds of objects, but all respond to "
 "the same set of primitives (with a few "
 "exceptions).  _Mirror operates by cloning the mirror prototype "
 "appropriate for the type of its receiver, installing the receiver as "
 "the reflectee of the cloned mirror, and returning the new cloned "
 "mirror."
},
{
"MirrorAnnotation", fntype(&get_annotation_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Returns the annotation for the mirror's reflectee."
},
{
"MirrorAnnotationAt:", fntype(&annotation_at_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Returns the annotation of the slot named by the argument."
},
{
"MirrorByteCodePosition", fntype(&bci_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it looks at the stack
 "Return the current position within the method.  (Future releases will "
 "include a way to find the current source position.)"
},
{
"MirrorCodes", fntype(&codes_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror on a method.  Returns the byte code vector."
},
{
"MirrorContentsAt:", fntype(&contents_at_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK,
 "Return a mirror on the contents of the specified slot of the "
 "reflectee.  The argument, a canonical string, is the name of the slot."
},
{
"MirrorCopyAt:Put:IsParent:IsArgument:Annotation:",
  fntype(&copy_add_slot_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // conservative; not sure if really can walk stack
 "Make a copy of the mirror's reflectee with a new or changed "
 "slot.  The arguments are: the name of the slot (a string), "
 "a mirror on the new contents, "
 "if it is a parent slot, if it is an argument slot, "
 "and the annotation for the slot (an object).  "
 "If the slot already "
 "exists, its attributes and contents are replaced; if it doesn't "
 "exist, a new slot is added.  Returns a mirror on the copy.  "
 "May fail due to lack of space for the copy."
},
{
"MirrorCopyAnnotation:", fntype(&copy_set_annotation_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Make a copy of the mirror's reflectee with its annotation set to the "
 "argument.  May fail due to lack of space for the copy."
},
{
"MirrorCopyRemoveSlot:", fntype(&copy_remove_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Make a copy of the mirror's reflectee with the named slot removed.  "
 "May fail if the reflectee cannot have the slot removed due to lack of space for the copy."
},
{
"MirrorCreateBlock",   fntype(&create_block_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror containing a block method.  Returns a block "
 "whose value slot contains the method of the receiver."
},
{
"MirrorDefine:", fntype(&mirror_define_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK,  // needs WALKSTACK because it includes a define
 "The mirror version of _Define:.  The receiver is a mirror on the "
 "object to be changed, the argument is an object defining the new "
 "contents.  Can fail due to lack of space."
},
{
"MirrorEvaluate:", fntype(&evaluate_in_context_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_CANABORT,
 "Receiver is a mirror.  Takes a mirror on a method as "
 "argument.  Evaluates the method in the context of the reflectee of "
 "the receiver."
},
{
"MirrorExpressionStack", fntype(&expr_stack_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "Return a vector containing the values of all expressions of a "
 "statement which have been evaluated but not yet consumed by any "
 "message send.  For example, if an activation were suspended just "
 "before sending the `+' message in the statement i: i + 1, the "
 "expression stack would contain i and 1."
},
{
"MirrorFile", fntype(&file_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror on a method.  Returns the file name from where "
 "the method was parsed.  Methods parsed at the prompt yields "
 "'<prompt>'."
},
{
"MirrorIsArgumentAt:", fntype(&is_argument_at_prim_glue),
  ExternalPrimitive, BooleanPrimType,
  SIDEEFFECTS_WALKSTACK,
 "Test if the specified slot is an argument slot.  The argument, a "
 "canonical string, is the name of the slot being tested."
},
{
"MirrorIsAssignableAt:", fntype(&is_assignable_at_prim_glue),
  ExternalPrimitive, BooleanPrimType,
  SIDEEFFECTS_WALKSTACK,
 "Test if the specified slot is assignable.  The argument, a "
 "canonical string, is the name of the slot being tested."
},
{
"MirrorIsParentAt:", fntype(&is_parent_at_prim_glue),
  ExternalPrimitive, BooleanPrimType,
  SIDEEFFECTS_WALKSTACK,
 "Test if the specified slot is a parent slot.  The argument, a "
 "canonical string, is the name of the slot being tested."
},
{
"MirrorLexicalParent", fntype(&parent_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "If the reflectee of the receiver is a block activation, return the "
 "activation corresponding to the lexically enclosing scope.  This "
 "primitive will fail for method activations, since method activations "
 "have no lexically enclosing scope."
},
{
"MirrorLine", fntype(&line_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror on a method.  Returns the line in which the "
 "method was parsed."
},
{
"MirrorLiterals", fntype(&literals_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror on a method.  Returns the literal vector."
},
{
"MirrorMethodHolder", fntype(&methodHolder_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "Return the activation's method holder, i.e., the object containing "
 "the slot whose evaluation created this activation."
},
{
"MirrorNames", fntype(&names_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS_WALKSTACK, // could be vframe oop,see mirror_names comment
 "Return a vector containing the names of all the slots of the "
 "reflectee."
},
{
"MirrorNameAt:", fntype(&name_at_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK,
 "Return the name of the local slot at index, for read/write local bytecodes."
},
{
"MirrorReceiver", fntype(&receiver_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "Return the receiver of this activation."
},
{
"MirrorReflectee", fntype(&get_reflectee_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Return this mirror's reflectee.  Fails if invoked on a mirror on a "
 "method."
},
{
"MirrorReflecteeEq:", fntype(&reflectee_eq_prim),
  ExternalPrimitive, BooleanPrimType,
  NOSIDEEFFECTS,
 "Test if the receiver and argument are mirrors on the same "
 "object.  See _Eq:."
},
{
"MirrorReflecteeIdentityHash",  
  fntype(&reflectee_id_hash_prim),
  ExternalPrimitive, IntegerPrimType,
  NOSIDEEFFECTS,  
 "Return the identity hash of the reflectee of the receiver.  See "
 "_IdentityHash."
},
{
"MirrorSelector", fntype(&selector_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "Return the name of the activation, i.e., the selector of the slot in "
 "which the activation's method is stored."
},
{
"MirrorSender", fntype(&sender_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK, // needs WALKSTACK to look up the stack
 "Return a mirror on the sender activation, i.e., the activation which "
 "created the receiver activation.  This primitive will fail if the "
 "activation was created by the VM (e.g., for a doIt method)."
},
{
"MirrorSize", fntype(&size_prim_glue),
  ExternalPrimitive, IntegerPrimType,
  SIDEEFFECTS,
 "Return the number of bytes the reflectee occupies in the heap."
},
{
"MirrorSource", fntype(&source_prim_glue),
  ExternalPrimitive, StringPrimType,
  SIDEEFFECTS,
 "Receiver is a mirror on a method.  Returns the source code of the "
 "method as a string.  If the method is nested in another (e.g., is a "
 "block), then the string may contain source of the enclosing code.  "
 "Use _MirrorSourceOffset and _MirrorSourceLength to determine the "
 "appropriate substring; if _MirrorSourceOffset is non-zero, then take "
 "the substring defined by offset and length, otherwise take the whole "
 "string."
},
{
"MirrorSourceOffset", fntype(&source_offset_prim_glue),
  ExternalPrimitive, IntegerPrimType,
  SIDEEFFECTS,
 "In combination with _MirrorSource (q.v.) and _MirrorSourceLength can be "
 "used to get the source of a method."
},
{
"MirrorSourceLength", fntype(&source_length_prim_glue),
  ExternalPrimitive, IntegerPrimType,
  SIDEEFFECTS,
 "In combination with _MirrorSource (q.v.) and _MirrorSourceOffset can be "
 "used to get the source of a method."
},
{
"NewProcessSize:Receiver:Selector:Arguments:", fntype(&NewProcess_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Returns a new process object which is obtained by cloning the "
 "current process object. The new process is not started. The first "
 "argument is an integer giving a minimal stack size in bytes; the "
 "system may actually allocate more stack space. The right size "
 "depends on the future behavior of the process so some "
 "experimentation may be necessary. A good first try could be 64"
 "Kb. The last two arguments are analogous to the arguments of "
 "_Perform:With:. They determine the first message that the process"
 "sends when it is started. The receiver of "
 "_NewProcessSize:Selector:Arguments will also be the receiver of the "
 "first message send of the new process."
},
{
"NoOfArgs", fntype(&get_noOfArgs_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS,
 "Receiver is a fctProxy.  Return how many arguments should be supplied "
 "when calling this function.  The value -1 designates a function that "
 "takes a variable number of arguments."
},
{
"NoOfArgs:", fntype(&set_noOfArgs_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Receiver is a fctProxy.  Set how many arguments it should take when "
 "called.  The value -1 will allow it to be called with any number of "
 "arguments.  Warning: this call is potentially dangerous, since no "
 "attempt is done at checking that the given value is "
 "reasonable.  Returns receiver.  Calls the foreign routine it "
 "represents, and return its return value converted to a Self "
 "object."
},
{
"ObjectID", fntype(&objectID_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SAFE_SIDEEFFECTS,
 "Returns the reference number of the receiver, assigning one if "
 "necessary.  _ObjectID is the reverse to _AsObject"
},
{
 "OnNonLocalReturn:", fntype(&oopClass::unwind_protect_prim),
 UnwindProtectPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Sends 'value' to the receiver; "
 "if this returns V normally, then return V as the result of the primitive.  "
 "Otherwise, it attempts to return V non-locally, or aborts; if so "
 "send 'value: V' to the first argument of the primitive.  "
 "If this in turn returns V', normally or non-locally, then "
 "continue the original non-local-return/abort with V' as the value "
 "returned.  Otherwise, continue aborting."
},
{
"OperatingSystem", fntype(&operating_system_prim_glue),
 ExternalPrimitive, StringPrimType,
 SAFE_NONIDEMPOTENT,
 "Returns a string describing the operating system.  The receiver is "
 "ignored."
},
{
"ParseObjectFileName:ErrorObj:",
 fntype(&parseObject_prim_glue),
 ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_CANABORT,
 "Parse the receiver, a string, as a Self object.  The argument"
 "filename is used to annotating parsed methods with "
 "source code information.  The last argument is an object for the  "
 "parse error information.  Returns the object created by the parser.  "
 "Can fail due to lack of space."
},
{
"ParseObjectIntoPositionTable",
 fntype(&parseObjectIntoPositionTable_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Receiver is the source code for a method (a string, typically obtained "
 "using _MirrorSource).  Returns a vector containing two elements for "
 "each byte code associated with the method.  For each byte code these "
 "two elements describe the corresponding text selection in the source "
 "code.  The first element is the selection start in the source, the "
 "second is the length of the selection."
},
{
"Perform:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Sends the unary message named by the first argument (a canonical string) to the "
 "receiver, and returns the result.\n"
 "Example: x _Perform: 'foo'  has exactly the same semantics as  x foo.\n"
 "_Performs never evaluate their IfFail: argument upon failure (in the case "
 "where the IfFail: extension is used); instead, one of the runtime message "
 "lookup errors will be invoked.  These messages are peculiar to "
 "_Perform:\n"
 "mismatchedArgumentCountSelector:Type:Delegatee:MethodHolder:Arguments: is "
 "used when the number of arguments does not match that specified by the "
 "selector, e.g. x _Perform: '+'.\n"
 "performTypeErrorSelector:Type:Delegatee:MethodHolder:Arguments: is used "
 "when the selector is not a canonical string, e.g., x _Perform: nil.\n"
},
{
"Perform:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _Perform:, except that the selector must be a binary or "
 "one-argument keyword selector, and the second argument to the primitive "
 "is passed as the argument to the message.\n"
 "Example:  x _Perform: '+' With: 3  is identical to  x+3."
},
{
"Perform:With:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _Perform:, except that the selector must be a "
 "two-argument keyword selector, and the second and third arguments to the primitive "
 "are passed as the arguments to the message.  Adding more With:s adds more "
 "arguments.\n"
 "Example:  x _Perform: 'foo:Bar:' With: 3 With: nil  is identical to  x "
 "foo: 2 Bar: nil."
},
{
"PerformResend:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _Perform:, except that it performs an undirected resend.\n"
 "Example: x _PerformResend: 'foo'  has exactly the same semantics as  x "
 "resend.foo.\n"
 "In the current implementation you cannont `perform' a directed resend."
},
{
"PerformResend:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _PerformResend:, except that the selector must be a binary or "
 "one-argument keyword selector, and the second argument to the primitive "
 "is passed as the argument to the message."
},
{
"Perform:With:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _PerformResend:, except that the selector must be a "
 "two-argument keyword selector, and the second and third arguments to the primitive "
 "are passed as the arguments to the message.  Adding more With:s adds more arguments."
},
{
"Perform:DelegatingTo:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "This primitive performs a delegated send.  "
 "It sends the unary message named by the first argument (a canonical string) to the "
 "receiver, but lookup starts in the second argument.  Any references to self "
 "in the resulting method will refer to the receiver of the _Perform."
},
{
"Perform:DelegatingTo:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _Perform:DelegatingTo:, except that the selector must be a binary or "
 "one-argument keyword selector, and the third argument to the primitive "
 "is passed as the argument to the message."
},
{
"Perform:DelegatingTo:With:With:",
 fntype(NULL),
 NotReallyAPrimitive, UnknownPrimType,
 SIDEEFFECTS_CANABORT,
 "Similar to _Perform:DelegatingTo:, except that the selector must be a "
 "two-argument keyword selector, and the third and fourth arguments to the primitive "
 "are passed as the arguments to the message.  Adding more With:s adds more "
 "arguments."
},
{
"PrimitiveDocumentation",
 fntype(&primitive_documentation_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Receiver is a name of a primitive, a string. "
 "Returns the documentation string for the primitive."
},
{
"PrimitiveList",
 fntype(&primitive_list_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Returns a vector containing the names of all primitives.  "
 "Receiver is ignored.  May fail due to lack of space."
},
{
"Print", fntype(&print_prim_glue),
 ExternalPrimitive, UnknownPrimType,
 SAFE_SIDEEFFECTS,
 "Print the receiver in a low-level format and return nil."
},
{
"PrintDebugSpace", fntype(&PrintDebugSize_prim),
 ExternalPrimitive, IntegerPrimType,
 SAFE_SIDEEFFECTS,
 "For internal consumption only."
},
{
"PrintEventLog:", fntype(&printEvent_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"PrintFlatProfile:", fntype(&PrintFlatProfile_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Print the flat profile; the argument is an integer specifying how "
 "many lines should be printed.  The profile shows the time consumed by "
 "each compiled method (including the time consumed by any methods "
 "inlined into the compiled method, but not including any time spent "
 "in the VM)."
},
{
"PrintMemory", fntype(&print_memory_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Print low-level debugging information about the memory system."
},
{
"PrintMemoryHistogram:", fntype(&print_memory_histogram_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Prints out two histograms.  The first one is based on VM object "
 "types, the second based on the word size of the objects.  The "
 "argument is a number defining the upper limit size in the second "
 "histogram."
},
{
"PrintNMethodCode", fntype(&printNMethodCode_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"PrintNMethodHistogram:", fntype(&print_nmethod_histogram_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"PrintSlotStats", fntype(&print_slot_stats_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Print slot statistics."
},
{
"PrintZoneStats", fntype(&print_zone_stats_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Print zone statistics in short form."
},
{
"PrintOptionPrimitives", fntype(&print_option_primitives_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Print a list of the available option primitives with a short "
 "explanation of their function and their current settings."
},
{
"PrintChangedOptionPrimitives", fntype(&print_changed_option_primitives_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Print a list of the available option primitives with a short "
 "explanation of their function and their current settings "
 "that have been changed from their defaults."
},
{
"PrintPICHistogram", fntype(&pic_histogram_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"PrintProcessStack", fntype(&PrintProcessStack_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_CANABORT, // needs WALKSTACK to look up the stack, CANABORT so you can interrupt it
 "Print the stack of the process associated with the receiver (a "
 "process object).  The number of stack frames printed is determined by "
 "_StackPrintLimit."
},
{
"PrintProfileCutoff:Skip:MaxDepth:", fntype(&PrintProfile_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Print the profile.  Subtrees of the call tree using a smaller "
 "fraction of the total time than cutoff (specified as a float, "
 "e.g., 0.02 = 2%) will be suppressed.  Furthermore, a method will only "
 "be displayed if its time (including the time of its callees) differs "
 "by more than skip from its caller.  MaxDepth (an integer) specifies "
 "the maximum call depth to be displayed.  As in the flat profile, an "
 "inlined method is charged to its caller (i.e., inlining is not "
 "transparent)."
},
{
"PrintVMObj", fntype(&printVMObj_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"ProcessReturnValue", fntype(&ProcessReturnValue_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 NOSIDEEFFECTS,
 "Receiver is a process.  Returns the result of the expression "
 "evaluated in the process if the process has terminated.  The return "
 "value is undefined if the process is still active."
},
{
"Profile:", fntype(&Profile_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Receiver is a process.  Activate/deactivate profiling of Self code "
 "executed by the receiver.  The argument is boolean, true activates "
 "the profiler.  Profiling incurs little run-time overhead.  By default "
 "profiling on a process is deactivated."
},
{
"ProfilerCopyGraphMethod:Block:Access:Prim:Leaf:Fold:Unknown:Cutoff:",
 fntype(&ProfilerCopyGraph_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler.  Returns a Self-level copy of the VM-level "
 "call graph created using _ProfilerEngage: and _ProfilerDisengage.  "
 "All but the penultimate argument are prototypes for the different kinds of "
 "call graph nodes.  The penultimate argument is used for reconstructing a "
 "dummy receiver object in case the receiver type of a node has no "
 "corresponding object. The last argument gives a percentage of total time.  "
 "Nodes smaller than that will not be copied."
},
{
"ProfilerDisengage", fntype(&ProfilerDisengage_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler.  Stops profiling the profiled process."
},
{
"ProfilerEngage:", fntype(&ProfilerEngage_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler and argument a process.  Starts profiling."
},
{
"ProfilerPrint:", fntype(&ProfilerPrint_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler.  Prints the VM-level call graph."
},
{
"ProfilerProcess", fntype(&ProfilerProcess_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler.  Returns the process currently being "
 "profiled."
},
{
"ProfilerReset", fntype(&ProfilerReset_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler.  Frees all VM-level state associated with the "
 "profiler."
},
{
"ProfilerTicks:", fntype(&ProfilerTicks_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler and argument a prototype for the tick "
 "information.  Returns a copy of the prototype with tick "
 "statistics.  The following data slots will be assigned by the "
 "primitive (unit is ticks):\n inSelf - executing Self code\n inPrim - "
 "executing primitives.\ninLookup - performing lookup.\n inSemaphore - "
 "unable to collect stack.  (switching process or inside the "
 "profiler)"
},
{
"ProfilerTimes:", fntype(&ProfilerTimes_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Receiver is a profiler and argument a prototype for the time "
 "information.  Returns a copy of the prototype with time "
 "statistics.  The following data slots will be assigned by the "
 "primitive (unit is milliseconds):\n runtime - program execution.\n "
 "collect - collecting stack samples\n build - constructing the "
 "VM-level call graph.\n monitor - spy.\n compile - compiling.\n "
 "recompile - recompiling.\n uncommon - compiling uncommon branches.\n "
 "scavenge - scavenges.\n garbageCollect - total garbage "
 "collections.\n methodCompact - compacting the instruction cache.\n "
 "methodFlush - flushing compiled methods."
},
{
  "ProgrammingTimestamp", fntype(&programming_timestamp_prim_glue),
  ExternalPrimitive, IntegerPrimType,
  SAFE_NONIDEMPOTENT,
 "Returns a count from the VM of the number of define's that have been "
 "executed.  Used by the pathCache to stay up to date."
},
{
  "SetProgrammingTimestamp", fntype(&set_programming_timestamp_prim_glue),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
 "Sets the count in the VM of the number of define's that have been "
 "executed.  Used by the pathCache to stay up to date."
},
{
"Quit", fntype(&quit_prim_glue),
 ExternalPrimitive, NoReturnPrimType,
 SAFE_SIDEEFFECTS,
 "Quit the system (equivalent to typing ^D (control-D) at the "
 "VM prompt).  The state of the world is not saved."
},
{
"RecompileLimits", fntype(&get_recompile_limits_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 NOSIDEEFFECTS,
 "For internal consumption only."
},
{
"RemoveAllSlots", fntype(&remove_all_slots_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it includes a define
 "Removes all slots from the receiver.  Fails if the receiver is a "
 "smallInteger, string, block or float.  May fail due to lack of space "
 "(I am not kidding)."
},
{
"RemoveSlot:", fntype(&remove_slot_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_WALKSTACK, // needs WALKSTACK because it includes a define
 "Remove the designated slot from the receiver (the argument is a "
 "string, the name of the slot).  May fail due to lack of space (yes, really)."
},
{
"ResetFlatProfile", fntype(&ResetFlatProfile_prim),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Reset the flat profile counters in order to start new "
 "measurements."
},
{
"ResetProfile", fntype(&ResetProfile_prim),
 ExternalPrimitive, ReceiverPrimType,
 SAFE_SIDEEFFECTS,
 "Reset the profile counters in order to start new measurements."
},
{
"ResetSpyLog", fntype(&SelfMonitor::resetLog_prim),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "For internal consumption only."
},
{
"Restart", NULL,
 RestartPrimitive, UnknownPrimType,
 false, true, false, true, true, true,
 "Restart the current method, i.e., jump to the beginning of the "
 "method.  Used to implement looping in blocks."
},
{
"RunScript", fntype(&run_script_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS_CANABORT,
 "Read in the file containing a list of Self expressions and evaluate "
 "the expressions.  The receiver "
 "is a string naming the file to be read.  Returns the result of the "
 "last expression on the file."
},
{
"SamePointerAs:", fntype(&same_pointer_as_glue),
 ExternalPrimitive, BooleanPrimType,
 SIDEEFFECTS,
 "The receiver and argument must both be proxy or fctProxy "
 "objects.  Returns true iff they encapsulate the same pointers."
},
{
"Scavenge", fntype(&scavenge_prim),
 ExternalPrimitive, ReceiverPrimType,
 false, true, false, true, false, false,
 "Force a scavenge (a fast, partial garbage collection; see [Ung84],"
 "[Ung86], [Lee88])."
},
{
"SetCPUTimer", fntype(&setCPUTimer_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Similar to _SetRealTimer, but sets the CPU timer."
},
{
"SetRealTimer", fntype(&setRealTimer_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Set real-time interval timer.  The receiver is an integer denoting "
 "the number of milliseconds per interval."
},
{
"Size", fntype(&ov_size_prim),
 SizePrimitive, IntegerPrimType,
 NOSIDEEFFECTS,
 "Returns an integer, the number of elements in the receiver object "
 "vector."
},
{
"StackDepth", fntype(&StackDepth_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS_WALKSTACK,     // treat as side-effecting because it can't be const, need walking for Mac
 "Return the number of activations on the process' stack."
},
{
"StringCanonicalize", fntype(&string_canonicalize_prim_glue),
 ExternalPrimitive, StringPrimType,
  NOSIDEEFFECTS,
 "Return the canonical version of the receiver (a byte vector).  All "
 "byte vectors containing the same sequence of bytes map to the same "
 "canonical string object, and only one canonical string object with a "
 "particular sequence of bytes ever exists in the system.  Therefore, "
 "two canonical strings can be tested for equality efficiently using "
 "_Eq: rather than by comparing byte-by-byte.  All string literals are "
 "canonical strings."
},
{
"StringPrint", fntype(&string_print_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Print the characters of the receiver, a byte vector, on "
 "stdout.  Returns the receiver."
},
{
"TWAINSResultSize", fntype(&TWAINSResultSize_prim_glue),
 ExternalPrimitive, IntegerPrimType,
 SIDEEFFECTS_CANABORT,
 "Maximum size required of the second argument of "
 "_TWAINS:ResultVector:."
},
{
  "TestA1:A2:A3:A4:A5:A6:A7:A8:A9:A10:A11:A12:A13:A14:A15:A16:", 
  fntype(&test_args_prim),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS_WALKSTACK,
  "Takes a bunch of arguments and tests things."
},
{
"TWAINSResultVector:SingleStep:StopAt:", fntype(&TWAINS_prim_glue),
 ExternalPrimitive, StringPrimType,
 SIDEEFFECTS_CANABORT,
 "Transfer and wait for next signal.  The receiver is a process "
 "object to transfer to. \nThe first argument must be an object vector "
 "of size at least _TWAINSResultSize.  Control is "
 "transferred to the indicated process.  When control is transferred "
 "back the return value indicates the cause of the transfer: "
 "'aborted', 'stackOverflow', 'nonLifoBlock', 'yielded', 'lowOnSpace', "
 "'couldntAllocateStack' or 'signal'.  The result vector is used to provide "
 "additional information.  If the cause is 'signal', the first element of the "
 "result vector is modified to contain the number of signals and the "
 "rest of the result vector is modified to contain a list of the "
 "signals that accumulated in between returns from the primitive.  The "
 "possible signals are: 'sigint', 'sigquit', 'sighup', 'sigwinch', "
 "'sigio', 'siguser1', 'siguser2', 'sigpipe', 'sigterm', 'sigurg', "
 "'sigchild', 'sigrealtimer' and 'sigcputimer'.\nThe third argument "
 "must be either true or false and specifies whether the process is to "
 "execute in single-stepping mode.  If the argument is true, the "
 "process will execute at most one message send before returning with "
 "a value of 'singleStepped'.  (If a signal occurs before the send "
 "could be executed, i.e., if the return value is 'signal', no step was "
 "executed.)\nThe last argument is either nil or a \"stop\" "
 "activation.  In the latter case, the process will stop with a return "
 "value of 'finishedActivation' as soon as this activation finishes "
 "(the activation must be a \"live\" activation of the process).  Note "
 "that TWAINS may return before this occurs (e.g., because of a "
 "signal)."
},
{
"Tenure", fntype(&tenure_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 false, true, false, true, false, false,
 "Tenure all objects.  Force the memory management system to move all "
 "objects into the old heap.  Receiver is ignored."
},
{
"ThisProcess", fntype(&ThisProcess_prim),
 ExternalPrimitive, UnknownPrimType,
 SAFE_NONIDEMPOTENT,
 "Return process object of current process.  Ignores receiver."
},
{
"TimeCPU", fntype(&cpu_time_glue),
  ExternalPrimitive, IntegerPrimType,
  SAFE_SIDEEFFECTS,
 "Returns the total CPU time (in milliseconds) used by the Self system "
 "(identical to _TimeUser + _TimeSystem).  Ignores receiver."
},
{
"TimeReal", fntype(&real_time_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS,
 "Returns a 2-element vector representing the real time and date, "
 "measured from 00:00 UTC, Jan 1, 1970.  "
 "The first element is the integer number of days, and the second element "
 "is the integer number of milliseconds since midnight, UTC.  "
 "Ignores the receiver."
},
{
"TimeSystem", fntype(&system_time_glue),
  ExternalPrimitive, IntegerPrimType,
  SAFE_SIDEEFFECTS,
 "Returns the system time (in milliseconds) used by the Self "
 "system.  Ignores the receiver."
},
{
"ConvertToDayMs", fntype(&convert_to_day_ms_prim_glue),
  ExternalPrimitive, ObjVectorPrimType,
  SIDEEFFECTS, 
 "Receiver is a vector with 8 integers describing a time: "
 "year, month, date, weekday, hour, minute, second, daylight savings (-1, 0, +1). "
 "Returns a vector with two equivalent integers: days and milliseconds since "
 "1/1/70 0:00:00 (like the unix function mktime). This primitive is approximately "
 "the inverse of _DateTime."
},
{
"TimeUser", fntype(&user_time_glue),
  ExternalPrimitive, IntegerPrimType,
  SAFE_SIDEEFFECTS,
 "Returns the user time (in milliseconds) used by the Self "
 "system.  Ignores the receiver."
},
{
"TypeSealResultProxy:", fntype(&get_type_seal_glue),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Receiver is a proxy or fctProxy object.  Return proxy representing "
 "type seal of receiver."
},
{
"Verify", fntype(&verify_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 true, true, false, true, true, false,
 "Verify the integrity of the Self virtual machine."
},
{
"VerifyOptions", fntype(&verify_opts_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Select which parts of the system the _Verify primitive will check.  "
 "The receiver is a byte vector containing characters to enable specific checks:\n"
 "\te - verify eden\n"
 "\ts - verify survivor spaces\n"
 "\tn - verify new generation (implies 'e' and 's')\n"
 "\to - verify old generation\n"
 "\tz - verify zone\n"
 "\tp - verify processes\n"
 "\tr - verify remembered set\n"
 "\tS - verify string table\n"
 "\tv - verify VM strings\n"
 "\tO - verify VM oops\n"
 "\tm - verify VM maps\n"
 "\tN - verify new maps\n"
 "\tM - verify map table.\n"
 "\ti - verify block slot iterator.\n"
},
{
"VMversion", fntype(&VMversion_prim),
 ExternalPrimitive, UnknownPrimType,
 SIDEEFFECTS,
 "Returns a three-element vector containing the VM (major,minor,snapshot) "
 "version numbers."
},
{
"WriteSnapshot", fntype(&write_snapshot_prim_glue),
 ExternalPrimitive, ReceiverPrimType,
 SIDEEFFECTS,
 "Write a snapshot to the file named by the receiver, a byte vector.  "
 "Returns the receiver."
},
{
"Yield:", fntype(&Yield_prim),
 ExternalPrimitive, ReceiverPrimType,
  false, true, false, true, true, true,
 "Gives up the CPU.  Control is returned to the TWAINS process.  Ignores "
 "receiver."
},
{
"ZombieProcesses", fntype(&zombie_prim_glue),
 ExternalPrimitive, ObjVectorPrimType,
 SIDEEFFECTS,
 "Returns a vector of processes that were unreachable at the last "
 "garbage collection.  This is only guaranteed to be valid immediately "
 "after a GC.  Receiver is ignored."
},
# ifdef SIC_COMPILER
{
    "SICParamsAt:", fntype(&get_sic_params_prim_glue),
    ExternalPrimitive, ObjVectorPrimType,
    SIDEEFFECTS,
    "For internal consumption only"
},
{
    "SICParamsAt:Put:", fntype(&set_sic_params_prim_glue),
    ExternalPrimitive, ObjVectorPrimType,
    SIDEEFFECTS,
    "For internal consumption only"
},
#  ifndef NOASM
{
    "ITrace:", fntype(&itrace_prim),
    ExternalPrimitive, ReceiverPrimType,
    SIDEEFFECTS,
    "For internal consumption only"
},
#  endif
# endif

{
    "SpyHeight", fntype(&monitor_height_prim_glue),
    ExternalPrimitive, IntegerPrimType,
    true, false, false, false, false, false,
    "Return the height (in pixels) of the screen area used by the "
    "system monitor."
},


# ifdef DYNLINK_SUPPORTED
{
      "Dlopen:ResultProxy:", fntype(&dlopen_wrap_glue),
      ExternalPrimitive, UnknownPrimType,
      SIDEEFFECTS,
      "This primitive corresponds to the Sun OS call dlopen. It "
      "dynamically links in a shared object file.  The receiver is a "
      "byte vector, the path of the shared object, the argument is an "
      "integer.  The last argument is a proxy object.  Returns the "
      "result proxy object, representing the loaded shared object."
},
{
      "Dlsym:ResultProxy:", fntype(&dlsym_wrap_glue),
      ExternalPrimitive, UnknownPrimType, SIDEEFFECTS,
      "The primitive corresponds to the Sun OS call dlsym().  It looks "
      "up a symbol address in a shared object.  The receiver is a proxy "
      "representing the shared object, the argument is a byte vector, "
      "the name of the symbol.  Note that symbol names in object files "
      "are not always exactly the same as in source code.  For example, "
      "the C convention is to prepend a '_' to the name in the object "
      "file.  The last argument is a proxy object.  Returns the result "
      "proxy with the address of the symbol."
},
{
      "Dlclose", fntype(&dlclose_wrap_glue),
      ExternalPrimitive, IntegerPrimType,
      SIDEEFFECTS,
      "The primitive corresponds to the Sun OS call dlclose().  It "
      "unlinks a shared object.  The receiver is a proxy.  "
      "Returns 0 if successful."
},
{
      "FctLookup:ResultProxy:", fntype(&fctLookup_glue),
      ExternalPrimitive, UnknownPrimType,
      SIDEEFFECTS,
      "This primitive is similar to _Dlsym:ResultProxy:.  However, it "
      "looks up a function and returns a fctProxy rather than a "
      "proxy.  Warning: no attempt is made to ensure that the given "
      "name really refers to a function."
},
{
      "NoOfArgsFct:", fntype(&noOfArgsFct_glue),
      ExternalPrimitive, IntegerPrimType,
      SIDEEFFECTS,
      "The receiver is a proxy representing a shared object, the "
      "argument is the name of a function defined using glue code in "
      "the shared object.  Returns the number of arguments this "
      "function takes.  Note: the call will always return -1 for "
      "functions not defined using glue code."
},
# endif

# define  IN_PRIM_TABLE // makeDeps will include these files up top, too
# include "prim_table_arch.hh"
# include "prim_table_os.hh"
# include "prim_table_quartz.hh"
# undef   IN_PRIM_TABLE


#ifdef stat_entries
stat_entries
#endif

#ifdef termcap_entries
termcap_entries
#endif

#ifdef transcendental_entries
transcendental_entries
#endif

# if defined(XLIB) && defined(xlib_entries)
xlib_entries
#endif

LARGE_INT_PRIM_TABLE_ENTRIES

{
  // must be last entry in each prim table
  NULL, fntype(&bad_prim),
  ExternalPrimitive, UnknownPrimType,
  true, false, false, true, false, false,
  ""
}
  };

// two prim tables because cfront runs out of input buffer space
static PrimDesc fntable2[] = {

# ifdef XLIB
    // Xlib primitives that are actual X calls must have canAbortProcess and
    // canWalkStack (and therefore canScavenge) set to true, because an
    // X error can abort Xlib calls.  See xError.c. -- Bay
    // That is why we use canAWS in the primMaker templates. -- Dave
{
    "XOpenDisplayResultProxy:", fntype(&XOpenDisplay_glue),
    ExternalPrimitive, UnknownPrimType,
    SIDEEFFECTS_CANABORT,
    "The receiver is a string (the X display name that you want to open a "
    "connection on).  The argument is a dead proxy (xlib display)."
},
{
    "XCloseDisplay", fntype(&XCloseDisplay_glue),
    ExternalPrimitive, UnknownPrimType,
    SIDEEFFECTS_CANABORT,
    "The receiver is a proxy (xlib display that was returned by "
    "_XOpenDisplayResultProxy:)"
},
# endif

# if GCC
  // define debug prims last 'cause they mess up indentation
  # define DefineDebugPrim(                                                     \
    flagName, flagType, flagTypeName, primReturnType,                         \
    initialValue, getCurrentValue, checkNewValue, setNewValue,                \
    explanation, wizardOnly)                                                  \
                                                                              \
    {XSTR(flagName), fntype(&CONC3(get_,flagName,_prim)),                     \
    ExternalPrimitive, primReturnType,  SAFE_SIDEEFFECTS,                     \
    "Simple "  flagTypeName  " primitive: "  explanation },                   \
    {XSTR(flagName:), fntype(&CONC3(set_,flagName,_prim)),                    \
    ExternalPrimitive, primReturnType, SIDEEFFECTS,                           \
    "Simple "  flagTypeName  " primitive: "  explanation},                    
# else
  // define debug prims last 'cause they mess up indentation
  # define DefineDebugPrim(                                                     \
    flagName, flagType, flagTypeName, primReturnType,                         \
    initialValue, getCurrentValue, checkNewValue, setNewValue,                \
    explanation, wizardOnly)                                                  \
                                                                              \
    {XSTR(flagName), fntype(&CONC3(get_,flagName,_prim)),                     \
    ExternalPrimitive, primReturnType,                                        \
    SAFE_SIDEEFFECTS,                                                         \
    "Simple " ## flagTypeName ## " primitive: " ## explanation},              \
    {XSTR(CONC(flagName,:)), fntype(&CONC3(set_,flagName,_prim)),             \
    ExternalPrimitive, primReturnType, SIDEEFFECTS,                           \
    "Simple " ## flagTypeName ## " primitive: " ## explanation},
# endif

    FOR_ALL_DEBUG_PRIMS(DefineDebugPrim)
# undef DefineDebugPrim
          
  // must be last entry in each prim table
{
  NULL, fntype(&bad_prim),
  ExternalPrimitive, UnknownPrimType,
  SIDEEFFECTS,
  ""
}   
  };

static PrimDesc* fntable[] = { &fntable1[0], &fntable2[0], NULL };

PrimDesc** primDescTable() { return fntable; }

PrimDesc* getPrimDescOfSelector(stringOop selector, bool internal) {
  char* s = selector->bytes();
  fint l = selector->length();
  return getPrimDescOfBytes(s, l, internal);
}

PrimDesc* getPrimDescOfBytes(const char* s, fint len, bool internal) {
  assert(str_is_prim_name(s, len), "primitive doesn't begin with a '_'");
  s++, len--;
  PrimDesc* e;
  for (PrimDesc** ft = &fntable[0]; *ft; ft++) {
    for (e = *ft; e->name(); e++) {
      if (strncmp(s, e->name(), len) == 0 && e->name()[len] == '\0') {
        if (  (e->type() == InternalPrimitive && !internal)
            || e->type() == NotReallyAPrimitive) {
          goto error;
        } else {
          goto done;
        }
      }
    }
  }
 error: ;
  // set e to the NULL primitive entry (bad primitive)
  e = &fntable1[sizeof(fntable1)/sizeof(PrimDesc) - 1];
 done: ;
  e->verify();
  return e;
}

// for the run-time system
PrimDesc* getPrimDescOfFunction(fntype fn, bool internal) {
  return getPrimDescOfFirstInstruction(first_inst_addr((void*)fn), internal);
}

PrimDesc* getPrimDescOfFirstInstruction(char* fn_start_arg, bool internal) {
  // compensate for trapdoors:
  pc_t fn_start = Memory->code->trapdoors->follow_trapdoors(fn_start_arg);

  PrimDesc* e;
  for (PrimDesc** ft = &fntable[0]; *ft; ft++) {
    for (e = *ft; true; e++) {
      if ( e->fn() != NULL
      &&   fn_start == first_inst_addr((void*)e->fn())) {
        if (  (e->type() == InternalPrimitive && !internal)
            || e->type() == NotReallyAPrimitive) {
          return NULL;
        } else {
          // NB: matches even badPrim because check for e->name() is below
          return e;
        }
      }
      if (e->name() == NULL) break;        // end of table
    }
  }
  return NULL;
}

const char* getPrimName(char* fn_start) {
  PrimDesc* e;
  for (PrimDesc** ft = &fntable[0]; *ft; ft++) {
    for (e = *ft; e->name(); e++) {
      if (first_inst_addr((void*)e->fn()) == fn_start) return e->name();
    }
  }
  return first_inst_addr((void*)e->fn()) == fn_start  ? "badPrimitive" : "??";
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

int32 getPrimCallEndOffset(char* fn_start) {
  // offset of first instruction after prim call, measured from sendDesc ptr
  // (in bytes)
  
  // This is easy on some platforms:
  if ( sendDesc::abortable_prim_continue_offset == sendDesc::nonabortable_prim_continue_offset )
    return sendDesc::abortable_prim_continue_offset;
    
  PrimDesc* e;
  for (PrimDesc** ft = &fntable[0]; *ft; ft++) {
    for (e = *ft; true; e++) {
      if (e->fn() != NULL  &&  first_inst_addr((void*)e->fn()) == fn_start) {
        int32 off =
          e->canAbortProcess() ? sendDesc::abortable_prim_continue_offset 
                               : sendDesc::nonabortable_prim_continue_offset;
        return off;
      }
      if (e->name() == NULL) break;
    }
  }
  ShouldNotReachHere(); // primitive not found
  return 0;
}

# endif


# if COMPILER == gcc_pre_2_4_5
  // gcc (pre 2.4.5) has a weird code generation strategy and calls a global
  // initializer function to initialize fntable.  Since we don't want to use
  // the GNU linker, the initializer is called here explicitly.

  // The "reason" why gcc creates the initializer is the & of the functions
  // (i.e. &SendMessage_stub triggers it but SendMessage_stub doesn't).
  // To get rid of the
  // hack below, we could change all entries to omit the & (but then, gcc
  // complains about fntype(someClass::somefunc) so we'd have to get rid
  // of member functions).
  
  extern "C" void initializePrimDesc();
  
  void prim_init() { initializePrimDesc(); }
# else // compiler not gcc_pre_2_4_5
  void prim_init() { }
# endif

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "dynLink.hh"
# include "_dynLink.cpp.incl"
  
# ifdef DYNLINK_SUPPORTED

  void initDynLinker(int argc, const char *argv[]) {
    Unused(argv); Unused(argc);
  }
  
  void *dlopen_wrap(char *path, int mode, void *FH) {
    void *dlHandle = dlopen(path, mode);
    if (!dlHandle) 
      failure(FH, dlerror());    /* Indicate error return. */
    return dlHandle;
  }
  
  void dlclose_wrap(void *dlHandle, void *FH) {
    /* Warning: it is Self level responsibility to make sure that all 
      proxies and function proxies depending on the presence of a code file
        are killed when the code file is unloaded. */
    if (dlclose(dlHandle))
      failure(FH, dlerror());
  }
  
  void *dlsym_wrap(void *dlHandle, char *symName, void *FH) {
    void *sym = dlsym(dlHandle, symName);
    if (!sym)
      failure(FH, dlerror());
    return sym;
  }
  
  void *fctLookup(void *dlHandle, char *fctName, void *FH) {
    // This function is similar to dlsym. However, it returns a fctProxy, 
    // rather than just a proxy. 
    void *sym = dlsym(dlHandle, fctName);
    if (!sym)
      failure(FH, dlerror());
    return sym;
  }
  
  smi noOfArgsFct(void *dlHandle, char *fctName) {
    smi *argcP = (smi *)dlsym(dlHandle, nameOfArgc(fctName));
    if (argcP && *argcP >= 0 && *argcP <= 100) {
      /* Check that it is reasonable! */
      return *argcP;
    } else {
      return unknownNoOfArgs;
    }
  }
    
# endif    /* DYNLINK_SUPPORTED. */

# ifndef DYNLINK_SUPPORTED
    // dummy functions to enable static linking of Self
    extern "C" void* dlopen(char*, int) { return NULL; }
    extern "C" void* dlsym(void*, char*) { return NULL; }
    extern "C" int   dlclose(void*) { return -1; }
    extern "C" char* dlerror() { return "dynamic linking not available"; }
# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "glueSupport.hh"
# include "_glueSupport.cpp.incl"

oop check_glue_return(oop value) {
  // Called by debug version of glue functions. Simply verifies that
  // 'value' is a good oop to return. Must return 'value' or fail.
  if (!value->verify_oop_mark_ok())
    fatal("invalid primitive result");
  if (value->is_mark() && !value->memify()->is_string())
    fatal("primitive error value isn't a string");
  return value;
}



oop glue_conversion_error(smi err, const char* additional_msg) {
  // Called from glue code, when argument/result conversion error detected.
  // E.g. err == BADTYPEERROR, additional_msg == ": s_arg_0 should be int".
  ResourceMark rm;
  int err_len = VMString[err]->length();
  char*   str = NEW_RESOURCE_ARRAY( char, err_len + strlen(additional_msg) +1);
  memcpy(str, VMString[err]->bytes(), err_len);
  strcpy(str + err_len, additional_msg);
  return ErrorCodes::general_prim_error(str);
}


static inline failure_handle* indicate_failure(void* FH) {
  failure_handle* FH0 = (failure_handle* )FH;
  if (FH0->fh_magic != FH_MAGIC)
    fatal("failure function called with invalid failure handle");
  FH0->failed = true;
  return FH0;
}


extern "C" void failure(void* FH, const char* msg) {
  // This function is called from 'glued-in' functions to raise an exception.
  failure_handle* FH0 = indicate_failure(FH);
  FH0->msg = ErrorCodes::general_prim_error(msg);
}


extern "C" void unix_failure(void* FH, int err) {
  // Like failure, but interpretes err as a unix error no (if err has
  // value -1, the current value of errno is used instead).
  if (err == -1)
    err = errno;
  failure_handle* FH0 = indicate_failure(FH);
  FH0->msg = ErrorCodes::os_prim_error(err);
}


extern "C" void prim_failure(void* FH, VMStringsIndex err) {
  // Like failure, but constructs an error msg in the same way as ErrorCodes::vmString_prim_error.
  failure_handle* FH0 = indicate_failure(FH);
  FH0->msg = ErrorCodes::vmString_prim_error(err);
}


extern "C" void out_of_memory_failure(void* FH, int32 size, int32 bsize) {
  // Out of memory (could not allocate an object of size oops and bsize 
  // words in byte area
  failure_handle* FH0 = indicate_failure(FH);
  FH0->msg = ErrorCodes::vmString_prim_error(OUTOFMEMORYERROR);
  // need to do something clever with sizes --miw
  Unused(size); Unused(bsize);
}



char* nameOfArgc(const char* fctName) {
  // A crude way to determine the name of the variable which contains the argc
  // for the function with SYMBOL name fctName. This function should probably
  // be generalized, and rely on the C++ name mangler/demangler. 
  static const int maxFNLen = 240;
  static char  str[maxFNLen]; 
  assert(strlen(fctName) + strlen(XSTR(glue_fctname_extension)) < maxFNLen, 
         "Fct name shouldn't be this long");
  strcpy(str, fctName);
  strcat(str, XSTR(no_of_args_extension));
  return str;
}

/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "miscPrims.hh"
# include "_miscPrims.cpp.incl"

# ifndef  NO_LONG_LONG
  typedef          long long int64;
  typedef unsigned long long uint64;
# endif


smi bitSize(char *typeDesc, void *FH) {
  // typeDesc:  string describing a C type or Self primitive type.
  // Return the size in bits of this type.
  if (!strcmp(typeDesc,"self_int"))
    return BitsPerByte * sizeof(smi) - Tag_Size;
  if (!strcmp(typeDesc,"self_float"))
    return BitsPerByte * sizeof(float) - Tag_Size;

  # define GETSIZE(typename,type) {                                           \
    if (!strcmp(typename,typeDesc))                                           \
      return BitsPerByte * sizeof(type);                                      \
  }
  GETSIZE("char",        char)
  GETSIZE("short",       short)
  GETSIZE("int",         int)
  GETSIZE("long",        long)
# ifndef  NO_LONG_LONG
  GETSIZE("long long",   long long)
# endif
  GETSIZE("float",       float)
  GETSIZE("double",      double)
  GETSIZE("long double", long double)
  GETSIZE("void *",      void *)
  # undef GETSIZE
  prim_failure(FH, PRIMITIVEFAILEDERROR);
  return 0;
}


// *************** Storing/retrieving C types in byte vectors *****************

static inline bool moveToRaw(char *raw, char *bytes, smi len,
                             smi idx, smi size, void *FH) {
  // Check that idx and idx+size-1 are valid indices in a byte vector of
  // length len. Return true if all is OK, else set prim error and return
  // false.
  if (idx < 0 || idx + size > len) {
    prim_failure(FH, BADINDEXERROR);
    return false;
  }
  copy_bytes_up(bytes + idx, raw, size);
  return true;
}

static inline bool moveFromRaw(char *raw, char *bytes, smi len,
                               smi idx, smi size, void *FH) {
  // Check that idx and idx+size-1 are valid indices in a byte vector of
  // length len. Return true if all is OK, else set prim error and return
  // false.

  if (idx < 0 || idx + size > len) {
    prim_failure(FH, BADINDEXERROR);
    return false;
  }
  copy_bytes_up(raw, bytes + idx, size);
  return true;
}


double CFloatDouble_At_prim(char *bytes, int len,
                            bool dobble, smi idx, void *FH) {
  // Retrieve float from byte vector given by bytes, len.
  //    dobble:    whether to read a float or a double,
  //    idx:       where to find float in byte vector.

  if (dobble) {
    double f;
    if (moveToRaw((char *)&f, bytes, len, idx, sizeof(f), FH)) return f;
  } else {
    float f;
    if (moveToRaw((char *)&f, bytes, len, idx, sizeof(f), FH)) return f;
  }
  return 0.0;
}


void CFloatDouble_At_Put_prim(char *bytes, int len,
                              bool dobble, smi idx, double val, void *FH) {
  // Store float value into byte vector given by bytes, len.
  //    dobble:    whether to store as float or double,
  //    idx:       where to store in byte vector,
  //    val:       the value to store.

  if (dobble) {
    double f = val;
    moveFromRaw((char *)&f, bytes, len, idx, sizeof(double), FH);
  } else {
    float f = val;
    moveFromRaw((char *)&f, bytes, len, idx, sizeof(float), FH);
  }
}

#define GET_CASE(base, nn)                                                    \
  case nn: { CONC(base, nn) i;                                                \
             if (moveToRaw((char *)&i, bytes, len, idx, sizeof(i), FH))       \
               return i;                                                      \
             break;                                                           \
           }                                                                  \

# ifndef NO_LONG_LONG

  long long CSignedIntSize_At_prim(char *bytes, int len,
                              smi bitsize, smi idx, void *FH) {
    // Retrieve signed integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    switch(bitsize) {
      GET_CASE(int,  8)
      GET_CASE(int, 16)
      GET_CASE(int, 32)
#     ifndef NO_LONG_LONG
      GET_CASE(int, 64)
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }

  unsigned long long CUnsignedIntSize_At_prim(char *bytes, int len,
                                              smi bitsize, smi idx, void *FH) {
    // Retrieve unsigned integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    switch(bitsize) {
      GET_CASE(uint,  8)
      GET_CASE(uint, 16)
      GET_CASE(uint, 32)
#     ifndef NO_LONG_LONG
      GET_CASE(uint, 64)
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }

  #define PUT_CASE(base, nn)                                                  \
    case nn: { CONC(base, nn) i = val;                                        \
               moveFromRaw((char *)&i, bytes, len, idx, sizeof(i), FH);       \
               break;                                                         \
             }

  void CSignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize,
                                  smi idx, long long val, void *FH) {
    // Store signed integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch(bitsize) {
      PUT_CASE(int,  8)
      PUT_CASE(int, 16)
      PUT_CASE(int, 32)
#     ifndef NO_LONG_LONG
      PUT_CASE(int, 64)
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
  }


  void CUnsignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize, smi idx,
                                    unsigned long long val, void *FH) {
    // Store unsigned integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch(bitsize) {
      PUT_CASE(uint,  8)
      PUT_CASE(uint, 16)
      PUT_CASE(uint, 32)
#     ifndef NO_LONG_LONG
      PUT_CASE(uint, 64)
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
}


  long long BigEndianSignedIntSize_At_prim(char *sbp, int len,
                              smi bitsize, smi idx, void *FH) {
    // Retrieve signed integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    typedef long long r_t;
    unsigned char* ubp = (unsigned char*)sbp;
    switch(bitsize) {
      case  8:  return  (r_t)sbp[idx + 0];
      case 16:  return ((r_t)sbp[idx + 0] <<  8) | ((r_t)ubp[idx + 1]);
      case 32:  return ((r_t)sbp[idx + 0] << 24) | ((r_t)ubp[idx + 1] << 16) | ((r_t)ubp[idx + 2] <<  8) | ((r_t)ubp[idx + 3] <<  0);
#     ifndef NO_LONG_LONG
      case 64:  return ((r_t)sbp[idx + 0] << 56) | ((r_t)ubp[idx + 1] << 48) | ((r_t)ubp[idx + 2] << 40) | ((r_t)ubp[idx + 3] << 32)
                     | ((r_t)ubp[idx + 4] << 24) | ((r_t)ubp[idx + 5] << 16) | ((r_t)ubp[idx + 6] <<  8) | ((r_t)ubp[idx + 7] <<  0);
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }

  unsigned long long BigEndianUnsignedIntSize_At_prim(char *sbp, int len,
                                              smi bitsize, smi idx, void *FH) {
    // Retrieve unsigned integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    // Retrieve signed integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    typedef unsigned long long r_t;
    unsigned char* ubp = (unsigned char*)sbp;
    switch(bitsize) {
      case  8:  return  (r_t)ubp[idx + 0];
      case 16:  return ((r_t)ubp[idx + 0] <<  8) | ((r_t)ubp[idx + 1]);
      case 32:  return ((r_t)ubp[idx + 0] << 24) | ((r_t)ubp[idx + 1] << 16) | ((r_t)ubp[idx + 2] <<  8) | ((r_t)ubp[idx + 3] <<  0);
#     ifndef NO_LONG_LONG
      case 64:  return ((r_t)ubp[idx + 0] << 56) | ((r_t)ubp[idx + 1] << 48) | ((r_t)ubp[idx + 2] << 40) | ((r_t)ubp[idx + 3] << 32)
                     | ((r_t)ubp[idx + 4] << 24) | ((r_t)ubp[idx + 5] << 16) | ((r_t)ubp[idx + 6] <<  8) | ((r_t)ubp[idx + 7] <<  0);
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }


  void BigEndianSignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize,
                                  smi idx, long long val, void *FH) {
    // Store signed integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch(bitsize) {
#     ifndef NO_LONG_LONG
      case 64:  bytes[idx++] = char(val >> 56);
                bytes[idx++] = char(val >> 48);
                bytes[idx++] = char(val >> 40);
                bytes[idx++] = char(val >> 32);
#     endif
      case 32:  bytes[idx++] = char(val >> 24);
                bytes[idx++] = char(val >> 16);
      case 16:  bytes[idx++] = char(val >>  8);
      case  8:  bytes[idx++] = char(val >>  0);
                break;
      default: prim_failure(FH, BADSIZEERROR);
    }
  }


  void BigEndianUnsignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize, smi idx,
                                    unsigned long long val, void *FH) {
    // Store unsigned integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch(bitsize) {
#     ifndef NO_LONG_LONG
      case 64:  bytes[idx++] = char(val >> 56);
                bytes[idx++] = char(val >> 48);
                bytes[idx++] = char(val >> 40);
                bytes[idx++] = char(val >> 32);
#     endif
      case 32:  bytes[idx++] = char(val >> 24);
                bytes[idx++] = char(val >> 16);
      case 16:  bytes[idx++] = char(val >>  8);
      case  8:  bytes[idx++] = char(val >>  0);
                break;
      default: prim_failure(FH, BADSIZEERROR);
    }
}

  long long LittleEndianSignedIntSize_At_prim(char *sbp, int len,
                              smi bitsize, smi idx, void *FH) {
    // Retrieve signed integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    typedef long long r_t;
    unsigned char* ubp = (unsigned char*)sbp;
    switch(bitsize) {
      case  8:  return  (r_t)sbp[idx + 0];
      case 16:  return ((r_t)sbp[idx + 1] <<  8) | ((r_t)ubp[idx + 0]);
      case 32:  return ((r_t)sbp[idx + 3] << 24) | ((r_t)ubp[idx + 2] << 16) | ((r_t)ubp[idx + 1] <<  8) | ((r_t)ubp[idx + 0] <<  0);
#     ifndef NO_LONG_LONG
      case 64:  return ((r_t)sbp[idx + 7] << 56) | ((r_t)ubp[idx + 6] << 48) | ((r_t)ubp[idx + 5] << 40) | ((r_t)ubp[idx + 4] << 32)
                     | ((r_t)ubp[idx + 3] << 24) | ((r_t)ubp[idx + 2] << 16) | ((r_t)ubp[idx + 1] <<  8) | ((r_t)ubp[idx + 0] <<  0);
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }

  unsigned long long LittleEndianUnsignedIntSize_At_prim(char *sbp, int len,
                                              smi bitsize, smi idx, void *FH) {
    // Retrieve unsigned integer from byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to find integer in byte vector.
    typedef unsigned long long r_t;
    unsigned char* ubp = (unsigned char*)sbp;
    switch(bitsize) {
      case  8:  return  (r_t)ubp[idx + 0];
      case 16:  return ((r_t)ubp[idx + 1] <<  8) | ((r_t)ubp[idx + 0]);
      case 32:  return ((r_t)ubp[idx + 3] << 24) | ((r_t)ubp[idx + 2] << 16) | ((r_t)ubp[idx + 1] <<  8) | ((r_t)ubp[idx + 0] <<  0);
#     ifndef NO_LONG_LONG
      case 64:  return ((r_t)ubp[idx + 7] << 56) | ((r_t)ubp[idx + 6] << 48) | ((r_t)ubp[idx + 5] << 40) | ((r_t)ubp[idx + 4] << 32)
                     | ((r_t)ubp[idx + 3] << 24) | ((r_t)ubp[idx + 2] << 16) | ((r_t)ubp[idx + 1] <<  8) | ((r_t)ubp[idx + 0] <<  0);
#     endif
      default: prim_failure(FH, BADSIZEERROR);
    }
    return 0;
  }


  void LittleEndianSignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize,
                                  smi idx, long long val, void *FH) {
    // Store signed integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch (bitsize) {
      case 64: idx += 8;  break;
      case 32: idx += 4;  break;
      case 16: idx += 2;  break;
      case  8: idx += 1;  break;
      default: prim_failure(FH, BADSIZEERROR);
    }      
    switch(bitsize) {
#     ifndef NO_LONG_LONG
      case 64:  bytes[--idx] = char(val >> 56);
                bytes[--idx] = char(val >> 48);
                bytes[--idx] = char(val >> 40);
                bytes[--idx] = char(val >> 32);
#     endif
      case 32:  bytes[--idx] = char(val >> 24);
                bytes[--idx] = char(val >> 16);
      case 16:  bytes[--idx] = char(val >>  8);
      case  8:  bytes[--idx] = char(val >>  0);
    }
  }


  void LittleEndianUnsignedIntSize_At_Put_prim(char *bytes, int len, smi bitsize, smi idx,
                                    unsigned long long val, void *FH) {
    // Store unsigned integer in byte vector given by bytes, len.
    //    bitsize:   size of integer in bits,
    //    idx:       where to store in byte vector,
    //    val:       the integer value to store.
    switch (bitsize) {
      case 64: idx += 8;  break;
      case 32: idx += 4;  break;
      case 16: idx += 2;  break;
      case  8: idx += 1;  break;
      default: prim_failure(FH, BADSIZEERROR);
    }      
    switch(bitsize) {
#     ifndef NO_LONG_LONG
      case 64:  bytes[--idx] = char(val >> 56);
                bytes[--idx] = char(val >> 48);
                bytes[--idx] = char(val >> 40);
                bytes[--idx] = char(val >> 32);
#     endif
      case 32:  bytes[--idx] = char(val >> 24);
                bytes[--idx] = char(val >> 16);
      case 16:  bytes[--idx] = char(val >>  8);
      case  8:  bytes[--idx] = char(val >>  0);
  }
}


# endif

const int dateTimeBufSize = 18;

oop date_time_prim(smi day, smi msec) {
  smi buf[dateTimeBufSize];
  OS::date_time(day, msec, buf);
  objVectorOop res = Memory->objVectorObj->cloneSize(dateTimeBufSize);
  for(int i = 0; i < dateTimeBufSize; i++) {
    res->obj_at_put(i, as_smiOop(buf[i]), false);
  }
  return res;
}

oop convert_to_day_ms_prim(objVectorOop l, void *FH) {
  if (l->length() != 8) {
    prim_failure(FH, BADSIZEERROR);
    return NULL;
  }
  for (int i=0; i < 8; i++) {
    if (!l->obj_at(i)->is_smi()) {
      prim_failure(FH, BADTYPEERROR);
      return NULL;
    }
  }
  smi msAndDays[2];
  if (!OS::time_to_day_and_ms(l, msAndDays)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return NULL;
  }
  objVectorOop res = Memory->objVectorObj->cloneSize(2);
  res->obj_at_put(0, as_smiOop(msAndDays[0]), false);
  res->obj_at_put(1, as_smiOop(msAndDays[1]), false);
  return res;
}


oop real_time_prim(void *FH) {
  smi buf[2];
  OS::real_time(buf);
  if (as_smiOop(buf[0])->value() != buf[0] ||
      as_smiOop(buf[1])->value() != buf[1]) {
    prim_failure(FH, OVERFLOWERROR);
    return NULL;
  }
  objVectorOop res = Memory->objVectorObj->cloneSize(2);
  res->obj_at_put(0, as_smiOop(buf[0]), false);
  res->obj_at_put(1, as_smiOop(buf[1]), false);
  return res;
}

oop current_time_string_prim() {
  ResourceMark rm;
  return new_string(OS::current_time_string());
}


// Returns a vector containing all primitives names.
oop primitive_list_prim(void *FH) {
  ResourceMark rm;
  char prim_name[BUFSIZ];
  int num_of_prim  = 0;

  PrimDesc* e;
  PrimDesc** ft;
  for (ft = primDescTable(); *ft; ft++)
    for (e = *ft; e->name(); e++)
      if (e->type() != InternalPrimitive)
        num_of_prim++;

  objVectorOop result= Memory->objVectorObj->cloneSize(num_of_prim, CANFAIL);
  if (oop(result) == failedAllocationOop) {
    out_of_memory_failure(FH, Memory->objVectorObj->size() + num_of_prim);
    return NULL;
  }

  int index = 0;
  for (ft = primDescTable(); *ft; ft++)
    for (e = *ft; e->name(); e++)
      if (e->type() != InternalPrimitive) {
        strcpy(prim_name, "_");
        strcat(prim_name, e->name());
        oop string= oop(new_string_or_fail(prim_name));
        if (string == oop(failedAllocationOop)) {
          out_of_memory_failure(FH);
          return NULL;
        }
        result->obj_at_put(index++, string);
      }
  return result;
}

// Returns the documentation for a primitive
oop primitive_documentation(const char *prim_name) {
  if (!str_is_prim_name(prim_name)) return NULL;
  const char* name = &prim_name[1];
  PrimDesc* e;
  for (PrimDesc** ft = primDescTable(); *ft; ft++)
    for (e = *ft; e->name(); e++)
      if (e->type() != InternalPrimitive)
        if (strcmp(e->name(), name) == 0) {
          const char* doc = e->docString();
          return new_string(doc ? doc : "");
        }
  return NULL;
}

// Returns command line arguments
oop command_line_prim(void *FH) {
  objVectorOop result= Memory->objVectorObj->cloneSize(prog_argc, CANFAIL);
  if (oop(result) == failedAllocationOop) {
    out_of_memory_failure(FH, Memory->objVectorObj->size() + prog_argc);
    return NULL;
  }

  for (int index= 0; index < prog_argc; index++) {
    oop argi= new_string_or_fail((char*)prog_argv[index]);
    if (argi == failedAllocationOop) {
      out_of_memory_failure(FH);
      return NULL;
    }
    result->obj_at_put(index, argi);
  }
  return result;
}

// ***************** objVector and byteVector copying prims *****************

#define BV_OV_COPY(name, kind, check, copy_action)                            \
  oop name(oop dst, oop dstPos, oop src, oop srcPos, oop len) {               \
    if (!dst->CONC(is_, kind)() ||                                            \
        !src->CONC(is_, kind)() ||                                            \
        check                                                                 \
        !dstPos->is_smi()       ||                                            \
        !dst->CONC(is_, kind)() ||                                            \
        !srcPos->is_smi()       ||                                            \
        !len->is_smi()) return ErrorCodes::vmString_prim_error(BADTYPEERROR); \
    CONC(kind, Oop) dst0 = CONC(kind, Oop)(dst);                              \
    CONC(kind, Oop) src0 = CONC(kind, Oop)(src);                              \
    smi dstPos0 = smiOop(dstPos)->value();                                    \
    smi srcPos0 = smiOop(srcPos)->value();                                    \
    smi len0    = smiOop(len)->value();                                       \
    if (dstPos0 < 0 || srcPos0 < 0 || len0 < 0 ||                             \
        dstPos0 + len0 > dst0->length() ||                                    \
        srcPos0 + len0 > src0->length()) return ErrorCodes::vmString_prim_error(BADINDEXERROR); \
    copy_action                                                               \
    return dst;                                                               \
  }

BV_OV_COPY(copyRange_prim, objVector,,                                        \
  oop *from = src0->objs(srcPos0);                                            \
  oop *to   = dst0->objs(dstPos0);                                            \
  copy_oops_overlapping(from, to, len0);                                      \
  Memory->record_multistores(from, from + len0);                              \
)

BV_OV_COPY(copyByteRange_prim, byteVector, dst->is_string() ||,               \
  copy_bytes_overlapping(src0->bytes(srcPos0), dst0->bytes(dstPos0), len0);)


// ***************** Primitives for 32/64 bit integers *****************

#define CONC4(a,b,c,d) a##b##c##d

typedef class {public: int32 s[2];} two_int32s;

inline static void memorycopy(void *to, void *from, int size) {
  if (size == 4)
    *(int32 *)to = *(int32 *)from;
  else if (size == 8)
    /* Cannot transfer the value by casting to int64 -- aligment of
       Self byte vectors is insufficient, resulting in a bus error. */
    *(two_int32s *)to = *(two_int32s *)from;
  else
    fatal("unsupported size");
}

#define CONVERT_ARG(ctype, arg_in, arg_out)                                   \
  if (arg_in->is_smi()) {                                                     \
    *arg_out = (ctype)smiOop(arg_in)->value();                                \
  } else if (arg_in->is_byteVector()) {                                       \
    if (byteVectorOop(arg_in)->length() < sizeof(ctype))                      \
      return ErrorCodes::vmString_prim_error(BADINDEXERROR);                  \
    memorycopy(arg_out, byteVectorOop(arg_in)->bytes(), sizeof(ctype));       \
  } else                                                                      \
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);                     \

#define CONVERT_1_ARG(ctype)                                                  \
  static oop CONC(convert1arg_, ctype)(oop o1, ctype *a1) {                   \
    CONVERT_ARG(ctype, o1, a1)                                                \
    return NULL;                                                              \
  }

#define CONVERT_2_ARGS(ctype)                                                 \
  static oop CONC(convert2args_, ctype)                                       \
                         (oop o1, oop o2, ctype *a1, ctype *a2) {             \
    CONVERT_ARG(ctype, o1, a1)                                                \
    CONVERT_ARG(ctype, o2, a2)                                                \
    return NULL;                                                              \
  }

#define CONVERT_INTEGER_RESULT(ctype)                                         \
  oop CONC(convert_result_, ctype)(oop proto, ctype cres) {                   \
    smiOop smiRes = as_smiOop(cres);                                          \
    if (cres == smiRes->value()) return smiRes;                               \
    if (!proto->is_byteVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);             \
    byteVectorOop res = byteVectorOop(proto)->cloneSize(sizeof(ctype));       \
    memorycopy(res->bytes(), &cres, sizeof(ctype));                           \
    return res;                                                               \
  }

#define SMI_RESULT(ctype, exp)                                                \
  ctype cres = exp;                                                           \
  assert(cres == (ctype)as_smiOop(cres)->value(), "smi overflow");            \
  return as_smiOop(cres);

#define INT_RESULT(ctype, exp)                                                \
  return CONC(convert_result_, ctype)(proto, exp);

#define FLOAT_RESULT(ctype, exp)                                              \
  return as_floatOop((float)(exp));

#define CMP_RESULT                                                            \
  Unused(proto);                                                              \
  if (a1  > a2) return smiOop_one;                                            \
  if (a1 == a2) return smiOop_zero;                                           \
  return as_smiOop(-1);

#define INT_PRIM(name, ctype, exp, check)                                     \
  BINARY_PRIM(name, ctype, exp, check, INT_RESULT(ctype, exp))

#define CMP_PRIM(ctype)                                                       \
  BINARY_PRIM(cmp, ctype, , , CMP_RESULT)

#define DIV_0_CHECK if(a2 == 0) return ErrorCodes::vmString_prim_error(DIVISIONBYZEROERROR);
#define MASK32      (a2 & 0x1F)
#define MASK64      (a2 & 0x3F)
#define FAST_CONV   if(o1->is_smi()) return o1;

#define BINARY_PRIM(name, ctype, exp, check, result)                          \
  oop CONC4(ctype, _, name, _prim)(oop proto, oop o1, oop o2) {               \
    ctype a1, a2;                                                             \
    oop conv = CONC(convert2args_, ctype)(o1, o2, &a1, &a2);                  \
    if (conv) return conv;                                                    \
    check                                                                     \
    result                                                                    \
  }

#define CONVERSION_PRIM(to_ctype, from_ctype, fast, result)                   \
  oop CONC4(to_ctype, _from_, from_ctype, _prim)(oop proto, oop o1) {         \
    Unused(proto);                                                            \
    fast                                                                      \
    from_ctype a1;                                                            \
    oop conv = CONC(convert1arg_, from_ctype)(o1, &a1);                       \
    if (conv) return conv;                                                    \
    result(to_ctype, (to_ctype)a1)                                            \
  }

#define FLOAT_FROM_INT_CONVERSION(ctype)                                      \
  oop CONC3(float_from_, ctype, _prim)(oop o1) {                              \
    ctype a1;                                                                 \
    oop conv = CONC(convert1arg_, ctype)(o1, &a1);                            \
    if (conv) return conv;                                                    \
    FLOAT_RESULT(ctype, (float)a1)                                            \
  }

#define INT_FROM_FLOAT_CONVERSION(ctype)                                      \
  oop CONC(ctype, _from_float_prim)(oop proto, oop o1) {                      \
    if (!o1->is_float()) return ErrorCodes::vmString_prim_error(BADTYPEERROR); \
    INT_RESULT(ctype, (ctype)floatOop(o1)->value())                           \
  }

# ifdef NO_LONG_LONG
# define INT_PRIMS_LL(name, exp, check)
# else
# define INT_PRIMS_LL(name, exp, check)  INT_PRIM(name, int64, exp, check)
# endif

#define INT_PRIMS(name, exp, check)                                           \
  INT_PRIM(name, int32, exp, check)                                           \
  INT_PRIMS_LL(name, exp, check)


/* Instantiate the argument and result converter helper functions.

   Although it would be easier to just have macroes that does the
   conversions inline, we call functions for these conversions
   because they are needed in many places (having zillions of copies
   of code boxing/unboxing  ints takes up valuable memory, not to
   mention cache space. A quick measurement of performance indicates
   some 2-3% slower execution of a primitive like int64_add(0,0) when
   calling functions.
*/

CONVERT_1_ARG(int32)
CONVERT_2_ARGS(int32)
CONVERT_INTEGER_RESULT(int32)

# ifndef NO_LONG_LONG
  CONVERT_1_ARG(int64)
  CONVERT_2_ARGS(int64)
  CONVERT_INTEGER_RESULT(int64)
# endif

/* Now instantiate the primitives themselves. */

INT_PRIMS(add,  a1 + a2, )
INT_PRIMS(sub,  a1 - a2, )
INT_PRIMS(mul,  a1 * a2, )
INT_PRIMS(div,  a1 / a2, DIV_0_CHECK)
INT_PRIMS(rem,  a1 % a2, DIV_0_CHECK)
INT_PRIMS(and,  a1 & a2, )
INT_PRIMS(or,   a1 | a2, )
INT_PRIMS(xor,  a1 ^ a2, )

INT_PRIM(shl,  int32, a1 << MASK32, )
INT_PRIM(shr,  int32, a1 >> MASK32, )
INT_PRIM(ushr, int32, (uint32)a1 >> MASK32, )

# ifndef NO_LONG_LONG
  INT_PRIM(shl,  int64, a1 << MASK64, )
  INT_PRIM(shr,  int64, a1 >> MASK64, )
  INT_PRIM(ushr, int64, (uint64)a1 >> MASK64, )
# endif

CMP_PRIM(int32)
# ifndef NO_LONG_LONG
  CMP_PRIM(int64)
# endif

CONVERSION_PRIM(int8,  int32,          , SMI_RESULT)
CONVERSION_PRIM(int16, int32,          , SMI_RESULT)
# ifndef NO_LONG_LONG
  CONVERSION_PRIM(int64, int32, FAST_CONV, INT_RESULT)
  CONVERSION_PRIM(int8,  int64,          , SMI_RESULT)
  CONVERSION_PRIM(int16, int64,          , SMI_RESULT)
  CONVERSION_PRIM(int32, int64, FAST_CONV, INT_RESULT)
# endif

FLOAT_FROM_INT_CONVERSION(int32)
INT_FROM_FLOAT_CONVERSION(int32)

# ifndef NO_LONG_LONG
  INT_FROM_FLOAT_CONVERSION(int64)
  FLOAT_FROM_INT_CONVERSION(int64)
# endif


oop test_args_prim(oop rcvr, oop a1, oop a2, oop a3, oop a4, oop a5, oop a6, oop a7,
                   oop a8, oop a9, oop a10, oop a11, oop a12, oop a13, oop a14, oop a15, oop a16) {
  rcvr->verify();  rcvr->print();
  a1 ->verify();  a1 ->print();
  a2 ->verify();  a2 ->print();
  a3 ->verify();  a3 ->print();
  a4 ->verify();  a4 ->print();
  a5 ->verify();  a5 ->print();
  a6 ->verify();  a6 ->print();
  a7 ->verify();  a7 ->print();
  a8 ->verify();  a8 ->print();
  a9 ->verify();  a9 ->print();
  a10->verify();  a10->print();
  a11->verify();  a11->print();
  a12->verify();  a12->print();
  a13->verify();  a13->print();
  a14->verify();  a14->print();
  a15->verify();  a15->print();
  a16->verify();  a16->print();
  return rcvr;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "glue.hh"
# include "_glue.cpp.incl"
extern "C" {
  double ceil(double x);
  double floor(double x);
  double rint(double x);
}

const char *sealSeal = "Type seal for proxies representing a type seal.";
const char *TS_SO    = "Type seal for ld shared object handles.";
const char *TS_symb  = "Type seal for symbol addresses.";
const char *TS_func  = "Type seal for functions.";


# define WHAT_GLUE FUNCTIONS
    byteVector_glue
    objVector_glue
    foreign_glue
    misc_glue_1
    misc_glue_2
    process_glue
    timer_glue
    fctProxy_glue_1
    fctProxy_glue_2
    fctProxy_glue_3
    fctProxy_glue_4
    fctProxy_glue_5
    fctProxy_glue_6
    fctProxy_glue_7
    mirror_glue
    oop_glue
    slots_glue
    smi_glue
    float_glue
    profiler_glue
# ifdef DYNLINK_SUPPORTED
    dynLink_glue
# endif
# ifdef SIC_COMPILER
    sic_glue
# endif
# undef  WHAT_GLUE


/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "glueDefs.hh"
# include "_glueDefs.cpp.incl"

bool       xlib_semaphore = false;
bool     quartz_semaphore = false;
/* Sun-$Revision: 30.2 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "quartzPrims.hh"

# if defined(QUARTZ_LIB)

# if TARGET_OS_VERSION == MACOSX_VERSION
  #  undef ASSEMBLER
  #  undef Alloc
  
  #  include <Carbon/Carbon.h>
  #  include <ApplicationServices/ApplicationServices.h>

  // implicit by the above. 
  // #  include <CoreGraphics/CoreGraphics.h>

  // remove Carbon macros to avoid name collisions
  #  undef assert
  #  undef assert_type
  #  undef assert_smi
  #  undef assert_byteVector
  #  undef assert_objVector

  #  include "asserts.hh"
  
  #  undef verify
  #  undef check
  #  undef XLIB

# endif


# include "_quartzPrims.cpp.incl"

# ifndef DYNAMIC
  // Removed to avoid warnings in Tiger -- dmu 5/05
  // # include "include_glue.hh"
# endif


static oop byteVectorFromCFString(CFStringRef cfs, const char* primName, void* FH) {
  int len = CFStringGetLength(cfs);
  byteVectorOop r = Memory->byteVectorObj->cloneSize(len);
  CFIndex usedLen = -1;
  int nBytes = CFStringGetBytes(cfs, CFRangeMake(0, len), kCFStringEncodingMacRoman, 0, false, (UInt8*)r->bytes(), len, &usedLen);
  CFRelease(cfs);
  if (usedLen > len)
    fatal3("%s: used %d, max was %d", primName, usedLen, len);
  if (usedLen < len)  {
    failure(FH, "did not use full buffer");
    return NULL;
  }
  if (nBytes != len)  {
    failure(FH, "did not copy right amount");
    return NULL;
  }
  return r;
}


static void* reportOSError(OSStatus e, const char* n, void* FH)  {
    static char buf[1000];
    sprintf(buf, "%s failed: error %d", n, (int)e);
    failure(FH, buf);
    return NULL;
  }



CGLayer* CGLayerCreateWithContext_wrap(CGContextRef context, float w, float h) {
  return CGLayerCreateWithContext( context, CGSizeMake(w, h), NULL);
}


CGContextRef QDBeginCGContext_wrap( OpaqueGrafPtr* port, void* FH ) {
  CGContextRef c;
  OSStatus e = QDBeginCGContext(port, &c);
  return  e != noErr  ?  (CGContextRef)reportOSError(e, "QDBeginContext", FH)  : c;
}



void QDEndCGContext_wrap( OpaqueGrafPtr* port, CGContext* carg, void* FH ) {
  CGContextRef c = carg;
  OSStatus e = QDEndCGContext(port, &c);
  if ( e != noErr) reportOSError(e, "QDEndCGContext", FH);
}



ATSFontFamilyRef ATSFontFamilyFindFromName_wrap( const char* name ) {
  CFStringRef cfs = CFStringCreateWithCStringNoCopy( NULL, name, 
                     kCFStringEncodingMacRoman, kCFAllocatorNull);
  return ATSFontFamilyFindFromName( cfs, kNilOptions);
}


oop ATSFontFamilyGetName_wrap( ATSFontFamilyRef id, void* FH ) {
  CFStringRef cfs;
  OSStatus e = ATSFontFamilyGetName(id, NULL, &cfs);
  return  e != noErr  ?  (oop)reportOSError(e, "ATSFontFamilyGetName", FH)  : 
                         byteVectorFromCFString(cfs, "ATSFontFamilyGetName", FH);
}



ATSFontRef ATSFontFindFromName_wrap( const char* name ) {
  CFStringRef cfs = CFStringCreateWithCStringNoCopy( NULL, name, 
                     kCFStringEncodingMacRoman, kCFAllocatorNull);
  return ATSFontFindFromName( cfs, kNilOptions);
}


ATSFontRef ATSFontFindFromPostScriptName_wrap( const char* name ) {
  CFStringRef cfs = CFStringCreateWithCStringNoCopy( NULL, name, 
                     kCFStringEncodingMacRoman, kCFAllocatorNull);
  return ATSFontFindFromPostScriptName( cfs, kNilOptions);
}


oop ATSFontGetName_wrap( ATSFontRef id, void* FH ) {
  CFStringRef cfs;
  OSStatus e = ATSFontGetName(id, NULL, &cfs);
  return  e != noErr  ?  (oop)reportOSError(e, "ATSFontGetName", FH)  : 
                         byteVectorFromCFString(cfs, "ATSFontGetName", FH);
}

oop ATSFontGetPostScriptName_wrap( ATSFontRef id, void* FH ) {
  CFStringRef cfs;
  OSStatus e = ATSFontGetPostScriptName(id, NULL, &cfs);
  return  e != noErr  ?  (oop)reportOSError(e, "ATSFontGetPostScriptName", FH)  : 
                         byteVectorFromCFString(cfs, "ATSFontGetPostScriptName", FH);
}


ATSFontFamilyIterator ATSFontFamilyIteratorCreate_wrap( ATSFontContext iContext, ATSOptionFlags iOptions, void* FH ) {
    ATSFontFamilyIterator it;
    OSStatus e = ATSFontFamilyIteratorCreate( iContext, NULL, NULL, iOptions, &it);
  return  e != noErr  ?  (ATSFontFamilyIterator)reportOSError(e, "ATSFontFamilyIteratorCreate", FH)  :  it;
}


ATSFontFamilyRef ATSFontFamilyIteratorNext_wrap( ATSFontFamilyIterator it, void* FH) {
    ATSFontFamilyRef fam;
    OSStatus e = ATSFontFamilyIteratorNext( it, &fam);
  return  e != noErr  ?  (ATSFontFamilyRef)reportOSError(e, "ATSFontFamilyIteratorNext", FH)  :  fam;
}


OSStatus ATSFontFamilyIteratorReset_wrap( ATSFontFamilyIterator it, ATSFontContext iContext, ATSOptionFlags iOptions ) {
    ATSFontFamilyIterator itx = it;
    return ATSFontFamilyIteratorReset( iContext, NULL, NULL, iOptions, &itx);
}


OSStatus ATSFontFamilyIteratorRelease_wrap(ATSFontFamilyIterator it) {
  ATSFontFamilyIterator x = it;
  return ATSFontFamilyIteratorRelease(&x);
}




ATSFontIterator ATSFontIteratorCreate_wrap( ATSFontContext iContext, ATSOptionFlags iOptions, void* FH ) {
    ATSFontIterator it;
    OSStatus e = ATSFontIteratorCreate( iContext, NULL, NULL, iOptions, &it);
  return  e != noErr  ?  (ATSFontIterator)reportOSError(e, "ATSFontIteratorCreate", FH)  :  it;
}


ATSFontRef ATSFontIteratorNext_wrap( ATSFontIterator it, void* FH) {
    ATSFontRef fam;
    OSStatus e = ATSFontIteratorNext( it, &fam);
  return  e != noErr  ?  (ATSFontRef)reportOSError(e, "ATSFontIteratorNext", FH)  :  fam;
}


OSStatus ATSFontIteratorReset_wrap( ATSFontIterator it, ATSFontContext iContext, ATSOptionFlags iOptions ) {
    ATSFontIterator itx = it;
    return ATSFontIteratorReset( iContext, NULL, NULL, iOptions, &itx);
}


OSStatus ATSFontIteratorRelease_wrap(ATSFontIterator it) {
  ATSFontIterator x = it;
  return ATSFontIteratorRelease(&x);
}


oop ATSUGetGlyphBounds_wrap( ATSUTextLayout     iTextLayout, 
                             float              fTextBasePointX,
                             float              fTextBasePointY,
                             int                iBoundsCharStart,  // uint32
                             int                iBoundsCharLength, // uint32
                             uint16             iTypeOfBounds,
                             void*              FH ) {
  ATSUTextMeasurement iTextBasePointX = X2Fix(fTextBasePointX);
  ATSUTextMeasurement iTextBasePointY = X2Fix(fTextBasePointY);
  
  ATSTrapezoid b;
  ItemCount oActualNumberOfBounds;
  
  OSStatus e = ATSUGetGlyphBounds( 
    iTextLayout,
    iTextBasePointX,  iTextBasePointY,
    iBoundsCharStart, iBoundsCharLength,
    iTypeOfBounds,
    1, //   ItemCount iMaxNumberOfBounds,
    &b, &oActualNumberOfBounds);       /* can be NULL */ 

  if (e != noErr) {
      return  (oop)reportOSError(e, "ATSUGetGlyphBounds", FH);
  }
  if (oActualNumberOfBounds < 1) {
   static char buf[1000];
    sprintf(buf, "ATSUGetGlyphBounds failed: returned %d bounds instead of 1", (int)oActualNumberOfBounds);
    failure(FH, buf);
    return NULL;
  }
  float L = min((float)Fix2X(b.upperLeft.x), (float)Fix2X(b.lowerLeft.x ));
  float T = max((float)Fix2X(b.upperLeft.y), (float)Fix2X(b.upperRight.y));
  
  float R = min((float)Fix2X(b.upperRight.x), (float)Fix2X(b.lowerRight.x));
  float B = max((float)Fix2X(b.lowerLeft.y ), (float)Fix2X(b.lowerRight.y));

  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(L), false);
  r->obj_at_put(1, as_floatOop(T), false);
  r->obj_at_put(2, as_floatOop(R), false);
  r->obj_at_put(3, as_floatOop(B), false);
  return r;
}




oop ATSUGetUnjustifiedBounds_wrap( ATSUTextLayout     iTextLayout, 
                                   int                iLineStart,  // uint32
                                   int                iLineLength, // uint32
                                   void*              FH ) {
  ATSUTextMeasurement oTextBefore, oTextAfter, oAscent, oDescent;
  OSStatus e = ATSUGetUnjustifiedBounds( 
   iTextLayout, (uint32)iLineStart, (uint32)iLineLength, 
   &oTextBefore, &oTextAfter, &oAscent, &oDescent );
  if (e != noErr) {
      return  (oop)reportOSError(e, "ATSUGetUnjustifiedBounds", FH);
  }
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(Fix2X(oTextBefore)), false);
  r->obj_at_put(1, as_floatOop(Fix2X(oAscent)),     false);
  r->obj_at_put(2, as_floatOop(Fix2X(oTextAfter)),  false);
  r->obj_at_put(3, as_floatOop(Fix2X(oDescent)),    false);
  return r;
}


oop ATSUMeasureTextImage_wrap( ATSUTextLayout     iTextLayout, 
                               int                iLineStart,  // uint32
                               int                iLineLength, // uint32
                               void*              FH ) {
  ATSUTextMeasurement oTextBefore, oTextAfter, oAscent, oDescent;
  Rect rect;
  OSStatus e = ATSUMeasureTextImage( 
   iTextLayout, (uint32)iLineStart, (uint32)iLineLength, 
   0, 0, &rect);
  if (e != noErr) {
      return  (oop)reportOSError(e, "ATSUMeasureTextImage", FH);
  }
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_smiOop(rect.left), false);
  r->obj_at_put(1, as_smiOop(rect.top),  false);
  r->obj_at_put(2, as_smiOop(rect.right),     false);
  r->obj_at_put(3, as_smiOop(rect.bottom),    false);
  return r;
}


ATSUStyle ATSUCreateStyle_wrap(void* FH) {
  ATSUStyle style;
  OSStatus e = ATSUCreateStyle(&style);
  return  e != noErr  ?  (ATSUStyle)reportOSError(e, "ATSUCreateStyle", FH)  :  style;
}


ATSUStyle ATSUCreateAndCopyStyle_wrap(ATSUStyle s, void* FH) {
  ATSUStyle style;
  OSStatus e = ATSUCreateAndCopyStyle(s, &style);
  return  e != noErr  ?  (ATSUStyle)reportOSError(e, "ATSUCreateAndCopyStyle", FH)  :  style;
}



ATSUTextLayout ATSUCreateTextLayout_wrap(void* FH) {
  ATSUTextLayout layout;
  OSStatus e = ATSUCreateTextLayout(&layout);
  return  e != noErr  ?  (ATSUTextLayout)reportOSError(e, "ATSUCreateTextLayout", FH)  :  layout;
}


ATSUTextLayout ATSUCreateAndCopyTextLayout_wrap(ATSUTextLayout s, void* FH) {
  ATSUTextLayout layout;
  OSStatus e = ATSUCreateAndCopyTextLayout(s, &layout);
  return  e != noErr  ?  (ATSUTextLayout)reportOSError(e, "ATSUCreateAndCopyTextLayout", FH)  :  layout;
}



ATSUTextLayout ATSUCreateTextLayoutWithTextPtr_wrap(
  ATSUStyle     style,
  u_char* text,        int     len,
  int32  textOffset,   int32  textLength,
  void*   FH ) {
  
  ATSUTextLayout layout;
  ResourceMark rm;

  UniChar* unis = NEW_C_HEAP_ARRAY(UniChar, len);
  for (uint i = 0;  i < len;  ++i)
      unis[i] = UniChar(text[i]);  // hack to unicode
      
  OSStatus e = ATSUCreateTextLayoutWithTextPtr(
                 unis, (uint32)textOffset, (int32)textLength, len,
                 1,  (const UniCharCount*)&textLength,  &style,
                 &layout);
  return  e != noErr  ?  (ATSUTextLayout)reportOSError(e, "ATSUCreateTextLayoutWithTextPtr", FH)  :  layout;
}


void CGContextSelectFont_wrap(CGContext* c, const char* s, float siz) {
  return CGContextSelectFont(c, s, siz, kCGEncodingMacRoman);
}


CGFontRef CGFontCreateWithPlatformFont_wrap( uint32 fontRef ) {
  return CGFontCreateWithPlatformFont( (void*)&fontRef );
}


float CGLayerWidth_wrap(CGLayer* la) {
  return CGLayerGetSize(la).width;
}  

float CGLayerHeight_wrap(CGLayer* la) {
  return CGLayerGetSize(la).height;
}  


void CGContextDrawLayerInRect_wrap( CGContext *con, CGLayer* lay, float x, float y, float width, float height ) {
  CGContextDrawLayerInRect( con, CGRectMake(x, y, width, height), lay);
}


void CGContextDrawLayerAtPoint_wrap( CGContext *con, CGLayer* lay, float x, float y ) {
  CGContextDrawLayerAtPoint( con, CGPointMake(x, y), lay);
}

static bool convertFloatObjVector( objVectorOop v, const char* prinName, void* FH, float*& floats, uint32& count) {
  count = v->length();
  floats = NEW_RESOURCE_ARRAY(float, count);
  int badI = -1;
  for (int i = 0;  i < count;  ++i) {
    oop o = v->obj_at(i);
    floats[i] = o->is_float() ? floatOop(o)->value()
              : o->is_smi()   ? smiOop(o)->value()
              : ((badI = i), (i = count), 0.0);
  }
  if (badI != -1) {
    static char buf[1000];
    sprintf(buf, "%s failed: bad oop at: %d", prinName, badI);
    failure(FH, buf);
    return false;
  }
  return true;
}


void CGContextSetLineDash_wrap(CGContext* con, float phase, objVectorOop lengthsOop, void *FH ) {
  ResourceMark rm;
  uint32 count;
  float* lengths;
  if (convertFloatObjVector( lengthsOop, "CGContextSetLineDash", FH, lengths, count))
    CGContextSetLineDash( con, phase, lengths, count);
}


# define SIMPLE_RECT_WRAP(name) \
  void XCONC(name,_wrap) ( CGContext* con,  float x, float y, float width, float height ) { \
    name( con, CGRectMake(x, y, width, height)); \
  }
  


SIMPLE_RECT_WRAP(CGContextAddRect);
SIMPLE_RECT_WRAP(CGContextAddEllipseInRect);
SIMPLE_RECT_WRAP(CGContextFillRect);
SIMPLE_RECT_WRAP(CGContextStrokeRect);

void CGContextStrokeRectWithWidth_wrap( CGContext *con, float x, float y, float width, float height, float stroke_width) {
  CGContextStrokeRectWithWidth( con, CGRectMake(x, y, width, height), stroke_width);
}


SIMPLE_RECT_WRAP(CGContextClearRect);
SIMPLE_RECT_WRAP(CGContextFillEllipseInRect);
SIMPLE_RECT_WRAP(CGContextStrokeEllipseInRect);
SIMPLE_RECT_WRAP(CGContextClipToRect);

void CGContextSetFillColor_wrap(CGContext *con, float c1, float c2, float c3, float alpha) {
  float color[4] = {c1, c2, c3, alpha};
  CGContextSetFillColor(con, color);
}

void CGContextSetStrokeColor_wrap(CGContext *con, float c1, float c2, float c3, float alpha) {
  float color[4] = {c1, c2, c3, alpha};
  CGContextSetStrokeColor(con, color);
}

void CGContextSetFillPattern_wrap(CGContext *con, CGPatternRef p, float c1, float c2, float c3, float alpha) {
  float color[4] = {c1, c2, c3, alpha};
  CGContextSetFillPattern(con, p, color);
}

void CGContextSetStrokePattern_wrap(CGContext *con, CGPatternRef p, float c1, float c2, float c3, float alpha) {
  float color[4] = {c1, c2, c3, alpha};
  CGContextSetStrokePattern(con, p, color);
}

void CGContextSetPatternPhase_wrap(CGContext *con, float w, float h) {
  CGContextSetPatternPhase(con, CGSizeMake(w, h));
}

void CGContextSetRGBFillColor_wrap(CGContext *con, float r, float g, float b, float a) {
  CGContextSetRGBFillColor(con, r, g, b, a);
}

void CGContextSetRGBStrokeColor_wrap(CGContext *con, float r, float g, float b, float a) {
  CGContextSetRGBStrokeColor(con, r, g, b, a);
}

void CGContextDrawImage_wrap(CGContext *con, CGImage *im, float x, float y, float w, float h) {
  CGContextDrawImage(con, CGRectMake(x, y, w, h), im);
}

void CGContextSetShadowWithColor_wrap(CGContext *con, float x, float y, float blur, CGColor* c) {
  CGContextSetShadowWithColor(con, CGSizeMake(x, y), blur, c);
}

void CGContextSetShadowWithColor_wrap2(CGContext *con, float x, float y, float blur,
                                        float r, float g, float b, float a) {
  float comps[] = {r, g, b, a};
  CGColorSpaceRef cs = CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB);
  CGColorRef c = CGColorCreate( cs, comps );
  CGContextSetShadowWithColor(con, CGSizeMake(x, y), blur, c);
  CGColorRelease(c);
  CGColorSpaceRelease(cs);
}

void CGContextSetShadow_wrap(CGContext *con, float x, float y, float blur) {
  CGContextSetShadow(con, CGSizeMake(x, y), blur);
}

void CGContextSetLineCap_wrap(CGContext *con, int cap) {
  CGContextSetLineCap(con, CGLineCap(cap));
}
void CGContextSetLineJoin_wrap(CGContext *con, int j) {
  CGContextSetLineJoin(con, CGLineJoin(j));
}
void CGContextSetBlendMode_wrap(CGContext *con, int j) {
  CGContextSetBlendMode(con, CGBlendMode(j));
}
void CGContextSetTextDrawingMode_wrap(CGContext *con, int j) {
  CGContextSetTextDrawingMode(con, CGTextDrawingMode(j));
}
void CGContextDrawPath_wrap(CGContext *con, int j) {
  CGContextDrawPath(con, CGPathDrawingMode(j));
}
void CGContextSetInterpolationQuality_wrap(CGContext *con, int j) {
  CGContextSetInterpolationQuality(con, CGInterpolationQuality(j));
}

void CGContextSetTextMatrix_wrap(CGContext* con, float a, float b, float c, float d, float tx, float ty) {
    CGContextSetTextMatrix(con, CGAffineTransformMake(a, b, c, d, tx, ty));
}
void CGContextConcatCTM_wrap(CGContext* con, float a, float b, float c, float d, float tx, float ty) {
    CGContextConcatCTM(con, CGAffineTransformMake(a, b, c, d, tx, ty));
}
objVectorOop CGContextGetCTM_wrap(CGContext* con) {
  CGAffineTransform ctm = CGContextGetCTM(con);
  objVectorOop r = Memory->objVectorObj->cloneSize(6);
  r->obj_at_put(0, as_floatOop(ctm.a ), false);
  r->obj_at_put(1, as_floatOop(ctm.b ), false);
  r->obj_at_put(2, as_floatOop(ctm.c ), false);
  r->obj_at_put(3, as_floatOop(ctm.d ), false);
  r->obj_at_put(4, as_floatOop(ctm.tx), false);
  r->obj_at_put(5, as_floatOop(ctm.ty), false);
  return r;
}
objVectorOop CGContextGetTextMatrix_wrap(CGContext* con) {
  CGAffineTransform ctm = CGContextGetTextMatrix(con);
  objVectorOop r = Memory->objVectorObj->cloneSize(6);
  r->obj_at_put(0, as_floatOop(ctm.a ), false);
  r->obj_at_put(1, as_floatOop(ctm.b ), false);
  r->obj_at_put(2, as_floatOop(ctm.c ), false);
  r->obj_at_put(3, as_floatOop(ctm.d ), false);
  r->obj_at_put(4, as_floatOop(ctm.tx), false);
  r->obj_at_put(5, as_floatOop(ctm.ty), false);
  return r;
}


OSStatus ATSTextLayoutSetContext(ATSUTextLayout lay, CGContextRef con) {
  ATSUAttributeTag tag[1] = {kATSUCGContextTag};
  ByteCount       count[1] = {sizeof(con)};
  ATSUAttributeValuePtr val[1] = {con};
  return ATSUSetLayoutControls( lay, 1, tag, count, val);
}


OSStatus ATSUStyleSetFontIDAndSize(ATSUStyle iStyle, ATSUFontID fid, float fsize) {
  ATSUAttributeTag        tags[2] = {kATSUFontTag, kATSUSizeTag};
  Fixed ffsize = X2Fix(fsize);
  ByteCount             counts[2] = {sizeof(ATSUFontID*), sizeof(Fixed*)};
  ATSUAttributeValuePtr   vals[2] = {&fid, &ffsize};
  return ATSUSetAttributes( iStyle, 2, tags, counts, vals);
}

int32 GetWindowPortLeft  (WindowRef w) { Rect r; return GetWindowPortBounds(w, &r)->left;   }
int32 GetWindowPortRight (WindowRef w) { Rect r; return GetWindowPortBounds(w, &r)->right;  }
int32 GetWindowPortTop   (WindowRef w) { Rect r; return GetWindowPortBounds(w, &r)->top;    }
int32 GetWindowPortBottom(WindowRef w) { Rect r; return GetWindowPortBounds(w, &r)->bottom; }


float GetClipBoundingBoxX_wrap(    CGContext *c) { return CGContextGetClipBoundingBox(c).origin.x; }
float GetClipBoundingBoxY_wrap(    CGContext *c) { return CGContextGetClipBoundingBox(c).origin.y; }
float GetClipBoundingBoxWidth_wrap(CGContext *c) { return CGContextGetClipBoundingBox(c).size.width; }
float GetClipBoundingBoxHeight_wrap(CGContext *c) { return CGContextGetClipBoundingBox(c).size.height; }


CGImageRef CGImageCreate_wrap( float w, float h, 
                               uint32 bitsPerComponent, uint32 bitsPerPixel,
                               uint32 bytesPerRow, 
                               CGColorSpaceRef colorSpace,
                               uint32 bitmapInfo,
                               CGDataProviderRef provider,
                               objVectorOop decodeArrayOop,
                               bool shouldInterpolate,
                               uint32 colorRenderingIntent,
                               void* FH) {
  float* decodeArray; uint32 decodeLen;                                
  return
    convertFloatObjVector( decodeArrayOop, "CGImageCreate", FH, decodeArray, decodeLen)
      ?  CGImageCreate(w, h, bitsPerComponent, bitsPerPixel, bytesPerRow, colorSpace,
                       bitmapInfo, provider, decodeArray, shouldInterpolate,
                       CGColorRenderingIntent(colorRenderingIntent))
      : NULL;
}
            

CGImageRef CGImageCreate_wrap( float w, float h, 
                               uint32 bitsPerComponent, uint32 bitsPerPixel,
                               uint32 bytesPerRow, 
                               CGColorSpaceRef colorSpace,
                               uint32 bitmapInfo,
                               CGDataProviderRef provider,
                               bool shouldInterpolate,
                               uint32 colorRenderingIntent) {
  return CGImageCreate(w, h, bitsPerComponent, bitsPerPixel, bytesPerRow, colorSpace,
                       bitmapInfo, provider, NULL, shouldInterpolate,
                       CGColorRenderingIntent(colorRenderingIntent));;
}
            

CGImageRef CGImageMaskCreate_wrap( float w, float h, 
                               uint32 bitsPerComponent, uint32 bitsPerPixel,
                               uint32 bytesPerRow, 
                               CGDataProviderRef provider,
                               objVectorOop decodeArrayOop,
                               bool shouldInterpolate,
                               void* FH) {
  float* decodeArray; uint32 decodeLen;                                
  return
    convertFloatObjVector( decodeArrayOop, "CGImageCreateMask", FH, decodeArray, decodeLen)
      ?  CGImageMaskCreate(w, h, bitsPerComponent, bitsPerPixel, bytesPerRow,
                           provider, decodeArray, shouldInterpolate)
      : NULL;
}
            

CGImageRef CGImageMaskCreate_wrap( float w, float h, 
                               uint32 bitsPerComponent, uint32 bitsPerPixel,
                               uint32 bytesPerRow, 
                               CGDataProviderRef provider,
                               bool shouldInterpolate) {
  return CGImageMaskCreate(w, h, bitsPerComponent, bitsPerPixel, bytesPerRow,
                           provider, NULL, shouldInterpolate);
}
            

CGImageRef CGImageCreateWithMaskingColors_wrap(CGImageRef im, 
             float min1, float max1, float min2, float max2,
             float min3, float max3, float min4, float max4) {
  float comps[8] = {min1, max2, min2, max2, min3, max3, min4, max4};
  return CGImageCreateWithMaskingColors(im, comps);
}

CGImageRef CGImageCreateWithJPEGDataProvider_wrap(
   CGDataProviderRef source,
   objVectorOop decodeArrayOop,
   bool shouldInterpolate,
   uint32 colorRenderingIntent,
   void* FH) {
  float* decodeArray; uint32 decodeLen;                                
  return
    convertFloatObjVector( decodeArrayOop, "CGImageCreateWithJPEGDataProvider", FH,
                           decodeArray, decodeLen)
      ?  CGImageCreateWithJPEGDataProvider( source, decodeArray, shouldInterpolate, CGColorRenderingIntent(colorRenderingIntent))
      : NULL;
}


CGImageRef CGImageCreateWithJPEGDataProvider_wrap(
   CGDataProviderRef source,
   bool shouldInterpolate,
   uint32 colorRenderingIntent) {
  return CGImageCreateWithJPEGDataProvider( source, NULL, shouldInterpolate, CGColorRenderingIntent(colorRenderingIntent));
}

CGImageRef CGImageCreateWithPNGDataProvider_wrap(
   CGDataProviderRef source,
   objVectorOop decodeArrayOop,
   bool shouldInterpolate,
   uint32 colorRenderingIntent,
   void* FH) {
  float* decodeArray; uint32 decodeLen;                                
  return
    convertFloatObjVector( decodeArrayOop, "CGImageCreateWithPNGDataProvider", FH,
                           decodeArray, decodeLen)
      ?  CGImageCreateWithPNGDataProvider( source, decodeArray, shouldInterpolate, CGColorRenderingIntent(colorRenderingIntent))
      : NULL;
}

CGImageRef CGImageCreateWithPNGDataProvider_wrap(
   CGDataProviderRef source,
   bool shouldInterpolate,
   uint32 colorRenderingIntent) {
  return CGImageCreateWithPNGDataProvider( source, NULL, shouldInterpolate, CGColorRenderingIntent(colorRenderingIntent));
}
            

CGImageRef CGImageCreateWithImageInRect_wrap(CGImageRef im, float x, float y, float w, float h) {
  return CGImageCreateWithImageInRect( im, CGRectMake(x, y, w, h));
}


CGColorSpaceRef CGColorSpaceCreateIndexed_wrap(CGColorSpaceRef base, u_char* table, uint32 len ) {
  return CGColorSpaceCreateIndexed(base, len, table);
}

CGColorSpaceRef          CGColorSpaceCreateGenericGray_wrap() { return CGColorSpaceCreateWithName(kCGColorSpaceGenericGray); }

CGColorSpaceRef          CGColorSpaceCreateGenericRGB_wrap() { return CGColorSpaceCreateWithName(kCGColorSpaceGenericRGB); }

CGColorSpaceRef          CGColorSpaceCreateGenericCMYK_wrap() { return CGColorSpaceCreateWithName(kCGColorSpaceGenericCMYK); }


static void releaseData(void* info, const void* data, size_t size) {
  FreeHeap((void*)data);
}

CGDataProviderRef CGDataProviderCreateFromBV(u_char* bytes, uint32 len) {
    u_char* stable_bytes = NEW_C_HEAP_ARRAY( u_char, len );
    copy_bytes((const char*)bytes, (char*)stable_bytes, len);
    return CGDataProviderCreateWithData(NULL, stable_bytes, len, releaseData);
}

CGDataProviderRef CGDataProviderCreateFromURL_wrap(u_char* bytes, uint32 len) {
  return CGDataProviderCreateWithURL(CFURLCreateWithBytes( kCFAllocatorDefault, bytes, len, kCFStringEncodingMacRoman, NULL));
}

CGImageSourceRef CGImageSourceCreateWithURL_wrap(u_char* bytes, uint32 len) {
  return CGImageSourceCreateWithURL(CFURLCreateWithBytes( kCFAllocatorDefault, bytes, len, kCFStringEncodingMacRoman, NULL), NULL);
}

CGImageRef CGImageSourceCreateImageAtIndex_wrap(CGImageSourceRef is, uint32 index) {
  return CGImageSourceCreateImageAtIndex(is, index, NULL);
}

uint32 CGGetDisplayCount_wrap(float x, float y, float w, float h, void* FH) {
  CGDisplayCount count;
  CGDisplayErr e = 
    w == 0.0  &&  h == 0.0
      ? CGGetDisplaysWithPoint( CGPointMake(x, y), 0, NULL, &count)
      : CGGetDisplaysWithRect(  CGRectMake(x, y, w, h), 0, NULL, &count);
  return e != CGDisplayNoErr
    ? (uint32)reportOSError(e, "CGGetDisplayCount", FH)
    : count;
}


CGDirectDisplayID CGGetDisplayAt_wrap(uint32 n, float x, float y, float w, float h, void* FH) {
  uint32 count;
  ResourceMark rm;
  CGDirectDisplayID *displays = NEW_RESOURCE_ARRAY(CGDirectDisplayID, n + 1);
  CGDisplayErr e = 
    w == 0.0  &&  h == 0.0
      ? CGGetDisplaysWithPoint( CGPointMake(x, y),      n + 1, displays, &count)
      : CGGetDisplaysWithRect(  CGRectMake(x, y, w, h), n + 1, displays, &count);
  return  e != CGDisplayNoErr  ?  (CGDirectDisplayID)reportOSError(e, "CGGetDisplayAt", FH)
  :       count < n + 1        ?  (CGDirectDisplayID)(failure(FH, "not enough displays"), NULL)
  :                               displays[n];
}


uint32 CGGetActiveDisplayCount_wrap(void* FH) {
  uint32 count;
  CGDisplayErr e = CGGetActiveDisplayList( 0, NULL, &count);
  return e != CGDisplayNoErr
    ? (uint32)reportOSError(e, "CGGetActiveDisplayCount", FH)
    : count;
}

CGDirectDisplayID CGGetActiveDisplayAt_wrap(uint32 n, void* FH) {
  uint32 count;
  ResourceMark rm;
  CGDirectDisplayID *displays = NEW_RESOURCE_ARRAY(CGDirectDisplayID, n + 1);
  CGDisplayErr e =  CGGetActiveDisplayList( n + 1, displays, &count);
  return  e != CGDisplayNoErr  ?  (CGDirectDisplayID)reportOSError(e, "CGGetActiveDisplayAt", FH)
  :       count < n + 1        ?  (CGDirectDisplayID)(failure(FH, "not enough displays"), NULL)
  :                               displays[n];
}


uint32 CGGetOnlineDisplayCount_wrap(void* FH) {
  uint32 count;
  CGDisplayErr e = CGGetOnlineDisplayList( 0, NULL, &count);
  return e != CGDisplayNoErr
    ? (uint32)reportOSError(e, "CGGetOnlineDisplayCount", FH)
    : count;
}

CGDirectDisplayID CGGetOnlineDisplayAt_wrap(uint32 n, void* FH) {
  uint32 count;
  ResourceMark rm;
  CGDirectDisplayID *displays = NEW_RESOURCE_ARRAY(CGDirectDisplayID, n + 1);
  CGDisplayErr e =  CGGetOnlineDisplayList( n + 1, displays, &count);
  return  e != CGDisplayNoErr  ?  (CGDirectDisplayID)reportOSError(e, "CGGetOnlineDisplayAt", FH)
  :       count < n + 1        ?  (CGDirectDisplayID)(failure(FH, "not enough displays"), NULL)
  :                               displays[n];
}

float CGDirectDisplayX_wrap(CGDirectDisplayID d) { return CGDisplayBounds(d).origin.x; }
float CGDirectDisplayY_wrap(CGDirectDisplayID d) { return CGDisplayBounds(d).origin.y; }
float CGDirectDisplayWidth_wrap(CGDirectDisplayID d) { return CGDisplayBounds(d).size.width; }
float CGDirectDisplayHeight_wrap(CGDirectDisplayID d) { return CGDisplayBounds(d).size.height; }

CGDisplayErr CGDisplayMoveCursorToPoint_wrap(CGDirectDisplayID d, float x, float y) {
  return CGDisplayMoveCursorToPoint(d, CGPointMake(x, y));
}

int CGGetLastMouseDelta_x() {  int x, y;  CGGetLastMouseDelta(&x, &y); return x; }
int CGGetLastMouseDelta_y() {  int x, y;  CGGetLastMouseDelta(&x, &y); return y; }


OSStatus ATSUDisposeTextLayout_wrap(ATSUTextLayout lay) {
  void* oText;
  OSStatus e = ATSUGetTextLocation(lay, &oText, NULL, NULL, NULL, NULL);
  if (!e)  FreeHeap(oText);
  return ATSUDisposeTextLayout(lay);
}


oop GetWindowRegion_wrap( WindowRef w, uint16 reg, void* FH) {
  // 
  // TODO: GetWindoRegion is deprecated and not avialiable in >=10.7
  // For now, we emulate with HIWindowGetBounds
  //
  HIRect bounds;
  OSStatus err = HIWindowGetBounds((WindowRef)w, reg, 
                                   kHICoordSpace72DPIGlobal, &bounds);
  if (err) {
    return  (oop)reportOSError(err, "GetWindowRegion", FH);
  }

  objVectorOop r = Memory->objVectorObj->cloneSize(4);
//  r->obj_at_put(0, as_floatOop(CGRectGetMinX(bounds)), false);
//  r->obj_at_put(1, as_floatOop(CGRectGetMinY(bounds)), false);
//  r->obj_at_put(2, as_floatOop(CGRectGetMaxX(bounds)), false);
//  r->obj_at_put(3, as_floatOop(CGRectGetMaxY(bounds)), false);
  r->obj_at_put(0, as_smiOop((short)CGRectGetMinX(bounds)), false);
  r->obj_at_put(1, as_smiOop((short)CGRectGetMinY(bounds)), false);
  r->obj_at_put(2, as_smiOop((short)CGRectGetMaxX(bounds)), false);
  r->obj_at_put(3, as_smiOop((short)CGRectGetMaxY(bounds)), false);
  
  return r;
}


oop GetWindowStructureWidths_wrap( WindowRef w, void *FH) {
  Rect rect;
  OSStatus e = GetWindowStructureWidths(w, &rect);
  if (e != noErr) {
    return  (oop)reportOSError(e, "GetWindowStructureWidths", FH);
  }
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_smiOop(rect.left  ),  false);
  r->obj_at_put(1, as_smiOop(rect.top   ),  false);
  r->obj_at_put(2, as_smiOop(rect.right ),  false);
  r->obj_at_put(3, as_smiOop(rect.bottom),  false);
  
  return r;
}


oop GetEventClass_wrap(EventRef evt) {
  byteVectorOop r = Memory->byteVectorObj->cloneSize(4);
  *(uint32*)(r->bytes()) = EndianU32_NtoB(GetEventClass(evt));
  return r;
}

# define GET_EVENT_PARAM_SCALAR(typename) \
  typename XCONC(GetEventParam_,typename)(EventRef evt, uint32* name, uint32 name_len, \
                                           uint32* type, uint32 type_len, void* FH) { \
  if (name_len != 1) { failure(FH, "name length is not 1 32-bit int"); return 0; } \
  if (type_len != 1) { failure(FH, "type length is not 1 32-bit int"); return 0; } \
  typename r; \
  OSStatus e = GetEventParameter( evt, EndianU32_BtoN(*name), EndianU32_BtoN(*type), \
                                  NULL, sizeof(r), NULL, &r); \
  if (e == noErr)  return r; \
  reportOSError(e, "GetEventParamScalar", FH); \
  return 0; \
}


GET_EVENT_PARAM_SCALAR(uint32)
GET_EVENT_PARAM_SCALAR(int32)
GET_EVENT_PARAM_SCALAR(uint16)
GET_EVENT_PARAM_SCALAR(int16)
GET_EVENT_PARAM_SCALAR(uint8)
GET_EVENT_PARAM_SCALAR(Boolean)

GET_EVENT_PARAM_SCALAR(WindowRef)
GET_EVENT_PARAM_SCALAR(GrafPtr)
GET_EVENT_PARAM_SCALAR(EventRef)
GET_EVENT_PARAM_SCALAR(CGContextRef)

// GET_EVENT_PARAM_SCALAR(MenuRef)
// GET_EVENT_PARAM_SCALAR(ControlRef)
// GET_EVENT_PARAM_SCALAR(RgnHandle)
// GET_EVENT_PARAM_SCALAR(HIShapeRef)
// GET_EVENT_PARAM_SCALAR(GDHandle)


oop GetEventParam_CGPoint(EventRef evt, uint32* name, uint32 name_len, void* FH) {
  if (name_len != 1) failure(FH, "name length is not 1 32-bit int");
  HIPoint p;
  OSStatus e = GetEventParameter( evt, EndianU32_BtoN(*name), typeHIPoint,
                                  NULL, sizeof(p), NULL, &p);
  if (e != noErr)  return (oop)reportOSError(e, "GetEventParam_CGPoint", FH);
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.x), false);
  r->obj_at_put(1, as_floatOop(p.y), false);
  return r;
}


oop GetEventParam_CGSize(EventRef evt, uint32* name, uint32 name_len, void* FH) {
  if (name_len != 1) failure(FH, "name length is not 1 32-bit int");
  HISize p;
  OSStatus e = GetEventParameter( evt, EndianU32_BtoN(*name), typeHISize,
                                  NULL, sizeof(p), NULL, &p);
  if (e != noErr)  return (oop)reportOSError(e, "GetEventParam_CGSize", FH);
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.width), false);
  r->obj_at_put(1, as_floatOop(p.height), false);
  return r;
}


oop GetEventParam_CGRect(EventRef evt, uint32* name, uint32 name_len, void* FH) {
  if (name_len != 1) failure(FH, "name length is not 1 32-bit int");
  CGRect x;
  OSStatus e = GetEventParameter( evt, EndianU32_BtoN(*name), typeHISize,
                                  NULL, sizeof(x), NULL, &x);
  if (e != noErr)  return (oop)reportOSError(e, "GetEventParam_CGRect", FH);
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(x.origin.x), false);
  r->obj_at_put(1, as_floatOop(x.origin.y), false);
  r->obj_at_put(2, as_floatOop(x.size.width), false);
  r->obj_at_put(3, as_floatOop(x.size.height), false);
  return r;
}

OSStatus SetMouseCoalescingEnabled_wrap(bool newState) {
  return SetMouseCoalescingEnabled(newState, NULL);
}


oop CGContextGetPathCurrentPoint_wrap(CGContextRef c) {
  CGPoint p = CGContextGetPathCurrentPoint(c);
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.x), false);
  r->obj_at_put(1, as_floatOop(p.y), false);
  return r;
}


oop CGContextGetTextPosition_wrap(CGContextRef c) {
  CGPoint p = CGContextGetTextPosition(c);
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.x), false);
  r->obj_at_put(1, as_floatOop(p.y), false);
  return r;
}


oop CGContextGetPathBoundingBox_wrap(CGContextRef c) {
  CGRect x = CGContextGetPathBoundingBox(c);
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(x.origin.x), false);
  r->obj_at_put(1, as_floatOop(x.origin.y), false);
  r->obj_at_put(2, as_floatOop(x.size.width), false);
  r->obj_at_put(3, as_floatOop(x.size.height), false);
  return r;
}


bool CGContextPathContainsPoint_wrap(CGContextRef c, float x, float y, uint32 mode) {
  return CGContextPathContainsPoint(c, CGPointMake(x, y), (CGPathDrawingMode)mode);
}


void CGContextClipToMask_wrap(CGContextRef c, float x, float y, float w, float h, CGImageRef mask) {
  CGContextClipToMask(c, CGRectMake(x, y, w, h), mask);
}


void CGContextBeginTransparencyLayer_wrap(CGContextRef c) {
  CGContextBeginTransparencyLayer(c, NULL);
}  


oop CGContextGetUserSpaceToDeviceSpaceTransform_wrap(CGContextRef c) {
  CGAffineTransform ctm = CGContextGetUserSpaceToDeviceSpaceTransform(c);
  objVectorOop r = Memory->objVectorObj->cloneSize(6);
  r->obj_at_put(0, as_floatOop(ctm.a ), false);
  r->obj_at_put(1, as_floatOop(ctm.b ), false);
  r->obj_at_put(2, as_floatOop(ctm.c ), false);
  r->obj_at_put(3, as_floatOop(ctm.d ), false);
  r->obj_at_put(4, as_floatOop(ctm.tx), false);
  r->obj_at_put(5, as_floatOop(ctm.ty), false);
  return r;
}

oop CGContextConvertPointToDeviceSpace_wrap(CGContextRef c, float x, float y) {
  CGPoint p = CGContextConvertPointToDeviceSpace(c, CGPointMake(x, y));
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.x), false);
  r->obj_at_put(1, as_floatOop(p.y), false);
  return r;
}

oop CGContextConvertPointToUserSpace_wrap(CGContextRef c, float x, float y) {
  CGPoint p = CGContextConvertPointToUserSpace(c, CGPointMake(x, y));
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.x), false);
  r->obj_at_put(1, as_floatOop(p.y), false);
  return r;
}

oop CGContextConvertSizeToDeviceSpace_wrap(CGContextRef c, float x, float y) {
  CGSize p = CGContextConvertSizeToDeviceSpace(c, CGSizeMake(x, y));
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.width),  false);
  r->obj_at_put(1, as_floatOop(p.height), false);
  return r;
}

oop CGContextConvertSizeToUserSpace_wrap(CGContextRef c, float x, float y) {
  CGSize p = CGContextConvertSizeToUserSpace(c, CGSizeMake(x, y));
  objVectorOop r = Memory->objVectorObj->cloneSize(2);
  r->obj_at_put(0, as_floatOop(p.width),  false);
  r->obj_at_put(1, as_floatOop(p.height), false);
  return r;
}

oop CGContextConvertRectToDeviceSpace_wrap(CGContextRef c, float xin, float y, float w, float h) {
  CGRect x = CGContextConvertRectToDeviceSpace(c, CGRectMake(xin, y, w, h));
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(x.origin.x), false);
  r->obj_at_put(1, as_floatOop(x.origin.y), false);
  r->obj_at_put(2, as_floatOop(x.size.width), false);
  r->obj_at_put(3, as_floatOop(x.size.height), false);
  return r;
}

oop CGContextConvertRectToUserSpace_wrap(CGContextRef c, float xin, float y, float w, float h) {
  CGRect x = CGContextConvertRectToUserSpace(c, CGRectMake(xin, y, w, h));
  objVectorOop r = Memory->objVectorObj->cloneSize(4);
  r->obj_at_put(0, as_floatOop(x.origin.x), false);
  r->obj_at_put(1, as_floatOop(x.origin.y), false);
  r->obj_at_put(2, as_floatOop(x.size.width), false);
  r->obj_at_put(3, as_floatOop(x.size.height), false);
  return r;
}

void CGContextSetRenderingIntent_wrap(CGContextRef c, uint32 i) {
     CGContextSetRenderingIntent(c, CGColorRenderingIntent(i));
}

const char*       CGDirectDisplayID_seal = "CGDirectDisplayID";
const char*            QuartzWindow_seal = "QuartzWindow";
const char*                 CGLayer_seal = "CGLayer";
const char*               CGContext_seal = "CGContext";
const char*          ATSFontMetrics_seal = "ATSFontMetrics";
const char*  ATSFontFamilyIterator__seal = "ATSFontFamilyIterator";
const char*        ATSFontIterator__seal = "ATSFontIterator";
const char*    OpaqueATSUTextLayout_seal = "OpaqueATSUTextLayout";
const char*         OpaqueATSUStyle_seal = "OpaqueATSUStyle";
const char*                  CGFont_seal = "CGFont";
const char*                  CGPath_seal = "CGPath";
const char*                 CGColor_seal = "CGColor";
const char*            CGColorSpace_seal = "CGColorSpace";
const char*               CGPattern_seal = "CGPattern";
const char*                 CGImage_seal = "CGImage";
const char*           CGImageSource_seal = "CGImageSource";
const char*               CGShading_seal = "CGShading";
const char*          CGDataProvider_seal = "CGDataProvider";
const char*          OpaqueEventRef_seal = "OpaqueEventRef";
const char*           OpaqueGrafPtr_seal = "OpaqueGrafPtr";
const char*         OpaqueWindowPtr_seal = "OpaqueWindowPtr";

// const char*            WindowRef_seal = "WindowRef";

extern const char* EventRecord_seal;



# pragma warn_unusedarg off // glue, sigh

# define WHAT_GLUE FUNCTIONS
# undef  PRIMITIVE_GLUE_FLAG_CODE
# define PRIMITIVE_GLUE_FLAG_CODE BlockGlueFlag gf(quartz_semaphore);

  quartz1_glue

# undef  WHAT_GLUE

# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "glueCheckSum.hh"

# include "_glueCheckSum.cpp.incl"

fint glue_checksum = GLUE_CHECKSUM;
/* Sun-$Revision: 30.18 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "universe.hh"
# pragma implementation  "universe_inline.hh"

# include "_universe.cpp.incl"


// increment VM_snapshot_version whenever old snapshots will break; reset
// it to zero when changing the minor or major version
smi VM_major_version    = 2017;
smi VM_minor_version    = 1;
smi VM_snapshot_version = 13;


universe* Memory;

bool NeedScavenge     = false;
bool bootstrapping    = true;
bool postReadSnapshot = true;
char *WorldName       = NULL;
bool compressed_snapshot;
char decompression_filter[80];
bool page_aligned;

mapOop universe:: true_mapOop() { return  true_map()->enclosing_mapOop(); }
mapOop universe::false_mapOop() { return false_map()->enclosing_mapOop(); }


universe::universe() {
  // must set Memory here because other methods refer to it
  Memory= this;

  major_version= VM_major_version;
  minor_version= VM_minor_version;
  snapshot_version= VM_snapshot_version;
  is_snapshot_other_endian= false;

  default_sizes.set_from_defaults();
  scavengeCount= 0;
  Desired_Surv_Size= 400 * K;
  

  FILE *snap;
  if (WorldName) {
    snap= Files->openSnapshotFile(WorldName, "r");
    if (!snap) {
      fatalNoMenu1("Can't read initial world from %s!\n\n", WorldName);
    }
    read_snapshot_header(snap);
  } else
    current_sizes= default_sizes;

  current_sizes.cleanup();

  map_table= new mapTable;
  
  string_table = new stringTable;

  code= new zone(current_sizes.code_size,
                 current_sizes.pic_size,
                 current_sizes.deps_size,
                 current_sizes.debug_size);
  age_table = new ageTable;

  object_table = NULL;

  verify_opts= OS::strdup("nozprSvOmNMi");

  if (WorldName)
    read_snapshot(snap);
  else
    genesis();
}

// Create the world (should take less than 6 days)
void universe::genesis()
{
  ResourceMark rm;

  programming_timestamp = smiOop_zero;

  new_gen= new newGeneration(current_sizes.eden_size,
                             current_sizes.surv_size);

  old_gen= new oldGeneration(current_sizes.old_size, new_gen->capacity());

  assert(new_gen->high_boundary <= old_gen->low_boundary,
         "old space allocated lower than new space!");

  remembered_set = new rSet; // uses _boundary's
  
  // create map_map
  objectAnnotationObj = smiOop_zero; // so mapMap.annotation is a vaild oop
  map_map = mapMap::create_mapMap();
  
  // create annotation objects; works cause these have no slots
    slotAnnotationObj= create_slots((slotList*)NULL);
  objectAnnotationObj= create_slots((slotList*)NULL);

  // they have the wrong object annotation; fix it
  objectAnnotationObj =
    objectAnnotationObj->mirror_copy_set_annotation(objectAnnotationObj, true);
  slotAnnotationObj = 
    slotAnnotationObj->mirror_copy_set_annotation(objectAnnotationObj, true);
  
  // create lobby
  lobbyObj= create_slots((slotList*)NULL);
  
  // create initial strings, string canonicalization table, and vm string array
  slotsOop stringParent= create_slots((slotList*)NULL);
  create_initial_strings(stringParent);
  stringParent = (slotsOop)
    stringParent->copy_add_slot(VMString[PARENT],
                                parent_map_slotType, 
                                lobbyObj,
                                Memory->slotAnnotationObj,
                                true);

  assert(stringParent->is_slots(), "add_slot failed");

  // slot list for empty objects that are children of the root
  slotList* slots = new slotList(VMString[PARENT],
                                 parent_map_slotType,
                                 lobbyObj);
  
  // create vectors
   objVectorObj = create_objVector (create_slots(slots));
  byteVectorObj = create_byteVector(create_slots(slots));
  
  // create nil object
  nilObj = create_slots(slots);
  
  // create boolean objects
  trueObj = create_slots(slots);
  falseObj = create_slots(slots);
  
  // create maps
    smi_map  =   smiMap::create_smiMap  (create_slots(slots));
  float_map  = floatMap::create_floatMap(create_slots(slots));
   mark_map  =  markMap::create_markMap();
  
  // create block traits
  blockTraitsObj = create_slots(slots);

  // create literalsObj
  literalsObj = Memory->objVectorObj->clone();

  // create dead block (for initialization of memoized blocks)
  ByteCode db(true);
  db.GenLiteralByteCode(0, 0, new_string("internal error: memoized block"));
  db.GenSendByteCode(0, 0, new_string("error:"), true, false, NULL);
  bool ok = db.Finish("<internal>", " \"prototype dead block method (internal)\" ");
  assert(ok, "just checkin");
  slotsOop deadBlockMethod = create_blockMethod(NULL, &db);
  deadBlockObj = create_block(deadBlockMethod);

  // create vframe prototypes
  ByteCode b(true);
  b.GenSelfByteCode(0, 0);
  ok = b.Finish("<predefined>", " \"prototype method\" self ");
  assert(ok, "just checkin");
  slotsOop outerMethod = create_outerMethod(NULL, &b);
  slotsOop blockMethod = create_blockMethod(NULL, &b);
  outerActivationObj = create_vframeOop(outerMethod);
  blockActivationObj = create_vframeOop(blockMethod);
  
  // create process object
  processObj = create_process(0);

  // create profiler object
  profilerObj = profilerMap::create_profiler();

  // create proxy objects
     proxyObj  =    proxyMap::create_proxy();
  fctProxyObj  = fctProxyMap::create_fctProxy();
  
  // create assignment object
  assignmentObj = assignmentMap::create_assignment();

  // create dummy error object
  errorObj= create_slots(slots);

  // create mirror objects and associated objects
       assignmentMirrorObj = mirrorMap::create_mirror(assignmentObj);
            blockMirrorObj = mirrorMap::create_mirror(create_block(blockMethod));
       byteVectorMirrorObj = mirrorMap::create_mirror(byteVectorObj);
      outerMethodMirrorObj = mirrorMap::create_mirror(outerMethod);
      blockMethodMirrorObj = mirrorMap::create_mirror(blockMethod);
            floatMirrorObj = mirrorMap::create_mirror(as_floatOop(0.0));
        objVectorMirrorObj = mirrorMap::create_mirror(objVectorObj);
            slotsMirrorObj = mirrorMap::create_mirror();
              smiMirrorObj = mirrorMap::create_mirror(as_smiOop(0));
          processMirrorObj = mirrorMap::create_mirror(processObj);
  outerActivationMirrorObj = mirrorMap::create_mirror(outerActivationObj);
  blockActivationMirrorObj = mirrorMap::create_mirror(blockActivationObj);
           stringMirrorObj = mirrorMap::create_mirror(stringObj);
            proxyMirrorObj = mirrorMap::create_mirror(proxyObj);
         fctProxyMirrorObj = mirrorMap::create_mirror(fctProxyObj);
         profilerMirrorObj = mirrorMap::create_mirror(profilerObj);
           mirrorMirrorObj = mirrorMap::create_mirror(slotsMirrorObj);
  
  // create systemObjects
  slots = new slotList(new_string("nil"), map_slotType, nilObj);
  slots = slots->add(new_string("true"), map_slotType, trueObj);
  slots = slots->add(new_string("false"), map_slotType, falseObj);
  slots = slots->add(new_string("vector"), map_slotType, objVectorObj);
  slots = slots->add(new_string("byteVector"), map_slotType, byteVectorObj);
  slots = slots->add(new_string("proxy"), map_slotType, proxyObj);
  slots = slots->add(new_string("fctProxy"), map_slotType, fctProxyObj);
  slots = slots->add(new_string("methodActivationMirror"), 
                     map_slotType,
                     outerActivationMirrorObj);
  slots = slots->add(new_string("blockMethodActivationMirror"), 
                     map_slotType,
                     blockActivationMirrorObj);
  slots = slots->add(new_string("assignmentMirror"), 
                     map_slotType,
                     assignmentMirrorObj);
  slots = slots->add(new_string("blockMirror"), map_slotType, blockMirrorObj);
  slots = slots->add(new_string("byteVectorMirror"),
                     map_slotType,
                     byteVectorMirrorObj);
  slots = slots->add(new_string("methodMirror"),
                     map_slotType,
                     outerMethodMirrorObj);
  slots = slots->add(new_string("blockMethodMirror"),
                     map_slotType, blockMethodMirrorObj);
  slots = slots->add(new_string("floatMirror"), map_slotType, floatMirrorObj);
  slots = slots->add(new_string("vectorMirror"),
                     map_slotType,
                     objVectorMirrorObj);
  slots = slots->add(new_string("mirrorMirror"),
                     map_slotType,
                     mirrorMirrorObj);
  slots = slots->add(new_string("slotsMirror"), map_slotType, slotsMirrorObj);
  slots = slots->add(new_string("proxyMirror"), map_slotType, proxyMirrorObj);
  slots = slots->add(new_string("fctProxyMirror"), 
                     map_slotType,
                     fctProxyMirrorObj);
  slots = slots->add(new_string("smiMirror"), map_slotType, smiMirrorObj);
  slots = slots->add(new_string("stringMirror"),
                     map_slotType,
                     stringMirrorObj);
  slots = slots->add(new_string("processMirror"),
                     map_slotType,
                     processMirrorObj);
  slots = slots->add(new_string("profiler"), map_slotType, profilerObj);
  slots = slots->add(new_string("profilerMirror"),
                     map_slotType,
                     profilerMirrorObj);
  slots = slots->add(new_string("slotAnnotation"),
                     map_slotType,
                     slotAnnotationObj);
  slots = slots->add(new_string("objectAnnotation"),
                     map_slotType,
                     objectAnnotationObj);
  slotsOop systemObjects = create_slots(slots);
  
  slots = new slotList(new_string("postRead"), map_slotType, nilObj);
  slotsOop snapshotAction = create_slots(slots);

  // Create the shell object with parent slot lobbyObj.
  // This object should be used as the receiver object for the prompt.
  slots = new slotList(VMString[PARENT], parent_map_slotType, lobbyObj);
  slotsOop shellObj = create_slots(slots);

  // create object ID array for stack dumps etc
  objectIDArray= objVectorObj->cloneSize(NumObjectIDs);
  
  // add slots to lobby
  oop lob = lobbyObj;
  lob = lob->copy_add_slot(new_string("systemObjects"),
                           parent_map_slotType,
                           systemObjects, 
                           Memory->slotAnnotationObj,
                           true);
  lob = lob->copy_add_slot(new_string("snapshotAction"),
                           map_slotType,
                           snapshotAction,
                           Memory->slotAnnotationObj,
                           true);
  lob = lob->copy_add_slot(new_string("shell"),
                           map_slotType,
                           shellObj,
                           Memory->slotAnnotationObj,
                           true);
  lob = lob->copy_add_slot(new_string("help"),
                           map_slotType,
                           new_string("\n"
   "\tTo begin using Self, you must read in the world of Self objects.\n"
   "\tTo read in the world, type:\n\n"
   "\t\t'worldBuilder.self' _RunScript\n\n"
   "\tWhen this process is complete, you will be at the Self prompt.\n"
   "\tAt the Self prompt, you can start the user interface by typing:\n\n"
   "\t\tdesktop open\n\n\n"),
                           Memory->slotAnnotationObj,
                           true);

  lobbyObj->define(lob); // assigns to the lobbyObj global var

  tenuring_threshold = age_table_size;  // don't tenure anything at first
  
  postReadSnapshot= false;

  lprintf("Self Virtual Machine Version %d.%d/%d, %s\n", 
         major_version, minor_version, snapshot_version, vmDate);
  lprintf("Copyright 1989-2016 AUTHORS (type _Credits for credits)\n\n");
}


void universe::switch_pointers(oop from, oop to) {
  assert(from->is_mem() && to->is_mem(),
         "unexpected kind of pointer switching");
  assert(!from->is_old() || to->is_old(),
         "shouldn't be switching an old oop to a new oop");
  APPLY_TO_VM_OOPS(SWITCH_POINTERS_TEMPLATE);
  new_gen->switch_pointers(from, to);
  old_gen->switch_pointers(from, to);
  code->switch_pointers(from, to);
  hprofiler->switch_pointers(from, to);
  profilers->switch_pointers(from, to);
  processes->switch_pointers(from, to);
  string_table->switch_pointers(from, to);
  VMStrings_switch_pointers(from, to);
  slotIterator_switch_pointers(from, to);
  APPLY_TO_VM_MAPS(MAP_SWITCH_POINTERS_TEMPLATE);
}


#define SPACE_SIZE_READ_TEMPLATE(s)                             \
        if (fscanf(file, STR(s) ": %d\n", &(current_sizes.s)) != 1)   \
          fatal1("Snapshot format error: %s", STR(s));

// leave a space for a - between the $0 and $@
static const char SNAPSHOT_HEADER[] = "exec Self -s $0   $@\n";
#define HEADER_PREFIX_LEN 9

bool okToUseCodeFromSnapshot= true;
bool SnapshotCode;
bool noCodeWarnings= true;

void noCodeWarning(const char *msg)
{
  okToUseCodeFromSnapshot= false;
  if (noCodeWarnings) return;
  warning("Optimized code saved in this snapshot cannot be used,");
  lprintf("\t%s.\n", msg);
  lprintf("\tUntil more optimized code is automatically generated,\n"
          "\tthis snapshot will run more slowly than it did at the\n"
          "\ttime it was saved.\n");
}


void universe::read_first_line_in_snapshot_header(FILE *file)
{
  char header[sizeof(SNAPSHOT_HEADER)];
  // cannot use fgets anymore because \n and \r are switched on Mac
  // if (fgets(header, sizeof(header), file) == NULL)
  //  fatalNoMenu("Could not read snapshot file header");
  for (fint i = 0;  i  <  sizeof(header) - 1; ++i ) {
    if (fread(header + i, 1, 1, file) != 1)
      break;
    if ( header[i] == '\n' || header[i] == '\r' ) {
      header[i+1] = '\0';
      break;
    }
  }
  // Compare only the start so that the line can be edited to ignore 
  // command-line VM flags (ie exec Self -s $0 - $@) --miw
  if (strncmp(SNAPSHOT_HEADER, header, HEADER_PREFIX_LEN))
    fatalNoMenu2("Not a valid Self snapshot; header should be %s but was %s",
                 SNAPSHOT_HEADER, header);
}


void universe::read_versions_in_snapshot_header(FILE *file)
{
  // check versions
  smi read_major_version, read_minor_version;
  if (fscanf(file, "Version: %d.%d.%d%*[\r\n]",
             &read_major_version,
             &read_minor_version,
             &snapshot_version)    != 3 )
    fatalNoMenu("\n\tThe \"Version:\" line in the snapshot could not be parsed.\n");
      
  // return for snapshots whose version matches the current snapshot version
  if (snapshot_version == VM_snapshot_version)
    return;

  // Between snapshot versions 10 and 11, a new primitive was added: CompileWithSICNames.
  // To maintain compatibility, and since there's only a minor addition, we also read
  // snapshot whose version is 10. -mabdelmalek 11/02

  bool can_read_snapshot_with_mismatched_version =
        (snapshot_version == 10  &&  VM_snapshot_version == 11)
    || ((snapshot_version == 10 || snapshot_version == 11)  &&  VM_snapshot_version == 12)
    || ((snapshot_version == 12) && VM_snapshot_version == 13);
    
  if (can_read_snapshot_with_mismatched_version)
  	warning6("\n\tThis snapshot was saved using a different version\n"
                 "\tof the Self Virtual Machine (%d.%d.%d) and may behave unexpectedly\n"
                 "\tor not work correctly with this version (%d.%d.%d).\n", 
                 read_major_version,
                 read_minor_version,
                 snapshot_version,
                 VM_major_version,
                 VM_minor_version,
                 VM_snapshot_version);

  if (!can_read_snapshot_with_mismatched_version)
    fatalNoMenu6("\n\tThis snapshot was saved using a different version\n"
                 "\tof the Self Virtual Machine (%d.%d.%d) and will not\n"
                 "\twork with this version (%d.%d.%d).\n", 
                 read_major_version,
                 read_minor_version,
                 snapshot_version,
                 VM_major_version,
                 VM_minor_version,
                 VM_snapshot_version);

}


void universe::read_timestamp_in_snapshot_header(FILE *file)
{
  if (fscanf(file, "Timestamp: %d%*[\r\n]", (int*)&programming_timestamp) != 1)
    fatal("Error in snapshot format (timestamp)");
}


void universe::read_snapshot_code_flag_in_snapshot_header(FILE *file)
{
  char c;
  if ((fscanf(file, "Snapshot code: %c%*[\r\n]", &c) != 1)
   || (c != 'y' && c != 'n'))
    fatal("Error in snapshot format (snapshot code)");
  SnapshotCode= c == 'y';

#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!SnapshotCode && !noCodeWarnings)
    warning("This snapshot does not contain any object code;\n"
            "it will run more slowly at first while code is generated.");
  okToUseCodeFromSnapshot= okToUseCodeFromSnapshot && SnapshotCode;
#   else
  if (SnapshotCode)
    fatal("Sorry, but this version of Self cannot read snapshots with code");
  okToUseCodeFromSnapshot= false;
#   endif
}


void universe::read_vm_date_in_snapshot_header(FILE *file)
{
  // check for newer VM than that used with snapshot
  char vm_date[128];
  if (fscanf(file, "VM date: %128[^\n\r] ", vm_date) != 1)
    fatal("could not find the VM date in snapshot file--\n"
          "invalid snapshot file format");

  if (okToUseCodeFromSnapshot && strcmp(vm_date, vmDate) != 0) {
    noCodeWarning("because the VM used to write the snapshot is not the same as this one");
    if (!noCodeWarnings)
      warning2("\n\tThis VM:\t\t\t%s,\n\tVM used to make snapshot:\t%s",
               vmDate, vm_date);
  }
}


void universe::read_compression_flag_in_snapshot_header(FILE *file)
{
  char c;
  if ((fscanf(file, "Compressed: %c%*[\r\n]", &c) != 1)
   || (c != 'y' && c != 'n'))
    fatal("Error in snapshot format (compression)");
  compressed_snapshot= c == 'y';
  if (compressed_snapshot) {
    if ((fscanf(file, "Compression filter: %*s%*[\r\n]") != 0)
     || (fscanf(file, "Decompression filter: %80s%*[\r\n]", decompression_filter) != 1))
      fatal("Error in snapshot format (compression filters)");
  }
}


void universe::read_page_aligned_flag_in_snapshot_header(FILE *file)
{
  char c;
  if ((fscanf(file, "Page aligned: %c%*[\r\n]", &c) != 1)
   || (c != 'y' && c != 'n'))
    fatal("Error in snapshot format (page alignment)");
  page_aligned= c == 'y';
}


void universe::read_snapshot_header(FILE *file)
{
  ResourceMark rm;

  read_first_line_in_snapshot_header(file);
  read_versions_in_snapshot_header(file);
  read_timestamp_in_snapshot_header(file);
  read_snapshot_code_flag_in_snapshot_header(file);
  read_vm_date_in_snapshot_header(file);

  APPLY_TO_SPACE_SIZES(SPACE_SIZE_READ_TEMPLATE);

  read_compression_flag_in_snapshot_header(file);
  read_page_aligned_flag_in_snapshot_header(file);

  while (fgetc(file) != '\f' && !feof(file))
    ;
  fgetc(file);

  if (feof(file))
    fatalNoMenu("Truncated snapshot");
}



// in the compressed version, delimit the parts of the snapshots with
// these strings as a check on the compression/decompression
static char    misc_data_delim[] = "\n\f\nMisc data\n\f\n!%n";
static char      new_gen_delim[] = "\n\f\nNew generation\n\f\n!%n";
static char      old_gen_delim[] = "\n\f\nOld generation\n\f\n!%n";
static char string_table_delim[] = "\n\f\nString table\n\f\n!%n";
static char   VM_strings_delim[] = "\n\f\nVM strings\n\f\n!%n";
static char        vtbls_delim[] = "\n\f\nvtbls\n\f\n!%n";
static char         code_delim[] = "\n\f\nCode\n\f\n!%n";
static char    processes_delim[] = "\n\f\nProcesses\n\f\n!%n";
static char          end_delim[] = "\n\f\nEnd of snapshot\n\f\n!%n";


void check_delim(FILE *file, char *expected) {
  if (!page_aligned) {
    int nChars= -1;
    if (fscanf(file, expected, &nChars) != 0
        || nChars != strlen(expected) - 2)
      fatal3("Snapshot is corrupt near: %.*s, file position: 0x%x", 
             strlen(expected) - 8,
             expected+3,
             ftell(file));
  }
}


static bool check_endianness_of_vm_oops() {
  # define check_endianness_template(o) \
    if ((*o)->is_mem())  ++as_is_mem; \
    else if (*o != NULL) ++as_is_not_mem; \
    x = (oop)*o; \
    swap_bytes((int32*)&x); \
    if (x->is_mem())    ++swapped_is_mem; \
    else if (x != NULL) ++swapped_is_not_mem; 
    
  oop x;
  fint  as_is_mem = 0, as_is_not_mem = 0, swapped_is_mem = 0, swapped_is_not_mem = 0;
  APPLY_TO_VM_OOPS(check_endianness_template);
  
  if ( as_is_not_mem == 0  &&  swapped_is_not_mem != 0)  return false;
  if ( as_is_not_mem != 0  &&  swapped_is_not_mem == 0)  return true;
  if ( as_is_not_mem == 0  &&  swapped_is_not_mem == 0)  {
    warning("cannot tell for sure about byte-order; guessing OK");
    return false;
  }
  fatal("neither byte-ordering seems to work");
  return false;
}


// read snapshot from an open file and close it
void universe::read_snapshot(FILE* file) {
  ResourceMark rm;

  if (compressed_snapshot)
    file= OS::start_decompressing_snapshot(file, decompression_filter);

  check_delim(file, misc_data_delim);

  { unsigned char canChar;
    OS::FRead(&canChar, sizeof(canChar), file);
    bool canonicalized= canChar != '\0';
    map_table->set_maps_are_canonical_in_snapshot(canonicalized);
  }

  long start_of_oops = ftell(file);
  
  APPLY_TO_VM_OOPS(READ_SNAPSHOT_TEMPLATE);
  is_snapshot_other_endian = check_endianness_of_vm_oops();
  if (is_snapshot_other_endian) {
    fseek(file, start_of_oops, SEEK_SET);
    APPLY_TO_VM_OOPS(READ_SNAPSHOT_TEMPLATE);
  }
    
  READ_SNAPSHOT_TEMPLATE(&tenuring_threshold);

  APPLY_TO_VM_MAPS(MAP_READ_SNAPSHOT_TEMPLATE);
  
  check_delim(file, new_gen_delim);
  new_gen= new newGeneration(current_sizes.eden_size,
                             current_sizes.surv_size,
                             file);
  check_delim(file, old_gen_delim);
  old_gen= new oldGeneration(file, current_sizes.old_size, new_gen->capacity());

  remembered_set= new rSet; // uses generation boundaries

  check_delim(file, string_table_delim);
  string_table->read_snapshot(file);

  check_delim(file, VM_strings_delim);
  VMStrings_read_snapshot(file);
  
  check_delim(file, vtbls_delim);
  Vtbls->read_snapshot(file);
  
  bool need_to_relocate= false;
  bool need_to_relocate_objs= false;
  APPLY_TO_SPACES(SPACE_NEED_TO_RELOCATE_TEMPLATE);
  
  if (need_to_relocate) {
    APPLY_TO_VM_OOPS(RELOCATE_TEMPLATE);
    APPLY_TO_VM_MAPS(MAP_RELOCATE_TEMPLATE);
    APPLY_TO_SPACES(SPACE_RELOCATE_TEMPLATE);
    string_table->relocate();
    VMStrings_relocate();
    
    // cannot read in code if need to move other addresses around
    // (e.g. nmlns in code pointing to memory spaces)
    if (okToUseCodeFromSnapshot && need_to_relocate_objs)
      noCodeWarning(old_gen->nSpaces > 1
                    ? "because the snapshot contains a segmented heap"
                    : "because relocation is required");
  }
  
  old_gen->record_new_pointers();

  check_delim(file, code_delim);
  
  code->read_snapshot(file);

  APPLY_TO_SPACES(SPACE_FIXUP_MAPS_TEMPLATE);
  APPLY_TO_SPACES(SPACE_FIXUP_KILLABLES_TEMPLATE);
  
  if (okToUseCodeFromSnapshot) code->fixup();
  
  check_delim(file, processes_delim);
  processes->read_snapshot(file);
  
  if (need_to_relocate) {
    APPLY_TO_SPACES(SPACE_RELOCATE_BYTES_TEMPLATE);
  }
  old_gen->coalesce_spaces();
  
  NeedScavenge = false;
  
  check_delim(file, end_delim);
  fclose(file);
    
  if (compressed_snapshot) {
    OS::end_decompressing_snapshot();
  }

  // must canonicalize each object's map 
  // Since garbage maps can exist that would be redundant with canonicalized
  // maps, we cannot just add every map to the map table.
  // Instead, an extra pass is needed (sigh) to find every map actually used
  // in an object. Given the extra pass, we might as well recanonicalize them
  // instead of just adding them in.
  // (If we could guarantee a GC before every writeSnapshot, this would not be
  // necessary).
  // must gc to eliminate all redundant, uncanonical maps to restore
  // canonicalization invariants

  if (CanonicalizeMaps) {
    // add these first to ensure that they are the canonical ones
    APPLY_TO_VM_MAPS(MAP_CANONICALIZATION_TEMPLATE);
    APPLY_TO_SPACES(SPACE_CANONICALIZE_MAPS_TEMPLATE);
  }

  CurrentObjectID = 0;
}


void write_delim(FILE *file, char *delim)
{
  if (!page_aligned) {
    char buf[80];
    strcpy(buf, delim);
    buf[strlen(delim)-2]= '\0'; // chop off %n
    fputs(buf, file);
  }
}

memOop universe::relocate(memOop p) {
  APPLY_TO_SPACES(SPACE_OOP_RELOCATE_TEMPLATE);
  ShouldNotReachHere(); // oop not in any old space
  return NULL;
}

#define WRITE_SPACE_SIZES_TEMPLATE(s) \
        fprintf(snapFile, STR(s) ": %d\n", snap_sizes->s);

// these are used during write_snapshot
static FILE *snapFile;
static SignalBlocker* sb;
static void *SFH;

void universe::snapshot_failed() {
  perror("Error while writing snapshot");
  OS::snapshot_failed(snapFile, compressed_snapshot, sb);
  unix_failure(SFH, errno);
  fatal("no recovery; sorry");
}

bool universe::write_snapshot(const char *fileName,
                              const char *compression_f,
                              const char *decompression_f,
                              spaceSizes *snap_sizes) {

  compressed_snapshot= compression_f && decompression_f;

  const char *fullFileName;
  snapFile= Files->openSnapshotFile(fileName, "w", &fullFileName);
  if (snapFile == NULL) return NULL;

  if (!snap_sizes) snap_sizes= &current_sizes;

  OS::FWrite(SNAPSHOT_HEADER, strlen(SNAPSHOT_HEADER), snapFile);

  fprintf(snapFile, "Version: %d.%d.%d\n",
          VM_major_version, VM_minor_version, VM_snapshot_version);
          
  fprintf(snapFile, "Timestamp: %d\n", (int)programming_timestamp);

  fprintf(snapFile, "Snapshot code: %c\n", SnapshotCode ? 'y' : 'n');

  fprintf(snapFile, "VM date: %s\n", vmDate);

  old_gen->set_write_only_old0();

  APPLY_TO_SPACE_SIZES(WRITE_SPACE_SIZES_TEMPLATE);

  fprintf(snapFile, "Compressed: %c\n", compressed_snapshot ? 'y' : 'n');
  if (compressed_snapshot) {
    fprintf(snapFile,   "Compression filter: %s\n",   compression_f);
    fprintf(snapFile, "Decompression filter: %s\n", decompression_f);
  }

# if TARGET_OS_VERSION == MACOSX_VERSION
  // Linux???
  // On OSX, when space::write_snapshot tries to write starting earlier than
  // bytes_bottom, since OSX page = 4096 is smaller than idealized_page_size = 8096,
  // it can try to write an unallocated address and the write fails. -- dmu 9/1
  page_aligned= false;
# else
  page_aligned= !compressed_snapshot && old_gen->write_only_old0;
# endif
  fprintf(snapFile, "Page aligned: %c\n", page_aligned ? 'y' : 'n');

  fprintf(snapFile, "The rest of this file is binary data.\n\f\n");

  if (compressed_snapshot) {
    fclose(snapFile);
    snapFile = OS::start_compressing_snapshot(compression_f, fullFileName, sb);
    if (snapFile == NULL) {
      delete sb;
      return false;
    }
  }

  write_delim(snapFile, misc_data_delim);

  { bool8 c = map_table->get_maps_are_canonical();
    OS::FWrite(&c, sizeof(c), snapFile);
  }
  
  APPLY_TO_VM_OOPS(WRITE_SNAPSHOT_TEMPLATE);
  WRITE_SNAPSHOT_TEMPLATE(&tenuring_threshold);
  APPLY_TO_VM_MAPS(MAP_WRITE_SNAPSHOT_TEMPLATE);
  
  OS::set_sequential_access_before_writing_snapshot();

  write_delim(snapFile, new_gen_delim);
  new_gen->write_snapshot(snapFile);

  write_delim(snapFile, old_gen_delim);
  old_gen->write_snapshot(snapFile);

  write_delim(snapFile, string_table_delim);
  string_table->write_snapshot(snapFile);

  write_delim(snapFile, VM_strings_delim);
  VMStrings_write_snapshot(snapFile);
  
  write_delim(snapFile, vtbls_delim);
  Vtbls->write_snapshot(snapFile);

  write_delim(snapFile, code_delim);
  code->write_snapshot(snapFile);

  write_delim(snapFile, processes_delim);
  processes->write_snapshot(snapFile); 

  OS::set_normal_access_after_writing_snapshot();

  write_delim(snapFile, end_delim);

  if (compressed_snapshot ? OS::end_compressing_snapshot(snapFile) : fclose(snapFile))
    return false;
    
  if (!OS::setup_snapshot_to_run(fullFileName))
    warning("could not make Snapshot runnable");

  if (compressed_snapshot) delete sb;

  return true;
}

// Get the value of the slotName from the map.
// Return true if the slot is not present, or has a positive integer value,
// otherwise return false.
static bool get_space_size(slotsOop obj, const char *slotName, smi &val)
{
  bool junk;
  oop *slotp= obj->get_slot_data_address_if_present(slotName, junk);
  if (slotp == NULL) return true;
  if (! (*slotp)->is_smi()) return false;
  val= smiOop(*slotp)->value();
  return val > 0;
}

#define GET_SPACE_SIZE_FROM_OBJ_TEMPLATE(s)                             \
        if (!get_space_size(sizeObj, STR(s), snap_sizes.s)) {           \
          failure(FH, "Invalid space size");                            \
          return 0; }


oop full_write_snapshot_prim(oop rcvrIgnored, byteVectorOop name,
                             slotsOop c_obj, slotsOop sizeObj,
                             bool snapCode, void *FH) {
  Unused(rcvrIgnored);
  ResourceMark rm;

  SnapshotCode= snapCode;

  // set defaults from running system
  spaceSizes snap_sizes= Memory->current_sizes;

  { APPLY_TO_SPACE_SIZES(GET_SPACE_SIZE_FROM_OBJ_TEMPLATE);

    const char *s= Memory->check_sizes_for_snapshot(snap_sizes);
    if (s) {
      failure(FH, s);
      return 0;
    }
  }
  char *comp_f= NULL, *decomp_f= NULL;
  { bool junk;
    oop *  compress_slotp=
      c_obj->get_slot_data_address_if_present(  "compression_filter", junk);
    oop *decompress_slotp=
      c_obj->get_slot_data_address_if_present("decompression_filter", junk);
    if ((compress_slotp == NULL) != (decompress_slotp == NULL)) {
      // either both or neither...
      failure(FH, "Must have both or neither of compression_filter and decompression_filter");
      return 0;
    }
    if (compress_slotp && decompress_slotp) {
      if (   !(*  compress_slotp)->is_byteVector()
          || !(*decompress_slotp)->is_byteVector()) {
        failure(FH, "(De)compression filters must be specified by byte vectors");
        return 0;
      }
        comp_f= byteVectorOop(*  compress_slotp)->copy_null_terminated();
      decomp_f= byteVectorOop(*decompress_slotp)->copy_null_terminated();
    }
  }

  char* fn= name->copy_null_terminated();
  SFH= FH;

  if (!Memory->write_snapshot(fn, comp_f, decomp_f, &snap_sizes)) {
    unix_failure(FH, errno);
    return NULL;
  }
  return name;
}

bool universe::verify_oop(memOop p, bool expectErrorObj) {
  if (!expectErrorObj && p == errorObj)
    error("refce to errorObj");
  APPLY_TO_SPACES(SPACE_VERIFY_OOP_TEMPLATE);
  error1("memOop 0x%lx not in any space", p);
  return false;
}

byteVectorOop universe::verifyOpts(char *newOpts) {
  byteVectorOop oldOpts= new_string(verify_opts);
  delete [] verify_opts;
  verify_opts= newOpts;
  return oldOpts;
}

bool universe::verify(bool postScavenge) {
  ResourceMark rm;
  initNmlnCache(); 
  bool r = true; 
  lprintf("verifying ");
  if (is_verify_opt('n') || is_verify_opt('e') || is_verify_opt('s')) {
    lprintf("newgen: ");
    r &= new_gen->verify();
  }
  if (is_verify_opt('o')) {
    lprintf("oldgen: ");
    r &= old_gen->verify(); 
  }
  if (is_verify_opt('z')) {
    lprintf("z "); 
    r &= code->verify(); 
  }
  if (is_verify_opt('p')) {
    lprintf("p ");
    r &= processes->verify();
    r &= hprofiler->verify();
    r &= profilers->verify(); 
  }
  if (is_verify_opt('r')) {
    lprintf("r ");  
    r &= remembered_set->verify(postScavenge); }
  if (is_verify_opt('S')) {
    lprintf("S "); 
    r &= string_table->verify(); }
  if (is_verify_opt('v')) {
    lprintf("v "); 
    VMStrings_verify(r); }
  if (is_verify_opt('O')) {
    lprintf("O "); 
    bool verify_result = true;
    APPLY_TO_VM_OOPS(VERIFY_TEMPLATE_EXPECT_ERROR_OBJ); 
    r &= verify_result; }
  if (is_verify_opt('m')) {
    lprintf("m "); 
    bool verify_result = true;
    APPLY_TO_VM_MAPS(MAP_VERIFY_TEMPLATE); 
    r &= verify_result; }
  if (is_verify_opt('N')) {
    lprintf("N "); 
    r &= new_gen->verify_new_maps(); }
  if (is_verify_opt('M')) {
    lprintf("M "); 
    r &= map_table->verify(); }
  if (is_verify_opt('i')) {
    lprintf("i "); 
    r &= slotIterator_verify(); }
    if (CheckAssertions) {
     lprintf("h "); 
     r &= malloc_verify();
    }
  resetNmlnCache();
  lprintf(" done\n");
  return r;
}

#define PRINT_OBJ_TEMPLATE(obj)              \
  lprintf(#obj ": "); printObjectID(obj);    \
  lprintf("\n"); printIndent();

#define PRINT_MAP_TEMPLATE(map)              \
  lprintf(#map ": 0x%lx\n", map->enclosing_mapOop()); \
  printIndent();



void universe::print() {
  printIndent();
  lprintf("Memory:\n");
  Indent++;
  new_gen->print();
  old_gen->print();
  code->print();
  if (WizardMode) {
    printIndent();
#   ifdef VERY_VERBOSE_UNIVERSE_PRINT   // I don't like this  -- Urs
      PRINT_OBJ_TEMPLATE(lobbyObj);
      PRINT_OBJ_TEMPLATE(nilObj);
      PRINT_OBJ_TEMPLATE(trueObj);
      PRINT_OBJ_TEMPLATE(falseObj);
      PRINT_OBJ_TEMPLATE(stringObj);
      PRINT_OBJ_TEMPLATE(byteCodeObj);
      PRINT_OBJ_TEMPLATE(objVectorObj);
      PRINT_OBJ_TEMPLATE(byteVectorObj);
      PRINT_OBJ_TEMPLATE(blockTraitsObj);
      PRINT_OBJ_TEMPLATE(assignmentMirrorObj);
      PRINT_OBJ_TEMPLATE(blockMirrorObj);
      PRINT_OBJ_TEMPLATE(byteCodeMirrorObj);
      PRINT_OBJ_TEMPLATE(byteVectorMirrorObj);
      PRINT_OBJ_TEMPLATE(floatMirrorObj);
      PRINT_OBJ_TEMPLATE(objVectorMirrorObj);
      PRINT_OBJ_TEMPLATE(slotsMirrorObj);
      PRINT_OBJ_TEMPLATE(smiMirrorObj);
      PRINT_OBJ_TEMPLATE(stringMirrorObj);
      PRINT_OBJ_TEMPLATE(proxyMirrorObj);
      PRINT_OBJ_TEMPLATE(fctProxyMirrorObj);
      PRINT_OBJ_TEMPLATE(mirrorMirrorObj);

      PRINT_MAP_TEMPLATE(smi_map);
      PRINT_MAP_TEMPLATE(float_map);
      PRINT_MAP_TEMPLATE(mark_map);
      PRINT_MAP_TEMPLATE(map_map);
#   endif
    printIndent();
    printIndent();
    lprintf("tenuring_threshold: %ld\n", tenuring_threshold);
  }
  Indent--;
}


void universe::increment_programming_timestamp() {
  programming_timestamp =
    programming_timestamp == smiOop_max
    ? smiOop_zero
    : programming_timestamp->increment();
}


void universe_init() {
  new universe;         // no assignment needed
  Memory->canonize_map_vtbls();

  if (CheckAssertions) {
    // check FOR_ALL macros
    // iterate through Memory object (don't rely on FOR_ALL_VM_OOPS)
    oop* start = (oop*)&Memory->lobbyObj;
    oop* end   = (oop*)&Memory->smi_map;

#   define FIND_OOP_TEMPLATE(ptr)  if (p == (oop*)ptr) continue;
    
    oop *p;
    for (p = start; p < end; p++) {
      APPLY_TO_VM_OOPS(FIND_OOP_TEMPLATE);
      error2("Memory structure at offset %ld: oop %#lx is not registered in APPLY_TO_VM_OOPS macro",
       (char*)p - (char*)Memory, *p);
    }
    for ( p = start; p < end; p++) {
      if (! Vtbls->contains((*p)->map()->vtbl_value())) {
        error2("Memory structure at offset %ld: map of oop %#lx is not registered in Vtbls", 
               (char*)p - (char*)Memory, *p);
      }
    }
  }
}


void universe::canonize_map_vtbls() {
  APPLY_TO_SPACES(SPACE_CANONIZE_MAP_VTBLS_TEMPLATE);
}


// checks of startup parameters

const char *universe::check_eden_size(spaceSizes &snap_sizes) {
  return
    snap_sizes.eden_size < new_gen->eden_space->used()
      ? "eden_size too small" : NULL; 
}

const char *universe::check_surv_size(spaceSizes &snap_sizes) {
  return
         snap_sizes.surv_size < new_gen->from_space->used()
      || snap_sizes.surv_size < new_gen->  to_space->used()
        ? "surv_size too small" : NULL; }

const char *universe::check_old_size(spaceSizes &snap_sizes) {
  if (snap_sizes.old_size < old_gen->used())
    return "old_size too small";
  if (!space_sizes_ok(snap_sizes.old_size,
                      snap_sizes.eden_size,
                      snap_sizes.surv_size))
    return "old_size too small relative to eden_size + 2*surv_size";
  if ( OS::is_directed_allocation_supported()
  &&   snap_sizes.old_size + snap_sizes.eden_size + 2*snap_sizes.surv_size  >  MaxHeapSize)
    return "total object heap size too big";
  return NULL; }


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

const char *universe::check_code_size(spaceSizes &snap_sizes) {
  if (snap_sizes.code_size > MaxNMethodSize)
    return "code_size too big";
  return
    SnapshotCode && snap_sizes.code_size < code->iZone->usedBytes()
      ? "code_size too small" : NULL; }

const char *universe::check_deps_size(spaceSizes &snap_sizes) {
  if (snap_sizes.deps_size > MaxDepsSize)
    return "deps_size too big";
  return
    SnapshotCode && snap_sizes.deps_size < code->dZone->usedBytes()
      ? "deps_size too small" : NULL; }

const char *universe::check_pic_size(spaceSizes &snap_sizes) {
  if (snap_sizes.pic_size > MaxStubsSize)
    return "pic_size too big";
  return
    SnapshotCode && snap_sizes.pic_size < code->stubs->zone()->usedBytes()
      ? "pic_size too small" : NULL; }

const char *universe::check_debug_size(spaceSizes &snap_sizes) {
  if (snap_sizes.debug_size > MaxScopesSize)
    return "debug_size too big";
  return
    SnapshotCode && snap_sizes.debug_size < code->sZone->usedBytes()
      ? "debug_size too small" : NULL; }

# else // not: defined(FAST_COMPILER) || defined(SIC_COMPILER)

const char* universe::check_code_size(spaceSizes& snap_sizes) {
  Unused(&snap_sizes); return NULL; }

const char* universe::check_deps_size(spaceSizes& snap_sizes) {
  Unused(&snap_sizes); return NULL; }

const char* universe::check_pic_size(spaceSizes& snap_sizes) {
  Unused(&snap_sizes); return NULL; }

const char* universe::check_debug_size(spaceSizes& snap_sizes) {
  Unused(&snap_sizes); return NULL; }
  
# endif


#define CHECK_SPACE_SIZE_TEMPLATE(s)                    \
        err_str= CONC(check_, s)(snap_sizes);           \
        if (err_str) return err_str;

// return NULL for OK, otherwise error string
const char *universe::check_sizes_for_snapshot(spaceSizes &snap_sizes)
{
  const char *err_str;
  APPLY_TO_SPACE_SIZES(CHECK_SPACE_SIZE_TEMPLATE);
  return NULL;
}


// primitive to expand heap space
oop expand_heap_prim(oop rcvrIgnored, smi grow_size)
{
  Unused(rcvrIgnored);
  if (grow_size < 0)
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_smiOop(Memory->old_gen->expand(grow_size));
}


oop VMversion_prim(oop rcvrIgnored) {
  Unused(rcvrIgnored);
  objVectorOop v= Memory->objVectorObj->cloneSize(3);
  v->obj_at_put(0, as_smiOop(Memory->major_version));
  v->obj_at_put(1, as_smiOop(Memory->minor_version));
  v->obj_at_put(2, as_smiOop(Memory->snapshot_version));
  return v;
}
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "universe.more.hh"

# include "_universe.more.cpp.incl"

bool GCInProgress = false;
bool ScavengeInProgress = false;

void universe::swapSurvivors() {
  {
    newSpace* s= new_gen->from_space;
    new_gen->from_space= new_gen->to_space;
    new_gen->to_space= s;
  }
  new_gen->eden_space->next_space= new_gen->from_space;
  new_gen->from_space->name= "from";
  new_gen->from_space->next_space= new_gen->to_space;
  new_gen->  to_space->name= "to";
  new_gen->  to_space->next_space= NULL;
}

smi set_memory_tenuring_threshold_prim(oop rcvrIgnored, smi newThresh, void *FH)
{
  Unused(rcvrIgnored);
  if (newThresh < 0 || newThresh > Memory->current_sizes.surv_size) {
    failure(FH, "Threshold out of range");
    return (smi)NULL;
  }
  smi oldThresh= Memory->Desired_Surv_Size;
  Memory->Desired_Surv_Size= newThresh;
  return oldThresh;
}

oop universe::tenure(oop p) {
  tenuring_threshold = 0;               // tenure everything
  oop r = scavenge(p);
# define checkIt(s) assert(s->used() == 0, "new spaces should be empty");
  APPLY_TO_YOUNG_SPACES(checkIt)
# undef checkIt
  return r;
}

oop universe::default_low_space_handler(oop p)
{
  if (MemoryAutomaticGC)
    p = garbage_collect(p);
  int32 cap= old_gen->capacity();
  int32 fre= old_gen->bytes_free();
  if (   MemoryAutomaticHeapExpansion
      && (   might_run_out_of_space_on_scavenge()
          || float(fre) / float(cap) < 0.3
          || old_gen->lowOnSpace)) {
    int32 ngc= new_gen->capacity(); // for scavenge reserve
    int32 desired= max(max(ngc * 2, cap / 2), old_gen->getLowSpaceThreshold());
    int32 exp;
    while (exp= old_gen->expand(desired),
           exp == 0  &&  desired > ngc)
      desired /= 2;
    if (exp == 0)
      warning("Could not expand heap");
  }
  return p;
}

oop universe::scavenge(oop p) {
  BlockProfilerTicks bpt(exclude_scavenging);
  if (VerifyBeforeScavenge) verify();

  FlagSetting fl(GCInProgress, true);
  FlagSetting fl2(ScavengeInProgress, true);
  ++scavengeCount;
  assert(!processSemaphore, "processSemaphore shouldn't be set");
  assert(SlotIterator::nActive == 0,
         "shouldn't be iterating over slots");

  if (might_run_out_of_space_on_scavenge()) {
    if (!twainsProcess)
      p = default_low_space_handler(p);
    else {
      warning("some memory reserved by the VM has been used;\n"
              "invoking emergency heap expansion...");
      p = expand_heap_prim(0, 10000000);
    }
  }
  
  if (might_run_out_of_space_on_scavenge()) {
      warning("GC didn't free enough; growing heap. -- dmu 5/06");
      p = garbage_collect(p);  
  }
  if (might_run_out_of_space_on_scavenge())
    warning("Despite everything, this scavenge could run out of space "
            "and cause the VM to crash. -- dmu & Mario 6/04");
  
  {
    ShowVMActivityInMonitor ss(" scavenge ");
    EventMarker em("scavenging");
    
    timer tmr;
    if (PrintScavenge) {
      lprintf("*scavenging...");
      tmr.start();
    }
    
    age_table->clear();
    new_gen->prepare_for_scavenge();
    old_gen->prepare_for_scavenge();
    
    SCAVENGE_TEMPLATE(&p);
    APPLY_TO_VM_OOPS(SCAVENGE_TEMPLATE);
    APPLY_TO_VM_MAPS(MAP_SCAVENGE_TEMPLATE);
    VMStrings_scavenge_contents();
    string_table->scavenge_contents();
    processes->scavenge_contents();
    hprofiler->scavenge_contents();
    profilers->scavenge_contents();
    code->scavenge_contents();
    slotIterator_scavenge_contents();
    {FOR_EACH_OLD_SPACE(s) s->scavenge_recorded_stores();}
    while (old_gen->scavenge_promotions() || new_gen->scavenge_contents())
      ;
    
    new_gen->adjust_maps();
    
    new_gen->eden_space->clear();
    new_gen->from_space->clear();
    
    swapSurvivors();
    
    tenuring_threshold =
      new_gen->from_space->used()  <=  Desired_Surv_Size
       ? age_mask 
         : age_table->tenuring_threshold(Desired_Surv_Size);
    
    old_gen->cleanup_after_scavenge();

    if (PrintScavenge) {
      lprintf("done: %ld ms., %ld bytes in new_gen\n", long(tmr.time()), long(new_gen->used()));
    }
    
    if (VerifyAfterScavenge) verify(true);
    
    // do this at end so an overflow during a scavenge doesnt cause another one
    NeedScavenge = false;
  }
  return p; 
}

#define NORMAL_ACCESS_TEMPLATE(s) \
 OS::normal_access((char*)s->oopsStart(), (char*)s->oopsEnd());

#define RANDOM_ACCESS_TEMPLATE(s) \
 OS::random_access((char*)s->oopsStart(), (char*)s->oopsEnd());

#define ZONE_NORMAL_ACCESS_TEMPLATE(z) \
 OS::normal_access((char*)z->startAddr(), (char*)z->endAddr());

#define ZONE_RANDOM_ACCESS_TEMPLATE(z) \
 OS::random_access((char*)z->startAddr(), (char*)z->endAddr());


oop universe::garbage_collect(oop p) {
  BlockProfilerTicks bpt(exclude_garbage_collection);
  ShowVMActivityInMonitor ss(" GC ");
  EventMarker em("GC");
  ResourceMark rm;
  // before GCInProgress for FrameIterator::check_for_overwriting_patched_frame_saved_outgoing_args
  if (VerifyBeforeScavenge || VerifyBeforeGC) verify();
  FlagSetting fl(GCInProgress, true);

  assert(SlotIterator::nActive == 0,
         "shouldn't be iterating over slots");

  // Try to flush obsolete methods because they keep the respective
  // maps alive.  Actually, that's only a half-baked solution: the
  // real problem is that pointers from the zone to the Self heap should
  // be weak pointers.
  code->flushZombies();
  
  APPLY_TO_OLD_SPACES(RANDOM_ACCESS_TEMPLATE);
  APPLY_TO_ZONES(ZONE_RANDOM_ACCESS_TEMPLATE);

  timer        cpu_timer;
  ElapsedTimer real_timer;
  fint oldUsed = old_gen->used();

  ProcessInfo::update();
  fint faults= ProcessInfo::page_faults_IO();

  if (PrintGC) {
    lprintf("*garbage collecting...");
    cpu_timer.start();
    real_timer.start();
  }

  findSlotCache.clear();
  
  object_table = new oTable;
  
  // mark roots
  MARK_TEMPLATE(&p);
  APPLY_TO_VM_OOPS(MARK_TEMPLATE);
  APPLY_TO_VM_MAPS(MAP_MARK_TEMPLATE);
  
       code->gc_mark_contents();
  hprofiler->gc_mark_contents();
  profilers->gc_mark_contents();
  processes->gc_mark_contents();
  VMStrings_gc_mark_contents();
  slotIterator_gc_mark_contents();

  // mark reachable objects
  object_table->gc_mark_contents();
  
  // Note unreachable processes, but keep them alive
  processes->gc_mark_remaining_processes();

  object_table->gc_mark_rest();

  // finalize unreachable objects - must be after all other gc_mark routines!
  string_table->gc_mark_contents();

  remembered_set->clear();
  
  new_gen->init_map_list(); // compact will add all surviving maps to list
  
  // compact/sweep memory
  if (PrintGC) { lprintf("compacting..."); }
# if  TARGET_OS_VERSION == SOLARIS_VERSION
  // making this sequential has terrible performance
  APPLY_TO_OLD_SPACES(NORMAL_ACCESS_TEMPLATE);
# elif  TARGET_OS_VERSION == SUNOS_VERSION
  sequential_access(0, ~0);
# endif
  // Linux???

  // precompute the unmarked map-map mapOop, to identify dying map objects
  mapOop unmarked_map_map = mapOop(Memory->map_map->enclosing_mapOop()->gc_unmark());

  oop *d, *bd;
  space *c2; // this variable is required to avoid the creation of a temporary
             // when calling compact().
  APPLY_TO_YOUNG_SPACES(YOUNG_SPACE_COMPACT_TEMPLATE);
  old_gen->prepare_for_compaction();
  oldSpace *copySpace= old_gen->first_space;
  c2= copySpace; // to satisfy type requirements
  APPLY_TO_OLD_SPACES(OLD_SPACE_COMPACT_TEMPLATE);
  copySpace= (oldSpace*)c2;
  copySpace= copySpace->next_space;
  while (copySpace != NULL) {
    copySpace->clear();
    copySpace= copySpace->next_space;
  }
  
  if (PrintGC) { lprintf("unmarking...");  }

  APPLY_TO_OLD_SPACES(RANDOM_ACCESS_TEMPLATE);

  // unmark objects
  APPLY_TO_SPACES(SPACE_UNMARK_TEMPLATE);
  
  UNMARK_TEMPLATE(&p);
  APPLY_TO_VM_OOPS(UNMARK_TEMPLATE);
  APPLY_TO_VM_MAPS(MAP_UNMARK_TEMPLATE);
  
       code->gc_unmark_contents();
  hprofiler->gc_unmark_contents();
  profilers->gc_unmark_contents();
  processes->gc_unmark_contents();
  VMStrings_gc_unmark_contents();
  slotIterator_gc_unmark_contents();
  
  string_table->gc_unmark_contents();


  ProcessInfo::update();
  faults= ProcessInfo::page_faults_IO() - faults;

  bool plentyOfMemory= faults < 50; // some arbitary reasonable threshold
  // More than one page fault per reclaimed page is too many.
  bool discardPgs=
       !plentyOfMemory
    && (   faults > (oldUsed - old_gen->used()) / OS::get_page_size()
        || old_gen->capacity() > OS::real_mem_size);

  {FOR_EACH_OLD_SPACE(s) {
# if  TARGET_OS_VERSION == SOLARIS_VERSION
    OS::normal_access((char*)s->oopsStart(), (char*)s->oopsEnd()); 
# endif
    // Linux???
    if (!plentyOfMemory) {
      if (discardPgs)
        // +1 for sentinel
        OS::discard_pages((char*)(s->oopsEnd() + 1), s->bytesStart());
      else
        // +1 for sentinel
        OS::dont_need_pages((char*)(s->oopsEnd() + 1), s->bytesStart());
    }
  }}

  APPLY_TO_ZONES(ZONE_NORMAL_ACCESS_TEMPLATE);

  // object_table contains a separate resource area which is freed by 
  // delete.
  delete object_table;
  object_table = NULL;
  
  old_gen->update_caches(false);

  if (PrintGC) {
    real_timer.stop();
    lprintf("done: real=%ld ms, cpu=%ld ms, freed %d bytes\n",
           long(real_timer.millisecs()),
           long(cpu_timer.time()),
           oldUsed - old_gen->used());
  }
  
  if (VerifyAfterScavenge || VerifyAfterGC) verify();

  return p;
}


// returns vector or failedAllocationOop 

void universe::enumerate_all_objs(enumeration *e) {
  APPLY_TO_SPACES(SPACE_ENUMERATE_ALL_OBJS_TEMPLATE);
}

void universe::enumerate_matches(enumeration *e) {
  APPLY_TO_SPACES(SPACE_ENUMERATE_MATCHES_TEMPLATE);
}

void universe::enumerate_maps(enumeration *e) {
  APPLY_TO_SPACES(SPACE_ENUMERATE_MAPS_TEMPLATE);
}

void universe::enumerate_families(enumeration *e) {
  APPLY_TO_SPACES(SPACE_ENUMERATE_FAMILIES_TEMPLATE);
}


bool universe::really_contains(void* p) {
  APPLY_TO_SPACES(SPACE_REALLY_CONTAINS_TEMPLATE);
  return false;
}


space* universe::spaceFor(void* p) {
  if (new_gen->from_space->contains(p))   return new_gen->from_space;
  if (new_gen->eden_space->contains(p))   return new_gen->eden_space;
  {FOR_EACH_OLD_SPACE(s) if (s->contains(p)) return s;}

  ShouldNotReachHere(); // not in any space
  return NULL;
}


typedef enum { okStatus,
               noSlotStatus,
               notAssignableStatus,
               noMemStatus } slotStatus;

static slotStatus get_space_usage(slotsOop proto, const char *slotName, space *s)
{
  bool inObj;
  oop *p= proto->get_slot_data_address_if_present(slotName, inObj); 
  if (p == NULL) return noSlotStatus;
  if (!inObj) return notAssignableStatus;
  oop vec= s->get_allocation_vector();
  if (vec == failedAllocationOop) return noMemStatus;
  Memory->store(p, vec);
  return okStatus;
}
  

#define FILL_IN_STATE_SLOT_TEMPLATE(s)                                  \
   st= get_space_usage(proto, STR(s), Memory->new_gen->CONC(s,_space));  \
   if (st == notAssignableStatus) goto unassignable;                    \
   if (st == noMemStatus) goto out_of_mem;                              \


static slotStatus set_slot(slotsOop proto, const char *slotName, smi val)
{
  bool inObj;
  oop *p= proto->get_slot_data_address_if_present(slotName, inObj); 
  if (p == NULL) return noSlotStatus;
  if (!inObj) return notAssignableStatus;
  Memory->store(p, as_smiOop(val));
  return okStatus;
}

oop universe::get_mem_current_state_prim(oop rcvrIgnored,
                                         slotsOop proto,
                                         void *FH) {
  Unused(rcvrIgnored);
  slotStatus st;
  oop *p;

  // new gen
  APPLY_TO_YOUNG_SPACE_NAMES(FILL_IN_STATE_SLOT_TEMPLATE);

  // old gen
  bool inObj;
  p= proto->get_slot_data_address_if_present("old", inObj); 
  if (p) {
    if (!inObj) goto unassignable;
    unsigned int nSpaces= Memory->old_gen->nSpaces;
    oop sv= Memory->objVectorObj->cloneSize(nSpaces, CANFAIL);
    if (sv == failedAllocationOop) goto out_of_mem;

    // sort spaces by address (so Self code can tell if first is only one used)
    oldSpace **sp= NEW_RESOURCE_ARRAY(oldSpace*, nSpaces);
    old_gen->sorted_space_list(sp, addr_cmp);

    for (int n= 0; n < nSpaces; n++) {
      oop vec= sp[n]->get_allocation_vector();
      if (vec == failedAllocationOop) goto out_of_mem;
      sv->obj_at_put(n, vec);
    }
    Memory->store(p, sv);
  }

  if (set_slot(proto, "card_table_size", Memory->remembered_set->byte_map_size())
      == notAssignableStatus) goto unassignable;
  if (set_slot(proto, "num_scavenge", Memory->scavengeCount)
      == notAssignableStatus) goto unassignable;

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  // zone
  if (set_slot(proto, "code",  Memory->code->iZone->usedBytes())
      == notAssignableStatus) goto unassignable;
  if (set_slot(proto, "deps",  Memory->code->dZone->usedBytes())
      == notAssignableStatus) goto unassignable;
  if (set_slot(proto, "pics",  Memory->code->stubs->zone()->usedBytes())
      == notAssignableStatus) goto unassignable;
  if (set_slot(proto, "debug", Memory->code->sZone->usedBytes())
      == notAssignableStatus) goto unassignable;
# endif

  return proto;

  out_of_mem: 
    out_of_memory_failure(FH);
    return NULL;

  unassignable: 
    prim_failure(FH, UNASSIGNABLESLOTERROR);
    return NULL;

}



// ---------------------- statistics stuff -----------------------

# define TitleLine              "%-20s %8s %6s %8s %6s\n"
# define LineFormat             "%-20s %8ld %5.1f%% %8ld %5.1f%%\n"
# define Percent(n,total)       (100.0 * n / total)

class InterestingMap: public ResourceObj {
 public:
  Map*  map;
  char* name;
  int32 count;
  int32 size;

  InterestingMap(Map* m, const char* n) {
    map = m; count = size = 0;
    name = NEW_RESOURCE_ARRAY( char, strlen(n));
    strncpy(name, n, strlen(n));
    name[strlen(n)-3] = '\0';           // cheap trick to get rid of "Map"
  }

  virtual bool matches(oop p, Map* m)   {
    Unused(p);
    return m->vtbl_value() == map->vtbl_value(); }
  void add(fint s)              { count++; size += s; }
  void print_im(int32 tCount, int32 tSize) {
    lprintf(LineFormat, name, long(count), Percent(count, tCount),
           long(size), Percent(size, tSize));
  }
    
};

typedef bool (*matchFn)(oop p, Map* m);

class GenericInterestingMap : public InterestingMap {
 public:
  matchFn pred;

  GenericInterestingMap(const char* nm, matchFn predicate) :
    InterestingMap(NULL, nm) { pred = predicate; }
  bool matches(oop p, Map* m) { return pred(p, m); }
};

class MemoryHistogram: public ResourceObj {
 public:
  fint   maxSize;               // max object size for histogram
  int32  totalSize;             // total size of all objects
  int32* sizeCounts;            // counters indexed by obj size
  int32  nobjs;                 // total # objects
  int32  nmaps;                 // total # maps
  int32  mapSize;               // size taken by them
  fint   interesting;           // # of interesting maps
  InterestingMap** iMaps;

  void init(fint maxSize, fint imaps);
  void addGenericMap(const char* name, matchFn predicate);
  void addMap(Map* m, const char* name);
  void add(oop p);
  void print();
};

MemoryHistogram* histogram;
void histoFn(oop* p) { histogram->add(*p); }

void MemoryHistogram::init(fint maxS, fint imaps) {
  maxSize = maxS;
  sizeCounts = NEW_RESOURCE_ARRAY( int32, maxSize);
  fint i;
  for (i = 0; i < maxSize; i++) sizeCounts[i] = 0;
  totalSize = nobjs = nmaps = mapSize = interesting = 0;
  iMaps = NEW_RESOURCE_ARRAY( InterestingMap*, imaps);
  for ( i = 0; i < imaps; i++) iMaps[i] = NULL;
}

void MemoryHistogram::addGenericMap(const char* name, matchFn predicate) {
  iMaps[interesting++] = new GenericInterestingMap(name, predicate);
}

void MemoryHistogram::addMap(Map* m, const char* name) {
  iMaps[interesting++] = new InterestingMap(m, name);
}

void MemoryHistogram::add(oop p) {
  nobjs++;
  fint size = p->size();
  totalSize += size;
  if (p->is_map()) { nmaps++; mapSize += size; }
  Map* m = p->map();
  sizeCounts[size < maxSize ? size - 1 : maxSize - 1]++;
  for (fint i = 0; i < interesting; i++ ) {
    if (iMaps[i]->matches(p, m)) {
      iMaps[i]->add(size);
      break;
    }
  }
}

int compareIM(const void* m1, const void* m2) {
  return (*(InterestingMap**)m2)->size - (*(InterestingMap**)m1)->size;
}

void MemoryHistogram::print() {
  lprintf(TitleLine, "OBJECT TYPE", "COUNT", "%", "SIZE", "%");
  qsort(iMaps, interesting, sizeof(void*), compareIM);
  fint i;
  for (i = 0; i < interesting; i++ ) iMaps[i]->print_im(nobjs, totalSize);
  lprintf(LineFormat, "TOTAL", long(nobjs), 100.0, long(totalSize), 100.0);
  lprintf("\n\n");

# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      int32 total = 0;
      for ( i = 0; i < maxSize; i++) total += sizeCounts[i];
      assert(total == nobjs, "wrong total");
    }
#   endif
  lprintf("object size distribution (size in words, including mark etc.):\n");
  int32 cumul = 0;
  fint median = 0;
  for ( i = 0; i < maxSize-1; i++) {
    cumul += sizeCounts[i];
    double percent = Percent(sizeCounts[i], nobjs);
    if (cumul / (double)nobjs < 0.5) median = i + 1;
    lprintf("%5ld: %6ld \t%4.1f%% \t%5.1f%%\n", long(i + 1),
           long(sizeCounts[i]), percent,
           Percent(cumul, nobjs));
  }
  lprintf(" >%3ld: %6ld \t%4.1f%% \t%5.1f%%\n",
         long(maxSize-1), long(sizeCounts[maxSize-1]),
         Percent(sizeCounts[maxSize-1], nobjs), 100.0);
  
  lprintf("nobjs: %6ld (%ld words); median size = %ld, avg. size = %3.1f\n",
         long(nobjs), long(totalSize), long(median + 1),
         (double)totalSize / nobjs);
  lprintf("maps:  %6ld (%ld words = %3.1f%%); avg. map size = %3.1f\n",
         long(nmaps), long(mapSize), 100.0 * mapSize / (double)totalSize,
         (double)mapSize / (double)nmaps);
  int32 bytes = Memory->new_gen->eden_space->bytes_used() +
                Memory->new_gen->from_space->bytes_used() +
                Memory->old_gen->bytes_used();
  lprintf("not counted above: bytes part of byte arrays: %ld words\n",
         long(bytes / oopSize));
}

# define INTERESTING_MAP_TEMPLATE(m)                                          \
  m CONC(m,_map);                                                             \
  histogram->addMap(&CONC(m,_map), STR(m));

static bool methodMatch(oop p, Map* pm) {
  Unused(pm);
  if (!p->is_map()) return false;
  Map* m = mapOop(p)->map_addr();
  return m->has_code() && m->kind() == OuterMethodType;
}

static bool blockMatch(oop p, Map* pm) {
  Unused(pm);
  if (!p->is_map()) return false;
  Map* m = mapOop(p)->map_addr();
  return m->is_block();
}

static bool blockMethodMatch(oop p, Map* pm) {
  Unused(pm);
  if (!p->is_map()) return false;
  Map* m = mapOop(p)->map_addr();
  return m->has_code() && m->kind() == BlockMethodType;
}

void universe::objectSizeHistogram(fint maxSize) {
  ResourceMark rm;
  maxSize++;
  histogram = new MemoryHistogram;

  histogram->init(maxSize, 100);
  histogram->addGenericMap("method mapXXX", methodMatch);
  histogram->addGenericMap("block mapXXX", blockMatch);
  histogram->addGenericMap("block method mapXXX", blockMethodMatch);
  FOR_ALL_MAP_TYPES(INTERESTING_MAP_TEMPLATE);
  new_gen->eden_space->oops_do(histoFn);
  new_gen->from_space->oops_do(histoFn);
  {FOR_EACH_OLD_SPACE(s) s->oops_do(histoFn);}
  histogram->print();
}

# if  GENERATE_DEBUGGING_AIDS
void universe::printRegion(char *&caddr, int count)    
{
  ResourceMark r;
  caddr= (char*)((int)caddr & ~Tag_Mask);
  oop *addr= (oop*)caddr;
  if (!Memory->really_contains(addr))
    fprintf(stderr, "%#lx not part of any space!\n", (long unsigned)addr);
  else {
    space *s= Memory->spaceFor(addr);
    if (!s->objs_contains(addr))
      fprintf(stderr, "%#lx not in the objs part of %s\n",
              (long unsigned)addr, s->name);
    else {
      fprintf(stderr, "%#lx (%s)\n", (long unsigned)addr, s->name);
      if (addr == s->oopsStart())
        fprintf(stderr, "---start of %s---\n", s->name);
      while (count--) {
        fprintf(stderr, "  %#lx: (%#lx) %s",
                (long unsigned)addr, (long unsigned)*addr,
                oop(*addr)->debug_print());
        if (   (*addr)->tag() == Int_Tag
            && s->bytes_contains(*addr)
            && (addr[-1])->tag() == Int_Tag)
          // probably a string
          fprintf(stderr, " \"%.*s\"", smiOop(addr[-1])->value(),
                  (char*)(*addr));
        fputc('\n', stderr);
        if (++addr == s->oopsEnd()) {
          fprintf(stderr, "---end of %s objs---\n", s->name);
          break;
        }
      }
    }
  }
}
#endif
  
static int nSlots, nDefaultAnnotations;

# if  GENERATE_DEBUGGING_AIDS
nmln* canonical_dep(nmln *d) {
  nmln *min= d;
  FOR_EACH_NMLN(d, elem, if (elem < d) d= elem);
  return min; }
#endif

void slotStats(slotDesc *s) {
  nSlots++;
  if (s->annotation == Memory->slotAnnotationObj)
    nDefaultAnnotations++;
# if  GENERATE_DEBUGGING_AIDS
  lprintf("%s %s ",
          s->augmentedName(),
            s->is_obj_slot() ? "obj"
          : s->is_map_slot() ? "map"
          : s->is_arg_slot() ? "arg"
          : "assignment");
  if (s->data->is_smi())
      lprintf("%d ", (smiOop(s->data))->value());
  else
      lprintf("%#lx ", (s->data));
  lprintf("%#lx\n",
         s->annotation == Memory->slotAnnotationObj
         ? 0 :(long unsigned)(s->annotation));
# endif
}

void allSlotDescsDo(oop *pp) {
  oop p= *pp;
  if (p->is_map()) {
    Map *m0= mapOop(p)->map_addr();
# if  GENERATE_DEBUGGING_AIDS
    lprintf("map: %s %s %s %s %#lx\n",
           m0->is_assignment()         ? "assignment"        
         : m0->is_block()              ? "block"             
         : m0->is_string()             ? "string"            
         : m0->is_byteVector()         ? "byteVector"        
         : m0->is_objVector()          ? "objVector"         
         : m0->is_mirror()             ? "mirror"            
         : m0->is_process()            ? "process"           
         : m0->is_vframe()             ? "vframe"            
         : m0->is_method_like()        ? "method_like"       
         : m0->is_fctProxy()           ? "fctProxy"          
         : m0->is_proxy()              ? "proxy"             
         : m0->is_foreign()            ? "foreign"           
         : m0->is_profiler()           ? "profiler"          
         : m0->is_slots()              ? "slots"             
         : m0->is_map()                ? "map"               
         : m0->is_smi()                ? "smi"
         : m0->is_float()              ? "float"
         : m0 == Memory->mark_map      ? "mark"
         : "???",
           m0->should_canonicalize()
           ? ((slotsMapDeps*)m0)->map_chain()->isEmpty() ? "nochain" : "chain"
           : "not_canon",
           m0->can_have_dependents()
           ? ((slotsMapDeps*)m0)->has_slot_dependents()
              ? "slotsdeps"
              : "noslotsdeps"
           : "not_slotsdeps",
           m0->can_have_dependents()
           ? ((slotsMapDeps*)m0)->add_slot_dependents()->isEmpty()
             ? "noaddslots" : "addslots"
           : "not_slots",
           m0->is_slots()
           ? ((slotsMap*)m0)->get_annotation() == Memory->objectAnnotationObj
             ? 0 : (unsigned long)(((slotsMap*)m0)->get_annotation())
           : 0);
# endif
    FOR_EACH_SLOTDESC(m0, s)
      slotStats(s);
  }
}

#define ALL_SLOTS_TEMPLATE(s)   s->oops_do(allSlotDescsDo);

void universe::printSlotDescStats() {
  nSlots= nDefaultAnnotations= 0;
  APPLY_TO_SPACES(ALL_SLOTS_TEMPLATE);
  lprintf("%d slot descriptors\n", nSlots);
  lprintf("%d (%3.1f%%) have the default annotation\n", nDefaultAnnotations,
         nDefaultAnnotations * 100.0 / nSlots);
}

void universe::nonCombiningMode() {
  code->nonCombiningMode();
}

void universe::setDepsMap(nmln *deps, slotsMapDeps *m) {
  code->setDepsMap(deps, m);
}

nmln* universe::allocateSlotDeps(slotsMapDeps *m) {
  char *d= code->allocateDeps(m->length_slots() * sizeof(nmln) + sizeof(m));
  nmln *deps= (nmln*)(d + sizeof(m));
  code->setDepsMap(deps, m);
  return deps;
}

void universe::deallocateSlotDeps(nmln *deps, fint ndeps) {
  code->deallocateDeps((char*)deps - sizeof(slotsMapDeps*),
                       ndeps * sizeof(nmln) + sizeof(slotsMapDeps*));
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "ageTable.hh"
# include "_ageTable.cpp.incl"

ageTable::ageTable() { clear(); }

int32 ageTable::tenure_size(fint age) {
  int32 total = 0;
  for (; age < age_table_size; age ++) {
    total += sizes[age];
  }
  return total;
}

fint ageTable::tenuring_threshold(int32 size) {
  int32 age, total = 0;
  for (age = 1; age < age_table_size; age ++) {
    total += sizes[age];
    if (total > size)
      // zero will promote too much garbage
      return age == 1  ?  1  :  age - 1;
  }
  return age;
}

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "oTable.hh"
# pragma implementation  "oTable_inline.hh"

# include "_oTable.cpp.incl"

void* oTableObj::operator new(size_t size){ 
  assert(Memory->object_table, "object_table must exist");
  return Memory->object_table->resource_area.allocate_bytes(size);
}

void oTable::grow() {
  if (!bottom) {
#  if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
     // Since resource marks can not be used on this separate resource area
     // the nesting level is incremented manually to prevent warnings.
     resource_area.nesting++;
    }
#   endif
   bottom = new oTableBuffer; 
   point.buffer = bottom; 
   offsets = NULL; 
 } else {
   point.buffer->next = new oTableBuffer;
   point.buffer = point.buffer->next;
   point.index = 0;
 }
}

void oTable::gc_mark_contents() {
  current.buffer= bottom;  current.index= 0;
  gc_mark_rest();
}

void oTable::gc_mark_rest() {
  for (; current.buffer;
         current.buffer= current.buffer->next, current.index= 0) {
    for (; current.index < object_table_size; current.index++) {
      if (   current.buffer == point.buffer
          && current.index == point.index) return;
      oopsOop p = (oopsOop) current.buffer->entries[current.index].obj;
#     if GENERATE_DEBUGGING_AIDS
        if (CheckAssertions) {
          LOG_EVENT2("Marking map of 0x%x index %d", p, current.index);
        }
#     endif
      p->map()->gc_mark_contents(p);
#     if GENERATE_DEBUGGING_AIDS
        if (CheckAssertions) {
          LOG_EVENT1("Marking referents of 0x%x", p);
        }
#     endif
      p->gc_mark_referents();
    }
  }
}

bool oTable::contains(memOopClass* p) {
  for (oTableBuffer* b = bottom; b; b = b->next) {
    if (b == point.buffer) {
      if ((oTableEntry*)p >= &b->entries[0] && 
          (oTableEntry*)p < &b->entries[point.index]) {
        return true;
      }
    } else if ((oTableEntry*)p >= &b->entries[0] && 
               (oTableEntry*)p < &b->entries[object_table_size]) {
        return true;
      }
  }
  return false;
}
      
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mapVtbls.hh"
# include "_mapVtbls.cpp.incl"

vtbls* Vtbls;

# define INIT_VTBLS_TEMPLATE(m)                                               \
    m CONC(m,_map);                                                           \
    if (CONC(m,_map).vtblMapType() != MAP_TYPE_NAME(m))                       \
      fatal1("mapType %s has wrong vtblMapType()", STR(MAP_TYPE_NAME(m)));    \
    vtbl_values[MAP_TYPE_NAME(m)] = (VtblPtr_t) CONC(m,_map).vtbl_value();

vtbls::vtbls() {
  FOR_ALL_MAP_TYPES(INIT_VTBLS_TEMPLATE);
}

# define READ_SNAPSHOT_VTBLS_TEMPLATE(m)                                      \
    OS::FRead_swap(&old_vtbl_values[MAP_TYPE_NAME(m)], sizeof(VtblPtr_t), file);   \
    old_are_correct = old_are_correct &&                                      \
    (old_vtbl_values[MAP_TYPE_NAME(m)] == vtbl_values[MAP_TYPE_NAME(m)]);

void vtbls::read_snapshot(FILE* file) {
  old_are_correct = !Memory->is_snapshot_other_endian;
  FOR_ALL_MAP_TYPES(READ_SNAPSHOT_VTBLS_TEMPLATE);
}

# define WRITE_SNAPSHOT_VTBLS_TEMPLATE(m)                                     \
    OS::FWrite(&vtbl_values[MAP_TYPE_NAME(m)], sizeof(VtblPtr_t), file);

void vtbls::write_snapshot(FILE* file) {
  FOR_ALL_MAP_TYPES(WRITE_SNAPSHOT_VTBLS_TEMPLATE);
}

# define TRANSLATE_VTBLS_TEMPLATE(m)                                          \
    if (old_vtbl_values[MAP_TYPE_NAME(m)] == old_vtbl_value)                  \
      return vtbl_values[MAP_TYPE_NAME(m)];

VtblPtr_t vtbls::translate(VtblPtr_t old_vtbl_value) {
  if (old_are_correct) return old_vtbl_value;
  FOR_ALL_MAP_TYPES(TRANSLATE_VTBLS_TEMPLATE);
  fatal("cannot find the old map type (please check APPLY_TO_VM_OOPS)");
  return 0;
}

# define FIND_VTBLS_TEMPLATE(m)                                               \
    if (vtbl_values[MAP_TYPE_NAME(m)] == value)                               \
      return true;

bool vtbls::contains(VtblPtr_t value) {
  FOR_ALL_MAP_TYPES(FIND_VTBLS_TEMPLATE);
  return false;
}

void mapVtbls_init() { Vtbls = new vtbls; }
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mapTable.hh"
# include "_mapTable.cpp.incl"

/*
  Invariants:

  1. If CanonicalizeMaps is false, the map table is empty and all maps''
     map_chains are empty.
  2. If CanonicalizeMaps is true and maps_are_canonical_in_snapshot is true, 
       for each map M such that M->should_canonicalize() is true,
     2A.  M is in the map table in the correct bucket.
     2B.  M is only in the map table once.
     2C.  There exist no other maps that are equivalent to M.
*/

// # define MONITOR_MAP_SETTING

# ifdef MONITOR_MAP_SETTING
  static int32 numMapsSet = 0;
  static int32 numMapsMerged = 0;
  static int32 numMapsAdded = 0;
# endif

mapTable::mapTable() {
  for (nmln* p = buckets;  p < &buckets[mapTableSize];  ++p)
    p->init();
  CanonicalizeMaps = true;
}

int32 mapTable::hash(int32 length, VtblPtr_t vtbl_value, slotDesc* slot) {
  long unsigned h= (long unsigned)vtbl_value;  
  
  for (slotDesc* end= slot + length; slot < end; ++slot) {
    h += slot->name->identity_hash();
    h += slot->data->identity_hash();
    h += slot->get_annotation()->identity_hash();
  }

  return h % mapTableSize;
}



nmln* mapTable::bucketFor(slotsMapDeps* m) {
  if (!CanonicalizeMaps  ||  !m->should_canonicalize()) return NULL;
  return &buckets[hash(m->length_slots(), m->vtbl_value(), m->slots())];
}

void mapTable::add(slotsMapDeps* m) {
  assert((m->map_chain()->isEmpty()),
         "map being added should not already point to other maps");  
  nmln* head = bucketFor(m);
  if (head == NULL) return;
  head->add(m->map_chain());
}


# ifdef MONITOR_MAP_SETTINGS
static int32 numEquivCalled = 0;
static int32 numCompareCalled = 0;
# endif

slotsMapDeps* mapTable::equivalent_map(slotsMapDeps* currMap) {
  nmln* head_nmln = bucketFor(currMap);
  if (head_nmln == NULL) return currMap;
  nmln* curr_nmln = head_nmln->next;
  
# ifdef MONITOR_MAP_SETTINGS
  numEquivCalled ++;
# endif
  
  // check if we have gotten to the end of the list
  while (curr_nmln != head_nmln) {
    // if it is an equivalent map, return it
    slotsMapDeps* aMap= map_from_map_chain(curr_nmln);
#   ifdef MONITOR_MAP_SETTINGS
      numCompareCalled++;
#   endif
    if (aMap->compare(currMap)) return aMap;
    curr_nmln = curr_nmln->next;
  }
  return NULL;
}


// return the canonical map for the map arg
// if none, canonicalize the arg and return it

slotsMapDeps* mapTable::canonical_map(slotsMapDeps* m) {
  slotsMapDeps* em= equivalent_map(m);
  if (em) {
#   ifdef MONITOR_MAP_SETTING
      ++numMapsMerged;
#   endif
    return em;
  }
  add(m);
#   ifdef MONITOR_MAP_SETTING
    ++numMapsAdded;
#   endif
  return m;
}

bool mapTable::verify_chain(nmln* head) {
  bool r = true;
  if (!CanonicalizeMaps) {
    if (head->notEmpty()) {
      error1("Bucket %d is not empty", head - buckets);
      r = false;
    }
    return r;
  }

  // first verify the list is okay
  r &= head->verify_list_integrity();        
  
  // now check on members of map list
  for (nmln* curr_nmln= head->next;
       curr_nmln != head;
       curr_nmln= curr_nmln->next) {          
    slotsMapDeps* currMap= map_from_map_chain(curr_nmln);
    if (currMap->has_code()) {
      error1("method map %#lx on map chain", (unsigned long)currMap);
      r = false;
    }
    if (!currMap->enclosing_mapOop()->verify()) {
      error1("nmln %#lx is part of a bogus map", (unsigned long)curr_nmln);
      r = false;
    }
    if (bucketFor(currMap) != head) {
      error1("map %#lx is chained off of the wrong bucket",
             (unsigned long)currMap);
      r = false;
    }
  }
  return r;
}

bool mapTable::verify() {
  bool r = true;
  for (fint i = 0; i < mapTableSize; i++) {
    r &= verify_chain(&buckets[i]);
  }

  // now, do quadratic search (with n log n compare in the inner loop!)
  // to ensure complete canonicalization
  for (fint i= 0; i < mapTableSize; i++) {
    nmln *head= &buckets[i];
    for (nmln *curr_nmln= head->next;
         curr_nmln != head;
         curr_nmln= curr_nmln->next) {          
      slotsMapDeps* currMap= map_from_map_chain(curr_nmln);

      nmln* n= curr_nmln->next;
      for (fint j= i; j < mapTableSize; j++) {
        nmln *head2= &buckets[j];
        if (i != j) n= head2->next;
        for (; n != head2; n= n->next) {
          slotsMapDeps* m= map_from_map_chain(n);
          if (currMap->compare(m)) {
            error3("map 0x%1x is same as map 0x%1x in bucket at 0x%1x", 
                   currMap, m, head);
            r = false;
          }
        }
      }
    }
  }
  return r;
}

void mapTable::print_bucket(int32 bucket_num) {
  nmln* head = &buckets[bucket_num];
  nmln* curr = head->next;
  slotsMapDeps* aMap;
  while (curr != head) {
    aMap = map_from_map_chain(curr);
    aMap->print_map();
    curr = curr->next;
  }
}

// if bug fixes are made here, also make them to the print_histogram
// routine in stringTable.c.
void mapTable::print_histogram() {
  const int32 results_length = 100;
  int32 results[results_length];
  
  // initialize results to zero
  int32 j;
  for ( j = 0; j < results_length; j++) {
    results[j] = 0;
  }

  int32 total = 0;
  int32 min_maps = 0;
  int32 max_maps = 0;
  int32 out_of_range = 0;
  lprintf("%8s %6s\n", "BUCKET", "COUNT");
  int32 i;
  for (i = 0; i < mapTableSize; i++) {
    nmln* head = &buckets[i];
    nmln* curr = head->next;
    int32 counter = 0;
    while (curr != head) {
      counter++;
      total++;
      curr = curr->next;
    }
    if (counter < results_length) {
      results[counter]++;
    } else {
      out_of_range++;
    }
    min_maps = min(min_maps, counter);
    max_maps = max(max_maps, counter);
    if (counter > 0) {
      lprintf("%8d %6d\n", i, counter);
    }
  }
  lprintf("%12s %6d\n", "TOTAL MAPS", total);
  lprintf("%8s %6d\n", "MINIMUM", min_maps);
  lprintf("%8s %6d\n", "MAXIMUM", max_maps);
  lprintf("%8s %6d\n", "AVERAGE", (total / mapTableSize));
  lprintf("%20s\n", "HISTOGRAM");
  lprintf("%14s %30s\n", "LENGTH", "NUMBER CHAINS THAT LENGTH");
  for (i = 0; i < results_length; i++) {
    if (results[i] > 0) {
      lprintf("%14d %20d\n ", i, results[i]);
    }
  }
  int32 line_length = 70;
  lprintf("%14s %30s\n", "LENGTH", "NUMBER CHAINS THAT LENGTH");
  for (i = 0; i < results_length; i++) {
    if (results[i] > 0) {
      lprintf("%4d", i);
      int32 j;
      for (j = 0; (j < results[i]) && (j < line_length);  j++) {
        lprintf("%1s", "*");
      }
      if (j == line_length) {
        lprintf("%1s", "+");
      }
      lprintf("\n");
    }
  }  
  lprintf("%30s %5d %20d\n", "NUMBER CHAINS LONGER THAN",
         results_length, out_of_range);
}


bool mapTable::verify_map(slotsMapDeps* m) {
  bool r = true;
  slotsMapDeps* em= equivalent_map(m);
  if (!CanonicalizeMaps) {
    if (m->map_chain()->notEmpty()) {
      error1("Map 0x%1x has non-empty map_chain", m);
      r = false;
    }
    return r;
  }
  if (em == NULL) {
    error1("Map 0x%1x is not in the map table", m);
    m->print_map();
    r = false;
  }
  if (em != m  &&  maps_are_canonical_in_snapshot) {
    error2("Map 0x%1x has a duplicate (0x%1x) in the map table", m, em);
     m->print_map();
    em->print_map();
    r = false;
  }
  return r;
}


bool mapTable::set_maps_are_canonical(bool f)
{
  if (CanonicalizeMaps == f) // no change
    return CanonicalizeMaps;

  if (f) { // must find and canonicalize all maps
    warning("In order to reenable map canonicalization, you must restart the VM and read in a snapshot");
    return false;
  } 

  // must clear maps out of table
  for (fint i = 0;  i < mapTableSize;  i++)
    buckets[i].init_chain();
  CanonicalizeMaps = false;
  return true;
}


void mapTable::set_maps_are_canonical_in_snapshot(bool f) {
  maps_are_canonical_in_snapshot = f;
}

void mapTable::add_map_from_snapshot(slotsMapDeps *m) {
  // an optimization
  if (maps_are_canonical_in_snapshot)
    add(m);
  else
    canonical_map(m);
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "spaceSize.hh"

# include "_spaceSize.cpp.incl"

unsigned long MaxHeapSize=        NMethodStart     - HeapStart;
unsigned long MaxNMethodSize=     StubsStart       - NMethodStart;
unsigned long MaxStubsSize=       DepsStart        - StubsStart;
unsigned long MaxDepsSize=        ScopesStart      - DepsStart;
unsigned long MaxScopesSize=      ZoneIDStart      - ScopesStart;
unsigned long MaxZoneIDSize=      CountStubIDStart - ZoneIDStart;
unsigned long MaxCountStubIDSize= UseCountStart    - CountStubIDStart;
unsigned long MaxUseCountSize=    (caddr_t)(512*M) - UseCountStart;


#define MAKE_SPACE_SLOT_TEMPLATE(s)                                         \
        spaceObj= spaceObj->copy_add_slot(new_string(STR(s)),               \
                                          obj_slotType,                     \
                                          as_smiOop(sizes->s),              \
                                          Memory->slotAnnotationObj,        \
                                          true);                            \

static oop get_space_sizes(spaceSizes *sizes) {
  oop spaceObj = create_slots((slotList*)NULL);
  APPLY_TO_SPACE_SIZES(MAKE_SPACE_SLOT_TEMPLATE);
  return spaceObj;
}

oop get_default_space_sizes_prim(oop rcvrIgnored) {
  Unused(rcvrIgnored);
  return get_space_sizes(&(Memory->default_sizes)); }


#define FILL_IN_SPACE_SLOT_TEMPLATE(s)                                  \
        p= proto->get_slot_data_address_if_present(STR(s), inObj);      \
        if (p) {                                                        \
          if (!inObj) goto unassignable;                                \
          Memory->store(p, as_smiOop(Memory->current_sizes.s));         \
        }                                                               \


oop get_current_space_sizes_prim(oop rcvrIgnored, slotsOop proto, void *FH) {
  Unused(rcvrIgnored);
  oop *p;
  bool inObj;
  APPLY_TO_SPACE_SIZES(FILL_IN_SPACE_SLOT_TEMPLATE);
  return proto; 
 unassignable:
  prim_failure(FH, UNASSIGNABLESLOTERROR);
  return NULL;
}


#define SET_FROM_DEFAULT_TEMPLATE(s) \
        s= CONC(default_ , s);

void spaceSizes::set_from_defaults()
{
  APPLY_TO_SPACE_SIZES(SET_FROM_DEFAULT_TEMPLATE);
}


#define ROUND_TO_IDEALIZED_PAGE_SIZE_TEMPLATE(s) \
        s= roundTo(s, idealized_page_size);

void spaceSizes::cleanup()
{
  APPLY_TO_SPACE_SIZES(ROUND_TO_IDEALIZED_PAGE_SIZE_TEMPLATE);
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotIterator.hh"
# include "_slotIterator.cpp.incl"

#if GENERATE_DEBUGGING_AIDS
SlotIterator *SlotIterator::blockIterator= NULL;
fint SlotIterator::nActive= 0;
#endif

slotDesc block_slots[2];

void SlotIterator::init(Map *m) {
#if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions) {
    nActive++;
    if ((!bootstrapping && !GCInProgress) || ScavengeInProgress)  {
      // if compacting, map might not have been unmarked
      assert_map(m->enclosing_mapOop(), "not a map");
    }
  }
#endif
  if (m->is_block()) {
#if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      assert(blockIterator==NULL, "conflict in use of block_slots");
      blockIterator= this;
    }
#endif
    sd= block_slots;
    end= block_slots + sizeof(block_slots)/sizeof(slotDesc);
    ((blockMap*)m)->setSlots();
  } else {
    sd= m->slots();
    end= sd + m->length_slots();
  }
}

// If you change the names or number of blocks slots, 
// change blockMap::find_slot too.
void slotIterator_init() {
  // block parent slot
  block_slots[0].name= VMString[PARENT];
  block_slots[0].type= parent_map_slotType;
  block_slots[0].data= Memory->blockTraitsObj;
  block_slots[0].annotation= Memory->slotAnnotationObj;

  // used and reused for value, value:, value:With:, etc..
  block_slots[1].name= stringOop(badOop);
  block_slots[1].type= map_slotType;
  block_slots[1].data= badOop;
  block_slots[1].annotation= Memory->slotAnnotationObj;
}

#define SLOT_ITERATOR_OOPS_DO(template)                         \
  template(&block_slots[0].name);                               \
  template(&block_slots[0].data);                               \
  template(&block_slots[0].annotation);                         \
  if (block_slots[1].name != stringOop(badOop))                 \
    template(&block_slots[1].name);                             \
  if (block_slots[1].data != badOop)                            \
    template(&block_slots[1].data);                             \
  template(&block_slots[1].annotation);

void slotIterator_scavenge_contents() { 
  SLOT_ITERATOR_OOPS_DO(SCAVENGE_TEMPLATE); }

void slotIterator_gc_mark_contents() { 
  SLOT_ITERATOR_OOPS_DO(MARK_TEMPLATE); }

void slotIterator_gc_unmark_contents() { 
  SLOT_ITERATOR_OOPS_DO(UNMARK_TEMPLATE); }

void slotIterator_switch_pointers(oop from, oop to) { 
  SLOT_ITERATOR_OOPS_DO(SWITCH_POINTERS_TEMPLATE); }

bool slotIterator_verify() { 
  bool verify_result = true;
  SLOT_ITERATOR_OOPS_DO(VERIFY_TEMPLATE);
  if (   block_slots[0].name != VMString[PARENT]
      || block_slots[0].type != parent_map_slotType
      || block_slots[0].data != Memory->blockTraitsObj
      || block_slots[0].annotation != Memory->slotAnnotationObj
      || block_slots[1].type != map_slotType
      || block_slots[1].annotation != Memory->slotAnnotationObj) {
    error("block slots corrupted");
    verify_result = false;
  }
  return verify_result;
}

/* Sun-$Revision: 30.9 $ */ 

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "enumeration.hh"
# pragma implementation "preserve.hh"

# include "_enumeration.cpp.incl"

static enumeration* package_enumeration;

void enumeration_list::add_more(oop obj, enumeration_list*& head,
                                fint& totCnt) {
  enumeration_list* p = new enumeration_list(my_enumeration);
  p->next = head;
  head = p;
  p->add(obj, head, totCnt);
}

void enumeration_list::oops_do(oopsDoFn f) {
  package_enumeration = my_enumeration;
  for (enumeration_list* list = this;  list;  list = list->next) {
    oop* p = list->contents;
    oop* e = p + list->count;
    while (p < e) {
      f(p++);
      if (!my_enumeration->is_ok()) return;
    }
  }
}

static void unmarkThisObject(oop* p) {(*p)->unmarkThisObject();}

void package_enumeration_result(oop* p) {
  oop obj = *p;
  obj->unmarkThisObject();
  obj= oop(obj->as_mirror_or_fail());
  if (obj == failedAllocationOop)
    package_enumeration->set_error(ErrorCodes::vmString_prim_error(OUTOFMEMORYERROR));
  else
    Memory->store(package_enumeration->resultp++, obj);
}

void package_enumeration_maps(oop* p) {
  (*p)->unmarkThisObject();
  *(package_enumeration->maps_p++) = *p;
}

void enumeration::set_error(markOop e) {
  objs->oops_do(unmarkThisObject);
  maps->oops_do(unmarkThisObject);
  error_code = e;
}

void enumeration::pack_result() {
  resultVector= Memory->objVectorObj->cloneSize(obj_count, CANFAIL);
  if (oop(resultVector) == failedAllocationOop) {
    set_error(ErrorCodes::vmString_prim_error(OUTOFMEMORYERROR));
    return;
  }
  resultp = resultVector->objs();
  objs->oops_do(package_enumeration_result);
}

void enumeration::pack_maps() {
  // Make one extra element for sentinel
  maps_array = maps_p = NEW_RESOURCE_ARRAY(oop, map_count + 1);
  maps->oops_do(package_enumeration_maps);
}


static bool compute_limit(oop              in_limit, 
                          unsigned long  &out_limit,
                          VMStringsIndex &failIndex) {
  if (in_limit == infinityOop) {
    out_limit = (unsigned long)AllBits;
    return true;
  }
  if (!in_limit->is_smi()) {
    failIndex = BADTYPEERROR;
    return false;
  }
  smi i = smiOop(in_limit)->value();
  if (i < 0) {
    failIndex = PRIMITIVEFAILEDERROR;
    return false;
  }
  out_limit = i;
  return true;
}

// Check all elements are mirrors and count the number of vframeOops, 
//  stringOops, and assignmentOops. 
// Returns false if found a non-mirror element.
static bool valid_targets(oop* oops, unsigned int n,
                          smi& num_live_vframes,
                          smi& num_assignments) {
  num_live_vframes = 0;
  num_assignments  = 0;
  
  for (oop* end = oops + n;  oops < end;  ++oops) {
    oop obj = *oops;
    if (!obj->is_mirror())
      return false;
    oop r = mirrorOop(obj)->reflectee();
    if (r->is_assignment())
      num_assignments++;
    else if (r->is_vframe()  &&  vframeOop(r)->is_live())
      num_live_vframes++;
  }
  return true;
}

bool enumeration::is_target(oop t) {
  for (int j = 0;  j < num_targets;  j++)
    if (t == targetp[j])
      return true;
  return false;
}

// Enumerate references
// given a vector of mirrors on objects, return a vector of references
//  to the objects
//
// Vframes are tricky, since blocks also refer to them via an
//  integer-tagged scope pointer.
// In order to handle them we copy and expand the vector so that the
//   first part of the vector holds the scope pointers, then the next part
//  holds the vframe oops, the the rest holds the non-vframe objects
//  (not mirrors)
//   -- LB & DU  12/91
oop referencesEnumeration::enumerate_vector_references(objVectorOop vector, oop limit) {
  unsigned long cLim;
  VMStringsIndex failIndex;
  if (!compute_limit(limit, cLim, failIndex))
    return ErrorCodes::vmString_prim_error(failIndex);
  
  smi num_targets = vector->length();
  smi num_live_vframes;
  smi num_assignments;
  
  if(num_targets == 0)
    return ErrorCodes::vmString_prim_error(ENUMERATIONTARGETERROR);
  
  if (!valid_targets(vector->objs(), num_targets,
                     num_live_vframes, num_assignments))
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  
  ResourceMark m;
  
  // Calculate targets for the enumeration.
  oop* targetp = NEW_RESOURCE_ARRAY(oop, num_targets + num_live_vframes
                                          - num_assignments);
  
  int current_vframe = 0;
  int current_obj    = 2 * num_live_vframes;
  
  for (int j = 0;  j < num_targets;  j++) {
    oop       obj = mirrorOop(vector->obj_at(j))->reflectee();
    vframeOop vfo = vframeOop(obj);
    if (obj->is_vframe()  &&   vfo->is_live())  {
      // 2 targets for each vframeOop.
      targetp[current_vframe + num_live_vframes] = obj;
      targetp[current_vframe++]                  = vfo->enumeration_target();
    } else if (!obj->is_assignment())
      targetp[current_obj++] = obj;
  }
  
  enumeration* e = new referencesEnumeration(targetp,
                                             num_targets - num_assignments,
                                             num_live_vframes, 
                                             num_assignments > 0, 
                                             cLim);
  e->enumerate();
  return e->is_ok() ? oop(e->resultVector) : oop(e->error_code);
}

void referencesEnumeration::enumerate() {
  // collect objects with slots in them, and maps of clone families
  // containing the desired slots.
  
  if (has_assignments) {
    // For each map in Memory this->filter_map is called
    Memory->enumerate_maps(this);
    if (!is_ok()) return;
  }
  
  // For each match in Memory this->filter_match is called
  Memory->enumerate_matches(this);
  if (!is_ok()) return;
  
  processes->enumerate_references(this);
  
  // collect family members
  if (map_count > 0) {
    pack_maps();
    Memory->enumerate_families(this);
    processes->enumerate_families(this);
    if (!is_ok()) return;
  }
  
  pack_result(); 
}

void referencesEnumeration::filter_match(oopsOop obj,
                                         oop*    matching_cell,
                                         smi     targetNo) {
  if (targetNo < num_live_vframes)   
    consider_vframe(obj, matching_cell, targetNo);
  else
    consider_obj   (obj, matching_cell, targetNo);
}

void referencesEnumeration::consider_vframe(oopsOop  obj,
                                            oop*     matching_cell,
                                            smi      targetNo) {
  if (              obj ->is_block()
      &&   blockOop(obj)->scope(true) ==  (frame*) *matching_cell
      &&   blockOop(obj)->desc()      ==  vframeOop(targetp[targetNo +
                                                            num_live_vframes])
      ->descOffset())
    add_obj(obj);
}

void referencesEnumeration::consider_obj(oopsOop  obj,
                                         oop*     matching_cell,
                                         smi      targetNo) {
  Unused(targetNo);
  if (obj ->is_map()) {
    if (mapOop(obj)->map_addr()->matching_slots_data(*matching_cell))
      add_map(mapOop(obj)->map_addr());
  } else if (obj->is_enumerable(matching_cell))
    add_obj(obj);
}

void referencesEnumeration::filter_map(mapOop obj) {
  if (obj->map_addr()->has_assignment_slots())
    add_map(obj->map_addr());
}


// Enumerate implementors.
oop implementorsEnumeration::enumerate_vector_implementors(objVectorOop vector, oop limit) {
  unsigned long cLim;
  VMStringsIndex failIndex;
  if (!compute_limit(limit, cLim, failIndex))
    return ErrorCodes::vmString_prim_error(failIndex);
  
  smi num_targets = vector->length();
  
  if (num_targets == 0)
    return ErrorCodes::vmString_prim_error(ENUMERATIONTARGETERROR);    
  
  // Count the number of strings in vector.
  // For 1-arg keywords, include the non-keyword variant to catch
  // assignable slots
  smi num_strings = 0;
  for (int i= 0; i < num_targets; i++) {
    oop el= vector->obj_at(i);
    if (el->is_string()) {
      stringOop s= stringOop(el);
      num_strings += s->length() > 0 && s->is_1arg_keyword() ? 2 : 1;
    }
  }

  if (num_strings == 0)
    return ErrorCodes::vmString_prim_error(ENUMERATIONTARGETERROR);    

  ResourceMark m;
  
  // Calculate targets for the enumeration.
  // Put the potentially spurious ones (the unary form of a 1-arg keyword)
  // at the end.
  oop* targetp = NEW_RESOURCE_ARRAY(oop, num_strings);
  
  int current_obj= 0;
  int current_poss_assignment= num_strings - 1;
  for (int j = 0; j < num_targets; j++) {
    oop obj = vector->obj_at(j);
    if (obj->is_string()) {
      targetp[current_obj++]= obj;
      stringOop s= stringOop(obj);
      if (s->length() > 0 && s->is_1arg_keyword())
        targetp[current_poss_assignment--]= s->unary();
    }
  }
  assert(current_obj == current_poss_assignment + 1,
         "mistake in counting names");
  
  enumeration* e = new implementorsEnumeration(targetp,
                                               num_strings,
                                               current_obj,
                                               cLim);
  e->enumerate();
  return e->is_ok() ? oop(e->resultVector) : oop(e->error_code);
}

void implementorsEnumeration::enumerate() {
  if (num_targets <= 0) return;
  
  Memory->enumerate_matches(this);
  if (!is_ok()) return;
  
  if (map_count > 0) {
    pack_maps();
    Memory->enumerate_families(this);
    processes->enumerate_families(this);
    if (!is_ok()) return;
  }
  
  pack_result();  
}

void implementorsEnumeration::filter_match(oopsOop obj,
                                           oop*    matching_cell,
                                           smi     targetNo) {
  if (obj->is_map()) {
    Map *map= mapOop(obj)->map_addr();
    if (targetNo < poss_assignments_index) {
      if (map->matching_slots_name(*matching_cell))
        add_map(map);
    } else if (map->matching_slots_assignment_name(*matching_cell))
      add_map(map);
  }
}


// Enumerate all objects in the self world.
oop allObjEnumeration::enumerate_all_objs(oop limit) {
  unsigned long cLim;
  VMStringsIndex failIndex;
  if (!compute_limit(limit, cLim, failIndex))
    return ErrorCodes::vmString_prim_error(failIndex);
  
  ResourceMark m;
  
  enumeration* e = new allObjEnumeration(cLim);
  e->enumerate();
  return e->is_ok() ? oop(e->resultVector) : oop(e->error_code);
}

void allObjEnumeration::enumerate() {
  Memory->enumerate_all_objs(this);
  // Cannot do a verify now, the objects are marked.
  if (!is_ok()) return;
  
  add_obj(as_floatOop(0.0));
  add_obj(as_smiOop(0));

  pack_result();
}

/* Sun-$Revision: 30.21 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "generation.hh"
# pragma implementation "generation_inline.hh"

# include "_generation.cpp.incl"


void generation::print()
{
  lprintf(" total capacity: %d\n", capacity());
  lprintf(" used: %d bytes for oops, %d bytes for bytes, total %d\n",
         oops_used() * oopSize, bytes_used(), used());
  lprintf(" address boundaries: 0x%x, 0x%x\n",
         low_boundary, high_boundary);
}


newGeneration::newGeneration(int32 &eden_size, int32 &surv_size, FILE *snap) {

  map_list= NULL;
  
  if (snap) {
    eden_space= new newSpace("eden", eden_size, snap);
    from_space= new newSpace("from", surv_size, snap);
      to_space= new newSpace("to",   surv_size, snap);
  }
  int32 new_size= eden_size + surv_size + surv_size;
  assert(is_idealized_page_multiple(new_size), "new size mismatch");

  // Need to allocate an extra page because the search operations
  // in sun4.enum.s can walk back a few words before their starting point.
  // This should be fixed sometime by recoding those ops.
  new_size += idealized_page_size;
  char *new_spaces= OS::allocate_idealized_page_aligned(
                           new_size, "new space",
                           HeapStart - idealized_page_size);
  new_spaces += idealized_page_size;
  new_size -= idealized_page_size;

  // must start on a card boundary
  assert(int(new_spaces) % card_size == 0,
         "page size not a multiple of card size");

  char *be=  new_spaces;
  char *bs1= be  + eden_size;
  char *bs2= bs1 + surv_size;
  char *es2= bs2 + surv_size;

  if (snap) {
    eden_space->read_snapshot(snap, be, bs1);
    bool fromAfterTo= from_space->old_objs_bottom > to_space->old_objs_bottom;
    (fromAfterTo ? to_space : from_space)->read_snapshot(snap, bs1, bs2);
    (fromAfterTo ? from_space : to_space)->read_snapshot(snap, bs2, es2);
  } else {
    eden_space= new newSpace("eden", eden_size, be );
    from_space= new newSpace("from", surv_size, bs1);
    to_space  = new newSpace("to",   surv_size, bs2);
  }

  eden_space->next_space = from_space;
  from_space->next_space =   to_space;
    to_space->next_space =       NULL;

   low_boundary= new_spaces;
  high_boundary= new_spaces + new_size;
}

void newGeneration::prepare_for_scavenge()
{
  to_space->prepare_for_scavenge();
}

bool newGeneration::scavenge_contents() {
  return to_space->scavenge_contents();
}

# define APPLY_TO_YOUNG_SPACES2(template,member)                      \
    template(eden_space,member)                                       \
    template(from_space,member)                                       \
    template(to_space,member)                                                 

#define ACCUMULATE_TEMPLATE(s,member) { sum += s->member(); }

#define ACCUMULATE_NEW(member)                          \
int32 newGeneration::member()                           \
{                                                       \
  int32 sum= 0;                                         \
  APPLY_TO_YOUNG_SPACES2(ACCUMULATE_TEMPLATE,member);   \
  return sum;                                           \
}

ACCUMULATE_NEW(capacity)
ACCUMULATE_NEW(oops_used)
ACCUMULATE_NEW(bytes_used)
ACCUMULATE_NEW(oops_free)
ACCUMULATE_NEW(used)
ACCUMULATE_NEW(bytes_free)

bool newGeneration::has_map(slotsMapDeps *m) {
  for (MapList *p= map_list;  p != NULL;  p= p->next)
    if (p->map == m)
      return true;
  return false; }

void newGeneration::adjust_maps() {
  int n= 0, nDead= 0, nSurv= 0; // gather stats of length of list
  MapList *mapl;
  for ( MapList **prevp= &map_list; 
        mapl= *prevp,  mapl != NULL; 
        n++) {
    slotsMapDeps *map= mapl->map;
    mapOop m= map->enclosing_mapOop();
    if (m->is_forwarded()) {
      slotsMapDeps* oldMap= map;
      m= mapOop(m->forwardee());
      map= (slotsMapDeps*)m->map_addr();
      map->forward_map(oldMap);      // shift dependency links
    }
    assert_map(m, "new map list contains a non-map");

    enum { 
      was_tenured,  died,  is_alive_and_young 
    } what_happened = 
        (char*)m >= high_boundary 
           ? was_tenured 
           : ( Memory->should_scavenge(m) ? died : is_alive_and_young );

    switch (what_happened) {

     case is_alive_and_young: // survived to to-space: adjust list
      nSurv++;
      mapl->map= map;         // update list 
      prevp= &(mapl->next);   // goto next list elem
      break;

    case died:                // died in eden- or from-space: delete map
      nDead++;
      map->delete_map();
      // and FALL THROUGH to remove link from list

    case was_tenured:  // leave map alive, but remove from list entirely
      MapList *next= mapl->next;  
      delete mapl;  
      *prevp= next;
      break;
    }
  }
  if (PrintNewMapListScavengeStats)
    lprintf("%d new maps, %d died, %d survived, %d tenured\n",
           n, nDead, nSurv, n-nDead-nSurv);
}


bool newGeneration::verify_new_maps() {
  int32 count = 0;
  for (MapList* mp= map_list;  mp;  mp= mp->next) {
    mapOop p = mp->map->enclosing_mapOop();
    if (count++ >= 100000) {
      error1("figure 6 bug in new maps list around map 0x%lx", p);
      return false;
    }
    if (! p->verify_oop()) {
      lprintf("\tof map on new maps list.\n");
      return false;
    }
    if (!p->map_addr()->can_have_dependents()) {
      error1("mapOop 0x%lx on new maps list can't have deps", p);
      return false;
    } 
    if (! p->is_new()) {
      error1("mapOop 0x%lx on new maps list isn't new", p);
      return false;
    }
  }
  return true;
}


void newGeneration::switch_pointers(oop from, oop to) {
  eden_space->switch_pointers(from, to);
  from_space->switch_pointers(from, to);
    to_space->switch_pointers(from, to);
}


bool newGeneration::objs_contains(void* p)
{
  return
      eden_space->objs_contains(p)
  ||  from_space->objs_contains(p)
  ||    to_space->objs_contains(p);
}


#define OMIT2(t,m) m(t)

void newGeneration::print()
{
  lprintf(" New generation\n");
  generation::print();
  APPLY_TO_YOUNG_SPACES2(OMIT2,SPACE_PRINT_TEMPLATE);
}

bool newGeneration::verify()
{
  bool r = true;
  if (   eden_space->next_space != from_space
      || from_space->next_space !=   to_space
      || to_space->next_space != NULL) {
    error("misconnnected spaces in new gen");
    r = false;
  }

  if (Memory->is_verify_opt('e') || Memory->is_verify_opt('n'))
    r = eden_space->verify()  && r;
  if (Memory->is_verify_opt('s') || Memory->is_verify_opt('n')) {
    r = from_space->verify() && r;
    r =   to_space->verify() && r;
  }
  return r;
}

void newGeneration::write_snapshot(FILE* file)
{
  eden_space->write_snapshot_header(file);
  from_space->write_snapshot_header(file);
    to_space->write_snapshot_header(file);
  eden_space->write_snapshot(file);
  from_space->write_snapshot(file);
    to_space->write_snapshot(file);
}


# undef FOR_EACH_OLD_SPACE
// this version used with old_gen
// ensure that you surround the call with {} to prevent s leaking out!
#define FOR_EACH_OLD_SPACE(s) \
  for (oldSpace *s= first_space;                                \
       s != NULL;                                               \
       s= s->next_space)


void oldGeneration::sorted_space_list(oldSpace *sp[],
                                      int (*cmp)(oldSpace**, oldSpace**))
{
  unsigned n= 0;
  {FOR_EACH_OLD_SPACE(s) sp[n++]= s;}
  assert(n == nSpaces, "old space count is wrong");
  qsort(sp, n, sizeof(sp[0]), (int(*)(const void *, const void *))cmp);
}

int addr_cmp(oldSpace **s1, oldSpace **s2)
{
  char *s1start= (*s1)->spaceStart();
  char *s2start= (*s2)->spaceStart();
  if (s1start < s2start) return -1;
  else if (s1start > s2start) return 1;
  else return 0;
}

static int most_used_space_first(oldSpace **s1, oldSpace **s2) {
  int u1= (*s1)->used(), u2= (*s2)->used();
  return u1 > u2 ? -1 : u1 < u2 ? 1 : addr_cmp(s1, s2); }

static int most_free_space_last(oldSpace **s1, oldSpace **s2) {
  int u1= (*s1)->oops_free(), u2= (*s2)->oops_free();
  return u1 < u2 ? -1 : u1 > u2 ? 1 : addr_cmp(s1, s2); }

// sort the linked list of spaces by the cmp_fn
void oldGeneration::resort_spaces(int (*cmp_fn)(oldSpace**, oldSpace**))
{
  // I can be called a lot if there is just one space,
  // and recreate_old_bars is slow.
  if (nSpaces == 1)
    return;
    
  ResourceMark rm;
  oldSpace **sp= NEW_RESOURCE_ARRAY( oldSpace*, nSpaces );
  sorted_space_list(sp, cmp_fn);
  first_space= sp[0];
  int n;
  for (n= 0;  n < nSpaces-1;  ++n)
    sp[n]->next_space= sp[n+1];
  last_space= sp[n];
  sp[n]->next_space= NULL;
  
  TrackObjectHeapInMonitor::recreate_old_bars();
}


oldGeneration::oldGeneration(int32 initial_size, int32 reserved_amt) {
  VM_reserved_mem= reserved_amt;
  setLowSpaceThreshold(VM_reserved_mem * 2);

  // make adjacent to new spaces
  old0= first_space= last_space= tenuring_space=
    new oldSpace("old0", initial_size, Memory->new_gen->high_boundary);

  assert( old0->spaceStart() >= Memory->new_gen->high_boundary,
          "old must be after new for scavenging");
          
  if (initial_size == 0)
    fatal("Couldn't allocate initial old space -- not enough swap space?");

  top_of_old_space= old0->spaceEnd();

  reserve_space= first_space;

   low_boundary= first_space->spaceStart();
  high_boundary= first_space->spaceEnd();

  nSpaces= 1;
  update_cached_free();
}


oldGeneration::oldGeneration(FILE* snap, int32 initial_size,
                             int32 reserved_amt) {
  oldSpace *s;

  VM_reserved_mem= reserved_amt;
  setLowSpaceThreshold(VM_reserved_mem * 2);

  OS::FRead_swap(&nSpaces, sizeof(nSpaces), snap);
  assert( nSpaces < 1000, "Snapshot corrupted, unbelievable number of spaces");

  oldSpace *prev= NULL;
  for (unsigned n= 0; n < nSpaces; n++) {
    char *name= new char[10];
    sprintf(name, "old%d", n);
    s= new oldSpace(const_cast<const char*>(name), snap);
    if (prev)
      prev->next_space= s;
    else
      first_space= s;
    prev= s;
  }
  last_space= s;

  caddr_t old_start= Memory->new_gen->high_boundary;
  char *old_heap= OS::allocate_idealized_page_aligned(initial_size, "old0", old_start);
  
  // I don't know why the Unix code does this, but
  //  the MAC always falls back on malloc--I don't know
  //  how else to do it -- dmu
  if (OS::is_directed_allocation_supported()  &&  caddr_t(old_heap) != old_start)
    fatal("Couldn't allocate old space contiguous with new space");

  assert(caddr_t(old_heap) >= old_start, "at least assume sequentiality");
  
  top_of_old_space= old_heap + initial_size;
  
  char *bottom= low_boundary=  old_heap;
  char *top=    high_boundary= old_heap + initial_size;
  
  {FOR_EACH_OLD_SPACE(ss) {
    ss->read_snapshot(snap, bottom, top);
    bottom= (char*)(ss->objs_top);
    top=    (char*)(ss->bytes_bottom);
  }}
}

void oldGeneration::coalesce_spaces()
{
  reserve_space= NULL; 
  
  if (nSpaces == 1) {
    old0= tenuring_space= first_space;
    update_cached_free();
    return;
  }
    
  first_space->objs_top=     last_space->objs_top;
  first_space->bytes_bottom= last_space->bytes_bottom;
  oldSpace *s= first_space->next_space;
  first_space->next_space= NULL;
  
  while (s) {
    oldSpace *n= s->next_space;
    delete s;
    s= n;
  }

  old0= tenuring_space= last_space= first_space;
  nSpaces= 1;
  update_cached_free();
  reselect_reserve_space();
}

bool oldGeneration::objs_contains(void* p)
{
  {FOR_EACH_OLD_SPACE(s) { if (s->objs_contains(p)) return true; }}
  return false;
}

bool oldGeneration::contains(void* p)
{
  {FOR_EACH_OLD_SPACE(s) { if (s->contains(p)) return true; }}
  return false;
}

#define ACCUMULATE_OLD(member)                          \
int32 oldGeneration::member()                           \
{                                                       \
  int32 sum= 0;                                         \
  {FOR_EACH_OLD_SPACE(s) { sum += s->member(); }}       \
  return sum;                                           \
}

ACCUMULATE_OLD(capacity)
ACCUMULATE_OLD(oops_used)
ACCUMULATE_OLD(bytes_used)
ACCUMULATE_OLD(oops_free)
ACCUMULATE_OLD(used)
ACCUMULATE_OLD(bytes_free)

void oldGeneration::check_for_end_of_low_space() {
  lowOnSpace= cached_free <= lowSpaceThreshold;
  if (!lowOnSpace) TrackObjectHeapInMonitor::reserve_changed();
}

void oldGeneration::update_caches(bool postScavenge) {
  update_cached_free();
  if (postScavenge)
    check_for_low_space();
  else {
    reselect_reserve_space();
    check_for_end_of_low_space();
  }
}


void oldGeneration::reset_tenuring_order()
{
  resort_spaces(most_free_space_last);
  tenuring_space = first_space;
}


void oldGeneration::prepare_for_scavenge()
{
  reset_tenuring_order();
  FOR_EACH_OLD_SPACE(s) {
    s->prepare_for_scavenge();
  }
}


space* oldGeneration::tenuring_space_for(fint size, fint bsize) 
{
  if (tenuring_space->would_fit(size, bsize))
    return tenuring_space;
  
  FOR_EACH_OLD_SPACE(s) {
    if (s->would_fit(size, bsize))
      return tenuring_space= s;
  }
  fatal2("out of space for tenuring %d oops %d bytes", size, bsize);
  return NULL;
}



bool oldGeneration::scavenge_promotions() {
  bool r = false;
  FOR_EACH_OLD_SPACE(s) {
    r = s->scavenge_promotions() || r;
  }
  return r;
}


void oldGeneration::switch_pointers(oop from, oop to) {
  if (from->is_new())
    {FOR_EACH_OLD_SPACE(space) space->switch_pointers_by_card(from, to);}
  else
    {FOR_EACH_OLD_SPACE(space) space->switch_pointers(from, to);}
}
    
    
void oldGeneration::cleanup_after_scavenge()
{
  reset_tenuring_order();
  update_caches(true);
}

// Expand the old space by size bytes. 
// Returns the amount actually allocated (0, maybe, if expansion isn't
// possible; or more than asked for, due to rounding).
int oldGeneration::expand(int32 size)
{
  EventMarker em("expanding heap by %d", (void*)size);
  assert(size >= 0, "negative expansion?");
  
  if (OS::is_directed_allocation_supported()
  &&  top_of_old_space + size > HeapStart + MaxHeapSize) 
    return 0;

  char *name= new char[10];
  sprintf(name, "old%d", nSpaces);
  oldSpace *s= new oldSpace(const_cast<const char*>(name), size, // modifies size for amount allocated
                            top_of_old_space); 

  if (size == 0) 
    delete s;
  else {
    if (s->spaceStart() < Memory->new_gen->high_boundary)
      fatal("allocation of old space before new space");
    
    top_of_old_space= caddr_t(s->spaceEnd());

    // it's OK for the new oldSpace to go between the new spaces and
    // existing old space (as there is already a card bytemap for this
    // region), or between two old spaces (for the same reason), or after
    // the last old space (in which case we will extend the card byte map).
    
    nSpaces++;
    append_space(s);
    update_caches(false);
      
    char *sStart= s->spaceStart();
    char *sEnd  = s->spaceEnd();
    if (sStart <  low_boundary)  low_boundary= sStart;
    if (sEnd   > high_boundary) high_boundary= sEnd;
    Memory->remembered_set->fixup(sStart, sEnd);
    Memory->current_sizes.old_size= capacity();
    TrackObjectHeapInMonitor::recreate_old_bars();
  }
  return size;
}

// Find oldSpace with most room.  Start with s.
oldSpace* oldGeneration::biggest_free_space(oldSpace *s)
{
  oldSpace *big= s;
  int32 room= big->bytes_free();
  for (;;) {
    s= s->next_space;
    if (s == NULL) return big;
    int32 t= s->bytes_free();
    if (t > room) {
      room= t;
      big= s;
    }
  }
}


// sort the spaces into least occupied last
// Actually, most used first -- dmu 7/04
void oldGeneration::prepare_for_compaction()
{
  resort_spaces(most_used_space_first);
}

  

void oldGeneration::append_space(oldSpace *last)
{
  last_space->next_space= last;
  last_space= last;
  last->next_space= NULL;
}  

void oldGeneration::reselect_reserve_space()
{
  reserve_space= biggest_free_space(first_space);
  if (!reserve_space->reserveFree())
    reserve_space= NULL;
}



static oop* HeapAllocateFailed(){
  lprintf("\n**** Heap allocation failed! ****\n");
  lprintf("This indicates that either you continued to allocate objects\n"
          "despite warnings, or, if you did not get any warnings, the Self\n"
          "low space handler is broken.\n");
  fatal("out of memory");
  return NULL;
}


// Unify with tenuring code?? -- dmu 7/04
oop* oldGeneration::alloc_objs_and_bytes(fint size, fint bsize, char*& bytes, bool mustAllocate) {

  oop *p = NULL;

  oldSpace* s;
  for ( s = tenuring_space;   p == NULL  &&  s != NULL;            s = s->next_space)
    if (s != reserve_space)
      p = s->alloc_objs_and_bytes(size, bsize, bytes, mustAllocate);

  for ( s = first_space;      p == NULL  &&  s != tenuring_space;  s = s->next_space)
    if (s != reserve_space)
      p = s->alloc_objs_and_bytes(size, bsize, bytes, mustAllocate);
      
  if ( p == NULL  &&  reserve_space != NULL )
    p = reserve_space->alloc_objs_and_bytes(size, bsize, bytes, mustAllocate);

  if (nSpaces > 1  &&  s == reserve_space)
    reset_tenuring_order(); // sorts & sets tenuring_space
  else
    tenuring_space = s;
    
  if ( p == NULL ) {
    if ( mustAllocate )  HeapAllocateFailed();
    return NULL;
  }
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  (p == (oop*)catchThisOne || bytes == (char*)catchThisOne)) {
      warning1("oldGeneration::alloc_objs_and_bytes caught 0x%lx",
               catchThisOne);
    }
  # endif
  
  cached_free -= (size + bsize)*oopSize;
  check_for_low_space();
  return p;
}


void oldGeneration::print()
{
  lprintf(" Old generation\n");
  generation::print();
  APPLY_TO_OLD_SPACES(SPACE_PRINT_TEMPLATE);
  lprintf(" VM reserve: %d  Low space threshold: %d\n", VM_reserved_mem,
          lowSpaceThreshold);
}

bool oldGeneration::verify()
{
  int n= 0;
  bool r = true;
  oldSpace *p;
  {FOR_EACH_OLD_SPACE(s) {
    n++;
    p= s;
  }}
  if (n != nSpaces) { error("Wrong nSpaces in old gen"); r = false; }
  if (p != last_space) { error("Wrong last_space in old gen"); r = false; }
  if (reserve_space  &&  !reserve_space->reserveFree()) {
     error("reserve_space doesn't have reserve"); r = false; }
  if (cached_free != bytes_free()) { error("cached_free in old gen wrong"); r = false; }
  if (cached_free < VM_reserved_mem) { error("VM reserve missing"); r = false; }
  if (lowOnSpace != (cached_free < lowSpaceThreshold)) {
    warning("lowOnSpace wrong");
    r = false;
  }
  bool verify_result = true;
  APPLY_TO_OLD_SPACES(SPACE_VERIFY_TEMPLATE);
  return r && verify_result;
}


void oldGeneration::set_write_only_old0() {
  write_only_old0= true;
  FOR_EACH_OLD_SPACE(s) {
    if (s != old0 && s->used() != 0)
      write_only_old0= false;
  }
}

void oldGeneration::write_snapshot(FILE* file)
{
  if (write_only_old0) {

    const unsigned n= 1;
    OS::FWrite(&n, sizeof(n), file);
    old0->write_snapshot_header(file);
    old0->write_snapshot(file);

  } else {

    OS::FWrite(&nSpaces, sizeof(nSpaces), file);
    
    // sort spaces by address
    oldSpace **sp= NEW_RESOURCE_ARRAY( oldSpace*, nSpaces);
    sorted_space_list(sp, addr_cmp);
    unsigned n= 0;
    for (n= 0; n < nSpaces; n++)
      sp[n]->write_snapshot_header(file);
    for (n= 0; n < nSpaces; n++)
      sp[n]->write_snapshot(file);
  }
}

void oldGeneration::record_new_pointers()
{
  FOR_EACH_OLD_SPACE(s) ((oldSpace*)s)->record_new_pointers();
}

smi set_memory_low_space_threshold_prim(oop rcvrIgnored, smi newLST, void *FH)
{
  Unused(rcvrIgnored);
  if (newLST < Memory->old_gen->get_VM_reserved_mem()) {
    failure(FH, "Threshold set below VM reserve");
    return (smi)NULL;
  }
  smi oldLST= Memory->old_gen->getLowSpaceThreshold();
  Memory->old_gen->setLowSpaceThreshold(newLST);
  TrackObjectHeapInMonitor::reserve_changed();
  return oldLST;
}

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "vmStrings.hh"
# include "_vmStrings.cpp.incl"

stringOop VMString[LAST_VM_STRING];

void vmStrings_init() {
  // called during universe::universe
# define VMStrings_Init_Template(x,s)                                         \
    VMString[x] = new_string(s);
    
    VMStrings_DO(VMStrings_Init_Template)
}

# define VMStrings_LOOP(name,template)                                        \
  void CONC(VMStrings_,name) {                                        \
    for (stringOop* p = &VMString[0];                                 \
      p < &VMString[LAST_VM_STRING];                                          \
      p ++)                                                                   \
      template                                                                \
}


static void read_one(stringOop* p, FILE* file) {
# ifdef had_to_debug_this_once
  int w = ftell(file);
# endif

  OS::FRead_swap(p, oopSize, file);
# ifdef had_to_debug_this_once

  lprintf("VString: %d 0x%x 0x%x\n", p - VMString, *p, w);
# endif
  if (*p == NULL)
    fatal1("entry %d of VMString was read as Null", p - VMString);
}

static void write_one(stringOop* p, FILE* file) {
# ifdef had_to_debug_this_once
  lprintf("VString: %d 0x%x 0x%x\n", p - VMString, *p, ftell(file));
# endif

  OS::FWrite(p, oopSize, file);
}
  


VMStrings_LOOP(oops_do(oopsDoFn f),OOPS_DO_TEMPLATE((oop*)p,f))
VMStrings_LOOP(gc_mark_contents(),MARK_TEMPLATE(p))
VMStrings_LOOP(gc_unmark_contents(),UNMARK_TEMPLATE(p))
VMStrings_LOOP(switch_pointers(oop from, oop to),SWITCH_POINTERS_TEMPLATE(p))
VMStrings_LOOP( read_snapshot(FILE* file),read_one(p, file);)
VMStrings_LOOP(write_snapshot(FILE* file),write_one(p, file);)
VMStrings_LOOP(relocate(),RELOCATE_TEMPLATE(p))
VMStrings_LOOP(verify(bool& r),if (oop(*p)->verify_oop() && !(*p)->is_string()) { error1("entry %#lx of vm string table isn't a string", *p); r = false; })
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "search.hh"
# include "_search.cpp.incl"

# undef  MIN
# define MIN(a,b) ((a) < (b) ? (a) : (b) )
# define CHUNK_SIZE (Vectorfind_max_chunk_size/sizeof(int32))



// searches [bottom, top), uses sentinel at *top, but that is OK
//   given how it is called -- dmu

static void search_chunk(int32 bottom[],  int32* top, 
                         int32 targets[], int32 num_targets,
                         match_func f) {
  assert( bottom  <=  top, "size of chunk is negative");
  
  int32 *sentinel_cell = top;
  int32 saved_value    = *sentinel_cell; // save to undo sentinel;
  int32 hit_num;
  
  int32  start_target = 0;
  bool   proceed      = true;
  while (start_target < num_targets) {
    int32 used_targets = MIN(Vectorfind_max_targets, num_targets-start_target);
    
    for (int32* objp = bottom;
         objp < sentinel_cell && proceed;
         ++objp) {

      *sentinel_cell = targets[start_target]; // place sentinel
      objp = vectorfind_next_match(objp,
                                   &targets[start_target],
                                   used_targets,
                                   &hit_num);

      assert( *objp == targets[hit_num+start_target],
              "Wrong target for match");
      assert( bottom <= objp && objp <= top, "objp outside area");

      *sentinel_cell = saved_value; // restore sentinel_cell

      if (objp == sentinel_cell) // just found sentinel
        break;
      
      if (!f( objp, hit_num))
        return;
    }
    start_target += used_targets;
  }
}


// searches in [bottom, top)
// uses sentinel, but only within [bottom, top)

void search_area(int32* bottom,  int32* top,
                 int32* targets,  int32 num_targets,
                 match_func f) {
  if (bottom >= top )
    return;
  
  if (num_targets <= Vectorfind_max_targets)
    search_chunk(bottom, top-1, targets, num_targets, f);
  else {
    int32 *chunk_bottom  = bottom;
    int32 *chunk_top     = MIN(bottom + CHUNK_SIZE, top-1);
    while ( chunk_bottom < top-1 ) {
      search_chunk(chunk_bottom, chunk_top, targets, num_targets, f);
      chunk_bottom = chunk_top;
      chunk_top    = MIN(chunk_bottom + CHUNK_SIZE, top-1);
    }
  }
  // Since the cell top-1 has been checked (sentinel) targets
  //  are checked against contents of top-1.
  for (int32 i = 0;  i < num_targets;  ++i) {
    if (*(top-1) == targets[i]) {
      if (!f( top-1, i)) return;
    }
  }
}

# if  ! HAVE_PLATFORM_SEARCH_FUNCTIONS
  // simple C implementation

  int32 Vectorfind_max_targets = 1 << 24;       // value isn't really important
  int32 Vectorfind_max_chunk_size = 64 * 1024;  // ditto

  int32* vectorfind_next_match(int32* start,
                               int32* targets, int32 num_targets, 
                               int32* hit_num) {
    for ( ; ; ++start) {
      for (int32 index = 0; index < num_targets; ++index) {
        if (*start == targets[index]) { *hit_num = index; return start; }
      }
    }
  }
  
  extern "C" {
    // enumeration:   // simple C implementation

    oop* find_this_object(oop* middle) {
      for ( ; !(*middle)->is_mark(); --middle) {} return middle; }

    oop* find_next_object(oop* middle) {
     for ( ++middle; !(*middle)->is_mark(); ++middle) {} return middle; }

    oop* find_prior_reference(oop* middle, oop target) {
     for ( ; (*middle) != target; --middle) {} return middle; }
  }

# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "oopClosures.hh"

# include "_oopClosures.cpp.incl"

void OopBlockAdjuster::do_oop(oop* p) { 
  blockOop b = (blockOop)*p;
  if (b->is_block() && b->scope(true) == olds) { 
    b->setScope(news);
    b->identity_hash();   /* for debugging; see remap() */
  }
}  
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stringTable.hh"
# include "_stringTable.cpp.incl"

# define FOR_ALL_ENTRIES(entry) \
  for (entry = firstBucket(); entry <= lastBucket(); entry ++)

# define FOR_ALL_STRING_ADDR(bucket, var, code)                         \
    { if (bucket->is_string()) {                                        \
         var = (stringOop*) bucket; code;                               \
      } else {                                                          \
        for (stringTableLink* l = bucket->get_link(); l; l = l->next) { \
          var = &l->string; code;                                       \
        }                                                               \
      }                                                                 \
    }


int32 hash(const char* name, int32 len) {
  // hash on at most 32 characters, evenly spaced
  int32 increment;
 
  if (len < 32) {
    increment = 1;
  } else {
    increment = len >> 5;
  }
 
  // hashpjw from Dragon book (ASU p. 436), except increment differently
 
  assert(BitsPerByte * BytesPerWord == 32, "assumes 32-bit words");
  long unsigned h = 0;
  long unsigned g;
  const char* s = name;
  const char* end = s + len;
  for (; s < end; s = s + increment) {
    h = (h << 4) + (long unsigned) *s;
    g = h & 0xf0000000;
    if (g) h ^= g | (g >> 24);
  }
  return h;

}

stringTable::stringTable() {
  for (int32 i = 0; i < string_table_size; i ++) buckets[i].clear();
  free_list = first_free_link = end_block = NULL;
}

stringOop stringTable::basic_add(const char *name, int32 len, int32 hashValue,
                                 bool mustAllocate) {
  stringOop str= make_string(name, len, mustAllocate);
  if (oop(str) != failedAllocationOop) basic_add(str, hashValue);
  return str; }

stringOop stringTable::lookup(const char* name, int32 len, bool mustAllocate) {
  int32 hashValue = hash(name, len);
  stringTableEntry* bucket = bucketFor(hashValue);
  if (bucket->is_string()) {
    if (bucket->get_string()->equals(name, len)) return bucket->get_string();
    return basic_add(name, len, hashValue, mustAllocate);
  } else {
    if (!bucket->get_link())
      return basic_add(name, len, hashValue, mustAllocate);
    for (stringTableLink* l = bucket->get_link(); l; l = l->next) {
      if (l->string->equals(name, len)) {
        return l->string;
      }
    }    
  }
  return basic_add(name, len, hashValue, mustAllocate);
}

void stringTable::add(stringOop s) {
  assert(s->is_string(),
         "adding something that's not a string to the string table");
  assert(s->is_old(), "all strings should be tenured");
  int32 hashValue = hash(s->bytes(),  s->length());
  basic_add(s, hashValue);
}

stringOop stringTable::basic_add(stringOop s, int32 hashValue) {
  assert(s->is_string(),
         "adding something that's not a string to the string table");
  assert(s->is_old(), "all strings should be tenured");
  // Canonical strings must have a hash value (for _IdentityHash) that
  // is a pure function of the chars in the string.  Otherwise, their
  // hash value would change when they are discarded by a GC and re-
  // created later.
  assert(s->mark()->hash() == no_hash, "should not have a hash yet");
  s->set_mark(s->mark()->set_hash(hashValue));
  assert(s->mark()->hash() != no_hash, "should have a hash now");

  stringTableEntry* bucket = bucketFor(hashValue);
  stringTableLink*  old_link;
  if (bucket->is_string()) {
    old_link = Memory->string_table->new_link(bucket->get_string());
  } else {
    old_link = bucket->get_link();
  }
  bucket->set_link(Memory->string_table->new_link(s, old_link));
  return s;
}

void stringTable::switch_pointers(oop from, oop to) {
  if (! from->is_string()) return;
  assert(to->is_string(),
         "cannot replace a string with a non-string");

  findSlotCache.clear();
  stringTableEntry* e;
  FOR_ALL_ENTRIES(e) {
    stringOop* addr;
    FOR_ALL_STRING_ADDR(e, addr, SWITCH_POINTERS_TEMPLATE(addr));
  }
}

void stringTable::gc_mark_contents() {
  // throw out unreachable strings
  stringTableEntry* e;
  FOR_ALL_ENTRIES(e) { 
    if (e->is_string()) {
      if (!e->get_string()->is_gc_marked()) e->clear();
      else MARK_TEMPLATE((stringOop*) e);
    } else {
      stringTableLink* l = e->get_link(), **p = (stringTableLink**) e; 
      while (l) {
        if (!l->string->is_gc_marked()) {
          // unreachable; remove from table
          stringTableLink* d = l;
          *p = l = l->next;
          d->next = NULL;
          Memory->string_table->delete_link(d);
        } else {
          MARK_TEMPLATE(&l->string);
          p = &l->next;
          l = *p;
        }
      }
      if (e->length() == 1) {
        // Compact the bucket
        stringTableLink* l_ = e->get_link();
        e->set_string(l_->string);
        Memory->string_table->delete_link(l_);
      }
    }
  }
}

void stringTable::gc_unmark_contents() {
  stringTableEntry* e;
  FOR_ALL_ENTRIES(e) {
    stringOop* addr;
    FOR_ALL_STRING_ADDR(e, addr, UNMARK_TEMPLATE(addr));
  }
}

#ifdef UNUSED
void stringTableEntry::deallocate() {
  if(!is_string() && get_link()) 
    Memory->string_table->delete_link(get_link());
}
#endif

bool stringTableEntry::verify(fint i) {
  bool flag = true;
  if (is_string()) {
    if (!get_string()->is_string()) {
      error1("entry 0x%lx in string table isn't a string", get_string());
      flag = false;
    }
  } else {
    if (get_link()) flag = get_link()->verify(i);
  }
  return flag;
}

bool stringTable::verify() {
  bool r = findSlotCache.verify();
  for (fint i = 0; i < string_table_size; i ++)
    if (!buckets[i].verify(i)) {
      lprintf("\tof bucket %ld of string table\n", long(i));
      r = false;
    }
  return r;
}

void stringTable::read_snapshot(FILE* file) {
  stringTableEntry* e;
  // Read entries
  FOR_ALL_ENTRIES(e) {
    int32 len = 0;
    OS::FRead_swap(&len, sizeof(int32), file);
    if (len == 0) e->clear();
    else {
      stringOop s;
      OS::FRead_swap(&s, oopSize, file);
      if (len == 1) e->set_string(s);
      else {
        stringTableLink* head = Memory->string_table->new_link(s);
        stringTableLink* tail = head;
        for (; len > 1; len --) {
          OS::FRead_swap(&s, oopSize, file);
          tail->next = Memory->string_table->new_link(s);
          tail       = tail->next;
        }
        e->set_link(head);
      }
    }
  }
}

void stringTable::write_snapshot(FILE* file) {
  stringTableEntry* e;
  FOR_ALL_ENTRIES(e) {
    stringOop* addr;
    int32 len = e->length();
    OS::FWrite(&len, sizeof(int32), file);
    FOR_ALL_STRING_ADDR(e, addr, OS::FWrite(addr, oopSize, file));
  }
}

void stringTable::relocate() {
  stringTableEntry* e;
  FOR_ALL_ENTRIES(e) {
    stringOop* addr;
    FOR_ALL_STRING_ADDR(e, addr, RELOCATE_TEMPLATE(addr));
  }
}

bool stringTableLink::verify(fint i) {
  bool flag = true;
  for (stringTableLink* l = this; l; l = l->next) {
    if (! l->string->is_string()) {
      error1("entry 0x%lx in string table isn't a string", l->string);
      flag = false;
    } else if (hash(l->string->bytes(), l->string->length()) 
               % string_table_size != i) {
      error1("entry 0x%lx in string table has wrong hash value", l->string);
      flag = false;
    } else if (!l->string->is_old()) {
      error1("entry 0x%lx in string table isn't tenured", l->string);
      flag = false;
    }
  }
  return flag;
}

fint stringTableEntry::length() {
  if (is_string()) return 1;
  if (!get_link()) return 0;
  fint count = 0;
  for (stringTableLink* l = get_link(); l; l = l->next) count ++;
  return count;
}

stringTableLink* stringTable::new_link(stringOop s, stringTableLink* n) {
  stringTableLink* res;
  if (free_list) {
    res = free_list;
    free_list = free_list->next;
  } else {
    const fint block_size = 500;
    if (first_free_link == end_block) {
      first_free_link = NEW_C_HEAP_ARRAY(stringTableLink, block_size);
      end_block = first_free_link + block_size;
    }
    res = first_free_link++;
  } 
  res->string = s;
  res->next   = n;
  return res;
}

void stringTable::delete_link(stringTableLink* l) {
  // Add the link to the freelist
  stringTableLink* end = l;
  while(end->next) end = end->next;
  end->next = free_list;
  free_list = l;
}

// much of this comes from the print_histogram routine in mapTable.c,
// so if bug fixes are made here, also make them in mapTable.c.
void stringTable::print_histogram() {
  const int32 results_length = 100;
  int32 results[results_length], i, j;
  
  // initialize results to zero
  for (j = 0; j < results_length; j++) {
    results[j] = 0;
  }

  int32 total = 0;
  int32 min_strings = 0;
  int32 max_strings = 0;
  int32 out_of_range = 0;
  lprintf("%8s %6s\n", "BUCKET", "COUNT");
  for (i = 0; i < string_table_size; i++) {
    stringTableEntry curr = buckets[i];
    int32 counter = curr.length();
    total += counter;
    if (counter < results_length) {
      results[counter]++;
    } else {
      out_of_range++;
    }
    min_strings = min(min_strings, counter);
    max_strings = max(max_strings, counter);
  }
  lprintf("%14s %6d\n", "TOTAL STRINGS", total);
  lprintf("%8s %6d\n", "MINIMUM", min_strings);
  lprintf("%8s %6d\n", "MAXIMUM", max_strings);
  lprintf("%8s %6d\n", "AVERAGE", (total / string_table_size));
  lprintf("%20s\n", "HISTOGRAM");
  lprintf("%14s %30s\n", "LENGTH", "NUMBER CHAINS THAT LENGTH");
  for (i = 0; i < results_length; i++) {
    if (results[i] > 0) {
      lprintf("%14d %20d\n ", i, results[i]);
    }
  }
  int32 line_length = 70;    
  lprintf("%14s %30s\n", "LENGTH", "NUMBER CHAINS THAT LENGTH");
  for (i = 0; i < results_length; i++) {
    if (results[i] > 0) {
      lprintf("%4d", i);
      for (j = 0; (j < results[i]) && (j < line_length);  j++) {
        lprintf("%1s", "*");
      }
      if (j == line_length) {
        lprintf("%1s", "+");
      }
      lprintf("\n");
    }
  }  
  lprintf("%30s %5d %20d\n", "NUMBER CHAINS LONGER THAN",
         results_length, out_of_range);
}
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "rSet.hh"
# pragma implementation  "rSet_inline.hh"

# include "_rSet.cpp.incl"

rSet::rSet() {
   low_boundary = Memory->new_gen->low_boundary;
  high_boundary = Memory->old_gen->high_boundary;
  clear(); 
  Set_Byte_Map_Base(byte_for(NULL));
}

void* rSet::operator new(size_t size) {
  assert((int32(Memory->new_gen->low_boundary) & (card_size - 1)) == 0,
         "new must start at card boundary");
  assert((int32(Memory->old_gen->low_boundary) & (card_size - 1)) == 0,
         "old must start at card boundary");
  assert((int32(Memory->old_gen->high_boundary) & (card_size - 1)) == 0,
         "old must end at card boundary");
  
  int32 bmsize =
    (Memory->old_gen->high_boundary - Memory->new_gen->low_boundary)
      / card_size;
  bmsize += byte_map_grain; // for next_zero_byte below
  return AllocateHeap(size + bmsize, "rSet");
}

// copy the bits from an older, smaller bitmap, add area [start,end)
rSet::rSet(rSet *old, char *start, char *end)
{
   low_boundary= Memory->new_gen->low_boundary;
  high_boundary= Memory->old_gen->high_boundary;
  char *old_low=  old->low_boundary;
  char *old_high= old->high_boundary;
  Set_Byte_Map_Base(byte_for(NULL));
  memcpy(byte_for(old_low),
         old->byte_for(old_low),
         old->byte_for(old_high) - old->byte_for(old_low));
  clear(byte_for(start), byte_for(end));
  delete old;
}  

// optimized out the wazoo, this is called after an object is copied
//  wholesale to (maybe) old, OR after gc to reset all bytes

void rSet::record_multistores(oop* start, oop* end) {
  assert(card_size_in_oops == 32, "wired in");
  register char *bound= Memory->new_gen->boundary();
  if (Memory->new_gen->is_new(as_memOop(start), bound))
    return;
  
  register oop x;
# define VISIT(w, next_card)                                                  \
    x = *(w);                                                                 \
    if (x->is_mem()                                                           \
        && Memory->new_gen->is_new(memOop(x), bound)) {                       \
         *byte_for(w) = 0;                                                    \
         next_card;                                                           \
      }                                                                       \
    
  
  // p = first card boundary on or after start
  register oop* p = (oop*)roundTo(int32(start), card_size);
  
  // but dont go past end!
  if (end < p) {
    p = end;
  }
  
  // if first card is partial, we can skip it if it is ALREADY marked
  if (p != start  &&  *byte_for(start) != 0) {
    switch (p - start) {
#     define S(i) case i: VISIT(p - i,  break)
     default: ShouldNotReachHere();                    S(31) S(30)
       S(29) S(28) S(27) S(26) S(25) S(24) S(23) S(22) S(21) S(20)
       S(19) S(18) S(17) S(16) S(15) S(14) S(13) S(12) S(11) S(10)
       S( 9) S( 8) S( 7) S( 6) S( 5) S( 4) S( 3) S( 2) S( 1)
#      undef S
    }
  }
  
  // middle cards, starting at p
  oop* end_card = rSet::card_for(end);
  for( ;
      p < end_card;
      p += card_size_in_oops) {
#   define S(i) VISIT(p + i,  continue)
      S( 0) S( 1) S( 2) S( 3) S( 4) S( 5) S( 6) S( 7) S( 8) S( 9) 
      S(10) S(11) S(12) S(13) S(14) S(15) S(16) S(17) S(18) S(19)
      S(20) S(21) S(22) S(23) S(24) S(25) S(26) S(27) S(28) S(29)
      S(30) S(31)
#   undef S
  }
  
  // end card, if partial, else we already did it
  if (p < end  &&  *byte_for(p) != 0) {
    switch (end - p) {
#     define S(i) case i: VISIT(end - i,  break)
     default: ShouldNotReachHere();                    S(31) S(30)
       S(29) S(28) S(27) S(26) S(25) S(24) S(23) S(22) S(21) S(20)
       S(19) S(18) S(17) S(16) S(15) S(14) S(13) S(12) S(11) S(10)
       S( 9) S( 8) S( 7) S( 6) S( 5) S( 4) S( 3) S( 2) S( 1)
#     undef S
    }
  }
# undef VISIT
}


// called to find roots and start scavenge
// first time, start will be start of old, after it will be
// wherever we need to start
// We dont attempt to fixup the bytes for partial cards--
//  it doesnt seem like a win.

// If AllowOffsetCheckStores is true:
// The compiler can generate stores at constant positive offsets off of 
// a base pointer, but mark the update at the card of the base pointer rather
// than the stored word.  This saves an add instruction with each store check,
// but can move the mark in the previous card (the maximum error in offset
// allowed is the size of a card, so the mark will be only one card off).
// I.e. when storing into [r1+11], the generated code will mark the card for
// address r1, not r1+11. Accordingly, for each marked card, we also scan the
// following card in case that's the real modified card.        -Urs

bool rSet::scavenge_contents(oop* start, oop* end) {
  assert(card_size_in_oops == 32, "wired in");
  char* bound = Memory->new_gen->boundary();
  
  bool changed = false;
  
  oop  x;
  char new_byte;
  
# define VISIT(w)                                                             \
    x = *(w);                                                                 \
    if (x->is_mem()) {                                                        \
      /* CSE the is_mem tests from the following is_new and scavenge calls */ \
      memOop mx = memOop(x);                                                  \
      if (Memory->new_gen->is_new(mx, bound)) {                               \
        *(w) = mx = memOop(mx->scavenge());                                   \
        changed = true;                                                       \
        if (new_byte != 0 && Memory->new_gen->is_new(mx, bound)) new_byte = 0;\
      }                                                                       \
    }
  
  oop* p = rSet::card_for(start);
  assert(p == start,
         "this routine assumes start on card boundary (start of old space)");
  
  // first & middle cards:
  char* cp = byte_for(p);
  char* end_byte = byte_for(end);
  for (;;) {
    cp = rSet::next_zero_byte(cp, end_byte);
    if (cp >= end_byte) break;
    assert(*cp == 0, "next_zero_byte failed");
    char oc;
    do {
      p = oop_for(cp);
      new_byte = AllBits;
#     define S(i)  VISIT(p + i)
        S( 0) S( 1) S( 2) S( 3) S( 4) S( 5) S( 6) S( 7) S( 8) S( 9)
        S(10) S(11) S(12) S(13) S(14) S(15) S(16) S(17) S(18) S(19)
        S(20) S(21) S(22) S(23) S(24) S(25) S(26) S(27) S(28) S(29)
        S(30) S(31)
#     undef S
      oc = *cp;
      *cp = new_byte;
      cp++;
    } while (oc == 0 && AllowOffsetCheckStores && cp < end_byte);
  }
  
  // last card, if partial
  assert(cp == byte_for(end), "just checking");
  p = card_for(end);
  new_byte = AllBits;
  switch (end - p) {
#   define S(i) case i: VISIT(end - i)
   default: ShouldNotReachHere();                    S(31) S(30)
     S(29) S(28) S(27) S(26) S(25) S(24) S(23) S(22) S(21) S(20)
     S(19) S(18) S(17) S(16) S(15) S(14) S(13) S(12) S(11) S(10)
     S( 9) S( 8) S( 7) S( 6) S( 5) S( 4) S( 3) S( 2) S( 1)
   case 0: break;
#   undef S
  }
  *cp = new_byte;
  return changed;
}


void rSet::clear(char *start, char *end) {
  OS::set_bytes(start, end, AllBits);
}


bool rSet::verify(bool postScavenge) {
  assert(card_size_in_oops == 32, "wired in");
  assert(byte_map_grain == 8 * BytesPerWord, "wired in");
  bool flag = true;
  if (Byte_Map_Base() != byte_map_base()) {
    error2("rSet: ByteMapBaseReg corrupted (%#lx instead of %#lx)",
           Byte_Map_Base(), byte_map_base());
    flag = false;
  }
  flag &= check_saved_byte_map_base();
  
  {FOR_EACH_OLD_SPACE(space) {
    oop* start = space->oopsStart();
    oop* end   = space->oopsEnd(); 
    oop* end_card = rSet::card_for(end);
    char should_be;
    char* bound = Memory->new_gen->boundary();
  
# define CHECK(w)                                                             \
    if (byte_for(w)[0] != should_be &&                                        \
        (!AllowOffsetCheckStores ||                                           \
         byte_for(w)[-1] != should_be || rSet::card_for(w) == start)) {       \
      if (!should_be) {                                                       \
        error3("byte at 0x%x for card at 0x%x is %d should be 0\n",           \
               byte_for(w), w, (int32)*byte_for(w));                                 \
        flag = false;                                                         \
      } else if (postScavenge) {                                              \
        warning3("byte at 0x%x for card at 0x%x is %d should be ~0\n",        \
                 byte_for(w), w, (int32)*byte_for(w));                               \
        flag = false;                                                         \
      } else if (false) {                                                     \
        /* false out the warning for now */                                   \
        warning4("byte at 0x%x for card at 0x%x is %d should be ~0 %s\n",     \
                 byte_for(w), w, (int32) *byte_for(w), "(some are normal)");  \
      }                                                                       \
    }
  
    oop *p;
    for (p= start; p < end_card; p += card_size_in_oops) {
      assert(p == rSet::card_for(p), "should be a card boundary");
#     define S(i) if (Memory->new_gen->is_new(p[i], bound)) should_be = 0; else
      S( 0) S( 1) S( 2) S( 3) S( 4) S( 5) S( 6) S( 7) S( 8) S( 9)
      S(10) S(11) S(12) S(13) S(14) S(15) S(16) S(17) S(18) S(19)
      S(20) S(21) S(22) S(23) S(24) S(25) S(26) S(27) S(28) S(29)
      S(30) S(31)
      should_be = AllBits;
#     undef S
      CHECK(p)
      }
    assert(p == end_card,  "just checking");
    assert(rSet::card_for(p) == rSet::card_for(end), "just checking");
  
    should_be = AllBits;
    switch (end - p) {
#     define S(i) case i: \
       if (Memory->new_gen->is_new(end[-i], bound)) { should_be = 0;  break; }
      S(31) S(30)
      S(29) S(28) S(27) S(26) S(25) S(24) S(23) S(22) S(21) S(20)
      S(19) S(18) S(17) S(16) S(15) S(14) S(13) S(12) S(11) S(10)
      S( 9) S( 8) S( 7) S( 6) S( 5) S( 4) S( 3) S( 2) S( 1)
      case 0:  break;
      default: ShouldNotReachHere();
#     undef S
    }
    CHECK(end)
    }}
    
  return flag;
# undef CHECK
}

// fast search for next_zero_byte starting at cp, upto (but not including) end
// returns end if no zero found
// exploits extra 8 words at end of map

char* rSet::next_zero_byte(char *cp, char* end) {
  // put zero sentinel at end before scanning
  char old_end = *end;
  *end = 0;

  int32* wp = (int32*)roundTo(int32(cp), sizeof(int32));

  char* cwp = (char*) wp;
  switch (cwp - cp) {
   default: ShouldNotReachHere();
   case 3: if ((cwp)[-3] == 0) { *end = old_end; return cwp - 3; }
   case 2: if ((cwp)[-2] == 0) { *end = old_end; return cwp - 2; }
   case 1: if ((cwp)[-1] == 0) { *end = old_end; return cwp - 1; }
   case 0: break;
  }

  const int32 ones = AllBits;
  
  for (  ;
       (wp[0] & wp[1] & wp[2] & wp[3] & wp[4] & wp[5] & wp[6] & wp[7]) == ones;
       wp += 8)
    ;
  
  if      (wp[0] != ones) wp += 0;
  else if (wp[1] != ones) wp += 1;
  else if (wp[2] != ones) wp += 2;
  else if (wp[3] != ones) wp += 3;
  else if (wp[4] != ones) wp += 4;
  else if (wp[5] != ones) wp += 5;
  else if (wp[6] != ones) wp += 6;
  else                    wp += 7;
  
  cwp = (char*) wp;
  if (cwp[0] == 0) { *end = old_end; return cwp + 0; }
  if (cwp[1] == 0) { *end = old_end; return cwp + 1; }
  if (cwp[2] == 0) { *end = old_end; return cwp + 2; }
  if (cwp[3] == 0) { *end = old_end; return cwp + 3; }
  
  ShouldNotReachHere();
  return 0;
}

// new old space added; fix the cards
void rSet::fixup(char *start, char *end)
{
  if (end > high_boundary)
    Memory->remembered_set= new rSet(this, start, end);
  else
    clear(byte_for(start), byte_for(end));
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotList.hh"
# include "_slotList.cpp.incl"

slotList* slotList::add(stringOop name, slotType type, oop contents, oop anno)
{
  if (this == NULL) {
    // empty list
    return new slotList(name, type, contents, anno);
  }

  if (name->cmp(this->name()) < 0)
    return new slotList(name, type, contents, anno, this);

  slotList* prev;
  slotList* l= this;
  do {
    prev= l;
    l= l->next;
  } while (l && l->name()->cmp(name) < 0);
  // insert
  prev->next= new slotList(name, type, contents, anno, l);
  return this;
}


fint slotList::length() {
  fint c = 0;
  for (slotList* l = this; l; l = l->next) c ++;
  return c;
}

fint slotList::obj_count() {
  fint c = 0;
  for (slotList* l = this; l; l = l->next) {
    if (l->type()->is_obj_slot()) c ++;
  }
  return c;
}

void slotList::oops_do(oopsDoFn f) {
  for (slotList* l= this; l; l= l->next) {
    (*f)((oop*)&l->desc.name);
    (*f)(      &l->desc.data);
    (*f)(      &l->desc.annotation);
  }
}
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "space.hh"
# pragma implementation "space_inline.hh"

# include "_space.cpp.incl"

void space::init_space(const char *n, int32 &size, char *bottom) {
  size = roundTo(size, idealized_page_size);
  name = n;
  objs_bottom = objs_top = (oop*)bottom;
  bytes_bottom = bytes_top = (oop*)(bottom + size);
}


space::space(const char *n, char *bottom, char *top) {
  assert(is_idealized_page_multiple(int(bottom)) && is_idealized_page_multiple(int(top)),
         "invalid space boundaries");
  name = n;
  objs_bottom = objs_top = (oop*)bottom;
  bytes_bottom = bytes_top = (oop*)top;
}


space::space(const char *n, FILE *snap) {
  name = n;

  OS::FRead_swap(&old_objs_bottom, oopSize, snap);
  OS::FRead_swap(&old_objs_top,    oopSize, snap);
  if (old_objs_top < old_objs_bottom)
    fatal2("snapshot format error reading %s: objs_top < objs_bottom (position 0x%x)", 
           name, ftell(snap));
  OS::FRead_swap(&old_bytes_bottom, oopSize, snap);
  OS::FRead_swap(&old_bytes_top,    oopSize, snap);
  if (old_bytes_top < old_bytes_bottom)
    fatal1("snapshot format error reading %s: bytes_top < bytes_bottom", name);
}


static void snap_error(const char *msg) { fatal1("snapshot format error: %s", msg); }

void space::read_snapshot(FILE* snap, char *bottom, char *top)
{
   objs_bottom = (oop*)bottom;
      objs_top = objs_bottom + (old_objs_top - old_objs_bottom);
     bytes_top = (oop*)top;
  bytes_bottom = bytes_top - (old_bytes_top - old_bytes_bottom);

  if (objs_top >= bytes_bottom)
    snap_error("space too small to read in snapshot");
               
  if (page_aligned) {
    char    *objs_page_end = OS::idealized_page_end(   objs_top);
    char *bytes_page_start = OS::idealized_page_start(bytes_bottom);
    if (fseek(snap, roundTo(ftell(snap), idealized_page_size), SEEK_SET) < 0)
      snap_error("seek failed");
    if (bytes_page_start <= objs_page_end)           
      OS::Mmap(objs_bottom, bytes_top, snap);
    else {
      OS::Mmap(objs_bottom, objs_page_end, snap);
      OS::Mmap(bytes_page_start, bytes_top, snap);
    }
  } else {
    OS::FRead_mem_swap(objs_bottom, objs_top, snap);
    OS::FRead_mem(bytes_bottom, bytes_top, snap);
  }    
}

oop space::get_allocation_vector() {
  oop vec= Memory->objVectorObj->cloneSize(3, CANFAIL);
  if (vec != failedAllocationOop) {
    vec->obj_at_put(0, as_smiOop(oops_used() * oopSize));
    vec->obj_at_put(1, as_smiOop(bytes_used()));
    vec->obj_at_put(2, as_smiOop(capacity()));
  }
  return vec;
}


newSpace::newSpace(const char *nm, int32 &size, FILE *snap) : space(nm, snap) {
  int32 snap_size = old_size_bytes();
  if (size < snap_size) {
    warning1("Default size for %s too small for snapshot; ignoring", nm);
    size= snap_size;
  }
  assert((size & (oopSize-1)) == 0,
         "space size must be a multiple of oop size");
}


oop* newSpace::alloc_more_objs_and_bytes(fint size, fint bsize, char*& bytes,
                                         bool mustAllocate) {
  Memory->need_scavenge();
  return next_space
         ?  next_space->     alloc_objs_and_bytes(size, bsize, bytes, mustAllocate)
         :  Memory->old_gen->alloc_objs_and_bytes(size, bsize, bytes, mustAllocate);
}


void newSpace::prepare_for_scavenge() {
  objs_scavenge_point= oopsStart();
}


bool newSpace::scavenge_contents() {
  if (objs_top == objs_scavenge_point) return false;
  assert(objs_scavenge_point < objs_top, "scavenging past top");

  oop *so_far = objs_scavenge_point; // for performance

  do {
    SCAVENGE_TEMPLATE(so_far); // advances objs_top when an object is moved
  } while (++so_far < objs_top);

  objs_scavenge_point = so_far;
  return true;
}


oldSpace::oldSpace(const char *nm, int32 &size, caddr_t desiredAddress) : space(nm, size)
{
  next_space= NULL;
  size++;  // +1 for the invariant
  // size rounded up for us in OS::allocate_idealized_page_aligned
  char *mem= OS::allocate_idealized_page_aligned(size, nm, desiredAddress, false);
  if (!mem) {
    size= 0;
    return;
  }
  if (OS::is_directed_allocation_supported()  
  &&  mem != desiredAddress) { // must have fallen back on malloc()
    size= 0;
    delete [] mem;
    return;
  }
  int32 old_size = size; // init_space rounds it up!
  init_space(nm, size, mem);
  if (size != old_size) fatal("size changed");
}

  
oop* oldSpace::alloc_objs_and_bytes(fint size,
                                    fint bsize,
                                    char*& bytes,
                                    bool mustAllocate) {
  oop *p= alloc_objs_and_bytes_local(size, bsize, bytes);
  if (!p) return NULL;

  if ( this == Memory->old_gen->reserve_space )
    Memory->old_gen->reselect_reserve_space();

  if ( Memory->old_gen->reserve_space  ||  mustAllocate )
    return p;

  unalloc_objs_and_bytes_local(size, bsize);
  Memory->old_gen->reselect_reserve_space();
  return NULL;
}


void oldSpace::prepare_for_scavenge() {
  objs_scavenge_point= oopsEnd();
}

bool oldSpace::scavenge_recorded_stores() {
  return Memory->remembered_set->scavenge_contents(objs_bottom, objs_top);
}

bool oldSpace::scavenge_promotions() 
{
  bool flag= false;

  for (oop* p = objs_scavenge_point;  p < objs_top;  ++p) {
    oop x = *p;
    if (x->is_new()) {
      flag = true;
      x = x->scavenge();
      // haven't check-stored newly tenured objects; do it here
      Memory->store(p, x);
    }
  }
  objs_scavenge_point = objs_top;
  return flag;
}

void space::compact(mapOop unmarked_map_map,
                    space*& copySpace,
                    oop*& d,
                    oop*& bd) {
  // compact oops and bytes (outwards in), place copies in copySpace
  // (and successors, if necessary)

  // Leave sentinel at end of oops part
  // (utilises extra word between objs and bytes part).
  // This causes the is_object_start() loop below to exit.
  set_objs_top_sentinel(badOop);

  if (copySpace == this) {
    d=  objs_bottom;
    bd= bytes_top;
  }
  
  for (oop* p= objs_bottom; p < objs_top; ) {
    oopsOop obj = as_oopsOop(p);

    if (obj->is_gc_marked()) {
      // object survives GC
      // figure out size
      Map* nm = mapOop(obj->map()->enclosing_mapOop()->gc_unmark())->map_addr();
      fint size = nm->object_size(obj);
      byteVectorOop bv= NULL;
      int32 bsize= 0;
      if (nm->is_byteVector()) {
        bv= byteVectorOop(obj);
        bsize= bv->lengthWords();
      }

      if (copySpace != this && !copySpace->would_fit(size, bsize)) {
        copySpace= ((oldSpace*)copySpace)->next_space;
        d=  copySpace->objs_bottom;
        bd= copySpace->bytes_top;
      }

      // check for special map processing
      if (obj->map() == Memory->map_map) {
        // adjust dependencies first
        as_mapOop(p)->map_addr()->shift_map(as_mapOop(d)->map_addr());
      }

      // do compaction
      if (bv) {
        // compact bytes part up
        oop* bp = (oop*) bv->bytes();
        assert(copySpace != this  ||  bp + bsize <= bd,
               "bytes parts aren't in order");
        copy_words_down((int32*)bp + bsize, (int32*)bd, bsize);
        bd -= bsize;
        bv->set_bytes((char*) bd);
      }
      // compact oops part down
      copy_oops_up(p, d, size);
      as_oopsOop(d)->gc_moved();
      d += size;
      p += size;
      if (copySpace != this) {
        copySpace->objs_top= d;
        copySpace->bytes_bottom= bd;
      }
    } else {
      // object is dying

      // check for special map processing
      if (((memOopClass*)p)->_map == unmarked_map_map) {
        // delete the dying map
        as_mapOop(p)->map_addr()->delete_map();
      }

      // skip to next object
      // (can't use object's map to compute object size,
      //  since it might be destroyed by now)
      for (p += 2;      // skip mark and map
           !is_object_start(*p);
           p++) ;
      assert(p <= objs_top, "compacter ran off end");
    }
  }
  assert(d < bd, "didn't compact anything");
  if (copySpace == this) {
    objs_top= d;
    bytes_bottom= bd;
  }
}

void space::gc_unmark_contents() {
  for (oop* p = objs_bottom; p < objs_top; p ++) UNMARK_TEMPLATE(p);
}

void oldSpace::gc_unmark_contents() {
  space::gc_unmark_contents();
  Memory->record_multistores(objs_bottom, objs_top);
}

oop space::find_oop_backwards(void* start) {
  if (objs_contains(start)) {
    oop* p = find_this_object((oop*)start);
    return as_memOop(p);
  }
  ShouldNotReachHere(); // not in this space
  return NULL; // for C++
}

void space::oops_do(oopsDoFn f) {
  oop* p = objs_bottom;
  while (p < objs_top) {
    oop m = as_oopsOop(p);
    f(&m);
    p += m->size();
  }
}

bool space::verify() {
  lprintf("%s ", name);
  oop* p = objs_bottom;
  char* b = (char*) bytes_top;
  bool r = true;
  while (p < objs_top) {
    oop m = as_oopsOop(p);
    r &= m->verify();
    if (r) {
      r &= m->verifyBytesPart(b);
    }
    p += m->size();
  }
  return r;
}

void oldSpace::switch_pointers_by_card(oop from, oop to)
{
  rSet* rs= Memory->remembered_set;
  char* start_byte = rs->byte_for(oopsStart());
  char* end_byte   = rs->byte_for(oopsEnd());
  
  // first & middle cards:
  char *cp;
  for (cp = rSet::next_zero_byte(start_byte, end_byte);
       cp < end_byte;
       cp = rSet::next_zero_byte(cp + 1, end_byte)) {
    assert(*cp == 0, "next_zero_byte failed");
    switch_pointers_in_region(from, to, rs->oop_for(cp), rs->oop_for(cp+1));
  }
  // last card (could be partial):
  if (*end_byte == 0)
    switch_pointers_in_region(from, to, rs->oop_for(cp), oopsEnd());
}


// This routine is not fully general
// is called carefully (see define in map.c)
//  so it works as is.
// It is only called for programmable_slots objects,
//  which excludes byteVectors, and smiOops,
// hashes are preserved by the caller, and
// the code is updated separately by the caller.
// Finally, to is always a newly-created object, so that
// if from is found in the value of a constant slot
// (i.e. in a map), the map can be mutated without
// creating a duplicate map, which would violate the
// canonical-map invariants. -- dmu
// Thanks, Craig for some of the explanation -- dmu 1/12/93

void space::switch_pointers_in_region(oop from, oop to,
                                      oop* bottom, oop* top) {
  // don't search bytes; they contain no oop references
  assert(in_objs_bounds(bottom) && in_objs_bounds(top),
         "switching pointers outside oops part");
  if (top <= bottom) return; // space could be empty
  oop saved_bottom = *bottom;
  *bottom = from;
  oop *p;
  for (p =  find_prior_reference(top - 1,  from);  
       p > bottom; 
       p =  find_prior_reference(p - 1, from)) {
    // must undo sentinel cause switch_pointer may need to look at that spot
    *bottom = saved_bottom;
    as_memOop(find_this_object(p))->switch_pointer(p, to);
    *bottom = from;  // redo sentinel
  }
  assert(p == bottom, "search missed sentinel");
  *bottom = saved_bottom;
  if (saved_bottom == from)
    as_memOop(find_this_object(bottom))->switch_pointer(bottom, to);
}

void space::write_snapshot_header(FILE* file) {
  OS::FWrite(&objs_bottom, oopSize, file);
  OS::FWrite(&objs_top, oopSize, file);
  OS::FWrite(&bytes_bottom, oopSize, file);
  OS::FWrite(&bytes_top, oopSize, file);
}

void space::write_snapshot(FILE* file) {
  OS::set_access_before_writing_space(objs_bottom, objs_top, bytes_bottom, bytes_top);
  if (page_aligned) {
    char *objs_page_end=    OS::idealized_page_end(   objs_top);
    char *bytes_page_start= OS::idealized_page_start(bytes_bottom);
    fseek(file, roundTo(ftell(file), idealized_page_size), SEEK_SET);
    if (bytes_page_start <= objs_page_end)
      OS::FWrite_mem(objs_bottom, bytes_top, file);
    else {
      OS::FWrite_mem(objs_bottom, objs_page_end, file);
      OS::FWrite_mem(bytes_page_start, bytes_top, file);
    }
  } else {
    OS::FWrite_mem(objs_bottom, objs_top, file);
    OS::FWrite_mem(bytes_bottom, bytes_top, file);
  }
  OS::reset_access_after_writing_space(objs_bottom, objs_top, bytes_bottom, bytes_top);
}

void space::relocate() {
  for (oop* p = objs_bottom; p < objs_top; p ++)
    RELOCATE_TEMPLATE(p);
}

void space::relocate_bytes() {
  if (bytes_bottom != old_bytes_bottom) {
    for (oop* p = objs_bottom; p < objs_top; ) {
      oopsOop m = as_oopsOop(p);
      if (m->is_byteVector()) {
        byteVectorOop(m)->relocate_bytes(this);
      }
      p += m->size();
    }
  }
}

void space::fixup_maps() {
  for (oop* p = objs_top; p  >  objs_bottom; ) {
    p = find_this_object(p - 1);
    memOop obj = as_memOop(p);
    Map* m = as_mapOop(p)->map_addr();
    if (obj->map() == Memory->map_map) {
      m->set_vtbl_value(Vtbls->translate(m->vtbl_value()));
      m->fixup();
    }
  }
}


void space::fixup_killables() {
  // because we can't snapshot them yet
  for (oop* p = objs_top; p  >  objs_bottom; ) {
    p = find_this_object(p - 1);
    oop obj = as_memOop(p);
    if (obj->is_killable())  // process, block, activation obj, foreignOop, ...
      obj->kill();
  }
}


void space::canonize_map_vtbls() {
  for (oop* p = objs_top; p  >  objs_bottom; ) {
    p = find_this_object(p - 1);
    memOop obj = as_memOop(p);
    Map* m = as_mapOop(p)->map_addr();
    if (obj->map() == Memory->map_map)
      m->set_vtbl_value(Vtbls->vtbl_values[m->vtblMapType()]);
  }
}

// called when reading a snapshot w/o canonicalized maps to recanonicalize them

void space::canonicalize_maps() {
  for (oop* p = objs_top; p  >  objs_bottom; ) {
    p = find_this_object(p - 1);
    as_memOop(p)->canonicalize_map();
  }
}


void space::print() {
  printIndent();
  lprintf("%4s: oops: 0x%-6lx -> 0x%-6lx; used: %ld bytes\n",
         name, objs_bottom, objs_top,
         (char*) objs_top - (char*) objs_bottom);
  printIndent();
  lprintf("     bytes: 0x%-6lx -> 0x%-6lx; used: %ld bytes\n",
         bytes_bottom, bytes_top,
         (char*) bytes_top - (char*) bytes_bottom);
  printIndent();
  lprintf("     total: used: %ld bytes; free: %ld bytes\n",
         used(), bytes_free());
}


static enumeration* theEnumeration;
bool call_filter_match(oop* matching_cell,  smi hit_num) {
  oop* obj = find_this_object(matching_cell);
  theEnumeration->filter_match(as_oopsOop(obj), matching_cell, hit_num);
  return theEnumeration->is_ok();
}

void space::enumerate_matches(enumeration* e) {
  smi num_targets = e->get_num_targets();
  if (num_targets <= 0)  return;
  theEnumeration = e;
  search_area((int32*) objs_bottom, (int32*) objs_top,
              (int32*) e->get_targets(), num_targets,
              (match_func)call_filter_match);
}

void space::enumerate_maps(enumeration* e) {
  for (oop* p = objs_top;  p > objs_bottom  &&  e->is_ok();  ) {
    p = find_this_object(p - 1);
    oopsOop obj = as_oopsOop(p);
    if (obj->is_map()) {
      e->filter_map(mapOop(obj));
    }
  }
}

void space::enumerate_all_objs(enumeration* e) {
  for (oop* p = objs_top; p > objs_bottom  &&  e->is_ok(); ) {
    p = find_this_object(p - 1);
    oopsOop obj = as_oopsOop(p);
    if (!obj->is_map() && obj != Memory->errorObj) {
      assert(obj->is_slots() || obj->is_block(), "weird type of object");
      e->add_obj(obj);
    }
  }
}

bool add_obj_map_match(oop* matching_cell,  smi hit_num) {
  Unused(hit_num);
  // could be a real match with memOop::map_index,
  // or could be a unwanted match with Map::my_index
  oop* obj = matching_cell - (map_offset() + Mem_Tag)/oopSize;
  assert((*obj)->is_mark(), "should only reference a map in map field");
  theEnumeration->add_obj(as_oopsOop(obj));
  return theEnumeration->is_ok();
}

void space::enumerate_families_small(enumeration* e) {
  theEnumeration = e;
  // need to skip searching errorObj
  // (This is used to fill frames and should not show up in other objects
  //  but DOES sit in the heap. If we search it by mistake,
  //  assertions will kvetch.)
  oop* errorObjp    = (oop*)Memory->errorObj->addr();
  oop* errorObj_end = find_next_object(errorObjp);
  
  if (objs_bottom <= errorObjp  &&  errorObjp < objs_top) {
    // two pieces:
    search_area((int32*) objs_bottom,  (int32*) errorObjp, (int32*) e->get_maps(), e->get_num_maps(), (match_func)add_obj_map_match);
    search_area((int32*) errorObj_end, (int32*)  objs_top, (int32*) e->get_maps(), e->get_num_maps(), (match_func)add_obj_map_match);
  }
  else
    search_area((int32*) objs_bottom,  (int32*) objs_top,  (int32*) e->get_maps(), e->get_num_maps(), (match_func)add_obj_map_match);
}


void space::enumerate_families(enumeration* e) {
 if (objs_bottom >= objs_top) {
   return;
 }

 oop* maps     = e->get_maps();
 smi  num_maps = e->get_num_maps();
 smi  hit_num;

 // An experiment showed it is faster to search for maps
 // in an area instead of examine the map of each object
 // iff num_maps < Vectorfind_max_targets
 if (num_maps < Vectorfind_max_targets) {
   enumerate_families_small(e);
   return;
 }

 set_objs_top_sentinel(*objs_bottom);  // must be a mark for find_next_object()

 slotsOopClass *errorObjp= Memory->errorObj->addr();
 for (oop* objp = objs_bottom; 
           objp < objs_top && !e->stop();
           objp = find_next_object(objp)) {
    if (objp == (oop*)errorObjp) continue;
    mapOop m = as_oopsOop(objp)->map()->enclosing_mapOop();
    maps[num_maps] = m; // Place sentinel
    oop* matching_cell =
      (oop*) vectorfind_next_match((int32*) maps, (int32*) &m, 1, &hit_num);
    assert(matching_cell <= &maps[num_maps], "match out of area");
    if (matching_cell != &maps[num_maps]) {
      assert(!as_oopsOop(objp)->is_map(), "objp is a map");
      e->add_obj(as_oopsOop(objp));
    }
  }
}

void oldSpace::record_new_pointers()
{
  for (oop* objp= objs_bottom; objp < objs_top; ++objp) {
    oop p= *objp;
    if (p->is_new())
      Memory->remembered_set->record_store(objp);
  }
}

/* Sun-$Revision: 30.18 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "nmethod.hh"
# pragma implementation  "nmethod_inline.hh"

# include "_nmethod.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

VtblPtr_t   nmethod::_vtbl_value;

#define FOR_MY_CODETABLE_ENTRIES(e) \
 for (codeTableEntry *e= codeTableLink;  e;  e= e->next_nm)

static char* iAddr;             // gross hack for 2.0 allocator
static nmethodScopes* sAddr;
static addrDesc* lAddr;
static nmln* dAddr;

static int32 iLen;              // this is even grosser because g++ doesnt
static int32 ilLen;             // eat stuff like "new(7) Y(foo)"
static int32 sLen;
static nmln* depsStart;
static nmln* depsEndArg;
static int32 dLen;

nmethod* nmethod::new_nmethod(AbstractCompiler* c, bool generateDebugCode) {
  // This grossness is brought to you by the great way in which C++
  // handles non-standard allocation...
  Assembler* instsA = c->instructions();
  iLen  = roundTo(instsA->instsEnd - instsA->instsStart, oopSize);
  ilLen = (char*) instsA->locsEnd - (char*) instsA->locsStart;

  sLen = c->scopeDescRecorder()->size();

  depsStart = c->L->deps->start();
  depsEndArg = c->L->deps->end();
  dLen = (char*) depsEndArg - (char*) depsStart;

  // This assertion is tempting, but zombie nmethods created in the conversion
  // break it because they are obsolete.
  // A dependency from the method to the nmethod would be cleaner, but
  // would take more space -- dmu 7/39
  //  
  //  assert (      dLen > 0
  //            ||  c->L->selector == VMString[DO_IT],
  //          "all nmethods must have some dependencies");
  
  nmethod* nm = new nmethod(c, generateDebugCode);
  Memory->code->used_per_compiler[c->nmName()] += nm->size();
  return nm;
}

void* nmethod::operator new(size_t size) {
  Unused(size);
  void* p;
  fint  number_of_tries = 0;

  // if we're really low on space - try once more (out-of-space detection in
  // zone is too complicated to get 100% right)
  while( number_of_tries++ < 2) {
    p = Memory->code->alloc(iLen, sLen, ilLen, dLen + sizeof(nmethod*),
                            iAddr, sAddr, lAddr, dAddr);
    if (p != NULL) {
      return p;
    }
  }
  fatal("Couldn't flush enough methods from the code cache.\n"
        "You should increase the size of the code cache by writing\n"
        "a snapshot with code_size increased (see the documentation\n"
        "for _MemoryWriteSnapshot:Compress:Sizes:).");
  return p; // Was NULL, but gcc3 complains
}

nmethod::nmethod(AbstractCompiler* c, bool generateDebugCode) {
  CHECK_VTBL_VALUE;
  _instsLen  = roundTo(iLen, oopSize);
  _locsLen   = ilLen;
  depsLen    = dLen;
  // backpointer is just before deps
  depsAddr   = (nmln*)     ((char*)dAddr + sizeof(nmethod*));

  *dBackLinkAddr() = this;
  
  // Copy the nmethodScopes scopeDescs generated by the ScopeDescRecorder
  // to the allocation area.
  c->scopeDescRecorder()->copyTo((VtblPtr_t*)sAddr, (int32)this);
  
  this->scopes = (nmethodScopes*) sAddr;

  oldCount = 0;
  flags.clear();
  flags.isDebug = generateDebugCode;
  setCompiler(c->name());
  flags.isUncommonRecompiled = currentProcess->isUncommon();
    
  verifiedOffset      = c->verifiedOffset();
  diCheckOffset       = c->diCheckOffset();

  frameCreationOffset = c->frameCreationOffset();
  
  rememberLink.init();
  codeTableLink= NULL;
  diLink.init(c->diLink);
  if (diLink.notEmpty()) flags.isDI = true;
  flags.level = c->level();
  if (flags.level >= MaxRecompilationLevels) { // added = zzzz
    warning1("setting invalid nmethod level %ld", flags.level);  // fix this
    flags.level = 0;
  }
  flags.version = c->version();
  if (c->nmName() == nm_nic && ((FCompiler*)c)->isImpure)
    makeImpureNIC();
  key.set_from(c->L->key);
  check_store();
  
  clear_frame_chain();
  assert(c->frameSize() >= 0, "frame size cannot be negative");
  frame_size = c->frameSize();
  _incoming_arg_count = c->incoming_arg_count();
  get_platform_specific_data(c);

  Assembler* instsA = c->instructions();
  copy_bytes(        instsA->instsStart,        insts(), instsLen());
  copy_words((int32*)instsA->locsStart,  (int32*)locs(),  ilLen/4);
  copy_words((int32*)depsStart,          (int32*)deps(),  depsLen/4);
  
  addrDesc *l, *lend;
  for (l = locs(), lend = locsEnd(); l < lend; l++) {
    l->initialShift(this, (char*)insts() - (char*)instsA->instsStart, 0);
  }

  char* bound = Memory->new_gen->boundary();
  for (l = locs(), lend = locsEnd(); l < lend; l++) {
    if (l->isOop())
      OopNCode::check_store(oop(l->referent(this)), bound); // cfront garbage
    else if (l->isSendDesc()) {
      l->asSendDesc(this)->dependency()->init();
    } else if (l->isDIDesc()) {
      l->asDIDesc(this)->dependency()->init();
      flags.isDI = true; 
    }
  }
  
  for (nmln* d = deps(), *dend = depsEnd(); d < dend; d++) {
    d->relocate();
  }
  
  MachineCache::flush_instruction_cache_range(insts(), instsEnd());
  MachineCache::flush_instruction_cache_for_debugging();
  
  if (this == (nmethod*)catchThisOne) warning("caught nmethod");
}

char* nmethod::entryPointFor(sendDesc *sd) {
  if (sd->pic()) return verifiedEntryPoint();
  bool rcvrStaticLookupType= sd->lookupType() & ReceiverStaticBit;
  if (!rcvrStaticLookupType) return insts();
  return ReuseNICMethods && findNMethod(sd)->reusable()
          ? insts() : verifiedEntryPoint();
}

fint nmethod::level() {
  assert(flags.level < MaxRecompilationLevels,  "invalid level"); // zzzz
  return flags.level;
}

void nmethod::setVersion(fint v) {
  assert(v > 0 && v <= MaxVersions, "bad version");
  flags.version = v;
}

ScopeDesc* nmethod::containingScopeDesc(char* pc) {
  return containingPcDesc(pc)->containingDesc(this);
}

void nmethod::check_store() {
  if (key.is_new() || scopes->is_new()) {
    remember();
    return;
  }
  FOR_MY_CODETABLE_ENTRIES(e)
    if (e->key.is_new()) {
      remember();
      return;
    }
}

void nmethod::moveTo_inner(NCodeBase* p, int32 delta, int32 size) {
  nmethod* to = (nmethod*)p;
  if (this == to) return;
  if (PrintCodeCompaction) {
    lprintf("*moving nmethod %#lx (", this);
    printName(0, key.selector);
    lprintf(") to %#lx\n", to);
  }

  OopNCode::moveTo_inner(to, delta, size);

  assert(iabs((char*)to - (char*)this) >= sizeof(NCodeBase),
         "nmethods overlap too much");
  assert(sizeof(NCodeBase) % oopSize == 0, "should be word-aligned");
  // init to's vtable
  copy_words((int32*)this, (int32*)to, sizeof(NCodeBase) / sizeof(int32));

  scopes->_nmethod_backPointer = to;
  *dBackLinkAddr() = to;
  flatProfiler->move(insts(), instsEnd(), to->insts());
  
  zoneLink.shift(delta);
  
  FOR_MY_CODETABLE_ENTRIES(e) {
    e->nm= to;
  }

  for (nmln *x = linkedSends.next, *y = x->next;
       x != &linkedSends;
       x = y, y = y->next) {
    NCodeBase* s = findThing(x);
    s->shift_target(x, delta);
  }
  linkedSends.shift(delta, this);
  
  if (diLink.notEmpty()) {
    assert(diLink.next->next == &diLink, "diLink should be a pair");
    diLink.next->asDIDesc()->shift_jump_addr(delta);
  }
  diLink.shift(delta);
  
  for (addrDesc* q = locs(), *pend = locsEnd(); q < pend; q++) {
    if (q->isSendDesc()) {
      sendDesc* sd = q->asSendDesc(this);
      sd->shift(delta, this);
    }
    else if (q->isDIDesc()) {
      nmln* l = q->asDIDesc(this)->dependency();
      l->shift(delta);
    }
    q->shift(this, delta);
  }
  if (frame_chain != NoFrameChain && frame_chain != SavedFrameChain)
    frame_chain->nmethod_moved_by(delta, this);
}


void nmethod::shift_target(nmln* l, int32 delta) {
  l->asSendDesc()->shift_jump_addr(delta);
}

NCodeBase* nmethod::unlink_me(nmln* l) {
  l->asSendDesc()->unlink();
  return this;
}


void nmethod::addDeps(dependencyList *dl) {
  nmln *d;
  for (d= dl->start();  d < dl->end();  d++) {
    for (nmln *p= d->next;  p != d;  p= p->next)
      if (p >= deps() && p < depsEnd()) {
        // nmethod already on this list
        p->remove();
        break;
      }
  }
  for (nmln *nd= deps();  nd < depsEnd();  nd++)
    if (nd->notEmpty()) {
      dl->add(nd);  // could make this a shade more efficient by adding
                    // a function to dependencyList
      nd->remove();
    }
  fint ndeps= dl->length();
  if (ndeps == depsEnd() - deps()) {
    // no need to allocate new region, just copy
    copy_words((int32*)dl->start(), (int32*)deps(), depsLen / sizeof(nmln*));
    for (d= deps();  d < depsEnd();  d++)
      d->relocate();
    return;
  }
  assert(ndeps > depsEnd() - deps(), "deps didn't grow!");
  // now make dl the method's deps
  fint newDepsLen= ndeps * sizeof(nmln);
  char *dAddr= Memory->code->allocateDeps(newDepsLen + sizeof(nmethod*));
  nmln *newDeps= (nmln*) (dAddr + sizeof(nmethod*));
  copy_words((int32*)dl->start(), (int32*)newDeps, newDepsLen / sizeof(nmln*));
  for (d= newDeps;  d < newDeps + ndeps;  d++)
    d->relocate();
  Memory->code->deallocateDeps((char*)dBackLinkAddr(),
                               depsLen + sizeof(nmethod*));
  depsAddr= newDeps;
  depsLen= newDepsLen;
  *dBackLinkAddr()= this;
}


void nmethod::moveDeps(nmln* newDeps, int32 delta) {
  for (nmln* d = deps(), *dend = depsEnd(); d < dend; d++) {
    d->shift(delta);
  }
  depsAddr = newDeps;
}

void nmethod::moveScopes(nmethodScopes* s) {
  // currently, the scopes & pcs are position-independent
  scopes = s;
}


void nmethod::remove_me_from_inline_cache() {
  static int n = 0;
  while (linkedSends.notEmpty()) {
    nmln *x= linkedSends.next;
    assert(x->next != NULL, "");
    findThing(x)->unlink_me(x);
  }
}


void nmethod::makeYoung() {
  // Relink nm into its callers; aging stubs will be inserted.
  // The agingLimit will be set to 1 because of nm's trapCount.
  assert(!isYoung(), "why call this?");
  flags.isYoung= 1;
  nmln *lnext;
  for (nmln* l= linkedSends.next;  l != &linkedSends; l= lnext) {
    lnext= l->next; // because we're mutating l
    sendDesc* sd= l->asSendDesc_or_null();
    if (sd)
      sd->rebind(this, NULL);
    else {
      CacheStub *pic= l->asCacheStub();
      if (pic)
        pic->rebind(l, this, NULL);
    }
  }
}


void nmethod::makeVeryYoung() {
  assert(isYoung(), "not young enough");
  // reset limits of aging stubs to 1 to provoke recompilation
  for (nmln* l= linkedSends.next;  l != &linkedSends;  l= l->next) {
    CountStub* cs= l->asCountStub();
    if (cs && cs->isAgingStub())
      // NB: in (rare) cases, may not have an aging stub
      ((AgingStub*)cs)->init(this);     // sets limit to 1
  }
}


void nmethod::makeOld() {
  flags.isYoung= 0;
  // remove all AgingStubs
  nmln* nextl;
  for (nmln* l= linkedSends.next;  l != &linkedSends;  l= nextl) {
    nextl= l->next;             // because we're mutating l
    NCodeBase* s= findThing(l);
    if (s->isAgingStub()) {
      AgingStub *a= (AgingStub*)s;
      sendDesc* sd= a->sd();
      if (sd) {
        a->deallocate();
        sd->setCounting(NonCounting);
        sd->rebind(this);
      } else {
        CacheStub *pic= a->pic();
        assert(pic, "no pic for sendDesc");
        pic->rebind(a->sdLink.next, this, NULL);
        a->deallocate();
      }
    }
  }
}


fint nmethod::agingLimit() {
# ifdef SIC_COMPILER
  // use exponential back-off for limit
  if (flags.trapCount > MapLoadTrapLimit) {
    // not really a young method, but has trapping map loads and thus needs
    // to be recompiled
    return 1;
  } else if (isUncommonRecompiled()) { 
    fint limit = MaturityInvocationLimit;
    for (fint i = 1; i < version(); i++) limit *= MaturityScalingFactor;
    return min(limit, 100000);
  } else {
    return MaturityInvocationLimit;
  }
# else
  return 17;
# endif
}



void nmethod::forwardLinkedSends(nmethod* to) {
  // the to nmethod is about to replace the receiver; replace receiver in
  // all inline caches
  if (key.receiverMapOop() != to->key.receiverMapOop()) {
    // receiver map has changed - either through programming or because
    // a block map has changed -- don't forward (too complicated e.g. if
    // called by CountStub via a PIC)
    return;
  }
  while(linkedSends.notEmpty()) {
    nmln *x = linkedSends.next;
    findThing(x)->forwardLinkedSend(x, to);
  }
}

void nmethod::removeFromCodeTable() {
  codeTableEntry *nexte;
  for (codeTableEntry *e= codeTableLink;  e;  e= nexte) {
    nexte= e->next_nm;
    e->next_hash.remove();
    delete e;
  }
  codeTableLink= NULL;
}


int nmethodFlushCount = 0;

void nmethod::unlink() {
  removeFromCodeTable();
  zoneLink.remove();
  remove_me_from_inline_cache();
  
  if (diLink.notEmpty()) {
    assert(diLink.next->next == &diLink, "should only be a pair on a diLink");
    diLink.next->asDIDesc()->unlink_me();
  }
  
  for (nmln* d = deps(), *dend = depsEnd(); d < dend; d++) {
    d->remove();
  }

  MachineCache::flush_instruction_cache_for_debugging();
}


// I believe there is a bug here--the original comment in nmethod::flush
// where makeZombie is called with a false argument
// says don't unlink(), but I think it should stil be unlinked from codeTable.
// Otherwise the assertion in CodeTable::lookup about duplicate nmethods trips.
// I cannot see why it should be left in the table anyway.
// So I added the call to removeFromCodeTable() below.
// -- dmu 1/12/03

void nmethod::makeZombie(bool unlnk) {
  // mark this nmethod as zombie (it is almost dead and can be flushed as
  // soon as it is no longer on the stack)
  if (!isZombie()) {
    flags.isZombie = 1;
    flags.isToBeRecompiled = 0;
    if (unlnk) unlink();
    else       removeFromCodeTable(); // added by dmu 1/03; see comment in nmethod::flush
    zoneLink.remove();
    Memory->code->zombies.add(&zoneLink);
  }
}

void nmethod::flush() {
  BlockProfilerTicks bpt(exclude_nmethod_flush);
  CSect cs(profilerSemaphore);          // for profiler
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      // for debugging
      if (nmethodFlushCount  &&  --nmethodFlushCount == 0)
        warning("nmethodFlushCount");
      if (this == (nmethod*)catchThisOne) warning("caught nmethod");
    }
# endif
  
  // EventMarker em("flushing nmethod %#lx %s", this, "");
  if (PrintMethodFlushing) {
    ResourceMark m;
    char *compilerName = VMString[compiler()]->copy_null_terminated();
    lprintf("*flushing %s%s%s-nmethod 0x%lx %d\t(",
           isZombie() ? "zombie " : "", 
           isAccess() ? "access " : "",
           compilerName, (void*)(long unsigned)this, (void*)useCount[id]);
    printName(0, key.selector);
    lprintf(")");
  }

  // always check the following - tests are really cheap
  if (flags.flushed) fatal1("nmethod %#lx already flushed", this);
  if (zone::frame_chain_nesting == 0) fatal("frames must be chained when flushing");

  if (frame_chain != NoFrameChain) {
    // Can't remove an nmethod from deps chains now, because later
    // programming changes may need to invalidate it.
    // That is, don't unlink() now.
   
    // See comment for makeZombie routine. The comment above is the 
    // "original comment" referred to there.
    // -- dmu 1/12/03

    if (this == recompilee) {
      // nmethod is being recompiled; cannot really flush yet
      // em.event.args[1] = "(being recompiled)";
      if (PrintMethodFlushing) {
        lprintf(" (being recompiled)\n");
      }
    } else {
      // nmethod is currently being executed; cannot flush yet
      // em.event.args[1] = "(currently active)";
      if (PrintMethodFlushing) {
        lprintf(" (currently active)\n");
      }
    }
    makeZombie(false);
  } else {
    unlink();
  
    // nmethod is not being executed; completely throw away
    // em.event.args[1] = "(not currently active)";
    if (PrintMethodFlushing) {
      lprintf("\n");
    }
    flatProfiler->flush((char*)this, instsEnd());
    zoneLink.remove();
    rememberLink.remove();
    for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
      if (p->isSendDesc()) {
        p->asSendDesc(this)->unlink();
      } else if (p->isDIDesc()) {
        p->asDIDesc(this)->dependency()->flush();
      }
    }
    flags.flushed = 1;                        // to detect flushing errors
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      set_oops((oop*)insts(), instsLen()/oopSize, 0); // for quicker detection
    }
#   endif
    Memory->code->free_nmethod(this);
  }
  MachineCache::flush_instruction_cache_for_debugging();
}


void nmethod::flushPartially() {
# ifdef SIC_COMPILER
  oldCount = min(recompileLimit(level()), invocationCount());
# else
  oldCount = invocationCount();
# endif
  useCount[id] = 0;
  removeFromCodeTable();
  diLink.remove();
  save_unlinked_frame_chain();  // protect from accidental flushing
  if (!UsePICRecompilation) {
    flush();                    // also flush the inline caches
  }
  assert(frame_chain != NoFrameChain, "frames should be chained now");
}





void nmethod::invalidate() {
  if (isInvalid()) return;
  processes->needsInvalidate = true;
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  this == (nmethod*)catchThisOne) warning("caught nmethod");
# endif
  
  if (PrintMethodInvalidation) {
    lprintf("*invalidating nmethod 0x%lx (", (void*)this);
    printName(0, key.selector);
    lprintf(")\n");
  }
  
  LOG_EVENT1("invalidating nmethod %#lx", this);
  flags.isInvalid = true;
  unlink();
}

#ifdef UNUSED
// remove this methods PICs; return total size of flushed PICs
int32 nmethod::flushPICs() {
  int32 flushed = 0;
  for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
    if (p->isSendDesc()) {
      sendDesc* sd = p->asSendDesc(this);
      CacheStub* s = sd->pic();
      if (s) {
        flushed += s->size();
        s->deallocate();
      }
    }
  }
  MachineCache::flush_instruction_cache_for_debugging();
  return flushed;
}
#endif

#ifdef UNUSED
// unlink di Links and save them 
int32 nmethod::unlinkDI(nmln*& savedDIChildren) {
  int32 nlinks = 0;
  savedDIChildren = NEW_RESOURCE_ARRAY(nmln, locsEnd() - locs());
  for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
    if (p->isDIDesc()) {
      nmln* l = p->asDIDesc(this)->dependency();
      savedDIChildren[nlinks].init();
      if (l->notEmpty()) {
        assert(l->next->next == l, "should be a pair");
        l->next->rebind(&savedDIChildren[nlinks]);
        assert(savedDIChildren[nlinks].notEmpty(), "should have saved link");
      }
      assert(p->asDIDesc(this)->dependency()->isEmpty(),
             "should be empty now");
      nlinks++;
    }
  }
  return nlinks;
}
#endif

# ifdef brokenDI        // DI recompilation is currently broken
void nmethod::relinkDI(int32 n, nmln*& savedDIChildren) {
  Unused(n);
  int32 nlinks = 0;
  for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
    if (p->isDIDesc()) {
      assert(nlinks < n, "too many DI caches in method");
      nmln* l = p->asDIDesc(this)->dependency();
      assert(l->isEmpty(), "should be empty");
      if (savedDIChildren[nlinks].notEmpty()) {
        nmln* ln = savedDIChildren[nlinks].next;        // cfront bug!
        ln->rebind(l);
        nmethod* target = findNMethod(ln);
        p->asDIDesc(this)->set_jump_addr(target->insts());
        assert(p->asDIDesc(this)->dependency()->notEmpty(),
               "should be rebound now");
      }
      nlinks++;
    }
  }
  MachineCache::flush_instruction_cache_for_debugging();
  assert(n == nlinks, "too few DI links in method");
}
# endif

PcDesc* nmethod::containingPcDescOrNULL(char* pc) {
  // returns PcDesc that is closest one before or == to pc, or NULL if
  // no stored pcDesc exists 
  // called a lot, so watch out for performance bugs
  assert(contains(pc), "nmethod must contain pc into frame");
  int32 offset = pc - insts();
  PcDesc* start = pcs();
  PcDesc* end = pcsEnd() - 1;

  // Skim the cream if only one pcDesc is present.
  if (start == end) return start; 
  assert(start < end, "no PcDescs to search");
  if (start->pc > offset)
    // in prologue; caller has to deal with this
    return NULL;

  // binary search to find approx. location
  PcDesc* middle;
  int32 l = 0, h = end - start;
  do {
    // avoid pointer arithmetic -- gcc uses a division for PcDesc* - PcDesc*
    int32 m = l + (h - l) / 2;
    middle = &start[m];
    if (middle->pc < offset) {
      l = m + 1;
    } else {
      h = m - 1;
    }
  } while (middle->pc != offset && l < h);

  // may not have found exact offset, so search for closest match
  while (middle->pc <= offset && middle < end  ) middle++;
  while (middle->pc >  offset && middle > start) middle--;
  
  assert(start <= middle && middle <= end, "should have found a pcDesc");
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      PcDesc* d = pcs();
      PcDesc* closest = d;
      for (; d <= end; d ++) {
        if (d->pc <= offset && (closest == NULL || closest->pc <= d->pc)) {
          closest = d;
        }
      }
      assert(closest == middle, "found different pcDesc");
    }
# endif

  return middle;
}

static PcDesc prologuePcDesc(0, 0, PrologueBCI);

PcDesc* nmethod::containingPcDesc(char* pc) {
  // returns PcDesc that is closest one before or == to pc
  PcDesc* p= containingPcDescOrNULL(pc);
  return p ? p : &prologuePcDesc;
}

fint nmethod::invocationCount() {
  if (useCount[id]) {
    // nmethod counts in prologue (to save space for count stubs)
    return useCount[id];
  } else {
    fint count = 0;
    for (nmln* l = linkedSends.next; l != &linkedSends; l = l->next) {
      NCodeBase* s = findThing(l);
      if (s->isCountStub()) count += ((CountStub*)s)->count();
    }
    return count;
  }
}

static int cmp_addrs(const void* p1,  const void* p2) {
  char** r1 = (char**) p1;
  char** r2 = (char**) p2;
  return *r1 - *r2;
}


bool nmethod::shouldNotRecompile() {
# ifdef SIC_COMPILER
  if (compiler() == NIC) return false;
  if (level() == MaxRecompilationLevels - 1)
    return true;                // no optimizable sends
  if (isYoung()) {
    if (invocationCount() > agingLimit() &&
        flags.trapCount < MapLoadTrapLimit) {
      // isn't really young anymore (but didn't trigger counter because it
      // has several callers)
      makeOld();
    } else if (nsends() > max(recompileLimit(0), 4 * agingLimit())) {
      // does a lot of sends - give it a chance anyway
    } else {
      // don't recompile yet - too young
      // but mark it for later recompilation
      makeToBeRecompiled();
      return true;
    }
  }
# endif
  return false; 
}


bool nmethod::mustNotRecompile() {
  // answers true if nm must not be recompiled (for non-performance-related
  // reasons)

  // never recompile new-compiled uncommon branches
  // don't recompile zombies (if they're zombies because of uncommon branch
  // traps, the recompiled nmethod already exists)
  if (isZombie() || isInvalid()) return true;

# ifdef SIC_COMPILER
  // the SIC doesn't compile access methods
  if (isAccess()) return true;
  if (version() >= MaxVersions && !isUncommonRecompiled())
    return true;
# endif

  // don't recompile VM-generated lookup error methods (breaks too many
  // assertions because method oop can change (new method generated during
  // lookup)
  methodMap* mm = (methodMap*)method()->map();
  return mm->file()->length() == 7
     &&  strncmp(mm->file()->bytes(), "<error>", 7) == 0;
}


bool nmethod::shouldRecompile() {
# ifdef SIC_COMPILER
  // recompile if uncommon
  if (isUncommonRecompiled()) return true;
  
  // don't recompile if highest level
  if (level() >= nstages) return false;

  // recompile if very small
  if (compiler() == NIC || isTiny()) return true;
# endif

  return false;
}


fint nmethod::ncallers() {
  // slow! need only approximate # callers, so clip at MaxCallers
  const fint MaxCallers = 10;
  AddressList callers(MaxCallers);
  fint i = 0;
  for (nmln* l = linkedSends.next; i < MaxCallers && l != &linkedSends; l = l->next, i++) {
    callers.append((char*)l);
  }
  if (i >= MaxCallers) return MaxCallers;
  qsort(callers.data_addr(), callers.length(), sizeof(char*), cmp_addrs);
  fint n = 0, len = callers.length();
  for (i = 0; i < len; ) {
    n++;
    nmethod* nm = ((nmln*)callers.nth(i))->asSender();
    while (++i < len && nm == ((nmln*)callers.nth(i))->asSender()) ;
  }
  return n;
}

fint nmethod::nsends(bool includeAll) {
  // add up the inlinable sends of the nmethod (i.e. those sent by inline
  // caches with comparing stubs); if includeAll, also count non-inlinable
  // sends
  fint nsends = 0;
  bool isNIC = compiler() == NIC;
  for (addrDesc* a = locs(), *aend = locsEnd(); a < aend; a++) {
    if (a->isSendDesc()) {
      sendDesc* sd = a->asSendDesc(this);
      if (!sd->isUninlinable()) {
        nsends += sd->nsends();
      } else if (isNIC && sd->countType() == Counting) {
        // must be a method containing _Restart
        nsends += sd->nsends();
      } else if (isNIC) {
        // NIC methods have no count stubs (to save space) but count the
        // # of invocations; assume all sends in NIC method are executed
        // once per execution of the method
        nsends += useCount[id];
      } else if (includeAll) {
        nsends += sd->nsends();
      }
    }
  }
  return nsends;
}


bool nmethod::isTiny() {
# ifdef SIC_COMPILER
  // is this a "tiny" nmethod, i.e. is it likely to be small when inlined?
  if (isAccess()) return true;
  if (compiler() == NIC) {
    // the NIC's code is large because it has many inline caches, so look
    // at the source
    fint len = ((methodMap*)method()->map())->codes()->length();
    fint max = key.receiverMap()->is_block() ?
                  TinyBlockSourceSize : TinyFnSourceSize;
    if  (len <= max) return true;
  } else {
    fint len = instsLen() - oopSize * PrologueSize;
    if (len < TinyFnObjSize) return true;
  }

  // try this last because it's relatively expensive
  return isCheapMessage((stringOop)key.selector);
# else
  return false;
# endif
}






// Memory operations: return true if need to inval cache

bool nmethod::scavenge_contents() {
  // scavenge all locations
  bool needToInvalICache = OopNCode::scavenge_contents();
  
  // do this after OopNCode::scavenge_contents, since it resets rememberLink
  key.scavenge_contents();
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.scavenge_contents();
  scopes->scavenge_contents();
  check_store();

  return needToInvalICache;
}

void nmethod::relocate() {
  key.relocate();
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.relocate();
  scopes->relocate();
  OopNCode::relocate();
}

bool nmethod::switch_pointers(oop from, oop to,
                              nmethodBList* nmethods_to_invalidate) {
  key.switch_pointers(from, to);
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.switch_pointers(from, to);
  scopes->switch_pointers(from, to, nmethods_to_invalidate);
  check_store();
  return OopNCode::switch_pointers(from, to, nmethods_to_invalidate);
}

void nmethod::gc_mark_contents() {
  key.gc_mark_contents();
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.gc_mark_contents();
  OopNCode::gc_mark_contents();
  scopes->gc_mark_contents();
}

bool nmethod::gc_unmark_contents() {
  key.gc_unmark_contents();
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.gc_unmark_contents();
  scopes->gc_unmark_contents();
  return OopNCode::gc_unmark_contents();
}

bool nmethod::code_oops_do(oopsDoFn f) {
  key.oops_do(f);
  FOR_MY_CODETABLE_ENTRIES(e)
    e->key.oops_do(f);
  scopes->oops_do(f);
  check_store();
  return OopNCode::code_oops_do(f);
}

bool nmethod::verify() {
  bool r = true;
  ResourceMark rm;

  r &= OopNCode::verify2("nmethod");
  
  if (insts() != (char*)(this + 1)) {
    error1("nmethod at 0x%lx has incorrect insts pointer", this);
    r = false;
  }
  if (!Memory->code->contains(this)) {
    error1("nmethod at 0x%lx not in zone", this);
    r = false;
  }
  if (!zoneLink.verify_list_integrity()) {
    lprintf("\tof zoneLink of nmethod 0x%lx\n", this);
    r = false;
  }
  { FOR_MY_CODETABLE_ENTRIES(e)
      if (e->nm != this) {
        error1("bad code table link for nmethod %#lx\n", this);
        r = false;
      }
  }
  bool isAligned = (frame_size & (frame_word_alignment-1)) == 0;
  if (!isAligned) {
    lprintf("nmethod at %#lx: frame size is not multiple of %d words\n",
           (long unsigned)this,
           frame_word_alignment);
    r = false;
  }
  if (codeTableLink != NULL) {
    nmethod *tableResult =
      isDebug() ? Memory->code->debugTable->lookup(key) :
                  Memory->code->table     ->lookup(key);
    if (tableResult != this) {
      error1("nmethod at %#lx: code table lookup failed", this);
      r = false;
    }
  }
  if (!key.verify()) {
    lprintf("\tof key of nmethod 0x%lx\n", this);
    r = false;
  }
  { FOR_MY_CODETABLE_ENTRIES(e)
      if (!e->key.verify()) {
        lprintf("\tof code table key %#lx of nmethod 0x%lx\n",
               (long unsigned)&e->key, (long unsigned)this);
        r = false;
      }
  }
  if (!linkedSends.verify_list_integrity()) {
    lprintf("\tof linkedSends of nmethod 0x%lx\n", this);
    r = false;
  }
  if (!diLink.verify_list_integrity()) {
    lprintf("\tof diLink of nmethod 0x%lx\n", this);
    r = false;
  }
  r &= scopes->verify();

  for (PcDesc* p = pcs(); p < pcsEnd(); p++) {
    if (! p->verify(this)) {
      lprintf("\t\tin nmethod at %#lx (pcs)\n", this);
      r = false;
    }
  }
  
  // more checks in ncode::verify called above
  bool shouldBeDI = diLink.notEmpty();
  for (addrDesc* l = locs(); l < locsEnd(); l++) {
    if (l->isDIDesc()) {
      shouldBeDI = true;
    }
  }
  
  if (shouldBeDI && !isDI()) {
    error1("nmethod %#lx should be marked isDI", this);
    r = false;
  } else if (!shouldBeDI && isDI()) {
    error1("nmethod %#lx should not be marked isDI", this);
    r = false;
  }

  if (! key.receiverMap()->is_block() ) {
    for (nmln* d = deps(); d < depsEnd(); d++) {
      if (! d->verify_list_integrity()) {
        lprintf("\tin nmethod at %#lx (deps)\n", this);
        r = false;
      }
    }
  }
  
  if (frame_chain != NoFrameChain) {
    error1("nmethod %#lx has non-zero frame chain value", this);
    r = false;
  }
  
  if (findNMethod( instsEnd() - oopSize) != this) {
    error1("findNMethod did not find this nmethod (%#lx)", this);
    r = false;
  }
  return r;
}


// Printing operations

void nmethod::print() {
  ResourceMark rm;
  printIndent();
  lprintf("(nmethod*)%#lx", this);
  if (scopes->root()->isDataAccessScope()) {
    lprintf(" (data access)");
  } else if (scopes->root()->isDataAssignmentScope()) {
    lprintf(" (data assignment)");
  } else {
    lprintf(" for method %#lx", method());
  }
  lprintf(" { ");
  if (isYoung()) lprintf("YOUNG ");
  switch (compiler()) {
   case NIC: lprintf("NIC "); if (isImpureNIC()) lprintf("impure "); break;
   case SIC: lprintf("SIC level %ld ", level()); break;
   default:  lprintf("!?!unknown compiler!?! "); break;
  }
  if (version())           lprintf("v%d ", version());
  if (isDI())              lprintf("DI ");
  if (isZombie())          lprintf("zombie ");
  if (isInvalid())         lprintf("INVALID ");
  if (isDebug())           lprintf("DEBUG ");
  if (isToBeRecompiled())  lprintf("TBR ");
  if (isUncommon() || isUncommonRecompiled()) lprintf("UNCOMMON ");
  lprintf("}:\n");
 
  lprintf( "incoming_arg_count = %d\n", incoming_arg_count() );
  
  print_platform_specific_data();
  
  Indent ++;
  
  key.print();
  
  printIndent();
  lprintf("code table link: %#lx\n", codeTableLink);
  
  printIndent();
  lprintf("remember link: ");
  rememberLink.print();
  lprintf("\n");
  
  printIndent();
  lprintf("linked sends: ");
  linkedSends.print();
  lprintf("\n");
  
  printIndent();
  lprintf("di link: ");
  diLink.print();
  lprintf("\n");
  
  printIndent();
  lprintf("instructions (%ld bytes): %#lx,%#lx/i / x/%ldi %#lx\n",
         instsLen(), 
         insts(),
         instsEnd() - oopSize,
         instsLen() / oopSize,
         insts());
  printIndent(); lprintf("p ((nmethod*)%#lx)->printCode() \n", this);
  scopes->print();
  // printLocs();
  // printDeps();
  Indent --;
}

void nmethod::printCode() {
  ResourceMark m;       // in case methods get printed from gdb
  print_code(this, (char*)insts(), (char*)instsEnd());
}

# if  GENERATE_DEBUGGING_AIDS
void nmethod::printLocs() {
  ResourceMark m;       // in case methods get printed from gdb
  printIndent();
  lprintf("locations:\n");
  Indent ++;
  for (addrDesc* l = locs(); l < locsEnd(); l ++) l->print(this);
  Indent --;
}

void nmethod::printDeps() {
  ResourceMark m;       // in case methods get printed from gdb
  printIndent();
  lprintf("dependents:\n");
  Indent ++;
  for (nmln* n = deps(); n < depsEnd(); n ++) {
    printIndent();
    n->print();
    nmln* n1;
    for (n1= n->next;  n1 != n;  n1= n1->next) {
      oop *q= (oop*)(Memory->code->dZone->findStartOfBlock(n1));
      oop mapP= *q;
      if (Memory->really_contains(mapP)) {
        lprintf("  ");
        oop p= Memory->spaceFor(mapP)->find_oop_backwards(mapP);
        p->print();
        lprintf("\n");
        break;
      }
    }
    if (n1 == n) lprintf(" ??not linked to any map??\n");
  }
  Indent --;
}

void nmethod::printPcs() {
  ResourceMark m;       // in case methods get printed from gdb
  printIndent();
  lprintf("pc-bytecode offsets:\n");
  Indent ++;
  for (PcDesc* p = pcs(); p < pcsEnd(); p ++) p->print(this);
  Indent --;
}
#endif

bool nmethod::isNMethod(void* p) {
  return ((NCodeBase*)p)->vtbl_value() == ((nmethod*)NULL)->static_vtbl_value();
}

nmethod* nmethod::nmethodContaining(char* pc, char* likelyEntryPoint) {
  assert(Memory->code->contains(pc), "should contain address");
  if (likelyEntryPoint && Memory->code->contains(likelyEntryPoint)) {
    nmethod* result = nmethod_from_insts(likelyEntryPoint);
    if (result->contains(pc)) return result;
  }
  return findNMethod(pc);
}

nmethod* nmethod::findNMethod(void* start) {
  nmethod* m = Memory->code->findNMethod(start);
  assert(m->encompasses(start), "returned wrong nmethod");
  return m;
}

nmethod* nmethod::findNMethod_maybe(void* start) {
  nmethod* m = Memory->code->findNMethod_maybe(start);
  assert(!m || m->encompasses(start), "returned wrong nmethod");
  return m;
}

static inline bool includes(void* p, void* from, void* to) {
  return from <= p && p < to;
}

bool nmethod::encompasses(void* p) {
  return includes(p, this, this + 1) ||
         includes(p, deps(), depsEnd()) ||
         includes(p, insts(), instsEnd()) ||
         includes(p, locs(), locsEnd()) ||
         scopes->includes((ScopeDesc*) p) ||
         includes(p, pcs(), pcsEnd());
}


ScopeDesc* nmethod::correspondingScopeDesc(ScopeDesc* s) {
  // find scope corresponding to s; return NULL if not found
  ScopeDesc* scope = NULL;

  FOR_EACH_SCOPE(scopes, c) {
    // compare c and its senders
    ScopeDesc *c1, *s1;
    for (c1= c, s1= s;
         c1 && s1 && c1->s_equivalent(s1);
         c1= c1->sender(), s1= s1->sender())
      ;
    if (c1 == NULL || s1 == NULL) {
      // c corresponds to s
      assert(scope == NULL, "found more than one scope");
      scope = c;
# if !GENERATE_DEBUGGING_AIDS
    if (!CheckAssertions) {
        break;
    }
# endif
    }
  }
  return scope;
}

# if  GENERATE_DEBUGGING_AIDS
PcDesc* nmethod::correspondingPC(ScopeDesc* sd, int32 bci) {
  // find the starting PC of this scope
  assert(scopes->includes(sd), "scope not in this nmethod");  
  int32 scope = scopes->offsetTo(sd);
  PcDesc *p, *end;
  for (p = pcs(), end = pcsEnd(); p < end; p ++) {
    if (p->scope == scope && p->byteCode == bci) break;
  }
  if (p < end ) {
    return p;
  } else {
    // no PC corresponding to this scope
    return NULL;
  }
}
#endif


class sendDescFinder : public abstract_interpreter {
  // Find inline cache for the current byte code index of vf.
  // Note that this may return a primitive call "inline cache".
  // The receiver must be an unoptimized nmethod (no inlining).
  // This code depends on the structure of code generated by the NIC.
  // If wantIntrCheck, return the intr check primitive call rather than the
  // actual send; if !intrCheck, don't return intrCheck if possible
  // (e.g. if current position is a literal, returns intrCheck call anyway).
  
 public:
 
  bool wantIntrCheck;
  nmethod* nm;
  compiled_vframe* vf;
  PcDesc* p;
  PcDesc* pend;
  
  addrDesc* a;
  addrDesc* aend;
  
  char*     target;
  
  sendDesc* sd; 
   
 
  sendDescFinder(compiled_vframe* vf_arg, bool wantIntrCheck, nmethod* nm_arg);
  
  void find_it();
  bool advance_to_pcDesc_for_this_vmethod();
  void advance_to_addrDesc_for_an_inlineCache_in_this_pcDesc();
  void try_for_non_intrCheck_sendDesc();
  bool want_primitiveFailure_branch();
  void find_failure_sendDesc();
  void next_addrDesc_if_not_restart();
  
       
  // shouldn't stop at these
  
  void do_UNDIRECTED_RESEND_CODE() { ShouldNotReachHere(); }
  void do_DELEGATEE_CODE()         { ShouldNotReachHere(); }
  void do_LEXICAL_LEVEL_CODE()     { ShouldNotReachHere(); }
  void do_INDEX_CODE()             { ShouldNotReachHere(); }
  void do_READ_LOCAL_CODE()        { ShouldNotReachHere(); }
  void do_WRITE_LOCAL_CODE()       { ShouldNotReachHere(); }
  void do_POP_CODE()               { ShouldNotReachHere(); }
  
  void do_BRANCH_CODE()            { ShouldNotReachHere(); }
  void do_BRANCH_TRUE_CODE()       { ShouldNotReachHere(); }
  void do_BRANCK_FALSE_CODE()      { ShouldNotReachHere(); }
  void do_BRANCH_INDEXED_CODE()    { ShouldNotReachHere(); }
  
  void do_SEND_CODE()           { next_addrDesc_if_not_restart(); }
  void do_IMPLICIT_SEND_CODE()  { next_addrDesc_if_not_restart(); }
};


sendDescFinder::sendDescFinder(compiled_vframe* vf_arg, bool wci, nmethod* nm_arg) 
 : abstract_interpreter(vf_arg->method()) {
  vf= vf_arg;
  pc= vf->real_bci();
  wantIntrCheck= wci;
  nm= nm_arg;
  
  p=    nm->pcs();
  pend= nm->pcsEnd();
  a=    nm->locs();
  aend= nm->locsEnd();
}


void sendDescFinder::find_it() {
  for ( ;; ++p ) {
    bool cannotIncrementPAnymore = advance_to_pcDesc_for_this_vmethod();
    advance_to_addrDesc_for_an_inlineCache_in_this_pcDesc();
    
    try_for_non_intrCheck_sendDesc();

    if (a >= aend || !a->isCall()) ShouldNotReachHere(); // no addrDesc found

    sd= a->isPrimitive()  ? a->asPrimitiveSendDesc(nm) 
                          : a->asSendDesc(nm);
    
    PcDesc* sdpc= nm->containingPcDesc((char*)sd);
    if ( ( sdpc->byteCode == pc  ||   (pc == PrologueBCI && sdpc->byteCode == mi.firstBCI()))
    &&   sdpc->scope == p->scope) {
      // found the right sendDesc: bytecodes & scope match, or got first
      // intr check (which may be in bci 0 instead of prologue)
      break;
    }
    if (cannotIncrementPAnymore)
      fatal("cannot possibly work to keep iterating");
  }
  if ( want_primitiveFailure_branch() )
    find_failure_sendDesc();
}


bool sendDescFinder::advance_to_pcDesc_for_this_vmethod() { 
  if (pc == PrologueBCI) {
    // Allocate the PcDesc in the resource area. (fix this, Lars)
    p= NEW_RESOURCE_OBJ(PcDesc);
    p->pc = mi.firstBCI();  p->scope = 0;  p->byteCode = PrologueBCI;
    assert(p->containingDesc(nm)->method()->codes() == mi.codes_object,
           "oops");
    return true;
  }
  for (  ;  p < pend;  ++p) {
    if ( p->byteCode == pc
    &&   p->containingDesc(nm)->method()->codes() == mi.codes_object)
       return false;
  }
  fatal("no pcDesc found");
  return false; // avoid stupid warning
}    


void sendDescFinder::advance_to_addrDesc_for_an_inlineCache_in_this_pcDesc() {
  for ( ; a < aend;  a++ ) {
    if ( !a->isCall()  ||  a->offset() < p->pc )
      continue;
    target= a->referent(nm);
      
    // This is probably the send we want; just make sure that lookup
    // miss code etc doesn't mess things up.
    // In the prologue, only InterruptCheck counts
    // NB: if receiver isDebug(), prologue doesn't have intr check
    // because there's one in the 1st byte code    
    
    if ( pc != PrologueBCI 
    ||   target == first_inst_addr(interruptCheck))
      return;
  }
  fatal("no addrDesc found");
}


void sendDescFinder::try_for_non_intrCheck_sendDesc() {
  if ( nm->isDebug()  // must have extra interruptChecks to bypass
  &&  !wantIntrCheck 
  &&  target == first_inst_addr(interruptCheck))
    interpret_bytecode();  // will try for non-intrCheck sendDesc
}


void sendDescFinder::next_addrDesc_if_not_restart() {
  // skip single-stepping code (interrupt check is at beginning of
  // every byte code, before the real send)

  // If send is a local access and !UseLocalAccessBytecodes
  // no sendDesc has been generated;
  // too tricky to check here

  // a is an interrupt check; the next addrDesc must be the call
  // we want, except if it is a _Restart
    
  is.index= mi.map()->get_index_at(pc);
  
  if (get_selector() == VMString[_RESTART]) 
    return; // no real sendDesc to find
    
  // local send (in debug method) will not have a matching addrDesc
  
  if ( &a[1] < aend 
  &&    a[1].isCall()
  &&    a[1].offset() >= p->pc)
    a++;  // the next one is the one we want
}    


bool sendDescFinder::want_primitiveFailure_branch() {
  assert(vf->fr->is_aligned(), "");
  return  a->isPrimitive()  &&  !vf->is_uncommonTrap()  &&  !vf->is_primCall();
}


void sendDescFinder::find_failure_sendDesc() {
  for (  ;  a < aend  &&  !a->isSendDesc();  ++a) 
    ;
  assert(a < aend, "did not find failure branch");
  sd= a->asSendDesc(nm);
  assert(   wantIntrCheck
      ||    sd->selector() == VMString[VALUE_WITH_]
      ||    sd->selector() == VMString[PRIMITIVE_FAILED_ERROR_NAME_],
          "found wrong failure branch");
}



sendDesc* nmethod::sendDescFor(compiled_vframe* vf, bool wantIntrCheck) {
  // Find inline cache for the current byte code index of vf.
  // Note that this may return a primitive call "inline cache".
  // The receiver must be an unoptimized nmethod (no inlining).
  // This code depends on the structure of code generated by the NIC.
  // If wantIntrCheck, return the intr check primitive call rather than the
  // actual send; if !intrCheck, don't return intrCheck if possible
  // (e.g. if current position is a literal, returns intrCheck call anyway).
  if (compiler() != NIC)
    ShouldNotReachHere(); // cannot find sendDesc in optimized method
    
  sendDescFinder sdf(vf, wantIntrCheck, this);
  sdf.find_it();
  return sdf.sd;
}


Map* nmethod::blockMapFor(blockOop bl) {
  // bl is a block that has been remapped; find a block reference in our
  // code which refers to the "same" block (different map, but same value
  // method)
  assert_block(bl, "not a block");
  if (compiler() != NIC) fatal("cannot find blocks in optimized method");
  oop valueMethod = bl->value();
  oop foundBlk = NULL;
  for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
    if (!p->isOop()) // not no oops here
      continue;
    oop blk = oop(p->referent(this));
    if (blk->is_block()) {
      oop method2 = blockOop(blk)->value();
      if (valueMethod == method2) {
        assert(foundBlk == NULL || foundBlk == blk, "duplicate block found");
        foundBlk = blockOop(blk);       // found it
# if !GENERATE_DEBUGGING_AIDS
    if (!CheckAssertions) {
      break;
    }
# endif
      }
    }
  }
  if (method() == valueMethod) {
    // it's the receiver block
    assert(foundBlk == NULL || foundBlk->map() == key.receiverMap(),
           "can't handle duplicate blocks");
    return key.receiverMap();
  }
  if (foundBlk) return foundBlk->map();
  ShouldNotReachHere(); // didn't find block
  return NULL;
}

addrDesc* nmethod::addrDesc_at(char* pc) {
  // return addrDesc for pc or NULL if none
  assert(insts() <= pc && pc < instsEnd(), "not in this nmethod");
  fint offset = pc - insts();
  for (addrDesc* p = locs(), *pend = locsEnd(); p < pend; p++) {
    if (p->offset() == offset) return p;
  }
  return NULL;
}


bool  nmethod::has_frame_at(char* pc) {
  if (isAccess()) return false;
  if (compiler() == NIC) {
    // if the save instruction at frameCreationOffset - word
    // has been overwriten follow the branch to find the save.
    int32* save_inst = (int32*)(insts() + frameCreationOffset- sizeof(long));
    char* save_addr = address_of_overwritten_NIC_save_instruction(save_inst);
    if ( save_addr != NULL ) // save was moved, is at save_addr
      return  pc >= insts() + frameCreationOffset  &&  pc != save_addr;
  }
  return pc >= insts() + frameCreationOffset;
}


bool  nmethod::in_self_code_at(char* pc) {
  PcDesc* pd = containingPcDescOrNULL(pc);
  if (pd == NULL) return false;
  return  pd->byteCode != PrologueBCI
      &&  pd->byteCode != EpilogueBCI;
}

void nmethod_init() {
  // make sure you didn't forget to adjust the filler fields
  assert(sizeof(nmFlags) <= 4, "nmFlags occupies more than a word");
}


static nmethod* constructDoItMethod_cont(oop receiver, oop method) {
  ResourceMark rm;
  compilingLookup L( receiver,
                     VMString[DO_IT],
                     NULL,     // delegatee
                     receiver, // method holder
                     NULL,     // vframe
                     sendDesc::first_sendDesc(),
                     NULL,     // DIDesc
                     false );  // don't want debug version
  L.setResult(method);
  nmethod *nm= L.lookupNMethod();
  nm->makeZombie(); // throw away as soon as it is off the stack
  return nm;
}

nmethod* constructDoItMethod(oop receiver, oop method) {
  return switchToVMStack(receiver, method, constructDoItMethod_cont);
}


# if  GENERATE_DEBUGGING_AIDS
// for debugging: given a descOffset (e.g. from a block), findOffset finds
// all nmethods that have such a descOffset
static fint offset_to_find;
static void findOffsetFn(nmethod* nm) {
  nmethodScopes* s = nm->scopes;
  FOR_EACH_SCOPE(s, scope) {
    if (scope->offset() == offset_to_find) {
      lprintf("nmethod* %#lx\n", nm);
      scope->print();
       return;
    }
  }
}

void findOffset(fint offset) {
  ResourceMark rm;
  offset_to_find = offset;
  Memory->code->nmethods_do(findOffsetFn);
}

#endif

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void nmethod_init() {}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "zoneHeap.hh"
# include "_zoneHeap.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)



enum chunkState {
  ZeroDistance = 0, MaxDistance = 128,
  unused = 128, unusedOvfl = 190,
  used = 192, usedOvfl = 254,
  invalid = 255};

# define MaxDistLog    7  /* log2(MaxDistance) */
# define maxOneByteLen (usedOvfl - used)

// format of chunks in free map: first & last byte hold chunk size
// unused..unusedOvfl-1:unused, length n+1
// used..usedOvfl-1     used, length n - used + 1
// unusedOvfl:          unused, encoded length in next (prev) 3 bytes
// usedOvfl:            used, encoded length in next (prev) 3 bytes

// The other bytes hold the distance to the chunk header (or an approximation
// thereof); headers are found by following the distance pointers downwards

const int32 minHeaderSize = 1;
const int32 maxHeaderSize = 4;

class ChunkMap;
inline ChunkMap* asChunkMap(u_char* c)  { return (ChunkMap*)c; }

class ChunkMap {
  u_char c(int32 which)                 { return ((u_char*)this)[which]; }
  u_char n(int32 which)                 { return c(which) - unused; }
 public:
  ChunkMap()                            { fatal("shouldn't create"); }
  u_char* asByte()                      { return (u_char*)this; }
  void markSize(int32 nChunks, chunkState s);
  void markUsed(int32 nChunks)          { markSize(nChunks, used); }
  void markUnused(int32 nChunks)        { markSize(nChunks, unused); }
  ChunkMap* findStart(ChunkMap* mapStart, ChunkMap* mapEnd);
  
  void read_snapshot(FILE* f);
  void write_snapshot(FILE* f);
  bool verify();
# if  GENERATE_DEBUGGING_AIDS
  void print();
#endif
  bool isValid();
  void invalidate()                     { asByte()[0] = invalid; }
  
  chunkState state()                    { return chunkState(c(0)); }
  bool isUsed()                         { return state() >= used; }
  bool isUnused()                       { return ! isUsed(); }
  int32 headerSize() {          // size of header in bytes
    int32 ovfl = isUsed() ? usedOvfl: unusedOvfl;
    return c(0) == ovfl ? maxHeaderSize : minHeaderSize;
  }
  int32 size() {                // size of this block
    int32 ovfl = isUsed() ? usedOvfl: unusedOvfl;
    int32 len;
    assert(c(0) != invalid && c(0) >= MaxDistance, "invalid chunk");
    if (c(0) != ovfl) {
      len = c(0) + 1 - (isUsed() ? used : unused);
    } else {
      len = (((n(1) << MaxDistLog) + n(2)) << MaxDistLog) + n(3);
    }
    assert(len > 0, "negative/zero chunk length");
    return len;
  }
  bool contains(u_char* p) { return asByte() <= p && p < asByte() + size(); }
  ChunkMap*  next() { return asChunkMap(asByte()+size()); }
  ChunkMap*  prev() {
    ChunkMap* p = asChunkMap(asByte() - 1);
    int32 ovfl = p->isUsed() ? usedOvfl: unusedOvfl;
    int32 len;
    if (c(-1) != ovfl) {
      len = p->size();
    } else {
      len = (((n(-4) << MaxDistLog) + n(-3)) << MaxDistLog) + n(-2);
    }
    assert(len > 0, "negative/zero chunk length");
    return asChunkMap(asByte() - len);
  }
};

void ChunkMap::markSize(int32 nChunks, chunkState s) {
  // write header
  u_char* p = asByte();
  u_char* e = p + nChunks - 1;
  if (nChunks < maxOneByteLen) {
    p[0] = e[0] = s + nChunks - 1;
  } else {
    assert(nChunks < (1<<(3*MaxDistLog)), "chunk too large");
    unsigned mask = nthMask(MaxDistLog);
    p[0] = e[0] = (s == used) ? usedOvfl : unusedOvfl;
    p[1] = e[-3] = unused +  (nChunks >> (2*MaxDistLog));
    p[2] = e[-2] = unused + ((nChunks >>    MaxDistLog) & mask);
    p[3] = e[-1] = unused + ( nChunks                   & mask);
  }
  assert(size() == nChunks, "incorrect size encoding");
  // mark distance for used blocks
  if (s == unused) {
    // don't mark unused blocks - not necessary, and would be a performance 
    // bug (start: *huge* free block, shrinks with every alloc -> quadratic)
    // however, the first distance byte must be correct (for findStart)
    if (nChunks > 2 * minHeaderSize) p[headerSize()] = headerSize();
  } else {
    if (nChunks < maxOneByteLen) {
      assert(maxOneByteLen <= MaxDistance, "oops!");
      for (int32 i = minHeaderSize; i < nChunks - minHeaderSize; i++) p[i] = i;
    } else {
      int32 max = min(nChunks - 4, MaxDistance);
      int32 i;
      for ( i = maxHeaderSize; i < max; i++) p[i] = i;
      // fill rest with large distance values (don't use MaxDistance - 1 because
      // the elems MaxDistance..MaxDistance+maxHeaderSize-1 would point *into*
      // the header)
      for ( ; i < nChunks - maxHeaderSize; i++)
        p[i] = MaxDistance - maxHeaderSize;
    }
  }
}

ChunkMap* ChunkMap::findStart(ChunkMap* mapStart, ChunkMap* mapEnd) {
  // this points into the middle of a chunk; return start of chunk
  u_char* p = asByte();
  u_char* start = mapStart->asByte();
  u_char* end   = mapEnd  ->asByte();
  ChunkMap* m;
  if (*p < MaxDistance) {
    // we're outside the header, so just walk down the trail
    # if  GENERATE_DEBUGGING_AIDS
      u_char *lastp = NULL, *lastlastp = NULL;
    # endif
    while (*p < MaxDistance) {
      assert(*p, "stuck in endless loop");
      # if  GENERATE_DEBUGGING_AIDS
        if (SpendTimeForDebugging) {
          lastlastp = lastp;
          lastp = p;
        }
      # endif
      p -= *p;
    }
    assert(p >= start, "not found");
    m = asChunkMap(p);
  } else {
    // pointing to a header, but we don't know whether long/short etc
    // first walk up to first non-header byte
    // (note that first distance byte of unused blocks is correct, but
    // the others aren't)
    while (*p >= MaxDistance && p < end) p++;
    if (p < end) {
      // find start of this block
      while (*p < MaxDistance) p -= *p;
      assert(p >= start, "not found");
    }
    m = asChunkMap(p);
    while (! m->contains(this->asByte())) m = m->prev();
  }
  assert(m->verify(), "invalid chunk map");
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions) {
    if (!m->contains(this->asByte())) {
      error4("ChunkMapFindStart found wrong chunk,\n"
             "\tthis = 0x%x, mapStart = 0x%x, mapEnd = 0x%x, found = 0x%x\n",
             this, mapStart, mapEnd, m);
      lprintf("this     is %s\n",           isValid() ? "valid" : "invalid");
      lprintf("mapStart is %s\n", mapStart->isValid() ? "valid" : "invalid");
      lprintf("mapEnd   is %s\n", mapEnd  ->isValid() ? "valid" : "invalid");
      lprintf("result   is %s\n", m       ->isValid() ? "valid" : "invalid");
      Memory->code->verify();
      fatal("wrong chunk");
    }
  }
# endif
  return m;
}

bool ChunkMap::isValid() {
  u_char* p = asByte();
  bool ok;
  if (p[0] == invalid || p[0] < MaxDistance) {
    ok = false;
  } else {
    u_char* e = next()->asByte() - 1;
    int32 ovfl = isUsed() ? usedOvfl: unusedOvfl;
    ok = p[0] == e[0] &&
        (p[0] != ovfl || (p[1] == e[-3] && p[2] == e[-2] && p[3] == e[-1]));
  }
  return ok;
}

# if  GENERATE_DEBUGGING_AIDS
void ChunkMap::print() {
  lprintf("chunk [%#lx..%#lx[\n", (long unsigned)this, (next()));
}
#endif

bool ChunkMap::verify() {
  if (! isValid()) {
    error1("inconsistent chunk map %#lx", this);
    return false;
  }
  return true;
}


void FreeList::append(HeapChunk* h) {
  h->link.init();
  link.add(&h->link);
}

void FreeList::remove(HeapChunk* h) {
  h->link.remove();
}
  
HeapChunk* FreeList::get() {
  if (link.isEmpty()) {
    return NULL;
  } else {
    HeapChunk* res = anchor()->next();
    remove(res);
    return res;
  } 
}

int32 FreeList::length() {
  int32 i = 0;
  HeapChunk* f = anchor();
  for (HeapChunk* p = f->next(); p != f; p = p->next()) i++;
  return i;
}


Heap::Heap(int32 s, int32 bs, int32 nf, char* baseAddr, bool och) {
  assert(nf > 0 && s > bs && bs >= 16, "invalid params");
  assert(s % bs == 0, "size not a multiple of blockSize");
  size = s;
  if (bs & (bs - 1)) fatal1("heap block size (%ld) isn't power of 2", bs);
  blockSize = bs;
  log2BS = 0;
  while (bs > 1) { bs >>= 1; log2BS++; }
  nfree = nf;
  if (baseAddr) {
    _base = baseAddr;
    on_c_heap= och;
  } else {
    _base = AllocateHeap(size + blockSize, "zone");
    on_c_heap = true;
  }
  base = (char*) ( ((int32)_base + blockSize - 1) & ~(blockSize - 1) );
  assert(int32(base) % blockSize == 0, "base not aligned to blockSize");
  heapMap = (ChunkMap*)(AllocateHeap(mapSize() + 2, "zone free map") + 1);
  // + 2 for sentinels
  freeList = NEW_C_HEAP_ARRAY( FreeList, nfree);
  newHeap = NULL;
  clear();
}

void Heap::clear() {
  bytesUsed = total = ifrag = 0;
  for(int32 i = 0; i < nfree; i++) freeList[i].clear();
  bigList.clear();
  heapMap->markUnused(mapSize());
  heapEnd()->markUsed(1);               // sentinels
  asChunkMap(heapMap->asByte() - 1)->markUsed(1);
  combineMode = false;
  lastCombine = heapMap;
  addToFreeList(heapMap);
}

Heap::~Heap() {
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      set_oops((oop*)base, capacity() / oopSize, NULL);
    }
# endif
  if (on_c_heap) selfs_free(_base);
  selfs_free(heapMap - 1);          // -1 to get rid of sentinel
  selfs_free(freeList);
}

void Heap::removeFromFreeList(ChunkMap* m) {
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      m->verify();
    }
# endif
  HeapChunk* p = (HeapChunk*)blockAddr(m);
  p->link.remove();
}

bool Heap::addToFreeList(ChunkMap* m) {
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      m->verify();
    }
# endif
  HeapChunk* p = (HeapChunk*)blockAddr(m);
  int32 sz = m->size();
  if (sz <= nfree) {
    freeList[sz-1].append(p);
    return false;
  } else {
    bigList.append(p);
    p->size = sz;
    return true;
  }
}

void* Heap::allocFromLists(int32 wantedBytes) {
  assert(wantedBytes % blockSize == 0, "not a multiple of blockSize");
  int32 wantedBlocks = wantedBytes >> log2BS;
  assert(wantedBlocks > 0, "negative alloc size");
  int32 blocks = wantedBlocks - 1;
  void* p = NULL;
  while (!p && ++blocks <= nfree) {
    p = freeList[blocks-1].get();
  }
  if (! p) {
    HeapChunk *c, *f = bigList.anchor();
    for (c = f->next();
         c != f && c->size < wantedBlocks;
         c = c->next()) {}
    if (c == f) {
      if (! combineMode && combineAll() >= wantedBlocks)
        return allocFromLists(wantedBytes);
    } else {
      p = c;
      blocks = c->size;
      bigList.remove(c);
    }
  } 
  if (p) {
    ChunkMap* m = mapAddr(p);
    assert(m->size() == blocks, "inconsistent sizes");
    m->markUsed(wantedBlocks);
    if (blocks > wantedBlocks) {
#ifdef LOG_LOTSA_STUFF
      if (!bootstrapping) LOG_EVENT("zoneHeap: splitting allocated block");
#endif
      int32 freeChunkSize = blocks - wantedBlocks;
      ChunkMap* freeChunk = m->next();
      freeChunk->markUnused(freeChunkSize);
      addToFreeList(freeChunk);
    }
  }
  return p;
}

void* Heap::allocate(int32 wantedBytes) {
  assert(wantedBytes > 0, "Heap::allocate: size <= 0");
  int32 rounded = ((wantedBytes + blockSize - 1) >> log2BS) << log2BS;
  
  void* p = allocFromLists(rounded);
  if (p) {
    bytesUsed += rounded;
    total += rounded;
    ifrag += rounded - wantedBytes;
  }
  if (VerifyZoneOften) {
    for (int32* pp = (int32*)p;  (char*)pp  <  (char*)p + wantedBytes;  ++pp)
      *pp = 0xdadbadee;
    verify();
  }
  return p;
}


void Heap::deallocate(void* p, int32 bytes) {
  if (VerifyZoneOften) {
    for ( int32* pp = (int32*)p;  (char*)pp < (char*)p + bytes; ++pp)
      *pp = 0xbadbad25;
  }
  ChunkMap* m = mapAddr(p);
  int32 myChunkSize = m->size();
  int32 blockedBytes = myChunkSize << log2BS;
  bytesUsed -= blockedBytes;
  ifrag -= blockedBytes - bytes;
  m->markUnused(myChunkSize);
  bool big = addToFreeList(m);
  HeapChunk* c = (HeapChunk*)p;
  if (combineMode || big) combine(c);   // always keep bigList combined

  if (VerifyZoneOften) {
    verify();
  }
}

# define INC(p, n)   p = asChunkMap(p->asByte() + n)

char* Heap::compact(moveChunkFn move) {
  if (usedBytes() == capacity()) return NULL;
  
  ChunkMap* m = heapMap;
  ChunkMap* end = heapEnd();
  
  ChunkMap* freeChunk = m;
  while (freeChunk->isUsed()) {                         // find 1st unused blk
    freeChunk = freeChunk->next();
  }
  ChunkMap* usedChunk = freeChunk;
  
  for(;;) {
    while (usedChunk->isUnused()) usedChunk = usedChunk->next();
    if (usedChunk == end) break;
    int32 uSize = usedChunk->size();
    assert(freeChunk < usedChunk, "compaction bug");
    move(blockAddr(usedChunk), blockAddr(freeChunk), uSize << log2BS);
    freeChunk->markUsed(uSize);   // must come *after* move
    INC(freeChunk, uSize);
    INC(usedChunk, uSize);
  }
  for(int i = 0; i < nfree; i++) freeList[i].clear();
  bigList.clear();
  int32 freeBlocks = end->asByte() - freeChunk->asByte();
  freeChunk->markUnused(freeBlocks);
  addToFreeList(freeChunk);
  assert(freeBlocks * blockSize == capacity() - usedBytes(),
         "usage info inconsistent");
  lastCombine = heapMap;
  return blockAddr(freeChunk);
}  

int32 Heap::combine(HeapChunk*& c) {
  // Try to combine c with its neighbors; on return, c will point to
  // the next element in the freeList, and the return value will indicate
  // the size of the combined block.
  ChunkMap* cm = mapAddr((char*)c);
  assert(cm < heapEnd(), "beyond heap");
  ChunkMap* cmnext = cm->next();
  ChunkMap* cm1;
  if (cm == heapMap) {
    cm1 = cm;
  } else {
    cm1 = cm->prev();                   // try to combine with prev
    while (cm1->isUnused()) {           // will terminate because of sentinel
      ChunkMap* free = cm1;
      cm1 = free->prev();
      removeFromFreeList(free);
      free->invalidate();               // make sure it doesn't look valid
    }
    cm1 = cm1->next();
  }
  ChunkMap* cm2 = cmnext;               // try to combine with next
  while (cm2->isUnused()) {             // will terminate because of sentinel
    ChunkMap* free = cm2;
    cm2 = cm2->next();
    removeFromFreeList(free);
    free->invalidate();                 // make sure it doesn't look valid
  }

  // The combined block will move to a new free list; make sure that c
  // returns an element in the current list so that iterators work.
  c = c->next();

  if (cm1 != cm || cm2 != cmnext) {
    removeFromFreeList(cm);
    cm->invalidate();   
    cm1->markUnused(cm2->asByte() - cm1->asByte());
    addToFreeList(cm1);
    lastCombine = cm1;
  }
  assert(cm1 >= heapMap && cm2 <= heapEnd() && cm1 < cm2, "just checkin'");
  return cm1->size();
}

// Try to combine adjacent free chunks; return size of biggest chunk (in blks).
int32 Heap::combineAll() {
  int32 biggest = 0;
  for (int32 i = 0; i < nfree; i++) {
    HeapChunk* f = freeList[i].anchor();
    for (HeapChunk* c = f->next(); c != f; ) {
      HeapChunk* c1 = c;
      int32 sz = combine(c);
      if (c1 == c) fatal("infinite loop detected while combining blocks");
      if (sz > biggest) biggest = sz;
    }
  }
  combineMode = true;
  if (VerifyZoneOften) {
    verify();
  }
  return biggest;
}

void* Heap::firstUsed() {
  if (usedBytes() == 0) return NULL;
  if (heapMap->isUsed()) {
    return base;
  } else {
    return nextUsed(base);
  }
}

// return next used chunk with address > p
void* Heap::nextUsed(void* p) {
  ChunkMap* m = mapAddr(p);
  if (m->isValid() && !lastCombine->contains(m->asByte())) {
    if (VerifyZoneOften) {
      ChunkMap *m1;
      for (m1 = heapMap; m1 < m; m1 = m1->next()) ;
      assert(m1 == m, "m isn't a valid chunk");
    }
    assert(m->verify(), "valid chunk doesn't verify");
    m = m->next();
  } else {
    // m is pointing into the middle of a block (because of block
    // combination)
    ChunkMap *n;
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      ChunkMap *m1;
      for (m1 = heapMap; m1 < lastCombine; m1 = m1->next()) ;
      assert(m1 == lastCombine, "lastCombine not found");
      assert(lastCombine->verify(), "invalid lastCombine");
    }
#   endif
    for (n = lastCombine; n <= m; n = n->next()) ;
    m = n;
    assert(m->isValid(), "something's wrong");
  }
  
  while (m->isUnused()) {
    m = m->next();
  } 
  
  assert(m->isValid(), "something's wrong");
  assert(m <= heapEnd(), "past end of heap");
  if (m == heapEnd()) {
    return NULL;                                        // was last one
  } else {
    void* next = blockAddr(m);
    assert(next > p, "must be monotonic");
    assert(!(int(next) & 1), "must be even");
    return next;
  }
}

void* Heap::findStartOfBlock(void* start) {
  // used a lot -- help the optimizer a bit
  if (newHeap && newHeap->contains(start))
    return newHeap->findStartOfBlock(start);
  
  const fint  blockSz = blockSize;      
  start = (void*)(int32(start) & ~(blockSz-1));
  assert(contains(start), "start not in this zone");
  ChunkMap* m = mapAddr(start)->findStart(heapMap, heapEnd());
  return blockAddr(m);
}

int32 Heap::sizeOfBlock(void* p) {
  return mapAddr(p)->size() << log2BS;
}

bool Heap::verify() {
  bool r = true;
  ChunkMap* m = heapMap;
  ChunkMap* end = heapEnd();
  if (end->isUnused() || end->size() != 1) {
    error2("wrong end-sentinel %d in heap 0x%lx", (int)*(u_char*)end, this);
    r = false;
  }
  ChunkMap* begin = asChunkMap(heapMap->asByte() - 1);
  if (begin->isUnused() || begin->size() != 1) {
    error2("wrong begin-sentinel %d in heap 0x%lx", (int)*(u_char*)begin, this);
    r = false;
  }

  
  // verify map structure
  for (  ;  m < end;  m = m->next()) {
    if (!m->verify()) {
      lprintf(" in heap 0x%lx", this);
      r = false;
    }
  }
  // verify free lists
  int32 i;
  for (i = 0; i < nfree; i++) {
    int32 j = 0;
    int32 lastSize = 0;
    HeapChunk* f = freeList[i].anchor();
    for(HeapChunk* h = f->next(); h != f; h = h->next(), j++) {
      ChunkMap* p = mapAddr(h);
      if (!p->verify()) {
        lprintf(" in free list %ld (elem %ld) of heap 0x%lx",
               long(i), long(j), (long unsigned)this); 
        r = false;
      }
      if (p->isUsed()) {
        error4("inconsistent freeList %ld elem 0x%lx in heap 0x%lx (map %#lx)",
               i, h, this, p);
        r = false;
      }
      if (p->size() != lastSize && j != 0) {
        error4("freeList %ld elem %#lx in heap %#lx (map %#lx) has wrong size",
               i, h, this, p);
        r = false;
      }
      lastSize = p->size();
      if (h->next() == h) {
        error3("loop in freeList %ld elem %#lx in heap %#lx", i, h, this);
        r = false;
        break;
      }
    }
  }
  int j = 0;
  HeapChunk* f = bigList.anchor();

  for(HeapChunk* h = f->next(); h != f; h = h->next(), j++) {
    ChunkMap* p = mapAddr(h);
    if (!p->verify()) {
      lprintf(" in bigList (elem %ld) of heap 0x%lx", long(j), this);
      r = false;
    }
    if (p->isUsed()) {
      error4("inconsistent freeList %ld elem 0x%lx in heap 0x%lx (map 0xlx)",
             i, h, this, p);
      r = false;
    }
  }
  if (! lastCombine->verify()) {
    error1("invalid lastCombine in heap %#lx", this);
    r = false;
  }
  return r;
}

void Heap::print() {
  lprintf("%#lx: [%#lx..%#lx)\n",
         this,  base,  base + capacity());
  printIndent();
  lprintf("  size %ld (blk %ld), used %ld (%1.1f%%), ifrag %1.1f%%;\n",
         capacity(), blockSize, usedBytes(),
         100.0 * usedBytes() / capacity(), 100.0 * intFrag());
  printIndent();
  lprintf("  grand total allocs = %ld bytes\n", long(total));
  printIndent();
  lprintf("  free lists: ");
  for (int i = 0; i < nfree; i++) lprintf("%ld ", freeList[i].length());
  lprintf("; %ld\n", bigList.length());
}

static char zone_heap_delim[] = "\n\f\na zone heap\n\f\n!%n";

void ChunkMap::write_snapshot(FILE* f) {
  u_char* p = asByte();
  int32 nChunks = size();
  if (nChunks < maxOneByteLen) {
    OS::FWrite(p, 1, f);
  } else {
    OS::FWrite(p, maxHeaderSize, f);
  }
}

void Heap::write_snapshot(FILE* f) {
  write_delim(f, zone_heap_delim);

  OS::FWrite(this, sizeof(Heap), f);
  OS::FWrite(freeList, sizeof(FreeList) * nfree, f);
  ChunkMap* m = heapMap;
  ChunkMap* end = heapEnd();
  while (m < end) {
    m->write_snapshot(f);
    char* nm = blockAddr(m);
    int sz = m->size();
    if (m->isUsed()) {
      // write used blocks
      OS::FWrite(nm, sz * blockSize, f);
    } else {
      // write link info of unused block
      OS::FWrite(nm, sizeof(HeapChunk), f);
    }
    m = m->next();
  }
}

void ChunkMap::read_snapshot(FILE* f) {
  u_char* p = asByte();
  OS::FRead(p, minHeaderSize, f);
  if (headerSize() > minHeaderSize) {
    assert(headerSize() == maxHeaderSize, "should be either min or max");
    OS::FRead(p + minHeaderSize, maxHeaderSize - minHeaderSize, f);
  }
  assert(size() > 0, "wrong size");
  markSize(size(), isUsed() ? used : unused);
}

void Heap::read_snapshot(FILE* f) {
  check_delim(f, zone_heap_delim);
  
  char* buf[sizeof(Heap)];
  Heap* theHeap = (Heap*)buf;
  
  OS::FRead(buf, sizeof(Heap), f);
  if (okToUseCodeFromSnapshot
      && (   theHeap->size != size
          || theHeap->base != base
          || theHeap->nfree != nfree
          || theHeap->blockSize != blockSize))
    noCodeWarning(
      "because of a mismatch in the size or address of a zone heap");

  if (okToUseCodeFromSnapshot) {
    ChunkMap* m = heapMap;
    FreeList* fr = freeList;
    *this = *theHeap;
    heapMap= m;
    if ((caddr_t)lastCombine < NMethodStart)
      lastCombine= heapMap + (lastCombine - theHeap->heapMap);
    freeList= fr;       // "relocation"
  }
  OS::read_or_seek(freeList, sizeof(FreeList) * theHeap->nfree, f);
  ChunkMap* m = heapMap;
  int32 blocksToRead = theHeap->heapEnd() - theHeap->heapMap;
  int32 sz;
  for (int32 blocksRead = 0; blocksRead < blocksToRead; blocksRead += sz) {
    m->read_snapshot(f);
    char* nm = blockAddr(m);
    sz = m->size();
    if (m->isUsed()) {
      // read used blocks
      OS::read_or_seek(nm, sz * theHeap->blockSize, f);
    } else {
      // read link info of unused block
      OS::read_or_seek(nm, sizeof(HeapChunk), f);
    }
    if (okToUseCodeFromSnapshot) m= m->next();
  }
    
  relocate_nmlns();

# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  okToUseCodeFromSnapshot) verify();
# endif
}

void Heap::relocate_nmlns() 
{
  // relocate nmlns after reading snapshot (zone may have moved)
  for (int i= 0;  i < nfree;  i++)
    Memory->code->relocate_nmln(&freeList[i].link);
  Memory->code->relocate_nmln(&bigList.link);
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "countPattern.hh"
# include "_countPattern.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  const fint CountCodePattern::BadOffset = -9999999;



# endif // FAST_COMPILER || SIC_COMPILER
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "fprofiler.hh"
# include "_fprofiler.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


# define ticksPerSec  100
# define SHIFT  5               
# define SCALE  (1 << SHIFT)   /* (# of insts mapped to same counter) / 2 */
# define PROFIL_SCALE   (0x10000 >> SHIFT)   /* see profil(2) man page */
# define scale(offset)  ((offset >> SHIFT) / sizeof(PCounter))

# if  TARGET_OS_VERSION == MACOSX_VERSION   &&  OSX_RELEASE >= MOUNTAIN_LION_RELEASE 

extern "C" int profil( char* /*buf*/,
            size_t   /*bufsiz*/,
            unsigned long    /*offset*/,
            unsigned int    /*scale*/ ) {
  fatal("unimp mac");
  return 0;
}
# elif TARGET_OS_FAMILY == UNIX_FAMILY
/* Pick this up from header file:
  extern "C" void profil(unsigned short *buf,
                         unsigned int bufsiz,
                         unsigned int offset,
                         unsigned int scale);
*/
                       
# endif


FlatProfiler* flatProfiler;
typedef uint16 PCounter;
static char* buf = NULL;
static int32 bufSize;

# define bufStart  ((PCounter*) buf)
# define bufEnd    ((PCounter*)(buf + bufSize))

// count buffer address for memory address m
inline PCounter* bufAddr(void* m) {
  int32 offset = (char*)m - Memory->code->instsStart();
  PCounter* p = bufStart + scale(offset);
  if (p < bufEnd) {
    // everything is ok
  } else {
    // the stubs zone has grown - can't handle this yet
    p = bufEnd - 1;
    *p = 0;     // just to make sure the results aren't bogus
  }
  assert(p >= bufStart && p < bufEnd, "outside of buffer");
  return p;
}

inline bool isInBuffer(void* m) {
  int32 offset = (char*)m - Memory->code->instsStart();
  PCounter* p = bufStart + scale(offset);
  return p < bufEnd;
}


int32 ticksFor(char* s, char* e) {
  PCounter* start = bufAddr(s);
  PCounter* end   = bufAddr(e);
  int32     sum   = 0;
  for (PCounter* p = start; p <= end; p++) sum += *p;
  return sum;
}

int32 ticksFor(nmethod* m) {
  assert(isInBuffer(m->insts()), "should be in buffer");
  assert(isInBuffer(m->instsEnd()), "should be in buffer");
  return ticksFor(m->insts(), m->instsEnd());
}

FlatProfiler::FlatProfiler() {
  flatProfiler = this;
  // BUG: doesn't handle growing PIC zone
  int32 size = Memory->code->stubs->zone()->endAddr() -
               Memory->code->instsStart();
  bufSize = roundTo(size >> SHIFT, oopSize) + oopSize;
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      // to double-check correct parameters for profil() - the 2nd half must
      // remain empty
      bufSize *= 2;
    }
# endif
  buf = AllocateHeap(bufSize, "profile counters");
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      bufSize /= 2;
      memset(buf + bufSize, 0, bufSize);
    }
# endif
  assert(isInBuffer(Memory->code->stubs->zone()->endAddr()), "oops");
  assert(isInBuffer(Memory->code->instsStart()), "oops");
  ResetFlatProfile_prim(0);
}

void initFlatProfiler() {
  if (flatProfiler) fatal("only one profiler, please");
  flatProfiler = new FlatProfiler();
}

void FlatProfiler::clear(char* startPC, char* endPC) {
  if (! flatProfiler) return;
  char* start = (char*)bufAddr(startPC);
  char* end   = (char*)(bufAddr(endPC) + 1);
  memset(start, 0, end - start);
}

void FlatProfiler::flush(char* startPC, char* endPC) {
  if (! flatProfiler) return;
  flushed += ticksFor(startPC, endPC);
  clear(startPC, endPC);
}

void FlatProfiler::move(char* startPC, char* endPC, char* toPC) {
  if (! flatProfiler) return;
  if (isInBuffer(startPC) && isInBuffer(toPC)) {
    PCounter* from = bufAddr(startPC);
    PCounter* end  = bufAddr(endPC);
    PCounter* to   = bufAddr(toPC);
    PCounter* p;
    for (p = from; p < end; ) *to++ = *p++;
    assert(p <= bufEnd, "outside of buffer");
  }
}

class nmTime {
 public:
  nmethod* m;
  int32 ticks;
};

typedef int compareFn(nmTime*, nmTime*);

# define FOR_ALL_NMETHODS(var)                                                \
    for (nmethod *var = Memory->code->first_nm();                             \
    var; var = Memory->code->next_nm(var))

static int compar(const void* t1,  const void* t2) {
  return ((nmTime*)t2)->ticks - ((nmTime*)t1)->ticks;
}

static int32 printStubNumbers() {
  int32 total;
  Heap* stubs = Memory->code->stubs->zone();
  if (isInBuffer(stubs->endAddr())) {
    assert(isInBuffer(stubs->startAddr()), "oops");
    total = ticksFor(stubs->startAddr(), stubs->endAddr() - 1);
    lprintf("Total ticks in PICs/count stubs = %ld (%3.2fs)\n",
           long(total), (float)total / ticksPerSec);
    int32 pics = 0, counts = 0, npics = 0, ncounts = 0,
          npics2 = 0, ncounts2 = 0;
    FOR_ALL_STUBS(st) {
      int32 t = ticksFor(st->insts(), st->instsEnd());
      if (st->isCacheStub()) {
        pics += t;
        npics++;
        if (t) npics2++;
      } else {
        counts += t;
        ncounts++;
        if (t) ncounts2++;
      }
    }
    lprintf("Approx. ticks in PICs     = %ld (%3.2fs)\t(%d/%d)\n",
           long(pics), (float)pics / ticksPerSec, npics2, npics);
    lprintf("Approx. ticks in counters = %ld (%3.2fs)\t(%d/%d)\n",
           long(counts), (float)counts / ticksPerSec, ncounts2, ncounts);
  } else {
    lprintf("Total ticks in PICs/count stubs = N/A (PIC zone has grown since start)\n");
    total = 0;
  }
  return total;
}

# if  GENERATE_DEBUGGING_AIDS
  PCounter* findGT(int32 x) {   // for debugging
    PCounter* p = (PCounter*)buf;
    PCounter* e = (PCounter*)(buf + bufSize);
    while (p < e && *p <= x) p++;
    if (p < e) {
      return p;
    } else {
      return NULL;
    }
  }
# endif

oop ResetFlatProfile_prim(oop r) {
  if (flatProfiler) {
    char* start = Memory->code->instsStart();
    char* end = Memory->code->stubs->zone()->endAddr();
    flatProfiler->clear(start, end - 1);
  } else {
    initFlatProfiler();
  }
  flatProfiler->flushed = 0;
  return r;
}


oop PrintFlatProfile_prim(oop rcvr, smi lines) {
  if (! flatProfiler) {
    lprintf("\nError: profiler is inactive; use _FlatProfile: true to activate\n");
    return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
  }
  ResourceMark mark;
# if GENERATE_DEBUGGING_AIDS
    PCounter* p;
    if (CheckAssertions) {
      for (p = bufEnd; p < bufEnd + bufSize / sizeof(PCounter); p++) {
        if (*p != 0)
          error2("fprofiler: word at %#lx (offset %d) is non-zero",
                 p, p - bufStart);
      }
    }
# endif
    
  int32 n = 0;
  FOR_ALL_NMETHODS(dummy) n++;
  if (n < lines) lines = n;
  nmTime* times = NEW_RESOURCE_ARRAY( nmTime, n);
  int32 total = 0;
  int32 access = 0;
  int32 nicCompiled = 0;
  int32 i = 0;
  FOR_ALL_NMETHODS(m) {
    times[i].m = m;
    int32 t = ticksFor(m);
    times[i].ticks = t;
    total += t;
    if (m->isAccess()) access += t;
    if (m->compiler() == NIC) nicCompiled += t;
    i++;
  }
  qsort(times, n, sizeof(nmTime), compar);
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      char* instsEnd = Memory->code->instsStart() + Memory->code->instsSize();
      int32 total2 = ticksFor(Memory->code->instsStart(), instsEnd);
      if (total != total2)
        error2("inconsistent total: is %d, should be %d", total, total2);
      // clear out all nmethod counters to see where the rest is
      int32 cleared = 0;
      FOR_ALL_NMETHODS(nm) {
        nmethod* nextnm = Memory->code->next_nm(nm);
        assert(!nextnm || bufAddr(nm->instsEnd()) < bufAddr(nextnm->insts()),
               "nmethods overlap (SHIFT too big)");
        cleared += ticksFor(nm);
        flatProfiler->clear(nm->insts(), nm->instsEnd());
      }
      PCounter* end = bufAddr(instsEnd);
      for (p = bufStart; p < end + 1; p++) {
        if (*p != 0) {
          // compute corresponding PC
          fint pc_offset = ((p - bufStart) << SHIFT) * sizeof(PCounter);
          char* pc = Memory->code->instsStart() + pc_offset;
          assert(bufAddr(pc) == p, "oops");
          error2("fprofiler: word at %#lx (offset %d) is non-zero",
                 p, p - bufStart);
        }
      }
    }
# endif
  total += flatProfiler->flushed;
  lprintf("\nTotal ticks in Self = %ld (%3.2fs)\n",
         long(total), (float)total / ticksPerSec);
  if (flatProfiler->flushed) {
    lprintf("Total ticks in flushed nmethods = %ld (%3.2fs)\n",
           long(flatProfiler->flushed), (float)flatProfiler->flushed / ticksPerSec);
  }

  int32 pics = printStubNumbers();

  total += pics;
  if (total) {
    int32 topN = 0;
    for(i = 0; i < lines && times[i].ticks; i++) {
      nmethod *m = times[i].m;
      lprintf("%6ld %5.1f%%  %s",
             long(times[i].ticks),
             (100.0 * times[i].ticks) / total,
             m->key.selector_string());
      if (m->isAccess()) {
        if (WizardMode) {
          lprintf(" (access method for map 0x%lx)",
                 (long unsigned)(m->key.receiverMap()));
        } else {
          lprintf(" (access method)");
        }
      } else {
        methodMap* mm = (methodMap *) m->method()->map();
        stringOop file = mm->file();
        if (file->length() > 0) {
          lprintf(" (");
          file->string_print();
          lprintf(":%ld)", long(mm->line()->value()));
        }
      }
      if (WizardMode)
        lprintf("  (%s %#lx%s%s%s)",
               VMString[m->compiler()]->copy_null_terminated(), long(m),
               m->isYoung() ? " Y" : "",
               m->isToBeRecompiled() ? " TBR" : "",
               m->isUncommonRecompiled() ? " UNCOMMON" : "");
      lprintf("\n");
      topN += times[i].ticks;
    }
    lprintf("%6ld %5.1f%%  %s\n", long(topN), (100.0 * topN) / total,
           "(above methods combined)");
    lprintf("%6ld %5.1f%%  %s\n", long(access), (100.0 * access) / total,
           "(all data access methods combined)");
    lprintf("%6ld %5.1f%%  %s\n\n", long(nicCompiled), (100.0 * nicCompiled) / total,
           "(all NIC-compiled methods combined)");
  }
  return rcvr;
}

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)

oop  ResetFlatProfile_prim(oop r) { return r; }
oop PrintFlatProfile_prim(oop r, smi lines) { 
  Unused(lines); Unused(r); return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR); }

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

// need this entry point either way

void SelfFlatProfile(bool on) {
  # if TARGET_OS_VERSION == MACOSX_VERSION
    typedef  char *           buf_t;
  # else // ! MACOSX_VERSION
    typedef unsigned short *  buf_t;
    // Linux???
  # endif

  # if !TARGET_IS_PROFILED && ( defined(FAST_COMPILER) || defined(SIC_COMPILER) )
      // won't work in the profiled Self version
      if (on) {
        if (! flatProfiler) initFlatProfiler();
          profil((buf_t) buf, bufSize, (int) Memory->code->instsStart(), PROFIL_SCALE);
        FlatProfile = true;
      }
      else {
        profil((buf_t)buf, 0, 0, 0);
        FlatProfile = false;
      }
  # else
      Unused(on);
      warning("_FlatProfile: only works with unprofiled VM");
  # endif
}

/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "scopeDescRecorder.hh"
# include "_scopeDescRecorder.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


# define INITIAL_SLOT_SIZE     10
# define INITIAL_EXPR_SIZE     10
# define INITIAL_BLOCK_SIZE    10

# define INITIAL_OOPS_SIZE    100
# define INITIAL_VALUES_SIZE  100

const u_char nameDescHeaderByte::code_width     = 2;
const u_char nameDescHeaderByte::index_width    = 5;
const u_char nameDescHeaderByte::hasId_bit_num  = code_width+index_width;
const u_char nameDescHeaderByte::max_code       = nthMask(code_width);
const u_char nameDescHeaderByte::illegal_value  = nthMask(BitsPerByte);
const u_char nameDescHeaderByte::no_index       = nthMask(index_width) - 1;
const u_char nameDescHeaderByte::max_index      = nthMask(index_width) - 2;

const u_char scopeDescHeaderByte::code_width        = 3;
const u_char scopeDescHeaderByte::lookup_width      = 3;
const u_char scopeDescHeaderByte::lite_bit_num      = code_width+lookup_width;
const u_char scopeDescHeaderByte::nameDescs_bit_num = lite_bit_num+1;
const u_char scopeDescHeaderByte::max_code          = nthMask(code_width);
const u_char scopeDescHeaderByte::max_lookup        = nthMask(lookup_width);

class Vector: public ResourceObj {
 public:
  fint index;
  fint size;

  fint offset;

  int32* values;

  Vector(fint size);

  fint length() { return index; }
  void extend(fint newSize);
  fint insertIfAbsent(int32 value);  // returns index for value
  void copy_to(int32*& addr);
};

class ByteArray : public ResourceObj {
 private:
  char* array;
  fint  top;
  fint  max;

  void extend();

 public:
  fint size()   { return top; }
  char* start() { return array; }

  ByteArray(fint size);

  void appendByte(u_char p) {
    if (top+sizeof(u_char) > max) extend();
    array[top++] = p;
  }

  void appendHalf(fint p);
#ifdef UNUSED
  void appendWord(int32 p);
#endif

  void putByteAt(u_char p, fint offset) {
    assert( offset < max, "index out of bound");
    array[offset] = p;
  }

  void putHalfAt(fint p, fint offset) {
    assert(fits_in_halfword(p), "oops!");
    assert( offset + 1 < max, "index out of bound");
    array[offset    ] = p >> BYTE_WIDTH;
    array[offset + 1] = lowerBits(p, 8);
  }

  // Cut off some of the generated code.
  void setTop(fint offset) {
    assert( this->top >= offset, "A smaller top is expected");
    this->top = offset;
  }

  void alignToWord();
  void copy_to(int32*& addr);
};

NameNode* newValueName(oop value) {
  if (value->is_block()) {
    return new BlockValueName((blockOop)value);
  } else {
    return new ValueName(value);
  }
}

bool NameNode::genHeaderByte(ScopeDescRecorder* rec, u_char code,
                             fint index) {
  // Since id is most likely to be 0, the info part of the header byte
  // indicates if it is non zero.
  // Experiments show id is zero in at least 90% of the generated nameDescs.
  // returns true if index could be inlined in headerByte.
  nameDescHeaderByte b;
  bool   can_inline  = index <= b.max_index;
  u_char coded_index = can_inline ? index : b.no_index;
  b.pack(code, id != 0, coded_index);
  rec->codes->appendByte( b.value());
  if (id && code != ILLEGAL_CODE) rec->genValue(id);
  return can_inline;
} 

inline fint ScopeDescRecorder::getValueIndex(int32 v) {
  // if v fits into 7 bits inline the value instead of creating index
  if ( 0 <= v && v <= MAX_INLINE_VALUE) return v;
  return MAX_INLINE_VALUE + 1 + values->insertIfAbsent(v);
}

inline fint ScopeDescRecorder::getOopIndex(oop o) {
  return o == 0 ? 0 : oops->insertIfAbsent((int32)o) + 1;
}

void LocationName::generate(ScopeDescRecorder* rec) {
  fint index = rec->getValueIndex(l);
  if (!genHeaderByte(rec, LOCATION_CODE, index)) rec->genIndex(index);
}

void ValueName::generate(ScopeDescRecorder* rec) {
  fint index = rec->getOopIndex(value);
  if (!genHeaderByte(rec, VALUE_CODE, index)) rec->genIndex(index);
}

void MemoizedName::generate(ScopeDescRecorder* rec) {
  fint index = rec->getValueIndex(l);
  if (!genHeaderByte(rec, MEMOIZEDBLOCK_CODE, index)) rec->genIndex(index);
  rec->genOop(oop(blk));
}

void BlockValueName::generate(ScopeDescRecorder* rec) {
  fint index = rec->getOopIndex(oop(blk));
  if (!genHeaderByte(rec, BLOCKVALUE_CODE, index)) rec->genIndex(index);
}

void IllegalName::generate(ScopeDescRecorder* rec) {
  (void) genHeaderByte(rec, ILLEGAL_CODE, 0);
}


// Please encapsulate iterator.
static ScopeDesc*     _sd;
static nmethodScopes* _scopes;
static ScopeDesc*     _getNextScopeDesc() {
  _sd = _scopes->getNext(_sd);
  if (!_sd) fatal("out of scopeDescs");
  return _sd;
}

Vector::Vector(fint sz) {
  size   = sz;
  index  = 0;
  values = NEW_RESOURCE_ARRAY(int32, sz);
}

fint Vector::insertIfAbsent(int32 value){
  for (fint i = 0; i < index; i ++)
    if (values[i] == value)
      return i;
  if (index == size)
    extend(size*2 + 1);
  assert( index < size, "not big enough");
  values[index] = value;
  assert(index < 0x10000, "16 bit index overflow");
  return index++;
}

void Vector::extend(fint newSize) {
  int32* newValues = NEW_RESOURCE_ARRAY(int32, newSize);
  for(fint i=0;i < index; i++)
    newValues[i] = values[i];
  values = newValues;
  size   = newSize;
}

void Vector::copy_to(int32*& addr) {
  for(fint i=0;i < length(); i++) {
    *addr++ = values[i];
  }
}

ByteArray::ByteArray(fint size) {
  array = NEW_RESOURCE_ARRAY(char, size); 
  max   = size;
  top   = 0;
}

void ByteArray::extend() {
  fint  newMax = max*2 + 1;
  char* newArray = NEW_RESOURCE_ARRAY(char, newMax);
  for(fint i=0;i < top; i++)
    newArray[i] = array[i];
  array = newArray;
  max  = newMax;
}

void ByteArray::appendHalf(fint p) {
  assert(fits_in_halfword(p), "oops!");
  while (top+sizeof(int16) > max) extend();
  // Saving the half as two bytes to avoid alignment problem.
  array[top++] = p >> BYTE_WIDTH;
  array[top++] = lowerBits(p, 8);
}

#ifdef UNUSED
void ByteArray::appendWord(int32 p) {
  while (top+sizeof(int32) > max) extend();
  int32* s = (int32*) &array[top];
  *s = p;
  top += sizeof(int32);
}
#endif

void ByteArray::alignToWord() {
  fint fill_size = (sizeof(int32) - (size()%sizeof(int32))) % sizeof(int32);
  for(fint i = 0; i < fill_size; i++)
    appendByte(0); 
}

void ByteArray::copy_to(int32*& addr) {
  int32* fromAddr = (int32*) start();
  fint len = (size() + sizeof(int32) - 1) / sizeof(int32);
  for(fint i= 0; i < len; i++) {
    *addr++ = *fromAddr++;
  }
}

class NameList: public ResourceObj {
 public:
  fint end;
  fint size;
  fint count;

  NameNode** nodes;

  NameList(fint size);
  bool is_empty() { return end == 0; }
  void extend(fint newSize);
  void generate(ScopeDescRecorder* rec);
  void insert(fint index, NameNode* node);
};

NameList::NameList(fint sz) {
  size  = sz;
  end   = 0;
  nodes = NEW_RESOURCE_ARRAY(NameNode*, size);
  for(fint i= 0 ; i < sz; i++) nodes[i] = NULL;
  count = 0;
}

void NameList::extend(fint newSize) {
  NameNode** newNodes = NEW_RESOURCE_ARRAY(NameNode*, newSize);
  fint i;
  for (i= 0; i < size; i++) {
    newNodes[i] = nodes[i];
  }
  for (    ; i < newSize; i++) {
    newNodes[i] = NULL;
  }
  nodes = newNodes;
  size  = newSize;
}

void NameList::generate(ScopeDescRecorder* rec) {
  for(fint i=0;i < end; i++){
    if (nodes[i]) {
      nodes[i]->generate(rec);
    }
  }
}

void NameList::insert(fint index, NameNode* node){
  if (index>=size) extend( index >= size*2 ? index*2 : size*2);
  if (index>=end)  end = index +1;
  assert( index < size, "did not extend enough");
  assert( nodes[index] == NULL,"overlapping name desc");
  nodes[index] = node;
}

// Class hierarchy for nodes generating scopeDescs.
// ScopeInfoClass
// -- CodeScopeNode
//    -- MethodScopeNode
//    -- TopLevelBlockScopeNode
//     -- BlockScopeNode
//    -- DeadBlockScopeNode
// -- AccessScopeNode
//    --DataAccessScopeNode
//    --DataAssignmentScopeNode
// -- RemoteAccessScopeNode
//    --RemoteDataAccessScopeNode
//    --RemoteDataAssignmentScopeNode

# define INVALID_OFFSET -1

class ScopeInfoClass: public ResourceObj {
public:
  ScopeLookupKey* key;
  int32      senderBCI;
  NameList*  slotList;
  NameList*  exprList;
  NameList* blockList;

  fint offset; // byte offset to the encoded scopeDesc
               // Initial value is  INVALID_OFFSET
  bool lite;
  bool usedInPcs;
  bool visible;

  bool hasNameDescs() {
    return !lite && 
          (   ! slotList->is_empty() 
           || ! exprList->is_empty() 
           || !blockList->is_empty());
  }

  ScopeInfo scopesHead;
  ScopeInfo scopesTail;
  ScopeInfo next;

  ScopeInfoClass(ScopeLookupKey* key, bool lite, fint senderBCI, bool vis);

  void addNested(ScopeInfo scope);

  virtual u_char code() = 0;

  virtual void generate(ScopeDescRecorder* rec, fint senderScopeOffset);
  void generateBody(ScopeDescRecorder* rec, fint senderScopeOffset);

  bool computeVisibility();

  virtual void verify(ScopeDesc* sd);
  void verifyBody();
};

class PcDescNode: public ResourceObj {
public:
  int32     pcOffset;
  ScopeInfo scope;
  int32     bci;
};

class PcDescInfoClass : public ResourceObj {
public:
  PcDescNode*   nodes;
  fint          end;
  fint          size;

  PcDescInfoClass(fint size);
  fint length() { return end; }
  void extend(fint newSize);
  void add(fint pcOffset, ScopeInfo scope, fint bci);
  void mark_scopes();
  void copy_to(int32*& addr);
};

ScopeInfoClass::ScopeInfoClass(ScopeLookupKey* k, bool lte,
                               fint sBCI, bool vis) {
  key        = k;
  lite       = lte;
  senderBCI  = sBCI;
  slotList   = new NameList(INITIAL_SLOT_SIZE);
  exprList   = new NameList(INITIAL_EXPR_SIZE);
  blockList  = new NameList(INITIAL_BLOCK_SIZE);
  offset     = INVALID_OFFSET;
  scopesHead = NULL;
  scopesTail = NULL;
  usedInPcs  = false;
  visible    = vis;
}

void ScopeInfoClass::addNested(ScopeInfo scope) {
  scope->next = NULL;
  if (scopesHead == NULL) {
    scopesHead = scopesTail = scope; 
  } else {
    scopesTail->next = scope;
    scopesTail = scope;
  }
}

void ScopeInfoClass::generate(ScopeDescRecorder* rec, fint senderScopeOffset) {
  offset = rec->codes->size();
  if (offset == 0) {
    // Since the lookupKey information for this scope is the same for
    // the compiled nmethod omit saving it.
    rec->genScopeDescHeader(code(), 0, lite, hasNameDescs());
  } 
  else {
    u_char index = getCommonLookupTypeIndex(key->lookupType);
    rec->genScopeDescHeader(code(), index, lite, hasNameDescs());
    if (index == 0) rec->genValue(key->lookupType);
    
    rec->genOop(key->selector);

    if (needsDelegatee(key->lookupType)) { 
      // delegatee is only used in less the 1% in the generated scopeDescs.
      rec->genOop(key->delegatee);
    }

    rec->genValue(offset - senderScopeOffset);
    rec->genValue(senderBCI);
  }
}

void ScopeInfoClass::generateBody(ScopeDescRecorder* rec, 
                                  fint senderScopeOffset) {
  if (hasNameDescs()) {
    slotList->generate(rec);
    
    fint exprStart = rec->codes->size();
    exprList->generate(rec);

    fint blockStart = rec->codes->size();
    blockList->generate(rec);
    
    rec->updateScopeDescHeader( offset, 
                                exprStart, 
                                blockStart,
                                rec->codes->size());
  }
  
  for(ScopeInfo p = scopesHead; p  != NULL; p = p->next) {
    if (p->visible) {
      p->generate(rec, offset);
      p->generateBody(rec, offset);
    }
  }
}

void ScopeInfoClass::verify(ScopeDesc* sd) {
  if (key->lookupType != sd->key.lookupType) fatal("lookupType is wrong");
  if (key->selector   != sd->key.selector)   fatal("selector is wrong");
  if (key->delegatee  != sd->key.delegatee)  fatal("delegatee is wrong");
  if (senderBCI != IllegalBCI && senderBCI != sd->senderByteCodeIndex())
    fatal("senderBCI is wrong");
}

void ScopeInfoClass::verifyBody() {
  for(ScopeInfo p = scopesHead; p  != NULL; p = p->next) {
    if (p->visible) {
      p->verify(_getNextScopeDesc());
      p->verifyBody();
    }
  }
}

bool ScopeInfoClass::computeVisibility() {
  for(ScopeInfo p = scopesHead; p  != NULL; p = p->next) {
    visible = p->computeVisibility() || visible;
  }
  visible = visible || (usedInPcs && GenerateLiteScopeDescs) || !lite;
  return visible;
}

class CodeScopeNode: public ScopeInfoClass {
 public:
  oop  method;
  fint scopeID;

  CodeScopeNode(ScopeLookupKey* k, oop meth, 
                bool lte, fint scopID, fint sBCI, bool vis)
    : ScopeInfoClass(k, lte, sBCI, vis) { 
      method    = meth;
      scopeID   = scopID;
    }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);
};

void CodeScopeNode::generate(ScopeDescRecorder* rec,
                                  fint senderScopeOffset) {
  ScopeInfoClass::generate(rec, senderScopeOffset);
  rec->genOop(method);
  rec->genValue(scopeID);
}

void CodeScopeNode::verify(ScopeDesc* sd) {
  ScopeInfoClass::verify(sd);
  if (!sd->isCodeScope()) fatal("CodeScope expected");
  if (senderBCI != IllegalBCI && senderBCI != sd->senderByteCodeIndex())
    fatal("senderBCI is wrong");
  if (method    != sd->method())              fatal("method is wrong");
  if (scopeID   != sd->scopeID())             fatal("scopeID is wrong");
}  

class MethodScopeNode: public CodeScopeNode {
 public:
  NameNode* self_name;
  oop       self_map;
  oop  methodHolder_or_map;

  u_char code() { return METHOD_CODE; }

  MethodScopeNode(ScopeLookupKey* k, oop meth, 
                  NameNode* self_nm, oop self_mp, oop meth_holder, 
                  bool lte, fint sID, fint sBCI, bool vis)
    : CodeScopeNode(k, meth, lte, sID, sBCI, vis) {
      assert(lte || !self_nm->isIllegal(), "self is always visible");
      self_name = self_nm;
      self_map  = self_mp;
      methodHolder_or_map = meth_holder;
  }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);
};

void MethodScopeNode::generate(ScopeDescRecorder* rec,
                               fint senderScopeOffset) {
  CodeScopeNode::generate(rec,senderScopeOffset);
  self_name->generate(rec);
  rec->genOop(self_map);
  rec->genOop(methodHolder_or_map);
}

void MethodScopeNode::verify(ScopeDesc* sd) {
  CodeScopeNode::verify(sd);
  if (!sd->isMethodScope()) fatal("MethodScope expected");
  if (methodHolder_or_map != sd->methodHolder_or_map())
    fatal("method holder is wrong");
}  

class TopLevelBlockScopeNode: public CodeScopeNode {
 public:
  NameNode* self_name;
  NameNode* cachedSelf_name; // cached copy  (for PPC)
  oop       self_map;
  oop       methodHolder_or_map;
  NameNode* block_name;
  NameNode* cachedBlock_name; // cached copy (for PPC)
  mapOop    _receiverMapOop;

  u_char code() { return TOPLEVELBLOCK_CODE; }

  TopLevelBlockScopeNode(ScopeLookupKey* k, oop meth, mapOop rm,
                         NameNode* self_nm, NameNode* self_nm2, oop self_mp, 
                         oop meth_holder, NameNode* receiver, NameNode* receiver2,
                         bool lte, fint scpID, fint sndrBCI, bool vis)
    : CodeScopeNode(k, meth, lte, scpID, sndrBCI, vis) { 
      assert(lte || !self_nm->isIllegal(), "self is always visible");
      assert(lte || !receiver ->isIllegal(), "receiver is always visible");
      self_name = self_nm;
      cachedSelf_name = self_nm2;
      self_map  = self_mp;
      methodHolder_or_map = meth_holder;
      block_name    = receiver;
      cachedBlock_name = receiver2;
      _receiverMapOop = rm;
  }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);

  mapOop receiverMapOop() { return _receiverMapOop; }
  Map*   receiverMap() { return receiverMapOop()->map_addr(); }
};

void TopLevelBlockScopeNode::generate(ScopeDescRecorder* rec, 
                                      fint senderScopeOffset) {
  CodeScopeNode::generate(rec, senderScopeOffset);
  self_name->generate(rec);
  cachedSelf_name->generate(rec);
  rec->genOop(self_map);
  rec->genOop(methodHolder_or_map);
  block_name->generate(rec);
  cachedBlock_name->generate(rec);
  assert(receiverMapOop()->is_map(), "receiver map should be a map");
  rec->genOop(receiverMapOop());
}

void TopLevelBlockScopeNode::verify(ScopeDesc* sd) {
  CodeScopeNode::verify(sd);
  if (!sd->isTopLevelBlockScope()) fatal("TopLevelBlockScope expected");
  if (methodHolder_or_map != sd->methodHolder_or_map())
    fatal("method holder is wrong");
}  

class BlockScopeNode: public CodeScopeNode {
 public:
  NameNode* blockName;
  mapOop _receiverMapOop;
  ScopeInfo parent;

  BlockScopeNode(ScopeLookupKey* k, oop meth, mapOop rm,
                 NameNode* receiver, ScopeInfo parnt,
                 bool lte, fint scpID, fint sndrBCI, bool vis) 
    : CodeScopeNode(k, meth, lte, scpID, sndrBCI, vis) { 
    assert(lte || !receiver ->isIllegal(), "receiver is always visible");
    blockName = receiver;
    _receiverMapOop = rm;
    parent    = parnt;
  }

  u_char code() { return BLOCK_CODE; }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);

  mapOop receiverMapOop() { return _receiverMapOop; }
  Map*   receiverMap() { return receiverMapOop()->map_addr(); }
};

void BlockScopeNode::generate(ScopeDescRecorder* rec, fint senderScopeOffset) {
  CodeScopeNode::generate(rec, senderScopeOffset);
  rec->genValue(offset - parent->offset);
  blockName->generate(rec);
  assert(receiverMapOop()->is_map(), "receiver map should be a map");
  rec->genOop(receiverMapOop());
}

void BlockScopeNode::verify(ScopeDesc* sd) {
  CodeScopeNode::verify(sd);
  if (!sd->isLiveBlockScope()) fatal("BlockScope expected");
} 

class DeadBlockScopeNode: public CodeScopeNode {
 public:
  NameNode* block_name;

  u_char code() { return DEADBLOCK_CODE; }

  DeadBlockScopeNode(ScopeLookupKey* k, oop meth, NameNode* block_nm,
                     bool lte, fint scpID, fint sndrBCI, bool vis)
    : CodeScopeNode(k, meth, lte, scpID, sndrBCI, vis) { 
      block_name    = block_nm;
  }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);
};

void DeadBlockScopeNode::generate(ScopeDescRecorder* rec,
                                  fint senderScopeOffset) {
  CodeScopeNode::generate(rec, senderScopeOffset);
  block_name->generate(rec);
}

void DeadBlockScopeNode::verify(ScopeDesc* sd) {
  CodeScopeNode::verify(sd);
  if (!sd->isDeadBlockScope()) fatal("DeadBlockScope expected");
}  

class AccessScopeNode: public ScopeInfoClass {
 public:
  NameNode* self_name;
  oop self_map;
  oop methodHolder_or_map;

  AccessScopeNode(ScopeLookupKey* k, NameNode* self_nm, oop self_mp,
                  oop meth_holder, fint sndrBCI, bool vis)
    : ScopeInfoClass(k, false, sndrBCI, vis) {
    self_name = self_nm;
    self_map  = self_mp;
    methodHolder_or_map = meth_holder;
  }

  void generate(ScopeDescRecorder* rec, fint senderScopeOffset);

  void verify(ScopeDesc* sd);
};

void AccessScopeNode::generate(ScopeDescRecorder* rec,
                               fint senderScopeOffset) {
  ScopeInfoClass::generate(rec, senderScopeOffset);
  self_name->generate(rec);
  rec->genOop(self_map);
  rec->genOop(methodHolder_or_map);
}

void AccessScopeNode::verify(ScopeDesc* sd) {
  ScopeInfoClass::verify(sd);
  if (!sd->isAccessScope())                fatal("AccessScope expected");
  if (methodHolder_or_map != sd->methodHolder_or_map())
    fatal("method holder is wrong");
}

class DataAccessScopeNode: public AccessScopeNode {
 public:
  DataAccessScopeNode(ScopeLookupKey* k, NameNode* self_nm, oop self_mp,
                      oop meth_holder, fint sndrBCI, bool vis)
    : AccessScopeNode(k, self_nm, self_mp, meth_holder, sndrBCI, vis){}
  u_char code() { return DATAACCESS_CODE; }
};

class DataAssignmentScopeNode: public AccessScopeNode {
 public:
  DataAssignmentScopeNode(ScopeLookupKey* k, NameNode* self_nm,
                          oop self_mp, oop meth_holder, fint sndrBCI,
                          bool vis)
    : AccessScopeNode(k, self_nm, self_mp, meth_holder, sndrBCI, vis){}
  u_char code() { return DATAASSIGNMENT_CODE; }
};


void ScopeDescRecorder::generate() {
  assert(root, "root scope must be present");
  // Generate the bytecodes for the ScopeDescs and their NameDescs.

  pcs->mark_scopes();
  (void) root->computeVisibility();
  root->generate(this, 0);
  root->generateBody(this, 0);
  codes->alignToWord();
  _hasCodeBeenGenerated = true;
}

ScopeInfo ScopeDescRecorder::addScope(ScopeInfo scope, ScopeInfo senderScope){
  if (root == NULL) {
    assert( senderScope == NULL, "Root scope must be the first"); 
    root = scope;
  } else {
    assert( senderScope != NULL, "More than one root scope is generated"); 
    senderScope->addNested(scope);
  }
  return scope;
}

int32 ScopeDescRecorder::offset(ScopeInfo scope) {
  assert(scope->offset != INVALID_OFFSET, "uninitialized offset");
  return scope->offset;
}

ScopeInfo ScopeDescRecorder::addMethodScope(ScopeLookupKey* k, 
                                            oop meth,
                                            NameNode* self_nm,
                                            oop self_mp,
                                            oop meth_holder,
                                            bool lte,
                                            int32 scpID,
                                            ScopeInfo senderScope, 
                                            fint sndrBCI,
                                            bool vis) {
  return addScope(new MethodScopeNode(k, meth, self_nm, self_mp,
                                      meth_holder, lte, scpID, sndrBCI, vis),
                                      
                  senderScope);
}

ScopeInfo ScopeDescRecorder::addTopLevelBlockScope(ScopeLookupKey* k,
                                                   oop meth,
                                                   mapOop rm,
                                                   NameNode* self_nm,
                                                   NameNode* self_nm2, // cached value for PPC
                                                   oop self_mp,
                                                   oop meth_holder,
                                                   NameNode* receiver,
                                                   NameNode* receiver2, // cached value for PPC
                                                   bool lte,
                                                   int32 scpID,
                                                   ScopeInfo senderScope, 
                                                   fint sndrBCI,
                                                   bool vis) {
  return addScope(new TopLevelBlockScopeNode(k, meth, rm,
                                             self_nm, self_nm2, self_mp, 
                                             meth_holder, receiver, receiver2, lte,
                                             scpID, sndrBCI, vis),
                                             senderScope);
}

ScopeInfo ScopeDescRecorder::addBlockScope(ScopeLookupKey* k, 
                                           oop meth,
                                           mapOop rm,
                                           NameNode* receiver,
                                           ScopeInfo parnt,
                                           bool lte,
                                           int32 scpID,
                                           ScopeInfo senderScope, 
                                           fint sndrBCI,
                                           bool vis) {
  return addScope(new BlockScopeNode(k, meth, rm, receiver, parnt, lte,
                                     scpID, sndrBCI, vis), senderScope);
}

ScopeInfo ScopeDescRecorder::addDeadBlockScope(ScopeLookupKey* k,
                                               oop meth,
                                               NameNode* block_nm,
                                               bool lte,
                                               int32 scpID,
                                               ScopeInfo senderScope, 
                                               fint sndrBCI,
                                               bool vis) {
  return addScope(new DeadBlockScopeNode(k, meth, block_nm, lte,
                                         scpID, sndrBCI, vis), senderScope);
}

ScopeInfo ScopeDescRecorder::addDataAccessScope(ScopeLookupKey* k,
                                                NameNode* self_nm,
                                                oop self_type,
                                                oop meth_holder,
                                                ScopeInfo senderScope, 
                                                fint sndrBCI,
                                                bool vis) {
  return addScope(new DataAccessScopeNode(k, self_nm, self_type,
                                          meth_holder, sndrBCI, vis),
                  senderScope);
}

ScopeInfo ScopeDescRecorder::addDataAssignmentScope(ScopeLookupKey* k, 
                                                    NameNode* self_nm,
                                                    oop self_type,
                                                    oop meth_holder,
                                                    ScopeInfo senderScope, 
                                                    fint sndrBCI,
                                                    bool vis) {
  return addScope(new DataAssignmentScopeNode(k, self_nm, self_type,
                                              meth_holder, sndrBCI, vis),
                  senderScope);
}

void ScopeDescRecorder::addSlot(ScopeInfo scope, fint slotIndex, NameNode* l){
  assert(!scope->lite, "cannot add slot to lite scopeDesc");
  scope->slotList->insert(slotIndex, l);
}

void ScopeDescRecorder::addExprStack(ScopeInfo scope, fint index, 
                                     NameNode* l) {
  assert(!scope->lite, "cannot add expression to lite scopeDesc");
  scope->exprList->insert(index, l);
}



// When a method includes a block, the code must hold
// onto that block from the time it is created to the time it
// is zapped. The live block descs record those locations.
// index is the bci of the literal code that pushes the block.

void ScopeDescRecorder::addBlock( ScopeInfo scope, fint index, 
                                  NameNode* l) {
  assert(!scope->lite, "cannot add expression to lite scopeDesc");
  scope->blockList->insert(index, l);
}

void ScopeDescRecorder::genScopeDescHeader(u_char code, 
                                           u_char lookupIndex, 
                                           bool lte,
                                           bool nameDescs) {
  scopeDescHeaderByte b;
  b.pack(code, lookupIndex, lte, nameDescs);
  codes->appendByte(b.value());
  if (nameDescs) {
    // refer to updateScopeDescHeader
    codes->appendHalf(0); // placeholder for expression offset
    codes->appendHalf(0); // placeholder for block      offset
    codes->appendHalf(0); // placeholder for next       offset
  }
}

void ScopeDescRecorder::updateScopeDescHeader( fint offset,
                                               fint startOfExpr,
                                               fint startOfBlocks,
                                               fint next) {
  if (!fits_in_halfword(startOfExpr) ||
      !fits_in_halfword(startOfBlocks) ||
      !fits_in_halfword(next))
    fatal("Can't fit index in preallocated half-word slot");

  // refer to genScopeDescHeader
  codes->putHalfAt(startOfExpr,   offset+1);
  codes->putHalfAt(startOfBlocks, offset+3);
  codes->putHalfAt(next,          offset+5);
}

inline void ScopeDescRecorder::genIndex(fint index) {
  if (index < EXTENDED_INDEX) {
    // generate 1 byte indexing the oop.
    codes->appendByte(index);
  } else {
    codes->appendByte(EXTENDED_INDEX);
    codes->appendHalf(index);
  }
}

void ScopeDescRecorder::genValue(int32 v) {
  genIndex(getValueIndex(v));
}

void ScopeDescRecorder::genOop(oop o) {
  genIndex(getOopIndex(o));
}

PcDescInfoClass::PcDescInfoClass(fint sz) {
  nodes = NEW_RESOURCE_ARRAY(PcDescNode, sz); 
  end   = 0;
  size  = sz;
}

void PcDescInfoClass::extend(fint newSize) {
  PcDescNode* newNodes = NEW_RESOURCE_ARRAY(PcDescNode, newSize);
  for(fint i=0;i < end; i++)
    newNodes[i] = nodes[i];
  nodes = newNodes;
  size  = newSize;
}

void PcDescInfoClass::add(fint pcOffset, ScopeInfo scope, fint bci) {
  if (scope->lite && !GenerateLiteScopeDescs) return;
  assert( end == 0 || pcOffset >= nodes[end-1].pcOffset, 
         "pcDesc in wrong order");
  if (end == size) extend(size*2 + 1);
  if (end > 0) {
    // skip if the previous had the same scope and bci.
    if (scope == nodes[end-1].scope && bci == nodes[end-1].bci) return;
    // overwrite if the previous had the same pcOffset.
    if (pcOffset == nodes[end-1].pcOffset) end--;
  }
  nodes[end].pcOffset = pcOffset;
  nodes[end].scope    = scope;
  nodes[end].bci      = bci;
  end++;
}

void PcDescInfoClass::mark_scopes() {
  for(fint i=0;i < end; i++) {
    if (nodes[i].scope) nodes[i].scope->usedInPcs = true;
  }
}
  
void PcDescInfoClass::copy_to(int32*& addr) {
  for(fint i = 0; i < end; i++) {
    PcDesc* pc = (PcDesc*) addr;
    pc->pc       = nodes[i].pcOffset;
    pc->scope    = nodes[i].scope ? nodes[i].scope->offset : IllegalBCI;
    pc->byteCode = nodes[i].bci;
    addr += sizeof(PcDesc)/sizeof(int32);
  }
}

fint ScopeDescRecorder::size() {
  return   sizeof(nmethodScopes)
         + codes->size()
         + oops->length()   * sizeof(oop)
         + values->length() * sizeof(int32)
         + pcs->length()    * sizeof(PcDesc);
}

ScopeDescRecorder::ScopeDescRecorder(fint byte_size, fint pcDesc_size) {
  // size is the initial size of the byte vector.
  root = 0;
  oops   = new Vector(INITIAL_OOPS_SIZE);
  values = new Vector(INITIAL_VALUES_SIZE);
  codes  = new ByteArray(byte_size);
  pcs    = new PcDescInfoClass(pcDesc_size);
  _hasCodeBeenGenerated = false;
}

void ScopeDescRecorder::copyTo(VtblPtr_t* addr, int32 backPointer) {
  { nmethodScopes d;
    addr[0] = d.vtbl_value();
  }
  { nmethodScopes*          d = (nmethodScopes*) addr;
    d->_nmethod_backPointer  = (nmethod*) backPointer;

    // Copy the body part of the nmethodScopes
    int32* start = (int32*)(d+1);
    int32* p     = start;

    codes->copy_to( p);

    d->set_oops_offset((char*) p - (char*) start);
    oops->copy_to( p);

    d->set_value_offset((char*) p - (char*) start);
    values->copy_to( p);

    d->set_pcs_offset((char*) p - (char*) start);
    pcs->copy_to( p);

    d->set_length((char*) p - (char*) start);

    assert( (char*) d + size() == (char*) p, "wrong size of nmethodScopes");
  }
}

void ScopeDescRecorder::addPcDesc(fint pcOffset, ScopeInfo scope, fint bci) {
  assert( scope, "scope must be specified in addPcDesc");
  assert( root, "root must be present");
  pcs->add(pcOffset, scope, bci);
}

#ifdef UNUSED
void ScopeDescRecorder::addIllegalPcDesc(fint pcOffset) {
  assert( root, "root must be present");
  pcs->add(pcOffset);
}
#endif

# if  GENERATE_DEBUGGING_AIDS
void ScopeDescRecorder::verify(nmethodScopes* scopes) {
  // Initialize iterator
  _scopes = scopes;
  _sd     = NULL;

  assert( root, "root must be present to verify");
  root->verify(_getNextScopeDesc());
  root->verifyBody();
}
#endif

// Contains the most common lookupTypes for scopeDescs.
// the percentages may no longer be accurate --MIW 12/14/94
static const LookupType commonLookupTypeTable[] = {
    NormalLookupType,                                // 47%
    ImplicitSelfLookupType,                          // 46%
    StaticNormalLookupType                           //  5%
};

static const fint commonLookupTypeTableSize =
  sizeof(commonLookupTypeTable)/sizeof(LookupType);

u_char getCommonLookupTypeIndex(LookupType l) {
  fint index;
  for(index = 0;
      index < commonLookupTypeTableSize && commonLookupTypeTable[index] != l;
      index++) {
  }
  return (index < commonLookupTypeTableSize) ? index + 1 : 0;
}

LookupType getCommonLookupTypeAt(u_char index) {
  assert( index <= commonLookupTypeTableSize, "index out of range");
  return commonLookupTypeTable[index-1];
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "nmln.hh"
# include "_nmln.cpp.incl"


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

// This nmln is about to move by delta; adjust all links.
void nmln::shift(int32 diff, NCodeBase* m) {
  if (isEmpty()) {
    // an empty list; shift myself
    next = (nmln*) ((char*) next + diff);
    prev = (nmln*) ((char*) prev + diff);
  } else {
    // shift next-door neighbors
    nmln* l = prev;
    if (m->encompasses(l)) {
      // prev is in this nmethod; shift my prev instead
      prev = (nmln*) ((char*) l + diff);
    } else {
      // make prev's next point to where I will be
      l->next = (nmln*) ((char*) l->next + diff);
    }
    l = next;
    if (m->encompasses(l)) {
      // next is in this nmethod; shift my next instead
      next = (nmln*) ((char*) l + diff);
    } else {
      // make next's prev point to where I will be
      l->prev = (nmln*) ((char*) l->prev + diff);
    }
  }
}

# endif

// traverse a chain and init every nmln

void nmln::init_chain() {
  nmln *c, *n;
  for (  c = next;  c != this;  c = n) {
    n = c->next;
    c->init();
  }
  init();
}
  

# define IgnoreBits  3
# define MapBits     15

inline int32 nmln::hash() {
  return (int32(this) >> IgnoreBits) & nthMask(MapBits);
}

static nmln** nmlnCache = NULL;
// direct-mapped cache of 2^MapBits nmlns to speed up verify

# if GENERATE_DEBUGGING_AIDS
  static int32 nmlnConflicts = 0;
  static int32 nmlnVerifies = 0;
# endif

void initNmlnCache() {
  nmlnCache = NEW_RESOURCE_ARRAY( nmln*, 1 << MapBits);
  memset((char*)nmlnCache, 0, sizeof(nmln*) * (1 << MapBits));
}

void resetNmlnCache() { nmlnCache = NULL; }

void nmln::print() {
  lprintf("0x%lx: prev: 0x%lx, next: 0x%lx",
         this, prev, next);
}



bool nmln::verify_list_integrity() {
  bool flag = true;
  int32 count = 0;
  for (nmln *p = next; p != this; p = p->next) {
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      nmlnVerifies++;
    }
#   endif
    if (nmlnCache) {
      int32 hash = p->hash();
      if (nmlnCache[hash] == p) {
        break;                  // already verified
      } else {
#       if GENERATE_DEBUGGING_AIDS
          if (CheckAssertions  &&  nmlnCache[hash]) nmlnConflicts++;
#       endif      
        nmlnCache[hash] = p;
      }
    }
    if (++count > 100000) {
      error2("bad nmln list: figure six bug around 0x%lx in list 0x%lx",
             p, this);
      flag = false;
      break;
    }
#   if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
#ifdef broken
    // with nmlns in the codeTableEntries, there is no fast test
    else if (! Memory->code->contains(p) &&
             ! Memory->code->stubs->contains(p) &&
             ! Memory->is_obj_heap((oop*)p) &&
             ! Memory->code->table->contains(p) &&
             ! Memory->code->debugTable->contains(p) &&
             ! Memory->map_table->contains(p) &&
             p != &Memory->code->rememberLink &&
             p != &Memory->code->replCandidates &&
             p != &Memory->code->zombies) {
      error2("bad nmln list: 0x%lx is out of bounds in list 0x%lx", p, this);
      flag = false;
      break;
    }
#endif
#   endif
    /* Too slow! and too specific!
    else if (Memory->code->contains(p) && ! nmethod::findNMethod(p)->encompasses(p)) {
      error2("bad nmln list: 0x%lx not in a valid method in list 0x%lx",
             p, this);
      flag = false;
      break;
    }
    */
    /* caught by figure six bug
    else if (p == p->next) {
      error2("bad nmln list: p == p->next at 0x%lx in list 0x%lx", p, this);
      flag = false;
      break;
    }
    */
    /* need to check only one direction
    else if (p->prev->next != p) {
      error2("bad nmln list: p->prev->next != p at 0x%lx in list 0x%lx",
             p, this);
      flag = false;
      break;
    }
    */
    else if (p->next->prev != p) {
      error2("bad nmln list: p->next->prev != p at 0x%lx in list 0x%lx",
             p, this);
      flag = false;
      break;
    }
  }
  if (next == this) flag = (prev == this);
  return flag;
}

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

void nmln::flush() {
  while (next != this) {
    nmethod *n;
    slotsMapDeps *ignored;
    Memory->code->findNMethodOrMap(next, n, ignored);
    if (n) {
      // flush an nmethod (also removes the nmethod from the list)
      n->flush();
    } else {
      // remove any other map links from the list, too
      next->remove();
    }
  }
}


// the asXXX functions are not inlined to reduce .h dependencies
CacheStub* nmln::asCacheStub() {
  NCodeBase* s =
    Memory->code->stubs->contains(this) ? findStub(this) : NULL;
  return s && s->isCacheStub() ? (CacheStub*)s : NULL;
}

CountStub* nmln::asCountStub() {
  NCodeBase* s =
    Memory->code->stubs->contains(this) ? findStub(this) : NULL;
  return s && s->isCountStub() ? (CountStub*)s : NULL;
}

sendDesc* nmln::asSendDesc_or_null() {
  return
    Memory->code->contains(this) 
      ?  sendDesc::sendDesc_from_nmln(this)
      :  NULL;
}

sendDesc* nmln::asSendDesc() {
  return sendDesc::sendDesc_from_nmln(this);
}

sendDesc* nmln::callingSendDesc() {
  sendDesc* sd = asSendDesc_or_null();
  if (sd) return sd;

  CacheStub* s = asCacheStub();
  if (s) return s->sd();

  CountStub* cs = asCountStub();
  if (cs) return cs->sd();

  ShouldNotReachHere(); // what is it?
  return NULL;
}

DIDesc* nmln::asDIDesc() {
  return (DIDesc*)((char*)this - DIDesc::di_depend_offset);
}

nmethod* nmln::asSender() {
  // the receiver is in an inline cache / stub; find the calling nmethod
  NCodeBase* s = findThing(this);
  if (s->isCountStub()) {
    return ((CountStub*)s)->sender();
  } else if (s->isNMethod()) {
    // was in an inline cache
    return (nmethod*)s;
  } else if (s->isCacheStub()) {
    return ((CacheStub*)s)->sender();
  } else {
    ShouldNotReachHere(); // what is it?
  }
  return NULL;
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


void nmln::invalidate() {
  while (next != this) {
    nmethod *n;
    slotsMapDeps *ignored;
    Memory->code->findNMethodOrMap(next, n, ignored);
    if (n) {
      // flush an nmethod (also removes the nmethod from the list)
      n->invalidate();
    } else {
      // remove any other map links from the list, too
      next->remove();
    }
  }
}

fint nmln::length() {
  fint counter = 0;
  FOR_EACH_NMLN(this, elem, counter++);
  return counter;
}

void print_chain(nmln* p) {     // for debugging
  nmln* first = p;
  nmln* prev = p->prev;
  do {
    // handle broken chains
    printf("%#lx <-> ", (long unsigned)p); 
    if (p->prev != prev || p->next->prev != p) {
      lprintf("## list is broken");
      break;
    }
    prev = p;
    p = p->next;
  } while (p != first);
  lprintf("%#lx\n", first); 
}

/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "codeTable.hh"
# include "_codeTable.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


codeTableEntry::codeTableEntry(nmethod *n, MethodLookupKey *k) {
  nm= n;
  key.set_from(*k);
  next_nm= n->codeTableLink;
  n->codeTableLink= this;
}


codeTable::codeTable(int32 size) {
  tableSize = size;
  buckets = NEW_C_HEAP_ARRAY(nmln, size);
  clear();
}

void codeTable::clear() {
  for (nmln* p = buckets;  p < &buckets[tableSize];  ++p)
    p->init();
}


static inline codeTableEntry *entryForLink(nmln *n) {
  static codeTableEntry* shutUpCompiler = NULL;
  static const pc_t next_hash_offset =
      (pc_t)&shutUpCompiler->next_hash ;

  return (codeTableEntry*)((pc_t)n - next_hash_offset); }


nmethod* codeTable::lookup(MethodLookupKey &k) {
  nmln* bucket = bucketFor(k.hash());
  for (nmln* p = bucket->next;  p != bucket;  p = p->next) {
    codeTableEntry *e= entryForLink(p);
    if (e->key.EQ(k)) {
      return e->nm;
    }
  }
  return NULL;
}


void codeTable::add(nmethod* nm, MethodLookupKey* k) {
  if (k == NULL) k= &nm->key;
  k->init_hash();
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      if (nm == (nmethod*)catchThisOne) warning("caught nmethod");
      if ( nm->isDebug() && this == Memory->code->table) warning("wrong table");
      if (!nm->isDebug() && this != Memory->code->table) warning("wrong table");
      if (lookup(*k)) {
        k->print();
        fatal2("adding duplicate key to code table: %#lx and new %#lx",
               lookup(*k), nm);
      }
    }
# endif
  nmln* b = bucketFor(k->hash());
  codeTableEntry *e= new codeTableEntry(nm, k);
  if (e->key.is_new()) nm->remember();
  b->add(&e->next_hash);
}


bool codeTable::verify() {
  bool flag = true;
  for (nmln* p = buckets;  p < &buckets[tableSize];  ++p) {
    flag = flag && p->verify_list_integrity();
    for (nmln* q = p->next;  q != p;  q = q->next) {
      codeTableEntry *e= entryForLink(q);
      nmethod* nm= e->nm;
      if (!nmethod::isNMethod(nm)) {
        error2("bad nmethod 0x%lx in bucket 0x%lx", nm, p);
        flag = false;
      }
      if (bucketFor(e->key.hash()) != p) {
        error2("code table entry 0x%lx not in bucket 0x%lx", e, p);
        flag = false;
      }
    }
  }
  return flag;
}

# if  GENERATE_DEBUGGING_AIDS
void codeTable::print_stats() {
  fint nmin = 9999999, nmax = 0, total = 0, nonzero = 0;
  const fint N = 10;
  fint histo[N];
  fint i;
  for (i = 0; i < N; i++) histo[i] = 0;
  for (nmln* p = buckets;  p < &buckets[tableSize];  ++p) {
    fint len = 0;
    for (nmln* q = p->next;  q != p;  q = q->next) len++;
    if (len < nmin) nmin = len;
    if (len > nmax) nmax = len;
    if (len) nonzero++;
    total += len;
    histo[min(len, N-1)]++;
  }
  float avg = float(total) / nonzero;
  
  lprintf("\ncodeTable statistics: %d nmethods; min chain = %d, max = %d, avg = %4.1f\n",
          (void*)total, (void*)nmin, (void*)nmax,
          *(void**)&avg);
  lprintf("histogram:\n", 0);
  for (i = 0; i < N - 1; i++) lprintf("%4d:\t%d", (void*)i, (void*)histo[i]);
  lprintf(">=%d:\t%d\n", (void*)(N-1), (void*)histo[N-1]);
}
#endif

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "trapdoors.hh"
# include "_trapdoors.cpp.incl"


/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "addrDesc.hh"
# include "_addrDesc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


pc_t* addrDesc::addr(OopNCode* m) {
  int32 off = offset();
  assert(off >= 0, "negative offset in addrDesc");
  if (off < m->instsLen()) {
    return (pc_t*) (m->insts() + off);
  }
  assert(m->isNMethod(), "must be a nmethod");
  nmethod* nm = (nmethod*)m;
  off -= nm->instsLen();
  if (off < nm->scopes->length()) {
    return (pc_t*) nm->scopes->at(off);
  }
  ShouldNotReachHere(); // there should be no addrDescs beyond scopes
  return NULL;
}

bool addrDesc::verify(CacheStub* m) {
  bool flag = true;
  if (offset() >= m->instsLen()) {
    error1("bad offset in addrDesc at %#lx", (long)this);
    flag = false;
  }
  if (isUncommonTrap()) {
    lprintf("bad type at %#lx\n", (long)this);
    flag = false;
  } else if (isOop() && !oop(referent(m))->verify_oop()) {
    lprintf("\tin addrDesc at %#lx\n", (long)this);
    flag = false;
  }
  if (isDIDesc()) {
    lprintf("bad type at %#lx\n", (long)this);
    flag = false;
  }
  return flag;
}

void addrDesc::print(nmethod* m) {
  printIndent();
  lprintf("%s%s%s%s%slocation %#lx = %#lx (offset: %ld)",
         isEmbedded() ? "embedded " : "",
         isSendDesc() ? "sendDesc " : "",
         isPrimitive() ? "primitiveSendDesc " : "",
         isDIDesc() ? "diDesc " : "",
         isUncommonTrap() ? "uncommonTrap " : "",
         (long unsigned)this, (long unsigned)addr(m), long(offset()));
  if (isSendDesc()) {
    lprintf("\n");
    Indent ++;
    asSendDesc(m)->print();
    Indent --;
  } else if (isPrimitive()) {
    lprintf(" (%s)\n", getPrimName((pc_t)referent(m)));
  } else if (isDIDesc()) {
    lprintf("\n");
    Indent ++;
    asDIDesc(m)->print();
    Indent --;
  } else {
    lprintf(": ");
    assert(isOop(), "should be oop addrDesc");
    oop(this->referent(m))->print_real_oop();
    lprintf("\n");
  }
}
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.19 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "zone.hh"
# include "_zone.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

int32 zone::frame_chain_nesting = 0;



# define LRU_RETIREMENT_AGE 10  /* min. age (# sweeps) for retirement */
# define LRU_MAX_VISITED    min(numberOfNMethods(), LRUMaxVisited)
                                /* max. # nmethods visited per sweep */
# define LRU_MAX_RECLAIMED 250000 /* stop after having found this amount of */
                                /* space occupied by replacement candidates */
# define LRU_CUTOFF        32   /* max. length (bytes) of "small" methods */
# define LRU_SMALL_BOOST    3   /* small methods live this many times longer */
         // (because access methods don't clear their unused bit)
# define LRU_OPTIMIZED_BOOST 10 /* optimized nm live longer (because they */
                                /* are expensive to recreate) */

# define COMPACT_OVERHEAD  0.05 /* desired max. overhead for zone compaction */


// Trade-offs for blocksizes/queue length below:
// - larger blocks mean larger internal fragmentation but less space 
//   overhead for the heap maps and increased effectiveness of the free lists
// - longer free lists cover a wider range of allocations but can slow down
//   allocation (when most lists are empty but still have to be scanned).
# define iBLOCK 32              /* block size for iZone */
# define iFREE  20              /* # of free lists for iZone */
# define dBLOCK 32              /* block size for dZone */
# define dFREE  20              /* # of free lists for dZone */
# define sBLOCK 16              /* block size for sZone */
# define sFREE  40              /* # of free lists for sZone */

# define StubBlock  16          /* block size for PIC zone */
# define StubFree   20          /* # of free lists for PIC zone */

# define MaxExtFrag  0.05       /* max. tolerated ext. fragmentation */

# define FOR_ALL_NMETHODS(var)                                                \
    for (nmethod *var = first_nm(); var; var = next_nm(var))


static nmethod* fieldOffsetDetector_nmethod = NULL;
# define NMETHOD_FROM(fieldName, p)                                           \
  ((nmethod*)((char*)p - (char*)&fieldOffsetDetector_nmethod->fieldName))


static OopNCode* fieldOffsetDetector_OopNCode = NULL;
# define OOPNCODE_FROM(fieldName, p)                                          \
  ((OopNCode*)((char*)p - (char*)&fieldOffsetDetector_OopNCode->fieldName))



inline int32 roundSize(int32 s, int32 blockSize) {
  return (s + blockSize - 1) / blockSize * blockSize;
}

static const int32 maxBlockSize = max(max(iBLOCK, dBLOCK), sBLOCK);


LRUcount* LRUtable;     // for optimized methods
int32* LRUflag;         // == LRUtable, just different type for convenience
int32* useCount;        // table with counts for unoptimized methods

static int32 LRUtime;       // virtual time; incremented after every full sweep

// We could directly run the sweeper from the interrupt handler, but
// this is tricky since the stack is in a strange state.  
void sweepTrigger() {
  if (UseLRUInterrupts) {
    Memory->code->_needsSweep = true;
    InterruptedContext::setupPreemptionFromSignal();
  };
}

# if TARGET_IS_PROFILED
  extern "C" void CompiledSelfCode();
  extern "C" void CompiledSelfCodeEnd();

  char* zoneStart()     { return first_inst_addr(CompiledSelfCode); }
  char* zoneEnd()       { return first_inst_addr(CompiledSelfCodeEnd); }
  
# else
  char* zoneStart()     { return NULL; }
  char* zoneEnd()       { return NULL; }
# endif


zone::zone(int32& codeSize, int32& stubSize, int32& depSize, int32& debugSize) {
  used_per_compiler[nm_nic]= 0;
  used_per_compiler[nm_sic]= 0;
  
  char* stb = NULL;
  if ( TARGET_IS_PROFILED )  set_sizes_for_statically_allocated_code_and_stub_area(codeSize, stubSize, depSize, debugSize, stb);
  else                                   set_sizes_and_allocate_code_and_stub_area(codeSize, stubSize, depSize, debugSize, stb);

  allocate_trapdoors_at_zone_base_stealing_from_code_space(codeSize);  
  allocate_pieces(codeSize, stubSize, depSize, debugSize, stb);
                             
  check_allocations();                               
  
  fint maxNMs = codeSize / (sizeof(nmethod) + 32); // conservative guess
  
  allocate_idManager(maxNMs);
  setup_LRU();
  allocate_useCount(maxNMs);
  clear();
  
  // start sweeper
  IntervalTimer::CPU_timer()->enroll_async_if_safe(1.0 / (float)IntervalTimer::LRU_resolution, sweepTrigger);
  zoneTimer.timeRestart();
}


# if TARGET_ARCH == PPC_ARCH
  static const int32 branch_disp_bits = li_bits; 
# else
  static const int32 branch_disp_bits = 32;
# endif

static const uint32 maxCodeAndStubSizeForMachine =  (uint32)(1  <<  (branch_disp_bits - 1)); // -1 because disp may be + or -



void zone::set_sizes_for_statically_allocated_code_and_stub_area(int32& codeSize, int32& stubSize, int32& depSize, int32& debugSize, char*& stb) {

  if (OS::make_memory_executable(zoneStart(), zoneEnd()-zoneStart()))
    fatal3("Couldn't make PIC area writable: zoneStart() = 0x%x, zoneEnd() = 0x%x, difference = %d",
          zoneStart(), zoneEnd(), zoneEnd() - zoneStart());
          
  char* end = zoneStart() +  umin(zoneEnd() - zoneStart(),  maxCodeAndStubSizeForMachine);

  for (;;) {
    round_sizes(codeSize, stubSize, depSize, debugSize);   
            
    // make PICs part of the profiled memory area
    bottom = (int32*)roundSize(int32(zoneStart()), maxBlockSize);
    codeSize =  (end - (char*)bottom) - stubSize - 2*1024;
    codeSize = codeSize / maxBlockSize * maxBlockSize;
    stb = (char*)bottom + codeSize + 1024; // why 1024??? profiling? -dmu 6/9
    
    if (stb + stubSize <= end) 
      break;
      
    int32 oldISize = codeSize;
    codeSize = codeSize / 2;
    codeSize = codeSize / maxBlockSize * maxBlockSize;
    warning2("Cannot have such a large zone (%ld bytes)\n"
              "in profiled version.  "
              "Will proceed by reducing size of code area to %ld bytes.\n",
              oldISize, codeSize);
  }    
}


void zone::set_sizes_and_allocate_code_and_stub_area(int32& codeSize, int32& stubSize, int32& depSize, int32& debugSize, char*& stb) {
  
    round_sizes(codeSize, stubSize, depSize, debugSize);
    int codeSize_p = roundTo(codeSize, idealized_page_size);
    int stubSize_p = roundTo(stubSize, idealized_page_size);
    int codeAndStubSize_p = codeSize_p + stubSize_p; // (possibly) changed by OS::allocate_idealized_page_aligned
    
    if ((uint32)codeAndStubSize_p > maxCodeAndStubSizeForMachine) {
      int32 nCodeSize_p;
      int32 codeAndStubSize = maxCodeAndStubSizeForMachine; // extra -1 because asm buffer needs to be in range, too
      
      // The following paragraph is a HACK to try to deal with too-large zone requests better.
      // It duplicates the computation of stubSize, etc. just above. -- dmu 6/03
      stubSize = codeAndStubSize / 16;
      stubSize = stubSize / StubBlock * StubBlock;
      stubSize_p = roundTo(stubSize, idealized_page_size);
      nCodeSize_p = codeAndStubSize - stubSize_p; 
      nCodeSize_p &= ~(idealized_page_size - 1); // be a multiple of idealized_page_size
      codeSize = codeSize_p = nCodeSize_p;

      warning2("zone::zone: code and stub size %d was too big for this machine, using %d instead", 
                codeAndStubSize_p, codeSize_p + stubSize_p);
      codeAndStubSize_p = codeSize_p + stubSize_p;
      assert((uint32)codeAndStubSize_p <= maxCodeAndStubSizeForMachine, "");
    }
          
    // allocate stubs and code together for span-limited branches -- dmu
    // Warning: PPC Assembler::Assembler counts on stubs being after izone
    // See zone::code_start() and zone::code::end()
    bottom= (int32*)OS::allocate_idealized_page_aligned(codeAndStubSize_p, "nmethod zone (inst + stubs)", NMethodStart);
    stb= (char*)bottom + codeSize_p; 
  
    if (OS::make_memory_executable(bottom, codeAndStubSize_p))
      fatal2("Couldn't make PIC area writable: start = 0x%x, length = %d",
             bottom, codeSize_p);

}



void zone::round_sizes(int32& codeSize, int32& stubSize, int32& depSize, int32& debugSize) {
   codeSize = roundSize(  codeSize, maxBlockSize);
    depSize = roundSize(   depSize, maxBlockSize);
  debugSize = roundSize( debugSize, maxBlockSize);
   stubSize = stubSize / StubBlock * StubBlock;
}


void zone::allocate_pieces(int32& codeSize, int32& stubSize, int32& depSize, int32& debugSize, char* stb) {
  int32 depSize_p = depSize, debugSize_p = debugSize; // alloate_idealized_page_aligned may change sizes
  char* db = OS::allocate_idealized_page_aligned(   depSize_p, "nmethod zone (deps)"  ,   DepsStart );
  char* sb = OS::allocate_idealized_page_aligned( debugSize_p, "nmethod zone (scopes)", ScopesStart );
  
  assert(int32(bottom) % maxBlockSize == 0, "zone misaligned");
  char* b = (char*)bottom;
  
  table      = new codeTable(codeTableSize); 
  debugTable = new codeTable(debugTableSize);

  iZone =            new Heap(codeSize,    iBLOCK,    iFREE,   b,   b != NMethodStart);
  dZone =            new Heap(depSize,     dBLOCK,    dFREE , db,  db !=    DepsStart);
  sZone =            new Heap(debugSize,   sBLOCK,    sFREE,  sb,  sb !=  ScopesStart);
  stubs = new Stubs( new Heap(stubSize, StubBlock, StubFree, stb, stb !=   StubsStart));
}

void zone::check_allocations() {
  if (iZone->contains(stubs->zone()->startAddr()))
    fatal("i and stub zones overlap");
  if (stubs->zone()->contains(dZone->startAddr()))
    fatal("d and stub zones overlap");
  if (dZone->contains(sZone->startAddr())) fatal("d and s zones overlap");
  assert(sizeof(LRUcount) == 4, "should be one word");
}

void zone::allocate_trapdoors_at_zone_base_stealing_from_code_space(int32& codeSize) {
  // allocate trapdoors at base of zone, steal space from code space
  trapdoors = new Trapdoors((char*)bottom, codeSize);
  int32 trapdoor_bytes_p = roundTo(trapdoors->trapdoor_bytes(), idealized_page_size);
  codeSize -= trapdoor_bytes_p;
  bottom    = (int32*)((char*)bottom + trapdoor_bytes_p);
}


static void idOverflowError(int32 delta) {
  Unused(delta);
  // fix this - maybe eliminate nmethod IDs altogether?
  fatal("zone: nmethod ID table overflowed");
}

void zone::allocate_idManager(fint maxNMs) {
  idManager = new IDManager(maxNMs, idOverflowError, ZoneIDStart);
}


void zone::setup_LRU() {
  LRUflag = idManager->data;
  LRUtable = (LRUcount*)LRUflag;
}

void zone::allocate_useCount(fint maxNMs) {
  int useCountSize= sizeof(LRUcount) * maxNMs;
  useCount= (int32*)OS::allocate_idealized_page_aligned(useCountSize, "use counts", UseCountStart);
}

void zone::clear() {
  table->clear();
  debugTable->clear();
  rememberLink.init();
  replCandidates.init();
  zombies.init();
  iZone->clear();
  dZone->clear();
  sZone->clear();
  stubs->clear();
  flatProfiler->clear(instsStart(), instsStart() + instsSize());
  LRUhand = NULL;
  LRUtime = 0;
  idManager->init();
  needsICompaction = needsDCompaction = needsSCompaction = _needsSweep = false;
  compactDuration = 1; minFreeFrac = 0.05;
  zoneTimer.timeRestart();
}

int32 zone::nextNMethodID() { return idManager->peekID(); }

nmethod* zone::alloc(int32 iLen, int32 sLen, int32 lLen,
                     int32 dLen, char*& insts, nmethodScopes*& scopes,
                     addrDesc*& locs, nmln*& deps) {
  CSect cs(profilerSemaphore);          // for profiler
  // must get method ID here! (because reclaim might change firstFree)
  // (compiler may have used peekID to get ID of new nmethod for LRU stuff)
  int32 myID = idManager->newID();
  if (needsSweep()) doSweep();
  assert(iLen % oopSize == 0 && sLen % oopSize == 0 &&
         lLen % oopSize == 0 && dLen % oopSize == 0,
         "must be multiple of oopSize");
  int32 iSize = sizeof(class nmethod) + iLen + lLen;
  int32 dSize = dLen;
  int32 sSize = sLen;

  nmethod* n;
  int32* s;
  int32* d;
  bool flushedZombies = false;
  bool chainedFrames = false;

  while (1) {
    // try to allocate the three parts
    n = (nmethod*)iZone->allocate(iSize);
    if (sSize) {
      s = (int32*)sZone->allocate(sSize);
    } else {
      s = (int32*)n;
    }
    if (dSize) {
      d = (int32*)dZone->allocate(dSize);
    } else {
      d = (int32*)s;
    }

    if (n && s && d) break;     // everything is ok
    
    // at least one allocation failed: undo them all and flush some stuff
    { const char* msg = n ? " sec reclaim " : " reclaiming ";
      ShowVMActivityInMonitor ss(msg);
      EventMarker em(msg);

      if (n == NULL)          needsICompaction = iZone->extFrag() > MaxExtFrag;
      if (dSize && d == NULL) needsDCompaction = dZone->extFrag() > MaxExtFrag;
      if (sSize && s == NULL) needsSCompaction = sZone->extFrag() > MaxExtFrag;
      
      if (PrintCodeReclamation && n) {
        lprintf("*flushing methods because secondary zone %s%s is full\n",
               needsDCompaction ? "D" : "", needsSCompaction ? "S" : "");
      }
      
      if (n) iZone->deallocate(n, iSize);
      if (sSize && s) sZone->deallocate(s, sSize);
      if (dSize && d) dZone->deallocate(d, dSize);
      
      if (!chainedFrames) {
        chainFrames();
        chainedFrames = true;
      }
      if (!flushedZombies) {
        flushZombies();
        flushedZombies = true;
      }
      
      if (needsWork()) {
        // schedule a zone compaction (will occur at next interrupt check)
        currentProcess->setupPreemption();
      }

      int32 reclaimed = 0;
      int32 toReclaim = min(LRU_MAX_RECLAIMED, iZone->usedBytes() / 4);
      for (fint i = 0; reclaimed < toReclaim && i < 20; i++) {
        reclaimed += flushNextMethod(toReclaim);
      }
      if (PrintCodeReclamation) {
        lprintf("*reclaimed %ld bytes in iZone\n", long(reclaimed));
      }      
      if (reclaimed < toReclaim) {
        // allocation failed
        if (chainedFrames) unchainFrames();
        idManager->freeID(myID);
        return NULL;
      }
    }
  }

  if (chainedFrames) unchainFrames();    

  n->id = myID;
  LRUtable[myID].set(0);
  useCount[myID] = 0;
  insts = (char*)(n + 1);
  locs  = (addrDesc*)((char*)insts + iLen);
  deps = (nmln*)d;
  scopes = (nmethodScopes*)s;
  if (VerifyZoneOften) {
    iZone->verify(); dZone->verify(); sZone->verify();
  }
  return n;
}

nmethod* zone::lookup(MethodLookupKey &k, bool needDebug) {
  return ( needDebug || currentProcess->isSingleStepping()
           ? debugTable
           : table
         ) -> lookup(k); }

int32 zone::used() {
  return iZone->usedBytes() + dZone->usedBytes() + sZone->usedBytes();
}

void zone::flush() {
  BlockProfilerTicks bpt(exclude_nmethod_flush);
  CSect cs(profilerSemaphore);          // for profiler
  timer tmr;
  ShowVMActivityInMonitor ss(" flushing ");
  EventMarker em("flushing method cache");
  if (PrintCodeReclamation) {
    lprintf("*flushing method cache...");
    tmr.start();
  }
  
  nonCombiningMode();
  chainFrames();
  stubs->flush();     // flush PICs first!  (otherwise: lots of stub shrinking)
  // To flush the nmethods, first mark them as zombies so they all end up
  // on the zombies list; then flush them.  This is more efficient than
  // flushing them directly: with heap block combination active, it is 
  // expensive to find the next nmethod after flushing the current one.
  // (This way, they're all on a nmln list.)
  FOR_ALL_NMETHODS(p) { p->makeZombie(); }
  flushZombies();

  unchainFrames();
  LRUhand = NULL;
  
  if (PrintCodeReclamation) {
    lprintf("done: %ld ms.\n", long(tmr.time()));
  }
  if (VerifyZoneOften) {
    iZone->verify(); dZone->verify(); sZone->verify();
  }

  // A MachineCache::flush_instruction_cache_for_debugging() isn't necessary because the flush is done
  // when writing the cache, i.e. when a new nmethod is created.
}

int32 zone::findReplCandidates(int32 needed) {
  // find replacement candidates; stop if > needed bytes found or if
  // there seem to be no more candidates
  assert(frame_chain_nesting > 0, "frames must be chained");
  int32 reclaimed = 0, iter = 0;
  while (iter++ < LRU_RETIREMENT_AGE && reclaimed < needed) {   
    int32 vis, recl;
    int32 limit = numberOfNMethods();           // because usedIDs may change
    int32 newTime = sweeper(limit, needed, &vis, &recl);
    reclaimed += recl;
    if (recl < needed && newTime > LRUtime + 1) {
      // next sweep wouldn't reclaim anything
      assert(vis == limit, "should have visited them all");
      LRUtime = newTime - 1;
      if (PrintLRUSweep) lprintf("\n*forced new LRU time: %ld", (void*)long(LRUtime));
    }
  }
  return reclaimed;
}

// flush next replacement candidate; return # bytes freed
int32 zone::flushNextMethod(int32 needed) {
  assert(frame_chain_nesting > 0, "frames must be chained");
  int32 freed = 0;
  while (freed < needed) {
    if (replCandidates.isEmpty()) {
      findReplCandidates(max(2*needed, LRU_MAX_RECLAIMED));
      if (replCandidates.isEmpty()) {
        // there's nothing to flush
        return freed;
      }
    }
    nmethod* p = NMETHOD_FROM(zoneLink, replCandidates.prev);
    if (p->isUsed()) {
      // has been used recently or is on stack - don't reclaim
      LRUtable[p->id].set(0);                   // on stack - mark used
      p->zoneLink.remove();
    } else {
      freed += iZone->sizeOfBlock(p);
      p->flush();
    }
  }
  return freed;
}

void moveInsts(char* from, char* to, int32 size) {
  Unused(size);
  nmethod* n = (nmethod*) from;
  nmethod* nTo = (nmethod*)to;
  
  n->moveTo(nTo, (char*)n->locsEnd() - (char*)n);
  if (Memory->code->LRUhand == n) Memory->code->LRUhand = nTo;
}

static void moveDeps(char* from, char* to, int32 size) {
  fint *backptr= *(fint**) from;
  if (Memory->code->iZone->contains(backptr)) {
    nmethod* n= (nmethod*)backptr;
    n->moveDeps((nmln*)(to + oopSize), to - from);
  } else {
    slotsMapDeps* m= (slotsMapDeps*)backptr;
    m->moveDeps((nmln*)(to + oopSize), to - from);
  }
  // copy *after* relocating since the two nmln arrays could overlap
  copy_words((int32*)from, (int32*)to, size / sizeof(int32));
}

static void moveScopes(char* from, char* to, int32 size) {
  nmethodScopes* scopes = (nmethodScopes*) from;
  scopes->my_nmethod()->moveScopes((nmethodScopes*)to);
  copy_words_up((int32*)from, (int32*)to, size / sizeof(int32));
}

void zone::flushZombies() {
  chainFrames();
  nmln nonFlushable;
  while (zombies.notEmpty()) {
    nmethod* nm = NMETHOD_FROM(zoneLink, zombies.next);
    assert(nmethod::isNMethod(nm), "invalid nmethod");
    assert(nm->isZombie(), "not a zombie");
    if (nm->frame_chain != NoFrameChain) {
      // temporarily remove non-flushable nmethods from zombie chain
      nmln* l = &nm->zoneLink;          // cfront bogosity
      l->rebind(&nonFlushable);
    } else {
      nm->flush();
    }
  }
  // move nonflushables back to zombie list
  zombies = nonFlushable;
  zombies.relocate();

  unchainFrames();
}

void zone::markAllUnused() {
  // mark all nmethods as unused (used for thesis measurements)
  chainFrames();
  FOR_ALL_NMETHODS(p) {
    p->oldCount = useCount[p->id];
    LRUtable[p->id].unused = true;
    LRUtable[p->id].lastUsed = LRUtime;
  } 
  unchainFrames();
}

void zone::flushUnused() {
  // flush all nmethods marked as unused
  // NB: access methods are always unused since they don't do the LRU thing
  chainFrames();
  FOR_ALL_NMETHODS(p) {
    // use makeZombie() for efficiency, not flush()
    // (see comment in zone::flush())
    if (!p->isUsed()) p->makeZombie();
  }
  flushZombies();
  MachineCache::flush_instruction_cache_for_debugging();
  unchainFrames();
}

void zone::adjustPolicy() {
  // This routine does the policy decisions about flushing, using feedback
  // from previous flushes.  If the heap is too full, then we'll soon have
  // to compact again, leading to frequent pauses and hight overhead.  On
  // the other hand, if we flush too many nmethods before compacting, we'll
  // have high compilation overhead to recreate the flushed code.

  if (IntervalTimer::dont_use_any_timer) return;          // for reproducible results & debugging
  float timeSinceLast = zoneTimer.millisecs();
  float overhead = compactDuration / timeSinceLast;
  if (overhead > COMPACT_OVERHEAD) {
    minFreeFrac = min(float(minFreeFrac * 1.5), float(minFreeFrac + 0.05));
    if (PrintCodeReclamation) {
      lprintf("(compact overhead %3.1f%%: increasing minFreeFrac to %3.1f%%) ",
             overhead *100, minFreeFrac * 100);
    }
  } else if (overhead < COMPACT_OVERHEAD / 2) {
    minFreeFrac = max(float(minFreeFrac / 1.5), float(minFreeFrac - 0.05));
    if (PrintCodeReclamation) {
      lprintf("(compact overhead %3.1f%%: decreasing minFreeFrac to %3.1f%%) ",
             overhead *100, minFreeFrac * 100);
    }
  }
  int32 minFree = int32(iZone->capacity() * minFreeFrac);
  while (1) {
    int32 toFlush = minFree - (iZone->capacity() - iZone->usedBytes());
    if (toFlush <= 0) break;
    flushNextMethod(LRU_MAX_RECLAIMED);
  }
}


void zone::doSweep() { sweeper(LRU_MAX_VISITED, LRU_MAX_RECLAIMED); }

void zone::doWork() {
  if (needsSweep()) doSweep();
  if (needsCompaction()) compact();
}

void zone::compact(bool forced) {
  BlockProfilerTicks bpt(exclude_nmethod_compact);
  if (VMProfileCompaction) OS::profile(true);
  CSect cs(profilerSemaphore);          // for profiler
  timer tmr;
  ShowVMActivityInMonitor ss(" compacting ");
  EventMarker em("compacting zone");
  if (PrintCodeReclamation) {
    lprintf("*compacting nmethod cache...");
    tmr.start();
  }
  
  chainFrames();
  flushZombies();
  char* firstFree = NULL;
  if (!forced) adjustPolicy();
  zoneTimer.timeRestart();
  if (needsICompaction) {
    if (PrintCodeReclamation) lprintf("I");
    firstFree = iZone->compact(moveInsts);
    MachineCache::flush_instruction_cache_for_debugging();
    flatProfiler->clear(firstFree, instsStart() + instsSize());
  }
  if (needsDCompaction) {
    if (PrintCodeReclamation) lprintf("D");
    dZone->compact(moveDeps);
  }
  if (needsSCompaction) {
    if (PrintCodeReclamation) lprintf("S");
    sZone->compact(moveScopes);
  }
  unchainFrames();
  MachineCache::flush_instruction_cache_for_debugging();
  
  if (PrintCodeReclamation) {
    lprintf(" done: %ld bytes free in iZone, %ld ms.\n",
           long(iZone->capacity() - iZone->usedBytes()), long(tmr.time()));
  }
  if (VerifyZoneOften) {
    iZone->verify(); dZone->verify(); sZone->verify();
  }
  needsICompaction = needsDCompaction = needsSCompaction = false;
  compactDuration = zoneTimer.millisecs();
  if (VMProfileCompaction) OS::profile(VMProfiling);
}

void zone::doChainFrames()   { processes->chainFrames(); }
void zone::doUnchainFrames() { processes->unchainFrames(); }

void zone::free_nmethod(nmethod* nm) {
  assert(nm->zoneLink.isEmpty(), "non-empty zone link");
  if (VerifyZoneOften) {
    iZone->verify(); dZone->verify(); sZone->verify();
  }
  if (LRUhand == nm) LRUhand = next_nm(nm);
  int32 iSize = sizeof(class nmethod) + nm->instsLen() + nm->locsLen();
  used_per_compiler[nm->flags.compiler] -= iSize;
  int32 dSize = nm->depsLen + sizeof(nmethod*);
  int32 sSize = nm->scopes->size();
  idManager->freeID(nm->id);
  int32* s = (int32*)nm->scopes;
  int32* d = (int32*)nm->deps()   - 1;  // -1 for backpointer
  if (dZone->contains(d)) {
    sZone->deallocate(s, sSize);
    dZone->deallocate(d, dSize);
  } else {
    // deps allocated in sZone
    sZone->deallocate(s, sSize + dSize);
  }
  iZone->deallocate(nm, iSize);
  if (VerifyZoneOften) {
    iZone->verify(); dZone->verify(); sZone->verify();
  }
}

void zone::addToCodeTable(nmethod* nm, MethodLookupKey *k) {
  (nm->isDebug() ? debugTable : table)->add(nm, k);
}

void zone::flush_inline_cache() {
  timer tmr;   
  ShowVMActivityInMonitor ss(" ic flush ");
  EventMarker em("flushing inline caches");
  
  if (PrintInlineCacheInvalidation) {
    lprintf("*flushing inline caches...");
    tmr.start();
  }
  
  stubs->flush();     // flush PICs first!  (otherwise: lots of stub shrinking)
  FOR_ALL_NMETHODS(p) { p->remove_me_from_inline_cache(); }
  MachineCache::flush_instruction_cache_for_debugging();
  
  if (PrintInlineCacheInvalidation) {
    lprintf("done: %ld ms.\n", long(tmr.time()));
  }
}

inline int32 retirementAge(nmethod* n) {
  fint delta = LRU_RETIREMENT_AGE;
  if (n->instsLen() <= LRU_CUTOFF)
    delta = max(delta, LRU_RETIREMENT_AGE * LRU_SMALL_BOOST);
  if (n->compiler() != NIC) 
    delta = max(delta, LRU_RETIREMENT_AGE * LRU_OPTIMIZED_BOOST);
  return n->lastUsed() + delta;
}

bool zone::verify() {
  bool r = true;
  r &= table->verify();
  r &= debugTable->verify();
  {
    fint n = 0;
    EventMarker("zone::verify %d nms", (void*)numberOfNMethods());
    FOR_ALL_NMETHODS(p) {
      n++;
      eventLog->log("verifying nm %d: 0x%x", (void*)n, p);
      r &= p->verify();
    }
    if (n != numberOfNMethods()) {
      error2("zone: inconsistent usedIDs value - should be %ld, is %ld",
             n, numberOfNMethods());
      r = false;
    }
  }
  r &= iZone->verify();
  r &= dZone->verify();
  r &= sZone->verify();  
  r &= stubs->verify();
  nmln *l;
  for (l = replCandidates.next; l->next != &replCandidates; l = l->next) {
    nmethod* n1 = NMETHOD_FROM(zoneLink, l);
    nmethod* n2 = NMETHOD_FROM(zoneLink, l->next);
    if (retirementAge(n1) < retirementAge(n2) &&
        n1->lastUsed() != 0) {
      warning4("wrong order in replCandidates: %#lx (%ld) and %#lx (%ld)",
               n1, n1->lastUsed(), n2, n2->lastUsed());
    }
  }
  for (l = zombies.next; l->next != &zombies; l = l->next) {
    nmethod* nn = NMETHOD_FROM(zoneLink, l);
    if (!nn->isZombie()) {
      error1("non-zombie method in zombies list: %#lx", nn);
      r = false;
    }
  }
  r &= idManager->verify();
  return r;
}

void zone::scavenge_contents() {
  nmln *p = rememberLink.next;
  rememberLink.init(); // cause OopNCode::scavenge_contents will remember methods
  bool needToInvalICache = false;
  for (nmln *q = p->next; p != &rememberLink; p = q, q = q->next) {
    OopNCode* m = OOPNCODE_FROM(rememberLink, p);
    needToInvalICache |= m->scavenge_contents();
  }
  if (needToInvalICache) MachineCache::flush_instruction_cache_for_debugging();
}

void zone::switch_pointers(oop from, oop to) {
  bool needToInvalICache = false;
  // this code used to invalidate as it went, but since invalidation
  // may free PICS and snap both rememberLinks and next pointers,
  // you cannot traverse and invalidate at the same time.
  // (Why invalidate? see nmethodScopes::switch_pointers())
  // So, make a list and invalidate later.
  // But, resource marks further down could cause part of list
  //  to be allocated and freed before using list was used.
  // So, make list big enough from the start.
  // dmu 7/94 (w/ help from Urs)
  
  ResourceMark rm;
  nmethodBList* nmethods_to_invalidate = new nmethodBList(numberOfNMethods());

  if (from->is_new()) {
    nmln *p = rememberLink.next;
    for (nmln *q = p->next; p != &rememberLink; p = q, q = q->next) {
      nmethod* m = NMETHOD_FROM(rememberLink, p);
      needToInvalICache |= m->switch_pointers(from, to,
                                              nmethods_to_invalidate);
    }
  } else {
    FOR_ALL_NMETHODS(r)
      needToInvalICache |= r->switch_pointers(from, to,
                                              nmethods_to_invalidate);
    needToInvalICache |= stubs->switch_pointers(from, to,
                                                nmethods_to_invalidate);
  }
  for (fint i = 0;  i < nmethods_to_invalidate->length();  i++)
    nmethods_to_invalidate->nth(i)->invalidate();

  if (needToInvalICache) MachineCache::flush_instruction_cache_for_debugging();
}

void zone::gc_mark_contents() {
  FOR_ALL_NMETHODS(p) p->gc_mark_contents();
  stubs->gc_mark_contents(); 
}

void zone::gc_unmark_contents() {
  bool needToInvalICache = false;
  FOR_ALL_NMETHODS(p) needToInvalICache |= p->gc_unmark_contents();
  needToInvalICache |= stubs->gc_unmark_contents(); 
  if (needToInvalICache) MachineCache::flush_instruction_cache_for_debugging();
}

void zone::oops_do(oopsDoFn f) {
  bool needToInvalICache = false;
  FOR_ALL_NMETHODS(p) needToInvalICache |= p->code_oops_do(f);
  needToInvalICache |= stubs->code_oops_do(f);
  if (needToInvalICache) MachineCache::flush_instruction_cache_for_debugging();
}

void zone::nmethods_do(nmethodDoFn f) { FOR_ALL_NMETHODS(p) f(p); }

char* zone::allocateDeps(fint nbytes) {
  char *d= (char*) dZone->allocate(nbytes);

  if (d == NULL) {
    // allocation failed -- need to do some deallocation
    // this code is depressingly similar to the code in zone::alloc.
    // It would be nice if it could be factored...
    
    // must get method ID here! (because reclaim might change firstFree)
    // (compiler may have used peekID to get ID of new nmethod for LRU stuff)
    int32 myID = idManager->newID();
    if (needsSweep()) doSweep();
 
    bool flushedZombies = false;
    bool chainedFrames = false;

    do {
      ShowVMActivityInMonitor ss(" reclaiming ");
      EventMarker em(" reclaiming ");
      
      needsDCompaction = dZone->extFrag() > MaxExtFrag;
      
      if (!chainedFrames) {
        chainFrames();
        chainedFrames = true;
      }
      if (!flushedZombies) {
        flushZombies();
        flushedZombies = true;
      }
      
      if (needsWork()) {
        // schedule a zone compaction (will occur at next interrupt check)
        currentProcess->setupPreemption();
      }
      
      int32 reclaimed = 0;
      int32 toReclaim = min(LRU_MAX_RECLAIMED, iZone->usedBytes() / 4);
      for (fint i = 0; reclaimed < toReclaim && i < 20; i++) {
        reclaimed += flushNextMethod(toReclaim);
      }
      if (PrintCodeReclamation) {
        lprintf("*reclaimed %ld bytes in iZone\n", long(reclaimed));
      }      
      if (reclaimed < toReclaim) {
        // allocation failed
        if (chainedFrames) unchainFrames();
        idManager->freeID(myID);
        return NULL;
      }
      d= (char*) dZone->allocate(nbytes);
    } while (d == NULL);

    if (chainedFrames) unchainFrames();    

    LRUtable[myID].set(0);
    useCount[myID] = 0;
    if (VerifyZoneOften) {
      iZone->verify(); dZone->verify(); sZone->verify();
    }
  }

  return d;
}

void zone::setDepsMap(nmln *deps, slotsMapDeps *m) {
  *(((slotsMapDeps**)deps)-1)= m; }


# define NMLINE(format, n, ntot, ntot2)                                       \
  lprintf(format, (n), 100.0 * (n) / (ntot), 100.0 * (n) / (ntot2))

class nmsizes {
 public:
  int32 n, insts, locs, deps, scopes;
  void  clear()   { n = insts = locs = deps = scopes = 0; }
  int32 total()   {
    return n * sizeof(nmethod)+ insts + locs + deps + scopes; }
  void add(nmsizes& other) {
    n += other.n; insts += other.insts; locs += other.locs;
    deps += other.deps; scopes += other.scopes;
  }
  bool isEmpty() { return n == 0; }
  void print(const char* name, nmsizes& tot) {
    int32 bigTotal = tot.total();
    int32 myTotal = total();
    if (! isEmpty()) {
      lprintf("%-13s (%ld methods): ", name, n);
      NMLINE("headers = %ld (%2.0f%%/%2.0f%%); ", n*sizeof(nmethod),
             myTotal, bigTotal);
      NMLINE("insts = %ld (%2.0f%%/%2.0f%%);\n", insts,
             myTotal, bigTotal);
      NMLINE("\tlocs = %ld (%2.0f%%/%2.0f%%); ",
             locs, myTotal, bigTotal);
      NMLINE("deps = %ld (%2.0f%%/%2.0f%%); ", deps, myTotal, bigTotal);
      NMLINE("scopes = %ld (%2.0f%%/%2.0f%%);\n",
             scopes, myTotal, bigTotal);
      NMLINE("\ttotal = %ld (%2.0f%%/%2.0f%%)\n",
             myTotal, myTotal, bigTotal);
    }
  }
  void print_stats(const char* name) {
    if (! isEmpty()) {
      lprintf("%-13s: %7ld %7ld %7ld %7ld %7ld %7ld\n",
             name, long(n), long(n*sizeof(nmethod)), long(insts), long(locs),
             long(deps), long(scopes));
    }
  }
};

void zone::print() { print_helper(false); }
void zone::print_stats() { print_helper(true); }

void zone::print_helper(bool stats) {
  nmsizes nms[nm_last];
  int32 i;
  for (i = 0; i < nm_last; i++) nms[i].clear();
  int32 n = 0, ignored = 0;
  FOR_ALL_NMETHODS(p) {
    if (stats && (p->isZombie() || p->isInvalid())) {
      ignored++;
      continue;                 // don't count obsolete nmethods
    }
    n++;
    fint comp = p->flags.compiler;
    assert(comp >= 0 && comp < nm_last, "invalid compiler");
    nms[comp].n++;
    nms[comp].insts  += p->instsLen();
    nms[comp].locs   += p->locsLen();
    nms[comp].deps   += p->depsLen;
    nms[comp].scopes += p->scopes->length();
  }
  if (n + ignored != numberOfNMethods()) warning("inconsistent usedIDs value");
  nmsizes total;
  total.clear();
  for (i = 0; i < nm_last; i++) total.add(nms[i]);

  if (stats) {
    int32 nstubs = 0, npics = 0, stubs_l = 0, pics = 0,
      stubsTotal = 0, picsTotal = 0;
    FOR_ALL_STUBS(st) {
      if (st->isCacheStub()) {
        npics++;  pics += st->instsLen();  picsTotal += st->size();
      } else {
        nstubs++; stubs_l += st->instsLen(); stubsTotal += st->size();
      }
    }
    nms[nm_nic].print_stats("NIC compiler");
    nms[nm_sic].print_stats("SIC compiler");
    lprintf("PICs:  %7ld %7ld %7ld\n", long(npics), long(pics),
           long(picsTotal));
    lprintf("stubs: %7ld %7ld %7ld\n", long(nstubs), long(stubs_l),
           long(stubsTotal));
  } else {
    printIndent();
    lprintf("zone: starts at %#-6lx; LRU time %ld; minFree %3.1f%%; %ld nmethods (%1.0f%%)\n",
           (long unsigned)bottom, long(LRUtime),
           minFreeFrac * 100, long(n), n * 100.0 / idManager->n);
    printIndent(); lprintf("iZone: "); iZone->print();
    printIndent(); lprintf("dZone: "); dZone->print();
    printIndent(); lprintf("sZone: "); sZone->print();
    printIndent(); lprintf("stubs: "); stubs->print();

    printIndent(); nms[nm_nic].print("NIC compiler", total);
    printIndent(); nms[nm_sic].print("SIC compiler", total);
    printIndent(); total.print("total", total);
  }
}

class nm_hist_elem{
 public:
  nmethod* nm;
  fint     count;
  fint     size;
  fint     sic_count;
  fint     sic_size;
};

static int compareOop(const void *m1, const void *m2) {
  ResourceMark rm;
  class nm_hist_elem *nmethod1 = (class nm_hist_elem *) m1;
  class nm_hist_elem *nmethod2 = (class nm_hist_elem *) m2;
  return nmethod2->nm->method() - nmethod1->nm->method();
}

static int compareCount(const void *m1, const void *m2) {
  class nm_hist_elem *nmethod1 = (class nm_hist_elem *) m1;
  class nm_hist_elem *nmethod2 = (class nm_hist_elem *) m2;
  return nmethod2->count - nmethod1->count;
}

void zone::print_nmethod_histogram(fint size) {
  ResourceMark rm;
  nm_hist_elem* hist_array = NEW_RESOURCE_ARRAY(nm_hist_elem, numberOfNMethods());

  fint*  compiled_nmethods = NEW_RESOURCE_ARRAY(fint, numberOfNMethods());

  int32 n = 0;
  FOR_ALL_NMETHODS(p) {
    if (!p->isAccess() && !p->isZombie()) {
      hist_array[n].nm     = p;
      hist_array[n].size   = p->instsLen() +
                             p->locsLen() +
                             p->depsLen +
                             p->scopes->length();
      n++;
    }
  }

  qsort(hist_array, n, sizeof(nm_hist_elem), compareOop);
  
  int32 i = 0;
  for (i = 0; i < n; i++) compiled_nmethods[i] = 0;

  i = 0;
  fint out = 0;
  while (i < n) {
    int counter     = 1;
    int all_size    = 0;
    int sic_counter = 0;
    int sic_size    = 0;

    oop method = hist_array[i].nm->method();
    all_size = hist_array[i].size;

    i++;

    while (i < n && method == hist_array[i].nm->method()) {
      if (hist_array[i].nm->flags.compiler == nm_sic) {
        sic_counter++;
        sic_size += hist_array[i].size;
      }
      all_size += hist_array[i].size;
      counter++; i++;
    }
    compiled_nmethods[counter]++;
    if (counter > size) {
      hist_array[out]           = hist_array[i-1];
      hist_array[out].count     = counter;
      hist_array[out].size      = all_size;
      hist_array[out].sic_count = sic_counter;
      hist_array[out].sic_size  = sic_size;
      out++;
    }
  }

  qsort(hist_array, out, sizeof(nm_hist_elem), compareCount);

  lprintf("\n# nm \t # methods \t%% acc.\n");
  int nm_count = 0;
  for (i = 0; i < n; i++) {
    if (compiled_nmethods[i] > 0) {
      nm_count += i * compiled_nmethods[i];
      lprintf("%5ld \t%5ld \t\t%3ld %%\n", long(i), long(compiled_nmethods[i]), 
             long((nm_count*100)/n));

    }
  }

  lprintf( "\nList of methods with more than %d nmethods compiled.\n", size);
  lprintf( " ALL(#,Kb)  SIC(#,Kb) Method:\n");
  for (i = 0; i < out; i++) {
    lprintf("%4d,%-4d   %4d,%-4d ",
           hist_array[i].count,     hist_array[i].size / 1024,
           hist_array[i].sic_count, hist_array[i].sic_size / 1024);
    printName((methodMap*) hist_array[i].nm->method()->map(),
               hist_array[i].nm->key.selector);
    lprintf("\n");
  }
}


nmethod* zone::findNMethod(void* start) {
  nmethod* n;
  if (iZone->contains(start)) {
    n = (nmethod*)iZone->findStartOfBlock(start);
    assert(nmethod::isNMethod(n), "not an nmethod");
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      if ((char*)start >= (char*)n->locsEnd()) {
        warning2("found wrong nmethod, start = 0x%x, n->locsEnd = 0x%x\n",
                 start, n->locsEnd());
        verify();
        fatal("found wrong nmethod");
      }
    }
#   endif
  } else if (dZone->contains(start)) {
    n = *(nmethod**)dZone->findStartOfBlock(start);
  } else {
    assert(sZone->contains(start), "not in any zone");
    nmethodScopes* scopes = (nmethodScopes*)sZone->findStartOfBlock(start);
    n = scopes->my_nmethod();
  }
  assert(iZone->contains(n), "not in zone");
  assert(n->isNMethod(), "findNMethod didn't find nmethod");
  assert(n->encompasses(start), "doesn't encompass start");
  return n;
}


void zone::findNMethodOrMap(nmln* n, nmethod* &nm, slotsMapDeps* &s) {
  nm= NULL;  s= NULL;
  if (iZone->contains(n)) {
    nm= (nmethod*)iZone->findStartOfBlock(n);
    assert(nmethod::isNMethod(nm), "not an nmethod");
    assert((char*)n < (char*)nm->locsEnd(), "found wrong nmethod");
  } else if (dZone->contains(n)) {
    void *p= *(void**)dZone->findStartOfBlock(n);
    if (iZone->contains(p))
      nm= (nmethod*)p;
    else
      s= (slotsMapDeps*)p;
  } else if (sZone->contains(n)) {
    nmethodScopes* scopes= (nmethodScopes*)sZone->findStartOfBlock(n);
    nm= scopes->my_nmethod();
  } else {
    assert(Memory->is_obj_heap((oop*)n), "not a map");
  }
  assert(nm == NULL || iZone->contains(nm), "not in zone");
  assert(nm == NULL || nm->isNMethod(), "findNMethod didn't find nmethod");
  assert(nm == NULL || nm->encompasses(n), "doesn't encompass start");
}


nmethod* zone::findNMethod_maybe(void* start) {
  // start *may* point into the instructions part of a nmethod; find it
  if (!iZone->contains(start)) return NULL;
  // relies on FOR_ALL_NMETHODS to enumerate in ascending order
  FOR_ALL_NMETHODS(p) {
    if (p->insts() > (char*)start) return NULL;
    if (p->instsEnd() > (char*)start) return p;
  }
  return NULL;
}

char* zone::instsStart() { return iZone->startAddr(); };
int32 zone::instsSize()  { return iZone->capacity(); }

inline nmethod* zone::next_circular_nm(nmethod* nm) {
  nm = next_nm(nm);
  if (nm == NULL) nm = first_nm();
  assert(nm == NULL || nmethod::isNMethod(nm), "not a valid nmethod");
  return nm;
}

// called every IntervalTimer::LRU_resolution seconds or by reclaimNMethods if needed
// returns time at which oldest non-reclaimed nmethod will be reclaimed
int32 zone::sweeper(int32 maxVisit, int32 maxReclaim,
                    int32* nvisited, int32* nbytesReclaimed) {
  ShowVMActivityInMonitor ss(" LRU sweep ");
  EventMarker em("LRU sweep");
  ResourceMark rm;

  timer tmr;   
  int32 visited = 0;
  int32 nused = 0;
  int32 nbytes = 0;
  int32 nextTime = LRUtime + LRU_RETIREMENT_AGE;
  nmethod* first = first_nm();
  if (! first) return -1;
  
  nmethod* p = LRUhand ? LRUhand : first;
  if (PrintLRUSweep || PrintLRUSweep2) {
    lprintf("*starting LRU sweep...");
    tmr.start();
  }
  
  chainFrames();
  
  do {
    if (PrintLRUSweep2) lprintf("\n*inspecting %#lx (id %ld): ",
                               (void*)(long unsigned)p, (void*)long(p->id));
    assert(nmethod::isNMethod(p), "invalid nmethod");
    
    if ((p->isZombie() ||
         p->isDebug()  ||
         (p->codeTableLink == NULL && !p->isUncommon() && !p->isDI())) &&
        p->frame_chain == NoFrameChain) {
      // can be flushed - nobody will ever use it again
      if (PrintLRUSweep2) lprintf(" %s; flushed",
                                 p->isZombie() ? "zombie" :
                                 (p->isDebug() ? "debug" : "unreachable"));
      nbytes += iZone->sizeOfBlock(p);
      p->flush();
    } else if (p->isUsed()) {
      // has been used
      nused++;
      if (PrintLRUSweep2) {
        lprintf("used");
        int32 diff = useCount[p->id] - p->oldCount;
        if (diff) lprintf(" %ld times", (void*)long(diff));
      }
      if (LRUDecayFactor > 1) {
        useCount[p->id] = int(useCount[p->id] / LRUDecayFactor);
      }
      p->oldCount = useCount[p->id];
      LRUtable[p->id].unused = true;
      LRUtable[p->id].lastUsed = LRUtime;
      if (p->zoneLink.notEmpty()) {
        if (PrintLRUSweep2) lprintf("; removed from replCandidates");
        p->zoneLink.remove();           // no longer a replacement candidate
        nbytes -= iZone->sizeOfBlock(p);
      }
    } else if (retirementAge(p) < LRUtime && p->zoneLink.isEmpty()) {
      if (PrintLRUSweep2) lprintf("added to replCandidates");
      replCandidates.add(&p->zoneLink);
      nbytes += iZone->sizeOfBlock(p);
    } else {
      int32 age = retirementAge(p);
      if (age < nextTime) {
        nextTime = age;
      }
      if (PrintLRUSweep2) {
        lprintf("unused (age %ld)", (void*)long(LRUtime - p->lastUsed()));
        if (p->zoneLink.notEmpty())
          lprintf(" already scheduled for replacement");
      }
    }
    
    nmethod* oldp = p;
    p = next_circular_nm(p);
    if (p < oldp) {                             // wrap around
      LRUtime++;
      // The LRU scheme will actually fail if LRUtime > 2^16, but that
      // won't happen very often (every 20*IntervalTimer::LRU_resolution CPU hours).
      if (PrintLRUSweep2) lprintf("\n*new LRU time: %ld", (void*)long(LRUtime));
    }
  } while (++visited < maxVisit && nbytes < maxReclaim && p);
  
  if (needsSweep() && LRUDecayFactor > 1) {
    // called from timer; decay count stubs
    stubs->decay(LRUDecayFactor);
  }

  unchainFrames();
  
  LRUhand = p;
  _needsSweep = false;
  if (PrintLRUSweep || PrintLRUSweep2) {
    lprintf(" done: %ld/%ld visits, %ld bytes, %ld ms.\n",
           (void*)long(nused), (void*)long(visited), (void*)long(nbytes), (void*)long(tmr.time()));
  }
  if (nvisited) *nvisited = visited;
  if (nbytesReclaimed) *nbytesReclaimed = nbytes;
  return nextTime;
}

int32 zone::LRU_time() { return LRUtime; }

void printAllNMethods() {
  for(nmethod* m = Memory->code->first_nm(); m != NULL;
      m = Memory->code->next_nm(m)) {
    lprintf("nmethod %#lx, id %ld (", m, long(m->id));
    printName(0, m->key.selector);
    lprintf("), ");
    lprintf("map %#lx, size %ld, ", m->key.receiverMapOop(),
                                 (long unsigned)m->size() );
    if (m->isUsed()) {
      lprintf("used");
    } else {
      if (! LRUtable[m->id].unused ||
          LRUtable[m->id].lastUsed > Memory->code->LRU_time())
        warning("inconsistent LRU table entry! ");
      lprintf("last used at %ld", long(LRUtable[m->id].lastUsed));
    }
    lprintf("\n");
    
  }
  lprintf("\nreplacement candidates:\n");
  nmln* p;
  for ( p = Memory->code->replCandidates.next;
        p != &Memory->code->replCandidates; 
        p = p->next) {
    nmethod* n = NMETHOD_FROM(zoneLink, p);
    lprintf("nmethod %#lx, id %ld, lastUsed %ld\n", n,
           long(n->id), long(n->lastUsed()));
  }
  lprintf("\nzombies:\n");
  for (p = Memory->code->zombies.next;
       p != &Memory->code->zombies;
       p = p->next) {
    nmethod* n = NMETHOD_FROM(zoneLink, p);
    lprintf("nmethod %#lx, id %ld, lastUsed %ld\n", n,
           long(n->id), long(n->lastUsed()));
  }
}


// fix nmln after reading in code in case the zone object has moved

void zone::relocate_nmln(nmln* p) {
  if ((caddr_t)p->next >= NMethodStart) {
    p->next->prev = p;
    p->prev->next = p;
  } else
    p->init();
}


#ifdef UNUSED
void zone::relocate() {
  FOR_ALL_NMETHODS(p) {
    p->relocate();
  }
  stubs->relocate();
}
#endif

void zone::fixup() {
  table->clear();
  debugTable->clear();
  FOR_ALL_NMETHODS(p) {
    if (p->codeTableLink == NULL) {
      // e.g. di methods; leave it empty
    } else {
      assert(! p->isZombie(), "codeTableLink not be empty");
      p->codeTableLink= NULL;
      if (p->isDebug()) {
        debugTable->add(p);
      } else {
        table->add(p);
      }
    }
  }
}


// the following is a quick hack to estimate the debugging-related space
// cost outside the zone (selectors etc)
static int32 debugBytes;

static void oopDebugSize(oop* p1) {
  oop p = *p1;
  if (p->is_objectMarked()) return;
  p->markThisObject();
  debugBytes += p->debug_size();
}

static void oopResetMark(oop* p) { (*p)->unmarkThisObject(); }

// for debugging
static bool ignoreRcvr;
static oop findNM_map;
static oop findNM_sel;
static fint findNM_found;
static void findNM_helper(nmethod* nm) {
  if (nm->key.selector == findNM_sel &&
      (ignoreRcvr || nm->key.receiverMapOop() == findNM_map)) {
    lprintf("nmethod 16r%lx \'%s\'\n", nm,
           selector_string(nm->key.selector));
    findNM_found++;
  }
}

static void findNM_helper2(nmethod* nm) {
  bool first = true;
  FOR_EACH_SCOPE(nm->scopes, s) {
    if (first) {
      first = false;    // ignore top-level scope -- already printed
    } else if (s->key.selector == findNM_sel) {
      if (ignoreRcvr || (!s->isDeadBlockScope() && s->selfMapOop() == findNM_map)) {
        lprintf("nmethod 16r%lx \'%s\'\n", nm,
               selector_string(nm->key.selector));
        findNM_found++;
      }
    }
  }
}

oop PrintDebugSize_prim(oop rcvr) {
  Unused(rcvr);
  debugBytes = 0;
  Memory->code->oops_do(oopDebugSize);
  Memory->code->oops_do(oopResetMark);
  return as_smiOop(debugBytes);
}

oop findNMethods_prim(oop rcvr, oop map, oop sel) {
  // print nmethods with selector sel and given receiver map (if rcvr is
  // a mirror); ignore rcvr type if non-mirror
  ResourceMark rm;
  ignoreRcvr = !map->is_mirror();
  findNM_map = badOop;
  findNM_sel = sel;
  findNM_found = 0;
  if (!ignoreRcvr) findNM_map = mirrorOop(map)->reflectee()->map()->enclosing_mapOop();
  lprintf("nmethods implementing \'%s\'%s:\n", selector_string(findNM_sel),
         ignoreRcvr ? "" : " for this receiver type");
  Memory->code->nmethods_do(findNM_helper);
  if (findNM_found == 0) lprintf("--none.\n");
  findNM_found = 0;
  lprintf("nmethods containing inlined copies of \'%s\'%s:\n",
         selector_string(findNM_sel),
         ignoreRcvr ? "" : " for this receiver type");
  Memory->code->nmethods_do(findNM_helper2);
  if (findNM_found == 0) lprintf("--none.\n");
  return rcvr;
}

oop printNMethodCode_prim(oop rcvr) {
  // rcvr is address of a nmethod
  if (rcvr->is_smi()) {
    nmethod* nm = (nmethod*)smiOop(rcvr)->value();
    nm->printCode();
  } else {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }
  return rcvr;
}

# else  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

oop PrintDebugSize_prim(oop rcvr) {
  Unused(rcvr);
  return as_smiOop(0);
}

oop findNMethods_prim(oop rcvr, oop map, oop sel) {
  Unused(map); Unused(sel); Unused(rcvr);
  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}

oop printNMethodCode_prim(oop rcvr) {
  Unused(rcvr);
  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}

void zone::findNMethodOrMap(nmln *n, nmethod* &nm, slotsMapDeps* &s) {
  Unused(n);
  nm = NULL;
  s = NULL;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


void zone::read_snapshot(FILE* f) {
  char* buf[sizeof(zone)];
  zone* theZone = (zone*)buf;

  if (SnapshotCode) { // must read or skip
#   if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
      OS::FRead(buf, sizeof(zone), f);
      fint n= idManager->read_snapshot(f);

      if (okToUseCodeFromSnapshot        &&  theZone->bottom != bottom)
        noCodeWarning("because of a zone address mismatch");

      OS::read_or_seek(useCount, n * sizeof(int32  ), f);
      OS::read_or_seek(&LRUtime,     sizeof(LRUtime), f);

      iZone->read_snapshot(f);
      dZone->read_snapshot(f);
      sZone->read_snapshot(f);

      stubs->zone()->read_snapshot(f);

          nmethod::set_vtbl_value();
        CacheStub::set_vtbl_value();
     CountingStub::set_vtbl_value();
    ComparingStub::set_vtbl_value();
        AgingStub::set_vtbl_value();

      if (!okToUseCodeFromSnapshot)
        clear();
      else {
        // copy across all the relevant fields
                  bottom= theZone->bottom;
                 LRUhand= theZone->LRUhand;
        needsICompaction= theZone->needsICompaction;
        needsSCompaction= theZone->needsSCompaction;
        needsDCompaction= theZone->needsDCompaction;
             _needsSweep= theZone->_needsSweep;
         compactDuration= theZone->compactDuration;
             minFreeFrac= theZone->minFreeFrac;
            rememberLink= theZone->rememberLink;
          replCandidates= theZone->replCandidates;
                 zombies= theZone->zombies;
       for (int i = 0; i < nm_last; ++i)
         used_per_compiler[i]= theZone->used_per_compiler[i];
          
        // in case the zone itself moved,
        // must relocate the ends of all nmlns -- dmu
        relocate_nmln(&rememberLink);
        relocate_nmln(&replCandidates);
        relocate_nmln(&zombies);
        
      }
      ((CountStub*)NULL)->read_snapshot(f);
      
      MachineCache::flush_instruction_cache_range(iZone->startAddr(), iZone->endAddr());
      MachineCache::flush_instruction_cache_for_debugging();
#   else // no compiler
      Unused(f);
      fatal("This version of Self has no compilers, and cannot read a snapshot with code.");
#   endif
  } else {
    clear();
  }
}


void zone::write_snapshot(FILE* f) {
  flushZombies();

  if (SnapshotCode) {
#   if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
      OS::FWrite(this, sizeof(zone), f);
      // write all information that's not directly in the zone
      idManager->write_snapshot(f);
      OS::FWrite(useCount, idManager->n * sizeof(int32), f);
      OS::FWrite(&LRUtime, sizeof(LRUtime), f);
      iZone->write_snapshot(f);
      dZone->write_snapshot(f);
      sZone->write_snapshot(f);
      stubs->zone()->write_snapshot(f);
      ((CountStub*)NULL)->write_snapshot(f);
#   else // no compiler
      Unused(f);
      fatal("cannot snapshot code without compilers");
#   endif
  }
}

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "nmethodScopes.hh"

# include "_nmethodScopes.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


ScopeDesc* nmethodScopes::at(fint offset) {
  // Read the first byte and decode the ScopeDesc type at the location.
  assert(offset >= 0  &&  offset < length(), "illegal desc offset");
  scopeDescHeaderByte b;
  b.unpack(peek_next_char(offset));
  ScopeDesc* sd;
  switch (b.code()) {
   case METHOD_CODE:
    sd = new MethodScopeDesc(this, offset);
    break;
   case TOPLEVELBLOCK_CODE:
    sd = new TopLevelBlockScopeDesc(this, offset);
    break;
   case BLOCK_CODE:
    sd = new BlockScopeDesc(this, offset);
    break;
   case DEADBLOCK_CODE:
    sd = new DeadBlockScopeDesc(this, offset);
    break;
   case DATAACCESS_CODE:
    sd = new DataAccessScopeDesc(this, offset);
    break;
   case DATAASSIGNMENT_CODE:
    sd = new DataAssignmentScopeDesc(this, offset);
    break;
   default:
    fatal("Unknown ScopeDesc code in nmethodScopes");
  }
  if (!b.nameDescs()) {
    sd->_exprOffset = sd->_blockOffset = 
      sd->_next = sd->_name_desc_offset;
  }
  return sd;
}

int16  nmethodScopes::get_next_half(int32& offset) {
  int16 v;
  v = get_next_char(offset) << BYTE_WIDTH;
  v = addBits(v, get_next_char(offset));
  return v;
}

inline u_char nmethodScopes::getIndexAt(int32& offset) { 
  return get_next_char(offset);
}


inline oop nmethodScopes::unpackOopFromIndex(u_char index, int32& offset) {
  if (index == 0)  return 0;
  if (index < EXTENDED_INDEX) return oop_at(index-1);
  return oop_at(get_next_half(offset)-1); 
}

inline int32 nmethodScopes::unpackValueFromIndex(u_char index, int32& offset) {
  if (index <= MAX_INLINE_VALUE) return index;
  if (index < EXTENDED_INDEX) return value_at(index-(MAX_INLINE_VALUE+1));
  return value_at(get_next_half(offset)-(MAX_INLINE_VALUE+1)); 
}

oop nmethodScopes::unpackOopAt(int32& offset) {
  u_char index = getIndexAt(offset);
  return unpackOopFromIndex(index, offset);
}

int32 nmethodScopes::unpackValueAt(int32& offset) {
  u_char index = getIndexAt(offset);
  return unpackValueFromIndex(index, offset);
}

NameDesc* nmethodScopes::unpackNameDescAt(int32& offset) {
  int32 startOffset = offset;
  nameDescHeaderByte b;
  b.unpack(get_next_char(offset));
  NameDesc* nd;
  if (b.code() == ILLEGAL_CODE) {
    nd = new IllegalNameDesc;
  } else {
    int32 id = b.hasId() ? unpackValueAt(offset) : 0;
    u_char index;
    index = b.isIndexInlined() ? b.index() : getIndexAt(offset);

    switch(b.code()) {
     case LOCATION_CODE: {
       Location l = Location(unpackValueFromIndex(index, offset));
       nd = new LocationNameDesc(l, id);
       break;
     }
     case VALUE_CODE: {
       oop v = unpackOopFromIndex(index, offset);
       nd = new ValueNameDesc(v, id);
       break;
     }
     case BLOCKVALUE_CODE: {
       oop blk = unpackOopFromIndex(index, offset);
       nd = new BlockValueNameDesc(blk, id);
       break;
     }
     case MEMOIZEDBLOCK_CODE: {
       Location l   = (Location) unpackValueFromIndex(index, offset);
       oop      blk = unpackOopAt(offset);
       nd = new MemoizedBlockNameDesc(l, blk, id);
       break;
     }
     default:
      fatal("no such name desc");
    }
  }
  nd->offset = startOffset;
  nd->next   = offset;
  return nd;
}

# define FOR_EACH_OOPADDR(VAR)                                                \
    for (oop* VAR = oops(), *CONC(VAR, _end) = oops() + oops_size();          \
         VAR < CONC(VAR, _end); VAR++)

bool nmethodScopes::verify() {
  bool verify_result = true;
  // Verify all oops
  FOR_EACH_OOPADDR(addr) {
    VERIFY_TEMPLATE(addr)
  }

  // Verify all scopedesc
  FOR_EACH_SCOPE(this, s) {
    if (!s->verify()) {
      lprintf("\t\tin nmethod at %#lx (scopes)\n",
             (long unsigned)(my_nmethod())); 
      verify_result = false;
    }
  }
  return verify_result;
}

void nmethodScopes::scavenge_contents() {
  FOR_EACH_OOPADDR(addr) {
    SCAVENGE_TEMPLATE(addr);
  }
}

void nmethodScopes::gc_mark_contents() {
  FOR_EACH_OOPADDR(addr) {
    MARK_TEMPLATE(addr);
  }
}

void nmethodScopes::gc_unmark_contents() {
  FOR_EACH_OOPADDR(addr) {
    UNMARK_TEMPLATE(addr);
  }
}

void nmethodScopes::switch_pointers(oop from, oop to,
                                    nmethodBList *nmethods_to_invalidate) {
  Unused(to);
/*
  This is tricky!
  First, since some inlined methods are not included in scopes
  (those that generate no code such as asSmallInteger),
  you might think that this would
  not be needed, since memory is swept and dependencies flushed
  (see space::switch_pointers_in_region).

  But, when nmethods are converted on the stack, zombie nmethods are
  produced. These are obsolete nmethods that carry on the execution of
  active but no longer referenced methods on the stack.
  Since they have no dependencies, they are not found from the heap.
  That is why this code is needed anyway.

  Now, since this info describes the code, you cannot change it,
  instead, you invalidate the nmethod if it has a ref to from in its scopes.
  For example the method oops in the scope determine the activations
  that might be on the stack, and you can't change these, because
  activations are clones of methods. This may be the last reference to
  a currently executing method, must keep it around.

  However, you may be confused by the fact that locs (embedded literals)
  in the code in the method are changed. But, those are just object literals,
  maps of inlined stuff, and are different than the scope oops.

  Although they could possibly be the same, by invalidating any
  match (beyond just the method holder and method) we are safe.
  
  -- dmu 7/93
*/

  if (my_nmethod()->isInvalid()) return;
  
  FOR_EACH_OOPADDR(addr) {
    if (*addr == from) {
      nmethods_to_invalidate->append(my_nmethod());
      return;
    }
  }
}

void nmethodScopes::oops_do(oopsDoFn f) {
  FOR_EACH_OOPADDR(addr) {
    OOPS_DO_TEMPLATE(addr,f);
  }
}

void nmethodScopes::relocate() {
  FOR_EACH_OOPADDR(addr) {
    RELOCATE_TEMPLATE(addr);
  }
}

bool nmethodScopes::is_new() {
  FOR_EACH_OOPADDR(addr) {
    if ((*addr)->is_new()) return true;
  }
  return false;
}

void nmethodScopes::print() {
  ResourceMark m;       // in case methods get printed from gdb
  printIndent();
  lprintf("scopes:\n");
  Indent ++;
  FOR_EACH_SCOPE(this, d)
    d->print();
  Indent --;
}



# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "countStub_abstract.hh"
# pragma implementation "countStub.hh"
# include "_countStub.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

int32* sendCounts;      // counter array
fint nsendCounts;       // size of counter array


void shiftCounts(int32 delta) {
  // shift all count addresses by delta
  FOR_ALL_STUBS(s) {
    if (s->isCountStub()) ((CountStub*)s)->shift_count_addr(delta);
  }
  sendCounts = sendCounts + delta / sizeof(int32);
}

void countStub1_init() {
  ResourceMark rm;
  CountStub::pattern[NonCounting] = NULL;     // invalid pattern
  CountStub::pattern[Counting] = new CountCodePattern(Counting);
  CountStub::pattern[Comparing] = new CountCodePattern(Comparing);
  AgingStub::initPattern();
}

void countStub2_init() {
  ResourceMark rm;
  if (CountStub::idManager == NULL || !okToUseCodeFromSnapshot) {
    fint size1 = sizeof(CountStub) + CountStub::pattern[Counting]->instsSize;
    fint size2 = sizeof(CountStub) + CountStub::pattern[Comparing]->instsSize;
    nsendCounts = Memory->code->stubs->zone()->capacity() / size1;
    CountStub::idManager = new IDManager(nsendCounts, shiftCounts, CountStubIDStart);
    sendCounts = CountStub::idManager->data;
    fint blockSize = Memory->code->stubs->zone()->blockSize;
    fint frag1 = (100 * size1 % blockSize) / blockSize;
    fint frag2 = (100 * size2 % blockSize) / blockSize;
    const fint fragLimit = 20;
    if (frag1 >= fragLimit || frag2 >= fragLimit)
      warning2("bad count stub blocksize: %ld%%/%ld%% int. fragmentation",
               frag1, frag2);
  }
}


void CountStub::read_snapshot(FILE *f)
{
  assert(SnapshotCode, "otherwise shouldn't call");
  IDManager *idm= new IDManager(f, CountStubIDStart);
  if (okToUseCodeFromSnapshot) {
    idManager= idm;
    nsendCounts= idManager->n;
    sendCounts= idManager->data;
  }
}

void CountStub::write_snapshot(FILE *f)
{
  idManager->write_snapshot(f);
}


VtblPtr_t    CountingStub::_vtbl_value;
VtblPtr_t   ComparingStub::_vtbl_value;
VtblPtr_t       AgingStub::_vtbl_value;

int32 AgingStub::add_inst = 0;
CountCodePattern* CountStub::pattern[Comparing + 1];
IDManager* CountStub::idManager = NULL;

void* CountingStub::operator new(size_t size) {
  int32 newSize = size + CountStub::pattern[Counting]->instsSize;
  return Memory->code->stubs->allocate(newSize);
}

void* ComparingStub::operator new(size_t size) {
  int32 newSize = size + CountStub::pattern[Comparing]->instsSize;
  return Memory->code->stubs->allocate(newSize);
}

// see comment in cacheStub.c for missing operator delete


nmethod* CountStub::sender() {
  return nmethod::findNMethod(sender_sendDesc()); 
}

void CountStub::shift_count_addr(int32 delta) {
  CountCodePattern* patt = CountStub::pattern[countType()];
  set_count_addr(patt, count_addr(patt) + delta);
}

fint CountStub::id() {
  return (int32*)count_addr(pattern[countType()]) - idManager->data;
}

CountStub* CountStub::new_CountStub(nmethod* target, pc_t entry,
                                    nmln* sd_nmln, CountType t){
  CountStub* s;
  switch (t) {
   case Counting:   s = new CountingStub(target, entry, sd_nmln);
                    break;
   case Comparing:  s = new ComparingStub(target, entry, sd_nmln);
                    break;
   default:         fatal1("invalid count type %ld", t);
  }
  // NB: init is here (not in constructor) to get dynamic dispatch...sigh
  s->init(target);
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  s == (CountStub*)catchThisOne) warning("caught count stub");
# endif
  if (PrintCountStub) {
    lprintf("*creating "); s->print();
  }
  if (CountStub::idManager->isNearlyFull()) {
    CountStub::idManager->grow();
    if (PrintCountStub) {
      lprintf("*growing count ID table;"); CountStub::idManager->print();
    }
    // NB: it would be cleaner to grow the table automatically, but this
    // doesn't work because the id table overflows during the count stub
    // constructor, i.e. there is a count stub in the zone that is only
    // partially initialized, and thus we can't iterate through the zone
    // to change all counter addresses at that point.
  }
  s->set_count(1);
  return s;
}
  
CountStub::CountStub(nmethod* target, pc_t entryPoint,
                     nmln* sd_nmln, CountCodePattern* patt) {
  _instsLen = patt->instsSize;
  fint id = idManager->newID();
  sendCounts[id] = 0;
  copy_bytes(patt->pattern, insts(), _instsLen);
  set_count_addr(patt, (int32)&sendCounts[id]);
  set_callee(patt, (int32)entryPoint);
  MachineCache::flush_instruction_cache_range(insts(), insts() + patt->instsSize);
  MachineCache::flush_instruction_cache_for_debugging();
  if (sd_nmln) initSendDesc(sd_nmln);
  nmLink.init();
  nmLink.rebind(&target->linkedSends);
};

CountingStub::CountingStub(nmethod* nm, pc_t ep, nmln* sd_nmln)
  : CountStub(nm, ep, sd_nmln, CountStub::pattern[Counting]) {
  CHECK_VTBL_VALUE;
}

ComparingStub::ComparingStub(nmethod* nm, pc_t ep, nmln* sd_nmln)
  : CountStub(nm, ep, sd_nmln, CountStub::pattern[Comparing]) {
  CHECK_VTBL_VALUE;
}

AgingStub::AgingStub(nmethod* nm, pc_t ep, nmln* sd_nmln)
  : ComparingStub(nm, ep, sd_nmln) {
  CHECK_VTBL_VALUE;
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  this == (AgingStub*)catchThisOne) warning("caught aging stub");
# endif
  init(nm);     // must init here -- not called from new_CountStub
  if (PrintCountStub) {
    lprintf("*creating AgingStub %#lx, nm %#lx\n",
           (long unsigned)this, (long unsigned)nm);
  }
}

void CountStub::initSendDesc(nmln* sd_nmln) {
  assert(sd_nmln->isEmpty(), "should be empty");
  sdLink.init(sd_nmln);
}


// unlink the sdLink and reset jump addr if called from sendDesc
void CountStub::unlinkFromSendDesc() {
  sendDesc* sd = this->sd();
  if (sd) sd->reset_jump_addr();
  sdLink.remove();
}


void CountStub::forwardLinkedSend(nmln* l, nmethod* to)  {
  UsedOnlyInAssert(l);
  assert(l == &nmLink, "wrong link");
  if (to->isYoung()) {
    rebind(to);
    return;
  }
  // must unlink this stub, because target is now old
  sendDesc *sd= this->sd();
  if (sd) {
    sd->setCounting(NonCounting);
    sd->rebind(to);
  } else {
    CacheStub *pic= this->pic();
    assert(pic, "must have a sendDesc or a pic");
    pic->rebind(sdLink.next, to, NULL);
  }
  deallocate();
}


// rebind: change target to new nmethod
void CountStub::rebind(nmethod* nm, pc_t entryPoint) {
  if (entryPoint == NULL) entryPoint= nm->entryPointFor(sender_sendDesc());
  nmLink.rebind(&nm->linkedSends);
  set_callee((int32)entryPoint);
}

// change entry point as caller has changed (sendDesc to pic)
void CountStub::setVerifiedEntryPoint(nmethod *nmHint) {
  assert(nmHint == NULL || nmHint == target(), "wrong hint");
  nmethod *nm= nmHint ? nmHint : target();
  char *entryPoint= nm->verifiedEntryPoint();
  set_callee((int32)entryPoint);
}

void CountStub::moveTo_inner(NCodeBase* p, int32 delta, int32 /* size */ ) {
  CountStub* to= (CountStub*)p;
  sendDesc* sd= this->sd();
  if (sd) {
    sd->shift_jump_addr(delta);
  } else {
    pic()->shift_target(sdLink.next, delta);
  }
  sdLink.shift(delta);
  nmLink.shift(delta);
}

void CountStub::shift_target(nmln* l, int32 delta) {
  assert(l == &nmLink, "wrong link");  UsedOnlyInAssert(l);
  set_callee(int32(jump_addr() + delta));
  MachineCache::flush_instruction_cache_for_debugging();
}

NCodeBase* CountStub::unlink_me(nmln* l) {
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  this == (CountStub*)catchThisOne) warning("caught count stub");
# endif
  assert(l == &nmLink, "wrong link");
  UsedOnlyInAssert(l);
  CacheStub* s = pic();
  if (s) {
    s->unlink_me(sdLink.next);
  } else {
    sd()->unlink_countStub(this);
  }
  return NULL;
}

  
void CountStub::deallocate() {
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  this == (CountStub*)catchThisOne) warning("caught count stub");
# endif
  if (PrintCountStub) {
    lprintf("*deallocating "); print();
  }
  sendDesc* sd = this->sd();
  sdLink.remove();
  nmLink.remove();
  idManager->freeID(id());
  fint s = size();
  kill_vtbl_value();        // to make isCountStub fail on me
  Memory->code->stubs->deallocate(this, s);
  if (sd) sd->reset_jump_addr();
}

fint CountingStub::size() {
  return sizeof(CountStub) + CountStub::pattern[Counting]->instsSize; }
fint ComparingStub::size() {
  return sizeof(CountStub) + CountStub::pattern[Comparing]->instsSize; }

void CountStub::print() {
  lprintf("p (%s*)%#lx (%ld): nm %#lx, sd %#lx, count %ld\n",
         name(), this, long(id()), target(),
         sd(), sendCounts[id()]);
}

nmethod* CountStub::target() { return nmethod::findNMethod(jump_addr()); }

sendDesc* CountStub::sd() {
  if (Memory->code->contains(sdLink.next)) {
    return sendDesc::sendDesc_from_nmln(sdLink.next);
  } else {
    return NULL;
  }
}

CacheStub* CountStub::pic() {
  if (Memory->code->contains(sdLink.next)) {
    return NULL;
  } else {
    assert(sdLink.notEmpty(), "not linked to anything");
    return findCacheStub(sdLink.next);
  }
}

int32 CountStub::count() { return sendCounts[id()]; }
void CountStub::set_count(int32 count) { sendCounts[id()] = count; }

bool CountStub::verify2(CacheStub *calling_pic) {
  bool r = true;
  if (sdLink.length() != 2) {
    error1("CountStub %#lx: # of elements in sdLink not 2", this);
    r = false;
  }
  if (nmLink.isEmpty()) {
    error1("CountStub %#lx: not linked to any nmethod", this);
    r = false;
  }
  if (count() < 0) {
    error1("CountStub %#lx: sendCount is negative", this);
    r = false;
  }
  if (pic() != calling_pic) {
    error3("CountStub %#lx: error in PIC ref %#lx (should be %#lx)",
           this, calling_pic, pic());
    r = false;
  }
  sendDesc *s= calling_pic ? calling_pic->sd() : sd();
  if (!s->checkLookupTypeAndEntryPoint(target(), jump_addr())) {
    error2("CountStub %#lx: wrong lookup type or entry point", this, target());
    r = false;
  }
  return r;
}

bool CountStub::isCountStub(void* p) {
  VtblPtr_t vtbl = ((NCodeBase*)p)->vtbl_value();
  return vtbl == (( CountingStub*)NULL)->static_vtbl_value() ||
         vtbl == ((ComparingStub*)NULL)->static_vtbl_value() ||
         vtbl == ((    AgingStub*)NULL)->static_vtbl_value();
}

# if  GENERATE_DEBUGGING_AIDS
CountStub* findCountStub(void* addr) {
  NCodeBase* s = findThing(addr);
  assert(s->isCountStub(), "expecting a count stub");
  return (CountStub*)s;
}
#endif

IDManager::IDManager(fint N, IDOverflowHandler handler, caddr_t desiredAddress) {
  n = N; growHandler = handler;
  int size= N * sizeof(int32);
  data= (int32*)OS::allocate_idealized_page_aligned(size, "ID manager data", desiredAddress);
  init();
}

static char id_manager_delim[] = "\n\f\nan ID manager\n\f\n!%n";

IDManager::IDManager(FILE* f, caddr_t desiredAddress) {
  // return ID manager stored in snapshot
  check_delim(f, id_manager_delim);
  assert(SnapshotCode, "shouldn't call");
  OS::FRead_swap(this, sizeof(IDManager), f);
  growHandler= shiftCounts;
  int32 *old_data= data;
  int orig_size= n * sizeof(int32);
  int size= orig_size;
  if (okToUseCodeFromSnapshot) {
    data= (int32*)OS::allocate_idealized_page_aligned(size, "ID manager data", desiredAddress);
  }
  OS::read_or_seek(data, orig_size, f);
  if (okToUseCodeFromSnapshot && data != old_data)
    shiftCounts((char*)data - (char*)old_data);
}

IDManager::~IDManager() { selfs_free(data); }

void IDManager::init() {
  // free list: firstFree keeps first free index, data[firstFree] keeps index
  // of next free element, etc.
  firstFree = usedIDs = 0;
  for (fint i = 0; i < n; i++) data[i] = i + 1;
}

fint IDManager::newID() {
  fint id = firstFree;
  if (id >= n - 2) grow();
  firstFree = data[firstFree];
  usedIDs++;
  return id;
}

void IDManager::grow() {
  // the table is full; double its size
  LOG_EVENT2("growing IDManager %ld to %ld ids", this, n * 2);
  int32* newData = (int32*)AllocateHeap(2 * n * sizeof(int32),
                                        "IDManager table");
  copy_words(data, newData, n);
  for (fint i = n; i < 2 * n; i++) newData[i] = i + 1;
  int32 delta = (char*)newData - (char*)data;
  growHandler(delta);
  n *= 2;
  selfs_free(data);
  data = newData;
}

fint IDManager::peekID() {
  return firstFree;
}

void IDManager::freeID(fint id) {
  assert(id >= 0 && id < n && id != firstFree, "invalid ID");
  data[id] = firstFree;
  firstFree = id;
  usedIDs--;
  assert(usedIDs >= 0, "freed too many IDs");
}

void IDManager::print() {
  lprintf("IDManager %#lx: capacity %ld (%ld used)\n",
         this, n, usedIDs);
}

void IDManager::write_snapshot(FILE* f) {
  write_delim(f, id_manager_delim);
  OS::FWrite(this, sizeof(IDManager), f);
  OS::FWrite(data, n * sizeof(int32), f);
}

fint IDManager::read_snapshot(FILE* f) {
  // return ID manager stored in snapshot
  assert(SnapshotCode, "shouldn't call");
  check_delim(f, id_manager_delim);
  IDManager* idm = NEW_RESOURCE_OBJ(IDManager);
  // hack - don't call new to avoid executing constructor
  OS::FRead_swap(idm, sizeof(IDManager), f);

  if (   okToUseCodeFromSnapshot
      && (n != idm->n || data != idm->data))
    noCodeWarning("because of an address configuration mismatch");

  OS::read_or_seek(data, idm->n * sizeof(int32), f);
  if (okToUseCodeFromSnapshot) {
    usedIDs = idm->usedIDs;
    firstFree = idm->firstFree;
  }
  return idm->n;
}

bool IDManager::verify() {
  bool r = true;
  ResourceMark rm;
  fint prev = -1;
  bool* check = NEW_RESOURCE_ARRAY(bool, n);
  for (fint i = 0; i < n; i++) check[i] = false;
  fint id= firstFree;
  for (fint j = 0; j < n - usedIDs; id = data[id], j++) {
    if (id < 0 || id >= n) {
      error2("IDManager: invalid ID %ld in free list (#%ld)\n", id, j);
      r = false;
    }
    if (check[id]) {
      error2("IDManager: loop with ID %ld in free list (#%ld)\n", id, j);
      r = false;
    }
    check[id] = true;
    prev = id;
  }
  if (id != n) {
    error2("IDManager: wrong free list length, last = %ld, prev = %ld\n",
           id, prev);
    r = false;
  }
  return r;
}

// used only by broken code below
#ifdef UNUSED
static void insertCounter(nmethod* nm) {
  for (addrDesc *l = nm->locs(), *lend = nm->locsEnd(); l < lend; l++) {
    if (l->isSendDesc()) {
      l->asSendDesc(nm)->insertCounter();
      if (Memory->code->stubs->needsWork) Memory->code->stubs->cleanup();
    }
  }
}
  
static void removeCounter(nmethod* nm) {
  for (addrDesc *l = nm->locs(), *lend = nm->locsEnd(); l < lend; l++) {
    if (l->isSendDesc()) {
      l->asSendDesc(nm)->removeCounter();
    }
  }
}
#endif

# else  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void countStub1_init() {}
void countStub2_init() {}

# endif  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

// needed for primitive either way:
  
void setCountSends(bool newFlag) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    if (CountSends == newFlag) return;
    CountSends = newFlag;
#   ifdef broken
      // not fully debugged -- Urs 12/93
      if (newFlag) {
        Memory->code->nmethods_do(insertCounter);
      } else {
        Memory->code->nmethods_do(removeCounter);
      }
#   endif
# else
    Unused(newFlag);
# endif
}
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "ncode.hh"
# include "_ncode.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

void NCodeBase::moveTo(NCodeBase* to, int32 size) {
  int32 delta = (char*) to - (char*) this;
  moveTo_inner(to, delta, size);
  copy_words_overlapping(  (int32*)this,  (int32*)to,  size / sizeof(int32));
  // must use "to"; cannot use "this" anymore since there may be overlap
  if (to->isNMethod())
    ; // cannot do this simple check because moveTo_inner fixes up branches in place for nmethods()
  else if (CheckAssertions  ||  TARGET_ARCH == PPC_ARCH /* PPC has been buggy */ )
    check_branch_relocation( (char*)to->insts() - delta,  to->insts(),  instsLen() );

  MachineCache::flush_instruction_cache_range(to->insts(), to->instsEnd());
}
  

void OopNCode::moveTo_inner(NCodeBase* /*to*/, int32 delta, int32 /*size*/) {
  rememberLink.shift(delta);
}
  

void OopNCode::remember() {
  if (rememberLink.isEmpty()) Memory->code->rememberLink.add(&rememberLink);
}

// The LocChange stuff is necessary because several setHis can share
// a single add/or.

class LocChange {
 public:
  addrDesc* p;
  oop newOop;
};

bool OopNCode::scavenge_contents() {
  ResourceMark m;
  bool needToInvalICache = false;
  char* bound = Memory->new_gen->boundary();
  rememberLink.init();
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (p->isOop()) {
      oop oldOop = (oop)p->referent(this);
      oop newOop = oldOop->scavenge();
      if (newOop != oldOop) {
        changes[locLen].p = p;
        changes[locLen].newOop = newOop;
        locLen ++;
        needToInvalICache = true;
      }
      check_store(newOop, bound);
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)l->newOop);
  }
  return needToInvalICache;
}

void OopNCode::gc_mark_contents() {
  ResourceMark m;
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (!p->isOop()) {
      // no oops here
    } else {
      oop oldOop = (oop)p->referent(this);
      oop newOop = oldOop;
      MARK_TEMPLATE(&newOop);
      if (newOop != oldOop) {
        changes[locLen].p = p;
        changes[locLen].newOop = newOop;
        locLen++;
      }
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)l->newOop);
  }
}

bool OopNCode::gc_unmark_contents() {
  ResourceMark m;
  bool needToInvalICache = false;
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (!p->isOop()) {
      // no oops here
    } else {
      oop oldOop = (oop)p->referent(this);
      oop newOop = oldOop;
      UNMARK_TEMPLATE(&newOop);
      if (newOop != oldOop) {
        changes[locLen].p = p;
        changes[locLen].newOop = newOop;
        locLen ++;
        needToInvalICache = true;
      }
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)l->newOop);
  }
  return needToInvalICache;
}

void OopNCode::relocate() {
  ResourceMark m;
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (!p->isOop()) {
      // no oops here
    } else {
      oop oldOop = (oop)p->referent(this);
      oop newOop = oldOop;
      RELOCATE_TEMPLATE(&newOop);
      if (newOop != oldOop) {
        changes[locLen].p = p;
        changes[locLen].newOop = newOop;
        locLen ++;
      }
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)l->newOop);
  }
}

// go through any oops embedded in the actual machine code -- dmu 7/93

bool OopNCode::switch_pointers(oop from, oop to,
                               nmethodBList* nmethods_to_invalidate) {
  Unused(nmethods_to_invalidate);
  ResourceMark m;
  bool needToInvalICache = false;
  char* bound = Memory->new_gen->boundary();
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (!p->isOop()) {
      // no oops here
    } else {
      oop oldOop = (oop)p->referent(this);
      if (oldOop == from) {
        changes[locLen].p = p;
        locLen ++;
        check_store(to, bound);
        needToInvalICache = true;
      }
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)to);
  }
  return needToInvalICache;
}

bool OopNCode::code_oops_do(oopsDoFn f) {
  ResourceMark m;
  bool needToInvalICache = false;
  char* bound = Memory->new_gen->boundary();
  addrDesc* p = locs(), *end = locsEnd();
  LocChange* changes = NEW_RESOURCE_ARRAY( LocChange, end - p);
  int32 locLen = 0;
  for (; p < end; p++) {
    if (!p->isOop()) {
      // no oops here
    } else {
      oop oldOop = (oop)p->referent(this);
      oop newOop = oldOop;
      OOPS_DO_TEMPLATE(&newOop, f);
      if (newOop != oldOop) {
        changes[locLen].p = p;
        changes[locLen].newOop = newOop;
        locLen ++;
        check_store(newOop, bound);
        needToInvalICache = true;
      }
    }
  }
  for (LocChange* l = &changes[0]; locLen > 0; locLen--, l++) {
    l->p->set_referent(this, (char*)l->newOop);
  }
  return needToInvalICache;
}


bool NCodeBase::verify2(const char* name) {
  bool r = true;
  if ((int32)this & (oopSize - 1)) {
    error2("alignment error in %s at %#lx", name, this);
    r = false;
  }
  if (instsLen() > 256 * K) {
    error3("instr length of %s at %#lx seems too big (%ld)", name, this, instsLen());
    r = false;
  }
  return r;
}
  
bool OopNCode::verify() {
  bool r = true;
  const char* name = isNMethod() ? "nmethod" : (isCacheStub() ? " PIC" : "count stub");
  NCodeBase::verify2(name);
  if (!rememberLink.verify_list_integrity()) {
    lprintf("\tof rememberLink of %s %#lx\n", name, this);
    r = false;
  }
  
  for (addrDesc* l = locs(); l < locsEnd(); l++) {
    bool ok = isNMethod() ?
      l->verify((nmethod*)this) : l->verify((CacheStub*)this);
    if (! ok) {
      lprintf("\t\tin %s at %#lx\n", name, this);
      r = false;
    } else if (l->isOop() && // not no oops here
               oop(l->referent(this))->is_new() &&
               rememberLink.isEmpty()) {
      error2("%s %#lx should be remembered", name, this);
      r = false;
    }
  }
  return r;
}

NCodeBase* findThing(void* addr) {
  if (Memory->code->stubs->contains(addr)) {
    return findStub(addr);
  } else if (Memory->code->contains(addr)) {
    nmethod* n= nmethod_from_insts((char*)addr);
    assert(nmethod::isNMethod(n), "not a method");
    return n;
  } else {
    return NULL;
  }
}  


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "pcDesc.hh"
# include "_pcDesc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


void PcDesc::print(nmethod* c) {
  printIndent();
  lprintf("PcDesc 0x%lx: pc: 0x%lx; scope: %5ld; byte code: %ld\n",
         this,
         c->insts() + pc, scope, byteCode);
}

bool PcDesc::verify(nmethod* c) {
  Unused(c);
  return true;
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "regs.hh"
# include "_regs.cpp.incl"


void printLocation(Location l) {
  lprintf("%s", locationName(l));
}


Location LocationOfSavedOutgoingArgInSendee(fint argNo /* -1 for rcvr*/) {
  return argNo < 0  ?  IReceiverReg  :  IArgLocation(argNo);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

  
# pragma implementation "fields.hh"
# include "_fields.cpp.incl"

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "label.hh"
# pragma implementation "label_inline.hh"
# include "_label.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


fint currentLabelID = 0;

Label* Label::unify(Label* other) {
  if (this == NULL) return other;
  if (other == NULL) return this;
  if (isDefined()) {
    other->define(_target, this);
  } 
  else if (other->isDefined()) {
    define(other->target(), other);
  } 
  else {
    Label* l = this;
    while(l->next) l = l->next;
    l->next = other;
  }
  return this;
}

void Label::define() { define(theAssembler->addr()); }

void Label::define(char* t, Label* other) {
  _target = t;
  if (patch) theAssembler->Backpatch(patch, t);
  if (theAssembler->printing) {
    if (other) {
      lprintf("L%ld = L%ld \n", (void*)id(), (void*)other->id());
    } else if (t != theAssembler->addr()) {
      lprintf("L%ld = %#lx\n", (void*)id(), t);
    } else {
      lprintf("L%ld: \n", (void*)id());
    }
  }
  if (next) next->define(t, other);
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "disasm.hh"
# include "_disasm.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "asm.hh" 
# pragma implementation  "asm_abstract.hh" 
# pragma implementation  "asm_inline_abstract.hh" 
# pragma implementation  "asm_inline.hh" 
 
# include "_asm.cpp.incl"

 
# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


bool BaseAssembler::do_the_tests = false;
const char* BaseAssembler::test_file_c   = "asm_test.c";
const char* BaseAssembler::test_file_co  = "asm_test.o";
const char* BaseAssembler::test_file_s   = "asm_test_asm.s";
const char* BaseAssembler::test_file_so  = "asm_test_asm.o";
const char* BaseAssembler::test_file_out = "asm_test_out";


BaseAssembler::BaseAssembler(int32 instsSize, int32 locsSize,
                             bool pr, bool isMain) {
  instsStart    = instsEnd =  NEW_RESOURCE_ARRAY( char, instsSize);
  instsOverflow = instsStart + instsSize;
  locsStart = locsEnd = (addrDesc*) NEW_RESOURCE_ARRAY( char, locsSize);
  locsOverflow = (addrDesc*) ((pc_t) locsStart + locsSize);
  printing = pr; isInstructions = isMain;
  if (isMain) {
    assert(theAssembler == NULL, "forgot to reset theAssembler");
    theAssembler = (Assembler*)this;
    currentLabelID = 0;
  }
  assert(initLabels(), "init label verification");
  lastBackpatch = 0;
  
  lprintf_file = stderr;
}


void BaseAssembler::printX(int32 d, OperandType t, bool imm) {
  if (imm) {
    if (d < 0)  asm_lprintf("%d",  d); 
    else        asm_lprintf("%#x", d);
    return;
  }
  if (t == RegisterOperand)
    lprintf("%s", RegisterNames[d]);
  else print_disp(d, t);
}


void BaseAssembler::print_disp(int32 d, OperandType t) {
  switch (t) {
   case RegisterOperand:
    lprintf("");
    break;
   case NumberOperand:
    lprintf("#%ld", d);
    break;
   case OopOperand:
    lprintf("0x%lx", d);
    if (oop(d)->is_smi()) {
      lprintf(" (%ld)",  smiOop(d)->value());
    } else if (oop(d)->is_float()) {
      lprintf(" (%g)", floatOop(d)->value());
    } else if (oop(d)->is_string()) {
      lprintf(" ('");
      stringOop(d)->string_print();
      lprintf("')");
    }
    break;
   case VMAddressOperand:
    lprintf("0x%lx <%s>", d, getPrimName((pc_t)d));
    break;
   case PVMAddressOperand:
    lprintf("0x%lx <%s> (p)", d, getPrimName((pc_t)d));
    break;
   case BPVMAddressOperand:
    lprintf("0x%lx <%s> (bp)", d, getPrimName((pc_t)d));
    break;
   case CodeAddressOperand: {
    nmethod * nm = nmethod::nmethodContaining((pc_t) d, NULL);
    lprintf("0x%lx <%s>", d, nm->key.selector_string());
    break; }
   case DIVMAddressOperand:
    lprintf("0x%lx <%s> (di)", d, getPrimName((pc_t)d));
    break;
   default:
    ShouldNotReachHere();
  }
}


const char* OperandTypeNames[] = {
  "RegisterOperand",
  "NumberOperand",
  "OopOperand",
  "VMAddressOperand",
  "PVMAddressOperand",
  "CodeAddressOperand",
  "DIVMAddressOperand"
  };
  
  

void BaseAssembler::genLoc(int32 x) {
  assert(sizeof(addrDesc) == sizeof(int32), "change this code");
  int32* p = (int32*)locsEnd;
  *p = x;
  locsEnd++;
  if (locsEnd >= locsOverflow) fatal("routine is too long to compile");
}

void BaseAssembler::doAddOffset(OperandType t, bool isEmbedded,
                                int32 mask) {
  if (is_testing())  return; // use locs for testing
                                  
  mask |= offset();
  switch (t) {
   case    RegisterOperand:  break;
   case      NumberOperand:  break;
   case         OopOperand:  break;
   case   VMAddressOperand:  break;
   case  PVMAddressOperand:  mask |= addrDesc::isPrimitiveMask;  break;
   case BPVMAddressOperand:  mask |= addrDesc::isSendDescMask;   break;
   case CodeAddressOperand:  mask |= addrDesc::isSendDescMask;   break;
   case DIVMAddressOperand:  mask |= addrDesc::isDIDescMask;     break;
  }
  if (isEmbedded) {
    mask |= addrDesc::isEmbeddedMask;
  } 
  genLoc(mask);
}

# if GENERATE_DEBUGGING_AIDS  // for gdb convenience
void printAsm() { theAssembler->printing = true; }

bool BaseAssembler::verifyLabels() {
  for (fint i = labels->length() - 1; i >= 0; i--) {
    Label* l = labels->nth(i);
    if (l && l->patch && !l->isDefined()) {
      error1("Label %#lx: referenced but not defined", l);
    }
  }
  return true;
}
# endif

// Automated testing:

void BaseAssembler::test() {
  Assembler* a = new Assembler(
    10000000, 10000000, true, true);
  a->self_test();
}    

# include <stdarg.h>

void BaseAssembler::asm_lprintf(lprint_format_t msg, ...) {
  va_list ap;
  va_start(ap, msg);
  
  if (lprintf_file == stderr)    vlprintf(msg, ap);
  else                           vfprintf(lprintf_file, msg, ap);
  va_end(ap);
}


void BaseAssembler::self_test() {
  prepare_to_test(); // need to do .origin
  ((Assembler*)this)->generate_test_instructions();
  assemble_test_file();
  compare_results(get_assembled_bytes());
  finalize();
  printf("The assembler looks good.\n");
}


static const char *underscoreOrNot =
  TARGET_OS_VERSION == LINUX_VERSION  ?  ""  :  "_";

void BaseAssembler::prepare_to_test() {
  lprintf_file = fopen(test_file_s, "w");
  asm_lprintf(" /*.org %d*/\n .globl %sstart\n %sstart: \n", instsStart,
	underscoreOrNot, underscoreOrNot);
  FILE *f = fopen(test_file_c, "w");
  fprintf(f,
   "int main() {\n"
      "\t extern char start, end; \n"
      "\t write(1, &start, &end - &start); \n"
    "} \n");
  fclose(f);    
}

void BaseAssembler::assemble_test_file() {
  asm_lprintf(".globl %send\n %send: \n", underscoreOrNot, underscoreOrNot);
  fclose(lprintf_file);
  lprintf_file = stderr;
  char buf[1000];
  sprintf(buf, "cc -c %s; cc %s -c %s; cc %s %s; ./a.out>%s", 
    test_file_c,
    # if TARGET_OS_VERSION == LINUX_VERSION
	"",
    # else
	"-arch i386",
    # endif
    test_file_s, test_file_co, test_file_so, test_file_out);
  lprintf( "Testing assembler by running: %s\n", buf);
  int ret = system(buf);
  if(ret != 0) {
    fprintf(stderr, "Cannot run command `%s'", buf);
    exit(ret);
  }
}


char* BaseAssembler::get_assembled_bytes() {
  int fd = open(test_file_out, O_RDONLY, 0);
  if (fd == -1) {
    perror("open test file");
    fatal("testing");
  }
  test_file_size = lseek(fd, 0, SEEK_END); lseek(fd, 0, SEEK_SET);
  char* buf = new char[test_file_size];
  for (int i = 0, r = 0;  i < test_file_size; i += r) {
    r = read(fd, buf + i, test_file_size - i);
    if (r <= 0)  { perror("read"); fatal("read"); }
  }
  close(fd);
  return buf;
}

void BaseAssembler::compare_results(char* buf) {
  int n = min(instsLen(), test_file_size);
  for (int i = 0;  i < n;  ++i)
    if (buf[i] != instsStart[i]) {
      addrDesc* j;
      for ( j = locsStart;  j->offset() <= i;  ++j)
        if (j >= locsEnd)  fatal("could not find loc, perhaps forgot to tally() after test instruction");
      fatal5("assembler flunked at byte %d, instruction %d, started at %d\n"
      "x/i 0x%x\nx/i 0x%x", 
      i, j - locsStart - 1, j[-1].offset(), buf + j[-1].offset(), instsStart + j[-1].offset());
    }
  if (instsLen() != test_file_size)
    fatal2("Self assembler generated %d bytes, but Unix assembler generated %d bytes\n",
           instsLen(), test_file_size);
}

// override me
void BaseAssembler::generate_test_instructions() {fatal("no automated tests for this platform");}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "nameDesc.hh"
# include "_nameDesc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


void LocationNameDesc::print() {
  lprintf("@");
  printLocation(location());
  lprintf(" (%d)", (void*)offset);
}

void ValueNameDesc::print() {
  lprintf("=");
  value()->print_real_oop();
  lprintf(" (%d)", (void*)offset);
}

void BlockValueNameDesc::print() {
  lprintf("[=");
  value()->print_real_oop();
  lprintf("]");
  lprintf(" (%d)", (void*)offset);
}

void MemoizedBlockNameDesc::print() {
  lprintf("[@");
  printLocation(location());
  lprintf("=");
  value()->print_real_oop();
  lprintf("]");
  lprintf(" (%d)", (void*)offset);
}

void IllegalNameDesc::print() {
  lprintf("<illegal>");
  lprintf(" (%d)", (void*)offset);
}

# ifdef SIC_COMPILER
  ValueLocationNameDesc::ValueLocationNameDesc(Location l, oop val,
                                               blockOop blk) : NameDesc(0) {
    // callers pass in badOop for blk if not needed
    v = val; loc = l; block = blk;
    assert(!val->is_block() ||
           oop(blk) == badOop || val->map()->equal(blk->map()), "bad block");
  }

  void ValueLocationNameDesc::print() {
    lprintf("@");
    printLocation(location());
    lprintf("=");
    value()->print_real_oop();
    lprintf(" [%#lx] (%d)", block, (void*)offset);
  }
# endif


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "scopeDesc.hh"
# include "_scopeDesc.cpp.incl"


fint compareBCI(fint bci1, fint bci2) {
  const fint BIG = 99999999;
  assert(bci1 != IllegalBCI && bci2 != IllegalBCI, "can't compare");
  assert(PrologueBCI < 0, "change this code - expects negative PrologueBCI");
  if (bci1 == EpilogueBCI) bci1 = BIG;
  if (bci2 == EpilogueBCI) bci2 = BIG;
  return bci1 - bci2;
}


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


ScopeDesc::ScopeDesc(nmethodScopes* scopes, int32 offset) {
  _scopes     = scopes;
  _offset     = offset;

  _name_desc_offset = offset;

  scopeDescHeaderByte b;
  b.unpack(_scopes->get_next_char(_name_desc_offset));

  lite = b.lite();

  assert(offset != 0 || !lite, "Root scopeDesc cannot be lite");

  if (b.nameDescs()) {
     _exprOffset = _scopes->get_next_half(_name_desc_offset);
    _blockOffset = _scopes->get_next_half(_name_desc_offset);
    _next        = _scopes->get_next_half(_name_desc_offset);
  }
  if (_offset == 0) {
    key.set_from(scopes->my_nmethod()->key);

    senderScopeOffset    = 0;
    _senderByteCodeIndex = IllegalBCI;
  } else {
    key.lookupType= b.lookupIndex()
      ? getCommonLookupTypeAt(b.lookupIndex())
      : _scopes->unpackValueAt(_name_desc_offset);

    key.selector= _scopes->unpackOopAt(_name_desc_offset); 

    key.delegatee= needsDelegatee(key.lookupType)
      ? _scopes->unpackOopAt(_name_desc_offset) : 0;

    senderScopeOffset    = _scopes->unpackValueAt(_name_desc_offset);  
    _senderByteCodeIndex = _scopes->unpackValueAt(_name_desc_offset);
  }
}

bool ScopeDesc::s_equivalent(ScopeDesc* s) {
  return key.equivalent(s->key)
    && (   _senderByteCodeIndex == s->_senderByteCodeIndex
           // don't check senderByteCodeIndex for pseudo BCIs
        || _senderByteCodeIndex < 0
        || s->_senderByteCodeIndex < 0);
}

bool ScopeDesc::l_equivalent(simpleLookup* l) {
  return key.equivalent((ScopeLookupKey&)l->key);
}

ScopeDesc* ScopeDesc::sender() {
  return senderScopeOffset ? _scopes->at(_offset - senderScopeOffset) : NULL; 
}

NameDesc* ScopeDesc::nameDescAt(int32 offset) {
  if (offset < _next)
    return _scopes->unpackNameDescAt(offset);
  else
    return NULL;
}

void ScopeDesc::print() {
  printIndent();
  printName();
  lprintf("ScopeDesc (%d%s): ", offset(), is_lite() ? ", lite" : "");
  key.print();
  if (isCodeScope()) {
    lprintf(" (ID %ld)", long(scopeID()));
  }
  lprintf("\n");
  Indent ++;
  printIndent();
  printMethod();
  printMethodHolder();
  lprintf("\n");
  printSelf();

  if (isCodeScope()) {
    if (!is_lite()) {  
      print_slots();
    }
  } else {
    print_locals();
  }

  if (!is_lite() && isCodeScope()) {
    print_stack();
    print_blocks();
  }

  ScopeDesc* s = sender();
  if (s) {
    printIndent();
    lprintf("sender: (%d) @ %ld", s->offset(), long(senderByteCodeIndex()));
  }
  ScopeDesc* p = parent();
  if (p) {
    if (s) {
      lprintf("; ");
    } 
    else {
      printIndent();
    }
    lprintf("parent: (%d)", p->offset());
  }
  if (s || p) {
    lprintf("\n");
  }
  Indent --;
}


void ScopeDesc::print_slots() {
  NameDesc* nd = getNextNameDesc(NULL);
  FOR_EACH_SLOTDESC(method()->map(), sd) {
    if (sd->is_obj_slot() || sd->is_arg_slot()) {
      assert( nd, "name desc is missing");
      printIndent();
      lprintf("slot \'");
      sd->printAugmentedName();
      lprintf("\'  ");
      nd->print();
      lprintf("\n");
      nd = getNextNameDesc(nd);
    }     
  }
}


void ScopeDesc::print_locals() {
  FOR_EACH_LOCAL_NAME_DESC(this, nd) {
    printIndent();
    lprintf("slot ");
    nd->print();
    lprintf("\n");
  }
}


void ScopeDesc::print_stack() {
  fint i = 0;
  nmethod* nm = this->_scopes->my_nmethod();
  bool debugMode = nm->isDebug();
  methodMap* mm= (methodMap*) method()->map();
  IntList* exprStackBCIList= mm->expression_stack_bcis(debugMode);
  IntListElem* e = exprStackBCIList->head();
  FOR_EACH_STACK_EXPR(this, ed) {
    printIndent();
    lprintf("expr stack %2d (bci %2d):", i++, e->data());
    ed->print();
    lprintf("\n");
    e = e->next();
  }
}


void ScopeDesc::print_blocks() {
  fint i = 0;
  methodMap* mm= (methodMap*) method()->map();
  IntList* blockBCIList= mm->all_blocks();
  IntListElem* e = blockBCIList->head();
  FOR_EACH_BLOCK(this, ed) {
    printIndent();
    lprintf("block %2d (bci %2d):", i++, e->data());
    ed->print();
    lprintf("\n");
    e = e->next();
  }
}


void ScopeDesc::doForNames(fint bci, nameDescStringDoFn fn) {
  // for disassembler: call fn on any name vaild at bci
  // quite inefficient, but used only for debugging
  if (isCodeScope()) {
    if (!is_lite()) {  
      NameDesc* nd = getNextNameDesc(NULL);
      FOR_EACH_SLOTDESC(method()->map(), sd) {
        if (sd->is_obj_slot() || sd->is_arg_slot()) {
          assert( nd, "name desc is missing");
          fn(nd, sd->name->copy_null_terminated());
          nd = getNextNameDesc(nd);
        }     
      }
      if (bci >= 0) {
        methodMap* mm = method_map();
        IntList* es = mm->expression_stack(bci, true);
        IntList* bl = mm->blocks_upto(bci);
        for (fint i = 0; i < mm->length_codes(); i++) {
          if (es->nonEmpty() && es->first() == i) {
            es->removeHead();
            NameDesc* n_d = exprStackElem(i);
            char buf[80];  sprintf(buf, "stk %d", i);  fn(n_d, buf);
          } 
          if (bl->nonEmpty() && bl->first() == i) {
            bl->removeHead();
            NameDesc* n_d = blockElem(i);
            char buf[80];  sprintf(buf, "block %d", i);  fn(n_d, buf);
          }
        }
      }
    }
  } else {
    // don't bother for access scopes
  }
}

bool ScopeDesc::verify() {
  // verifies mostly structure, not contents
  bool ok = true;

  if (!is_lite()) {
    FOR_EACH_LOCAL_NAME_DESC(this, n) {
      if (!n->verify()) {
        lprintf("in name desc %ld of ScopeDesc %#lx", long(n->offset),
               (long unsigned)this);
        ok = false;
      }
    }

    FOR_EACH_STACK_EXPR(this, e) {
      if (!e->verify()) {
        lprintf("in expr desc %ld of ScopeDesc %#lx", long(e->offset),
               (long unsigned)this);
        ok = false;
      }
    }
    FOR_EACH_BLOCK(this, b) {
      if (!b->verify()) {
        lprintf("in block desc %ld of ScopeDesc %#lx", long(b->offset),
               (long unsigned)this);
        ok = false;
      }
    }
  }

  // don't do a full verify of parent/sender -- too many redundant verifies
  ScopeDesc* s = sender();
  if (s && !s->shallow_verify()) {
    lprintf("invalid sender %#lx of ScopeDesc %#lx", s,
           (long unsigned)this);
    ok = false;
  }
  ScopeDesc* p = parent();
  if (p && !p->shallow_verify()) {
    lprintf("invalid parent %#lx of ScopeDesc %#lx", p,
           (long unsigned)this);
    ok = false;
  }
  return ok;
}

CodeScopeDesc::CodeScopeDesc(nmethodScopes* scopes, int32 offset)
  : ScopeDesc(scopes, offset) {
  _method              = _scopes->unpackOopAt(_name_desc_offset); 
  _scopeID             = _scopes->unpackValueAt(_name_desc_offset);  
}

bool CodeScopeDesc::s_equivalent(ScopeDesc* s) {
  if (s->isCodeScope() && ScopeDesc::s_equivalent(s)) {
    CodeScopeDesc* sc = (CodeScopeDesc*)s;
    // compare methods, not rcvr maps because of block map cloning
    return _method == sc->_method;
  } else {
    return false;
  }
}

bool CodeScopeDesc::l_equivalent(simpleLookup* l) {
  return ScopeDesc::l_equivalent(l)
                 && l->result()->as_real()->desc->data == _method;
}

NameDesc* CodeScopeDesc::slot(stringOop name, bool canFail) {
  if (name == VMString[SELF]) return self();

  NameDesc* nd = getNextNameDesc(NULL);
  FOR_EACH_SLOTDESC(method()->map(), s) {
    if (s->is_vm_slot()) continue;
    if (s->is_map_slot()) {
      if (s->name == name) return new ValueNameDesc(s->data);
    } else {
      if (s->name == name) return nd;
      nd = getNextNameDesc(nd);
    }     
  }
  if (!canFail) ShouldNotReachHere(); // slot not found
  return NULL;
}


// -----------------------------------------------------------


class trivialExprStackElemFinder: public abstract_interpreter {
 public:
  CodeScopeDesc* csd;
  NameDesc* result;
  
  trivialExprStackElemFinder( oop meth, CodeScopeDesc* c, int32 bci) 
   : abstract_interpreter(meth ) {  pc = bci;  csd= c;  result =NULL; }
   
  void do_SELF_CODE() { result= csd->self(); }
  void do_READ_LOCAL_CODE();
  void do_WRITE_LOCAL_CODE()  { result= csd->self(); }
  
  void do_IMPLICIT_SEND_CODE();
  void do_LITERAL_CODE();
};


void trivialExprStackElemFinder::do_READ_LOCAL_CODE() {
    if ( pc > 0  
    &&   getOp(mi.codes[pc - 1]) == LEXICAL_LEVEL_CODE)
      is.lexical_level = mi.map()->get_index_at(pc - 1);
    
  // slot() below does not handle slots in an enclosing scope
  if (is.lexical_level > 0) 
    return;
    
  is.index = mi.map()->get_index_at(pc);
    
  slotDesc* s= mi.map()->getLocalSlot(is.lexical_level, is.index);
  if (s->is_arg_slot()) {
    // don't need to duplicate arg slot name descs
    // return name desc of local slot
    result= csd->slot(s->name);
  }
}


void trivialExprStackElemFinder::do_IMPLICIT_SEND_CODE() {
  if ( UseLocalAccessBytecodes)
    return;
    
  is.index= mi.map()->get_index_at(pc); 
  stringOop sel= get_selector();
  slotDesc* s= mi.map()->find_slot(sel);
  if ( s &&  s->is_arg_slot() ) {
    // don't need to duplicate arg slot name descs
    // return name desc of local slot
    result= csd->slot(sel);
  }
}


void trivialExprStackElemFinder::do_LITERAL_CODE() {
  is.index= mi.map()->get_index_at(pc);
  oop lit= get_literal();
  if ( !lit->has_code()  &&  !lit->is_block_with_code()) {
    // make up a nameDesc for this constant
    result= new ValueNameDesc(lit);
  }
}


NameDesc* CodeScopeDesc::exprStackElem(int32 bci, bool includeTrivial) {
  // Find NameDesc describing result of byte code bci.
  // If includeTrivial is set, fill in the information for trivial bcs
  // (e.g. literals) which isn't actually stored; see
  // methodMap::expression_stack_bcis.

  bool debugMode = _scopes->my_nmethod()->isDebug();
  oop m = method();
  methodMap* mm = (methodMap*) m->map();
    
  // exprStackBCIList contains the bcis who are described in the scopeDesc
  // so if we find our pc there, we can use its NameDesc
  IntList* exprStackBCIList = mm->expression_stack_bcis(debugMode);
  NameDesc* nd = getNextExprDesc(NULL);
  for (IntListElem* e = exprStackBCIList->head(); e; e = e->next()) {
    if (e->data() == bci) {
      // assert(!nd->isIllegal(), "shouldn't be accessing illegal name desc");
      // don't break too eagerly - e.g. disassembler may access illegals
      return nd;
    }
    nd = getNextExprDesc(nd);
  }

  // haven't found it in the ScopeDesc, so it must be a trivial byte code
  if (!includeTrivial) return NULL;
  
  trivialExprStackElemFinder esf(m, this, bci);
  esf.interpret_bytecode();
  NameDesc* r = esf.result;
  assert(r, "did not find name desc");
  return r;
}

NameDesc* CodeScopeDesc::blockElem(int32 bci) {
  // Find NameDesc describing block cloned at bci.
  // methodMap::all_blocks gives possible bci's

  oop m = method();
  methodMap* mm = (methodMap*) m->map();
    
  IntList* blockBCIList = mm->all_blocks();
  NameDesc* nd = getNextBlockDesc(NULL);
  for (IntListElem* e = blockBCIList->head();  e;  e = e->next()) {
    if (e->data() == bci) {
      // assert(!nd->isIllegal(), "shouldn't be accessing illegal name desc");
      // don't break too eagerly - e.g. disassembler may access illegals
      return nd;
    }
    nd = getNextBlockDesc(nd);
  }
  fatal("did not find block");
  return NULL;
}

  
// ----------------------------------------------------------------

void CodeScopeDesc::printMethod() {
  lprintf("method: 0x%lx", (method()));
}

bool MethodScopeDesc::s_equivalent(ScopeDesc* s) {
  return s->isMethodScope() && CodeScopeDesc::s_equivalent(s);
  // don't check self_type or method holder
}

bool MethodScopeDesc::l_equivalent(simpleLookup* l) {
  // xxx miw
  return CodeScopeDesc::l_equivalent(l) && selfMapOop()->equal(l->receiverMapOop());
  // note: don't check _methodHolder_or_map == l->methodHolder because l->mh is
  // the sending mh, not the receiving mh
}

MethodScopeDesc::MethodScopeDesc(nmethodScopes* scopes, 
                                 int32 offset)
  : CodeScopeDesc(scopes, offset) {
  self_name         = _scopes->unpackNameDescAt(_name_desc_offset);
  self_type         = _scopes->unpackOopAt(_name_desc_offset);  
  _methodHolder_or_map     = _scopes->unpackOopAt(_name_desc_offset);  
}

void MethodScopeDesc::printName() {
  lprintf("Method");
}

void MethodScopeDesc::printSelf() {
  printIndent();
  lprintf("self: ");
  self()->print();
  lprintf("; self type: ");
  self_type->print_real_oop();
  lprintf("\n");
}

void MethodScopeDesc::printMethodHolder() {
  lprintf("; method holder: ");
  _methodHolder_or_map->print_real_oop();
  bool b = _methodHolder_or_map->is_map();
  if (b) {
    lprintf(" (same as receiver)");
  }
}


TopLevelBlockScopeDesc::TopLevelBlockScopeDesc(nmethodScopes* scopes,
                                               int32 offset)
  : LexicalScopeDesc(scopes, offset) {
  self_name         = _scopes->unpackNameDescAt(_name_desc_offset);
  cachedSelf_name   = _scopes->unpackNameDescAt(_name_desc_offset);
  self_type         = _scopes->unpackOopAt(_name_desc_offset);  
  _methodHolder_or_map     = _scopes->unpackOopAt(_name_desc_offset);  
  blockName         = _scopes->unpackNameDescAt(_name_desc_offset);
  blockName_cached  = _scopes->unpackNameDescAt(_name_desc_offset);
  _receiverMapOop   = (mapOop)_scopes->unpackOopAt(_name_desc_offset);
  assert_map(receiverMapOop(), "should be a map");
}

void TopLevelBlockScopeDesc::printMethodHolder() {
  lprintf("; method holder: ");
  _methodHolder_or_map->print_real_oop();
  bool b = _methodHolder_or_map->is_map();
  if (b) {
    lprintf(" (same as receiver)");
  }
}

void LexicalScopeDesc::printSelf() {
  printIndent();
}

void TopLevelBlockScopeDesc::printSelf() {
  LexicalScopeDesc::printSelf(); 
  lprintf("self: ");
  self()->print();
  lprintf("; self type: ");
  self_type->print_real_oop();
  lprintf("; rcvr type: ");
  receiverMapOop()->print_real_oop();
  lprintf("\n");
}

void BlockScopeDesc::printSelf() {
  LexicalScopeDesc::printSelf();
  lprintf("; rcvr type: ");
  receiverMapOop()->print_real_oop();
  lprintf("\n");
}

bool TopLevelBlockScopeDesc::s_equivalent(ScopeDesc* s) {
  // programming can tranform a nested block to a top-level block
  return s->isBlockScope() && CodeScopeDesc::s_equivalent(s);
}

void TopLevelBlockScopeDesc::printName() {
  lprintf("TopLevelBlock");
}


BlockScopeDesc::BlockScopeDesc(nmethodScopes* scopes, int32 offset)
  : LexicalScopeDesc(scopes, offset) {
  parentScopeOffset = _scopes->unpackValueAt(_name_desc_offset);
  blockName         = _scopes->unpackNameDescAt(_name_desc_offset);
  _receiverMapOop   = (mapOop)_scopes->unpackOopAt(_name_desc_offset);
  assert_map(receiverMapOop(), "must be a map");
}

bool BlockScopeDesc::s_equivalent(ScopeDesc* s) {
  return s->isBlockScope() && LexicalScopeDesc::s_equivalent(s);
}

void BlockScopeDesc::printName() {
  lprintf("Block");
}

ScopeDesc* BlockScopeDesc::parent() {
  return parentScopeOffset ? _scopes->at(_offset - parentScopeOffset) : NULL; 
}


DeadBlockScopeDesc::DeadBlockScopeDesc(nmethodScopes* scopes,
                                       int32 offset)
  : CodeScopeDesc(scopes, offset) {
  blockName         = _scopes->unpackNameDescAt(_name_desc_offset);
}

void DeadBlockScopeDesc::printMethodHolder() {
  // none
}

void DeadBlockScopeDesc::printSelf() {
  printIndent();
  lprintf("block: ");
  block()->print();
  lprintf("\n");
}

bool DeadBlockScopeDesc::s_equivalent(ScopeDesc* s) {
  return s->isBlockScope() && CodeScopeDesc::s_equivalent(s);
}

void DeadBlockScopeDesc::printName() {
  lprintf("DeadBlock");
}


NameDesc* AccessScopeDesc::slot(stringOop name, bool canFail) {
  Unused(name); Unused(canFail);
  assert(name == VMString[SELF], "can only access the self slot");
  return self();
}

void AccessScopeDesc::printMethodHolder() {
  lprintf("method holder: ");
  _methodHolder_or_map->print_real_oop();
  bool b = _methodHolder_or_map->is_map();
  if (b) {
    lprintf(" (same as receiver)");
  }
}

AccessScopeDesc::AccessScopeDesc(nmethodScopes* scopes, int32 offset)
  : ScopeDesc(scopes, offset) {
  self_name     = _scopes->unpackNameDescAt(_name_desc_offset);
  self_type     = _scopes->unpackOopAt(_name_desc_offset);  
  _methodHolder_or_map = _scopes->unpackOopAt(_name_desc_offset); 
}

bool AccessScopeDesc::l_equivalent(simpleLookup* l) {
  // xxx miw
  return ScopeDesc::l_equivalent(l) && selfMapOop()->equal(l->receiverMapOop());
  // _methodHolder_or_map != l->methodHolder; see above
}

bool DataAccessScopeDesc::s_equivalent(ScopeDesc* s) {
  return s->isDataAccessScope() && AccessScopeDesc::s_equivalent(s);
}

void DataAccessScopeDesc::printName() {
  lprintf("DataAccess");
}

bool DataAssignmentScopeDesc::s_equivalent(ScopeDesc* s) {
  return s->isDataAssignmentScope() && AccessScopeDesc::s_equivalent(s);
}

void DataAssignmentScopeDesc::printName() {
  lprintf("DataAssignment");
}


# ifdef SIC_COMPILER
  static SExpr* getSExpr(oop self_type) {
    if (self_type->is_map()) {
      if (self_type == Memory->true_mapOop()) {
        return new ConstantSExpr(Memory->trueObj, NULL, NULL);
      } else if (self_type == Memory->false_mapOop()) {
        return new ConstantSExpr(Memory->falseObj, NULL, NULL);
      } else {
        return new MapSExpr(mapOop(self_type), NULL, NULL);
      }
    } else {
      return new ConstantSExpr(self_type, NULL, NULL);
    }
  }
  
  SExpr* MethodScopeDesc::selfExpr() { return getSExpr(self_type); }
  SExpr* TopLevelBlockScopeDesc::selfExpr() { return getSExpr(self_type); }
  SExpr* AccessScopeDesc::selfExpr() { return getSExpr(self_type); }

  SExpr* TopLevelBlockScopeDesc::rcvrExpr() {
    nmethod* nm = this->_scopes->my_nmethod();
    return getSExpr(nm->reusable()
                    ? mapOop(badOop) : nm->key.receiverMapOop()); // xxx miw
  }
  SExpr* BlockScopeDesc::rcvrExpr() {
    return getSExpr(receiverMapOop());
  }
# endif

static mapOop getSelfMap(oop self_type) {
  if (self_type->is_map()) {
    return (mapOop)self_type;
  } else {
    return self_type->map()->enclosing_mapOop();
  }
}
  
mapOop MethodScopeDesc::selfMapOop() { return getSelfMap(self_type); }
mapOop TopLevelBlockScopeDesc::selfMapOop() { return getSelfMap(self_type); }
mapOop AccessScopeDesc::selfMapOop() { return getSelfMap(self_type); }


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "diDesc.hh"
# include "_diDesc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)



void DIDesc::unlink_me() {
  dependency()->remove();
  set_jump_addr(Memory->code->trapdoors->SendDIMessage_stub_td());
  MachineCache::flush_instruction_cache_for_debugging();
}

void DIDesc::print() {
  printIndent();
  lprintf("DIDesc 0x%lx\n", this);
  Indent ++;
  printIndent();
  lprintf("addr: 0x%lx; dependency: ", jump_addr());
  dependency()->print();
  lprintf("\n");
  Indent --;
}


pc_t SendDIMessage(sendDesc* sd, frame* lookupFrame, DIDesc* dc,
                    int32 verified, oop receiver, oop arg1) {
  Unused(verified);                   
  NMethodLookupKey& k =
    nmethod::findNMethod(nmethod_from_insts((pc_t)dc))->key;
  assert(k.methodHolder_or_map() != MH_TBD,
         "should be a real method holder");
  NumberOfDILookups++;
  return dc->sendMessage(lookupFrame,
                         receiver, 
                         k.selector,
                         k.delegatee,
                         sd,
                         arg1);
}


static nmethod* SendDIMessage_cont( compilingLookup* L ) {
  if ( Interpret ) {
    L->perform_full_lookup();
    return NULL;
  }
  return L->di_desc()->lookup_compile_and_backpatch(L);
}


pc_t DIDesc::sendMessage( frame* lookupFrame,
                           oop receiver,
                           oop selector,
                           oop delegatee,
                           sendDesc* sd,
                           oop arg1 ) {
  ShowLookupInMonitor sl;
  sd->sendMessagePrologue( receiver, lookupFrame );

  ResourceMark m;
  FlushRegisterWindows(); // for vframe conversion below
  compilingLookup L( receiver,
                     selector,
                     delegatee,
                     MH_TBD,  // method holder
                     new_vframe(lookupFrame),
                     sd,
                     this,
                     false ); // don't want a debug version

  nmethod* nm = switchToVMStack(SendDIMessage_cont,  &L);
  if (SilentTrace) LOG_EVENT1("DIDesc::sendMessage: found %#lx", nm);

  return 
    Interpret
    ? L.interpretResultForCompiledSender(arg1)
    : nm->insts();
}


nmethod* DIDesc::lookup_compile_and_backpatch( compilingLookup* L ) {

  nmethod* nm= L->lookupNMethod();
  set_jump_addr(nm->insts());
  return nm;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "simpleLookup.hh"
# pragma implementation "simpleLookup_inline.hh"
# include "_simpleLookup.cpp.incl"


void simpleLookup::assert_static_selector() {
  assert(!isPerform(), "selector should be static"); }

void simpleLookup::assert_static_delegatee() {
  assert(isDelegateeStatic(), "delegatee should be static"); }


static bool isLookupErrorSelector(oop sel) {
  return
    sel == VMString[UNDEFINEDSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_] ||
    sel == VMString[AMBIGUOUSSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_] ||
    sel == VMString[MISSINGPARENTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_] ||
    sel == VMString[MISMATCHEDARGUMENTCOUNTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_] ||
    sel == VMString[PERFORMTYPEERRORSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
}


simpleLookup::simpleLookup(LookupType type,
                           oop rcvr,
                           oop sel,
                           oop dgt,
                           oop mhOrMap, 
                           dependencyList *dps,
                           assignableDependencyList* adps ) {
  key.lookupType= type;
  key.selector= sel;
  key.delegatee= dgt;
  key.set_methodHolder_or_map( mhOrMap );

  assert(rcvr->verify_oop(), "");
  if (rcvr->is_map()) {
    // all we know is map
    receiver= badOop;
    key._receiverMapOop= mapOop(rcvr);
  } else {
    receiver= rcvr;
    key._receiverMapOop= rcvr->map()->enclosing_mapOop();
  }
  status = foundNone;
  _result = NULL;

  deps = dps;
  adeps = adps;
}


void simpleLookup::add_asg_parent_dependency( objectLookupTarget* otarg,
                                              objectLookupTarget* ntarg,
                                              slotDesc* sd) {
  if ( adeps == NULL ) return;
  
  if (otarg->links == EMPTY)  adeps->add(otarg);
  otarg->links = otarg->links->add(sd, ntarg);
}



// check for perform errors and return status, or foundNone

LookupStatus simpleLookup::perform_error_status(int32 perform_arg_count) {
  
  // check selector first
  if (!isPerformLookupType(lookupType()))
    ; // static is cool
  else if (!selector()->is_string() || !stringOop(selector())->is_slot_name()) 
    return performTypeError;
  else if (arg_count() != perform_arg_count)
    return mismatchedArgCount;

  // check delegatee
  if ( baseLookupType(lookupType()) != DirectedResendBaseLookupType )
    ; // no delegatee needed, cool
  else if (isDelegateeStatic())
    ; // static delegatee, cool
  else if (    !delegatee()->is_string()
           ||  !stringOop(delegatee())->is_slot_name())
    return performTypeError;

  return foundNone;
}


void simpleLookup::perform_full_lookup_n(int32 perform_arg_count) {

  status= perform_error_status(perform_arg_count);
  if (status != foundNone) {
    lookupErrorCode(perform_arg_count);
  } else {
    assert_methodHolder_is_object();
    perform_lookup();
    if (result() == NULL)
      lookupErrorCode(perform_arg_count);
  }
}


void simpleLookup::perform_lookup() {
  lookupTarget* t = create_initial_target();

  if ( t == NULL ) // check for errors
    return;
  
  if ( baseLookupType(lookupType()) == ResendBaseLookupType )
    // special case for undirected resends!!
    parentsLookup(t);
  else
    objectLookup(t);
}


lookupTarget* simpleLookup::create_initial_target() {
  switch (baseLookupType(lookupType())) {
   case        NormalBaseLookupType:  return isImplicitSelf()
                                              ?  implicit_self_lookup_target()
                                              :       receiver_lookup_target();

   case      DelegatedBaseLookupType: return         delegated_lookup_target();
   case         ResendBaseLookupType: return undirected_resend_lookup_target();
   case DirectedResendBaseLookupType: return   directed_resend_lookup_target();

   default:
    ShouldNotReachHere(); // unexpected base lookup type
    return NULL;
  }
}


lookupTarget* simpleLookup::receiver_lookup_target() {
  return (new objectLookupTarget(receiver))->be_receiver();
}
      

lookupTarget* simpleLookup::delegated_lookup_target() {
  return new objectLookupTarget(delegatee());
}


lookupTarget* simpleLookup::undirected_resend_lookup_target() {
  assert_methodHolder_is_object();
  return new objectLookupTarget(methodHolder_or_map());
}


lookupTarget* simpleLookup::directed_resend_lookup_target() {
  assert_string(delegatee(), "should be a string");
  Map* rm= methodHolder_map();
  slotDesc* desc= rm->find_slot(stringOop(delegatee()));
  add_dependency(desc, rm);

  if (desc == NULL || desc->is_vm_slot()) {
    // didn't find parent to delegate through
    status = delegateeNotFound;
    return NULL;
  }
  if ( ! check_slot_for_directed_resend(desc) )
    return NULL;
  
  oop delegatee= methodHolder_or_map()->get_slot(desc);
  return new objectLookupTarget(delegatee);
}


bool simpleLookup::check_slot_for_directed_resend(slotDesc* desc){
  Unused(desc);
  return true;
}


bool simpleLookup::objectLookup(lookupTarget* target) {
  if (target->check_cycle_mark()) {
    // we've been here before -- break out of the cycle
    return false;
  }
  
  assert_string(selector(), "should be a string");
  slotDesc* desc= target->map()->find_slot(stringOop(selector()));
  target->add_dependency(desc, this);
  
  if (!desc || desc->is_vm_slot()) {          // no matching slot
    target->set_cycle_mark();
    bool found = parentsLookup(target);
    target->clear_cycle_mark();
    return found;
  }
  
  realSlotRef* slot = new realSlotRef(target, desc);

  if (result() != NULL  &&  slot->EQsr(result()->as_real())) {
    // found same slot, no change
    return true;
  }
  
  switch (status) {
   case foundNone:
    status = foundOne;
    _result = slot;
    break;
    
   case foundOne:
    status = foundSeveral;
    _result = NULL;
    break;
    
   case foundSeveral:
    ShouldNotReachHere();
    
   case foundAssignableParent:
   case resendUndecidable:
    _result = NULL;
    break;
    
   case delegateeNotFound:
   case mismatchedArgCount:
   case performTypeError:
   default:
    ShouldNotReachHere();
  }
  return true;
}



bool simpleLookup::parentsLookup(lookupTarget* target) {
  bool found = false;
  
  target->add_slot_dependency(this);
  
  FOR_EACH_SLOTDESC(target->map(), s) {
    if (! s->is_parent()) {
      continue;
    }

    target->add_dependency(s, this);    // record a dependency
    
    lookupTarget* parent = target->get_target_for_slot(s, this);
    if (parent == NULL) {
      status = foundAssignableParent;       // can only occur at compile-time
      _result = NULL;
      return true;
    }
    
    found |= objectLookup(parent);
    
    switch (status) {
     case foundNone:
     case foundOne:
      continue;
      
     case foundSeveral:
     case foundAssignableParent:
     case resendUndecidable:
      return true;      // lookup has failed along this path, so quit search
      
     case delegateeNotFound:
     case mismatchedArgCount:
     case performTypeError:
     default:
      ShouldNotReachHere(); // bad lookup status
    }
  }
  return found;
}



// make error method, and change lookup to point to it

void simpleLookup::lookupErrorCode(int32 perform_arg_count) {
  LOG_EVENT1("lookup error %#lx", selector());
  if (isLookupErrorSelector(selector())) {
    handleRecursiveLookupError();
  }
  breakpoint(); // for debugging

  generateLookupErrorMethod(perform_arg_count);
}



void simpleLookup::handleRecursiveLookupError() {
  // avoid infinite recursion
  lprintf("A lookup error happened while sending the message\n\t");

  frame *f= currentProcess->last_self_frame(false);
  abstract_vframe* v= new_vframe(f);
  oop failSel= v->selector();

  if (failSel->is_string()) {
    stringOop(failSel)->string_print();
  } else {
    failSel->print_oop();
  }  
  
  lprintf("\nto\n\t");
 
  v->receiver()->print_oop();

  lprintf(".\nSubsequently, the lookup error message\n\t");
  stringOop(this->selector())->string_print();

  lprintf("\nwas sent to\n\t");
  receiver->print_oop();

  lprintf(",\nand was also not understood, causing the process to be aborted"
         " by the Self VM.\n\n");
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions) {
    breakpoint();
  }
# endif

  currentProcess->stack()->print();
  
  remove_all_deps();
  Process::abort_process();
}
  

void simpleLookup::selectorAndSourceForLookupError(stringOop& sel,
                                                   const char*& source) {
  switch (status) {

   case foundNone:
    sel = VMString[UNDEFINEDSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"undefined selector error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case foundSeveral:
    sel = VMString[AMBIGUOUSSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"ambiguous selector error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case delegateeNotFound:
    sel = VMString[MISSINGPARENTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"missing parent error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case foundOne:
    ShouldNotReachHere(); // failing w/o an error condition

   case foundAssignableParent:
   case resendUndecidable:
    ShouldNotReachHere(); // run-time lookup failing w/ a compile-time condition

   case mismatchedArgCount:
    sel = VMString[MISMATCHEDARGUMENTCOUNTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"mismatched argument count error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case performTypeError:
    sel = VMString[PERFORMTYPEERRORSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"perform type error;\nthis method was automatically generated by the VM.\"\n";
    break;

   default:
    ShouldNotReachHere(); // unknown status
  }
}


stringOop simpleLookup::messageTypeForLookupError() {
  switch (baseLookupType(lookupType())) {
   case NormalBaseLookupType:
     return VMString[isImplicitSelf() ? IMPLICITSELF : NORMAL];
   case ResendBaseLookupType:
     return VMString[RESEND];
   case DirectedResendBaseLookupType:
     return VMString[DIRECTEDRESEND];
   case DelegatedBaseLookupType:
     return VMString[DELEGATED];
   default:
     ShouldNotReachHere();  // bad lookup type
     return NULL;
  }
}


int32 simpleLookup::argCountForLookupError(int32 perform_arg_count) {
  int32 argc;

  if (isPerform()) {
    assert(perform_arg_count >= 0,
           "should have a static selector or a perform arg count");
    argc= perform_arg_count;
  } else {
    assert_string(selector(), "should be a string if static");
    argc= stringOop(selector())->arg_count();
  }
  assert(argc >= 0 && argc < 100, "bad arg count");
  return argc;
}


void simpleLookup::generateLookupErrorMethod(int32 perform_arg_count) {
  stringOop sel;
  const char* source;
  selectorAndSourceForLookupError(sel, source);
  stringOop msgType = messageTypeForLookupError();
  int32 argc = argCountForLookupError(perform_arg_count);

  // Create a method sending the lookup error message to the
  // current process.
  ByteCode b(true);
  slotList* slots = EMPTY;

  // push the receiver of the method send
  b.GenSendByteCode(0, 0, new_string("_ThisProcess"), true, false, NULL);

  // push the arguments
  
  // First push the selector. Must put it in a slot because
  // for a perform type error, it might be a block with an uplevel access.
  // That confuses the fixup code, which expects block literals to be lexically part of this method.
  // So, replaced   b.GenLiteralByteCode(0, 0, selector()); 
  // with: (dmu 1/03)
  stringOop attemptedSelector = new_string("selector");
  slots = slots->add( attemptedSelector, map_slotType, selector());
  b.GenSendByteCode(0, 0, attemptedSelector, true, false, NULL);

  b.GenSelfByteCode(0, 0);
  
  b.GenLiteralByteCode(0, 0, msgType);
  
  b.GenLiteralByteCode(0, 0,
                       delegatee() == NULL ? Memory->nilObj : delegatee());
  
  assert_methodHolder_is_object();
  assert(!methodHolder_or_map()->has_code(),
         "method holder shouldn't have code");
  
  stringOop del = new_string("delegatee");
  slots = slots->add( del, map_slotType,
                      isResendLookupType(lookupType())
                      ? methodHolder_or_map() : Memory->nilObj);
  b.GenSendByteCode(0, 0, del, true, false, NULL);
  
  // create vector to hold args: "vector _Clone: arc _FillingWith: nil"
  b.GenLiteralByteCode(0, 0, Memory->objVectorObj);
  b.GenLiteralByteCode(0, 0, as_smiOop(argc));
  b.GenLiteralByteCode(0, 0, Memory->nilObj);
  b.GenSendByteCode(0, 0, VMString[_CLONE_FILLER_], false, false, NULL);
  
  // also cons up arg names, add arg slots, get args into stack
  for (fint i = 0; i < argc; i++) {
    // make argNameOop
    char argName[20];
    sprintf(argName, "arg%ld", (long)i + 1);
    stringOop arg = new_string(argName);
    
    // make slot and store arg in vector
    slots = slots->add(arg, arg_slotType, as_smiOop(i));
    
    // <vector> at: i Put: <arg>
    b.GenLiteralByteCode(0, 0, as_smiOop(i));
    b.GenSendByteCode(0, 0, arg, true, false, NULL);
    b.GenSendByteCode(0, 0, VMString[_AT_PUT_], false, false, NULL);
  }
  
  // send the message to the receiver.
  b.GenSendByteCode(0, 0, sel, false, false, NULL);
  
  // NB: don't change <error> below without consulting recompile.c;
  // the code there uses it to recognize these weird methods.  Should
  // really be a flag somewhere
  bool ok = b.Finish("<error>", source);
  assert(ok, "no errors here");

  slotsOop method= create_outerMethod(slots, &b);
  setResult(method);
}


void simpleLookup::print() {
  printIndent();
  lprintf("simpleLookup: status = %s\n", lookupStatusString(status));
  printIndent();
  lprintf("rcvMap = 0x%lx, mh = ",
         receiverMapOop());
  if (methodHolder_or_map() == MH_NOT_A_RESEND) {
    lprintf("<not a resend>");
  } else {
    methodHolder_or_map()->print_real_oop();
  }
  lprintf(",\n");
  printIndent();
  lprintf("           sel = ");
  selector()->print_real_oop();
  lprintf(", del = ");
  delegatee()->print_real_oop();
  lprintf(",\n");
  printIndent();
  lprintf("           type = ");
  printLookupType(lookupType());
  lprintf("\n");
}


const char* lookupStatusString(LookupStatus status) {
  switch (status) {
   case foundNone:                    return "not found";
   case foundOne:                     return "found one";
   case foundSeveral:                 return "message ambiguous";
   case foundAssignableParent:        return "found assignable parent";
   case resendUndecidable:            return "resend undecidable";
   case delegateeNotFound:            return "delegatee not found";
   case mismatchedArgCount:           return "mismatched arg count";
   case performTypeError:             return "selector isn't a string";
   default:
    fatal("unknown lookup failure status");
  }
  return NULL;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "key.hh"
# include "_key.cpp.incl"


const char* selector_string(oop selector) {
  if (selector->is_string()) {
    return stringOop(selector)->copy_null_terminated();
  } else {
    return "<not a string>";
  }
}


void ScopeLookupKey::set_from(ScopeLookupKey &k) {
  lookupType= k.lookupType;
  selector= k.selector;
  delegatee= k.delegatee;
}


void ScopeLookupKey::print() {
  if (delegatee != NULL) {
    if (delegatee->is_string()) {
      stringOop(delegatee)->string_print();
    } else {
      delegatee->print_real_oop();
    }
    lprintf(".");
  }
  if (selector->is_string()) {
    stringOop(selector)->string_print();
  } else {
    selector->print_real_oop();
  }
}


void MethodLookupKey::set_from(MethodLookupKey &k) {
  ScopeLookupKey::set_from(k);
  _receiverMapOop=         k._receiverMapOop;
  _methodHolder_or_map=   k._methodHolder_or_map;
}


int32 MethodLookupKey::hash() {
  int32 i;

  // Coded for speed - called for every codeTable lookup

  // XOR the identity-hash values of the fields.  For efficiency, just
  // xor the marks and extract hash bits at the end instead of doing
  // this for every field.
  // Also, don't need to check for no_hash: real keys are guaranteed
  // to have an id hash value because init_hash is called before a
  // key is added to the table.  If a lookup key contains oops with
  // no_hash, we'll have a bogus hash value, but we won't find anything
  // anyway.
  i  = (int32)receiverMapOop()->mark();
  if (selector->is_mem())
    i ^= (int32)memOop(selector)->mark();

  // Can't do the optimization for the methodHolder - it might be
  // e.g. a smiOop in the lookupError glue method.
  
  // Not non-zero often enough to hash on
  // (an optimization for speed; Bay's idea)
  //  i ^= (int32)delegatee             ->mark();

  return   (lookupType & ~ImplicitSelfBit)
         ^ markOop(i)->hash()
         ^ methodHolder_or_map()->identity_hash();
}


void MethodLookupKey::init_hash() {
  receiverMapOop()      ->identity_hash();
  methodHolder_or_map()->identity_hash();
  selector              ->identity_hash();
  delegatee             ->identity_hash();
}


void MethodLookupKey::print() {
  printIndent();
  lprintf("MethodLookupKey: rcvMap = 0x%lx, mh = ",
         (long unsigned)receiverMapOop());
  if (methodHolder_or_map() == MH_TBD) {
    lprintf("<to be determined>");
  } else {
    methodHolder_or_map()->print_real_oop();
  }
  lprintf(",\n");
  printIndent();
  lprintf("           sel = ");
  selector->print_real_oop();
  lprintf(", del = ");
  delegatee->print_real_oop();
  lprintf(",\n");
  printIndent();
  lprintf("           type = ");
  printLookupType(lookupType);
  lprintf("\n");
}



void NMethodLookupKey::relocate() {
  RELOCATE_TEMPLATE(&_receiverMapOop);
  RELOCATE_TEMPLATE(&_methodHolder_or_map);
  RELOCATE_TEMPLATE(&selector);
  RELOCATE_TEMPLATE(&delegatee);
}


bool NMethodLookupKey::verify() {
  bool flag = true;
  if (!oop(receiverMapOop())->verify_oop()) {
    lprintf("\tin receiverMap of NMethodLookupKey 0x%lx\n", this);
    flag = false;
  } else if (!receiverMapOop()->is_map()) {
    error1("receiverMapOop() 0x%lx isn't a map", receiverMapOop());
    flag = false;
  }
  if ( methodHolder_or_map() != MH_TBD 
  &&  !methodHolder_or_map()->verify_oop()) {
    lprintf("\tin methodHolder of NMethodLookupKey 0x%lx\n", this);
    flag = false;
  }
  if (selector) {
    if (!selector->verify_oop()) {
      lprintf("\tin selector of NMethodLookupKey 0x%lx\n", this);
      flag = false;
      /* uncommon but legal
      } else if (!selector->is_string()) {
        lprintf("warning: selector ");
        selector->print_oop();
        lprintf(" isn't a string\n");
        flag = false;
        */
      }
  }
  if (delegatee) {
    if (!delegatee->verify_oop()) {
      lprintf("\tin delegatee of NMethodLookupKey 0x%lx\n", this);
      flag = false;
    } else {
      BaseLookupType l = baseLookupType(lookupType);
      if (l == DirectedResendBaseLookupType &&
          ! delegatee->is_string()) {
        error1("delegatee 0x%lx isn't a string", delegatee);
        flag = false;
      }
    }
  }
  return flag;
}


void NMethodLookupKey::scavenge_contents() {
  SCAVENGE_TEMPLATE(&_receiverMapOop);
  SCAVENGE_TEMPLATE(&_methodHolder_or_map);
  SCAVENGE_TEMPLATE(&selector);
  SCAVENGE_TEMPLATE(&delegatee);
}


void NMethodLookupKey::gc_mark_contents() {
  MARK_TEMPLATE(&_receiverMapOop);
  MARK_TEMPLATE(&_methodHolder_or_map);
  MARK_TEMPLATE(&selector);
  MARK_TEMPLATE(&delegatee);
}

void NMethodLookupKey::gc_unmark_contents() {
  UNMARK_TEMPLATE(&_receiverMapOop);
  UNMARK_TEMPLATE(&_methodHolder_or_map);
  UNMARK_TEMPLATE(&selector);
  UNMARK_TEMPLATE(&delegatee);
}

void NMethodLookupKey::switch_pointers(oop from, oop to) {
  SWITCH_POINTERS_TEMPLATE(&_receiverMapOop);
  SWITCH_POINTERS_TEMPLATE(&_methodHolder_or_map);
  SWITCH_POINTERS_TEMPLATE(&selector);
  SWITCH_POINTERS_TEMPLATE(&delegatee);
}

void NMethodLookupKey::oops_do(oopsDoFn f) {
  OOPS_DO_TEMPLATE(&_receiverMapOop,f);
  OOPS_DO_TEMPLATE(&_methodHolder_or_map,f);
  OOPS_DO_TEMPLATE(&selector,f);
  OOPS_DO_TEMPLATE(&delegatee,f);
}


/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotRef.hh"
# include "_slotRef.cpp.incl"

void nonexistentSlotRef::print_short() {
  lprintf("nonexistentSlotRef 0x%lx", (unsigned long) this);
}

void nonexistentSlotRef::print() {
  printIndent();
  lprintf("contents: 0x%lx\n", contents());
  contents()->print();
}

void realSlotRef::print_short() {
  lprintf("realSlotRef 0x%lx", (unsigned long)this);
}

void realSlotRef::print() {
  holder->print();
  printIndent();
  lprintf("desc: 0x%lx (", desc);
  desc->name->string_print();
  lprintf(")\n");
}

void counterfactualSlotRef::print_short() {
  lprintf("counterfactualSlotRef 0x%lx", (unsigned long) this);
}

void counterfactualSlotRef::print() {
  lprintf("contents: 0x%lx\n", (unsigned long)contents());
  lprintf("methodHolder: 0x%lx\n", (unsigned long)_methodHolder_or_map);
  contents()->print();
  _methodHolder_or_map->print();
}

oop realSlotRef::contents() {
  return
    desc->is_map_slot()
    ? desc->data
    : holder->get_slot(desc);
}


void realSlotRef::set_contents(oop x) {
  assert(desc->is_obj_slot(), "must be object slot to assign");
  holder->set_slot(desc, x);
}


ResultType realSlotRef::resultType(oop sel) {
  if (desc->is_arg_slot())  return dataResult;
  if (desc->is_map_slot()) {
    if (desc->data->has_code()) {
      // not sure why this is here, compiler uses it (dmu)
      ((objectLookupTarget*) holder)->value_constrained = true;
      return methodResult;
    }
    return constantResult;
  }
  if (stringOop(sel)->is_1arg_keyword())  return assignmentResult;
  else                                    return dataResult;
}

// common routine used when mh obj may not be same as rcvr

oop realSlotRef::methodHolder_if_not_rcvr() {
  return holder->is_object()
    ?  ((objectLookupTarget*)holder)->obj
    :  MH_TBD; // not known statically
}

oop realSlotRef::methodHolder_or_map(oop rcvr) {
  // holder same as receiver, not necessarily same object
  return holder->is_receiver() 
    ?  rcvr
    :  methodHolder_if_not_rcvr();
}


// optimizes for objects with methods in them by generalizing mh to map
// see key.h

oop realSlotRef::generalized_methodHolder_or_map(oop rcvr) {
  Unused(rcvr);
  // holder same as receiver, not necessarily same object
  return holder->is_receiver() 
    ?  holder->map()->enclosing_mapOop()
    :  methodHolder_if_not_rcvr();
}


bool abstractSlotRef::EQsr(abstractSlotRef* s) {
  return as_real()->EQsr(s->as_real());
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

// returns entry point

char* abstractSlotRef::interpretForCompiledSender(oop receiver, oop sel,
                                                  oop arg1 ) {
  oop res = interpretData( receiver, sel, arg1 );
  if ( res != badOop ) {
    ReturnResult_stub_result = res;
    return first_inst_addr(ReturnResult_stub);
  } else {
    sneaky_method_argument_to_interpret = contents();
    return first_inst_addr(interpret_from_compiled_sender);
  }
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)



// handles data & assgn slots

oop abstractSlotRef::interpretData(oop receiver, oop sel, oop arg1) {
  switch (resultType(sel)) {

   case constantResult:
   case dataResult:
    return contents();

   case assignmentResult:
    set_contents(arg1);
    return receiver;

   case methodResult:
    return badOop;

   default:
    fatal("should never happen");
    return NULL;
  }
}


// caller must preserve args!

oop abstractSlotRef::interpret( oop receiver, oop sel, oop del,
                                oop* argp, int32 nargs) {
  oop res = interpretData( receiver, sel, *argp );
  if (res != badOop) return res;
  res = ::interpret( receiver,
                     sel,
                     del,
                     contents(),
                     methodHolder_or_map(receiver),
                     argp,
                     nargs);
  return res;
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "target.hh"
# include "_target.cpp.incl"

lookupTarget* objectLookupTarget::get_target_for_slot(slotDesc* s,
                                                      simpleLookup* L) {
  if ( s->is_map_slot() )
    return new objectLookupTarget( obj->get_slot(s));
  
  if (L->is_for_inlining())
    return NULL;

  objectLookupTarget* target =
    new objectLookupTarget(obj->get_slot(s));

  target->prevTargetSlot = new realSlotRef(this, s);

  L->add_asg_parent_dependency(this, target, s);

  return target;
}

lookupTarget* mapLookupTarget::get_target_for_slot(slotDesc* s,
                                                   simpleLookup* L) {
  Unused(L);
  if ( s->is_map_slot() )
    return new objectLookupTarget( s->data);
  
  return NULL;
}
    
  
lookupTarget*
 vframeLookupTarget::get_target_for_slot(slotDesc* s, simpleLookup* L) {
  Unused(L);
  if (s->name == VMString[LEXICAL_PARENT]) {
    return new vframeLookupTarget(vf->parent(), receiver);
  } else if (s->name == VMString[SELF]) {
    return (new objectLookupTarget(receiver)) -> be_receiver();
  } else {
    assert(s->is_map_slot(), "sorry, can't handle assignable local parents");
    oop p = vf->get_slot(s);
    assert(p != badOop, "sorry, local slot not located");
    return new objectLookupTarget(p);
  }
}

# ifdef SIC_COMPILER
  
  lookupTarget* sicScopeLookupTarget::get_target_for_slot(slotDesc* s,
                                                          simpleLookup* L) {
    Unused(L);
    if (s->name == VMString[LEXICAL_PARENT]) {
      return new sicScopeLookupTarget(scope->parent());
    } else if (s->name == VMString[SELF]) {
      SExpr* t = scope->receiverExpr();
      if (t->isConstantSExpr()) {
        return (new objectLookupTarget(t->constant())) -> be_receiver();
      } else if (t->hasMap()) {
        return ( new mapLookupTarget(t->map())) -> be_receiver();
      } else {
        // don't know the receiver type
        return NULL;
      }
    } else if (! s->is_map_slot()) {
      return NULL;
    } else {
      return new objectLookupTarget(s->data);
    }
  }
# endif


# ifdef SIC_COMPILER
  sicScopeLookupTarget::sicScopeLookupTarget(SScope* s)
  : lookupTarget(s->method()->map()) {
    scope = s;
  }
# endif

assignableSlotLink* assignableSlotLink::add(slotDesc* s,
                                            objectLookupTarget* t) {
  assignableSlotLink* e = new assignableSlotLink(s, t);
  if (this == NULL) return e;
  assignableSlotLink *l, *n;
  for (l = this, n = l->next;
       n;
       l = n, n = l->next)
    ;
  l->next = e;
  return this;
}




// just printing from here on down


void assignableSlotLink::print() {
  printIndent();
  lprintf("{ ");
  bool first = true;
  Indent ++;
  for (assignableSlotLink* l = this; l; l = l->next) {
    if (first) {
      first = false;
    } else {
      lprintf(",\n");
      printIndent();
    }
    lprintf("desc 0x%lx (", (l->slot));
    l->slot->name->string_print();
    lprintf(") -> 0x%lx", (unsigned long)(l->target));
  }
  Indent --;
  lprintf(" }\n");
}

void objectLookupTarget::print() {
  printIndent();
  print_short();
  lprintf(" 0x%lx: ", this);
  obj->print_real_oop();
  lprintf(" (map: 0x%lx)\n", map());
  Indent ++;
  if (prevTargetSlot) {
    printIndent();
    lprintf("prevTarget:\n");
    Indent ++;
    prevTargetSlot->print();
    Indent --;
  }
  if (links) {
    links->print();
  }
  printIndent();
  lprintf("valueConstrained: %s\n", value_constrained ? "yes" : "no");
  Indent --;
}

void objectLookupTarget::print_short() {
  lprintf("objectLookupTarget");
}


void mapLookupTarget::print() {
  printIndent();
  lprintf("mapLookupTarget 0x%lx: map: 0x%lx\n",
         this, map());
}

void vframeLookupTarget::print() {
  printIndent();
  lprintf("vframeLookupTarget 0x%lx: ", this);
  receiver->print_real_oop();
  lprintf(" (map: 0x%lx)\n", map());
  Indent ++;
  vf->print_frame(0);
  Indent --;
}

# ifdef SIC_COMPILER
  void sicScopeLookupTarget::print() {
    printIndent();
    lprintf("sicScopeLookupTarget %#lx: map: 0x%lx scope %#lx\n",
           (long unsigned)this, (long unsigned)map(), (long unsigned)scope);
  }
# endif

oop objectLookupTarget::get_slot(slotDesc* sd) {
  return obj->get_slot(sd);
}

void objectLookupTarget::set_slot(slotDesc* sd, oop x) {
  obj->set_slot(sd, x);
}

oop vframeLookupTarget::get_slot(slotDesc* sd) {
  return vf->get_slot(sd);
}

void vframeLookupTarget::set_slot(slotDesc* sd, oop x) {
  vf->set_slot(sd, x);
}
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "complexLookup.hh"
# include "_complexLookup.cpp.incl"


vframeLookup::vframeLookup( LookupType l,
                            oop rcvr,
                            oop sel,
                            oop dgt,
                            oop smhOrMap,
                            abstract_vframe* f,
                            dependencyList* dps,
                            assignableDependencyList *adps )
 : simpleLookup( l, rcvr, sel, dgt, smhOrMap, dps, adps ) {

  sendingVFrame= f;

  if (f == NULL) return;

  if (!isResendLookupType(lookupType())) {
    key.set_methodHolder_or_map( MH_NOT_A_RESEND);
    return;
  }
  oop vfmh= sendingVFrame->methodHolder_or_map();
  if (!vfmh->is_map()) {
    key.set_methodHolder_or_map( vfmh);
    return;
  }
  // method holder is the same as self; replace it now
  if (smhOrMap == MH_TBD) {
    // don't have run-time smh, so compute it (e.g. for conversions)
    key.set_methodHolder_or_map( sendingVFrame->methodHolder_object() );
    return;
  }
  // just use smh
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      oop smh2 = sendingVFrame->methodHolder_object();
      assert(smhOrMap == smh2,
             "runtime sending method holder should be correct");
      /* this is not always true with programming
         mapOop map2 = smh2->map()->enclosing_mapOop();
         assert(vfmh == map2, "should be the same map");
      */
    }
# endif
}


lookupTarget* vframeLookup::directed_resend_lookup_target() {
  assert_methodHolder_is_object();
  return simpleLookup::directed_resend_lookup_target();
}


lookupTarget* vframeLookup::implicit_self_lookup_target() {
  return new vframeLookupTarget(sendingVFrame, receiver);
}



# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

compilingLookup::compilingLookup(oop rcvr,
                                 oop sel,
                                 oop dgt,
                                 oop smhOrMap,
                                 abstract_vframe* f,
                                 sendDesc *s,
                                 DIDesc *d,
                                 bool debug,
                                 bool c)
  : vframeLookup( s->lookupType(), rcvr, sel, dgt, smhOrMap, f,
                  new           dependencyList,
                  new assignableDependencyList) {
  sd= s;
  dc= d;
  needDebug= debug;
  canReuseNM= c && ReuseNICMethods; }



// Compile an nmethod
nmethod* compilingLookup::lookupNMethod() {
  if (result() == NULL)                                       
    perform_full_lookup();

# if GENERATE_DEBUGGING_AIDS
    if ( dc != NULL  &&  status == foundOne )
      assert_methodHolder_is_object();
# endif

  chooseCompiler();
  nmethod* nm= compileOrReuse();
  if (Trace) trace(nm);
  return nm;
}


void compilingLookup::chooseCompiler() {
  if (mustUseNIC()) {  compiler = NIC;  return;  }
  if (mustUseSIC()) {  compiler = SIC;  return;  }

  # ifdef SIC_COMPILER
    if (needDebug) {
      compiler= NIC;
      return;
    }
    compiler= status == foundOne ? currentCompiler() : compilers[0];
    if (sd->isOptimized() && allowedToRecompile()) compiler = SIC;
    if (isCriticalMessage(selector()) &&
        nstages > 1 && compilers[1] == SIC) {
      compiler = SIC;             // for better recompilation
    }
    if (currentProcess->isSingleStepping()) compiler = NIC;

    // don't bother using the SIC for access methods - the NIC compiles
    // faster and the code is almost as good (and if speed really counts,
    // the access should be inlined anyway)
    if (compiler == SIC  &&  resultType() != methodResult) {
      compiler = NIC;
    }
  
    // Implement count features for debugging:
    // Increment count if are or would be using SIC
    // Then if min or max are not == -1 (-1 means ignore), and
    //  count is not bounded by min/max, use NIC instead.
    // Note: count is incremented BEFORE a Sic compile
    if (compiler == SIC) {
      ++SICCompilationCount;
      if ((SICMinCompilationCount != -1  &&  SICMinCompilationCount > SICCompilationCount)
      ||  (SICMaxCompilationCount != -1  &&  SICMaxCompilationCount < SICCompilationCount))
        compiler = NIC;
    }
  # endif // SIC_COMPILER
}


nmethod* compilingLookup::compileOrReuse() {
  nmethod *nm= compileNMethod();
  assert(nm->compiler() == compiler, "wrong compiler!");
  return nm;
}


nmethod* compilingLookup::compileNMethod() {
  nmethod* other;
  nmln* diDeps = NULL;
  
  if (dc != NULL) {
    diDeps = dc->dependency();
    assert(diDeps->isEmpty(), "should be empty");
    // make sure that other method won't be flushed from the zone
    other = nmethod::findNMethod(dc);
    other->save_unlinked_frame_chain();
  }

  nmethod* nm = doCompile(diDeps);

  if (dc != NULL) {
    assert(diDeps->notEmpty(), "should be rebound now");
    other->unlink_saved_frame_chain();
  }
  return nm;
}


nmethod* compilingLookup::doCompile(nmln* diLink) {
  BlockProfilerTicks ex(exclude_compile);
  
  if (compiler == NIC) {
    FCompiler* fc= new FCompiler(this, sd, diLink);
    fc->generateDebugCode= needDebug || currentProcess->isSingleStepping();
    activeCompiler= fc;

#   ifdef SIC_COMPILER
  } else if (compiler == SIC) {
      activeCompiler= new SICompiler(this, sd, diLink);
#   endif

  } else {
    fatal("invalid compiler setting");
  }
  nmethod* nm = activeCompiler->compile();
  activeCompiler->finalize();
  activeCompiler = NULL;
  return nm;
}


bool compilingLookup::isCompilerForced() {
  return CompileWithSICNames->is_objVector();
}

bool compilingLookup::isSelectorInCompileWithSICNames(oop selector) {
  objVectorOop names = (objVectorOop)CompileWithSICNames;
  for (fint i = names->length() - 1; i >= 0; --i) {
    if (selector == names->obj_at(i)) {
      lprintf("\n\n<<< SIC-compiling \"%s\" (Reason: CompileWithSICNames) >>>\n\n",
              stringOop(selector)->copy_null_terminated());
      return true;
    }
  }
  return false;
}


bool compilingLookup::isMethodOrBlockInCompileWithSICNames() {
  if (!CompileWithSICNames->is_objVector())
    return false;

  if (resultType() != methodResult)
    return false;

  oop selector_to_test;
  switch (result()->contents()->kind()) {
  case OuterMethodType:
    // we're compiling a method.  simply, test selector() against selectors in
    // CompileWithSICNames
    selector_to_test = selector();
    break;
  case BlockMethodType: {
    // we're compiling a block.  need to find the method containing the block, and
    // test that method's selector against the ones in CompileWithSICNames
    assert(receiver->is_block(), "expecting a block");
    selector_to_test = blockOop(receiver)->outermostMethodSelector();
    break;
  }
 default:
    ShouldNotReachHere();
  }

  return isSelectorInCompileWithSICNames(selector_to_test);
}


bool compilingLookup::forcedCompilerIsSIC() {
  static bool haveWarned = false;
  if (!haveWarned) {
    warning("Methods/blocks will be compiled NIC-only unless the selector is in "
            "_CompileWithSICNames. (Reason: _CompileWithSICNames is a vector.)");
    haveWarned = true;
  }

  return isMethodOrBlockInCompileWithSICNames();
}


# ifdef SIC_COMPILER
  bool compilingLookup::mustUseSIC() {
    return isCompilerForced()  &&  forcedCompilerIsSIC();
  }

  bool compilingLookup::mustUseNIC() {
    return isCompilerForced()  && !forcedCompilerIsSIC();
  }
# else
  bool compilingLookup::mustUseSIC() { return false; }

  bool compilingLookup::mustUseNIC() { return true; }
# endif


void compilingLookup::trace(nmethod *nm) {
  if (sendingVFrame == NULL)
    return; // nothing I can do, top fram??
  // print out some tracing information
  if (nm->scopes->root()->isDataAssignmentScope()) {
    sendingVFrame->fr->traceAssignment(receiver, nm);
  } else if (nm->scopes->root()->isDataAccessScope()) {
    sendingVFrame->fr->traceLookup(receiver, nm);
  } else {
    sendingVFrame->fr->trace(receiver, nm);
  }
}




cacheProbingLookup::cacheProbingLookup(oop rcvr,
                                       oop sel,
                                       oop dgt,
                                       oop smh_or_map,
                                       abstract_vframe* sendingVf, 
                                       sendDesc *sd,
                                       DIDesc *dc,
                                       bool debug,
                                       bool canReuseNM )
 : compilingLookup( rcvr, sel, dgt, smh_or_map, sendingVf, sd, dc,
                    debug, canReuseNM) { 

  // clear bit; nmethod will be used for other receivers
  if (baseLookupType(lookupType()) == NormalBaseLookupType)
    clearReceiverStatic();
}



// Locate an nmethod, compiling a new one if necessary
nmethod* cacheProbingLookup::lookupNMethod() {
  nmethod* nm= probeCache();
  
  if (nm) {
    if (Trace) trace(nm);
    return nm;
  }
  return compilingLookup::lookupNMethod();
}



// First cut at excluding reuse:

bool cacheProbingLookup::mightBeAbleToReuseNMethod() {

  if ( !canReuseNM )  return false; // user-disabled

  if ( compiler != NIC )  // haven't figured out SIC reuse yet
    return false; 

  // calling convention is different for methods with a static delegatee
  // send desc is one word longer to hold del, so return has different offset

  LookupType l= lookupType();
  if ( needsDelegatee(l)  &&  isDelegateeStatic() )
    return false;

  if ( isPerformLookupType(l) ) return false; // send desc is also different

  // for immediate oops, the prologue is different so cannot reuse
  //  non-imm oop method for immediate, and imms have no descendants

  if ( key.receiverMap()->is_smi()  ||  key.receiverMap()->is_float() )
    return false;

  // accessor methods not reusable cause offset might not match &
  // we don't check it yet
  
  switch ( resultType() ) {
   case dataResult:
   case assignmentResult:
    return false;
   default:
    break;
  }
  // cannot deal with DI, so forget it if have dc (DI desc)
  if (dc != NULL)  return false;

  return true; // TA DA!
}


bool cacheProbingLookup::shouldCompileReusableNMethod( nmethod* nm) {
  // if NIC is inlining data accesses,
  // compile a specialized method so it can exploit
  // inlined accesses

  // does this next combo work? XX miw
  if ( NICInlineDataAccess ) return false;

  if ( key.EQ(canonical_key) ) // key is canonical, reuse is meaningless
    return false;
  
  // ``something do to w.r.t DI'' better safe than sorry
  if ( !adeps->isEmpty() )  return false;
  
  // got one, but it is not reusable, so need a specific one,
  // since general one is not reusable
  if (nm && !nm->reusable()) return false;

  return true;
}


static const nmethod* cannotReuse = NULL;
static const nmethod* compileAndReuse = (nmethod*)-1;


nmethod* cacheProbingLookup::findMethodToReuse() {
  if ( !mightBeAbleToReuseNMethod() )
    return (nmethod*)cannotReuse;

  // build key for canonical method
  
  oop resultMH= result()->methodHolder_or_map(receiver);
  mapOop resultMHmapOop= 
    resultMH->is_map() ? mapOop(resultMH) : resultMH->map()->enclosing_mapOop();
  MethodLookupKey ck( NormalLookupType, MH_NOT_A_RESEND, resultMHmapOop, selector(), 0);
  canonical_key= ck; // copy info 

  nmethod* nm= Memory->code->lookup(canonical_key, needDebug);
  if ( nm &&  nm->reusable() )
    return nm;
  return (nmethod*)
         (shouldCompileReusableNMethod( nm) ? compileAndReuse : cannotReuse);
}


nmethod* cacheProbingLookup::compileOrReuse() {
  nmethod* nm = findMethodToReuse();
    
  if ( nm == cannotReuse ) {
    nm= compilingLookup::compileOrReuse();
    updateCache(nm);
    return nm;
  }
  if ( nm == compileAndReuse ) {
    // compile in context of method holder and reuse

    MethodLookupKey orig(&key);
    
    key= canonical_key;
    nm= compilingLookup::compileOrReuse();
    key= orig;
    
    updateCache(nm);
    updateCache(nm, &canonical_key);
    return nm;
  }
  // reuse nm
  const char *s= sprintName(NULL, selector());
  if (PrintCompilation)
    lprintf("*Reusing NIC method %#lx for selector %s\n", nm, s);
  LOG_EVENT1("Reusing NIC method %#lx", nm);
  nm->addDeps(deps);
  updateCache(nm, &key);    
  return nm;
}


nmethod *cacheProbingLookup::probeCache() {
  if (dc) return NULL; // don't cache DI methods
  nmethod *nm= Memory->code->lookup(key, needDebug);
  if (nm == NULL) return NULL;
  // Only check for perform errors if cache hits, because perform_full_lookup
  // also does same check (since caching is subclass-add-on) -- dmu
  if (perform_error_status(sd->quick_perform_arg_count()) != foundNone)
    return NULL;
  if (nm->needToRecompileFor(sd))
    nm= also_Recompile(sd, this, nm);
  return nm;
}



void cacheProbingLookup::updateCache(nmethod *nm, MethodLookupKey *k) {

  // Enter nm in codeTable at k (use this->key if k==NULL), if sensible.
  // NB static receiver bit already cleared in lookup type (important 'cause
  // cache is receiver-identity-insensitive)
  
  if (   dc == NULL          // don't cache DI methods
      && status == foundOne  // don't cache error methods
      && result()->is_real() // don't cache doIts or conversions
      && result()->as_real()->holder->is_object_or_map() )
    Memory->code->addToCodeTable(nm, k ? k : &key);

}



lookupTarget* baseCompileTimeLookup::receiver_lookup_target() {
  return 
    receiver == badOop
    ? (new mapLookupTarget(receiverMap())) ->be_receiver()
          : simpleLookup::receiver_lookup_target();
}


lookupTarget* baseCompileTimeLookup::delegated_lookup_target() {
  assert_static_delegatee();
  return  simpleLookup::delegated_lookup_target();
}


lookupTarget* baseCompileTimeLookup::undirected_resend_lookup_target() {
  if (!methodHolder_or_map()->is_map()) {
    return simpleLookup::undirected_resend_lookup_target();
  } else {
    // don't know actual value of lookup start location
    assert(methodHolder_or_map() == receiverMapOop(), 
           "expecting receiver and sending method holder to be in same map");
    return
      (new mapLookupTarget(methodHolder_map()))
      -> be_receiver();
  }
}


lookupTarget* baseCompileTimeLookup::directed_resend_lookup_target() {
  assert_static_delegatee();
  return simpleLookup::directed_resend_lookup_target();
}


bool baseCompileTimeLookup::check_slot_for_directed_resend(slotDesc* desc) {
  if (desc->is_map_slot()) return true;
  // cannot inline a delegated send through an assignable parent
  status = foundAssignableParent;
  return false;
}


void baseCompileTimeLookup::perform_lookup() {
  assert_static_selector();
  if (status == resendUndecidable) {
    // couldn't inline a resend because
    // a method holder along the way wasn't constant
    return;
  }
  simpleLookup::perform_lookup();
#if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  result()) {
    // result()->holder->add_dependency(result()->desc, this);
    Map *m= result()->as_real()->holder->map();
    slotDesc *s= result()->as_real()->desc;
    assert(   !result()->as_real()->holder->is_object_or_map()
           || m->is_block()
           || s->is_parent()
           || !m->can_have_dependents()
           || (  ((slotsMapDeps*)m)->has_slot_dependents()
              && deps->includes(((slotsMapDeps*)m)->dependents_for_slot(s))),
           "result should have a dependency");
  }
#endif
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


# ifdef SIC_COMPILER

SICLookup::SICLookup( LookupType l, oop rcvr, oop sel, oop dgt,
                      dependencyList* dps, SCodeScope* sc )
  : baseCompileTimeLookup(l, rcvr, sel, dgt, sc->methodHolder_or_map(), dps) {
  scope = sc;
}


lookupTarget* SICLookup::implicit_self_lookup_target() {
  return new sicScopeLookupTarget((SScope*)scope);
}

# endif
  

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "deps.hh"
# include "_deps.cpp.incl"

nmln* dependencyList::dependentsArrayS;

# if GENERATE_DEBUGGING_AIDS
  bool dependencyList::includes(nmln* dep) {
    for (nmln* d = dependentsArrayS; d < top; d ++) {
      // check to see if already in dependents array
      if (d->next == dep || d->prev == dep) {
        return true;
      }
    }
    return false;
  }
# endif

void dependencyList::add(nmln* dep) {
  if (alreadyThere(dep)) {
    return;
  }
  if (length() >= DependentsArraySize) {
    fatal("dependency list overflow");
  }
  top->init();
  top->check_alignment(); // for Intel
  dep->add(top);
  top++;
}

void dependencyList::remove() {
  for (nmln* d = dependentsArrayS; d < top; d ++) {
    d->remove();
  }
}

objectLookupTarget** assignableDependencyList::dependentsArrayS;

void assignableDependencyList::add(objectLookupTarget* target) {
  for (objectLookupTarget** t = dependentsArrayS; t < top; t ++) {
    if (*t == target) {
      // already in dependency list; ignore this one
      return;
    }
  }
  if (length() >= AssignableDependentsArraySize) {
    ShouldNotReachHere(); // assignable dependency list overflow
  }
  *top++ = target;
}
/* Sun-$Revision: 30.17 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sendDesc_abstract.hh"
# pragma implementation "sendDesc.hh"
# include "_sendDesc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

sendDesc* sendDesc::first_sendDesc() {
  // assertion is in sendDes::init
  return sendDesc::sendDesc_from_return_PC(
           first_inst_addr((void*)firstSelfFrame_returnPC));
}


bool sendDesc::checkLookupTypeAndEntryPoint() {
  char *insts= jump_addr();
  if (insts == lookupRoutine()) return true;
  NCodeBase *n= findThing(insts);
  if (n->isCacheStub()) {
    return true; // checked in CacheStub::verify
  }
  nmethod* nm;
  if (n->isNMethod()) {
    nm= (nmethod*)n;
  } else {
    assert(n->isCountStub(), "what is it?");
    CountStub *cs= (CountStub*)n;
    nm= cs->target();
    insts= cs->jump_addr();
  }
  return checkLookupTypeAndEntryPoint(nm, insts);
}
  

bool sendDesc::checkLookupTypeAndEntryPoint(nmethod *nm, char *entryPoint) {
  LookupType lsrc= lookupType() & ~ImplicitSelfBit;
  LookupType ldst= nm->key.lookupType & ~ImplicitSelfBit;
  bool rcvrStaticLookupType= lsrc & ReceiverStaticBit;
  // dstStaticLookupType==true implies no map check in prologue of nm
  bool  dstStaticLookupType= ldst & ReceiverStaticBit;
  lsrc &= ~ReceiverStaticBit;
  ldst &= ~ReceiverStaticBit;

  bool callsPrologue= entryPoint == nm->insts() && !dstStaticLookupType;
  bool skipsPrologue= entryPoint == nm->verifiedEntryPoint();
  // if things are really broken, neither callsPrologue nor
  // skipsPrologue is true

  if (pic())
    return skipsPrologue
      && (ReuseNICMethods && nm->reusable()
          ? lookupMatch(lsrc, ldst) : lsrc == ldst);
      
  if (!ReuseNICMethods)
    return lsrc == ldst
        && (rcvrStaticLookupType ? skipsPrologue : callsPrologue);

  if (!rcvrStaticLookupType)
    return lookupMatch(lsrc, ldst) && callsPrologue;

  // check entry point
  if (   (lookupType() & ImplicitSelfBit)
      && !isResendLookupType(lsrc)
      && nmethod::findNMethod(this)->reusable()) {
    // If we don't really know the receiver type for sure
    // (because the it's an implicit self send and the
    // sending method is reusable),
    // we must use the non-verified entry point.  Resends are OK --
    // we'll still be invoking the right method.
    if (!callsPrologue) return false;
  } else {
    if (!skipsPrologue) return false;
  }

  // Check lookup types.  If target is reusable,
  // lookup types need not match precisely, only enough to ensure
  // prologues and return addresses match
  return nm->reusable() ? lookupMatch(lsrc, ldst) : lsrc == ldst;
}


void sendDesc::extend(nmethod* nm, mapOop receiverMapOop,
                      CountStub *cs_from_pic) {
  char *addr;
  bool isPerform= isPerformLookupType(raw_lookupType());
  if (   PIC && !isPerform
      && (addr= jump_addr(), addr != lookupRoutine())) {
    // already has at least one nmethod linked to the send
    CacheStub* s= pic();
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        assert(cs_from_pic==NULL, "discarding countStub?");
        if (!s) {
          // turning monomorphic ic into PIC; PIC will carry dependencies
          assert(dependency()->notEmpty(), "shouldn't be empty");
          // check for duplicate nmethod & rcvr map
          CountStub *oldcs= countStub();
          nmethod *oldnm= oldcs ? oldcs->target() : nmethod::findNMethod(addr);
          assert(   oldnm != nm
                 || receiverMapOop != oldnm->key.receiverMapOop(),
                 "already linked to this nmethod, why rebind?? (maybe I-cache did not get flushed)");
        }
      }
    # endif
    s= s->extend(this, nm, receiverMapOop);
    assert(s, "should have a PIC now");
    assert(dependency()->next == dependency()->prev, "more than one link");
  } else {
    // first time we execute this send, or PICs disabled, or perform, or
    // rebinding after a PIC has been removed, or non-map related misses
    if (isPerform) NumberOfPerformMisses++;
    if (!PIC) {
      CacheStub *s= pic();
      // remove existing PIC
      if (s) s->deallocate();
    }
    rebind(nm, NULL, cs_from_pic);
  }
  MachineCache::flush_instruction_cache_for_debugging();
  if (VerifyZoneOften) verify();
}

// rebind the sendDesc to nm at addr, with cs_from_pic if non-NULL
void sendDesc::rebind(nmethod* nm, char* addr, CountStub *cs_from_pic) {
  if (VerifyZoneOften) {
    nm->linkedSends.verify_list_integrity();
    if (dependency()->next == NULL  &&  dependency()->prev == NULL)
      ; // not initted yet
    else
      verify();
  }
  if (addr == NULL) addr= nm->entryPointFor(this);
  assert(pic() == NULL, "shouldn't call");
  assert(nm->key.selector
         == static_or_dynamic_selector(nm->key.selector, nm->key.lookupType),
         "mismatched selector");
  nmethod* current= target();
  assert(   RecompilationInProgress
         || current != nm
         || nm->isYoung(),  // rebind to insert aging stub
         "why rebind?? (maybe I-cache did not get flushed)");

  if (nm->isDI() && countType() != NonCounting) {
    // turn off counting/comparing flag - cannot inline DI nmethods yet
    setCounting(NonCounting);
    assert(countType() == NonCounting, "oops");
  }
  CountStub *oldcs= countStub();
  assert(cs_from_pic == NULL || oldcs == NULL,
         "got count stub from pic and count stub from sendDesc");

  if (cs_from_pic) {
    assert(   nm->isYoung() && cs_from_pic->isAgingStub()
           || isCounting()  && cs_from_pic->isCountStub(),
              "count stub from pic doesn't match send desc");
    set_jump_addr(cs_from_pic->insts());
    dependency()->rebind(&cs_from_pic->sdLink);
    cs_from_pic->set_callee((int32)addr);
  } else if (UseAgingStubs && nm->isYoung()) {
    // need to insert an aging stub
    if (oldcs) {
      oldcs->deallocate();
    } else if (current) {
      dependency()->remove();
    }
    CountStub *newcs= new AgingStub(nm, addr, dependency());
    set_jump_addr(newcs->insts());
  } else if (UseAgingStubs && oldcs) {
    // change target of existing count stub
    oldcs->rebind(nm, addr);
  } else if (UseAgingStubs && isCounting()) {
    // create count stub
    if (current) dependency()->remove();        // unlink current target
    CountStub *newcs = 
      CountStub::new_CountStub(nm, addr, dependency(), countType()); 
    set_jump_addr(newcs->insts());
  } else {
    // rebind inline cache
    set_jump_addr(addr);
    dependency()->rebind(&nm->linkedSends);
  }
  if (VerifyZoneOften) verify();
  MachineCache::flush_instruction_cache_for_debugging();
}

void sendDesc::unlink_nmethod() {
  dependency()->remove();
  reset_jump_addr();
  MachineCache::flush_instruction_cache_for_debugging();
}

void sendDesc::unlink_countStub(CountStub *stub) {
  assert(stub && stub == countStub(), "wrong stub");
  stub->deallocate();
  MachineCache::flush_instruction_cache_for_debugging();
  assert_unbound();
}

void sendDesc::unlink_pic(CacheStub *pic) {
  assert(pic && pic == this->pic(), "wrong pic");
  pic->deallocate();
  MachineCache::flush_instruction_cache_for_debugging();
  assert_unbound();
}

void sendDesc::unlink() {
  CacheStub* s= pic();
  if (s)
    unlink_pic(s);
  else {
    CountStub* cs= countStub();
    if (cs)
      unlink_countStub(cs);
    else
      unlink_nmethod();
  }
}

bool sendDesc::wasNeverExecuted() {
  // the "dirty bit" is used to distinguish sendDescs that are accidentally
  // empty (because of a flush) from sends that were never executed
  // (the former are "dirty", the latter clean)
  return jump_addr() == lookupRoutine() && !isDirty();
}

bool sendDesc::verify() {
  if (isPrimCall()) return true;
  LookupType l= lookupType();
  bool flag= checkLookupTypeAndEntryPoint();
  
  if (isPerformLookupType(l)) {
    if (arg_count() < 0 || arg_count() > 100) {
      error2("sendDesc %#lx arg count %ld is invalid", this, arg_count());
      flag = false;
    }
  } else {
    if (! oop(selector())->verify_oop()) {
      flag = false;
    } else if (! selector()->is_string()) {
      error1("sendDesc %#lx selector isn't a string", this);
      flag = false;
    }
    nmethod* nm = target();
    if (nm == NULL) {
      CountStub* cs = countStub();
      if (cs) nm = cs->target();
    }
    if (nm == NULL) {
      CacheStub* cs = pic();
      if (cs) {
        nm= cs->get_method(0);
        oop sel= nm->key.selector;
        for (fint i= 1; i < cs->arity(); ++i)
          if (cs->get_method(i)->key.selector != sel)
            error2("sendDesc %#lx: selector != PIC case %d selector",
                   this, i);
      }
    }
    if (nm && nm->key.selector != selector())
      error1("sendDesc %#lx: selector != target nmethod's selector", this);
  }
  if (l & DelegateeStaticBit) {
    if (! delegatee()->verify_oop()) {
      flag = false;
    } else if (baseLookupType(l) == DirectedResendLookupType &&
               ! delegatee()->is_string()) {
      error1("sendDesc %#lx delegatee isn't a string", this);
      flag = false;
    }
  }
  if (!dependency()->verify_list_integrity()) {
    lprintf("\tof sendDesc %#lx\n", this);
    flag = false;
  }
  if (pic()) {
    if (dependency()->next != dependency()->prev)
      error1("sendDesc %#lx: more than one elem in dependency chain", this);
    pic()->verify();
  } else {
    CountStub *cs= countStub();
    if (cs == NULL) {
      if (isCounting() && jump_addr() != lookupRoutine())
        error1("sendDesc %#lx: doesn't have countStub but is counting", this);
    } else {
      if (!isCounting() && !cs->isAgingStub())
        error1("sendDesc %#lx: has countStub but is not counting", this);
      if (dependency()->next != dependency()->prev)
        error1("sendDesc %#lx: more than one elem in dependency chain", this);
      countStub()->verify2(NULL);
    }
  }
  return flag;
}


bool sendDesc::isPrimCall() {
  char *insts= jump_addr();
  bool b=    !Memory->code->contains(insts)
          && !Memory->code->stubs->contains(insts)
          && insts != lookupRoutine();
  if (b)  { assert(getPrimDescOfFirstInstruction(insts, true), "not a prim call"); }
  return b;
}

void sendDesc::print() {
  if (isPrimCall()) {
    PrimDesc *pd= getPrimDescOfFirstInstruction(jump_addr(), true);
    lprintf("primitive: %s\n", pd->name());
    return;
  } 
  printIndent();
  printLookupType(raw_lookupType());
  LookupType l= lookupType();
  if (isPerformLookupType(l)) {
    lprintf(": argc: %ld", arg_count());
  } else {
    lprintf(": selector: ");
    selector()->print_real_oop();
  }
  if (l & DelegateeStaticBit) {
    lprintf(": delegatee: ");
    delegatee()->print_real_oop();
  }
  Indent++;
  printIndent();
  lprintf("addr: %#lx", jump_addr());
  if (Memory->code->contains(jump_addr())) {
    lprintf(" (nmethod %#lx)", nmethod::findNMethod(jump_addr()));
  }
  lprintf("; mask: "); printMask(mask());
  lprintf("\n");
  printIndent();
  lprintf("dependency: ");
  dependency()->print();
  lprintf("\n");
  if (pic()) {
    printIndent();
    lprintf("PIC: p ((CacheStub*)%#lx)->print()\n", pic());
  }
  if (countStub()) {
    printIndent();
    lprintf("count stub: p ((CountStub*)%#lx)->print()\n",
            countStub());
  }
  Indent --;
}

fint sendDesc::endOffset(LookupType l) {
  fint offset =  normal_sendDesc_end_offset;
  if (needsDelegatee(l) && (l & DelegateeStaticBit)) {
    // add space for delegatee word
    offset += sizeof(oop);
  }
  return offset;
}

void sendDesc::link(CacheStub* s) {
  set_jump_addr(s->insts());
  assert(dependency()->isEmpty(), "not empty");
  dependency()->add(&s->cacheLink);
  MachineCache::flush_instruction_cache_for_debugging();
}

CacheStub* sendDesc::pic() {
  char* addr = jump_addr();
  if (Memory->code->contains(addr)) {
    // linked to a nmethod
    return NULL;
  } else if (Memory->code->stubs->contains(addr)) {
    NCodeBase* stub = findStub(addr);
    return stub->isCacheStub() ? (CacheStub*)stub : NULL;
  } else {
    return NULL;
  }
}

CountStub* sendDesc::countStub() {
  char* addr= jump_addr();
  if (Memory->code->contains(addr)) {
    // linked to a nmethod
    assert(!Memory->code->stubs->contains(addr), "zones overlap");
    return NULL;
  } else if (Memory->code->stubs->contains(addr)) {
    NCodeBase* stub = findStub(addr);
    return stub->isCountStub() ? (CountStub*)stub : NULL;
  } else {
    return NULL;
  }
}
  
nmethod* sendDesc::target() {
  char* addr = jump_addr();
  return Memory->code->contains(addr) ? nmethod_from_insts(addr) : NULL;
} 

// NB: get_method() is not quite equivalent to target(): the former returns
// an nmethod even when it is called via a count stub, the latter returns
// NULL in this case.
// It can also be called on dummy sendDescs representing glue code
nmethod* sendDesc::get_method() {
  char *addr= jump_addr();
  if (Memory->code->contains(addr)) return nmethod_from_insts(addr);
  if (!Memory->code->stubs->contains(addr)) return NULL;
  NCodeBase *n= findStub(addr);
  assert(n->isCountStub(), "shouldn't call on PICs");
  return ((CountStub*)n)->target();
}

fint sendDesc::ntargets() {
  char *addr= jump_addr();
  if (addr == lookupRoutine()) return 0;
  if (Memory->code->contains(addr)) return 1;
  assert(Memory->code->stubs->contains(addr), "what is it?");
  NCodeBase *n= findStub(addr);
  return n->isCountStub() ? 1 : ((CacheStub*)n)->arity();
} 

fint sendDesc::nsends() {
  CacheStub* s = pic();
  CountStub* cs;
  fint n = 0;
  if (s) {
    for (fint i = s->arity() - 1; i >= 0; i--) {
      cs = s->countStub(i);
      if (cs) {
        n += cs->count();
      } else {
        // this one doesn't have a count stub
      }
    }
  } else if ((cs = countStub()) != NULL) {
    n = cs->count();
  }
  return n;
} 

sendDesc* sendDesc::sendDesc_from_nmln(nmln* l) {
  sendDesc* sd = (sendDesc*) (((char*)l) - depend_offset);
  assert(Memory->code->contains(sd), "not in zone");
  return sd;
}

// Test the size of the assembler generated sendDesc in 
// EnterSelf (<machine>.runtime.s). Lars July 92
void sendDesc::init() {
  sendDesc::init_platform();
  
  # if HOST_ARCH == PPC_ARCH && TARGET_ARCH == I386_ARCH
    if (true) return; // just testing asm
  # endif
  sendDesc* f = sendDesc::first_sendDesc();

  // cannot do this test on sparc, it has a register-call which does not read as a call
  // if (!isCall((int32*)f->jump_addr_addr()))
  //  fatal("first_sendDesc() does not have a call");
  
  if (f->raw_lookupType() != StaticNormalLookupType)
    fatal5("first_sendDesc() has wrong lookup type: 0x%x, should be: 0x%x\n"
            "  firstSelfFrame_returnPC: 0x%x,  first_inst_addr:  0x%x, first_sendDesc 0x%x",
            f->raw_lookupType(), StaticNormalLookupType,
            firstSelfFrame_returnPC, 
            first_inst_addr((void*)firstSelfFrame_returnPC), 
            sendDesc::first_sendDesc());

  char* computedEnd = (char*) f + f->endOffset();
  char* realEnd     = first_inst_addr((void*)firstSelfFrameSendDescEnd);
  if (computedEnd != realEnd)
    fatal2("sendDesc of firstSelfFrame has wrong size, "
           "computedEnd = 0x%x, realEnd = 0x%x", computedEnd, realEnd);
}


// called from asm stubs called from empty inline caches:
char* SendMessage(sendDesc* sd, frame* lookupFrame, oop receiver,
                  oop perform_selector, oop perform_delegatee, oop arg1) {
                  
  assert(lookupFrame->is_aligned(), "alignment");

  return sd->sendMessage(lookupFrame,
                         receiver,
                         perform_selector,
                         perform_delegatee,
                         arg1);
}


void sendDesc::sendMessagePrologue( oop  receiver, frame* lookupFrame ) {
                                   
  NumberOfLookups++;

  assert(receiver != Memory->deadBlockObj, "should have created real block");
  assert(Byte_Map_Base() == Memory->remembered_set->byte_map_base(),
         "byte map base reg corrupted");
  assert(!processSemaphore, "processSemaphore shouldn't be set");

  if (SilentTrace)
    LOG_EVENT3("sendDesc::sendMessage %#lx %#lx %#lx",
               this,
               receiver,
               lookupFrame);
}


static nmethod* SendMessage_cont( compilingLookup* L) {
  if ( Interpret ) {
    L->perform_full_lookup();
    return NULL;
  }
  return L->send_desc()->lookup_compile_and_backpatch(L);
}


char* sendDesc::sendMessage( frame* lookupFrame,
                             oop receiver,
                             oop perform_selector,
                             oop perform_delegatee,
                             oop arg1 ) {
  ShowLookupInMonitor sl;

  LookupType type= lookupType();

  oop sel = static_or_dynamic_selector(  perform_selector,  type);
  oop del = static_or_dynamic_delegatee( perform_delegatee, type);

  sendMessagePrologue( receiver, lookupFrame );

  if ( !Trace) {
    // try codeTable first (order of magnitude faster)
  
    char* r = fastCacheLookupAndBackpatch(type,
                                          receiver->map()->enclosing_mapOop(),
                                          sel,
                                          del);
    if (r) {
      if (SilentTrace) LOG_EVENT1("SendMessage: fast-found %#lx", r);
      return r;
    }
  }

  // have to do it the slow way
  ResourceMark m;
  FlushRegisterWindows(); // for vframe conversion below
  cacheProbingLookup L( receiver,
                        sel,
                        del,
                        MH_TBD,  // method holder
                        new_vframe(lookupFrame),
                        this,
                        NULL,    // DIDesc
                        false ); // don't want a debug version

  // should we have switched stacks sooner? (dmu) also in SendDIMessage
  
  nmethod* nm = switchToVMStack( SendMessage_cont, &L );
  if (SilentTrace) LOG_EVENT1("sendDesc::sendMessage: found %#lx", nm);

  return
    Interpret
    ? L.interpretResultForCompiledSender(arg1)
    : nm->verifiedEntryPoint();
}



char* sendDesc::fastCacheLookupAndBackpatch( LookupType t,
                                             mapOop receiverMapOop,
                                             oop sel,
                                             oop del ) {
  if (needsDelegatee(t) || isResendLookupType(t) || isPerformLookupType(t)) {
    // too complicated to short-circuit these lookups
    return NULL;
  }
  // try mh-independent 
  MethodLookupKey key(t, MH_TBD, receiverMapOop, sel, del);
  nmethod* nm= Memory->code->lookup(key);
  
  if (nm == NULL) {
    NumberOfFastLookupMisses++;
    return NULL;
  }
  NumberOfFastLookupHits++;

  if (nm->needToRecompileFor(this))
    return NULL;

  if (InlineCache) {
    NumberOfICMisses++;
    extend(nm, receiverMapOop, NULL);
  }
  return nm->verifiedEntryPoint();
}


nmethod* sendDesc::lookup_compile_and_backpatch( compilingLookup* L ) {

  nmethod* nm= L->lookupNMethod();
  
  assert(zone::frame_chain_nesting == 0 || recompilee != NULL,
         "should not be nested");
  
  if (InlineCache &&
      (InlineCacheNonStatic || L->isReceiverStatic())) {
    // add to PIC
    extend(nm, L->receiverMapOop(), NULL);
    NumberOfICMisses++;
  }
  return nm;
}

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  void sendDesc::init() { }
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.14 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "cacheStub_abstract.hh"
# pragma implementation "cacheStub.hh"
# pragma implementation "cacheStub_inline_abstract.hh"
# pragma implementation "cacheStub_inline.hh"
# include "_cacheStub.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

# define CHECK_PIC(var1, var2)                                                \
    if (! var1->isCacheStub()) continue;                                      \
    CacheStub* var2 = (CacheStub*)var1;

VtblPtr_t   CacheStub::_vtbl_value;
Assembler*  CacheStub::a;
Assembler*  CacheStub::oldAssembler;
nmethod**   CacheStub::n;
CountStub** CacheStub::st;
fint CacheStub::newMethods;
sendDesc* CacheStub::theSendDesc;
nmethod *CacheStub::nsmi;
nmethod *CacheStub::nfloat;
CountStub *CacheStub::stsmi;
CountStub *CacheStub::stfloat;

const fint MaxStubSize = 32000; // generous to allow experiments with PICs

inline NCodeBase* CacheStub::get_thing(fint which) {
  return findThing(getJumpAddr(which));
}

mapOop CacheStub::get_map(fint which) {
  fint imm= immediateCount();
  if (which < imm) {
    switch (which) {
    case 0: return (hasSmi() ? Memory->smi_map : Memory->float_map)->enclosing_mapOop();
    case 1: return Memory->float_map->enclosing_mapOop();
    default: ShouldNotReachHere(); return NULL;
    }
  } else
    return mapOop(locs()[getMapLocsIndex(which)].referent(this)); 
}

nmethod* CacheStub::get_method(fint which) {
  NCodeBase* s = get_thing(which);
  if (s->isNMethod()) {
    return (nmethod*)s;
  } else {
    assert(s->isCountStub(), "what is it?");
    return ((CountStub*)s)->target();
  }
}

int32 CacheStub::getJumpLocsIndex(int32 which) {
  // Find index of addrDesc corresponding to nth jump.
  // (Very dependent on form of generated code, of course, but it happens
  // that both PPC and Sparc have the same version.  Other architectures
  // can implement this function on their own, if need be.)
  int32 immed, index;
  immed = immediateCount();
  if (which < immed) {
    index = which;    // immediate cases have one addrDesc for the jump
  } else {
    // map cases have two addrDesc's - one for the map load and one for
    // the jump.  we add "1" below because we want the second addrDesc
    // in the pair (the jump).  we subtract immed because the first
    // immed cases have just 1 addrDesc and the 2 * which assumes 2.
    index = 2 * which + 1 - immed;
  }
  return index;
}

int32 CacheStub::getMapLocsIndex(int32 which) {
  // Find index of addrDesc corresponding to nth map constant.
  // (Very dependent on form of generated code, of course, but it happens
  // that both PPC and Sparc have the same version.  Other architectures
  // can implement this function on their own, if need be.)
  // The map addrDesc comes before the corresponding jump addrDesc.
  return getJumpLocsIndex(which) - 1;
}

fint CacheStub::nOccurrences(nmethod *nm) {
  fint nocc= 0;
  for (fint i= 0; i < arity(); i++) {
    if (get_method(i) == nm) nocc++;
  }
  return nocc;
}

CountStub* CacheStub::countStub(fint which) {
  NCodeBase* s = get_thing(which);
  if (s->isNMethod()) {
    return NULL;
  } else {
    assert(s->isCountStub(), "what is it?");
    return (CountStub*)s;
  }
}
    
inline fint realLocsLen(fint arity, bool has_smi,
                        bool has_float) {
# define FailLoc  1     /* call of runtimeLookup has a location */
  fint n =  2 * arity + FailLoc - (has_smi ? 1 : 0) - (has_float ? 1 : 0);
  return n * sizeof(addrDesc);
}

void* CacheStub::operator new(size_t size) {
  fint newSize = size + a->len() + newMethods * sizeof(nmln);
  return Memory->code->stubs->allocate(newSize);
}

// NB: deallocation is not done via destructor/delete because g++ (2.2.2)
// changes the vtable pointer before calling the destructor of the superclass
// i.e. it would not be possible to call Heap::deallocate in the destructor
// because the vtbl pointer change would write into a deallocated block of
// memory.  Furthermore, it's inconvenient to deallocate in delete because
// we need an additional argument (the size).

CacheStub::CacheStub() {
  assert(sizeof(cacheStubInfo) == 4, "check that bit fields work right");
  CHECK_VTBL_VALUE;
  cacheLink.init();
  info.arity= newMethods;
  info.has_smi= nsmi != NULL;
  info.has_float= nfloat != NULL;
  info.is_megamorphic= false;
  _instsLen = roundTo(a->instsLen(), oopSize);
  _locsLen  = a->locsLen();
  
  copy_oops((oop*)a->instsStart, (oop*)insts(), _instsLen/oopSize);
  int32 delta = (pc_t)insts() - (pc_t)a->instsStart;

  assert(realLocsLen(arity(), info.has_smi, info.has_float) == locsLen(),
         "wrong number of locs");
  copy_oops((oop*)a->locsStart, (oop*)locs(), locsLen() / oopSize);
  for (addrDesc* p = locs(), *end = locsEnd(); p < end; p++) {
    if (p->isOop()  &&  oop(p->referent(this))->is_new()) remember();

    // Because the PIC has been moved,  we must fix branches with relative
    // displacements targeting code outside the PIC.  Only these types of
    // branches have entries in the "loc" table.  So to check if an addrDesc
    // has to be shifted, we just have to check if the instr is a branch.
    // (This is only needed/done under PPC.) -abdelmalek 9/02
    // No, I think this is also needed for SPARC. -- dmu 3/06
    //
    // refactored for Intel -- dmu 3/06
    if (p->isShiftNeededAfterMovingMe(this))
      p->shift(this, delta);
  }
  
  nmln* d = deps(); 
  for (fint i = 0; i < newMethods; i++, d++) {
    d->init();
    if (st[i]) {
      st[i]->initSendDesc(d);
    } else {
      n[i]->linkedSends.add(d);
    }
  }
  assert(d == depsEnd(), "just checkin'");

  MachineCache::flush_instruction_cache_range(insts(), instsEnd());
  MachineCache::flush_instruction_cache_for_debugging();
  
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  this == (CacheStub*)catchThisOne) warning("caught cache stub");
# endif
}

Label* CacheStub::generate(nmethod* nm, mapOop receiverMapOop,
                           CountStub* stub, Label* prev) {
  Map *rmap= receiverMapOop->map_addr();
  assert(!rmap->is_smi() && !rmap->is_float(), "missed an immediate");
  pc_t jumpAddr;
  computeJumpAddr(nm, theSendDesc, stub, jumpAddr);
  
# if defined(SIC_COMPILER)
  if (SICCountTypeTests) theAssembler->doOneTypeTest();
# endif

  prev = test(receiverMapOop, jumpAddr, prev);
  n[newMethods] = nm;
  st[newMethods++] = stub;
  assert(newMethods <= MaxPICSize, "arity overflow");
  return prev;
}


void CacheStub::computeJumpAddr(nmethod* nm, sendDesc* sd,
                                CountStub*& stub, pc_t& addr) {
  // compute jump address (nmethod or count stub); create count stub if needed
  // CAUTION: stubs are deallocated later (or not at all, if they are
  // passed on to the new PIC), so don't deallocate stubs here
  if (nm == NULL) return;
  if (stub) {
    assert(stub->target() == nm, "wrong target");
    addr= stub->insts();
    stub->setVerifiedEntryPoint(nm); // xxx miw
  } else if (UseAgingStubs  &&  nm->isYoung()) {
    stub= new AgingStub(nm, nm->verifiedEntryPoint(), NULL);
    addr= stub->insts();    
  } else if (sd->isCounting()) {
    stub= CountStub::new_CountStub(nm, nm->verifiedEntryPoint(), NULL, sd->countType());
    addr= stub->insts();
  } else {
    addr= nm->verifiedEntryPoint();
  }
}

CacheStub* CacheStub::extendMegamorphic(sendDesc* send_desc, nmethod* nm,
                                        mapOop receiverMapOop) {
  // A megamorphic stub has missed; just replace first memOop case with
  // the missing map.
  assert(isMegamorphic(), "not megamorphic");
  NumberOfMegamorphicPICMisses++;
  Map* rmap = receiverMapOop->map_addr();
  if (rmap->is_smi() || rmap->is_float()) {
    // need to add a new case; delete last entry to make room
    return copy_replace_immediate(send_desc, MaxPICSize-1, nm, receiverMapOop);
  }
  fint index = immediateCount();
  // pick random case to discard, move first memOop case there
  // random replacement is usually better than rotate because the miss
  // penalty is much lower and because the behavior is more uniform.
  fint i= (rand() >> 4) % (arity() - index - 1) + index + 1;
  assert(i > index && i < arity(), "oops");
  CountStub* cs= countStub(i);          // reuse this count stub for new case
#ifdef LOG_LOTSA_STUFF
  // pollutes event log too much for normal use
  LOG_EVENT2( "CacheStub::extendMegamorphic: index %d, count stub 0x%x",
             i, cs);
#endif
  addrDesc* l = locs();
  nmln* d = deps();
  d[i].remove();                    // unlink random nmethod
  // find indices of addrDescs corresponding to jumps and maps
  fint j0= getJumpLocsIndex(index);
  fint m0= j0 - 1; 
  fint ji= getJumpLocsIndex(i);
  fint mi= ji - 1;
  // patch code (map constant and jump address)
  l[mi].set_referent(this, l[m0].referent(this));
  l[ji].set_referent(this, l[j0].referent(this));
  d[i] = d[index]; d[i].relocate();             // copy nmln
  d[index].init();                              // and zap source nmln

  // change map and jump address of first memOop case
  addrDesc *maploc= &l[m0];
  assert(oop(maploc->referent(this))->is_map(), "not a mapOop");
  maploc->set_referent(this, receiverMapOop);
  if (receiverMapOop->is_new()) remember();
  rebind(index, nm, cs);
  if (VerifyZoneOften) verify();
  return this;
}


void CacheStub::rebind(fint index, nmethod* nm, CountStub* cs) {
  // rebind jump address (but NOT map) of indexth branch
  // make sure young nmethods always have an aging stub

  if (UseAgingStubs  &&  cs == NULL && nm->isYoung()) {
    cs = new AgingStub(nm, nm->verifiedEntryPoint(), NULL);
  }
  if (cs == NULL) {
    // when a young method is replaced by an old one, there is no count stub
    // assert(!sd()->isCounting(),
    //        "call site claims to have a count stub but does not have one");
    setJumpAddr(index, nm->verifiedEntryPoint());
    nmln* d = &deps()[index];
    d->remove();                        // unlink old nmethod
    nm->linkedSends.add(d);             // link in new nmethod
  } else {
    // keep count stub for efficiency reasons --> in megamorphic PICs,
    // the individual counters cannot be trusted, but the sum is ok
    setJumpAddr(index, cs->insts());
    cs->rebind(nm, nm->verifiedEntryPoint());
    nmln* d = &deps()[index];
    d->remove();                        // unlink old count stub
    cs->initSendDesc(d);                // link in new count stub
  }
  MachineCache::flush_instruction_cache_for_debugging(); // set_referent does word-wise flushes
}

CacheStub* CacheStub::makeMegamorphic(sendDesc* send_desc, nmethod* nm,
                                      mapOop receiverMapOop) {
  // convert PIC to a megamorphic PIC;
  // doesn't do anything particularly special now

  if (PrintPIC || PrintMegamorphism)
    lprintf("*making PIC %#lx megamorphic ", this);
  info.is_megamorphic= true;
  CacheStub *s= extendMegamorphic(send_desc, nm, receiverMapOop);
  if (PrintPIC || PrintMegamorphism) lprintf(" --> %#lx\n", s);
  return s;
}

CacheStub* CacheStub::extend(sendDesc* send_desc, nmethod* nm,
                             mapOop receiverMapOop) {

#if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions) {
    if (this && this == (CacheStub*)catchThisOne) warning("caught cache stub");
    for (fint i = 0; i < arity(); i++) {
      nmethod* nm1 = get_method(i);
      if (ReuseNICMethods) {
        assert(   nm1 != nm
               || get_map(i) != receiverMapOop,
               "nmethod already in PIC with same map");
      } else {
        assert(nm1 != nm, "nmethod already in PIC");
        assert(nm1->key.receiverMapOop() != nm->key.receiverMapOop(),
               "rcvr map already in PIC");
      }
    }
  }
#endif

  if (isMegamorphic()) {
    if (PrintPIC && PrintMegamorphism)
      lprintf("*megamorphic PIC %#lx miss, nm = %#lx\n", this, nm);
    return extendMegamorphic(send_desc, nm, receiverMapOop);
  }

  if (isPerformLookupType(nm->key.lookupType))
    ShouldNotReachHere(); // can't handle these kinds of misses

  if (arity() < MaxPICSize)
    return copy_add_nmethod(send_desc, nm, receiverMapOop);
  
  assert(arity() == MaxPICSize, "PIC should never be > MaxPICSize");
  // stub just now turning into a megamorphic stub
  return makeMegamorphic(send_desc, nm, receiverMapOop);
}

#ifdef UNUSED
void CacheStub::insertCounters() {
  sendDesc* send_desc = sd();
  for (fint i = arity() - 1; i >= 0; i--) {
    nmethod* nm = get_method(i);
    CountStub* cs = countStub(i);
    if (cs == NULL) {
      pc_t addr;
      computeJumpAddr(nm, send_desc, cs, addr);
      assert(cs, "should have created count stub");
      nmln* d = &deps()[i];
      d->remove();                      // unlink nmethod
      cs->initSendDesc(d);
      setJumpAddr(i, addr);
    }
  }
}

void CacheStub::removeCounters() {
  sendDesc* send_desc = sd();
  if (send_desc->isCounting()) return;  // need them anyway
  for (fint i = arity() - 1; i >= 0; i--) {
    CountStub* cs = countStub(i);
    if (cs) {
      nmethod* nm = cs->target();
      cs->deallocate();
      deps()[i].rebind(&nm->linkedSends);
      setJumpAddr(i, nm->verifiedEntryPoint());
    }
  }
}
#endif



// setup stuff in anticipation of extending a PIC
void CacheStub::copy_prologue(sendDesc *sd) {

  oldAssembler= theAssembler;   // save current assembler
  theAssembler= NULL;
  a= new Assembler(MaxStubSize, MaxStubSize, false, true);

  n=  NEW_RESOURCE_ARRAY(nmethod*,   MaxPICSize);
  st= NEW_RESOURCE_ARRAY(CountStub*, MaxPICSize);

  theSendDesc= sd;
  newMethods= 0;
}



CacheStub* CacheStub::copy_epilogue() {
  // allocate new stub
  CacheStub* s= new CacheStub;
  theSendDesc->link(s);         // install new stub
  if (VerifyZoneOften) s->verify();
  theAssembler= oldAssembler;
  theSendDesc= NULL;
  return s;
}



// Locate nmethods for the smi and float cases which will be propagated 
// to the copy, and put them and their count stubs in nsmi, nfloat,
// stsmi and stfloat.

void CacheStub::find_immediate_nmethods(nmethod *del) {

  nsmi= nfloat= NULL;  stsmi= stfloat= NULL;
  fint arty= arity();
  if (arty == 0) return;

  nmethod *nm0= get_method(0);
  Map *m0= get_map(0)->map_addr();

  nmethod *nm1= arty == 1 ? NULL : get_method(1);
  Map      *m1= arty == 1 ? NULL : get_map(1)->map_addr();

  assert(!m1->is_smi(), "smallInt case must be first");

  if (m0->is_smi()  &&  nm0 != del) {
    nsmi= nm0;  stsmi= countStub(0);
  }

  if (m0->is_float()  &&  nm0 != del) {
    nfloat= nm0;  stfloat= countStub(0);
  } else if (m1->is_float()  &&  nm1 != del) {
    nfloat= nm1;  stfloat= countStub(1);
  }
}



// This is the body of the PIC copy code.
// Copies the receiver, making a PIC with `total' entries,
// eliminating the case at delIndex (set it negative
// otherwise), or eliminating all cases which are bound to delnm (set it to
// NULL otherwise).
// Adds up to two new cases, a1/m1/s1 (if a1!=NULL), and a2/m2/s2 (a2!=NULL).
// Uses the class statics nsmi, nfloat, stsmi, stfloat.

void CacheStub::gen_copy(fint total, fint delIndex, nmethod *delnm,
                         nmethod *a1, mapOop m1, CountStub *s1,
                         nmethod *a2, mapOop m2, CountStub *s2) {

  assert(!stsmi   || stsmi  ->target() == nsmi,   "found wrong smi stub"  );
  assert(!stfloat || stfloat->target() == nfloat, "found wrong float stub");

  fint immed= (nsmi ? 1 : 0) + (nfloat ? 1 : 0);
  bool immedOnly= total == 0;
  total += immed;
  assert(total > 1, "can't make PIC with less than two entries");

  // generate code
  
# if defined(SIC_COMPILER)
  if (SICCountTypeTests) {
    theAssembler->startTypeTest(total, true, immedOnly);
    if (immed >= 1) {
      theAssembler->doOneTypeTest();
      if (immed == 2) theAssembler->doOneTypeTest();
    }
  }
# endif

  Label* miss= prologue(immedOnly);

  Label* prev= NULL;
  for (fint i= 0; i < arity(); i++) {
    if (i != delIndex) {
      nmethod* nm= get_method(i);
      if (nm != delnm && nm != nsmi && nm != nfloat) {
        prev= generate(nm, get_map(i), countStub(i), prev);
      }
    }
  }
  if (a1) prev= generate(a1, m1, s1, prev);
  if (a2) prev= generate(a2, m2, s2, prev);
  finish(miss, prev);
}


// Can be called with this==NULL to create a PIC

CacheStub* CacheStub::copy_add_nmethod(sendDesc* send_desc,
                                       nmethod* add, mapOop receiverMapOop) {
  ResourceMark rm;

  if (PrintPIC) {
    if (this)
      lprintf("*extending PIC %#lx with nmethod %#lx, map %#lx",
              this, add, receiverMapOop);
    else
      lprintf("*creating PIC with nmethod %#lx, map %#lx for old nmethod %#lx",
              add, receiverMapOop, send_desc->get_method());
  }

  copy_prologue(send_desc);

  // if no existing PIC, fst was the method called from send_desc
  nmethod* fst= this ? NULL : send_desc->get_method();
  Map* mf= fst ? fst->key.receiverMap() : NULL;
  CountStub *stub= send_desc->countStub();
  find_immediate_nmethods(NULL);
  Map* ma= receiverMapOop->map_addr();

  if (ma->is_smi()) {
    nsmi= add; stsmi= NULL; add= NULL;
  } else if (mf->is_smi()) {
    nsmi= fst; stsmi= stub; fst= NULL; 
  }

  if (ma->is_float()) {
    nfloat= add; stfloat= NULL; add= NULL;
  } else if (mf->is_float()) {
    nfloat= fst; stfloat= stub; fst= NULL;
  }

  gen_copy(arity() + (add ? 1 : 0) + (fst ? 1 : 0), -1, NULL,
           add, receiverMapOop, NULL,
           fst, mf->enclosing_mapOop(), stub);

  // Delete current PIC (must do before allocating new one because this might 
  // be moved during allocation of new PIC).
  if (this) {
    // deallocate old cache stub, but preserve count stubs
    deallocate2(true);
  } else if (stub) {
    // unlink count stub from calling sendDesc 
    stub->unlinkFromSendDesc();
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      fint i;
      for (i= 0; i < newMethods && st[i] != stub; i++) ;
      assert(i < newMethods, "should have reused stub somewhere");
    }
#   endif
    stub->setVerifiedEntryPoint(fst);
  } else {
    send_desc->unlink_nmethod();
  }
  
  CacheStub *s= copy_epilogue();
  if (PrintPIC) lprintf(" --> %#lx\n", s);
  return s;
}



// Remove the case at delIndex
CacheStub* CacheStub::copy_remove_nmethod(sendDesc* send_desc, fint delIndex) {
  ResourceMark rm;
  nmethod *delnm= get_method(delIndex);
  if (PrintPIC) lprintf("*removing nmethod %#lx from PIC %#lx", delnm, this);
  copy_prologue(send_desc);
  find_immediate_nmethods(delnm);
  gen_copy(arity() - 1, delIndex);
  CountStub *cs= countStub(delIndex);
  deallocate2(true);
  if (cs) cs->deallocate();
  CacheStub *s= copy_epilogue();
  if (PrintPIC) lprintf("--> %#lx\n", s);
  return s;
}


// Remove all cases bound to nmethod delnm
CacheStub* CacheStub::copy_remove_all(sendDesc* send_desc, nmethod *delnm) {
  ResourceMark rm;

  copy_prologue(send_desc);
  find_immediate_nmethods(delnm);
  fint i, ndel, arty= arity();
  CountStub **delVec= NEW_RESOURCE_ARRAY(CountStub*, arty);
  for (i= ndel= 0; i < arty; i++) {
    nmethod* nm= get_method(i);
    if (nm != delnm) {
      delVec[i]= NULL;
    } else {
      ++ndel;
      delVec[i]= countStub(i);
    }
  }

  if (PrintPIC)
    lprintf("*removing nmethod %#lx from PIC %#lx (%d cases)",
            delnm, this, (void*)ndel);

  gen_copy(arty - ndel, -1, delnm);
  deallocate2(true);
  for (i= 0; i < arty; i++)
    if (delVec[i]) delVec[i]->deallocate();
  CacheStub* s= copy_epilogue();
  if (PrintPIC) lprintf("--> %#lx\n", s);
  return s;
}


// Remove the case at index delIndex.  Add add/receiverMapOop, which
// is an immediate case (smi/float).
CacheStub* CacheStub::copy_replace_immediate(sendDesc* send_desc,
                                             fint delIndex,
                                             nmethod* add,
                                             mapOop receiverMapOop) {
  ResourceMark rm;
  nmethod *delnm= get_method(delIndex);
  copy_prologue(send_desc);
  find_immediate_nmethods(delnm);
  Map* ma= receiverMapOop->map_addr();
  if (ma->is_smi()) {
    nsmi= add; stsmi= NULL;
  } else if (ma->is_float()) {
    nfloat= add; stfloat= NULL;
  } else {
    fatal("should only be adding immediate case");
  }
  gen_copy(arity(), delIndex);
  CountStub *cs= countStub(delIndex);
  deallocate2(true);
  if (cs) cs->deallocate();
  return copy_epilogue();
}


// unlink method at l from this cache (all cases)
NCodeBase* CacheStub::unlink_me(nmln *l) { 
#if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  this == (CacheStub*)catchThisOne) warning("caught cache stub");
#endif
  fint index= indexOfDep(l);
  nmethod *delnm= get_method(index);
  fint nremaining= arity() - nOccurrences(delnm);
  if (nremaining == 0) {
    // clear the inline cache
    deallocate();
    return NULL;
  }
  if (nremaining > 1)
    // make new PIC for remaining 2 or more entries
    return copy_remove_all(sd(), delnm);

  // install last remaining entry in inline cache
  fint remain;
  for (remain= 0;  get_method(remain) == delnm;  remain++)
    assert(remain < arity(), "ran off end of PIC");
  replace_with_inline_cache(remain);
  return NULL;
}


// replace PIC with inline cache of i'th case
void CacheStub::replace_with_inline_cache(fint i)
{
  NCodeBase *remainingThing= get_thing(i);
  nmethod *remainingMeth;
  CountStub *cs;
  if (remainingThing->isNMethod()) {
    remainingMeth= (nmethod*)remainingThing;
    cs= NULL;
  } else {
    cs= (CountStub*)remainingThing;
    remainingMeth= cs->target();
  }
  mapOop remainingMap= get_map(i);
  sendDesc* s= sd();
  fint arty= arity();
  if (PrintPIC)
    lprintf("*deallocating %d-element PIC %#lx\n", (void*)arty, this); 
  fint j;
  CountStub **delVec= NEW_RESOURCE_ARRAY(CountStub*, arty);
  for (j= 0; j < arty; j++)
    delVec[j]= i == j ? NULL : countStub(j);
  deallocate2(true);
  for (j= 0; j < arty; j++)
    if (delVec[j]) delVec[j]->deallocate();
  s->extend(remainingMeth, remainingMap, cs);
}

#ifdef UNUSED
// unlink case at l from this cache
NCodeBase* CacheStub::unlink_one(nmln* l) {
#if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  this == (CacheStub*)catchThisOne) warning("caught cache stub");
#endif
  fint index= indexOfDep(l);
  nmethod *delnm= get_method(index);
  fint nremaining= arity() - 1;
  assert (nremaining > 0, "1-element PIC?");
  if (nremaining == 1) {
    // install last remaining entry in inline cache
    assert(arity() == 2, "bizarre case in PIC contraction");
    replace_with_inline_cache(1 - index);
    return NULL;
  }
  // make new PIC for remaining 2 or more entries
  CacheStub* s=  copy_remove_nmethod(sd(), index);
  return s;
}
#endif

void CacheStub::moveTo_inner(NCodeBase* p, int32 delta, int32 size) {
  CacheStub *to= (CacheStub*)p;
  OopNCode::moveTo_inner(to, delta, size);
  // currently, the instructions are all position-independent (at least on
  // the sparc), but we need to adjust the deps
  if (cacheLink.notEmpty())
    sd()->shift_jump_addr(delta);
  cacheLink.shift(delta);
  for (nmln* l = deps(), *end = depsEnd(); l < end; l++)
    l->shift(delta, this);
}
  

void CacheStub::deallocate2(bool dontDeallocateStubs) {
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  this == (CacheStub*)catchThisOne) warning("caught cache stub");
# endif
  for (nmln* l = deps(); l < depsEnd(); l++) l->remove();
  rememberLink.remove();

  sendDesc* send_desc = sd();
  send_desc->reset_jump_addr();
  cacheLink.remove();
  assert(send_desc->dependency()->isEmpty(), "should be empty now");
  for (fint i = arity() - 1; i >= 0; i--) {
    CountStub* cs = countStub(i);
    if (cs) {
      if (dontDeallocateStubs) {
        cs->unlinkFromSendDesc();
      } else {
        cs->deallocate();
      }
    }
  }
  MachineCache::flush_instruction_cache_for_debugging();
  fint s = size();
  kill_vtbl_value();        // to make isCacheStub fail on me
  Memory->code->stubs->deallocate(this, s);
}

#ifdef UNUSED
int32 CacheStub::flush() {              // try to flush this stub
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  this == (CacheStub*)catchThisOne) warning("caught cache stub");
# endif
  assert(zone::frame_chain_nesting > 0, "frames must be chained");
  nmethod* myMethod = nmethod::findNMethod(sd());
  if (myMethod->frame_chain != NoFrameChain) return 0;
  int32 s = size();
  deallocate();
  return s;
}
#endif

// The target at the branch with nmln l is being shifted by delta;
// update jump address
void CacheStub::shift_target(nmln* l, int32 delta) {
  addrDesc *loc= getJumpLoc(indexOfDep(l));
  pc_t jumpAddr= loc->referent(this);
  loc->set_referent(this, jumpAddr + delta);  
}

void CacheStub::forwardLinkedSend(nmln* l, nmethod* to) {
  fint index= indexOfDep(l);
  CountStub *cs= countStub(index);
  rebind(index, to, cs && to->isYoung() ? cs : NULL);
  if (cs && !to->isYoung()) {
    // discard count stub and make
    cs->deallocate();
  }
}

bool CacheStub::verify() {
  bool r = true;
  r &= OopNCode::verify2("cache stub");

  if (insts() != (pc_t)(this + 1)) {
    error1("cache stub at %#lx has incorrect insts pointer", this);
    r = false;
  }
  
  if (!Memory->code->stubs->contains(this)) {
    error1("cache stub at %#lx not in zone", this);
    r = false;
  }
  
  if (!Memory->code->contains(sd())) {
    error1("cache stub at %#lx: sendDesc not in zone", this);
    r = false;
  } else if (sd()->pic() != this) {
    error1("cache stub at %#lx: sendDesc doesn't point back to me", this);
    r = false;
  }
  if (cacheLink.next != cacheLink.prev) {
    error1("cache stub %#lx: more than one element in cacheLink", this);
    r = false;
  }
  
  if (arity() < 0 || arity() > MaxPICSize) {
    error2("cache stub at %#lx: invalid arity", this, arity());
    r = false;
  }
  else if (isMegamorphic() && arity() < MaxPICSize) {
    error1("cache stub at %#lx: wrong isMegamorphic flag", this);
    r = false;
  }
  
  if (instsLen() < 3*oopSize || instsLen() > 1000) {
    error2("cache stub at %#lx: invalid instsLen", this, instsLen());
    r = false;
  }
  
  fint lLen = realLocsLen(arity(), hasSmi(), hasFloat());
  if (lLen != locsLen()) {
    error3("cache stub at %#lx: wrong number of locs %ld (should be %ld)",
           this, locsLen(), lLen);
    r = false;
  }
  
  for (fint i = 0; i < arity(); i++) {
    nmethod* nm = get_method(i);
    CountStub* cs = countStub(i);
    if (sd()->isCounting()) {
      // when a method is replaced by an old SIC method, there is no count stub
      // if (cs == NULL) error2("cache stub at %#lx, branch %ld: has no count stub", this, i);
    } else if (cs != NULL && !cs->isAgingStub()) {
      error2("cache stub at %#lx, branch %ld: has count stub", this, i);
      r = false;
    }

    if (cs) {
      r &= cs->verify2(this);
    } else {
      if (!sd()->checkLookupTypeAndEntryPoint(nm, getJumpAddr(i))) {
        error2("cache stub at %#lx, branch %ld: wrong lookup type or entry point",
               this, i);
        r = false;
      }
      bool found = false;
      for (nmln* e = &nm->linkedSends, *d = e->next; d != e; d = d->next) {
        if (encompasses(d)) {
          found = true;
          break;
        }
      }
      if (! found) {
        error2("cache stub at %#lx, branch %ld: not linked to nmethod",
               this, i);
        r = false;
      }
    }

    Map* rmap = nm->key.receiverMap();
    if (rmap != Memory->smi_map &&
        rmap != Memory->float_map) {
      mapOop receiverMapOop= get_map(i);
      assert(receiverMapOop->is_map(), "unexpected kind of map location");
      if (!ReuseNICMethods && rmap != receiverMapOop->map_addr()) {
        error4("cache stub %#lx, branch %ld: wrong map (%#lx instead of %#lx)",
               this, i, receiverMapOop->map_addr(), rmap);
        r = false;
      }
    }
    if (!ReuseNICMethods) {
      for (fint j = 0; j < i; j++) {
        nmethod* nm1 = get_method(j);
        if (nm == nm1) {
          error4("cache stub %#lx: duplicate nmethod %#lx (branch %ld & %ld)",
                 this, nm, j, i);
          r = false;
        }
        if (nm->key.receiverMapOop() == nm1->key.receiverMapOop()) {
          error4("cache stub %#lx: duplicate map %#lx (branch %ld & %ld)",
                 this, nm->key.receiverMapOop(), j, i);
          r = false;
        }
      }
    }
  }
  
  if (!ReuseNICMethods) {
    Map* m0 = (arity() > 0 ? get_method(0)->key.receiverMap() : NULL);
    Map* m1 = (arity() > 1 ? get_method(1)->key.receiverMap() : NULL);
    if (hasSmi() && (m0 == NULL || !m0->is_smi())) {
      error1("cache stub at %#lx: wrong hasSmi flag", this);
      r = false;
    }
    if (hasFloat() &&
        (m0 == NULL || !m0->is_float()) && (m1 == NULL || !m1->is_float())) {
      error1("cache stub at %#lx: wrong hasFloat flag", this);
      r = false;
    }
  }
  if (findCacheStub((pc_t) instsEnd() - oopSize) != this) {
    error1("findCacheStub didn't find this stub (0x%lx)", this);
    r = false;
  }
  return r;
}

sendDesc* CacheStub::sd() {  
  assert(cacheLink.notEmpty(), "no sd");
  return sendDesc::sendDesc_from_nmln(cacheLink.next);
}
nmethod* CacheStub::sender() { return nmethod::findNMethod(cacheLink.next); }

void CacheStub::print() {
  printIndent();
  lprintf("cache stub at %#lx (arity %ld); flags = { ",
         this, long(arity()));
  if (hasSmi()) lprintf("smi ");
  if (hasFloat()) lprintf("float ");
  if (isMegamorphic()) lprintf("megamorphic ");
  lprintf("}\n");
  Indent ++;
  printIndent();
  lprintf("sendDesc: %#lx selector: ", sd());
  sd()->selector()->print_real_oop();
  lprintf("\n");
  printIndent();
  lprintf("instructions: x/%ldi %#lx\n", (void*)long(instsLen() / oopSize),
         insts());

  printIndent();
  lprintf("           maps: ");
  fint i;
  for (i = 0; i < arity(); i++) {
    lprintf("%#lx ", get_map(i));
  }
  lprintf("\n");

  printIndent();
  lprintf("  linked things: ");
  for (i = 0; i < arity(); i++) {
    lprintf("%#lx ", get_thing(i));
  }
  lprintf("\n");

  printIndent();
  lprintf("linked nmethods: ");
  for (i = 0; i < arity(); i++) {
    lprintf("%#lx ", get_method(i));
  }
  lprintf("\n");

  Indent--;
}

bool isCacheStub(void* p) {
  return ((CacheStub*)p)->vtbl_value() == ((CacheStub*)NULL)->static_vtbl_value();
}

NCodeBase* findStub(void* addr) {
  NCodeBase* s =
    (NCodeBase*)Memory->code->stubs->zone()->findStartOfBlock(addr);
  assert(isCacheStub(s) || CountStub::isCountStub(s), "what is it?");
  return s;
}

CacheStub* findCacheStub(void* addr) {
  NCodeBase* s = findStub(addr);
  assert(s->isCacheStub(), "not a cache stub");
  assert(((CacheStub*)s)->encompasses(addr), "found wrong stub");
  return (CacheStub*)s;
}

CacheStub* findCacheStub_maybe(void* start) {
  // start may point to code part of a cache stub; return that stub or NULL
  if (!Memory->code->stubs->zone()->contains(start)) return NULL;
  // relies on FOR_ALL_STUBS to enumerate in ascending order
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    if ( p->insts()    > (pc_t)start) return NULL;
    if ( p->instsEnd() > (pc_t)start) return p;
  }
  return NULL;
}

# if  GENERATE_DEBUGGING_AIDS
CacheStub* StubFromNmln(nmln* l) {
  // given the cacheLink nmln, return the PIC
  assert(Memory->code->stubs->contains(l), "not in PIC zone");
  CacheStub* s = NULL;
  fint offset = (pc_t)&s->cacheLink - (pc_t)s;
  s = (CacheStub*)((pc_t)l - offset);
  assert(s->vtbl_value() == s->static_vtbl_value(), "not a PIC");
  return s;
}
#endif

const fint ReserveFree     = 16 * K;    // size of emergency allocation reserve

Stubs::Stubs(Heap* s) {
  stubZone = s; needsWork = false;
  reserve = s->allocate(ReserveFree);
}

void Stubs::gc_mark_contents() {
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    p->gc_mark_contents();
  }
}

bool Stubs::gc_unmark_contents() {
  bool needToInvalCache = false;
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    if (p->gc_unmark_contents()) needToInvalCache = true;
  }
  return needToInvalCache;
}

bool Stubs::code_oops_do(oopsDoFn f) {
  bool needToInvalCache = false;
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    if (p->code_oops_do(f)) needToInvalCache = true;
  }
  return needToInvalCache;
}

bool Stubs::switch_pointers(oop from, oop to,
                            nmethodBList* nmethods_to_invalidate) {
  bool needToInvalCache = false;
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    if (p->switch_pointers(from, to, nmethods_to_invalidate))
      needToInvalCache = true;
  }
  return needToInvalCache;
}

#ifdef UNUSED
void Stubs::relocate() {
  FOR_ALL_STUBS(_p) {
    CHECK_PIC(_p, p);
    p->relocate();
  }
}
#endif

void Stubs::decay(float factor) {
  FOR_ALL_STUBS(_p) {
    if (_p->isCountStub()) {
      CountStub* p = (CountStub*)_p;
      p->set_count(int(p->count() / factor));
    }
  }
}

void printAllStubs() {
  FOR_ALL_STUBS(p) { p->print(); }
}
    
static void moveStubs(pc_t from, pc_t to, int32 size) {
  ((NCodeBase*)from)->moveTo((NCodeBase*)to, size);
}

const fint MinFree     = 8 * K;         // desired min bytes free (absolute)
const fint MinFreePerc = 10;            // desired min bytes free (%)
const fint MaxFreePerc = 20;            // max. % free before doubling size

void* Stubs::allocate(int32 size) {
  void* p = stubZone->allocate(size);
  int32 nfree = stubZone->freeBytes();
  if (p == NULL ||
      nfree < MinFree || nfree * 100 / stubZone->capacity() < MinFreePerc) {
    LOG_EVENT1("stubs low on space: %ld bytes left", nfree);
    currentProcess->setupPreemption();
    needsWork = true;
  }
  if (p == NULL) {
    // NB: cannot grow / compact here - callers would have to guard against
    // moved "this" pointers etc.
    if (reserve) {
      assert((verify(), true), "bad zone before freeing reserve");
      stubZone->deallocate(reserve, ReserveFree);
      reserve = NULL;
      assert((verify(), true), "bad zone after freeing reserve");
      LOG_EVENT("stubs: freeing reserve");
      p = stubZone->allocate(size);

      // cannot verify stubs, because one is allocated, but not initialized
      assert((stubZone->verify(), true), "bad zone after using reserve");
    }
    if (p == NULL) fatal("couldn't allocate in PIC zone");
  }
  return p;
}

void Stubs::cleanup() {
  // PICs / CountStubs are fairly uniform in size, so fragmentation should be
  // negligible; only compact if > MaxFree % space free, or if emergency
  // happened
  needsWork = false;
  if (reserve == NULL ||
      (stubZone->freeBytes() * 100 / stubZone->capacity() > MaxFreePerc &&
      stubZone->freeBytes() > MinFree)) {
    LOG_EVENT("compacting PIC zone");
    if (PrintCodeReclamation) {
      lprintf("*compacting PIC zone\n");
    }
    if (reserve) {
      assert((verify(), true), "bad zone before freeing reserve");
      stubZone->deallocate(reserve, ReserveFree);
      reserve = NULL;
    }
    assert((verify(), true), "bad zone before compaction");
    stubZone->compact(moveStubs);
    MachineCache::flush_instruction_cache_for_debugging();
    reserve = stubZone->allocate(ReserveFree);
    if (reserve) {
      assert((verify(), true), "bad zone after compaction");
      return;
    }
  }
  // grow the zone
  int32 newSize = stubZone->capacity() * 2;
  LOG_EVENT1("growing PIC zone to %ld bytes", newSize);
  if (WizardMode) {
    warning1("PIC code area overflowed - growing it to %ld bytes.", newSize);
    printIndent(); lprintf("Current zone: "); stubZone->print();
    space_print();
  }
  assert((verify(), true), "bad zone before growth");
  Heap* newHeap = new Heap(stubZone->capacity() * 2,
                           stubZone->blockSize, stubZone->nfree);
  stubZone->newHeap = newHeap;
  FOR_ALL_STUBS(s) {
    void* to = newHeap->allocate(s->size());
    assert(to, "couldn't allocate destination PIC");
    s->moveTo((NCodeBase*)to, newHeap->sizeOfBlock(to));
  }
  MachineCache::flush_instruction_cache_for_debugging();
  delete stubZone;
  stubZone = newHeap;
  reserve = stubZone->allocate(ReserveFree);
  TheSpy->adjust_after_resize();
  assert((verify(), true), "bad zone after growth");
}

void Stubs::clear() {
  stubZone->clear();
  reserve = stubZone->allocate(ReserveFree);
}

bool Stubs::verify() {
  bool r = true;
  r &= stubZone->verify();
  NCodeBase* last_s = NULL; // for debugging
  FOR_ALL_STUBS(s) {
    r &= s->verify();
    last_s = s;
  }
  return r;
}

void Stubs::flush() {
  NCodeBase* next;
  if (reserve) stubZone->deallocate(reserve, ReserveFree);
  reserve = NULL;
  NCodeBase* s = (NCodeBase*)stubZone->firstUsed();
  // Don't use FOR_ALL_STUBS for efficiency (in combining mode, flushed
  // stub might be combined with neighbors -> have to scan zone from
  // beginning to find next stub).
  for ( ; s; s = next) {
    next = (NCodeBase*)stubZone->nextUsed(s);
    if (s->isCountStub() && ((CountStub*)s)->sd() == NULL) {
      // this count stub is linked to a PIC; don't deallocate before 
      // the PIC is deallocated
    } else {
      s->deallocate();
      if (!next || isCacheStub(next) || CountStub::isCountStub(next)) {
        // ok
      } else {
        // next was a count stub called by PIC s, i.e. it has been deallocated
        next = (NCodeBase*)stubZone->firstUsed();
      }
    }
  }
  reserve = stubZone->allocate(ReserveFree);
}

void Stubs::space_print() {
  FlagSetting fs(PrintVMMessages, true);
  int32 pic = 0, npic = 0, count = 0, ncount = 0, aging = 0, naging = 0;
  FOR_ALL_STUBS(s) {
    if (s->isCacheStub()) {
      npic++; pic += s->size();
    } else if (s->isCountStub()) {
      ncount++; count += s->size();
    } else if (s->isAgingStub()) {
      naging++; aging += s->size();
    } else {
      fatal(" unexpected kind of stub");
    }
  }
  int32 total = pic + count + aging;
  int32 ntotal = npic + ncount + naging;
#define PRINT(name, cnt, size)                                                \
  { \
    float f1, f2; \
    f1 = 100.0 * cnt/ntotal; \
    f2 = 100.0 * size / total; \
    lprintf("%s: %d (%4.1f) %d bytes (%4.1f)\n", name, \
            (void*)cnt, *(void**)&f1, \
            (void*)size, *(void**)&f2); \
  }
  PRINT("PICs", npic, pic);
  PRINT("counters", ncount, count);
  PRINT("aging counters", naging, aging);
}

# define FOR_ALL_NMETHODS(var)                                                \
    for (nmethod *var = Memory->code->first_nm();                             \
    var; var = Memory->code->next_nm(var))


# endif  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

oop pic_histogram_prim(oop rcvr) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  ResourceMark rm;
  fint max = MaxPICSize * 2;    // just to be safe
  smi *count= NEW_RESOURCE_ARRAY(smi, max + 1);
  smi *noblocks = NEW_RESOURCE_ARRAY(smi, max + 1);
  set_oops((oop*)count,    max + 1, 0);
  set_oops((oop*)noblocks, max + 1, 0);
  fint npics = 0, ncounting = 0;

  fint nmethods = 0, nsds = 0;
  FOR_ALL_NMETHODS(nm) {
    nmethods++;
    for (addrDesc* a = nm->locs(), *aend = nm->locsEnd(); a < aend; a++) {
      if (a->isSendDesc()) {
        nsds++;
        sendDesc* sd = a->asSendDesc(nm);
        fint arity = sd->ntargets();
        assert(0 <= arity && arity < MaxPICSize, "bad arity");
        count[arity]++;
        switch (arity) {
          case 0: break;
          case 1: {
            Map *rcvrMap= sd->get_method()->key.receiverMap(); // best guess xxx miw
            noblocks[rcvrMap->is_block() ? 1 : 0]++;
            break;
          }
          default: {
            npics++;
            if (sd->isCounting()) ncounting++;
            // count non-block receivers
            CacheStub *pic= sd->pic();
            fint n= 0;
            for (fint i= 0; i < pic->arity(); i++) {
              if (pic->get_map(i)->map_addr()->is_block()) n++;
            }
            noblocks[n]++;
            break;
          }
        }
      }
    }
  }
  
  lprintf("%d nmethods, %d sendDescs (%d pics, %d counting)\n",
         nmethods, nsds, npics, ncounting);
  noblocks[0] = count[0];
  for (fint i = 0; i < max; i++) {
    if (count[i] + noblocks[i] > 0)
      lprintf("%2d: %5d %5d\n", i, count[i], noblocks[i]);
  }

# endif  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

  return rcvr;
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "abstract_interpreter.hh"
# pragma implementation "abstract_interpreter_inline.hh"
# include "_abstract_interpreter.cpp.incl"


void abstract_interpreter_method_info::init(byteVectorOop c, objVectorOop l) {
     codes_object = c;
  literals_object = l;
  codes           = (unsigned char*)codes_object->bytes();
  literals        = literals_object->objs();
  length_codes    =    codes_object->length();
  length_literals = literals_object->length();
  
  if (codes_object->length() == 0)
    instruction_set = TWENTIETH_CENTURY_INSTRUCTION_SET;
  else {
    char first_code = codes_object->byte_at(0);
    instruction_set =     getOp((u_char)first_code) == INSTRUCTION_SET_SELECTION_CODE
                        ? (InstructionSetKind) getIndex(first_code)
                        : TWENTIETH_CENTURY_INSTRUCTION_SET;
    assert( instruction_set == TWENTIETH_CENTURY_INSTRUCTION_SET
        ||  0 <= instruction_set  &&  instruction_set <= LAST_INSTRUCTION_SET,
        "bad instruction set");
  }    
}



abstract_interpreter_method_info::abstract_interpreter_method_info(
                                                              methodMap *m)  {
  assert(m->has_code(), "cannot interpret data");
  _map_oop       = m->enclosing_mapOop();
  init(m->codes(), m->literals());
}    


abstract_interpreter_method_info::abstract_interpreter_method_info(
                                         byteVectorOop codes,
                                         objVectorOop literals) {
  _map_oop= mapOop(badOop);
  init(codes, literals);
}    


void abstract_interpreter_method_info::print_short() {
  lprintf("method_map %#lx, "
         "codes %#lx, length %d,  literals %#lx, length %d\n",
         map(), 
         codes,         length_codes,
         literals,      length_literals);
}


void abstract_interpreter_bytecode_info::print_short() {
  lprintf( "code %d, op %d, index %d\n", code, op, x);
}


void abstract_interpreter_interbytecode_state::print_short() {
  lprintf( "lexical_level %d, index %d,"
          " delegatee %#lx, is_undirected_resend %s,"
          " argument_count %d\n", 
          lexical_level, index, 
          (unsigned long)delegatee, 
          is_undirected_resend ? "true" : "false",
          argument_count);
}




void abstract_interpreter::print_short() {
  lprintf( "pc %d\n", pc);
  mi.print_short();
  bc.print_short();
  is.print_short();
}



void abstract_interpreter::interpret_method() {
  for ( ;  pc < mi.length_codes;  ++pc ) {
    interpret_bytecode();
    if ( get_error_msg() )
      return;
  }
}

void abstract_interpreter::fetch_and_decode_bytecode() {
  bc.decode(mi.codes[pc]);

  // when asserts are turned on, it is illegal to carry
  // a non-zero index during NO_OPERAND_CODE's.
  // So the following predicate is technically not needed.
  // But I don't want illegal programs doing strange things
  // with asserts off, so put it in anyway -- dmu.
  if ( bc.op != NO_OPERAND_CODE ) 
    is.index = (is.index << INDEXWIDTH) | bc.x;
}

#   define interpret(opExpr) \
      CONC( pre_,opExpr)(); \
      CONC(  do_,opExpr)(); \
      CONC(post_,opExpr)();

# define case_op(opName) \
  case opName: interpret(opName)
  
void abstract_interpreter::dispatch_bytecode() {
  switch (bc.op) {
   default: interpret(illegal_code);               break;
   case_op(LEXICAL_LEVEL_CODE);                    break;
   case_op(READ_LOCAL_CODE);                       break;
   case_op(WRITE_LOCAL_CODE);                      break;
   case_op(INDEX_CODE);                            break;
   case_op(LITERAL_CODE);                          break;
   case_op(DELEGATEE_CODE);                        break;
   case_op(ARGUMENT_COUNT_CODE);                   break;
   case_op(SEND_CODE);                             break;
   case_op(IMPLICIT_SEND_CODE);                    break;
   
   case_op(BRANCH_CODE);                           break;
   case_op(BRANCH_TRUE_CODE);                      break;
   case_op(BRANCH_FALSE_CODE);                     break;
   case_op(BRANCH_INDEXED_CODE);                   break;
   
   case     NO_OPERAND_CODE:
    switch (bc.x) {
     default: interpret(illegal_no_operand_code);  break;
     case_op(SELF_CODE);                           break;
     case_op(POP_CODE);                            break;
     case_op(UNDIRECTED_RESEND_CODE);              break;
     case_op(NONLOCAL_RETURN_CODE);                break;
    }
    break;
  }
} 



void abstract_interpreter::do_LITERAL_CODE() { 
 do_literal_code( get_literal()); 
}



bool abstract_interpreter_method_info::verify() {
  if (_map_oop->verify_oop())
    ;
  else {
    error1("bad oop in abstract_interpreter_method_info 0x%x", this);
    return false;
  }
  if ( codes           == (unsigned char*)map()->codes()->bytes()
  &&   length_codes    == map()->codes()->length()
  &&   literals        == map()->literals()->objs()
  &&   length_literals == map()->literals()->length() )
    ;
  else {
    error1("inconsistency in abstract_interpreter_method_info 0x%x", this);
    return false;
  }
  if (!literals_object->is_objVector()) {
    error2("literals_object 0x%x in "
           "abstract_interpreter_method_info 0x%x not objVector",
           literals_object,
           this);
    return false;
  }
  if (!codes_object->is_byteVector()) {
    error2("codes_object 0x%x in "
           "abstract_interpreter_method_info 0x%x not byteVector",
           codes_object,
           this);
    return false;
  }
  return true;
}
  

bool abstract_interpreter::verify() {
  return mi.verify();
}


void abstract_interpreter::check_index_range(abstract_interpreter *ai, oop) {
  if ( ai->is.index < ai->mi.length_literals ) return;
  ai->set_error_msg( "index out of bounds");
}

void abstract_interpreter::check_selector_string(abstract_interpreter *ai, oop s) {
  if ( s->is_string() ) return;
  ai->set_error_msg( "selector must be a string");
}

void abstract_interpreter::check_branch_target(abstract_interpreter *ai, oop p) {
  ai->check_branch_target(p);
}

void abstract_interpreter::check_no_send_modifiers(abstract_interpreter *ai, oop) {
  ai->set_error_msg( ai->is.check_no_send_modifiers());
}

void abstract_interpreter::check_no_lexical_level(abstract_interpreter *ai, oop) {
  ai->set_error_msg( ai->is.check_no_lexical_level());
}

void abstract_interpreter::check_no_two_send_modifiers(abstract_interpreter *ai, oop) {
  ai->set_error_msg( ai->is.check_no_two_send_modifiers());
}

void abstract_interpreter::check_no_operand(abstract_interpreter *ai, oop) {
  ai->set_error_msg( ai->is.check_no_operand());
}

void abstract_interpreter::check_delegatee(abstract_interpreter *ai, oop) {
  oop p= ai->get_literal();
  Unused(p); //debugging
  if ( !ai->error_msg  &&  !ai->get_literal()->is_string())
    ai->set_error_msg( "delegatee must be string"); 
}

void abstract_interpreter::check_no_argument_count(abstract_interpreter *ai, oop) {
  if (ai->is.argument_count != 0)
    ai->set_error_msg( "should not have argument count before argument count setter");
}

void abstract_interpreter::check_branch_vector(abstract_interpreter *ai, oop) {
  oop p= ai->get_literal();
  if (ai->error_msg)  return;
  if (!p->is_objVector()) {
    ai->set_error_msg( "branch vector must be object vector");
    return;
  }
  objVectorOop v= objVectorOop(p);
  for (int32 i = 0;  i < v->length();  ++i) {
    oop p= v->obj_at(i);
    check_branch_target(ai, p);
    if (ai->error_msg) return;
  }
}

void abstract_interpreter::check_for_pop(abstract_interpreter *ai, oop n) {
  ai->check_for_pop(n);
}

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stacking_interpreter.hh"
# include "_stacking_interpreter.cpp.incl"


void stacking_interpreter::do_send_code( bool isSelfImplicit, 
                                            stringOop /*selector*/,
                                            fint arg_count ) {
  if ( !get_error_msg()
  &&    check_and_pop( arg_count + !isSelfImplicit))
    push(); // for result
}
/* Sun-$Revision: 30.14 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "interpreter.hh"
# include "_interpreter.cpp.incl"


oop sneaky_method_argument_to_interpret;


inline frame* interpreter::block_scope_or_NLR_target() {
  return _block_scope_or_NLR_target 
    ?  _block_scope_or_NLR_target
    :  _block_scope_or_NLR_target
         = currentProcess->last_self_frame(true)->block_scope_of_home_frame();
}

  
// called as a result of inline cache miss in compiled code

oop interpret_from_compiled_sender() {
  // need to invoke sneaky method_argument_to_interpret
  // args are passed in in registers, etc
  fatal("XXX not implemented yet (interp from compiled IC miss)");
  return smiOop_zero;
}
  

void InterpreterLookup_cont( simpleLookup *L, int32 arg_count) {
  L->perform_full_lookup_n(arg_count);
}


inline interpreter::interpreter( oop rcv,
                                 oop sel,
                                 oop del,
                                 oop meth,
                                 oop _mh,
                                 oop* _args,
                                 int32 _nargs)
: abstract_interpreter(meth) {
  
  assert( meth->is_method_like()  &&  !meth->is_assignment(),
          "method must be method");

  
  receiver= rcv;
  selector= sel;
  delegatee= del;
  method_object = meth;
  set_methodHolder( _mh);
  args= _args;
  length_args= _nargs;
  rcvToSend= rcv;
  selToSend= VMString[VALUE]; // just a placeholder
  return_patch_reason= not_patched;
  current_primDesc = NULL;
  _block_scope_or_NLR_target = NULL;
  
  if (mi.map()->kind() == OuterMethodType) {
    setup_for_method();
  } 
  else {
    assert( receiver->is_block_with_code(), "receiver must be block" );
    setup_for_block();
  }

    
  // init non-const locals
  // because there is no good way to find the min offset, must
  //   find it here--argh! XXXX
  // must do this AFTER setup_for_block/method cause it may set hasParentLocalSlot
  
  minOffset = smiOop_max->value();
  { // macro needs scope
    FOR_EACH_SLOTDESC(mi.map(), sd) {
      if (sd->is_obj_slot()) {
        minOffset = min(minOffset, smiOop(sd->data)->value());
      }
      if ( !sd->is_vm_slot() && sd->is_parent() )
        hasParentLocalSlot = true;
    }
  }
  if (minOffset == smiOop_max->value())
    minOffset = 0; // so range checks can fail properly

  _length_locals= mi.map()->length_obj_slots();
}


inline int32 interpreter::length_cloned_blocks() { return mi.length_literals; }

inline void  interpreter::set_cloned_blocks(void* p) { 
  cloned_blocks= (oop*)p; 
  for (int32 i = 0;  i < length_cloned_blocks();  ++i)   
    cloned_blocks[i] = NULL;
}


// code length is good guess for stack size

inline void  interpreter::set_stack(void* p) { stack= (oop*)p; }


inline void interpreter::set_locals(void* p) {
  locals = (oop*)p;
  { // macro FOR_EACH_SLOTDESC needs the new C-scope ({})
    FOR_EACH_SLOTDESC(mi.map(), sd) {
      if (sd->is_obj_slot()) {
        set_slot(sd, method_object->get_slot(sd));
      }
    }
  }
}
  

// top-level interpreter routine
// Note: callers must arrange for _args to be scavenged, etc

oop interpret( oop rcv,
               oop sel,
               oop del,
               oop meth,
               oop _mh,
               oop* _args,
               int32 _nargs) {
  
  interpreter interp(rcv, sel, del, meth, _mh, _args, _nargs);

  interp.set_cloned_blocks( alloca(interp.length_cloned_blocks() * sizeof(oop)));
  interp.set_stack (        alloca(interp.length_stack()         * sizeof(oop)));
  interp.set_locals(        alloca(interp.length_locals()        * sizeof(oop)));

  ((interpreter*)save1Arg(&interp))->interpret_method();
  return interp.top();
}


void interpreter::setup_for_method() {
  self = receiver;
  hasParentLocalSlot = false;
  parentI = NULL;
}


void interpreter::setup_for_block() {
  blockOop b = blockOop(receiver);
  if ( ! b->is_live() ) {
    currentProcess->nonLifoError();
  }
  interpreter* p;
  p = b->scope()->get_interpreter_of_block_scope();
  if (p) { // parent is interp
    parentI = p;
    self = p->self;
    _methodHolder = p->methodHolder();
    // if any enclosing methods (or me) have parents, need to use slower lookup
    hasParentLocalSlot = p->hasParentLocalSlot;
  }
  else {
    parentI = NULL;
    ResourceMark rm; // for vf
    abstract_vframe* vf = parentVF();
    self = vf->self();
    _methodHolder = vf->methodHolder_object();
    hasParentLocalSlot = true; // be conservative
  }
}



void interpreter::interpret_method() {
  
  do {
    sp = 0;
    pc = PrologueBCI;
  
    if (fastPreemptionCheck() /* || PendingSelfSignals::are_any_pending() XXX right? */ ) {
      // save non vol regs cause ctrl C causes abort which NLR's 
      //  through c frames -- dmu 1/96
      SaveNonVolRegsAndCall0(interruptCheck);
      if (NLRSupport::have_NLR_through_C()) {
        continue_NLR();
        stack[sp++] = NLRSupport::NLR_result_from_C();
        pc= return_pc();
      }
    }
    pc = mi.firstBCI();
    abstract_interpreter::interpret_method();
  } while ( pc == restart_pc() + 1); // interpret_method incremented it

  // zap blocks
  for ( oop* cb = cloned_blocks;  
        cb < cloned_blocks + mi.length_literals;  
        cb++ ) {
    if (*cb != NULL) {
      assert_block(*cb, "must be a block");
      blockOop(*cb)->kill_block();
    }
  }
}


void interpreter::do_SELF_CODE()  { stack[sp++]= self; }

void interpreter::do_POP_CODE()  { --sp; assert(sp >= 0, "too many pops"); }

void interpreter::do_NONLOCAL_RETURN_CODE() {
  start_NLR(stack[sp - 1]);
  pc= return_pc();
}


void interpreter::do_branch_code( int32 target_PC, oop target_oop ) {
  if ( target_oop != badOop ) { // conditional
    assert(sp > 0, "conditional branch needs stack element");
    if ( stack[--sp] != target_oop )
      return;
  }
  pc = target_PC;
}


void interpreter::do_BRANCH_INDEXED_CODE() {
  assert(sp > 0, "indexed branch needs stack element");
  oop p = stack[--sp];
  if (!p->is_smi())
    return;
  objVectorOop v= get_branch_vector();
  int32 index= smiOop(p)->value();
  if ( 0 <= index  &&  index <= v->length())
    ;
  else
    return;
  
  oop npco= v->obj_at(index);
  pc = smiOop(v->obj_at(smiOop(npco)->value()))->value();
}
 
 
void interpreter::do_literal_code(oop lit) {
  if (lit->is_block()) {
    oop cb = cloned_blocks[is.index];
    if (cb == NULL ) {
      // When mix w/ compiled code may need to clone_and_set_desc(smiOop(0))
      //  to give it new map to avoid false cache hits
      //  No, I think it's OK, cause the COMPILERS change the map
      // Note that this half-frame is recognizable as interp frame,
      //  AND is exactly the right half-frame for getting whole-frame
      //  of top interp frame.
      // If change this, must also change methods in vframe.c

      // XXX too eager, clones fail blocks, and too slow for mixed
      
      cloned_blocks[is.index] = cb =
        blockOop(lit)->clone_block_for_interpreter(block_scope_or_NLR_target());
    }
    lit= cb;
  }
  stack[sp++]= lit;
  assert(stack[sp-1] != badOop,  "no bad oops on the stack");
}


abstract_vframe* interpreter::parentVF() {
  assert_block(receiver, "must be block receiver");
  assert( mi.map()->kind() == BlockMethodType, "must be block method");
  return blockOop(receiver)->parentVFrame(currentFrame());
}

// access a local slot


void interpreter::local_slot_desc( interpreter*& r, 
                                   abstract_vframe*& rvf,
                                   slotDesc*& sd) {
  
  interpreter* interp = this;
  abstract_vframe* vf = NULL;
  
  for ( fint i = 0;  i < is.lexical_level;  ++i) {
    if ( interp ) 
      if ( interp->parentI )    
        interp = interp->parentI;
      else {
        vf = interp->parentVF();  
        interp = NULL; 
      }
    else {
      vf = vf->parent();
      if ( vf->is_interpreted() )
        interp= vf->as_interpreted()->interp();
    }
  }
  r = interp;
  rvf = vf;
  Map* m =  interp  ?  interp->mi.map() 
                    :  vf->method()->map();
  sd = m->slot(is.index);
}


void interpreter::do_read_write_local_code(bool isWrite) {
  interpreter* interp;
  slotDesc* sd;
  ResourceMark rm; // for vf
  abstract_vframe* vf;
  local_slot_desc(interp, vf, sd);
  assert(interp || vf, "must have vframe if not interpreted");
  if (isWrite) {
    assert(sp > 0, "need argument to write");
    if (interp)   interp->set_slot(sd, stack[--sp]);
    else          vf    ->set_slot(sd, stack[--sp]);
    stack[sp++] = self;
  }
  else 
    stack[sp++] = interp  ?  interp->get_slot(sd)  
                          :  vf    ->get_slot(sd);
}

 
void interpreter::do_send_code(bool isSelfImplicit, stringOop selector, fint arg_count) {
  LookupType type;
  
  if      ( !isSelfImplicit )          type =         NormalLookupType;
  else if ( is.is_undirected_resend)   type =         ResendLookupType;
  else if ( is.delegatee != NULL)      type = DirectedResendLookupType;
  else                                 type =   ImplicitSelfLookupType;
        
      
  if (selector == VMString[_RESTART])
    pc= restart_pc();
  else {
    selToSend= selector;
    send(type, is.delegatee, arg_count);
    assert(stack[sp-1] != badOop  ||  NLRSupport::have_NLR_through_C(),
           "no bad oops on the stack");
    if (NLRSupport::have_NLR_through_C()) {
      continue_NLR();
      pc= return_pc();
    }
  }
}


void interpreter::block_scope_and_desc_of_home( frame*& block_scope_frame, 
                                                 int32& block_desc) {
  interpreter* interp= this;
  // try fast case first
  frame* f;
  do {
    assert_block(interp->receiver, "must be a block"); 
    f = blockOop(interp->receiver)->scope();
    interp= f->get_interpreter_of_block_scope();
  } while ( interp  &&  interp->mi.map()->kind() == BlockMethodType );
  
  if (interp) {
      block_scope_frame= f;
      block_desc = BLOCK_PROTO_DESC->value();
  }
  else {
    ResourceMark rm; // for vf
    abstract_vframe* vf = blockOop(interp->receiver)->parentVFrame(currentFrame())->home();
    block_scope_frame = vf->fr->block_scope_of_home_frame();
    block_desc = vf->scopeID();
  }
}
  
  
void interpreter::start_NLR(oop res) {
  // is receiver block?
  if ( method_object->map()->kind() == OuterMethodType )
    return;
  frame* block_home_scope_frame;
  int32  block_home_desc;
  if (CatchInterprocessReturns)
    catch_interprocess_returns(receiver);
  block_scope_and_desc_of_home( block_home_scope_frame, block_home_desc);
  NLRSupport::save_NLR_results( res,  int32(block_home_scope_frame),  block_home_desc);
}


void interpreter::continue_NLR() {
  if ((int32)block_scope_or_NLR_target() == NLRSupport::NLR_home_from_C()) {
    // this is the home frame (mixed) of the block
    NLRSupport::reset_have_NLR_through_C();  // home, that's it
  }
}


void interpreter::send(LookupType type, oop delOrNameToSend, fint arg_count ) {

  assert_string(selToSend, "better be string");

  // NormalLookupType means rcvr is on stack
  
  rcvToSend = type == NormalLookupType ? stack[sp - arg_count - 1] : self;
  
  if (mi.instruction_set != TWENTIETH_CENTURY_PLUS_ARGUMENT_COUNT_INSTRUCTION_SET)
    arg_count = stringOop(selToSend)->arg_count(); // XXXX slow, fix w/ lookup cache 
  
  int32 resSP = sp - arg_count - (type == NormalLookupType);
    
  oop res;
  for (;;) {

    res =
        stringOop(selToSend)->is_prim_name()
        ? send_prim()
        : lookup_and_send( type, methodHolder(), delOrNameToSend);
    

    if (!is_return_patched())
      break;
    if (get_return_patch_reason() == patched_for_profiling) {
      break; // don't handle profiling interp yet XXX
    }
    // save non vol regs because HandleReturnTrap can call convert which
    //  can call continueNLRAfterReturnTrap which (I think) cuts back the stack
    // -- dmu 2/96

    SaveNonVolRegsAndCall5( HandleReturnTrap,
                            NLRSupport::have_NLR_through_C() ? NLRSupport::NLR_result_from_C() : stack[sp-1],
                            (char*)currentFrame(),
                            NLRSupport::have_NLR_through_C(),
                            (frame*)NLRSupport::NLR_home_from_C(),
                            NLRSupport::NLR_home_ID_from_C());
    if (!restartSend)
      break;
  }
  stack[resSP] = res;
  sp = resSP + 1; // sp points one past top
}



oop interpreter::send_prim() {
  
  bool hasFailBlock = false;
  assert_string(selToSend, "must be a string by now");
  if ( stringOop(selToSend)->has_IfFail() ) {
    selToSend = stringOop(selToSend)->without_IfFail();
    hasFailBlock = true;
  }
  
  oop res;
  
  bool is_perform;
  res = try_perform_prim( hasFailBlock, is_perform );
  if (is_perform ||  NLRSupport::have_NLR_through_C()) return res;
  
  current_primDesc = ::getPrimDescOfSelector(stringOop(selToSend));

  /* (see runtime.h) was:
    res = CallPrimitiveFromInterpreter( first_inst_addr(current_primDesc->fn()),
                                        rcvToSend,
                                        &stack[sp - arg_count],
                                        arg_count - (hasFailBlock == true));
  */
  
  res = SaveNonVolRegsAndCall4 ( CallPrimitiveFromInterpreter,
                                 (void*)first_inst_addr((void*)current_primDesc->fn()),
                                 rcvToSend,
                                 &stack[sp - arg_count],
                                 arg_count - (hasFailBlock == true));
  
  if (NLRSupport::have_NLR_through_C()) { // for tests unwindProtectFn2
    return NLRSupport::NLR_result_from_C(); // might be returning badOop if killing proc
  }
  current_primDesc = NULL;
  if (!res->is_mark())
    return res;
    
  // cope with failure:
  
  res = res->memify(); // cvt to string
  assert_string(res, "prim fail must return string");

  if (hasFailBlock) rcvToSend = stack[sp-1];
  
  sp -= arg_count;
  stack[sp++] = res;
  stack[sp++] = selToSend;
  arg_count = 2;

  selToSend = VMString[
                       hasFailBlock
                       ? VALUE_WITH_
                       : PRIMITIVE_FAILED_ERROR_NAME_];
  
  return lookup_and_send( NormalLookupType,
                          rcvToSend,
                          NULL);
}


// do the perform, return result, set is_perform if is a perform

oop interpreter::try_perform_prim( bool hasFailBlock,
                                   bool& is_perform ) {            
  assert_string(selToSend, "must be a string");
  char* sel = stringOop(selToSend)->bytes();
  fint performKeywordLen = 0;
  bool isResend = false;
  LookupType t;
  if (stringOop(selToSend)->has__Perform__prefix()) {
    performKeywordLen = 9;
    t = NormalPerformType;
  } else if (stringOop(selToSend)->has__PerformResend__prefix()) {
    performKeywordLen = 15;
    isResend = true;
    t = ResendPerformType;
  } else  {
    is_perform = false;
    return smiOop_zero;
  }
  is_perform = true;
    
  selToSend = stack[sp - arg_count];  --arg_count;
  oop delToSend = NULL;
  if ( strncmp(sel + performKeywordLen, "DelegatingTo:", 13) == 0) {
    performKeywordLen += 13;
    t = DelegatedPerformType;
    delToSend = stack[sp - arg_count]; --arg_count;
  }
  
  if ( hasFailBlock) --arg_count, --sp; // ignore perform fail block
  return lookup_and_send( t, rcvToSend, delToSend );
}


oop interpreter::lookup_and_send( LookupType type,
                                         oop mh,
                                         oop delOrNameToSend ) {
  ResourceMark rm; // for sub-objects of L and vf
  // since we come here from perform, selToSend may not be a string!
   
  if (UseLocalAccessBytecodes && !hasParentLocalSlot) {
    simpleLookup L( type,
                    rcvToSend,
                    selToSend,
                    delOrNameToSend,
                    mh );
    
    // XXXXXX check code table, use compiled method, get compiler to call me

    switchToVMStack_intSend( &L, arg_count, InterpreterLookup_cont);
    if (NLRSupport::have_NLR_through_C()) { // recursive lookup error 
      return NLRSupport::NLR_result_from_C();
    }
    return  L.evaluateResult(&stack[sp - arg_count], arg_count, NULL);
  }
  else {
    FlushRegisterWindows();
    interpreted_vframe ivf(currentProcess->last_self_frame(true));
    vframeLookup L( type,
                    rcvToSend,
                    selToSend,
                    delOrNameToSend,
                    mh,
                    &ivf,
                    NULL,
                    NULL);
                  
    // XXXXXX check code table, use compiled method, get compiler to call me

    /* see runtime.h for explanation of SaveNonVol...
       used to be:
       switchToVMStack_intSend( (simpleLookup*)&L, arg_count, InterpreterLookup_cont);
    */
    SaveNonVolRegsAndCall3( switchToVMStack_intSend, 
                            (simpleLookup*)&L, 
                            arg_count, 
                            InterpreterLookup_cont);
                            
    if (NLRSupport::have_NLR_through_C()) { // recursive lookup error 
      return NLRSupport::NLR_result_from_C();
    }
  
    return L.evaluateResult(&stack[sp - arg_count], arg_count, NULL);
  }
  return NULL; // silence compiler
}


oop interpreter::get_slot(slotDesc* sd) {
  if (sd->is_map_slot())  
    return method_object->get_slot(sd);
  int32 off = smiOop(sd->data)->value();
  if (sd->is_arg_slot()) {
    assert( off < length_args,  "offset too big");
    return args[off];
  }
  off -= minOffset;
  assert( off  <  length_locals(),  "offset too big");
  return locals[off];
}
  

void interpreter::set_slot(slotDesc* sd, oop x) {
  int32 off = smiOop(sd->data)->value();
  off -= minOffset;
  assert( off  <  length_locals(),  "offset too big");
  locals[off] = x;
}


void interpreter::print() {
  lprintf("Interpreter state:\n");
  lprintf("\n\trcv: ");  receiver->print_oop();
  lprintf("\n\tself: "); self->print_oop();
  lprintf("\n\tsel: ");  selector->print_oop();
  if (delegatee != NULL) { lprintf("\n\tdel: ");  delegatee->print_oop(); }
  abstract_interpreter::print();
  lprintf("\n\tmethodHolder: ");  methodHolder()->print_oop();
  lprintf("\n\tpc: %d", pc);

  int i;
  FOR_EACH_SLOTDESC(mi.map(), sd) {
    if (sd->is_arg_slot()) {
      lprintf("\n\t:"); sd->name->print_oop();  lprintf(" = ");
      get_slot(sd) -> print_oop();
    }
    else if (sd->is_obj_slot()) {
      lprintf("\n\t"); sd->name->print_oop();  lprintf(" = ");
      get_slot(sd) -> print_oop();
    }
  }
  for (i = 0;  i < mi.length_literals;  i++) {
    if (cloned_blocks[i] != NULL) {
      lprintf("\n\tcloned_blocks[%d] = ", i); cloned_blocks[i]->print_oop();
    }
  }
  for (i = 0;  i < sp;  i++) {
    lprintf("\n\tstack[%d] = ", i); stack[i]->print_oop();
  }
  lprintf("\n\trcvToSend: "); rcvToSend->print_oop();
  lprintf("\n\tselToSend: "); selToSend->print_oop();
}

// XXX look at these:
// all 4 sends
// sending to interp
// sending to native

// nl return
//   must zap blocks when method returns
//   must check id, continue or not

// primitives
//   incl stack check,

// lookup caching, no stack switching


// spy

/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sic.hh"
# include "_sic.cpp.incl"

# ifdef SIC_COMPILER

  bool verifyOften = false;

  SICompiler* theSIC = NULL;
  SICompiler* lastSIC = NULL;       // for debugging
  BBIterator* last_bbIterator;

  static const int32 SICScopesSize              = 25 * K;
  static const int32 SICPCsSize                 =  5 * K;

  static const fint DefaultSICMaxSplitCost        =   50;
  static const fint DefaultSICMaxBlockInstrSize   =  600;
  static const fint DefaultSICMaxFnInstrSize      =  400;
  static const fint DefaultSICMaxBlockFnInstrSize =  600;
  static const fint DefaultSICMaxNmethodInstrSize = 5000;

  // inlining parameters (set via primitives)
  static IntBList* limits[LastLimit];

  void sic_init() {
    assert(bootstrapping, "should be allocating on C heap");
    for (fint i = NormalFnLimit; i < LastLimit; i++) {
      limits[i] = new IntBList(MaxRecompilationLevels, true);
      limits[i]->push(0);
    }    
    read_sic_limits();
  }
  
 // read_sic_limits exists so these can by set by prims
 
 void read_sic_limits() {
    limits[        NormalFnLimit ]->nthPut(0,      MaxFnInlineSize);
    limits[      BlockArgFnLimit ]->nthPut(0, MaxBlockFnInlineSize);
    limits[         BlockFnLimit ]->nthPut(0,   MaxBlockInlineSize);
    
    limits[   NormalFnInstrLimit ]->nthPut(0,      DefaultSICMaxFnInstrSize);
    limits[ BlockArgFnInstrLimit ]->nthPut(0, DefaultSICMaxBlockFnInstrSize);
    limits[          NmInstrLimit ]->nthPut(0, DefaultSICMaxNmethodInstrSize);
    limits[     BlockFnInstrLimit ]->nthPut(0,   DefaultSICMaxBlockInstrSize);
    limits[       SplitCostLimit ]->nthPut(0,   DefaultSICMaxSplitCost);
  }

  inline fint getLimit(IntBList* l, fint which) {
    if (which >= l->length()) which = l->length() - 1;
    return l->nth(which);
  }

  inline IntBList* stringToList(char* which) {
    if (strcmp(which, "Fn") == 0) {
      return limits[NormalFnLimit];
    } else if (strcmp(which, "BlockFn") == 0) {
      return limits[BlockArgFnLimit];
    } else if (strcmp(which, "Block") == 0) {
      return limits[BlockFnLimit];
    } else if (strcmp(which, "FnInstrs") == 0) {
      return limits[NormalFnInstrLimit];
    } else if (strcmp(which, "BlockFnInstrs") == 0) {
      return limits[BlockArgFnInstrLimit];
    } else if (strcmp(which, "BlockInstrs") == 0) {
      return limits[BlockFnInstrLimit];
    } else if (strcmp(which, "NMInstrs") == 0) {
      return limits[NmInstrLimit];
    } else if (strcmp(which, "Split") == 0) {
      return limits[SplitCostLimit];
    } else {
      return NULL;
    }
  }

# define SIC_PARAMS_ERROR_MSG \
    "arg1 should be Fn | BlockFn | Block | FnInstrs | BlockFnInstrs | BlockInstrs | Split"

  oop get_sic_params_prim(oop, char* which, void* FH) {
    IntBList* l = stringToList(which);
    if (!l) {
      failure(FH, SIC_PARAMS_ERROR_MSG);
      return 0;
    }
    objVectorOop res= Memory->objVectorObj->cloneSize(l->length());
    for (fint i = 0; i < l->length(); i++) {
      res->obj_at_put(i, as_smiOop(l->nth(i)));
    }
    return res;
  }
    
  oop set_sic_params_prim(oop r, char* which, objVectorOop params, void* FH) {
    ResourceMark rm;
    IntBList* l = stringToList(which);
    if (!l) {
      failure(FH, SIC_PARAMS_ERROR_MSG);
      return 0;
    }
    fint len = params->length();
    fint *n= NEW_RESOURCE_ARRAY(fint, len);
    fint i;
    for (i = 0; i < len; i++) {
      oop param = params->obj_at(i);
      if (!param->is_smi() || smiOop(param)->value() < 1) {
        char msg[BUFSIZ];
        sprintf(msg, "arg2[%ld]: invalid parameter (not a positive integer)",
                long(i));
        failure(FH, msg);
        return 0;
      }
      n[i] = smiOop(param)->value();
    }
    oop res = get_sic_params_prim(r, which, NULL);
    l->clear();
    for (i = 0; i < len; i++) l->push(n[i]);
    return res;
  }

  ScopeDescRecorder* SICompiler::scopeDescRecorder() { return rec; }
  Assembler* SICompiler::instructions() { return genHelper->a; }
    
  SICompiler::SICompiler(compilingLookup* k, sendDesc* sd, nmln* d)
    : AbstractCompiler(k, sd, d) {
    if (VMSICProfiling) OS::profile(true);
    initialize();
  }
  
  
  void SICompiler::finalize() {
    theSIC = NULL;
    theNodeGen = NULL;
    genHelper = NULL;
    theAssembler->finalize();
    last_bbIterator = bbIterator;
    bbIterator = NULL;
    AbstractCompiler::finalize();
    if (VMSICProfiling) OS::profile(false);
  }

  fint SICompiler::level() {
    return noInlinableSends ? MaxRecompilationLevels - 1  :  nextLevel;
  }
  
  fint SICompiler::estimatedSize() {
    // estimated target nmethod size (bytes)
    return BasicNode::cumulCost;
  }

  void SICompiler::initialize() {
    assert(theSIC == NULL, "shouldn't have but one compiler at a time");
    theSIC = lastSIC = this;
    theAssembler = new Assembler(SICInstructionsSize, SICInstructionsSize / 2,
                                 PrintSICCompiledCode, true);
    stackLocCount = argCount = 0;
    countID = 0;
    nodeGen = new NodeGen(L, send_desc, diLink);
    topScope = NULL;
    splitSig = NULL;
    bbIterator = new BBIterator;
    /* theAllocator = */ new SICAllocator();
    if (theRecompilation && theRecompilation->recompileeVScopes) {
      vscopes = theRecompilation->recompileeVScopes;
    } else {
      vscopes = NULL; 
    }
    if (baseLookupType(L->lookupType()) == NormalBaseLookupType) {
      // ignore the receiver static bit (same nmethod covers both cases)
      L->clearReceiverStatic();
    }

    dispatchToCode();
    

    SScope::currentScopeID = 0;
    ncodes = 0;
    rec = new ScopeDescRecorder(SICScopesSize, SICPCsSize);

    initTopScope();
    initLimits();

    initializeForPlatform();
  }

  void SICompiler::initLimits() {
    fint level;
    if (currentProcess->isUncommon()) {
      // when recompiling because of an uncommon trap, reset level
      level = nextLevel = 0;
    } else {
      level = AbstractCompiler::level();
      nextLevel = MaxRecompilationLevels - 1;
      if (nstages > 1 && recompilee == NULL) {
        // always use a counter in first version
        nextLevel = nstages - 1;
      }
    }
    noInlinableSends = true;

    inlineLimit[NormalFnLimit]         = getLimit(limits[NormalFnLimit],
                                                  level);
    inlineLimit[BlockFnLimit]          = getLimit(limits[BlockFnLimit],
                                                  level);
    inlineLimit[BlockArgFnLimit]       = getLimit(limits[BlockArgFnLimit],
                                                  level);
    inlineLimit[NormalFnInstrLimit]    = getLimit(limits[NormalFnInstrLimit],
                                                  level);
    inlineLimit[BlockFnInstrLimit]     = getLimit(limits[BlockFnInstrLimit],
                                                  level);
    inlineLimit[BlockArgFnInstrLimit]  = getLimit(limits[BlockArgFnInstrLimit],
                                                  level);
    inlineLimit[SplitCostLimit]        = getLimit(limits[SplitCostLimit],
                                                  level);
    inlineLimit[NmInstrLimit]          = getLimit(limits[NmInstrLimit],
                                                  level);
    if (SICAdjustLimits) {
      // adjust NmInstrLimit if top-level method is large
      fint cost = sicCost( method(), topScope, costP);
      if (cost > NormalMethodLen) {
        float l = (float)cost / NormalMethodLen * inlineLimit[NmInstrLimit];
        inlineLimit[NmInstrLimit] = min(int(l), SICInstructionsSize / 3);
      }
    }
  }

  void SICompiler::allocateArgs(fint nargs, bool isPrimCall) {
    Unused(isPrimCall);
    argCount = max(argCount, nargs);
  }

  void SICompiler::registerUninlinable(SendInfo* info, InlineLimitType t,
                                       fint cost) {
    // All sends that aren't inlined for some reason are registered here
    // to determine the minimum optimization level needed for recompilation
    // (i.e. if the send wouldn't be inlined even at the highest optimization
    // level there's no point in recompiling).
    // At the end of compilation, nextLevel will contain the lowest
    // optimization level that will generate better code than the current
    // level.
    // Also decides if this send should trigger a recompilation, and if so,
    // sets the count type of info to comparing.

    if (cost > 0) {
      info->uninlinable = true;

      while(    nextLevel > 0
            &&  getLimit(limits[t], nextLevel - 1) >= cost)
        --nextLevel;
    }
    else {
      // cost == 0 means unknown receiver 
      nextLevel = min(nextLevel, min(recompileLevel() + 2, nstages));
    }

    if ( nextLevel < nstages - 1
    &&   info->countType == NonCounting
    // The next line needs is here for performance: to quote Urs:
    // "for an doIt nmethod called by the VM it
    //  doesn't make sense to have counting sends because it can't be replaced
    //  on the stack (I think) and it won't be called again so the
    //  optimization effort would be wasted.  At least that's what I *think*
    //  the test was there for."
    &&   topScope->selector() != VMString[DO_IT]
    &&   getLimit( limits[t], MaxRecompilationLevels - 1) >= cost )
      info->countType = Comparing;

  }
  
  nmethod* SICompiler::compile() {
    EventMarker em("SIC-compiling %#lx %#lx", L->selector(), NULL);
    ShowCompileInMonitor sc(L->selector(), "SIC", recompilee != NULL);

    // cannot recompile uncommon branches in DI nmethods & top nmethod yet 
    FlagSetting fs2(SICDeferUncommonBranches,
                    SICDeferUncommonBranches &&
                    diLink == NULL && L->adeps->length() == 0 &&
                    L->selector() != VMString[DO_IT]);
    // don't use uncommon traps when recompiling because of trap
    useUncommonTraps = 
      SICDeferUncommonBranches && !currentProcess->isUncommon();
    
    // don't inline into doIt
    FlagSetting fs3(Inline, Inline && L->selector() != VMString[DO_IT]);

    # if TARGET_ARCH != I386_ARCH // no FastMapTest possible on I386
      // don't use fast map loads if this nmethod trapped a lot
      FlagSetting fs4(FastMapTest, FastMapTest &&
                      (recompilee == NULL ||
                      recompilee->flags.trapCount < MapLoadTrapLimit));
    # endif

    FlagSetting fs5(PrintCompilation, PrintCompilation || PrintSICCompilation);
    timer t;
    
    FlagSetting fs6(verifyOften, SICDebug || CheckAssertions);
    
    if(PrintCompilation || PrintLongCompilation ||
       PrintCompilationStatistics || VMSICLongProfiling) {
      t.start();
    }
    if (PrintCompilation || PrintSICCode) {
      lprintf("*SIC-%s%scompiling %s%s: (SICCompilationCount=%d)",
              currentProcess->isUncommon() ? "uncommon-" : "",
              recompilee ? "re" : "",
              sprintName( (methodMap*) method()->map(), L->selector()),
              sprintValueMethod( L->receiver ),
              (void*)SICCompilationCount);
    }

    topScope->genCode();
    
    buildBBs();
    if (verifyOften) bbIterator->verify(false); 
    
    bbIterator->eliminateUnreachableNodes(); // needed for removeUptoMerge to work

    // compute exposed blocks and up-level accessed vars
    bbIterator->computeExposedBlocks();
    bbIterator->computeUplevelAccesses();

    // make defs & uses and insert flush nodes for uplevel-accessed vars
    bbIterator->makeUses();

    // added verify here cause want to catch unreachable merge preds 
    // before elimination -- dmu
    if (verifyOften) bbIterator->verify(); 

    if (SICLocalCopyPropagate) {
      bbIterator->localCopyPropagate();
      if (verifyOften) bbIterator->verify(); 
    }
    if (SICGlobalCopyPropagate) {
      bbIterator->globalCopyPropagate();
      if (verifyOften) bbIterator->verify(); 
    }
    if (SICEliminateUnneededNodes) {
      bbIterator->eliminateUnneededResults();
      if (verifyOften) bbIterator->verify(); 
    }

    // do after CP to explot common type test source regs
    if (SICOptimizeTypeTests) {
      bbIterator->computeDominators();
      bbIterator->optimizeTypeTests();
      if (verifyOften) bbIterator->verify(); 
    }

    // allocate the temp (i.e. volatile) registers
    bbIterator->allocateTempRegisters();
    // allocate the callee-saved (i.e. non-volatile) registers
    SICAllocator* a = theAllocator;
    a->allocate(bbIterator->globals, topScope->incoming);
    stackLocCount = a->stackTemps;

    // make sure frame size is aligned properly
    int32 frame_size_so_far = frameSize();
    stackLocCount += roundTo(frame_size_so_far, frame_word_alignment) - frame_size_so_far;

    // compute the register masks for inline caches
    bbIterator->computeMasks(stackLocCount, nonRegisterArgCount());
    topScope->computeMasks(regStringToMask(topScope->incoming),
                           stackLocCount, nonRegisterArgCount());

    if (PrintSICCode) {
      print_code(false);
      lprintf("\n\n");
    }

    topScope->describe();    // must come before gen to set scopeInfo   
    genHelper = new SICGenHelper;
    bbIterator->gen();
    assert(theAssembler->verifyLabels(), "undefined labels");

    rec->generate();
    topScope->fixupBlocks();        // must be after rec->gen to know offsets
    if (vscopes) computeMarkers();  // ditto

    nmethod* nm = new_nmethod(this, false);

    if (theAssembler->lastBackpatch >= theAssembler->instsEnd)
      fatal("dangling branch");
    
    em.event.args[1] = nm;
    fint ms = IntervalTimer::dont_use_any_timer ? 0 : t.millisecs();
    if (PrintCompilation || PrintLongCompilation) {
      if (!PrintCompilation && PrintLongCompilation && ms >= MaxCompilePause) {
        lprintf("*SIC-%s%scompiling ",
               currentProcess->isUncommon() ? "uncommon-" : "",
               recompilee ? "re" : "");
        methodMap* mm = method() ? (methodMap*) method()->map() : NULL;
        printName(mm, L->selector());
        lprintf(": %#lx (%ld ms; level %ld)\n", nm, (void*)ms, (void*)nm->level());
      } else if (PrintCompilation) {
        lprintf(": %#lx (%ld ms; level %ld v%d)\n", (void*)nm, (void*)ms,
                (void*)nm->level(), (void*)nm->version());
      }
    }
    if (SICDebug && estimatedSize() > inlineLimit[NmInstrLimit]) {
      float rat = (float)estimatedSize() / (float)nm->instsLen();
      lprintf("*est. size = %ld, true size = %ld, ratio = %4.2f\n",
              (void*)estimatedSize(), (void*)nm->instsLen(),
              *(void**)&rat);
    }
    if (PrintCompilationStatistics) {
      static fint counter = 0;
      lprintf("\n*SIC-time= |%ld| ms; to/co/sc/lo/de= |%ld|%ld|%ld|%ld|%ld| %ld|%ld|%ld| %ld |", 
             (void*)ms, 
             (void*) (nm->instsLen() + nm->scopes->length() +
                      nm->locsLen() + nm->depsLen),
             (void*)nm->instsLen(), 
             (void*)nm->scopes->length(),
             (void*)nm->locsLen(), 
             (void*)nm->depsLen,
             (void*)BasicNode::currentID,
             (void*)bbIterator->bbCount,
             (void*)ncodes,
             (void*)counter++);
    }
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        //      nm->verify();
      }
#   endif
    return nm;
  }

  void SICompiler::buildBBs() {
    bbIterator->build(nodeGen->start);
  }
  
  void SICompiler::computeMarkers() {
    // weed out bogus marker nodes and determine which of the remaining markers
    // is the restart point
    fint len = theRecompilation->markers->length();
    fint *count= NEW_RESOURCE_ARRAY(fint, len); // how many markers at level i?
    MarkerNode **node= NEW_RESOURCE_ARRAY(MarkerNode*, len);    
    fint i;
    for (i = 0; i < len; i++) count[i] = 0;

    // check all markers
    for (i = len - 1; i >= 0; i--) {
      MarkerNode* n = theRecompilation->markers->nth(i);
      assert(0 <= n->scope()->depth && n->scope()->depth < len,
               "invalid depth");
      n->check();
      if (n->invalid) {
        // not really a marker - types don't match
      } else {
        fint depth = n->scope()->depth;
        count[depth]++;
        node[depth] = n;
      }
    }

    // find deepest marker path
    // NB: path can have "holes" (invalid markers followed by valid ones);
    // however, all markers after first invalid one must be ignored (they
    // are locally ok but one of their callers isn't, so globally they're
    // invalid as well)
    for (i = 0; i < len && count[i]; i++) ;
    fint deepest = i - 1;

    if (deepest < 0) {
      // no restart point
    } else if (count[deepest] > 1) {
      // no unique restart point - is this an error?
      if (PrintRecompilation) warning("SIC: cannot find unique restart point");
      theRecompilation->isReplacementSimple = false;
    } else {
      assert(count[deepest] == 1, "expected one");
      assert(node[deepest]->scope()->depth == deepest, "wrong node");
      node[deepest]->active = true;
      theRecompilation->activeMarker = node[deepest];
      theRecompilation->activeMarker->describe();
    }
  }
  
  void SICompiler::initTopScope() {
    recompileeScope =
      recompilee ? (RScope*)constructRScopes(recompilee) 
                 : (RScope*)new RNullScope(NULL);
    if (PrintPICScopes) recompileeScope->printTree(0, 0);

    nodeGen->haveStackFrame = true;
    MethodKind kind = method()->kind();
    countID = Memory->code->nextNMethodID();
    SCodeScope* s;
    SScope* parentScope = NULL;
    
    if (L->receiverMap()->is_block()) {
      blockOop block = (blockOop) L->receiver;
      assert_block(block, "expecting a block literal");
      // caution: parentFrame could be on conversion stack, so use
      // sending frame as a starting point
      // I think this hint is bogus--dmu (see NIC)
      frame* sender =
        L->sendingVFrame
          ? L->sendingVFrame->fr
          : currentProcess->last_self_frame(false);
      parentVFrame = block->parentVFrame(sender, true)->as_compiled();
      if (parentVFrame) parentScope = new_SVFrameScope(parentVFrame);
    }

    MethodLookupKey* k= new_MethodLookupKey(L->key);
    SendInfo* info = new SendInfo(k);
    
    switch (kind) {
     case OuterMethodType:
      s = new SMethodScope(method(), methodHolder_or_map(),
                           NULL, recompileeScope, info);
      break;
     case BlockMethodType:
      // taken from NIC by dmu 7/1
      assert(L->receiverMap()->is_block(), "was expecting block");
      if (parentScope) {
        s = new SBlockScope(method(), parentScope, 
                            NULL, recompileeScope, info);
      } else {
        s = new SDeadBlockScope(method(), info);
      }
      break;
     default:
      fatal("unexpected byte code kind");
    }
    
    needRegWindowFlushes = false;
    topScope = s;
    s->vscope = vscopes ? vscopes->top() : NULL;
  }
  
  void SICompiler::print() {
    print_short(); lprintf(":");
    L->print();
    if (L->result()) {
      L->result()->print();
      if (method()) {
        lprintf("\tmethod: ");
        method()->print_real_oop();
        lprintf("\n");
      }
    } else {
      lprintf("\tmethod: ");
      method()->print_real_oop();
      lprintf("; methodHolder: ");
      methodHolder_or_map()->print_real_oop();
      lprintf("\n");
    }
    lprintf("\treceiver: ");
    L->receiver->print_real_oop();
    lprintf("\tnodeGen: %#lx\n", nodeGen);
    lprintf("\tp ((SICompiler*)%#lx)->print_code()\n", this);
  }

  void SICompiler::print_short() { lprintf("SIC %#lx", this); }

  void SICompiler::print_code(bool suppressTrivial)  {
    if (theSIC == NULL) {
      last_bbIterator->print_code(suppressTrivial);
      last_bbIterator->print();
    } else {
      bool hadBBs = bbIterator != NULL;
      if (! hadBBs) {
        // need BBs for printing
        bbIterator = new BBIterator;
        buildBBs();
      }
      bbIterator->print_code(suppressTrivial);
      bbIterator->print();
      if (!hadBBs) {
        bbIterator->clear();
        bbIterator = NULL;
      }
    }
  }

  void SICompiler::print_vcg_code(bool suppressTrivial)  {
    char fn[256];
    sprintf(fn, "/tmp/sic.vcg.%lx", (unsigned long)this);
    FILE* f = fopen(fn, "w");
    fprintf(f, "graph: {\n");
    fprintf(f, "xmax: 800 ymax: 800  xspace: 10 yspace: 10");
    if (theSIC == NULL) {
      last_bbIterator->print_vcg_code(f, suppressTrivial);
    } else {
      bool hadBBs = bbIterator != NULL;
      if (! hadBBs) {
        // need BBs for printing
        bbIterator = new BBIterator;
        buildBBs();
      }
      bbIterator->print_vcg_code(f, suppressTrivial);
      if (!hadBBs) {
        bbIterator->clear();
        bbIterator = NULL;
      }
    }
    fprintf(f, "}\n");
    fclose(f);
  }

  
  int32 SICompiler::incoming_arg_count() {
    return topScope->nargs;
  }


# else
  void sic_init() {}

# endif

/* Sun-$Revision: 30.16 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "node.hh"
# pragma implementation "node_inline.hh"

# include "_node.cpp.incl"

# ifdef SIC_COMPILER

  int32 BasicNode::currentID;
  int32 BasicNode::currentCommentID;
  int32 BasicNode::cumulCost;
  fint BasicNode::lastBCI;
  ScopeInfo BasicNode::lastScopeInfo;
  int32 BasicNode::thisFrameSize;
  PrimDesc* BlockCloneNode::blockClone;
  PrimDesc* InterruptCheckNode::intrCheck;
  ConstPReg* BlockCloneNode::deadBlockPR;
  const Location TypeTestNode::RcvrMapReg = Temp1;
  const Location TypeTestNode::MapReg     = Temp2;
  
  const Location IndexedBranchNode::IndexReg  = Temp1;
  const Location IndexedBranchNode::BoundsReg = Temp2;

  void initNodes() {
    Node::currentID = Node::currentCommentID = Node::cumulCost = 0;
    BlockCloneNode::blockClone
      = getPrimDescOfSelector(VMString[BLOCK_CLONE], true);
    InterruptCheckNode::intrCheck
      = getPrimDescOfSelector(VMString[INTERRUPT_CHECK], true);
    BlockCloneNode::deadBlockPR = NULL;
    Node::lastScopeInfo = (ScopeInfo)-1;  Node::lastBCI = IllegalBCI;
    initDeadBlockNode();
  }

  void BasicNode::setScope(SSelfScope* s) {
    _scope = s; _bci = s ? s->bci() : IllegalBCI;
    assert(!s || !s->isCodeScope() ||
           s->bci() < ((SCodeScope*)s)->ncodes, "illegal bci");
  }

  BasicNode::BasicNode() {
    _id = currentID++; _bb = NULL;
    setScope(theNodeGen->currentScope());       // no scope e.g. for accessors
    _num = -1; _dest = _src = NULL; srcUse = NULL; destDef = NULL;
    hasSideEffects_now = deleted = false; l = NULL;
  }
  
 fint BasicNode::spOffset(Location l) {
   return genHelper->spOffset(l);
 }

 fint BasicNode::spOffset(Location l, nmethod* nm) {
   return genHelper->spOffset(l, nm);
 }

  void BasicNode::eliminateDest(BB* bb) {
    if (hasDest()  &&  canChangeDest())
      setDest(bb, theNodeGen->noPR);
  }



  // Public entry point for eliminateNode
  // This function does full virtual eliminateNode, THEN after
  // all parts gone, calls checkUses once.
  // It saves having to do non-virtual checkUses in an elimNode per class
  // that just resends and calls checkUses.  -- dmu 8/96

  void BasicNode::eliminateNodeAndUsedPRs(BB* bb, PReg* r, bool removing,
                                          void* misc) {
    if (deleted) return;  // prevent recursive deletion
    eliminateNode(bb, r, removing, misc);
    checkUses(r);
  }


  MarkerNode::MarkerNode(SExprStack* es, SplitSig* signat) {
    exprStack = new SExprStack(scope(), es->length());
    exprStack->appendList(es);
    sig = signat;
    theRecompilation->markers->append(this);
    active = invalid = false; locs = NULL;
    // NODE_COST(MarkerNode)
  }
  
  CommentNode::CommentNode(const char* s) {
    comment = s;
    // give all comments negative ids (don't disturb node numbers by turning
    // SICDebug off and on)
    _id = --currentCommentID; currentID--;
    // NODE_COST(CommentNode)
  }

  ArrayAtNode::ArrayAtNode(PReg* r, PReg* idx, bool ia,
                           PReg* res, PReg* err, fint doff)
  : AbstractArrayAtNode(r, idx, ia, res, err, doff, objVector_len_offset()) {
    NODE_COST(ArrayAtNode);    
  }

  ByteArrayAtNode::ByteArrayAtNode(PReg* r, PReg* idx, bool ia,
                                   PReg* res, PReg* err)
  : AbstractArrayAtNode(r, idx, ia, res, err,
                        byteVectorOopClass::byteVector_bytes_offset(), 
                        byteVectorOopClass::byteVector_len_offset()) {
    NODE_COST(ByteArrayAtNode);    
  }

  ArrayAtPutNode::ArrayAtPutNode(PReg* r, PReg* idx, bool ia,
                                 PReg* el, PReg* res, PReg* err, fint doff)
  : AbstractArrayAtPutNode(r, idx, ia, el, res, err, doff,
                           objVector_len_offset()) {
    NODE_COST(ArrayAtPutNode);  
  }

  ByteArrayAtPutNode::ByteArrayAtPutNode(PReg* r, PReg* idx,
                                         bool ia, PReg* el, bool ie,
                                         PReg* res, PReg* err)
  : AbstractArrayAtPutNode(r, idx, ia, el, res, err,
                           byteVectorOopClass::byteVector_bytes_offset(),
                           byteVectorOopClass::byteVector_len_offset()) {
    intElem = ie; NODE_COST(ByteArrayAtPutNode);    
 }

  TypeTestNode::TypeTestNode(PReg* rr, OopBList* m, bool nml, bool u) {
    _src = rr; maps = m; needMapLoad = nml;  hasUnknown = u;
    fint len = m->length();
    assert(len > 0, "should have at least one thing to test");
    mapPRs = new ConstPRegBList(len);
    fint i;
    for (i = 0; i < len; i++) {
      mapPRs->append(new_ConstPReg(_scope, m->nth(i)));
    }
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      for (i = 0; i < len; i++) {
        if (m->nth(i) == Memory-> true_mapOop() ||
            m->nth(i) == Memory->false_mapOop())
          error( "true/false performance bug");
        for (fint j = i + 1; j < len; j++) {
          assert(m->nth(i) != m->nth(j), "duplicate map");
        }
      }
    }
#   endif
    NODE_COST(TypeTestNode);    
  }

  IndexedBranchNode::IndexedBranchNode(PReg* testPR, int32 n) {
    _src = testPR;  nCases = n;  srcMustBeSmi = false;
    assert(nCases >= 0, "nCases must be non-negative");
    NODE_COST(IndexedBranchNode);    
  }

  TArithRRNode::TArithRRNode(ArithOpCode o, PReg* s, PReg* o2, PReg* d,
                             bool a1, bool a2) {
    op = o; _src = s; oper = o2, _dest = d;
    arg1IsInt = a1; arg2IsInt = a2;
    hasSideEffects_now = true; constResult = NULL;
    // don't eliminate even if result unused because primitive might fail
    NODE_COST(TArithRRNode);    
  }
  
  PReg* BasicNode::dest() {
    if (!hasDest()) fatal("has no dest");
    return _dest;
  }

  void BasicNode::setDest(BB* bb, PReg* d) {
    // bb == NULL means don't update defs
    if (!hasDest()) fatal("has no dest");
    assert(bb || !destDef, "shouldn't have a def");
    if (destDef) _dest->removeDef(bb, destDef);
    _dest = d;
    if (bb) destDef = _dest->addDef(bb, (Node*)this);
  }
  
  PReg* BasicNode::src() {
    if (!hasSrc()) fatal("has no src");
    return _src;
  }

  oop AssignNode::constantSrc() {
    assert(hasConstantSrc(), "no constant src");
    return ((ConstPReg*)_src)->constant;
  }

  BB* PNode::newBB() {
    if (_bb) return _bb;        // already has a BB
    fint len = 0;
    _bb = new BB(this, this, 1);     
    Node *n;
    for (n = this; !n->endsBB() && n->next(); n = n->next()) {
      n->_num = len++; n->_bb = _bb;
    }
    n->_num = len++; n->_bb = _bb;
    _bb->last = n;
    _bb->nnodes = len;
    return _bb;
  }

  MergeNode::MergeNode(Node* prev1, Node* prev2) :
    AbstractMergeNode(prev1, prev2) { _isLoopHead = didStartBB = false; 
                                       why = "beats me"; }
  MergeNode::MergeNode(const char* w) { _isLoopHead = didStartBB = false; 
                                  why = w;
  }

  BB* MergeNode::newBB() {
    if (_bb) {
      // already constructed BB
    } else {
      _bb = PNode::newBB();
    }
    return _bb;
  }
  
  void MergeNode::eliminateUnreachableNodes() {
    for (  int32 i = nPredecessors() - 1;  i >= 0;  --i ) {
      Node* n= prev(i);
      if (!n->isReachable())
        n->removeNext(this);
    }
  }

  MethodReturnNode::MethodReturnNode(fint o, bool hsf, PReg* r)
  : ReturnNode(r, new TempPReg(theNodeGen->currentScope(), ResultReg,
                               false, false)) {
    offset = o; haveStackFrame = hsf; NODE_COST(MethodReturnNode);
  }

  MergeNode* CallNode::nlrPoint() {
    if (nSuccessors() > 1) {
      assert(next1()->isMergeNode(), "should be a merge");
      return (MergeNode*)next1();
    } else {
      return NULL;
    }
  }

  CallNode::CallNode(MergeNode* n, fint nargs, PRegBList* e, SplitSig* s) {
    if (n) append1(n); exprStack = e; argc = nargs; sig = s;
    _dest = new SAPReg(scope(), ResultReg, false, false, _bci, _bci);
  }

  SendNode::SendNode(LookupType lt, oop s, oop d, oop m,
                     MergeNode* n, fint arg_c, PRegBList* e, SplitSig* sg)
  : CallNode(n, arg_c, e, sg) {
    l = lt; sel = s; del = d; mh = m;
    assert(exprStack, "should have expr stack");
    NODE_COST(SendNode);  }

  RegisterString CallNode::mask() {
    RegisterString m = scope()->mask(bci());
    fint stackLocs = theSIC->stackLocCount;
    fint nonRegisterArgs = theSIC->nonRegisterArgCount();
    if (exprStack) {
      for (fint i = exprStack->length() - 1; i >= 0; i--) {
        PReg* r = exprStack->nth(i);
        assert(r->isLiveAt(this), "expr stack elem not live");
        m |= registerMaskBit(r->loc, stackLocs, nonRegisterArgs);
      }
    }
    // add split regs
    for (SCodeScope* s = scope(); s; s = s->sender()) {
      for (fint i = s->splitRegs->length() - 1; i >= 0; i--) {
        PReg* r = s->splitRegs->nth(i);
        if (r->isLiveAt(this)) {
          m |= registerMaskBit(r->loc, stackLocs, nonRegisterArgs);
        }
      }
    }
    // mark ExtraArgRegisters if more than NumArgRegisters-1 args
    // (the -1 is for the receiver, therefore the <= below)
    for (fint i = NumRcvrAndArgRegisters; i <= argc; i++) {
      m |= registerMaskBit(ArgLocation(i), stackLocs, nonRegisterArgs);
    }
    return m;
  }

  PrimNode::PrimNode(PrimDesc* d, MergeNode* n, fint arg_c, PRegBList* e,
                     SplitSig* sg, BlockPReg* failBlock)
  : CallNode(n, arg_c, e, sg) {
    pd = d; primFailBlock = failBlock;
    assert(!failBlock || failBlock->primFailBlockScope,
           "should have primFailBlockScope");
    assert(pd->needsNLRCode() || n == NULL, "no NLR target needed");
    if (d->canScavenge()) {
      assert(e, "should have expr stack");
    } else {
      // the expression stack is only needed if the primitive can walk the
      // stack (then the elements will be debug-visible) or if the primitive
      // can scavenge (then the elems must be allocated to GCable regs)
      exprStack = NULL;        
    }
    NODE_COST(PrimNode);
  }

  RegisterString PrimNode::mask() {
    RegisterString m = CallNode::mask();
    if (primFailBlock && primFailBlock->loc != UnAllocated) {
      // a primFailBlock is live only in the failure branch, i.e. it is *not*
      // live at the call point
      // (the register is maked live for this bci because there is no
      // distinction between the prim call and the failure call -- both
      // have the same bci)
      fint stkLocs   = theSIC->stackLocCount;
      fint nonRegisterArgs = theSIC->nonRegisterArgCount();
      int32 bit = registerMaskBit(primFailBlock->loc, stkLocs, nonRegisterArgs);
      assert(!bit  ||  m & bit, "should be set if is a register");
      m &= ~bit;
    }
    return m;
  }

  UncommonNode::UncommonNode(PRegBList* e, bool r) {
    exprStack = e; restartSend = r; NODE_COST(UncommonNode);
  }

  bool PrimNode::hasSideEffects() {
    return pd->cantBeMovedOrCut() 
    ||     pd->canFail() 
    ||     PNode::hasSideEffects();
  }

  void ConstInitNode::addConstant(ConstPReg* r) {
    assert(!constants.contains(r), "shouldn't be there");
    constants.append(r);
  }

  // uplevel-read vars have to be flushed to the stack after each def
  // (block nodes and array at/atPut handle the flushing themselves)
  // performance bug: FlushNode should have backptr to def node so that
  // it can be eliminated when def gets eliminated  -- fix this

  void BasicNode::addFlushNode(FlushNode* n) {
    bb()->addAfter((Node*)this, n);
  }

  void FlushNode::addFlushNode(FlushNode* n) { Unused(n); } // break recursion
  void BlockCloneNode::addFlushNode(FlushNode* n) { Unused(n); }
  void BlockZapNode::addFlushNode(FlushNode* n) { Unused(n); }

  void AbstractArrayAtNode::addFlushNode(FlushNode* n) {
    if (n->src() == error) {
      // flush along failure branch
      Node* n1 = next1();
      assert(n1->isTrivial(), "unexpected node");
      n1->bb()->addAfter(n1, n);
    } else {
      assert(n->src() == dest(), "what is being defined?");
      BasicNode::addFlushNode(n);
    }
  }

  // cloning: copy the node during splitting
  // only need to copy the basic state; defs, uses etc haven't yet been 
  // computed
  Node* BasicNode::copy(PReg* from, PReg* to) {
    Node* c = clone(from, to);
    if (c) { c->_scope = _scope; c->_bci = _bci; }
    return c;
  }
  Node* PrologueNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* NonLocalReturnNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* MethodReturnNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* BranchNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* TBranchNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* TypeTestNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* IndexedBranchNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* DeadBlockNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* DeadEndNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  Node* RestartNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }
  // flush nodes are created after splitting, so they can't ever be cloned
  Node* FlushNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    ShouldNotCallThis(); return NULL; }

# define TRANSLATE(s) ((s == from) ? to : s)

  Node* ConstInitNode::clone(PReg* from, PReg* to) {
    // need to remember all copies of a ConstInitNode so that the scope can
    // add the constants to be initialized to all copies; otherwise, only
    // one branch of a split would initialize the constant regs
    Unused(from); Unused(to);
    ConstInitNode *n, *newNode = new ConstInitNode;
    for ( n = this;  n->nextCopy;  n = n->nextCopy) 
      ;
    n->nextCopy = newNode;
    return newNode;
  }
  Node* MergeNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to); return NULL; }
  Node* LoadOffsetNode::clone(PReg* from, PReg* to) {
    return new LoadOffsetNode(base, offset, TRANSLATE(_dest)); }
  Node* LoadIntNode::clone(PReg* from, PReg* to) {
    return new LoadIntNode(value, TRANSLATE(_dest)); }
  Node* LoadStackNode::clone(PReg* from, PReg* to) {
    return new LoadStackNode(TRANSLATE(frame), nm, nd,
                             TRANSLATE(_dest), name);
  }
  Node* StoreOffsetNode::clone(PReg* from, PReg* to) {
    return new StoreOffsetNode(TRANSLATE(_src), base, offset, needCheckStore);}
  Node* StoreStackNode::clone(PReg* from, PReg* to) {
    return new StoreStackNode(TRANSLATE(_src), TRANSLATE(frame), nm,
                              nd, name);
  }
  Node* AssignNode::clone(PReg* from, PReg* to) {
    return new AssignNode(TRANSLATE(_src), TRANSLATE(_dest)); }
  Node* ArithRRNode::clone(PReg* from, PReg* to) {
    return new ArithRRNode(op, TRANSLATE(_src), oper, TRANSLATE(_dest)); }
  Node* TArithRRNode::clone(PReg* from, PReg* to) {
    return new TArithRRNode(op, TRANSLATE(_src), oper, TRANSLATE(_dest),
                            arg1IsInt, arg2IsInt);
  }
  Node* ArithRCNode::clone(PReg* from, PReg* to) {
    return new ArithRCNode(op, TRANSLATE(_src), oper, TRANSLATE(_dest)); }
  Node* SendNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    // NB: use current split signature, not the receiver's sig!
    SendNode* n =
      new SendNode(l, sel, del, mh, nlrPoint(), argc, exprStack,
                   theSIC->splitSig);
    n->_dest = _dest;       // don't give it a new dest!
    return n;
  }
    
  Node* PrimNode::clone(PReg* from, PReg* to) {
    // NB: use scope's current sig, not the receiver's sig!
    PrimNode* n = new PrimNode(pd, nlrPoint(), argc, exprStack, scope()->sig, 
                               (BlockPReg*)TRANSLATE(primFailBlock));
    assert(_dest != from, "shouldn't change dest");
    n->_dest = _dest;       // don't give it a new dest!
    return n;
  }
  Node* InterruptCheckNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    // NB: use scope's current sig, not the receiver's sig!
    Node* n = new InterruptCheckNode(exprStack, scope()->sig);
    assert(_dest != from, "shouldn't change dest");
    n->_dest = _dest;       // don't give it a new dest!
    return n;
  }
  Node* BlockCloneNode::clone(PReg* from, PReg* to)  {
    // NB: use scope's current sig, not the receiver's sig!
    Node* n = new BlockCloneNode((BlockPReg*)TRANSLATE(block()), scope()->sig,
                                 clonePrimFailBlock);    
    assert(_dest != from, "shouldn't change dest");
    n->_dest = _dest;       // don't give it a new dest!
    return n;
  }
  Node* BlockCreateNode::clone(PReg* from, PReg* to) {
    // NB: use scope's current sig, not the receiver's sig!
    Node* n = new BlockCreateNode((BlockPReg*)TRANSLATE(block()), scope()->sig);
    assert(_dest != from, "shouldn't change dest");
    n->_dest = _dest;       // don't give it a new dest!
    return n;
  }
  Node* BlockZapNode::clone(PReg* from, PReg* to)    {
    return new BlockZapNode((BlockPReg*)TRANSLATE(block())); }

  Node* ArrayAtNode::clone(PReg* from, PReg* to) {
    return new ArrayAtNode(TRANSLATE(_src), TRANSLATE(arg), intArg,
                           TRANSLATE(_dest), TRANSLATE(error),
                           dataOffset);
  }
  Node* ByteArrayAtNode::clone(PReg* from, PReg* to) {
    return new ByteArrayAtNode(TRANSLATE(_src), TRANSLATE(arg), intArg,
                               TRANSLATE(_dest), TRANSLATE(error));
  }
  Node* ArrayAtPutNode::clone(PReg* from, PReg* to) {
    return new ArrayAtPutNode(TRANSLATE(_src), TRANSLATE(arg), intArg,
                              TRANSLATE(elem), TRANSLATE(_dest),
                              TRANSLATE(error), dataOffset);
  }
  Node* ByteArrayAtPutNode::clone(PReg* from, PReg* to) {
    return new ByteArrayAtPutNode(TRANSLATE(_src), TRANSLATE(arg),
                                  intArg, TRANSLATE(elem), intElem,
                                  TRANSLATE(_dest), TRANSLATE(error));
  }
  Node* UncommonNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);
    return new UncommonNode(exprStack, restartSend); }

  Node* MarkerNode::clone(PReg* from, PReg* to) {
    // code containing a marker is split
    // NB: use scope's current sig, not the receiver's sig!
    Unused(from); Unused(to);
    return new MarkerNode(exprStack, scope()->sig);
  }

  Node* InlinedReturnNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);  return NULL; }
  Node* InlinedNonLocalReturnNode::clone(PReg* from, PReg* to) {
    Unused(from); Unused(to);  return NULL; }
  Node* NopNode::clone(PReg* from, PReg* to) { 
    Unused(from); Unused(to);  return NULL; }
  Node* CommentNode::clone(PReg* from, PReg* to) { 
    Unused(from); Unused(to);  return NULL; }

  // makeUses: construct all uses and defs
  
  void PrologueNode::makeUses(BB* bb) {
    if (isAccessMethod) return;
    // build initial def of incoming args & self
    SCodeScope* s = scope();
    bb->addDef(this, s->receiver->preg());
    if (s->isBlockScope()) bb->addDef(this, s->self->preg());
    for (fint i = 0; i < s->nslots; i++) {
      if (s->args[i]) bb->addDef(this, s->args[i]->preg());
    }
  }    
  
  // removeUses: remove all uses and defs

  void AbstractArrayAtPutNode::removeUses(BB* bb) {
    elem->removeUse(bb, elemUse);
    AbstractArrayAtNode::removeUses(bb);
  }
  
  void CallNode::removeUses(BB* bb) {
    _dest->removeDef(bb, destDef);
    PNode::removeUses(bb);
  }


  // The receiver has been disconnected from its predecessor.
  // Go forward and remove everything reachable only from it.
  // (If it has other predecessors, do nothing.)
  
  // Note: this works for today's restart because the backwards
  // branch is not a normal predecessor of the merge; the restart
  // node just points back to the loop head node.
  // Now not having an explicit link would screw up copy propagation
  // of locals, but locals are not cp'ed anyway. And nothing else
  // survives a restart. -- dmu & urs 8/96
  
  void BasicNode::removeUpToMerge() {
    BB* thisBB = _bb;
    Node* n = (Node*)this;
    
    if ( n->nPredecessors() > 0  ) {
      assert(  n->isMergeNode(),
               "To get here, node must have been unlinked from"
               " some other node, yet have remaining preds."
               " Only merge should have multiple preds.");
      return; // cannot remove, needed by other node
    }
      
    for ( n = (Node*)this; 
          n 
     &&   n->hasSinglePredecessor() // cannot elim if other nodes need n
     &&  !n->isLoopHead(); // hasSinglePred does not count backwards branches
         ) {
      while (n->nSuccessors() > 1) {
        fint i = n->nSuccessors() - 1;
        Node* succ = n->nexti(i);
        n->removeNext(succ);
        succ->removeUpToMerge();
      }
      Node* nextn = n->next();
      if (!n->deleted) n->eliminateNodeAndUsedPRs(thisBB, NULL, true);
      if (nextn) {
        BB* nextBB = nextn->bb();
        if (nextBB != thisBB) {
          if (nextBB->isPredecessor(thisBB)) {
            // cut the link between thisBB and nextBB
            thisBB->removeNext(nextBB);
          }
          if (nextn->nPredecessors() >= 2) {
            // also remove n's successor so that we can delete past merges
            // if all incoming branches of the merges are deleted
            n->removeNext(nextn);
            // nextn had at least 2 predecessors, so must stop deleting here
            // NB: must break here -- if was 2 successors, will now be one
            // and loop would continue (was a bug)  -Urs 8/94
            assert(nextn->isMergeNode(), "oops");
            break;
          }
          thisBB = nextBB;
        }
      }
      n = nextn;
    }
    BB* nextBB = n ? n->bb() : NULL;
    if (nextBB && nextBB != thisBB && nextBB->isPredecessor(thisBB)) {
      // cut the link between thisBB and nextBB
      thisBB->removeNext(nextBB);
    }
  }

  void PrimNode::eliminateNode(BB* bb, PReg* r, bool rem, void* misc) {
    assert(!deleted, "eliminateNodeAndUsedPRs should have caught this");
    assert(rem || !hasSideEffects(), "shouldn't call");
    Node* np = nlrPoint();
    if (np) {
      _nxt->pop();              // remove nlrPoint
      assert(nlrPoint() == NULL, "should be NULL now");
      np->removePrev(this);     // and unlink the pred pointer
    }
    CallNode::eliminateNode(bb, r, rem, misc); 
    // remove all unused nodes along NLR branch
    // performance bug: should also remove computation of args
    // (not done today because %o0 etc have incorrectDU)
    if (np) {
      // must not call removeUpToMerge before CallNode::eliminateNode
      //  because CallNode::elim... will set deleted() predicate
      //  on some nodes and this must be set before nlrPoint->remove...
      np->removeUpToMerge(); 
    }
  }

  // override this to use misc arg to select which type to NOT eliminate
  
  void TypeTestNode::eliminateNode(BB* bb, PReg* r, bool rem, 
                                   void* misc) {
    Unused(rem);
    if (deleted) return;
    mapOop m = mapOop(NULL);
    assert( !m->is_map(),
            "Since NULL is default value of misc, it had better not" 
            "look like a mapOop" );
    assert( misc == NULL  ||  mapOop(misc)->is_map(), "better be a mapOop");
    mapOop theMap = misc == NULL  ?  mapOop(badOop)  :  mapOop(misc);
    // completely eliminate receiver and all successors
    eliminateAllBut(bb, r, (ConstPReg*)NULL, theMap);    
    AbstractBranchNode::eliminateNode(bb, r, true, misc);
  }
  


  // If you need to call this routine, call eliminateNodeAndUsedPRs.
  // That will call eliminateNode, which will call this routine.
  // Then that will call checkUses so that the PR's defs get scorched. -- dmu
    
  void TypeTestNode::eliminateAllBut(BB* bb, PReg* r, ConstPReg* c,
                                     mapOop theMap) {
    // remove node and all successor branches (except for one if rcvr is known)
#    if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
       bool unknownDead = false;
       for (Node* nn = next();  nn; nn = nn->next()) {
         if (nn->isDeadEndNode()) {unknownDead = true; break;}
         if (nn->nSuccessors() != 1)
           break;
         if (nn->isExitNode() && !nn->deleted)
           break;
       }
       if ( theMap == (mapOop)badOop && unknownDead) 
         fatal("about to remove all good cases");
       Node* k = NULL;
       for (fint i = 0;  i < _nxt->length(); i++) {
         if ( (c && c->constant == maps->nth(i))  ||  theMap == maps->nth(i)
             || maps->nth(i)->map() == theMap->map_addr()) {
                 if (k) fatal(">1");
                 else k = _nxt->nth(i);
             }
             if ( unknownDead && !k) fatal("about to remove all good cases");
          }
        }
#   endif
    
    NodeBList* successors = _nxt;
    _nxt = new NodeBList(1);
    oop constant = c ? c->constant : 0;
    Node* keep = NULL;
    if (PrintSICEliminateUnneededNodes) {
      lprintf("*eliminating tt node %#lx const %#lx map %#lx\n",
              this, constant, theMap);
    }
    Node* un = next();          // save unknown branch
       
    // remove all successor nodes
    for (fint i = 0; i < successors->length(); i++) {
      Node* succ = successors->nth(i);
      succ->removePrev(this);
      oop m = maps->nth(i);
      if ( (c && constant == m)          // have a constant
      ||   theMap == m                 // have a map, found it
      ||   m->map() == theMap->map_addr()  // have an oop, looking for a map
         ) {
        assert(keep == NULL, "shouldn't have more than one match");
        keep = succ;
      } else {
        succ->removeUpToMerge();
      }
    }

    if (keep || theMap == mapOop(badOop)) {
      // found correct prediction, so can delete unknown branch, or
      // delete everything (theMap == badOop)
      removeNext(un);
      un->removeUpToMerge();    // delete unknown branch
    } else {
      // the type tests didn't predict for theMap (e.g. programming change)
      // (performance bug: should inline correct case since it's known now;
      // also, unknown branch may be uncommon)
      if (WizardMode) {
        warning1("SIC: typetest didn't predict map %#lx", theMap);
        lprintf("predicted maps: ");
        maps->print();
      }
    }
    assert(this == bb->last, "should end my BB");
    
    // remove bb links
    while (bb->nSuccessors() > 0) {
      bb->removeNext(bb->nexti(bb->nSuccessors() - 1));
    }

    if (keep) {
      // append remaining case as fall-through
      append(keep);
      bb->append(keep->bb());
    }
  }
    

  void TypeTestNode::eliminateUnnecessary(mapOop m) {
    // eliminate unnecessary type test: receiver is known to have map m
    eliminateNodeAndUsedPRs(bb(), NULL, NULL, (void*)m);
  }


  // override this to use misc arg to select which type to NOT eliminate
  
  void IndexedBranchNode::eliminateNode(BB* bb, PReg* r, bool rem, 
                                        void* misc) {
    Unused(rem);
    if (deleted) return;
    // if misc not null, source is known to be a constant, misc is the oop
    oop constSrc = misc == NULL  ?  badOop  :  ((ConstPReg*)misc)->constant;
    // completely eliminate receiver and all successors
    eliminateAllBut(bb, r, constSrc);    
    AbstractBranchNode::eliminateNode(bb, r, true, misc);
  }
  
  
  // see comment for TypeTestNode::eliminateAllBut
  
  void IndexedBranchNode::eliminateAllBut(BB* bb, PReg* r,
                                          oop constSrc) {
    // remove node and all successor branches (except for one )
    // derived from TypeTestNode::eliminateAllBut -- dmu
    
    NodeBList* successors = _nxt;
    _nxt = new NodeBList(1);
    Node* keep = NULL;
    if (PrintSICEliminateUnneededNodes) {
      lprintf("*eliminating indexed branch node %#lx oop %#lx\n",
              this, constSrc);
    }
    Node* un = next();          // save unknown branch
    assert(un, "must have unknown case");
    // remove all successor nodes
    for (fint i = 0; i < successors->length(); i++) {
      Node* succ = successors->nth(i);
      succ->removePrev(this);
      if (constSrc == as_smiOop(i)) {
        assert(keep == NULL, "shouldn't have more than one match");
        keep = succ;
      } else {
        succ->removeUpToMerge();
      }
    }

    if (keep || constSrc == badOop) {
      // found correct prediction, so can delete unknown branch, or
      // delete everything (constSrc == badOop)
      removeNext(un);           // unlink it
      un->removeUpToMerge();    // delete unknown branch
    } 
    assert(this == bb->last, "should end my BB");

    // remove bb links
    while (bb->nSuccessors() > 0) {
      bb->removeNext(bb->nexti(bb->nSuccessors() - 1));
    }

    if (keep) {
      // append remaining case as fall-through
      append(keep);
      bb->append(keep->bb());
    }
  }


  void AbstractArrayAtNode::eliminateNode(BB* bb, PReg* r, 
                                          bool rem, void* misc) {
    UsedOnlyInAssert(rem);
    assert(rem, "shouldn't eliminate because of side effects (errors)");
    // (It seems to me that despite the above assert, if it could be
    //  established that this operation could never fail, it could be
    //  eliminated if its result were unused. So the assert above
    //  may be bogus.) -- dmu 8/96
    assert(!deleted, "eliminateNodeAndUsedPRs should have caught this");
    // remove fail branch nodes first
    Node* fail = next1();
    // next1 returns NULL if there is no next1
    // NULL test inserted by dmu 4/26/96 to fix a bug
    if (fail != NULL) {
      // eliminateNode assumes single succ, so remove fail first
      fail->removePrev(this); 
    }
    AbstractBranchNode::eliminateNode(bb, r, rem, misc);
    if (fail)
      fail->removeUpToMerge(); // do this AFTER deleted is set by above call
  }

  
  void FlushNode::eliminateNode(BB* bb, PReg* r, bool rem, void* misc) {
    assert(!deleted, "eliminateNodeAndUsedPRs should have caught this");
    // flush nodes are never really eliminated; if the original reg is
    // copy-propagated away, we need to flush another register, but we still
    // need to flush
    if (eliminated) return;
    
    if (rem)  // really remove
      PNode::eliminateNode(bb, r, rem, misc);
    else // see comment above
      removeUses(bb);
  }
  
  
  void FlushNode::eliminateNodeAndUsedPRs(BB* bb, PReg* r, bool rem, void* misc) {
    if (deleted) return;
    PNode::eliminateNodeAndUsedPRs(bb, r, rem, misc);
    eliminated = true;
  }


  void BranchNode::eliminateBranch(int32 op1, int32 op2, int32 res) {
    // the receiver can be eliminated because the result it is testing
    // is a constant (res)
    bool ok;
    switch (op) {
     case ALBranchOp:   ok = true;              break;
     case EQBranchOp:   ok = op1 == op2;        break;
     case NEBranchOp:   ok = op1 != op2;        break;
     case LTBranchOp:   ok = op1 <  op2;        break;
     case LEBranchOp:   ok = op1 <= op2;        break;
     case GTBranchOp:   ok = op1 >  op2;        break;
     case GEBranchOp:   ok = op1 >= op2;        break;
     case LTUBranchOp:  ok = (unsigned)op1 <  (unsigned)op2;    break;
     case LEUBranchOp:  ok = (unsigned)op1 <= (unsigned)op2;    break;
     case GTUBranchOp:  ok = (unsigned)op1 >  (unsigned)op2;    break;
     case GEUBranchOp:  ok = (unsigned)op1 >= (unsigned)op2;    break;
     case VSBranchOp:   return;         // can't handle yet
     case VCBranchOp:   return;         // can't handle yet
     default:           fatal("unexpected branch type");
                      }
    fint nodeToRemove;
    if (ok) {
      nodeToRemove = 0; // branch is taken
    } else {
      nodeToRemove = 1;
    }

    // discard one successor 
    Node* discard = nexti(nodeToRemove);
    removeNext(discard);
    discard->removeUpToMerge();
    Node* succ = nexti(1 - nodeToRemove);
    removeNext(succ);
    
    // check that if I am deleted, the right node will be next
    // This check is conservative in that it does not exactly duplicate
    // algorithm used by BB::gen
    Node* nn = this;
    do {
       nn =  nn->bb()->last != nn
               ?  nn->next()
               : (nn->bb()->next()
                    ?  nn->bb()->next()->first
                    :  NULL);
    } while ( nn  &&  nn != succ &&  nn->deleted); 
    if ( nn == succ ) {
      //  make the remaining one the fall-thru case
      append(succ);
      bb()->remove(this); // delete the branch    
    }
    else {
      // fall-thru not guaranteed; use a branch
      op = ALBranchOp; // cannot just delete me 
      append(discard); 
      append1(succ);
    }
  }
  
  // copy propagation: replace a use by another use; return false if
  // unsuccsessful

# define CP_HELPER(_src, srcUse)                                              \
    /* live range must be correct - otherwise reg. allocation breaks   */     \
    /* (even if doing just local CP - could fix this by checking for   */     \
    /* local conflicts when allocating PRegs, i.e. keep BB alloc info) */     \
    if (replace || d->extendLiveRange((Node*)this)) {                         \
      _src->removeUse(bb, srcUse);                                            \
      _src = d;                                                               \
      srcUse = _src->addUse(bb, (Node*)this);                                 \
      return true;                                                            \
    } else {                                                                  \
      return false;                                                           \
    }
  
  bool BasicNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    assert(canCopyPropagate(), "can't cp");
    assert(hasSrc(), "has no src");
    if (srcUse == u) {
      CP_HELPER(_src, srcUse);
    } else {
      fatal("copyPropagate: not the source use");
    }
    return false;
  }
  
  bool LoadOffsetNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == baseUse) {
      // minor performance bug: prev. node should probably be deleted
      // (loads base reg) - eliminateUnneeded doesn't catch it - fix this
      // (happens esp. if d is a constant)
      CP_HELPER(base, baseUse);
    } else {
      return LoadNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool LoadStackNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == frameUse) {
      CP_HELPER(frame, frameUse);
    } else {
      return LoadNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool StoreOffsetNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == baseUse) {
      CP_HELPER(base, baseUse);
    } else {
      return StoreNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool StoreStackNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == frameUse) {
      CP_HELPER(frame, frameUse);
    } else {
      return StoreNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool TBranchNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    bool haveFailure = !(intRcvr && intArg);
    bool res = doCopyPropagate(bb, u, d, replace);
    if (intArg && intRcvr && haveFailure) {
      assert(res, "CP must have worked");
      // failure branch has been eliminated through CP - remove the nodes
      Node* fail = _nxt->pop();
      fail->removePrev(this);
      fail->removeUpToMerge();
    }
    return res;
  }

  bool TBranchNode::doCopyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == srcUse) {
      if (d->isConstPReg() && ((ConstPReg*)d)->constant->is_smi())
        intRcvr = true;
      CP_HELPER(_src, srcUse);
    } else if (u == argUse) {
      if (d->isConstPReg() && ((ConstPReg*)d)->constant->is_smi())
        intArg = true;
      CP_HELPER(arg, argUse);
    } else {
      fatal("copyPropagate: not the source use");
    }
    return false;
  }

  bool  ArithRRNode::operIsConst() { return oper->isConstPReg(); }
  int32 ArithRRNode::operConst()   {
    assert(operIsConst(), "not a constant");
    return int32(((ConstPReg*)oper)->constant); }
  
  bool ArithNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    bool success = doCopyPropagate(bb, u, d, replace);
    if (_src->isConstPReg() && operIsConst()) {
      assert(success, "CP must have worked");
      // can constant-fold this operation
      int32 c1 = (int32)((ConstPReg*)_src)->constant;
      int32 c2 = (int32)operConst();
      int32 res;
      bool cc = false;
      switch (op) {
       case AddCCArithOp:               
        cc = true;      // fall through
       case AddArithOp:
        res = c1 + c2; break;

       case AndCCArithOp:               
        cc = true;      // fall through
       case AndArithOp:         
        res = c1 & c2; break;

       case OrCCArithOp:                
        cc = true;      // fall through
       case OrArithOp:          
        res = c1 | c2; break;

       case XOrArithOp:
        res = c1 ^ c2; break;

       case SubCCArithOp:               
        cc = true;      // fall through
       case SubArithOp:
        res = c1 - c2; break;

       default: return success;         // can't constant-fold
      }

      hasSideEffects_now = false;
      if (cc) {
        // condition codes set -- see if there's a branch we can eliminate
        assert(dest()->isNoPReg(), "shouldn't use result");
        Node* branch = next();
        if (branch->isBranchNode()) {
          eliminateNodeAndUsedPRs(_bb, NULL, true);    // delete comparison
          ((BranchNode*)branch)->eliminateBranch(c1, c2, res);
        }
      } else {
        constResult = new_ConstPReg(scope(), (oop)res);
        // enable further constant propagation of the result
        _src->removeUse(bb, srcUse);
        _src = constResult;
        srcUse = bb->addUse(this, _src);
      }
    }
    return success;
  }

  bool ArithNode::doCopyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    return PNode::copyPropagate(bb, u, d, replace);
  }
  
  bool ArithRRNode::doCopyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == operUse) {
      CP_HELPER(oper, operUse);
    } else {
      return ArithNode::doCopyPropagate(bb, u, d, replace);
    }
    return false;
  }
  
  bool TArithRRNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    bool res = doCopyPropagate(bb, u, d, replace);
    if (_src->isConstPReg() && oper->isConstPReg()) {
      assert(res, "CP must have worked");
      // can constant-fold this operation
      oop c1 = ((ConstPReg*)_src)->constant;
      oop c2 = ((ConstPReg*)oper)->constant;
      oop result;
      switch (op) {
       case TAddCCArithOp:      result = smi_add_prim((smiOop)c1, (smiOop)c2); break;
       case TSubCCArithOp:      result = smi_sub_prim((smiOop)c1, (smiOop)c2); break;
       case TMulCCArithOp:      result = smi_mul_prim((smiOop)c1, (smiOop)c2); break;
       case TDivCCArithOp:      result = smi_div_prim((smiOop)c1, (smiOop)c2); break;
       case TModCCArithOp:      result = smi_mod_prim((smiOop)c1, (smiOop)c2); break;
       
       case TALShiftCCArithOp:  result = smi_arithmetic_shift_left_prim((smiOop)c1, (smiOop)c2); break;
       case TARShiftCCArithOp:  result = smi_logical_shift_left_prim(   (smiOop)c1, (smiOop)c2); break;
       case TLLShiftCCArithOp:  result = smi_logical_shift_left_prim((   smiOop)c1, (smiOop)c2); break;
       case TLRShiftCCArithOp:  result = smi_logical_shift_right_prim(  (smiOop)c1, (smiOop)c2); break;

       case TAndCCArithOp:  result = smi_and_prim((smiOop)c1, (smiOop)c2); break;
       case TOrCCArithOp:   result = smi_or_prim(( smiOop)c1, (smiOop)c2); break;
       case TXorCCArithOp:  result = smi_xor_prim((smiOop)c1, (smiOop)c2); break;
       
       default:
        fatal1("unknown tagged opcode %ld", op);
      }
      bool ok = !result->is_mark();
      if (ok) {
        // constant-fold this operation
        constResult = new_ConstPReg(scope(), result);
        Node* discard = next1();
        removeNext(discard);
        discard->bb()->remove(discard);
        discard->removeUpToMerge();
#       if GENERATE_DEBUGGING_AIDS
          if (CheckAssertions) {
            bb->verify();
            ((BB*)bb->next())->verify();
          }
#       endif
        // enable further constant propagation of the result
        hasSideEffects_now = false;
        _src->removeUse(bb, srcUse);
        _src = constResult;
        srcUse = bb->addUse(this, _src);
      } else {
        // for now, can't constant-fold failure - can't use marks in ConstPReg
      }
    }
    return res;
  }

  bool TArithRRNode::doCopyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == srcUse) {
      if (d->isConstPReg() && ((ConstPReg*)d)->constant->is_smi())
        arg1IsInt = true;
      CP_HELPER(_src, srcUse);
    } else if (u == operUse) {
      if (d->isConstPReg() && ((ConstPReg*)d)->constant->is_smi())
        arg2IsInt = true;
      CP_HELPER(oper, operUse);
    } else {
      fatal("copyPropagate: not the source use");
    }
    return false;
  }
  
  bool TypeTestNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == srcUse) {
      if (d->isConstPReg()) {
        // we know the receiver - the type test is unnecessary!
        ConstPReg* c = (ConstPReg*)d;
        eliminateNodeAndUsedPRs(bb, NULL, false, c->constant->map()->enclosing_mapOop());
        return true;
      } else {
        CP_HELPER(_src, srcUse);
      }
    } else {
      fatal("don't have this use");
    }
    return false;
  }
  
  bool IndexedBranchNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == srcUse) {
      if (d->isConstPReg()) {
        // we know the receiver - the branch is unnecessary!
        ConstPReg* c = (ConstPReg*)d;
        eliminateNodeAndUsedPRs(bb, NULL, false, c);
        return true;
      } else {
        CP_HELPER(_src, srcUse);
      }
    } else {
      fatal("don't have this use");
    }
    return false;
  }
  
  bool AbstractArrayAtNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (u == argUse) {
      CP_HELPER(arg, argUse);
    } else {
      return BasicNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool AbstractArrayAtPutNode::copyPropagate(BB* bb, PUse* u, PReg* d,
                                             bool replace) {
    if (u == argUse) {
      CP_HELPER(arg, argUse);
    } else if (u == elemUse) {
      CP_HELPER(elem, elemUse);
    } else {
      return BasicNode::copyPropagate(bb, u, d, replace);
    }
    return false;
  }

  bool FlushNode::copyPropagate(BB* bb, PUse* u, PReg* d, bool replace) {
    if (replace && u == srcUse) {
      CP_HELPER(_src, srcUse);
    }
    return false;
  }

  inline void computeExposedBlocks(PReg* src, BlockPRegBList* l) {
    if (src->isBlockPReg()) {
      BlockPReg* r = (BlockPReg*)src;
      r->markEscaped();
      if (!l->contains(r)) l->append(r);
    }
  }
  
  void ArrayAtPutNode::computeExposedBlocks(BlockPRegBList* ll) {
    ::computeExposedBlocks(elem, ll);
  }

  void StoreNode::computeExposedBlocks(BlockPRegBList* ll) {
    ::computeExposedBlocks(_src, ll);
  }

  void ReturnNode::computeExposedBlocks(BlockPRegBList* ll) {
    if (_src) ::computeExposedBlocks(_src, ll);
  }

  void FlushNode::computeExposedBlocks(BlockPRegBList* ll) {
    // A flush of an uplevel-read does not expose a block because the block
    // could have been eliminated completely (i.e. the flush is a nop).
    // If the block wasn't eliminated completely, it must be exposed
    // somewhere else (arg to a real send or a store)
    if (_src->uplevelW) ::computeExposedBlocks(_src, ll);
  }

  // type test elimination
  
  bool BasicNode::endsWithUncommonTrap(Node* n) {
    // does n's basic block end with an uncommon trap?
    BB* bb = n->bb();
    while (n && !n->isUncommonNode() && n->bb() == bb) n = n->next();
    return n && n->isUncommonNode() && n->bb() == bb;
  }

  bool TArithRRNode::isFailureUncommon() {
    Node* n = next1();
    return n && endsWithUncommonTrap(n);
  }

  PReg* TArithRRNode::testedReg(fint n) {
    assert(n == 0, "only one reg tested");
    return oper;
  }

  void TArithRRNode::simplify(PReg* r, oop m) {
    if (r == oper) {
      if (PrintSICTypeTestOpt)
        lprintf("*could optimize tagged arith N%d\n", (void*)id());
      if (m == Memory->smi_map->enclosing_mapOop()) {
        // could eliminate tag check, but it's currently free (SPARC)
      } else {
        // non-int argument - will always fail
        // currently can't optimize much either
      }
    }
  }

  bool TBranchNode::isFailureUncommon() {
    if (nSuccessors() < 3) return true;
    Node* n = nexti(2);
    return n && endsWithUncommonTrap(n);
  }

  PReg* TBranchNode::testedReg(fint n) {
    assert(n == 0, "only one reg tested");
    return arg;
  }

  void TBranchNode::simplify(PReg* r, oop m) {
    if (r == arg) {
      if (PrintSICTypeTestOpt)
        lprintf("*optimizing tagged comparison N%d\n", (void*)id());
      if (m == Memory->smi_map->enclosing_mapOop()) {
        // can eliminate tag check
        intArg = true;
        if (intRcvr && intArg && nSuccessors() > 2) {
          Node* n = nexti(2);
          removeNext(n);
          n->removeUpToMerge();
        }
      } else {
        // non-int argument - will always fail
        // currently can't optimize much
        assert(!intArg, "conflicting type info");
      }
    }
  }

  bool TypeTestNode::isFailureUncommon() {
    Node* n = next();
    return n && hasUnknown && maps->length() == 1 && endsWithUncommonTrap(n);
  }

  oop TypeTestNode::testedType(fint n) {
    assert(n == 0 && maps->length() == 1, "can handle only one type");
    return maps->first();
  }

  void TypeTestNode::simplify(PReg* r, oop m) {
    if (deleted) return;
    if (r == _src) {
      fint i;
      for (i = maps->length() - 1; i >= 0 && maps->nth(i) != m; i--) ;
      if (i < 0) {
        // true type does not appear in our list!
        if (SICDebug) error("possible type misprediction?");
        if (!hasUnknown) fatal("conflicting type information");
        return;
      }
      if (PrintSICTypeTestOpt) lprintf("*eliminating type test N%d\n", (void*)id());
      eliminateUnnecessary(mapOop(m));
    }
  }

  oop IndexedBranchNode::testedType(fint n) {
    assert(n == 0, "only tests one type");
    return Memory->smi_map->enclosing_mapOop();
  }

  void IndexedBranchNode::simplify(PReg* r, oop m) {
    if (r == _src  &&  m == Memory->smi_map->enclosing_mapOop()) {
      if (PrintSICTypeTestOpt)
        lprintf("*optimizing type test of indexed branch N%d\n", (void*)id());
      srcMustBeSmi = true;
    }
  }


  bool AbstractArrayAtNode::isFailureUncommon() {
    Node* n = next1();
    return n && endsWithUncommonTrap(n);
  }

  // arrays: 0 = array, 1 = index, 2 = element
  PReg* AbstractArrayAtNode::testedReg(fint n) {
    assert(n < 2, "bad index");
    return n == 0 ? _src : arg;
  }

  void AbstractArrayAtNode::simplify(PReg* r, oop m) {
    if (r == arg) {
      if (PrintSICTypeTestOpt)
        lprintf("*optimizing array access N%d\n", (void*)id());
      if (m == Memory->smi_map->enclosing_mapOop()) {
        // can eliminate tag check on index
        intArg = true;
      } else {
        // non-int argument - will always fail
        // currently can't optimize much
        assert(!intArg, "conflicting type info");
      }
    }
  }

  oop ArrayAtNode::testedType(fint n) {
    assert(n < 2, "bad index");
    return n == 0 ?
      Memory->objVectorObj->map()->enclosing_mapOop() : Memory->smi_map->enclosing_mapOop();
  }

  oop ByteArrayAtNode::testedType(fint n) {
    assert(n < 2, "bad index");
    return n == 0 ?
      Memory->byteVectorObj->map()->enclosing_mapOop() : Memory->smi_map->enclosing_mapOop();
  }

  oop ArrayAtPutNode::testedType(fint n) {
    assert(n < 2, "bad index");
    return n == 0 ?
      Memory->objVectorObj->map()->enclosing_mapOop() : Memory->smi_map->enclosing_mapOop();
  }

  oop ByteArrayAtPutNode::testedType(fint n) {
    assert(n < 3, "bad index");
    return n == 0 ?
      Memory->byteVectorObj->map()->enclosing_mapOop() : Memory->smi_map->enclosing_mapOop();
  }

  PReg* ByteArrayAtPutNode::testedReg(fint n) {
    assert(n < 3, "bad index");
    switch (n) {
     case 0:    return _src;
     case 1:    return arg;
     case 2:    return elem;
     default:   fatal("bad index");
    }
    return NULL;
  }

  void ByteArrayAtPutNode::simplify(PReg* r, oop m) {
    if (r == elem) {
      if (PrintSICTypeTestOpt)
        lprintf("*optimizing byte array access N%d\n (elem)", (void*)id());
      if (m == Memory->smi_map->enclosing_mapOop()) {
        // can eliminate tag check on index
        intElem = true;
      } else {
        // non-int argument - will always fail
        // currently can't optimize much
        assert(!intElem, "conflicting type info");
      }
    }
    AbstractArrayAtPutNode::simplify(r, m);
  }

  // printing code (for debugging)

  void BasicNode::print_short() { char buf[1024]; lprintf("%s", print_string(buf)); }

  char* PrologueNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf,
              "Prologue %s%s nargs=%d frameSize=%d",
              needToFlushRegWindow ? "flush " : "",
              isAccessMethod ? "access" : "",
              (void*)nargs,
              (void*)thisFrameSize);
    if (printAddr) mysprintf(buf, "      p *(PrologueNode*)%#lx", this);
    return b;
  }

  char* ConstInitNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "ConstInitNode ");
    for (fint i = 0; i < constants.length(); i++) {
      mysprintf(buf, "%s ", constants.nth(i)->name());
    }
    if (printAddr) mysprintf(buf, "    p *(ConstInitNode*)%#lx", this);
    return b;
  }

  char* InterruptCheckNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "InterruptCheckNode");
    if (printAddr) mysprintf(buf, "      p *(InterruptCheckNode*)%#lx", this);
    return b;
  }

  char* LoadOffsetNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "LoadOffset %s[%#lx], %s",
              base->name(),
              (void*)offset,
              _dest->name());
    if (printAddr) mysprintf(buf, "      p *(LoadOffsetNode*)%#lx", this);
    return b;
  }

  char* LoadIntNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "LoadInt %#lx, %s", (void*)value, _dest->name());
    if (printAddr) mysprintf(buf, "      p *(LoadIntNode*)%#lx", this);
    return b;
  }

  char* LoadStackNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "LoadStack %s \"%s\"", frame->name(),
           stringOop(name)->copy_null_terminated());
    // if (nd) nd->print();
    mysprintf(buf, ", %s", _dest->name());
    if (printAddr) mysprintf(buf, "      p *(LoadStackNode*)%#lx", this);
    return b;
  }

  char* StoreOffsetNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "StoreOffset %s, %s[%#lx]",
           _src->name(), base->name(), (void*)offset);
    if (printAddr) mysprintf(buf, "      p *(StoreOffsetNode*)%#lx", this);
    return b;
  }

  char* StoreStackNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "StoreStack %s, %s \"%s\"", frame->name(),
           _src->name(), stringOop(name)->copy_null_terminated());
    // nd->print();
    if (printAddr) mysprintf(buf, "      p *(StoreStackNode*)%#lx", this);
    return b;
  }

  char* AssignNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s := %s", _dest->name(), _src->name());
    if (printAddr) mysprintf(buf, "      p *(AssignNode*)%#lx", this);
    return b;
  }
 
  char* SendNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s %s NLR %ld ", lookupTypeName(l), selector_string(sel),
           (void*)nlrPoint()->id());
/* too verbose for normal use (line gets too long)
    if (del) mysprintf(buf, "del=%#lx ", del);
    if (mh) mysprintf(buf, "mh=%#lx ", mh);
*/
    if (printAddr) mysprintf(buf, "      p *(SendNode*)%#lx", this);
    return b;
  }
 
  char* PrimNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "PrimCall _%s NLR %ld", pd->name(), (void*)nlrPoint()->id());
    if (printAddr) mysprintf(buf, "      p *(PrimNode*)%#lx", this);
    return b;
  }
 
  char* BlockCloneNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "BlockClone %s", _dest->name());
    if (printAddr) mysprintf(buf, "      p *(BlockCloneNode*)%#lx", this);
    return b;
  }
 
  char* BlockCreateNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "BlockCreate %s", _dest->name());
    if (printAddr) mysprintf(buf, "      p *(BlockCreateNode*)%#lx", this);
    return b;
  }
 
  char* InlinedReturnNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "InlinedReturn");
    if (printAddr) mysprintf(buf, "      p *(InlinedReturnNode*)%#lx", this);
    return b;
  }
 
  char* InlinedNonLocalReturnNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "InlinedNLR");
    if (printAddr) mysprintf(buf, "      p *(InlinedNonLocalReturnNode*)%#lx", this);
    return b;
  }
 
  char* NonLocalReturnNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "NonLocalReturn");
    if (printAddr) mysprintf(buf, "      p *(NonLocalReturnNode*)%#lx", this);
    return b;
  }
 
  char* MethodReturnNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "MethodReturn  %s", _src->name());
    if (printAddr) mysprintf(buf, "      p *(MethodReturnNode*)%#lx", this);
    return b;
  }
 
  const char* ArithNode::opName() { return ArithOpName[op]; }
 
  char* ArithRRNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s %s, %s, %s",
           opName(), _src->name(), oper->name(), _dest->name());
    if (printAddr) mysprintf(buf, "      p *(ArithRRNode*)%#lx", this);
    return b;
  }
 
  char* TArithRRNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s %s, %s, %s   N%d, N%d",
              ArithOpName[op], _src->name(), oper->name(), _dest->name(),
              (void*)next1()->id(),
              (void*)next()->id());
    if (printAddr) mysprintf(buf, "      p *(TArithRRNode*)%#lx", this);
    return b;
  }
 
  char* ArithRCNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s %s, #%#lx, %s",
           opName(), _src->name(), (void*)oper, _dest->name());
    if (printAddr) mysprintf(buf, "      p *(ArithRCNode*)%#lx", this);
    return b;
  }
 
  char* BranchNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "%s  N%ld N%ld",
           BranchOpName[op], (void*)next1()->id(), (void*)next()->id());
    if (printAddr) mysprintf(buf, "      p *(BranchNode*)%#lx", this);
    return b;
  }
 
  char* TBranchNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    Node* n1 = nSuccessors() > 1 ? nexti(1) : NULL;
    Node* n2 = nSuccessors() > 2 ? nexti(2) : NULL;
    mysprintf(buf, "T%s %s %s  N%ld N%ld fail N%ld",
              BranchOpName[op], _src->name(), arg->name(),
              (void*)(n1 ? n1->id() : -1),
              (void*)(next()->id()),
              (void*)(n2 ? n2->id() : -1));
    if (printAddr) mysprintf(buf, "      p *(TBranchNode*)%#lx", this);
    return b;
  }
 
  char* TypeTestNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "TypeTest %s, ", _src->name());
    for (fint i = 1; i <= maps->length(); i++) {
      oop m = maps->nth(i-1);
      if (m == Memory->smi_map->enclosing_mapOop()) mysprintf(buf, "int");
      else if (m == Memory->float_map->enclosing_mapOop()) mysprintf(buf, "float");
      else mysprintf(buf, m->debug_print());
      mysprintf(buf, ": N%ld; ",
                (void*)(nSuccessors() > i ? nexti(i)->id() : -1));
    }
    mysprintf(buf,
              "N%ld%s",
              (void*)(next() ? next()->id() : -1),
              hasUnknown ? "" : "*");
    if (printAddr) mysprintf(buf, "      p *(TypeTestNode*)%#lx", this);
    return b;
  }
 
  char* IndexedBranchNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "IndexedBranch %s, ", _src->name());
    for (fint i = 1;  i <= nCases;  i++) {
      mysprintf(buf, "%d: N%ld; ",
                (void*)i,
                (void*)(nSuccessors() > i ? nexti(i)->id() : -1));
    }
    mysprintf(buf,
              "N%ld%s",
              (void*)(next() ? next()->id() : -1),
              srcMustBeSmi ? "*" : "");
    if (printAddr) mysprintf(buf, "      p *(IndexedBranchNode*)%#lx", this);
    return b;
  }
 
  char* BlockZapNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "BlockZap %s", _dest->name());
    if (printAddr) mysprintf(buf, "      p *(BlockZapNode*)%#lx", this);
    return b;
  }
 
  char* FlushNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "Flush %s", _src->name());
    if (printAddr) mysprintf(buf, "      p *(FlushNode*)%#lx", this);
    return b;
  }

  char* ArrayAtNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "ArrayAt %s[%s], %s",
           _src->name(), arg->name(), _dest->name());
    if (printAddr) mysprintf(buf, "      p *(ArrayAtNode*)%#lx", this);
    return b;
  }
 
  char* ByteArrayAtNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "ByteArrayAt %s[%s], %s",
           _src->name(), arg->name(), _dest->name());
    if (printAddr) mysprintf(buf, "      p *(ByteArrayAtNode*)%#lx", this);
    return b;
  }
 
  char* ArrayAtPutNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "ArrayAtPut %s[%s] := %s, %s     p *(ArrayAtPutNode*)%#lx",
           _src->name(), arg->name(), elem->name(), _dest->name(), this);
    if (printAddr) mysprintf(buf, "      p *(ArrayAtPutNode*)%#lx");
    return b;
  }
 
  char* ByteArrayAtPutNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf,
              "ByteArrayAtPut %s[%s] := %s, %s",
              _src->name(), arg->name(), elem->name(), _dest->name());
    if (printAddr) mysprintf(buf, "      p *(ByteArrayAtPutNode*)%#lx", this);
    return b;
  }
 
  char* DeadEndNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "DeadEnd");
    if (printAddr) mysprintf(buf, "      p *(DeadEndNode*)%#lx", this);
    return b;
  }

  static fint prevsLen;
  static char* mergePrintBuf;
  static void printPrevNodes(Node* n) {
    mysprintf(mergePrintBuf, "N%ld%s", (void*)n->id(),
              --prevsLen > 0 ? ", " : "");
  }
  
  char* MergeNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "Merge (%s)", why);
    prevsLen = _prevs->length();
    mergePrintBuf = buf;
    _prevs->apply(printPrevNodes);
    buf = mergePrintBuf;
    if (printAddr) mysprintf(buf, "      p *(MergeNode*)%#lx", this);
    return b;
  }
 
  char* RestartNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "Restart N%ld", (void*)loopStart->id());
    if (printAddr) mysprintf(buf, "      p *(RestartNode*)%#lx", this);
    return b;
  }
 
  char* UncommonNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "UncommonBranch");
    if (printAddr) mysprintf(buf, "      p *(UncommonNode*)%#lx", this);
    return b;
  }

  char* NopNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "Nop");
    if (printAddr) mysprintf(buf, "      p *(NopNode*)%#lx", this);
    return b;
  }
 
  char* MarkerNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "Marker \"%s\" depth %d %s %s",
              selector_string(scope()->selector()),
              (void*)scope()->depth,
              invalid ? "(invalid)" : "", active ? "ACTIVE" : "");
    if (printAddr) mysprintf(buf, "      p *(MarkerNode*)%#lx", this);
    return b;
  }
 
  char* CommentNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "'%s' ", comment);
    if (printAddr) mysprintf(buf, "      p *(CommentNode*)%#lx", this);
    return b;
  }

  void BasicNode::printID() {
    char buf[32];
    lprintf("%4ld:%1s %-4s", (void*)id(), deleted ? "D" : " ",
            hasSplitSig() ? splitSig()->prefix(buf) : " ");
  }
  
  void PNode::verify() {
    if (deleted) return;
    if (!firstPrev() && !isPrologueNode())
      error1("PNode %#lx: no predecessor", this);
    if (firstPrev() && !firstPrev()->isSuccessor(this))
      error1("prev->next != this for PNode %#lx", this);
    if (_bb && !_bb->contains(this))
      error1("BB doesn't contain PNode %#lx", this);
    if (next() && ! next()->isPredecessor(this))
      error1("next->prev != this for PNode %#lx", this);
    if (_bb == NULL) error1("PNode %#lx: doesn't belong to any BB", this);
    if (next() == NULL && !isExitNode() && !isRestartNode() &&
        !isCommentNode())   // for the "rest of method omitted (dead)" comment
      error1("Node %#lx has no successor", this);
    if (next() != NULL && isExitNode()) {
      Node *n;
      for (n = next();
           n && (n->isCommentNode() || n->isDeadEndNode());
           n = n->next()) ;
      if (n) error2("exit node %#lx has a successor (%#lx)", this, next());
    }
    if (hasSrc()) src()->verify();
    if (hasDest()) {
      dest()->verify();
      if (dest()->isConstPReg())
        error2("Node %#lx: dest %#lx is ConstPR", this, dest());
    }
    if (isAssignmentLike() && (!hasSrc() || !hasDest()))
      error1("Node %#lx: isAssignmentLike() implies hasSrc/Dest", this);
    if (l && !l->isDefined())
      error1("Node %#lx has undefined label", this);
  }

  void LoadOffsetNode::verify() {
    if (deleted) return;
    PNode::verify();
    base->verify();
  }
  
  void LoadStackNode::verify() {
    if (deleted) return;
    PNode::verify();
    frame->verify();
  }

  void StoreOffsetNode::verify() {
    if (deleted) return;
    PNode::verify();
    base->verify();
  }
  
  void StoreStackNode::verify() {
    if (deleted) return;
    PNode::verify();
    frame->verify();
  }

 
  static MergeNode* mergeNodeBeingVerified;
  
  static void verifyPrevNodes(Node* n) {
    static char buf[BUFSIZ];
    if ( !n->isReachable() )
      error3("MergeNode %#lx, %s, has unreachable predecessor %#lx",
        mergeNodeBeingVerified, 
        mergeNodeBeingVerified->print_string(buf),
        n);
  }
  
  
  void MergeNode::verify() {
    if (deleted) return;
    
    // removeUpToMerge assumes that all predecessors are reachable,
    //  so check that here:
    mergeNodeBeingVerified= this;
    _prevs->apply(verifyPrevNodes);
    
    PNode::verify();
  }
  

  void BlockCloneNode::verify() {
    if (deleted) return;
    PrimNode::verify();
    if (!_dest->isBlockPReg())
      error2("BlockCloneNode %#lx: dest %s isn't a block", this, _dest->name());
    _dest->verify();
  }

  void MethodReturnNode::verify() {
    if (deleted) return;
    ReturnNode::verify();
    if (next()) error1("MethodReturnNode %#lx has a successor", this);
  }

  void NonLocalReturnNode::verify() {
    if (deleted) return;
    ReturnNode::verify();
    if (next()) error1("NonLocalReturnNode %#lx has a successor", this);
  }

  void InlinedReturnNode::verify() {
    if (deleted) return;
    ReturnNode::verify();
    if (!next()) error1("InlinedReturnNode %#lx has no successor", this);
    if (next() && next()->scope() == scope())
      error1("InlinedReturnNode %#lx: successor is in same scope", this);
  }

  void InlinedNonLocalReturnNode::verify() {
    if (deleted) return;
    ReturnNode::verify();
    if (!next())
      error1("InlinedNonLocalReturnNode %#lx has no successor", this);
    if (next() && next()->scope() == scope())
      error1("InlinedNonLocalReturnNode %#lx: successor is in same scope",
             this);
  }

  void ArithRRNode::verify() {
    if (deleted) return;
    ArithNode::verify();
    oper->verify();
  }

  void TArithRRNode::verify() {
    if (deleted) return;
    AbstractBranchNode::verify();
  }

  void AbstractBranchNode::verify() {
    if (deleted) return;
    PNode::verify();
  }

  void BlockZapNode::verify() {
    if (deleted) return;
    PNode::verify();
    if (!_dest->isBlockPReg())
      error2("BlockZapNode %#lx: dest %s isn't a block", this, _dest->name());
    _dest->verify();
  }

  void UncommonNode::verify() {
    if (deleted) return;
    // optimizeTypeTests relies on UncommonNode being last
    if (this != bb()->last)
      error1("UncommonNode %#lx: not last node in BB", this);
    PNode::verify();
  }
        
  void TypeTestNode::verify() {
    if (deleted) return;
    // optimizeTypeTests relies on TypeTestNode being last
    if (this != bb()->last)
      error1("TypeTestNode %#lx: not last node in BB", this);
    PNode::verify();
  }
        
 void IndexedBranchNode::verify() {
    if (deleted) return;
    // optimizeTypeTests relies on IndexedBranchNode being last
    if (this != bb()->last)
      error1("IndexedBranchNode %#lx: not last node in BB", this);
    PNode::verify();
  }
        
  void MarkerNode::checkSplitting(MergeSExpr* m, SExpr* splitExpr) {
    // The receiver is a copy of the original marker, created by splitting
    // on m.  Along this path of the split, m is replaced by splitExpr
    // (which is a part of m).
    // Change the xpr stack if necessary (so we can later find the correct
    // marker among all the copies).
    for (fint i = exprStack->length() - 1; i >= 0; i--) {
      if (exprStack->nth(i) == m) {
        exprStack->nthPut(i, splitExpr);
      }
    }
  }

  void MarkerNode::checkMap(SExpr* expr, oop p, const char* msg, fint n) {
    // assert(p != badOop, "should know p");
    // NB: can have badOops if in primitive failure -- expr stack is
    // still the primitive call expr stack, not the fail send expr stack
    // (this is a general problem with primitive failures)
    if (expr->hasMap() && !expr->map()->equal(p->map())) {
      invalid = true;
      if (PrintSICMarkers) {
        lprintf("*invalidating Marker %#lx: ", this);
        lprintf(msg, (void*)n);
        lprintf(" map %#lx vs %#lx\n", expr->map(), p->map()->enclosing_mapOop());
      }
    }
  }

  fint MarkerNode::depth() { return scope()->depth; }

  void MarkerNode::check() {
    // check if current node could be a restart point; if not, mark it invalid
    if (deleted || _bb == NULL) {
      invalid = true;
      return;
    }
    SCodeScope* s = scope();
    compiled_vframe* vf = s->vscope->vf;
    assert(s->selector() == vf->selector(), "mismatched selectors");
    checkMap(s->receiver, vf->receiver(), "receiver");
    assert(invalid || s->method() == vf->method(), "mismatched methods");
    checkMap(s->self     , vf->self()   , "self");
    {
      FOR_EACH_SLOTDESC_N(s->method()->map(), sd, i) {
        if (s->args[i]) {
          checkMap(s->args[i], vf->get_slot(sd), "arg %d", i);
        } else {
          // locals have no map (yet?)
        }
      }
    }
    oop* stack;
    smi len;
    vf->get_expr_stack(stack, len, true);
    for (fint i = exprStack->length() - 1; i >= 0; i--) {
      checkMap(exprStack->nth(i), stack[i], "stk %d", i);
    }
    assert(len == exprStack->length(), "expr stack lengths don't match");
  }

  // for gdb
  void printNodes(Node* n) {
    for ( ; n; n = n->next()) {
      n->printID(); n->print_short(); lprintf("\n");
    }
  }

  // architecture-independent gen nodes
        
  extern Location arith_genHelper(PReg* sreg, PReg* oper, PReg* dest,
                                  ArithOpCode op,
                                  Location& t1, Location& t2, bool& reversed);

  void ArithRRNode::gen() {
    BasicNode::gen();
    
    if (constResult) {
      Location dest = isRegister(_dest->loc) ? _dest->loc : Temp2;
      Location l = genHelper->moveToReg(constResult, dest);
      if (l != _dest->loc) genHelper->moveRegToLoc(dest, _dest->loc);
    } else {
      Location dummy;
      bool dummy2;
      Location dest = arith_genHelper(_src, oper, _dest, op, dummy, dummy, dummy2);
      if (dest != _dest->loc)
        genHelper->store(dest, spOffset(_dest->loc), SP);
    }
  }
  
  
  void AssignNode::gen() {
    BasicNode::gen();
    if (_src->loc == _dest->loc) return;
    if (_src->isConstPReg()) {
      genOop();
      return;
    }
    
    Location src = _src->loc;
    Location dest = _dest->loc;
    if (isRegister(src) && isRegister(_dest->loc)) {
      // common case: reg-to-reg
      genHelper->moveRegToReg(src, dest);
    } else if (isRegister(_dest->loc)) {
      // mem-to-reg
      genHelper->load(SP, spOffset(src), dest);
    } else if (isRegister(src)) {
      // reg-to-mem
      genHelper->store(src, spOffset(dest), SP);
    } else {
      // mem-to-mem
      genHelper->load(SP, spOffset(src), Temp1);
      genHelper->store(Temp1, spOffset(dest), SP);
    }
  }
  
  void BasicNode::genPcDesc() {
    fint b = bci();
    ScopeInfo si = scope()->scopeInfo;
    if (b != IllegalBCI && (b != lastBCI || si != lastScopeInfo)) {
      theSIC->rec->addPcDesc(theAssembler->offset(), si, b);
      lastBCI = b; lastScopeInfo = si;
    }
  }

  void BasicNode::gen() {
    if (GenerateAllPcDescs) genPcDesc();
  }

  void BlockCloneNode::gen() {
    BasicNode::gen();
    BlockPReg* blk = block();
    Location l_ = blk->loc;
    if (blk->primFailBlockScope && !clonePrimFailBlock) {
      // The current test for using prim fail blocks is broken - the location
      // needs to be initialized if there is a call or uncommon trap between
      // its creation.  For now, always initialize the location.
      genHelper->setToZero(l_);
      return;
    }

    if (isMemoized()) {
      // initialize memoized location
      if (isRegister(l_)) {
        genHelper->loadImmediateOop(deadBlockPR, l_, true);
      } else {
        Location t = genHelper->loadImmediateOop(deadBlockPR, Temp1, false);
        genHelper->moveRegToLoc(t, l_);
      }
    } else {
      genCall();
    }
  }
  
  void CommentNode::gen() { 
    BasicNode::gen();
  }
     
  void ConstInitNode::gen() {
    BasicNode::gen();
    for (fint i = constants.length() - 1; i >= 0; i--) {
      ConstPReg* r = constants.nth(i);
      genHelper->loadImmediateOop(r->constant, r->loc);
    }
  }
  
  void FlushNode::gen() {
    BasicNode::gen();
    PReg* regToFlush = _src->cpReg();
    Location l_ = regToFlush->loc;
    // Removed following assertion because code elimination 
    //  may eliminate all defs and uses of uplevel PReg.
    //   -- dmu
    // assert(l_ != UnAllocated || regToFlush->isConstPReg(),
    //       "should have a location to flush");
    if (!isRegister(l_)) return;
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        NameNode* n = _src->nameNode();
        assert(n->hasLocation() && n->location() == l_,
                 "cpReg() inconsistent with debug info");
      }
#   endif

    flushRegister(regToFlush);
  }

  void InlinedReturnNode::gen() {
    BasicNode::gen();
    theAssembler->Comment("inlined local return");
  }
  
  void InlinedNonLocalReturnNode::gen() {
    BasicNode::gen();
    theAssembler->Comment("inlined non-local return");
  }
  
  void LoadNode::genLoad(PReg* source, fint srcOffset) {
    BasicNode::gen();
    Location src = genHelper->moveToReg(source, Temp1);
    if (isRegister(_dest->loc)) {
      genHelper->load(src, srcOffset, _dest->loc);
    } else if (!_dest->isNoPReg()) {
      genHelper->load(src, srcOffset, Temp2);
      genHelper->store(Temp2, spOffset(_dest->loc), SP);
    }
  }

  void LoadOffsetNode::gen() {
    genLoad(base, offset);
  }
  
  void LoadStackNode::gen() {
    int32 offset = nd ? spOffset(nd->location(), nm) : 0;
    genLoad(frame, offset);
  }
  
  void MarkerNode::gen() {
    BasicNode::gen();
    pcOffset = theAssembler->offset();
  }

  void MergeNode::gen() {
    if (!l) l = new DefinedLabel(theAssembler->printing);
    BasicNode::gen();
  }
  
  void NopNode::gen() {
    BasicNode::gen();
  }

  bool MarkerNode::describePReg(PReg* r, SCodeScope** scopes,
                                ValueLocationNameDesc*& nd,
                                bool describeUnallocated) {
    // set nd (if possible) and return whether PReg is live and allocated
    fint dummy1, dummy2;
    if (r == NULL || (r->loc == UnAllocated && !describeUnallocated)) {
      // not used
      return false;
    } else if (r->scope == NULL) {
      assert(isGloballyAllocatedRegister(r->loc), "should be a hardwired reg");
      return false;
    } else if (r->scope != scopes[r->scope->depth]) {
      // not in current call stack --> not live
      return false;
    } else if (!r->scope->isSenderOrSame(PReg::findAncestor(_scope, dummy1,
                                                          r->scope, dummy2))) {
      // scope of r is below marker scope, so it cannot be live
      return false;
    } else if (r->isLiveAt(this)) {
      // preg is live; try to find nd
      assert(r->scope->isCodeScope(), "oops");
      nd = ((SCodeScope*)r->scope)->nameDescFor(r);
      if (nd) {
        nd->offset = r->scope->descOffset();
      } else if (r->isSAPReg()) {
        // must be copy-propagated SAPReg
        for (SCodeScope* s = ((SAPReg*)r)->creationScope;
             s != r->scope;
             s = s->sender()) {
          if (s->vscope) {
            nd = s->nameDescFor(r);
            if (nd) break;
          }
        }
      } else {
        // either a SplitReg, or the reg contains a block whose map we
        // couldn't figure out
      }
      
      if (nd == NULL && r->cpRegs) {
        // try to get info from equivalent regs
        for (fint i = r->cpRegs->length() - 1; i >= 0; i--) {
          if (describePReg(r->cpRegs->nth(i), scopes, nd, describeUnallocated))
            break;
        }
      }
      if (nd && r->isBlockPReg()) nd->block = ((BlockPReg*)r)->block;
      return true;
    } else {
      // not live
      return false;
    }
  }

  bool MarkerNode::checkContents(ValueLocationNameDesc* nd) {
    // check if nd is alrady in locs; if so, make sure contents agree
    // always returns true - makes it easier to use (don't need if GENERATE_DEBUGGING_AIDS)
    Location loc = nd->location();
    oop val = nd->value();
    for (fint i = locs->length() - 1; i >= 0; i--) {
      NameDesc* nd2 = locs->nth(i);
      if (loc == nd2->location()) {
        if (val != nd2->value()) {
          fail("SIC recompile: inconsistent register contents for %s",
               locationName(loc));
          if (WizardMode) error_breakpoint();
          return true;
        }
      }
    }
    return true;
  }
  
  void MarkerNode::describe() {
    assert(active, "should be active");
    locs = new NameDescBList(50);
    // get all scopes on current call stack
    SCodeScope* scope = this->scope();
    fint depth = scope->depth;
    SCodeScope **scopes= NEW_RESOURCE_ARRAY(SCodeScope*, depth + 1);
    scopes[depth] = scope;
    fint i;
    for (i = depth - 1; i >= 0; i--) {
      scopes[i] = scopes[i + 1]->sender();
      methodMap* mm = (methodMap*)scopes[i]->method()->map();
      if (mm->get_selector_at(scopes[i + 1]->senderBCI()) !=
          scopes[i + 1]->selector()) {
        // can't express live range of fail block args in bci terms
        // because there are 2 sends in the same bci -- arghh!
        // (see also SCodeScope::computeMasks)
        fail("SIC: can't recover prim. failure args for %s",
             selector_string(mm->get_selector_at(scopes[i + 1]->senderBCI())));
        return;
      }
    }

    // describe all live PRegs
    PRegBList* pregs = theSIC->bbIterator->pregTable;
    PRegBList unresolved(10);
    for (i = pregs->length() - 1; i >= 0; i--) {
      PReg* r = pregs->nth(i);
      ValueLocationNameDesc* nd;
      if (!describePReg(r, scopes, nd, false)) continue;        // not live
      if (PrintSICMarkers) {
        lprintf("*value of %s: vf %#lx ", r->name(), r->scope->vscope->vf);
        if (nd) nd->print(); else lprintf(" - not found");
        lprintf("\n"); 
      }
      if (nd == NULL) {
        if (r->weight < 0) {
          // was targeted to a register - will probably find its
          // equivalent PReg later
          unresolved.append(r);
          continue;
        } else {
          fail("SIC: couldn't find runtime value for %s", r->name());
          return;
        }
      }
      assert(checkContents(nd), "oops");
      locs->append(nd);
    }

    // add args to current send (aren't marked live if already in out regs)
    for (i = exprStack->length() - 1; i >= 0; i--) {
      SExpr* e = exprStack->nth(i);
      PReg* r = e->preg();
      Node* n = e->node();
      if (n && !r->isUnused() && n->hasDest()) {
        PReg* dest = n->dest();
        assert(r->isLiveAt(this), "should be live");
        if (isArgRegister(dest->loc)) {
          ValueLocationNameDesc* nd;
          describePReg(r, scopes, nd, true);
          if (nd) {
            nd->loc = dest->loc;
            assert(checkContents(nd), "oops");
            locs->append(nd);
          } else {
            fail("couldn't find expr stack elem %s", r->name());
            return;
          }
        }
      }
    }

    // try to resolve as yet unresolved PReg values
    while (unresolved.nonEmpty()) {
      PReg* r = unresolved.pop();
      fint i;
      for (i = locs->length() - 1; i >= 0; i--) {
        if (locs->nth(i)->location() == r->loc) {
          // ok, found run-time value via some other PReg
          break;
        }
      }
      if (i < 0) {
        fail("SIC: couldn't find runtime value (2) for %s", r->name());
        return;
      }
    }
           
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        bool haveReceiver = false;
        for (i = locs->length() - 1; i >= 0; i--) {
          NameDesc* nd = locs->nth(i);
          if (nd->location() == IReceiverReg) {
            haveReceiver = true;
            break;
          }
        }
        assert(haveReceiver, "receiver reg. should always be live");
      }
#   endif
  }

  sendDesc* MarkerNode::send_desc(nmethod* nm) {
    // The receiver is the active marker node, but it doesn't represent the
    // top of the stack.  Therefore, it must be "immediately" before a
    // non-inlined send.  Find this send and return the corresponding
    // sendDesc.
    assert(active, "not active");
    Node *n;
    for (n = next(); !n->isSendNode(); n = n->next()) {
      if (!n->canBeBetweenMarkerAndSend()) {
        // unexpected node between marker and send; e.g. send was
        // type-tested but branch currently on stack was not inlined
        // just fail for now
        if (PrintSICMarkers) {
          lprintf("*can't find Marker's sendDesc because of this node:\n");
          n->print();
        }
        return NULL;
      }
    }
    sendDesc* sd = sendDesc::sendDesc_from_call_instruction(
                     nm->insts() + ((SendNode*)n)->offset);
    assert(sd->verify(), "bad sendDesc");
    return sd;
  }

  void MarkerNode::fail(const char* msg, const void* arg) { 
    if (PrintRecompilation) warning1(msg, arg);
    invalid = true;
    theRecompilation->isReplacementSimple = active = false;
  }

  
  void PrologueNode::checkReceiverMap() {
    assert(theSIC->diLink == NULL && !L->isReceiverStatic(), "shouldn't be here");

    // test receiver map
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        switch (L->lookupType()) {
        case NormalLookupType:  break;
        case StaticNormalLookupType:
        case ImplicitSelfLookupType:
        case ResendLookupType:
        case DirectedResendLookupType: ShouldNotReachHere(); break;
        default: break;
        }
      }
#   endif

    Map* m = L->receiverMap();
    bool imm =  m == Memory->smi_map  ||  m == Memory->float_map;

    if (SICCountTypeTests) {
      theAssembler->startTypeTest(1, true, imm);
      theAssembler->doOneTypeTest();
    }

    if (m == Memory->smi_map)
      genHelper->smiOop_prologue(Memory->code->trapdoors->SendMessage_stub_td());
    else if (m == Memory->float_map)
      genHelper->floatOop_prologue(Memory->code->trapdoors->SendMessage_stub_td());
    else
      genHelper->memOop_prologue(L->receiverMapOop(),
                                 Memory->code->trapdoors->SendMessage_stub_td());
  }

  void PrologueNode::doMapSelectorDelegateeChecks() {
    if (!L->isReceiverStatic())
      checkReceiverMap(); // do approriate smi/float/memOop prologue
      
    theSIC->_verifiedOffset = theAssembler->offset();

    if (SICCountTypeTests) theAssembler->endTypeTest();
    theAssembler->Comment("verified entry point:");

    Label generalMiss(theAssembler->printing, NULL);
    if (L->isPerform()) {
      theAssembler->Comment("check selector");
      genHelper->checkOop(generalMiss, L->selector(), PerformSelectorLoc);
    }
      
    if (needsDelegatee(L->lookupType()) && !L->isDelegateeStatic()) {
      theAssembler->Comment("check delegatee");
      genHelper->checkOop(generalMiss, L->delegatee(), PerformDelegateeLoc);
    }
  }

  void PrologueNode::generateCountCode() {
    assert(GenerateCountCode, "should not be generating count code");

    MethodKind kind = isAccessMethod ? MethodKind(-1) : theSIC->method()->kind();
    fint assignableParents = L->adeps->length();
    int32* counter;

    if (assignableParents != 0) {
      counter = &NumberOfDIMethodCalls;
    } else if (isAccessMethod) {
      counter = &NumberOfAccessMethodCalls;
    } else if (kind == BlockMethodType) {
      counter = &NumberOfBlockMethodCalls;
    } else {
      counter = &NumberOfMethodCalls;
    }
    genHelper->genCountCode(counter);
  }

  void PrologueNode::resetUnusedBit() {
    assert(!isAccessMethod && GenerateLRUCode, "shouldn't be resetting LRU bit");

    theAssembler->Comment("reset unused bit");
    void* unused_addr = &LRUflag[Memory->code->nextNMethodID()];
    genHelper->setToZeroA(unused_addr, Temp2);
  }
  
  void PrologueNode::verifyAssignableParents() {
    theAssembler->Comment("verify state of assignable parents");
    fint assignableParents = L->adeps->length();
    fint count = 0;
    for (fint i = 0; i < assignableParents; ++i) {
      objectLookupTarget* target = L->adeps->start()[i];
      Location t = genHelper->loadPath(Temp2, target, LReceiverReg);
      count = genHelper->verifyParents(target, t, count);
    }
  }


  // Prologue format:
  //
  // *if not DI child
  //    <smi/float/memOop prologue>
  // _verified:                       (entry point from PICs)
  //    if necessary <check selector>
  //    if necessary <check delegatee>
  // *endif DI
    
  // _diCheck:                        (entry point after recompile)
  //    <verify assignable parents>
    
  // *if haveStackFrame
  //    create stack frame
  // *endif
    
  // *if GenerateCountCode
  //    <countCode>
  // *endif
    
  // <flush register windows if neceessary>
  // <clear stack temporaries and excess argument locations
  void PrologueNode::gen() {
    BasicNode::gen();
    genPcDesc(); 

    fint assignableParents = L->adeps->length();
    
    theAssembler->Comment("Begin Prologue");

    prePrologue();

    if (theSIC->diLink == NULL)
      doMapSelectorDelegateeChecks(); // only do what's necessary
    else {
      // don't check receiver map, selector, delegatee if a DI cache miss
      assert(assignableParents > 0, "should have some di parents to check");
    }

    
    theSIC->_diCheckOffset = theAssembler->offset();
    theAssembler->Comment("DI entry point:");
    
    if (assignableParents > 0)
      verifyAssignableParents();
    
    if (haveStackFrame())
      createStackFrame();
    
    if (GenerateCountCode)
      generateCountCode();

    if (!isAccessMethod && GenerateLRUCode)
      resetUnusedBit();
    
    postPrologue();

    theAssembler->Comment("End Prologue");
  }


  void StoreStackNode::gen() {
    BasicNode::gen();
    int32 offset = spOffset(nd->location(), nm);
    Location fr = genHelper->moveToReg(frame, Temp1);
    if (_src->isConstPReg()) {
      // store constant
      ConstPReg* value = (ConstPReg*)_src;
      Location t2 = genHelper->loadImmediateOop(value, Temp2, false);
      genHelper->store(t2, offset, fr);
    } else {
      if (isRegister(_src->loc)) {
        genHelper->store(_src->loc, offset, fr);
      } else {
        genHelper->load(SP, spOffset(_src->loc), Temp2);
        genHelper->store(Temp2, offset, fr);
      }
    }
  }
  

  void TBranchNode::gen() {
    BasicNode::gen();
    Location rcvrReg, argReg;
    bool haveImmediate = false;

    assert(intRcvr ||
           !( _src->isConstPReg() && ((ConstPReg*)_src)->constant->is_smi()),
           "intRcvr should be set");
    assert(intArg ||
           !(  arg->isConstPReg() && ((ConstPReg*) arg)->constant->is_smi()),
           "intArg  should be set");

    rcvrReg = genHelper->moveToReg(_src, Temp1);

    if (intArg  &&  arg->isConstPReg()  && isImmediate(smiOop(((ConstPReg*)arg)->constant))) {
      haveImmediate = true;
    } else {
      argReg = genHelper->moveToReg(arg, Temp2);
    }
           
    genCompare(haveImmediate, rcvrReg, argReg);
    if (intRcvr && intArg) {
      // no tag tests necessary
    }
    else {
      // on Sparc, we compare using tagged subtracts.  if rcvr - arg overflows,
      // then the overflow bit will be set.  but we only care about the cases
      // where the overflow bit is set because the arg and/or the rcvr aren't an
      // smiOop.
      // testTagsIfNecessary, on Sparc, checks the tags of the rcvr and/or arg
      // to see how the overflow was triggered (subtraction overflow vs. non-smiOop
      // rcvr and/or arg).  -mabdelmalek 11/02
      testTagsIfNecessary(haveImmediate, rcvrReg, argReg);
    }
    BranchNode::gen();
  }
  

  void TypeTestNode::define(fint index, Label* l_) {
    Node* n = nexti(index);
    n->l = l_->unify(n->l);
  }
  


 // next 2 for debugging (used when in gdb in the middle of a compile)

  // print me and my successors

  void BasicNode::print_succs(Node* n) {
    Unused(n);
    printID(); print_short(); lprintf("\n");
  }

  void BasicNode::print_preds(Node* n) {
    Unused(n);
    printID(); print_short(); lprintf("\n");
  }

  void PNode::print_succs(Node* n) {
    if (n  &&  nPredecessors() > 1  &&  n != prev(0))
      return;
    Node::print_succs();
    for ( fint i = 0;  i < nSuccessors(); ++i)
      nexti(i)->print_succs(this);
  }

  void PNode::print_preds(Node* n) {
    if (n  &&  nSuccessors() > 1  &&  n != nexti(0))
      return;
    for ( fint i = 0;  i < nPredecessors(); ++i)
      prev(i)->print_preds(this);
    Node::print_preds();
  }
  
  
  // for debugging: if set, compiler generates call to _Breakpoint before
  // every non-inlined send
  bool breakpointBeforeCall = false;
    
  void SendNode::genBreakpointBeforeCall() {
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  breakpointBeforeCall) {
        theNodeGen->enterScope(scope());
        Node* n =
          new PrimNode(getPrimDescOfString("_Breakpoint"),
                        NULL, 0, exprStack,
                        NULL, NULL);
        theNodeGen->exitScope(scope());
        n->gen();
      }
    # endif
  }
  
  void SendNode::verifySendInfo() {
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        switch (withoutExtraBits(l)) {
        case DirectedResendLookupType:
          assert(l & DelegateeStaticBit, "should have static delegatee");
          assert_string(del, "should be a string");
          // fall through
        case ImplicitSelfLookupType:
        case ResendLookupType:
        case StaticNormalLookupType:
        case NormalLookupType:
          assert(!isPerformLookupType(l), "should have a static selector");
          assert_string(sel, "should be a string");
          break;
        default:
          break;
        }
      }
    # endif
  }


# endif


/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "rscope.hh"
# include "_rscope.cpp.incl"

# ifdef SIC_COMPILER

  RScope::RScope(RAbstractSelfScope* s, fint bci) {
    _sender = s; _senderBCI = bci; 
    if (s) {
      s->addScope(bci, this);
      nsends = s->nsends;
    } else {
      nsends = -1;
    }
  }

  RAbstractSelfScope::RAbstractSelfScope(RAbstractSelfScope* s, fint bci,
                                         ScopeDesc* d, mapOop m)
  : RScope(s, bci), uncommon(1) {
    desc = d;
    if (d == NULL || d->isAccessScope()) {
      ncodes = 1;
    } else {
      methodMap* mm = (methodMap *) d->method()->map();
      ncodes = mm->length_codes();
    }
    _subScopes = NEW_RESOURCE_ARRAY(RScopeBList*, ncodes);
    for (fint i = 0; i < ncodes; i++) _subScopes[i] = NULL;
    _receiverMapOop = m;
  }

  RPICScope::RPICScope(nmethod* cllr, PcDesc* pc, sendDesc* s, mapOop rcvrMap,
                       nmethod* clle, CountStub *cs, fint lev, bool tr)
    : RAbstractSelfScope(NULL, pc->byteCode,
                         clle ? clle->scopes->root() : NULL,
                         rcvrMap) {
    caller= cllr;
    pcDesc= pc;
    sd= s;
    _receiverMapOop= rcvrMap;
    callee= clle;
    nsends= cs ? cs->count() :
        callee ? callee->invocationCount() / max(callee->ncallers(), 1) : 0;
    level= lev;
    trusted= tr;
    lookupType= sd->lookupType();
    extended= false;
  }
  
  RUntakenScope::RUntakenScope(nmethod* c, PcDesc* pc, sendDesc* s, fint lev)
    : RPICScope(c, pc, s, mapOop(badOop), NULL, NULL, lev, true) { }
  
  bool RAbstractSelfScope::equivalent_lookup(simpleLookup* l) {
    if (desc->l_equivalent(l)) {
      _receiverMapOop = l->receiverMapOop();        // hack for blocks
      return true;
    } else {
      return false;
    }
  }
  
  RSelfScope::RSelfScope(RAbstractSelfScope* s, fint bci, ScopeDesc* d)
    : RAbstractSelfScope(s, bci, d,
                         d ? d->rcvrExpr()->myMapOop() : mapOop(badOop))
  {}

  bool RSelfScope::equivalent_scope(SScope* s) {
    if (!s->isSelfScope()) return false;
    SSelfScope* ss = (SSelfScope*)s;
    // don't use ss->rscope because it may not be set yet; but ss's sender
    // must have an rscope if ss is equivalent to this.
    return ss->senderBCI() == desc->senderByteCodeIndex() &&
           ss->sender()->rscope == _sender;
  }
  
  bool RPICScope::equivalent_scope(SScope* s) {
    Unused(s);
    // an RPICScope represents a non-inlined scope, so it can't be equivalent
    // to any SScope
    return false;
  }

  oop RAbstractSelfScope::method() { return desc->method(); }

  void RAbstractSelfScope::addScope(fint bci, RScope* s) {
    assert(bci >= 0 && bci < ncodes, "bci out of range");
    if (_subScopes[bci] == NULL) _subScopes[bci] = new RScopeBList(5);
    assert(!_subScopes[bci]->contains(s), "already there");
    _subScopes[bci]->append(s);
  }

  RScope* RAbstractSelfScope::subScope(fint bci, simpleLookup* k) {
    // return the subscope matching the lookup
    assert(bci >= 0 && bci < ncodes, "bci out of range");
    RScopeBList* list = _subScopes[bci];
    if (list == NULL) return new RNullScope(this);
    for (fint i = 0; i < list->length(); i++) {
      RScope* rs = list->nth(i);
      if (rs->equivalent_lookup(k))
        return rs;
    }
    return new RNullScope(this);
  }

  RScopeBList* RAbstractSelfScope::subScopes(fint bci) {
    // return all subscopes at bci
    assert(bci >= 0 && bci < ncodes, "bci out of range");
    RScopeBList* list = _subScopes[bci];
    if (list == NULL) return new RScopeBList(1);
    return list;
  }

  bool RAbstractSelfScope::hasSubScopes(fint bci) {
    assert(bci >= 0 && bci < ncodes, "bci out of range");
    return _subScopes[bci] != NULL;
  }

  bool RAbstractSelfScope::isUncommonAt(fint bci, bool primCall) {
    assert(bci >= 0 && bci < ncodes, "bci out of range");

    // check if program got uncommon trap in the past
    for (fint i = 0; i < uncommon.length(); i++) {
      if (uncommon.nth(i)->bci() == bci) return false;
    }

    if (_subScopes[bci]) {
      RScope* s = _subScopes[bci]->first();

      // isTrusted() added by dmu 4/96 to avoid untrusted PICs
      //   from prevented prim failure being uncommon

      if (primCall && s && !s->isUntakenScope()  &&  isTrusted()) {
        // primitive failure was executed at least once - don't make it
        // uncommon
        return false;
      }
      
      if (_subScopes[bci]->length() == 1 && s->isUntakenScope()) {
        if (primCall) {
          // ok, prim failure was never taken - guess it's ok
          // (this is optimistic - might be primitive that always fails
          // but hasn't been executed yet)
          return true;
        } 

        // don't make uncommon if send was never executed (we are probably
        // optimizing a loop before the rest of the method containing the
        // loop has executed
        return false;
      }

      // performance bug: currently, PICs cannot handle performs.
      // Thus, do not allow uncommon branches
      // for the "otherwise" case because the inline cache contains only the
      // last case.
      RScopeBList* l = _subScopes[bci];
      for (fint i = l->length() - 1; i >= 0; i--) {
        RScope* s = l->nth(i);
        if (s->isPICScope()) {
          RPICScope* ps = (RPICScope*)s;
          if (isPerformLookupType(ps->lookupType)) 
            return false;
        }
      }
    }
    return true;
  }

  SExpr* RAbstractSelfScope::receiverExpr() {
    // guess that true/false map really means true/false object
    // (gives more efficient testing code)
    if (receiverMapOop() == mapOop(badOop)) {
      if (WizardMode) warning("SIC: unknown receiver expression");
      return new UnknownSExpr(NULL);
    } else if (receiverMap() == Memory->true_map()) {
      return new ConstantSExpr(Memory->trueObj, NULL, NULL);
    } else if (receiverMap() == Memory->false_map()) {
      return new ConstantSExpr(Memory->falseObj, NULL, NULL);
    } else {
      return new MapSExpr(receiverMapOop(), NULL, NULL);
    }
  }

  SExpr* RUntakenScope::receiverExpr() {
    return new UnknownSExpr(NULL, NULL, sd->wasNeverExecuted());
  }

  bool RUntakenScope::isUnlikely() { return sd->wasNeverExecuted(); }

  void RPICScope::unify(RAbstractSelfScope* s) {
    assert(ncodes == s->ncodes, "should be the same");
    for (fint i = 0; i < ncodes; i++) {
      _subScopes[i] = s->_subScopes[i];
      if (_subScopes[i]) {
        for (fint j = _subScopes[i]->length() - 1; j >= 0; j--) {
          _subScopes[i]->nth(j)->_sender = this;
        }
      }
    }
    uncommon.appendList(&s->uncommon);
  }

  RScopeBList* RNullScope::subScopes(fint bci) {
    Unused(bci);
    return new RScopeBList(1);
  }

  static int compare_pcDescs(const void* p1,  const void* p2) {
    PcDesc** r1 = (PcDesc**) p1;
    PcDesc** r2 = (PcDesc**) p2;
    return (*r2)->scope - (*r1)->scope;
  }
  
  static int compare_RPICScopes(const void* p1,  const void* p2) {
    RPICScope** r1 = (RPICScope**) p1;
    RPICScope** r2 = (RPICScope**) p2;
    return (*r2)->pcDesc->scope - (*r1)->pcDesc->scope;
  }
  
  
  typedef BoundedListTemplate<PcDesc*   >    PcDescBList;
  typedef BoundedListTemplate<RPICScope*> RPICScopeBList;

  static void getCallees(nmethod* nm, PcDescBList*& uncommon,
                         RPICScopeBList*& sends, bool trusted, fint level) {
    // return a list of all taken uncommon branches of nm, plus a list
    // of all nmethods called by nm (in the form of PICScopes)
    // both lists are sorted by scope (biggest offset first)
    if (PrintPICScopes) {
      lprintf("%*s*searching nm %#lx \"%s\" (%strusted; %ld callers)\n",
             (void*)(2 * level), "",
             nm, selector_string(nm->key.selector), trusted ? "" : "not ",
             (void*)nm->ncallers());
    }
    uncommon = new PcDescBList(1);
    sends = new RPICScopeBList(UsePICRecompilation ? 10 : 1);
    if (nm->compiler() == SIC) {
      for (addrDesc* a = nm->locs(), *aend = nm->locsEnd(); a < aend; a++) {
        if (a->isUncommonTrap()) {
          int32* instp = (int32*) a->addr(nm);
          assert(isUnimp(instp), "should be an unimp instruction");
          if (trapCount(instp) != 0) {
            uncommon->append(nm->containingPcDesc((char*)instp));
          }
        }
      }
    }
    qsort(uncommon->data_addr(), uncommon->length(), sizeof(PcDesc*),
          compare_pcDescs);
    if (UsePICRecompilation) {
      for (addrDesc* a = nm->locs(), *aend = nm->locsEnd(); a < aend; a++) {
        if (a->isSendDesc()) {
          sendDesc* sd = a->asSendDesc(nm);
          PcDesc* p = nm->containingPcDesc((char*)sd);
          fint len= sd->ntargets();
          switch (len) {
            case 0:
              // this send was never taken
              sends->append(new RUntakenScope(nm, p, sd, level));
              break;
            case 1: {
              // monomorphic IC
              assert (sd->pic() == NULL, "shouldn't have a PIC");
              nmethod *callee= sd->get_method();
              // If we are reusing nmethods and the method is reusable and
              // we skip the prologue, then we don't really know the rcvr
              // type...
              mapOop m=    ReuseNICMethods
                        && callee->reusable()
                        && sd->jump_addr() == callee->verifiedEntryPoint()
                          ? mapOop(badOop)
                          : callee->key.receiverMapOop();

              // If a method calls _IntAdd: indirectly and
              //  some other call to _IntAdd: fails,
              //  still need to make its failure uncommon.
              //  The problem is that the send of value:With: in the
              //  wrapper method has a 1-entry PIC.
              //  In order to prevent 1-entry PICs from always being trusted,
              //    I have added the "&& trusted" below.  dmu 4/96

              sends->append(new RPICScope(nm, p, sd, m,
                                          callee, sd->countStub(),
                                          level, 
                                          m != mapOop(badOop)  && trusted));
              break;
            }
            default: {
              CacheStub *pic= sd->pic();
              assert(pic, "should have PIC");
              bool useInfo= trusted || len <= UntrustedPICLimit;
              if (!useInfo) {
                // NB: ignore limit for block receivers - right case will be
                // picked by compiler; use last, not first, because first
                // case might be an immediate.

                // This is a terrible hack--the compiler should be restructured.
                // The problem is that this code is called BEFORE it knows
                // that it will have a constant (block lit) for the rcvr
                // of this send.
                // For now, just try all entries, instead of just last one.
                // was: pic->get_map(len-1)->map_addr()->is_block() (dmu 5/96)
                for (fint i = 0;  i < len;  i++) {
                  if (pic->get_map(i)->map_addr()->is_block()) {
                    useInfo= true;
                    break;
                  }
                }
              }
              if (useInfo) {
                for (fint i = 0; i < len; i++) {
                  sends->append(new RPICScope(nm, p, sd, pic->get_map(i),
                                              pic->get_method(i), pic->countStub(i),
                                              level, trusted));
                }
              } else if (PrintPICScopes) {
                lprintf("%*s*not trusting PICs in sd %#lx \"%s\" (%ld cases)\n",
                        (void*)(2*level), "", sd,
                        selector_string(sd->selector()), (void*)len);
              }
            }
          }
        }
      }
      qsort(sends->data_addr(), sends->length(), sizeof(RPICScope*),
            compare_RPICScopes);
    }
  }
    
  RAbstractSelfScope* constructRScopes(nmethod* nm, bool trusted,
                                       fint level) {
    // construct nm's RScope tree and return the root
    // level > 0 means recursive invocation through a RPICScope (level
    // is the recursion depth); trusted means PICs info is considered
    // accurate
    RAbstractSelfScope* current = NULL;
    RAbstractSelfScope* root = NULL;
    PcDescBList* uncommon;
    RPICScopeBList* sends;
    getCallees(nm, uncommon, sends, trusted, level);
    // visit each scope in the debug info and enter it into the tree
    FOR_EACH_SCOPE(nm->scopes, s) {
      // search s' sender RScope
      ScopeDesc* sender = s->sender();
      RAbstractSelfScope* rsender;
      for (rsender = current; rsender;
           rsender = rsender->sender()) {
        if (rsender->isSelfScope() &&
            ((RSelfScope*)rsender)->desc->is_equal(sender))
          break;
      }
      fint bci = sender ? s->senderByteCodeIndex() : IllegalBCI;
      current = new RSelfScope(rsender, bci, s);
      if (!root) {
        root = current;
        root->nsends = nm == recompilee ? nm->oldCount : nm->invocationCount();
      }

      // enter bcis with taken uncommon branches
      while (uncommon->nonEmpty() &&
             uncommon->top()->scope == s->offset()) {
        current->uncommon.push(new RUncommonBranch(current, uncommon->pop()));
      }
      // enter info from PICs
      while (sends->nonEmpty() &&
             sends->top()->pcDesc->scope == s->offset()) {
        RPICScope* ps = sends->pop();
        ps->_sender = current; 
        current->addScope(ps->bci(), ps);
      }
    }
    assert(sends->isEmpty(), "sends should have been connected to rscopes");
    assert(uncommon->isEmpty(),
           "uncommon branches should have been connected to rscopes");
    return root;
  }

  inline bool trustPICs(nmethod* nm) {
    fint ncallers = nm->ncallers();
    oop sel = nm->key.selector;
    if (sel == VMString[PLUS] || sel == VMString[MINUS] ||
        sel == VMString[TIMES] || sel == VMString[DIVIDE]) {
      // code space optimization: try to avoid unnecessary mixed-type
      // arithmetic
      return ncallers <= 1;
    } else {
      return ncallers <= PICTrustLimit;
    }
  }

  void RPICScope::extend() {
    // try to follow PIC info one level deeper (i.e. extend rscope tree)
    if (!extended && callee->isValid() && !callee->isZombie() && caller != callee) {
      // search the callee for type info
      RAbstractSelfScope* s =
        constructRScopes(callee, trusted && trustPICs(callee), level + 1);
      // s and receiver represent the same scope - unify them
      unify(s);
    }
    extended = true;
  }

  void RScope::print() {
    lprintf("; sender: %#lx@%ld; count %ld\n", _sender,
            (void*)_senderBCI,
            (void*)nsends);
  }

  void RAbstractSelfScope::printSubScopes() {
    fint i;
    for (i = 0; i < ncodes && _subScopes[i] == NULL; i++) ;
    if (i < ncodes) {
      lprintf("{ ");
      for (i = 0; i < ncodes; i++) {
        lprintf("%#lx ", _subScopes[i]);
      }
      lprintf("}");
    } else {
      lprintf("none");
    }
  }
  
  void RSelfScope::print_short() {
    lprintf("p *(RSelfScope*)%#lx \"%s\" #%ld",
           this, selector_string(desc->key.selector), (void*)nsends);
  }
  
  void RSelfScope::print() {
    print_short();
    lprintf(": scope %#lx; subScopes: ", desc);
    printSubScopes();
    if (uncommon.nonEmpty()) { lprintf("; uncommon "); uncommon.print(); }
    RScope::print();
  }

  void RPICScope::print_short() {
    lprintf("p *(RPICScope*)%#lx \"%s\" #%ld",
           this, selector_string(desc->key.selector), (void*)nsends);
  }
  
  void RPICScope::print() {
    print_short();
    lprintf(": sendDesc %#lx; subScopes: ", sd);
    printSubScopes();
    if (uncommon.nonEmpty()) { lprintf("; uncommon "); uncommon.print(); }
  }

  void RUntakenScope::print_short() {
    lprintf("p *(RUntakenScope*)%#lx \"%s\"",
           this, selector_string(sd->selector()));
  }
  
  void RUntakenScope::print() {
    print_short();
    lprintf(": sendDesc %#lx\n", sd);
    assert(!*_subScopes, "should have no subscopes");
    assert(uncommon.isEmpty(), "should have no uncommon branches");
  }

  void RNullScope::print_short() {
    lprintf("p *(RNullScope*)%#lx\n", this); 
  }

  void RUncommonBranch::print() {
    lprintf("p *(RUncommonScope*)%#lx : %#lx@%ld\n", this, scope, (void*)bci());
  }

  void RNullScope::printTree(fint bci, fint level) {
    Unused(bci); Unused(level); }

  void RScope::printTree(fint bci, fint level) {
    lprintf("%*s%3ld: ", (void*)(level * 2),
            "", (void*)bci);
    print_short(); lprintf("\n");
  }

  void RAbstractSelfScope::printTree(fint bci, fint level) {
    RScope::printTree(bci, level);
    for (fint i = 0; i < ncodes; i++) {
      if (_subScopes[i]) {
        if (_subScopes[i]->length() == 1 &&
            _subScopes[i]->first()->isUntakenScope()) {
          lprintf("  %*s%3ld: uncommon trap\n", (void*)(level * 2),
                  "", (void*)bci);
        }
        for (fint j = 0; j < _subScopes[i]->length(); j++) {
          _subScopes[i]->nth(j)->printTree(i, level + 1);
        }
      }
    }
  }

  void VScope::print() {
    lprintf("(VScope*)%#lx (vf %#lx) \"%s\"\n", this, vf,
           selector_string(vf->selector()));
  }
  
  void VScope::print_short() { lprintf("VScope %#lx", this); }
  
# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "genHelper.hh"

# include "_genHelper.cpp.incl"


# if defined(SIC_COMPILER)


  SICGenHelper* genHelper;

  SICGenHelper::SICGenHelper() { a = theAssembler; }

  void SICGenHelper::moveRegToLoc(Location srcReg, Location dest) {
    assert(isRegister(srcReg), "not a register");
    if (isRegister(dest))
      moveRegToReg(srcReg, dest);
    else
      store(srcReg, spOffset(dest), SP);
  }
  
  void SICGenHelper::moveLocToReg(Location src, Location destReg) {
    assert(isRegister(destReg), "not a register");
    if (isRegister(src))
      moveRegToReg(src, destReg);
    else
      load(SP, spOffset(src), destReg);
  }

  Location SICGenHelper::moveToReg(PReg* r, Location tempReg) {
    assert(isRegister(tempReg), "not a register");
    Location l = r->loc;
    if (isRegister(l)) {
      return l;
    } else if (l == UnAllocated) {
      assert(r->isConstPReg(), "only consts can be unallocated");
      return loadImmediateOop((ConstPReg*)r, tempReg, false);
    } else {
      load(SP, spOffset(l), tempReg);
      return tempReg;
    }
  }
  
  Location SICGenHelper::loadPath(Location dest,
                                  lookupTarget* target,
                                  Location receiver) {
    // *if root of path...
    //   *if holder is receiver...
    //     <move receiver, dest/t>
    //   *else...
    //     <loadImmediateOop obj, dest/t>
    //   *end
    // *else...
    //   <lookup dest/t, previousPath, receiver>
    // *end
      
    Location t = isRegister(dest) ? dest : Temp1;
    if (target->is_receiver()) {
      if (isRegister(receiver)) {
        t = receiver;
      } else {
        // Before the Intel port, the following was store(t, spOffset(receiver), SP);
        // I think it was wrong. -- dmu 5/06
        load(SP, spOffset(receiver), t);
      }
    } else {
      assert(target->is_object(), "must be an object path search");
      objectLookupTarget* otarget = (objectLookupTarget*) target;
      if (otarget->prevTargetSlot) {
        lookup(Temp1, otarget->prevTargetSlot, receiver);
        t = Temp1;
      } else {
        loadImmediateOop(otarget->obj, t);    // load oop
      }
    }
    return t;
  }


  void SICGenHelper::lookup(Location dest,
                            realSlotRef* path,
                            Location receiver) {
    // <loadPath dest, path, receiver>
    // load receiver/dest/t, offset - Mem_Tag, dest/t
    // <move t, dest>
      
    Location t = isRegister(dest) ? dest : Temp1;
    if (path->holder->is_object_or_map()) {
      Location t1 = loadPath(t, path->holder, receiver);
      load(t1, smiOop(path->desc->data)->byte_count() - Mem_Tag, t);
      // load data slot
    } else {
      fatal("don't support vframe lookups");
    }
    if (!isRegister(dest)) {
      store(t, spOffset(dest), SP);
    }
  }
    

# endif // SIC_COMPILER
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// primitive inlining in the SIC


# pragma implementation "sicPrimline.hh"

# include "_sicPrimline.cpp.incl"

# ifdef SIC_COMPILER

SExpr* SPrimScope::tryConstantFold() {
    const fint MaxPrimArgs = 5; // fix switch stmt below when increasing this
    if (!pd->canBeConstantFolded() || nargs > MaxPrimArgs) return NULL;
    oop a[MaxPrimArgs];         // argument oops
    if (!receiver->isConstantSExpr()) return NULL;
    oop r = receiver->constant();
    // get arguments (NB: args is in reverse order)
    fint i;
    for (i = 0; i < nargs; i++) {
      if (!args->nth(i)->isConstantSExpr()) return NULL;
      a[nargs - i - 1] = args->nth(i)->constant();
    };
    // ok, all args are consts: call the primitive
    oop res;
    fntype f = pd->fn();
    switch(nargs) {
     case 0:    res = f(r); break;
     case 1:    res = f(r, a[0]); break;
     case 2:    res = f(r, a[0], a[1]); break;
     case 3:    res = f(r, a[0], a[1], a[2]); break;
     case 4:    res = f(r, a[0], a[1], a[2], a[3]); break;
     case 5:    res = f(r, a[0], a[1], a[2], a[3], a[4]); break;
     default:   ShouldNotReachHere();
    }

    if (PrintInlining) {
      char buf[1024];
      char* b = buf;
      mysprintf(b, "%*s*constant-folding %s %s ", (void*)depth, "",
                r->debug_print(), pd->name());
      for (i = 0; i < nargs; i++) {
        mysprintf(b, "%s ", a[i]->debug_print());
      }
      mysprintf(b, "--> %s\n", res->debug_print());
      lprintf(buf);
    }

    // should scan backwards here and discard all nodes computing the args

    if (res->is_mark()) {
      // primitive will always fail
      ConstPReg* error = new_ConstPReg(_sender, res->memify());
      Node* dummy;
      MergeNode* mdummy = NULL;
      SExpr* failExpr =
        genPrimFailure(NULL, error, dummy, mdummy, resultPR, false);
      return failExpr;
    } else {
      theNodeGen->loadOop(res, resultPR);
      return new ConstantSExpr(res, resultPR, theNodeGen->current);
    }
  }

# define CHECK_INT(r)                                                         \
    ((r)->hasMap() && (r)->map() != Memory->smi_map)

# define CHECK_FLOAT(r)                                                       \
    ((r)->hasMap() && (r)->map() != Memory->float_map)

# define TYPECHECK(r, expr)                                                   \
    ((r)->hasMap() && !(r)->map()->expr)

  SExpr* SPrimScope::tryTypeCheck() {
    // for inlined prims, try to see if primitive will always fail
    if (!InlinePrimitives) return NULL;
    
    bool fail = false;
    switch (pd->type()) {
     case NotReallyAPrimitive:
     case InternalPrimitive:
      fatal("cannot call an internal primitive from Self code");
      return NULL;
     case IntComparisonPrimitive:
     case IntArithmeticPrimitive:
      // must have two smis
      fail = CHECK_INT(receiver) || CHECK_INT(args->last());
      break;

     case FloatArithmeticPrimitive:
     case FloatComparisonPrimitive:
      // must have two floats
      fail = CHECK_FLOAT(receiver) || CHECK_FLOAT(args->last());
      break;
      
     case AtPrimitive:
     case AtPutPrimitive:
      // must have array rcvr and smi arg
      fail = TYPECHECK(receiver, is_objVector()) || CHECK_INT(args->last());
      break;
     case SizePrimitive:
      // must have array rcvr
      fail = TYPECHECK(receiver, is_objVector());
      break;

     case ByteAtPutPrimitive:
      // stored value must be 0..255; for now, test only for integer
      fail = CHECK_INT(args->nth(0));
      // fall through
     case ByteAtPrimitive:
      // must have array rcvr and smi arg
      fail |= TYPECHECK(receiver, is_byteVector()) || CHECK_INT(args->last());
      break;
     case ByteSizePrimitive:
      // must have array rcvr
      fail = TYPECHECK(receiver, is_byteVector());
      break;

     default:
      return NULL;          
    }
    
    if (fail) {
      // primitive will always fail
      ConstPReg* error = new_ConstPReg(_sender, VMString[BADTYPEERROR]);
      Node* dummy;
      MergeNode* mdummy = NULL;
      return genPrimFailure(NULL, error, dummy, mdummy, resultPR, false);
    } else {
      return NULL;
    }
  }

  SExpr* SPrimScope::tryInline() {
    if (!InlinePrimitives) return NULL;
    switch (pd->type()) {
     case NotReallyAPrimitive:
     case InternalPrimitive:
      fatal("cannot call an internal primitive from Self code");
      return NULL;
     case ClonePrimitive:
      return inlineClone(); break;
     case IntComparisonPrimitive:
      return inlineIntComparison(); break;
     case IntArithmeticPrimitive:
      return inlineIntArithmetic(); break;
     case EQPrimitive:
      return inlineEQ(); break;
     case AtPrimitive:
      return inlineAt(true);
     case AtPutPrimitive:
      return inlineAtPut(true);
     case SizePrimitive:
      return inlineVectorSize(true);  
     case ByteAtPrimitive:
      return inlineAt(false);
     case ByteAtPutPrimitive:
      return inlineAtPut(false);
     case ByteSizePrimitive:
      return inlineVectorSize(false);  
     case RestartPrimitive:
      fatal("shouldn't call this for _Restart");
      break;
     case CloneVectorPrimitive:     // not worth inlining
     case UnwindProtectPrimitive:   // not worth inlining (yet)
     case FloatArithmeticPrimitive:
     case FloatComparisonPrimitive:
     case ExternalPrimitive:        // can't inline these
     default:
      return NULL;          
    }
    return NULL;
  }

  ConstPReg* SPrimScope::truePR() {
    return new_ConstPReg(_sender, Memory->trueObj);
  }
  ConstPReg* SPrimScope::falsePR() {
    return new_ConstPReg(_sender, Memory->falseObj);
  }
    

  SExpr* SPrimScope::inlineEQ() {
    if (PrintInlining) lprintf("%*s*inlining _Eq\n", (void*)(depth-1), ""); 
    NodeGen* ng = theNodeGen;
    if (SICDebug) ng->comment("inlined _Eq");
    SExpr* arg = args->top();
    PReg* r  = receiver->preg();
    PReg* ar = arg->preg();
    ng->append(new ArithRRNode(SubCCArithOp, r, ar, ng->noPR));
    Node* test = ng->append(new BranchNode(EQBranchOp));
    Node* falseNode;
    test->append(falseNode = new AssignNode(falsePR(), resultPR));
    SExpr* e1 = new ConstantSExpr(Memory->falseObj, resultPR, falseNode);
    MergeNode* merge = new MergeNode("inlineEQ merge");
    falseNode->append(merge);
    Node* trueNode = new AssignNode(truePR(), resultPR);
    ng->current = test->append1(trueNode);
    SExpr* e2 = new ConstantSExpr(Memory->trueObj, resultPR, trueNode);
    ng->branch(merge);
    SExpr* res = e1->copyMergeWith(e2, resultPR, ng->current);
    return res;
  }

  SExpr* SPrimScope::inlineClone() {
    // clones aren't really inlined, but can call specialized version if size
    // is known
    if (!receiver->hasMap()) return NULL;

    Map* map = receiver->map();
    if (map->is_smi() || map->is_float() || map->is_string()) {
      // cloning is a no-op
      if (PrintInlining) {
        lprintf("%*s*constant-folding (eliminating) _Clone\n",
                (void*)(depth-1), "");
      }
      return receiver;
    }

    if ( map->empty_object_size() != EMPTY_SLOTS_OOP_SIZE  // inlined clone method assumes this
    ||  !map->can_inline_clone())
      return NULL;

    fint slotCount = map->length_obj_slots();
    if (slotCount < 10) {
      // replace with specialized Clone primitive
      pd = getPrimDescOfSelector(VMString[_CLONE_0 + slotCount], true);
      assert(pd->type() == InternalPrimitive, "should be internal");
    }
    return NULL;
  }


  SExpr* SPrimScope::inlineIntArithmetic() {
    ArithOpCode op = opcode_for_selector(_selector);
      
    bool intRcvr =
      receiver->hasMap() && receiver->map() == Memory->smi_map;
    SExpr* arg = args->nth(0);
    bool intArg = arg->hasMap() && arg->map() == Memory->smi_map;
    if ( intArg
    &&   arg->isConstantSExpr()
    &&   intRcvr
    &&   arg->constant() == as_smiOop(0)
    &&   can_fold_rcvr_op_zero_to_zero(op)) {
      if (PrintInlining)
        lprintf("%*s*constant-folding %s: 0\n", (void*)(depth-1), "", ArithOpName[op]);
      return receiver;
    }
    if (PrintInlining) lprintf("%*s*inlining %s:\n", (void*)(depth-1),
                               "", ArithOpName[op]);

    if (!TArithRRNode::isOpInlinable(op))
      return NULL;
      
    NodeGen* n = theNodeGen;
    Node* arith = n->append(new TArithRRNode(op, receiver->preg(), arg->preg(),
                                             resultPR, intRcvr, intArg));

    // success case - no overflow, int tags
    MergeNode* ok = (MergeNode*)n->append(new MergeNode("inlineIntArithmetic ok"));
    SExpr* succExpr = new MapSExpr(Memory->smi_map->enclosing_mapOop(), resultPR, ok);
    // merge of success & failure branches
    MergeNode* done = (MergeNode*)ok->append(new MergeNode("inlineIntArithmetic done"));

    // failure case
    n->current = arith->append1(new NopNode);
    if (theSIC->useUncommonTraps &&
        sender()->rscope->isUncommonAt(sender()->bci(), true)) {
      n->uncommonBranch(currentExprStack(0), true);
      n->current = done;
      if (PrintInlining) {
        lprintf("%*s*making arithmetic failure uncommon\n", (void*)(depth-1),
                "");
      }
      return succExpr;
    } else {
      fint b = bci();
      PReg* error = new SAPReg(_sender, b, b);
      if (intRcvr && intArg) {
        // must be overflow
        n->loadOop(VMString[OVERFLOWERROR], error);
      } else {
        arith->hasSideEffects_now = true;    // may fail, so can't eliminate
        if (intRcvr || TARGET_ARCH == I386_ARCH) {
          // arg & TagMask already done by TArithRRNode
          // I386 does 'em all
        } else {
          PReg* t = new TempPReg(this, Temp1, false, true);
          n->append(new ArithRCNode(AndCCArithOp, t, Tag_Mask, t));
          n->current->hasSideEffects_now = true;
        }
        // Note: this code assumes that condcode EQ means overflow
        Node* branch = n->append(new BranchNode(EQBranchOp));
        // no overflow, must be type error
        n->loadOop(VMString[BADTYPEERROR], error);
        MergeNode* cont = (MergeNode*)n->append(
          new MergeNode("inlineIntArithmetic cont"));
        // overflow error
        PReg* err = new_ConstPReg(_sender, VMString[OVERFLOWERROR]);
        n->current = branch->append1(new AssignNode(err, error));
        n->branch(cont);
      }
      Node* dummy;
      SExpr* failExpr = genPrimFailure(NULL, error, dummy, done, resultPR);
      assert(done, "merge should always exist");
      return succExpr->mergeWith(failExpr, done);
    }
  }

  SExpr* SPrimScope::inlineIntComparison() {
    BranchOpCode cond;
           if (_selector == VMString[_INT_EQ_]) {
      cond = EQBranchOp;
    } else if (_selector == VMString[_INT_LT_]) {
      cond = LTBranchOp;
    } else if (_selector == VMString[_INT_LE_]) {
      cond = LEBranchOp;
    } else if (_selector == VMString[_INT_GT_]) {
      cond = GTBranchOp;
    } else if (_selector == VMString[_INT_GE_]) {
      cond = GEBranchOp;
    } else if (_selector == VMString[_INT_NE_]) {
      cond = NEBranchOp;
    } else {
      return NULL;
    }

    bool intRcvr =
      receiver->hasMap() && receiver->map() == Memory->smi_map;
    SExpr* arg = args->nth(0);
    bool intArg = arg->hasMap() && arg->map() == Memory->smi_map;
    if (PrintInlining)
      lprintf("%*s*inlining int comparison prim\n", (void*)(depth-1), "");

    NodeGen* n = theNodeGen;
    Node* branch = n->append(new TBranchNode(cond, receiver->preg(), intRcvr,
                                             arg->preg(), intArg));

    // false branch
    n->move(falsePR(), resultPR);
    SExpr* falseExpr= new ConstantSExpr(Memory->falseObj, resultPR, n->current);
    MergeNode* done = (MergeNode*)n->append(
      new MergeNode("inlineIntComparison done"));

    // true branch
    n->current = branch->append1(new AssignNode(truePR(), resultPR));
    SExpr* trueExpr = new ConstantSExpr(Memory->trueObj, resultPR, n->current);
    n->branch(done);

    SExpr* res = trueExpr->copyMergeWith(falseExpr, resultPR, done);
    // failure branch
    if (!intRcvr || !intArg) {
      branch->hasSideEffects_now = true;
      if (theSIC->useUncommonTraps &&
          sender()->rscope->isUncommonAt(sender()->bci(), true)) {
        n->current = branch->append(2, new NopNode);
        n->uncommonBranch(currentExprStack(0), true);
        n->current = done;
        if (PrintInlining) {
          lprintf("%*s*making int comparison failure uncommon\n",
                  (void*)(depth-1), "");
        }
      } else {
        fint b = bci();
        PReg* error = new SAPReg(_sender, b, b);
        PReg* err = new_ConstPReg(_sender, VMString[BADTYPEERROR]);
        n->current = branch->append(2, new AssignNode(err, error));
        Node* dummy;
        SExpr* failExpr = genPrimFailure(NULL, error, dummy, done, resultPR);
        assert(done, "merge should always exist");
        res = (MergeSExpr*)res->mergeWith(failExpr, done);
      }
    }
    return res;
  }

  SExpr* SPrimScope::inlineVectorSize(bool objVector) {
    bool okRcvr = receiver->hasMap();
    Map* rm;
    if (okRcvr) {
      rm = receiver->map();
      if (objVector) {
        okRcvr = rm->is_objVector();
      } else {
        okRcvr = rm->is_byteVector();
      }
    }
    if (!okRcvr) {
      // receiver type not known statically
      return NULL;
    }
    if (PrintInlining)
      lprintf("%*s*inlining %s\n", (void*)(depth-1), "",
             objVector ? "_Size:" : "_ByteSize:");
    
    NodeGen* n = theNodeGen;
    if (SICDebug) n->comment("inlined _Size:/_ByteSize:");
    fint offset =  objVector ? 
                   objVectorOopClass:: objVector_len_offset() : 
                  byteVectorOopClass::byteVector_len_offset();
    n->append(new LoadOffsetNode(receiver->preg(), offset, resultPR));
    return new MapSExpr(Memory->smi_map->enclosing_mapOop(), resultPR, n->current);
  }

  SExpr* SPrimScope::inlineAt(bool objVector) {
    assert(_selector == VMString[objVector ? _AT_ : _BYTE_AT_],
           "bad selector");
    bool okRcvr = receiver->hasMap();
    Map* rm;
    if (okRcvr) {
      rm = receiver->map();
      if (objVector) {
        okRcvr = rm->is_objVector();
      } else {
        okRcvr = rm->is_byteVector();
      }
    }
    if (!okRcvr) {
      // receiver type not known statically
      return NULL;
    }
    if (PrintInlining)
      lprintf("%*s*inlining %s\n", (void*)(depth-1), "",
              objVector ? "_At:" : "_ByteAt:");
    
    SExpr* arg = args->nth(0);
    NodeGen* ng = theNodeGen;
    if (SICDebug) ng->comment("inlined _At:/_ByteAt:");
    fint b = bci();
    bool intArg   = arg->hasMap() && arg->map() == Memory->smi_map;
    bool willFail = arg->hasMap() && arg->map() != Memory->smi_map;
    bool useUncommonTrap = !willFail && theSIC->useUncommonTraps &&
      sender()->rscope->isUncommonAt(sender()->bci(), true);
    // optimization: don't set error reg if using uncommon trap
    // (primitive will be reexecuted anyway)
    PReg* errorPR = useUncommonTrap ? NULL : new SAPReg(_sender, b, b);
    Node* at;
    if (objVector) {
      fint size = ((slotsMap*)rm)->empty_vector_object_size();
      at = new ArrayAtNode(receiver->preg(), arg->preg(), intArg,
                           resultPR, errorPR, size * oopSize - Mem_Tag);
                           
    } else {
      at = new ByteArrayAtNode(receiver->preg(), arg->preg(), intArg,
                               resultPR, errorPR);
    }
    ng->append(at);
    
    // success case - int index, in bounds
    NopNode* ok = (NopNode*)ng->append(new NopNode);
    // merge of success & failure branches
    MergeNode* done = (MergeNode*)ok->append(new MergeNode("inlineAt done"));

    // failure case
    ng->current = at->append1(new NopNode);
    if (useUncommonTrap) {
      if (PrintInlining) {
        lprintf("%*s*making at: failure uncommon\n", (void*)(depth-1), "");
      }
      ng->uncommonBranch(currentExprStack(0), true);
      ng->current = done;
    } else {
      Node* dummy;
      SExpr* failExpr = genPrimFailure(NULL, errorPR, dummy, done, resultPR);
      assert(done, "merge should exist");
    }
    return new UnknownSExpr(resultPR, ok);
  }

  SExpr* SPrimScope::inlineAtPut(bool objVector) {
    assert(_selector == VMString[objVector ? _AT_PUT_ : _BYTE_AT_PUT_],
           "bad selector");
    bool okRcvr = receiver->hasMap();
    Map* rm;
    if (okRcvr) {
      rm = receiver->map();
      if (objVector) {
        okRcvr = rm->is_objVector();
      } else {
        okRcvr = rm->is_byteVector();
      }
    }
    if (!okRcvr) {
      // receiver type not known statically
      return NULL;
    }
    if (PrintInlining)
      lprintf("%*s*inlining _%sAtPut:\n", (void*)(depth-1),
              "", objVector ? "" :"Byte");
    
    SExpr* arg = args->nth(1);
    NodeGen* ng = theNodeGen;
    if (SICDebug) ng->comment("inlined _At:Put:/_ByteAt:Put:");
    fint b = bci();
    bool intArg   = arg->hasMap() && arg->map() == Memory->smi_map;
    bool willFail = arg->hasMap() && arg->map() != Memory->smi_map;
    bool useUncommonTrap = !willFail && theSIC->useUncommonTraps &&
      sender()->rscope->isUncommonAt(sender()->bci(), true);
    PReg* errorPR = useUncommonTrap ? NULL : new SAPReg(_sender, b, b);
    Node* at;
    if (objVector) {
      PReg* elementArgPR = args->nth(0)->preg();
      
      // materialize value arg
      theNodeGen->materializeBlock(elementArgPR, _sender->sig, new PRegBList(1));
      
      fint size = ((slotsMap*)rm)->empty_vector_object_size();
      at = new ArrayAtPutNode(receiver->preg(), arg->preg(), intArg,
                              elementArgPR, resultPR, errorPR,
                              size * oopSize - Mem_Tag);
    } 
    else {
      SExpr* value = args->nth(0);
      bool intVal = value->hasMap() && value->map() == Memory->smi_map;
      willFail   |= value->hasMap() && value->map() != Memory->smi_map;
      at = new ByteArrayAtPutNode(receiver->preg(), arg->preg(), intArg, 
                                  value->preg(), intVal, resultPR, errorPR);
    }
    ng->append(at);
    
    // success case - int index, in bounds
    MergeNode* ok = (MergeNode*)ng->append(new NopNode);
    // merge of success & failure branches
    MergeNode* done = (MergeNode*)ok->append(new MergeNode("inlineAtPut done"));

    // failure case
    SExpr* res = receiver->shallowCopy(resultPR, ok);
    ng->current = at->append1(new MergeNode("inlineAtPut current"));
    if (useUncommonTrap) {
      if (PrintInlining) {
        lprintf("%*s*making atPut: failure uncommon\n", (void*)(depth-1), "");
      }
      ng->uncommonBranch(currentExprStack(0), true);
      ng->current = done;
    } else {
      Node* dummy;
      SExpr* failExpr = genPrimFailure(NULL, errorPR, dummy, done, resultPR);
      assert(done, "node should always exist");
      res = res->mergeWith(failExpr, done);
    }
    return res;
  }
  
  SExpr* SPrimScope::genPrimFailure(PrimNode* call, PReg* errorReg,
                                    Node*& test, MergeNode*& merge,
                                    PReg* resultReg, bool failure) {
    // generate primitive failure code
    // two modes:
    //    if call == NULL, omit the test for failure because it's already
    //          been generated (inlined prim.); in this case, errorReg
    //          must be set
    //    if call != NULL, generate test code (setting test & merge node args)
    // returns the result of the failure branch

    // pop prim args (they're not on the expr stack anymore in the fail branch)
    while (npop-- > 0) exprStack()->pop();
    
    SCodeScope* s = sender();
    NodeGen* ng = theNodeGen;
    if (call) {
      fint b = bci();
      SAPReg* t = new SAPReg(s, b, b);
      // extract tag field and test for mark tag
      ng->append(new ArithRCNode(AndArithOp, call->dest(), Tag_Mask, t));
      ng->append(new ArithRCNode(SubCCArithOp, t, Tag_Mask, ng->noPR));
      test = ng->append(new BranchNode(NEBranchOp));
      // failure branch; load error string
      if (!errorReg) errorReg = new SAPReg(s, b, b);
      ng->current = 
        test->append(new ArithRCNode(SubArithOp, call->dest(),
                                     Mark_Tag-Mem_Tag, errorReg));
    }

    SExpr* failReceiver = hasFailBlock ? failBlock : receiver;
    SendInfo* info = new SendInfo(failReceiver, NormalLookupType, false, false,
                                  (stringOop)failSelector, NULL);
    info->computeNSends(rscope, bci());
    info->primFailure = failure;
    info->restartPrim = call == NULL;   // restart inlined prims (unc. traps)
    s->exprStack->push(failReceiver);
    if (errorReg->isConstPReg()) {
      s->exprStack->push(new ConstantSExpr(((ConstPReg*)errorReg)->constant,
                                           errorReg, ng->current));
    } else {
      s->exprStack->push(new MapSExpr(Memory->stringObj->map()->enclosing_mapOop(),
                                      errorReg, ng->current));
    }
    ConstPReg* failSelReg = new_ConstPReg(s, selector());
    s->exprStack->push(new ConstantSExpr(selector(), failSelReg, NULL));
    SExpr* res = s->inlineSend(info);

    if (res->isNoResultSExpr()) {
      // never returns
      ng->current = merge; // set to NULL if no merge
    } 
    else {
      if (needZap) {
        assert(failBlock->preg()->isBlockPReg(), "should be a block");
        ng->zapBlock((BlockPReg*)failBlock->preg());
      }
      ng->move(res->preg(), resultReg);
      res = res->shallowCopy(resultReg, ng->current);
      // moved creation down from before if res->isNoResult... 
      //   to avoid creating unreachable merge -- dmu
      if (merge == NULL) merge = new MergeNode("genPrimFailure merge"); 
      ng->append(merge);
    }
    return res;
  }

# endif
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "sicSplit.hh"
# include "_sicSplit.cpp.incl"

  const uint32 SplitSig::LevelMask = 0xf;

  class SplitSetting : public StackObj {
  public:
    SplitSig*& sig;
    SplitSig* saved;
    SplitSetting(SplitSig*& oldsig, SplitSig* newsig) : sig(oldsig) {
      saved = oldsig; oldsig = newsig;
    }
    ~SplitSetting() { sig = saved; }
  };

  SplitSig* new_SplitSig(SplitSig* current, fint splitID) {
    fint level = current->level() + 1;
    assert(level <= MaxSplitDepth, "max. split level exceeded");
    uint32 newID = splitID << ((MaxSplitDepth - level + 1) << 2);
    SplitSig* sig =
      (SplitSig*)((uint32(current) & ~SplitSig::LevelMask) | newID | level);
    assert(current->contains(sig), "should be in same branch");
    return sig;
  }

  void SplitSig::print() {
    char buf[MaxSplitDepth + 1];
    lprintf("SplitSig %#lx: %s", this, prefix(buf));
  }

  const char* SplitSig::prefix(char* buf) {
    // fill buf with an ASCII representation of the receiver and return buf
    // e.g. a level-2 sig with first branch = 1 and 2nd branch = 3 --> "AB"
    fint l = level();
    buf[l--] = 0;
    uint32 sig = uint32(this) >> 4;
    while (l >= 0) {
      buf[l--] = 'A' + (sig & 0xf);
      sig = sig >> 4;
    }
    return buf;
  }


  // compiler code for splitting

  bool SCodeScope::shouldSplit(SendInfo* info) {
    assert(info->rcvr->isMergeSExpr(), "should be merge expr");
    MergeSExpr* r = (MergeSExpr*) info->rcvr;
    assert(r->isSplittable(), "should be splittable");
    if (!SICSplitting) return false;
    if (sig->level() == MaxSplitDepth) return false;
    Node* current = theNodeGen->current;
    if (!current->isSplittable()) return false;

    fint cost = 0;
    fint costLimit = theSIC->inlineLimit[SplitCostLimit];

    Node* n = NULL;
    // compute the cost of all nodes that would be copied (i.e. all exprs
    // with a map type)
    fint i;
    for (i = 0; i < r->exprs->length(); i++) {
      SExpr* expr = r->exprs->nth(i);
      if (!expr->hasMap()) continue;    // won't copy these

      // ConstantExprs, for example have no node, so have no cost
      if ( expr->node() == NULL ) continue;

      SSelfScope* theScope = expr->node()->scope();
      fint theBCI = expr->node()->bci();
      for (SExpr* e = expr; e; e = e->next) {
        if (e->node()->scope() != theScope || e->node()->bci() != theBCI) {
          // make sure all subexpressions have the same scope
          // (otherwise can't describe live range of that value when split)
          // could fix this with better splitting (introduce temps to
          // "synchronize" the value's scopes)
          if (PrintInlining) {
            lprintf("%*s*not splitting %s: too complicated (scopes)\n",
                    (void*)depth, "", info->sel->copy_null_terminated());
          }
          r->setSplittable(false);      // no sense trying again
          return false;
        }
        Node* prev;
        // This for loop misses some merges becaues it only follows first successor of n.
        // That's why the assertion below is commented out. -- dmu 6/03
        for ( n = e->node(); 
              cost <= costLimit  &&  n != NULL  &&  n != current; 
              n = n->next()) {
          cost += n->cost();
          if (!n->isSplittable()) {
            if (PrintInlining) {
              lprintf("%*s*not splitting %s: unsplittable node\n",
                    (void*)depth, "", info->sel->copy_null_terminated());
            }
            return false;
          }
#         if GENERATE_DEBUGGING_AIDS
            if (CheckAssertions) {
              prev = n;       // for easier debugging
            }
#         endif
        }
        // assert(n, "why didn't we find current?"); -- because only followed first succ above (dmu)
        if ( n == NULL  ||  cost > costLimit) goto done;
      }
    }
    
   done:
    if ( n != current  ||  cost > costLimit ) {
      if (PrintInlining) {
        lprintf("%*s*not splitting %s: cost too high (%ld > %ld) or did not find current\n",
                (void*)depth, "",
                info->sel->copy_null_terminated(),
                (void*) cost, 
                (void*) costLimit );
      }
      if (n == current) {
        // When we changed cost to count up from 0 instead of down,
        // we found a bug: Urs was passing in what would be
        // costLimit-cost, but we think it should just be cost.
        //  -- dmu & ole a 6/96
        theSIC->registerUninlinable(info, SplitCostLimit, cost );
      }
      return false;
    }
    
    return true;
  }

  SExpr* SCodeScope::splitMerge(SendInfo* info, MergeNode*& newMerge) {
    // Split this send: copy nodes between sources of oldRcvr's parts and the current
    // node, then inline the send along all paths; merge paths back at newMerge,
    // result in info->resReg.
    // (rewritten & refactored by dmu 5/96)
    
    newMerge = new MergeNode("splitMerge newMerge");
    MergeSExpr* oldRcvr = (MergeSExpr*)info->rcvr;
    assert(oldRcvr->isSplittable(), "should be splittable");

    // performance bug - fix this: can't split on MergeSExpr more than once
    // because after changing the CFG the old expr points to the wrong nodes.
    //  -- urs
    // 
    // (After talking to Urs, I believe I can explain the above.)
    // Before splitting, oldRcvr contained a reference
    //  to a MergeNode. 
    // During splitting, the MergeNode was used
    //  to find the end-point of the nodes to copy.
    // After splitting, the original MergeNode survives on only one
    //  path.
    // So the Expr could not be split again because its MergeNode no longer
    //  identifies all the paths that would need to be copied.
    // 
    // Now, it seems to me that the merge expr should be mutated to
    //  point to the new merge node and new merged values.
    // But, since Exprs are generally treated as immutable, 
    //  that could be problematic.
    //    -- dmu 5/96
    //
    // For now, just prohibit future splitting on this expr.
    
    if ( !SICMultipleSplitting )
      oldRcvr->setSplittable(false);
    
    memoizeBlocks(info->sel);
    if (PrintInlining) {
      lprintf("%*s*splitting %s\n", (void*)depth,
              "", selector_string(info->sel));
    }

    Node* lastNodeToSplit = theNodeGen->current;

    OopBList*   splitRcvrMaps = new   OopBList(10);     // receiver map of each branch
    SExprBList* splitRcvrs    = new SExprBList(10);     // receiver expr of each branch
    NodeBList*  splitHeads    = new  NodeBList(10);     // first node of each branch

    bool needMapLoad = false;
    SExpr* resultOfInlinedSends = NULL;
    splitInlinablePaths( newMerge,
                         info,
                         oldRcvr,  
                         lastNodeToSplit, 
                         
                         splitRcvrMaps,
                         splitRcvrs,
                         splitHeads,
                         needMapLoad,
                         resultOfInlinedSends );    

    UnknownSExpr* u = oldRcvr->findUnknown();

    if (u && splitRcvrMaps->length() > 0) {
      addTypeTestWhereUnknownsMerge(  info,  oldRcvr,  u,  needMapLoad,
                                      splitRcvrMaps,  splitRcvrs,  splitHeads,
                                      lastNodeToSplit );
    }

    if (info->needRealSend) {
      // The non-inlined send will be generated along the original (merged)
      // path, which will then branch to "newMerge".
      theNodeGen->current = lastNodeToSplit;
    } else {
      // discard the original path - can no longer reach it
      theNodeGen->current = newMerge;
    }

    if ( resultOfInlinedSends &&  resultOfInlinedSends->isMergeSExpr()) 
      resultOfInlinedSends->setNode(newMerge, info->resReg);

    if ( SICMultipleSplitting ) {
      //  Try mutating oldRcvr. This may be dangerous since SExpr's are
      //  in general immutable. However, its node must be set to the final merge
      //  for this split (will be used for future splitting).
      //  And its exprs must cover all the receiver expressions.

      if (u)
          splitRcvrs->append(u);
      oldRcvr->resetTo( newMerge, splitRcvrs );
    }
    return resultOfInlinedSends;
  }



  // for all paths with map info, split off paths and inline the send
  // set info->needRealSend if send is needed,
  // store maps, rerceivers, heads in the 3 lists
  // set needMapLoad, resultOfInlinedSends
  
  void SCodeScope::splitInlinablePaths( MergeNode*  newMerge,
                                        SendInfo*   info,
                                        MergeSExpr* oldRcvr, 
                                        Node*       lastNodeToSplit,
                                        
                                        OopBList*   splitRcvrMaps,
                                        SExprBList* splitRcvrs,
                                        NodeBList*  splitHeads,
                                        bool&       needMapLoad,
                                        SExpr*&     resultOfInlinedSends ) {
    needMapLoad = false;
    resultOfInlinedSends = NULL;
    
    fint ncases = oldRcvr->exprs->length();
    for (fint i = 0; i < ncases; i++) {
      bool didSplit = splitPathIfInlinable(  
                                 newMerge,
                                 info,
                                 oldRcvr, 
                                 i, 
                                 lastNodeToSplit, 
                                      
                                 splitRcvrMaps, 
                                 splitRcvrs, 
                                 splitHeads,
                                 needMapLoad,
                                 resultOfInlinedSends);
      if (didSplit)
        ;   // cool
      else  if ( oldRcvr->exprs->nth(i)->isUnknownUnlikely() )  
        ;   // also cool, the send is unlikely
      else  // can't inline - need to append a real send after current
        info->needRealSend = true;
    }
  }


  bool SCodeScope::splitPathIfInlinable( 
                               MergeNode*  newMerge, 
                               SendInfo*   info,
                               MergeSExpr* oldRcvr, 
                               fint        i,
                               Node*       lastNodeToSplit,
                               
                               OopBList*   splitRcvrMaps,
                               SExprBList* splitRcvrs,
                               NodeBList*  splitHeads,
                               bool&       needMapLoad,
                               SExpr*&     resultOfInlinedSends ) {
                               
    // is path inlinable?
    
    SExpr* nthOldRcvr = oldRcvr->exprs->nth(i);
    assert(!nthOldRcvr->isConstantSExpr() || nthOldRcvr->next == NULL ||
           nthOldRcvr->constant() == nthOldRcvr->next->constant(),
             "shouldn't happen: merged consts - convert to map");
    if ( ! nthOldRcvr->hasMap() )
      return false;
    SSelfScope* nthScope = tryLookup(info, nthOldRcvr);
    if ( nthScope == NULL )
      return false;
    
    
    // Create a new PReg&SExpr for the receiver so that it has the right
    // scope (the nodes will replace the old result reg with the new
    // one while they are copied).  
    // (Since we're splitting starting at the original producer, the
    // original result may be in an arbitrary subscope of the receiver
    // or even in a sibling.  For reg. allocation et al., the result
    // must have a PReg that's live from the producing point to here.)
    
    SplitSetting setting(theSIC->splitSig, new_SplitSig(sig, i + 1));
    if (nthScope->isCodeScope())
      ((SCodeScope*)nthScope)->sig = theSIC->splitSig;
    SplitPReg* newPR = coveringRegFor(nthOldRcvr, theSIC->splitSig);
    SExpr* nthNewRcvr = nthOldRcvr->shallowCopy(newPR, nthOldRcvr->node());
    
    
      
    Node* mapMerge = new MergeNode("splitPathIfInlinable mapMerge"); // where all copied paths merge
    splitHeads->append( mapMerge);
    splitRcvrs->append( nthNewRcvr);
    
    // compute map needed for type test for this path
    // if not immediate, will need to load it
    
    if (nthOldRcvr->isConstantSExpr()) {
      splitRcvrMaps->append( nthOldRcvr->constant());
    } 
    else {
      splitRcvrMaps->append( nthOldRcvr->map()->enclosing_mapOop());
      Map* m = nthOldRcvr->map();
      needMapLoad |= m != Memory->smi_map && m != Memory->float_map;
    }
    
    // split off paths of all SExprs with this map up to merge point (mapMerge)
    splitPathsWithSameMap(mapMerge, oldRcvr, nthOldRcvr, nthNewRcvr, newPR);

    // copy everything between mapMerge and lastNodeToSplit
    theNodeGen->current = copyPath(mapMerge, oldRcvr->node(), lastNodeToSplit,
                               NULL, NULL, oldRcvr, nthNewRcvr);
      
    // now inline the send
    SExpr* e = doInline(nthScope, nthNewRcvr, theNodeGen->current, NULL);
    if (!e->isNoResultSExpr()) {  
      theNodeGen->append(new NopNode);
      e = e->shallowCopy(info->resReg, theNodeGen->current);
      resultOfInlinedSends = 
        resultOfInlinedSends 
          ? resultOfInlinedSends->mergeWith(e, newMerge) 
          : e;
    }
    theNodeGen->branch(newMerge);

    return true;
  }


  void SCodeScope::splitPathsWithSameMap( Node*        mapMerge,
                                          MergeSExpr*  oldRcvr, 
                                          SExpr*       nthOldRcvr, 
                                          SExpr*       nthNewRcvr,
                                          SplitPReg*   newPR ) {
                                      
    // split off paths of all SExprs with this map up to merge point (mapMerge)
    
    Node* rmerge = oldRcvr->node();
    assert(rmerge, "should have a node");
    for (SExpr* expr = nthOldRcvr;   expr;   expr = expr->next) {
      Node* n = expr->node();
      PReg* oldPR = expr->preg();
      assert(n->isSplittable(), "can't handle branches etc. yet");
      Node* frst = n->next();
      n->removeNext(frst);

      // insert an assignment
      // It is not safe to replace n's dest
      //  because other nodes might be using its value.
      //  cf the bytesDo: bug -- dmu 5/96
      
      if (newPR)
        n = n->append(new AssignNode(oldPR, newPR));
        
      n = copyPath(n, frst, rmerge, oldPR, newPR, oldRcvr, nthNewRcvr);
      n = n->append(mapMerge);
    }
  }



  void SCodeScope::addTypeTestWhereUnknownsMerge( SendInfo*     info,
                                                  MergeSExpr*   oldRcvr, 
                                                  UnknownSExpr* unknownOldRcvr,
                                                  bool          needMapLoad,
                                                  OopBList*     splitRcvrMaps,
                                                  SExprBList*   splitRcvrs,
                                                  NodeBList*    splitHeads,
                                                  
                                                  Node*& lastNodeToSplit) {
    // insert a type test after oldMerge (all unknown paths should meet
    // at that node)
    
    
    // Performance bug: the known-but-uninlinable sends will also go
    // through the type test; they should be redirected until after the
    // test.  The problem is that oldMerge may not be the actual merge
    // point but slightly later (i.e. a few InlinedReturns later).
    fint diff;
    if (WizardMode && PrintInlining &&
      (diff = oldRcvr->exprs->length() - splitRcvrMaps->length()) > 1) {
      lprintf("*unnecessary %d-way type test for %d cases\n",
                (void*)splitRcvrMaps->length(),
                (void*)diff);
    }
    
    Node* oldMerge = oldRcvr->node();
    Node* oldNext = oldMerge->next();
    if (oldNext) 
      oldMerge->removeNext(oldNext);
      
    PReg* oldRcvrPR = oldRcvr->preg();
    Node* typeCase = new TypeTestNode(oldRcvrPR, splitRcvrMaps, needMapLoad, true);
    oldMerge->append(typeCase);
    
    if ( !theSIC->useUncommonTraps) // uncommon, unshmommon
      info->needRealSend = true;
      
    if (info->needRealSend ) {
      // connect fall-through (unknown) case to old merge point's successor (oldNext)
      // i.e. oldNext leads to the real send
      if (oldNext) {
        typeCase->append(oldNext);
      }
      else {
        assert(lastNodeToSplit == oldMerge, "oops");
        lastNodeToSplit = typeCase->append(new NopNode);
      }
    } else {
      // make unknown case uncommon
      if (oldNext) {
        // must copy nodes between old merge point (i.e. end of the send
        // generating the receiver value) and last node to split; this code
        // computes the args of the current send
        theNodeGen->current = copyPath(typeCase, oldNext, lastNodeToSplit,
                                         NULL, NULL, oldRcvr, unknownOldRcvr);
      } 
      else {
        assert(lastNodeToSplit == oldMerge, "oops");
        theNodeGen->current = typeCase;
      }
      theNodeGen->uncommonBranch(currentExprStack(0), info->restartPrim);
      if (PrintInlining) {
        lprintf("%*s*making %s uncommon (3)\n",
                 (void*)depth, "", selector_string(info->sel));
      }
    }
    // finally add the success branches to the type case
    //  (link up with the known split paths)
    for (fint j = 0; j < splitRcvrMaps->length(); j++) {
      Node* n = new AssignNode(oldRcvrPR, splitRcvrs->nth(j)->preg());
      typeCase->append(j + 1, n);
      n->append(splitHeads->nth(j));
    }
  }
  

  Node* SCodeScope::copyPath(Node* n, Node* start, Node* end,
                             PReg* oldPR, PReg* newPR,
                             MergeSExpr* rcvr, SExpr* newRcvr) {
    // copy the path from start to end, replacing occurrences of oldPR
    // with newPR; append copies to n, return last node
    if (SICDebug) {
      char* s = NEW_RESOURCE_ARRAY(char, 100);
      sprintf(s, "start of copied code: %#lx(N%d) --> %#lx(N%d) @ %#lx(N%d)",
              (unsigned long)start, start->id(),
              (unsigned long)end, end->id(),
              (unsigned long)n, n->id());
      n = n->append(new CommentNode(s));
    } 
    assert(!oldPR || !oldPR->isBlockPReg(), "cannot handle BlockPRegs");
    for (Node* c = start; true; c = c->next()) {
      assert(c->isSplittable(), "can't handle branches yet");
      Node* copy = c->copy(oldPR, newPR);
      if (copy && copy->isMarkerNode()) {
        ((MarkerNode*)copy)->checkSplitting(rcvr, newRcvr);
      }
      if (copy) n = n->append(copy);
      if (c == end) break;
    }
    if (SICDebug) n = n->append(new CommentNode("end of copied code"));
    return n;
  }

  SplitPReg* SCodeScope::coveringRegFor(SExpr* expr, SplitSig* sg) {
    // create a PReg with a live range covering all nodes between the
    // producer and the receiver scope/bci
    // see also SAPReg::isLiveAt
    SSelfScope* s = expr->node()->scope();
    fint bci = expr->node()->bci();
    assert(s->isCodeScope(), "oops");
    SplitPReg* r = SAPReg::regCovering(this, _bci, (SCodeScope*)s, bci, sg);
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        for (SExpr* e = expr; e; e = e->next) {
          SSelfScope* s2 = e->node()->scope();
          fint bci2 = e->node()->bci();
          assert(s2 == s, "oops");
          assert(bci2 == bci, "oops");
        }
      }
#   endif
    return r;
  }

# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "opcode.hh"
# include "_opcode.cpp.incl"

const char* BranchOpName[] = {
  "Bra",
  "Beq", "Bne",
  "Blt", "Ble", "Bltu", "Bleu",
  "Bge", "Bgt", "Bgeu", "Bgtu",
  "Bvs", "Bvc",
  "BrIfIntTag", "BrIfNotIntTag",
  "BrIfFloatTag", "BrIfNotFloatTag",
  "BrIfMemTag", "BrIfNotMemTag",
  "BrIfDelayedValue", "BrIfNotDelayedValue",
  "Last (INVALID)"
  };

const char* ArithOpName[] = {
  "Nil (INVALID)",
  "Add", "Sub", "Mul", "Div",
  "And", "Or", "XOr",
  "Asl", "Lsl",
  "Asr", "Lsr",

  "AddCC", "Cmp", "AndCC", "OrCC",
  "TAddCC", "TSubCC", "TMulCC", "TDivCC", "TModCC",
  "TALShiftCC", "TARShiftCC", "TLLShiftCC", "TLRShiftCC",
  "TAndCC", "TOrCC", "TXorCC",
  
  "Last (INVALID)"
  };

void opcode_init() {
  if (sizeof(BranchOpName) / sizeof(char*) != LastBranchOp + 1)
    fatal("forgot to change BranchOpName after changing BranchOpCode");
  if (sizeof(ArithOpName) / sizeof(char*) != LastArithOp + 1)
    fatal("forgot to change ArithOpName after changing ArithOpCode");
}


ArithOpCode opcode_for_selector(oop sel) {
  return
       sel == VMString[_INT_ADD_    ]    ?        TAddCCArithOp
  :    sel == VMString[_INT_AND_    ]    ?        TAndCCArithOp
  :    sel == VMString[_INT_ALSHIFT_]    ?    TALShiftCCArithOp
  :    sel == VMString[_INT_MOD_    ]    ?        TModCCArithOp
  :    sel == VMString[_INT_ARSHIFT_]    ?    TARShiftCCArithOp
  :    sel == VMString[_INT_DIV_    ]    ?        TDivCCArithOp
  :    sel == VMString[_INT_LLSHIFT_]    ?    TLLShiftCCArithOp
  :    sel == VMString[_INT_LRSHIFT_]    ?    TLRShiftCCArithOp
  :    sel == VMString[_INT_MUL_    ]    ?        TMulCCArithOp
  :    sel == VMString[_INT_OR_     ]    ?         TOrCCArithOp  
  :    sel == VMString[_INT_SUB_    ]    ?        TSubCCArithOp
  :    sel == VMString[_INT_XOR_    ]    ?        TXorCCArithOp
  :                                                  NilArithOp;
}

bool can_fold_rcvr_op_zero_to_zero(ArithOpCode op) {
  switch (op) {
   case  TALShiftCCArithOp:  return true;
   case  TARShiftCCArithOp:  return true;
   case  TLLShiftCCArithOp:  return true;
   case  TLRShiftCCArithOp:  return true;
   case       TOrCCArithOp:  return true;
   case      TAddCCArithOp:  return true;
   case      TSubCCArithOp:  return true;
   case      TXorCCArithOp:  return true;
   default:                  return false;
  }
}
/* Sun-$Revision: 30.17 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sicScope.hh"
# pragma implementation "sicScope_inline.hh"

# include "_sicScope.cpp.incl"

# ifdef SIC_COMPILER

  fint SScope::currentScopeID = 0;


  SendInfo::SendInfo(MethodLookupKey* k) {
    rcvr = NULL;
    l = k->lookupType;
    isSelfImplicit = (l &  ImplicitSelfBit) ? true : false;
    isUndirectedResend = baseLookupType(l) == ResendBaseLookupType;
    sel = (stringOop)k->selector;
    del = (stringOop)k->delegatee;
    key = k;
    init();
  }

  void SendInfo::computeNSends(RScope* rscope, fint bci) {
    RScopeBList* lst = rscope->subScopes(bci);
    nsends = 0;
    for (fint i = lst->length() - 1; i >= 0; i--) {
      nsends += lst->nth(i)->nsends;
    }
  }

  SSelfScope::SSelfScope(SCodeScope* sender, RScope* rs, SendInfo* info) {
    _sender = sender;
    if (sender) {
      _senderBCI = sender->_bci;
      sender->subScopes->append(this);
      depth = _sender->depth + 1;
      loopDepth = _sender->loopDepth;
    } else {
      _senderBCI = IllegalBCI;
      depth = loopDepth = 0;
    }
    receiver = result = nlrResult = NULL;
    if (info && info->resReg) {
      resultPR = info->resReg;
    } else {
      resultPR = NULL;
    }
    rscope = rs;
    if (rscope) {
      rs->extend();
    }
      
    vscope = BadVScope;           // for better debugging
    predicted = info ? info->predicted : false;
  }

  void SSelfScope::computeVScope() {
    if (vscope && vscope != BadVScope) return;          // already set
    if (_sender && _sender->vscope &&
        _sender->bci() == _sender->vscope->vf->bci()) {
      vscope = _sender->vscope->callee;
      if (vscope) {
        if (!vscope->vf->receiver()->map()->equal(receiver->map())) {
          // receiver types don't match (e.g. wrong branch of PIC-predicted
          // send)
          vscope = NULL;
        } else if (method() != vscope->vf->method() ||
                   selector() != vscope->vf->selector()) {
          // this scope was affected by programming -- don't use it
          vscope = NULL;
        }
      }
    } else {
      vscope = NULL;
    }
  }
  
  void SSelfScope::addResult(SExpr* e) {
    if (result == NULL) {
      result = e;
    } else {
      result = result->mergeWith(e, result->node());
    }
  }
  
  SCodeScope::SCodeScope( oop         m,
                          SCodeScope* sender,
                          RScope*     rs,
                          SendInfo*   info )
  : SSelfScope(sender, rs, info) {
    _key = info->key; _method = m;
#   if GENERATE_DEBUGGING_AIDS
      // not an assertion because it may fail in very rare cases (when
      // recompiling lookup error methods - new method gets created by lookup)
      if (CheckAssertions  &&  !(rscope->isNullScope() || rscope->method() == m))
        warning("wrong rscope");
#   endif
    endsDead = inEpilogue = _haveNLRPoint = false;

    nargs = 0; // set in initialize
    nslots = m->map()->length_slots();
    args = NEW_RESOURCE_ARRAY(SExpr*, nslots);
    locals = NEW_RESOURCE_ARRAY(PReg*, nslots);
    fint i;
    for (i = 0; i < nslots; i++) { args[i] = NULL; locals[i] = NULL; }
    method_map = (methodMap *) m->map();
    ncodes = method_map->length_codes();
    theSIC->ncodes += ncodes;
    primFailure = info->primFailure || (sender ? sender->primFailure : false);
    nsends = 0;
    exprStack = new SExprStack(this, ncodes);
    exprStackElems = new SExprBList(ncodes);
    blockElems = new SExprBList(ncodes);
    subScopes = new SSelfScopeBList(ncodes);
    blocks = new BlockPRegBList(ncodes);
    _bci = PrologueBCI;
    incoming = 0;
    allocs = NEW_RESOURCE_ARRAY(RegisterString, ncodes);
    for (i = 0; i < ncodes; i++) { allocs[i] = 0; }
    regs   = NEW_RESOURCE_ARRAY(LongRegisterString*, ncodes);
    for (i = 0; i < ncodes; i++) { regs[i] = new LongRegisterString(); }
    expressions = NEW_RESOURCE_ARRAY(PReg*, ncodes);
    pushedLocal = NEW_RESOURCE_ARRAY(PReg*, ncodes);
    for (i = 0; i < ncodes; i++) { expressions[i] = pushedLocal[i] = NULL; }
    
    init_branch_targets();
        
    _scopeID = currentScopeID++;
    scopeInfo = NULL;
    marker = NULL;
    splitRegs = new PRegBList(5);
    sig = sender ? sender->sig : NULL;
    // for correct register allocation, result has to belong to sender scope
    if (!resultPR) {
      if (sender) {
        SAPReg* r = new SAPReg(sender, sender->bci(), sender->bci());
        r->creationScope = this;
        r->creationStartBCI = ncodes - 1;
        resultPR = r;
      } else {
        resultPR = new SAPReg(this, ncodes - 1, ncodes - 1);
      }
    }
  }
  
  
  void SCodeScope::init_branch_targets() {
    BoolBList* btBCIs;
    method_map->branch_targets(hasBranchBC, &btBCIs, &backwards_branch_targets);
    if (hasBranchBC) {
      branchTargets      = NEW_RESOURCE_ARRAY(MergeNode*,  ncodes + 1);
      branchTargetStacks = NEW_RESOURCE_ARRAY(BranchBCTargetStack*, ncodes + 1);
      for (int32 i = 0;  i < ncodes+1;  i++) { 
        branchTargets[i] = 
          btBCIs->nth(i) || backwards_branch_targets->nth(i)
            ?  new MergeNode("br bc merge") 
            :  NULL;
        branchTargetStacks[i] = NULL;
      }
    }
  }
  

  SMethodScope::SMethodScope(oop m, oop mh_or_map, SCodeScope* sen, RScope* rs,
                             SendInfo* info)
  : SCodeScope(m, sen, rs, info) {
    _methodHolder_or_map = mh_or_map;
    initialize();
  }
  
  SBlockScope::SBlockScope(oop m, SScope* p, SCodeScope* s, RScope* rs,
                           SendInfo* info)
  : SCodeScope(m, s, rs, info) {
    _parent = p; initialize();
    if (info->rcvr && info->rcvr->preg()->isBlockPReg()) {
      BlockPReg* r = (BlockPReg*)info->rcvr->preg();
      if (r->primFailBlockScope) {
        // (some) prim failure blocks are special-cased to avoid  
        // initialization and zapping in the success path; since the
        // receiver block is being inlined, we need to make sure it's
        // properly initialized
        // (e.g. it could be created along some paths but not along others)
        r->memoize();
        theNodeGen->append(new BlockCloneNode(r, sig, true));
      }
    }
  }                           
    

  void SCodeScope::initialize() {
    containsNLR= method_map->containsNLR();
    if (method_map->containsLoop()) loopDepth++;
    assert(nslots == method()->map()->length_slots(), "just checkin'");
    assert(nargs == 0, "expecting nargs not to be set yet");
    if (isTop()) {
      // preallocate receiver, incoming args, locals
      self = receiver =
        new MapSExpr(receiverMapOop(),
                     new SAPReg(this, IReceiverReg, false, false,
                                PrologueBCI, ncodes-1),
                     NULL);
      incoming = allocateRegister(incoming, IReceiverReg);
      FOR_EACH_SLOTDESC_N(method()->map(), s, i) {
        if (s->is_arg_slot()) {
          oop ind= s->data;
          assert_smi(ind, "bad index");
          fint argIndex= smiOop(ind)->value();
          Location l = IArgLocation(argIndex);
          args[i]= new UnknownSExpr(new SAPReg(this, l, false, false, PrologueBCI, ncodes-1));
          incoming = allocateRegister(incoming, l);
          ++nargs;
        } 
      }
    } else {
      self = receiver = NULL;            // will be set by caller
      // get args from sender's expression stack
      fint firstArgIndex=   sender()->exprStack->length()
                          - method()->map()->arg_count();
      FOR_EACH_SLOTDESC_N(method()->map(), s, i) {
        if (s->is_arg_slot()) {
          oop ind= s->data;
          assert_smi(ind, "bad index");
          fint argIndex= smiOop(ind)->value();
          args[i]= sender()->exprStack->nth(firstArgIndex + argIndex);
          ++nargs;
        }
      }
    }
  }
  
  void SCodeScope::prologue() {
    computeVScope();
    theNodeGen->enterScope(this);

    nlrPoints = NEW_RESOURCE_ARRAY(MergeNode*, ncodes + 1);
    for (fint i = 0; i < ncodes + 1; i++) { nlrPoints[i] = NULL; }

    if (isTop()) {
      theNodeGen->prologue(theSIC->needRegWindowFlushes, false, nargs);
    }

    if (SICDebug) {
      char* s = NEW_RESOURCE_ARRAY(char, 100);
      sprintf(s, "%s@prologue: ", selector_string(selector()));
      theNodeGen->comment(s);
    }

    // NB: const init must come before slot initializations because some
    // slot may be initialized via a ConstPReg
    constInitNode = (ConstInitNode*)theNodeGen->append(new ConstInitNode);
    assert(!isTempReg(receiver->preg()->loc), "shouldn't be temp?");
    assert(!self || self == receiver, "what's self?");

    initLocals();

    postPrologue();

    flushPoint = theNodeGen->append(new NopNode);

    preallocateBlockExprs();

    if (isTop()) theNodeGen->testStackOverflow(currentExprStack(), sig);
    
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      assert(receiver->scope()->isSenderOrSame(this),
             "receiver must be in parent scope");
      assert(self->scope()->isSenderOrSame(this),
             "self must be in parent scope");
      for (fint j= 0; j < nslots; j++) {
        assert(args[j] == NULL || args[j]->scope()->isSenderOrSame(this),
               "arg must be in parent scope");
      }
    }
#   endif
  }

  void SCodeScope::initLocals() {
    oop m = method();
    FOR_EACH_SLOTDESC_N(m->map(), sd, n) {                // init locals
      if (sd->is_obj_slot()) {
        oop initialContents = m->get_slot(sd);
        locals[n]= new PReg(this);
        theNodeGen->loadOop(initialContents, locals[n]);
      } else if (sd->is_arg_slot() && isInlined()) {
        assert(!isTempReg(args[n]->preg()->loc), "shouldn't be temp?");
      }
    }
  }

  void SCodeScope::preallocateBlockExprs() {
    if (hasBranchBC) {
      // must preallocate blocks
      OopList* literals;
      IntList* blockBCIs = method_map->all_blocks(&literals);
      blockExprs = NEW_RESOURCE_ARRAY( SExpr*, ncodes );
      for ( int32 i = 0;  i < blockBCIs->length();  ++i ) {
        oop lit = literals->nth(i);
        assert_block(lit, "must be block");
        blockExprs[blockBCIs->nth(i)] = blockLiteral(blockOop(lit));
      }
    }
    else
      blockExprs = NULL;
  }

  void SCodeScope::postPrologue() {
    if (self == NULL) {
      self = receiver;
      assert(!isBlockScope(), "should set self differently");
    }
  }

  bool SCodeScope::isSenderOf(SSelfScope* callee) {
    assert(callee, "should have a scope");
    if (depth > callee->depth) return false;
    for (SSelfScope* s = callee->sender(); s; s = s->sender()) {
      if (this == s) return true;
    }
    return false;
  }

  
  
  // called by either materializeBlock or computeUplevelAccesses
  //  in order to recurse
  
  void SCodeScope::doUplevelAccesses(BlockPReg* blk, pregDoFn f) {
    // Blk is an escaping block.  Call f on all slots uplevel-accessed by
    // blk (except for constant slots).
    
    // compute uplevel-accessed locals
    slotsOop blockMeth= blk->block->value();
    blockMethodMap* bmm= (blockMethodMap*)blockMeth->map();
    IntList* slotIndices = 
      UseLocalAccessBytecodes
      ? bmm->uplevel_accessed_slots(method_map)
      : method_map->accessedSlots(bmm);

    // self is always up-level accessed
    f(blk, self->preg(), false);
    // In blocks, the receiver is uplevel accessed implicitly by NLRs of
    // nested blocks.  The code below may sometimes flush the receiver
    // unnecessarily, but it may be too complicated to figure out the
    // right thing.  Fix this if it becomes a performance problem.
    f(blk, receiver->preg(), false);
    
    for (IntListElem* e = slotIndices->head(); e; e = e->next()) {
      fint i= e->data();
      slotDesc* slot= method_map->slot(i);
      if (slot->name == VMString[SELF]) {
        // already handled
      } else if (slot->is_arg_slot()) {
        PReg* r = args[i]->preg();
        // NB: must call function repeatedly (i.e. even if r->uplevelR)
        // because r might be a local of a caller and be updated between
        // two calls that both uplevel-read it.
        f(blk, r, false);
      } else if (slot->is_map_slot()) {
        // constant slot - don't do anything
      } else {
        assert(slot->is_obj_slot(), "should be a data slot");
        PReg* r = locals[i];
        // for now, an up-level accessed local is always stack-allocated
        //  so pass in true for isWritten whether actually written or not.
        f(blk, r, true);
      }
    }
  }
  
  
  class sic_code_generator: public abstract_interpreter {
   protected:
    SCodeScope* sscope;
    
    IntListStream  exprStackBCIs;
    
   protected:
    SExpr* expr;
    
   public:
    fint           exprStackLength;
    
    
    void interpret_method();
    
  protected:

    void fetch_and_decode_bytecode();
    void interpret_bytecode();
    
    void do_SELF_CODE()  { sscope->exprStack->push( expr= sscope->self ); }
    void do_POP_CODE() { if ( !sscope->endsDead ) sscope->exprStack->pop();}

    void do_NONLOCAL_RETURN_CODE()  { sscope->returnCode(); }
    void do_read_write_local_code(bool isWrite);
    void do_literal_code(oop literal);
    void do_send_code( bool isSelfImplicit, stringOop sel, fint arg_count );
    
    void do_branch_code( int32 target_PC, oop target_oop = badOop ) {
      sscope->branchCode( pc, target_PC, target_oop);
    }
  
    void do_BRANCH_INDEXED_CODE()  { 
      sscope->branchIndexedCode( pc, get_branch_vector());
    }
    
    void write_expr_stack_info();
    
   public:
    sic_code_generator(SCodeScope*);
  
  };
  
  
  sic_code_generator::sic_code_generator(SCodeScope* ss) 
   : abstract_interpreter(ss->method()) {
    IntList* exprStackBCIList= 
      mi.map()->expression_stack_bcis( false);
                                    
    exprStackLength= exprStackBCIList->length();
    exprStackBCIs.init( exprStackBCIList, -1);
    sscope= ss;
  }
  
  
  void sic_code_generator::interpret_method() {
    abstract_interpreter::interpret_method();
    sscope->mergeInBranchTo(pc);
  }
  
  void sic_code_generator::fetch_and_decode_bytecode() {
    abstract_interpreter::fetch_and_decode_bytecode();
    expr= NULL;
    sscope->prepareForBytecode(pc);
  }
  
  void sic_code_generator::interpret_bytecode() {
    sscope->mergeInBranchTo(pc);
    abstract_interpreter::interpret_bytecode();

    if (expr  &&  !sscope->endsDead) 
      sscope->expressions[pc] = expr->preg();

    write_expr_stack_info();
  }


  void sic_code_generator::do_read_write_local_code(bool isWrite) {
    expr= sscope->genLocalSend( !isWrite, is.lexical_level, is.index );
    sscope->exprStack->push(expr);
  }
  
  
  void sic_code_generator::do_literal_code(oop literal) {
    if (literal->is_block_with_code()) {
      expr= sscope->blockLiteral(blockOop(literal));
    }
    else {
      // don't clone empty blocks
      PReg* r = new SAPReg(sscope);
      theNodeGen->loadOop(literal, r);
      expr= new ConstantSExpr(literal, r, theNodeGen->current);
    }
    sscope->exprStack->push(expr);
  }
  
    
  void sic_code_generator::do_send_code( bool isSelfImplicit, stringOop sel, fint arg_count ) {
    expr= sscope->genSend(isSelfImplicit,
                   is.is_undirected_resend,
                   sel,
                   is.delegatee,
                   arg_count);
    sscope->exprStack->push(expr);
    assert(expr->isNoResultSExpr() || expr->scope()->isSenderOrSame(sscope),
           "result should be in my scope (or sender), not callee");
           
    if (expr->isNoResultSExpr()) {
      // the send we just compiled will never return (e.g. block with
      // NLR), so everything after this is dead code
      if (theNodeGen->current) {
        theNodeGen->comment("rest of method omitted (dead)");
        theNodeGen->append(new DeadEndNode);
      }
      sscope->endsDead = true;
      pc= mi.length_codes - 1; // signal early return (see interpret_bytecode)
    }
  }
  
  
  void sic_code_generator::write_expr_stack_info() {
    if (pc == exprStackBCIs.current) {
      // remember expr stack elem for debugging info
      sscope->exprStackElems->append(sscope->endsDead
                                     ? NULL
                                     : sscope->exprStack->top());
      exprStackBCIs.advance();
    }
    if (sscope->endsDead) {
      // write placeholders for the dead expr stack elems
      while (sscope->exprStackElems->length()  <  exprStackLength) {
        sscope->exprStackElems->append(NULL);
      } 
    }
  }



  void SCodeScope::prepareForBytecode(fint pc) {
    _bci= pc;
    if (SICDebug) {
      char* s = NEW_RESOURCE_ARRAY(char, 100);
      oop sel= selector();
      sprintf(s, "%s@%ld: ", 
              sel->is_string()
                ? stringOop(sel)->copy_null_terminated()
                : "<not a string>",
              long(pc));
      theNodeGen->comment(s);
    }
    if ( vscope  &&  pc == vscope->vf->bci()) {
      // mark this code position as a restart point when replacing nmethods
      // on the stack
      // potential bug: don't distinguish between prim call and fail send
      theNodeGen->append(marker = new MarkerNode( exprStack, sig));
    }
  }
  
   
  void SCodeScope::genCode() {
    startOfScope = (MergeNode*)theNodeGen->append(
      new MergeNode("genCode startOfScope"));
    endOfScope = isTop() ? NULL : new MergeNode("genCode endOfScope");
    
    sic_code_generator scg(this);
    
    scg.interpret_method();    
    assert( exprStackElems->length() ==  scg.exprStackLength,
           "not enough expr stack elems written");
    _bci = ncodes - 1;
  }


  void SCodeScope::mergeInBranchTo(int32 pc) {
    // called each bc (and after last one)
    // to handle case if bc is target of a branch bc
    
    if (!hasBranchBC)
      return;
      
    MergeNode* bt = branchTargets[pc];
    if ( !bt ) 
      return; // not target
    bt->setScope(this);
    
    // must move expr stack to common locs for merge
    BranchBCTargetStack* stk = getBranchTargetStack(pc);
    // move values to common locs
    stk->mergeInExprsFromStack(exprStack, bt, false); 
    stk->moveExprsToStack(exprStack);      // fix exprs in exprStack
    
    theNodeGen->append(bt);                // now merge
  }
  
  
  BranchBCTargetStack* SCodeScope::getBranchTargetStack( int32 pc ) {
    // get the stack of SExprs for a branch going to bci pc
    BranchBCTargetStack*& res = branchTargetStacks[pc];
    if ( res == NULL ) {
      res = new_BranchBCTargetStack( 
              this,
              backwards_branch_targets->nth(pc),
              exprStack->length(),
              pc );
    }
    return res;
  }


  void SCodeScope::branchCode(  int32 pc,
                                int32 target_PC, 
                                oop target_oop ) {
     
    assert(hasBranchBC, "better know in advance");
    SExpr* testExpr;
    PReg*  targetPR;
    if ( target_oop == badOop ) {
      targetPR = NULL;
      testExpr = NULL;
    }
    else {
      targetPR = new_ConstPReg( this, target_oop);
      testExpr = exprStack->pop();
    }
      
    theNodeGen->branchCode( branchTargets[target_PC], 
                            target_PC <= pc,
                            targetPR,
                            testExpr,
                            getBranchTargetStack(target_PC),
                            exprStack,
                            currentExprStack(),
                            sig );
  }


  void SCodeScope::branchIndexedCode(  int32 pc,
                                       objVectorOop targets ) {
     
    assert(hasBranchBC, "better know in advance");
    PReg* testPR = exprStack->pop()->preg();
    int32 n = targets->length();
    MergeNode** targetNodes = NEW_RESOURCE_ARRAY( MergeNode*, n );
    bool*        isBackward = NEW_RESOURCE_ARRAY( bool,       n );
    BranchBCTargetStack**  myTargetStacks = 
      NEW_RESOURCE_ARRAY( BranchBCTargetStack*, n);
    for (int32 i = 0;  i < n;  ++i) {
      oop t = targets->obj_at(i);
      assert_smi(t, "target must be smi");
      int32 tt = smiOop(t)->value();
      assert( 0 <= tt  &&  tt < ncodes + 1,  "target must be in range");
      targetNodes[i] = branchTargets[tt];
      isBackward[i]  = tt <= pc;
      myTargetStacks[i] = getBranchTargetStack(tt);
    }
      
    theNodeGen->branchIndexedCode( n,
                                   targetNodes, 
                                   isBackward,
                                   testPR,
                                   myTargetStacks,
                                   exprStack,
                                   currentExprStack(),
                                   sig );
  }


  // code for block literal (possibly memoizing)
  SExpr* SCodeScope::blockLiteral(blockOop literal) {
    // Here's how it works: we clone the block and its map and store the
    // SScope* pointer in the block map's _desc field (because we don't know
    // the correct scope desc value yet).  Later, after generating
    // the scope descs, we go through the list of cloned blocks and fix up
    // their _descs by directly overwriting (so they
    // don't get yet another map).
    
    // For branches, this is called when bci == PrologueBCI
    // to create these early. Then it is called again
    // when bci != PrologueBCI and just returns what was earlier
    // created. The early creation is needed in case the normal
    // creation point is branched around; the "register" needs to
    // be allocated and initialized for the zapping code. -- dmu

    if ( hasBranchBC  &&  _bci != PrologueBCI ) {
      // was preallocated
      return blockExprs[_bci];
    }
    blockMap *map= (blockMap*)literal->map();
    assert(map->desc() == BLOCK_PROTO_DESC, "should be a prototype block");

    blockOop clone = literal->clone_and_set_desc(
                                     as_byte_count_smiOop((int32)this));
    assert(!clone->is_mark() && clone->map() != map, "should have new map");

    BlockPReg* loc = new BlockPReg(this, clone, _bci, ncodes - 1);
    if ( _bci == PrologueBCI )
      loc->memoize(); // creating for branch, memoize for sure
    blocks->append(loc);

    theNodeGen->loadBlockOop(loc, sig);
    SExpr* r =  new ClonedBlockSExpr(clone->map()->enclosing_mapOop(), loc,
                                     theNodeGen->current);
    blockElems->append(r);
    return r;
  }
  
  
  void SMethodScope::returnCode() {
    // normal return -- handled in standard epilogue
  }
  
  void SBlockScope::returnCode() {
    if (CatchInterprocessReturns) {
      SScope* s = this;
      PReg* t = theNodeGen->nlrHomePR;
      do {
        s->loadParentScope(t);
        s = s->parent();
      } while (s->isBlockScope() || s->isVFrameBlockScope());
      PrimDesc* pd = getPrimDescOfFunction(
                fntype(&catch_interprocess_returns), true);
      theNodeGen->loadArg( -1, receiver->preg(), true);
      theNodeGen->append(
        new PrimNode( pd,
                      NULL, 0, currentExprStack(0),
                      NULL, NULL));
    }
    // load home scope and other NLR registers
    SScope* s = this;
    PReg* t = theNodeGen->nlrHomePR;
    do {
      s->loadParentScope(t);
      s = s->parent();
    } while (s->isBlockScope() || s->isVFrameBlockScope());
    if (s->isVFrameScope()) {
      theNodeGen->prepareNLR(exprStack->pop()->preg(), t, homeID());
      if (!nlrPoint()) theNodeGen->continueNonLocalReturn();
    } else {
      // inlined NLR
      assert(s->isMethodScope(), "must be a method");
      nlrResult = exprStack->pop();
      // BUG: returned block won't be marked as escaped because NLRNode
      // has no src -- but it cannot be invoked, so that's ok
      // need to materialize - otherwise dead block isn't recognized
      theNodeGen->materializeBlock(nlrResult->preg(), sig, new PRegBList(1));

      const fint MaxBlocksToKill = 10;
      // If less than MaxBlocksToKill need to be killed, do it here & return;
      // otherwise, share code wih normal NLR sequence (slower but smaller).
      // The expected cost really is smaller than MaxBlocksToKill because
      // many blocks will be eliminated and thus have zero kill cost.
      fint nblocks = 0;
      BlockPRegBList kill(MaxBlocksToKill);
      for (SCodeScope* ss = this;
           nblocks <= MaxBlocksToKill; 
           ss = ss->sender()) {
        BlockPRegBList* l = ss->blocks;
        for (fint i = l->length() - 1; i >= 0; i--) {
          BlockPReg* bl = l->nth(i);
          if (!bl->primFailBlockScope) {
            nblocks++;
            kill.append(bl);
          }
        }
        if (ss == s) break;
      }
      if (nblocks <= MaxBlocksToKill) {
        theNodeGen->comment("inlined NLR");
        for (fint i = kill.length() - 1; i >= 0; i--) {
          BlockPReg* bl = kill.nth(i);
          if (!bl->primFailBlockScope) theNodeGen->zapBlock(bl);
        }
        if (s->isTop()) {
          fint o = theSIC->send_desc->endOffset(theSIC->L->lookupType());
          theNodeGen->append(new MethodReturnNode(o, true, nlrResult->preg()));
        } 
        else {
          assert(s->isCodeScope(), "must be a SelfScope");
          SCodeScope* s2= (SCodeScope*)s;
          theNodeGen->move(nlrResult->preg(), s2->resultPR);
          theNodeGen->branch(s2->endOfScope);
          s2->addResult(nlrResult->shallowCopy(s2->resultPR, NULL));
        }
        return;
      } 
      else {
        // have to set up the NLR regs so we can share the nlrPoint code
        // (not optimal since we'll do a superflous test for "home reached",
        // but it saves code space)
        assert(nlrPoint(), "should have NLR point");
        theNodeGen->append(new LoadIntNode(homeID(), theNodeGen->nlrHomeIDPR));
        theNodeGen->move(theNodeGen->framePR, theNodeGen->nlrHomePR);
        theNodeGen->move(nlrResult->preg(), theNodeGen->nlrResultPR);
        assert(s->isSelfScope(), "must be a SelfScope");
        SSelfScope* s2 = (SSelfScope*)s;
        s2->addResult(nlrResult->shallowCopy(s2->resultPR, NULL));
      }
    }
    // jump to NLRpoint (to kill off blocks etc.)
    if (nlrPoint()) theNodeGen->branch(nlrPoint());
  }
  
  void SCodeScope::epilogue() {
    // _bci = EpilogueBCI;
    // EpilogueBCI would be more correct but creates lots of bugs
    _bci = ncodes - 1;
    inEpilogue = true;
    SExpr* resExpr = NULL;
    if (!endsDead) {
      bool noNLR = exprStack->length() == 1;
      if (noNLR) {
        resExpr = exprStack->pop();
        // need to materialize - otherwise dead block isn't recognized
        theNodeGen->materializeBlock(resExpr->preg(), sig, new PRegBList(1));
      }
      if (isMethodScope() || noNLR) {
        // either a normal method, or no NLR from block
        // zap all blocks
        for (fint i = blocks->length() - 1; i >= 0; i--) {
          BlockPReg* bl = blocks->nth(i);
          if (!bl->primFailBlockScope) theNodeGen->zapBlock(bl);
        }
      }
      // return last expr if no explicit return
      if (resExpr) {
        theNodeGen->move(resExpr->preg(), resultPR);
        addResult(resExpr->shallowCopy(resultPR, theNodeGen->current));
        if (isTop()) {
          fint offset = theSIC->send_desc->endOffset(theSIC->L->lookupType());
          theNodeGen->append(new MethodReturnNode(offset, true, resultPR));
        } else {
          theNodeGen->branch(endOfScope);
        }
      }
      assert(exprStack->length() == 0, "non-empty expr stack");
    }
    genNLRPoints();
    if (endOfScope) {
      // if the endOfScope node is not reached it must not
      //  be put in theNodeGen->current, cause that would create
      //  a merge with a predecessor that is never run, which messes up
      //  dead-code elimination. -- dmu
      if (endOfScope->nPredecessors() > 0) {
        theNodeGen->current = endOfScope;
        if (resExpr)
          theNodeGen->append(new InlinedReturnNode(resExpr->preg(), resultPR));
      }
      else
        theNodeGen->current = NULL;
    }
    if (!result) {
      // scope never returns anything
      assert(isBlockScope() && containsNLR || endsDead,
             "should have a result");
      result = new NoResultSExpr;
    } else if (result->hasMap() && result->map()->is_block()) {
      SCodeScope* s = BlockPReg::scopeFromBlockMap(result->myMapOop());
      if (s && isSenderOrSame(s)) {
        // prevent block from being inlined because block is non-LIFO
        result = new UnknownSExpr(resultPR);
      }
    }
    theNodeGen->exitScope(this);
  }
  
  inline MergeNode* SCodeScope::nlrPoint(fint n) {
    // wrapper for nlrPoints array - creates merge nodes on demand
    if (nlrPoints[n] == NULL) {
      nlrPoints[n] = new MergeNode("nlrPoint n");
      nlrPoints[n]->setScope(this); // 'cause we might not be current scope
    }
    return nlrPoints[n];
  }

  bool SCodeScope::haveNLRPoint() {
    // does this scope have at least one NLR point?
    // it must have one if 
    //  - any subscope has one (including primitive subscopes), or
    //  - it has a NLR, or
    //  - it has non-inlined sends, and any superscope has an NLR point

    // for efficiency, cache result (once true it will always be true,
    // and once we're done with this scope it won't change)
    if (_haveNLRPoint || inEpilogue) return _haveNLRPoint;        

    _haveNLRPoint = haveNLR();
    for (fint i = subScopes->length() - 1; i >= 0 && !_haveNLRPoint; i--) {
      SSelfScope* ss = subScopes->nth(i);
      _haveNLRPoint = ss->haveNLRPoint();
    }

    for (SCodeScope* s = _sender; s && !_haveNLRPoint; s = s->_sender) {
      _haveNLRPoint = s->_haveNLRPoint;
    }
    return _haveNLRPoint;
  }

  MergeNode* SCodeScope::nlrPoint() {
    // return nlr point for current # of blocks
    return haveNLRPoint() ? nlrPoint(blocks->length()) : NULL;
  }

  PRegBList* SCodeScope::currentExprStack(fint exclude) {
    PRegBList* es = new PRegBList(exprStack->length() - exclude);
    for (fint i = 0; i < exprStack->length() - exclude; i++) {
      es->push(exprStack->nth(i)->preg());
    }
    return es;
  }
  
  void SCodeScope::genNLRPoints() {
    if (!haveNLRPoint()) return;       // no code needed
    
    // generate NLR code for all inline caches in this method
    fint n = blocks->length();
    
    // We don't want to add unreachable merges
    
    for ( ; n >= 0  &&  nlrPoint(n)->nPredecessors() == 0;  --n)
      ;
    if (n < 0) // cannot reach any nlrPoints, do not make any code
      return;

    if (n == 0) {
      // we don't have (reachable--dmu) nested blocks, 
      //  so we can't be the home of any NLR
      // but we need to be able to continue NLRs, so generate code for that
      theNodeGen->current = nlrPoint(n);
      continue_NLR();
      theNodeGen->current = NULL;
      return;
    }
    // order NLR code so can share code -- so NLR point with longest
    // block list to kill comes first
    // kill all blocks (in reverse order of creation)
    
    theNodeGen->current = nlrPoint(n);
    
    for (fint i = 0; i < n; i++) {
      theNodeGen->branch(nlrPoint(n - i));
      BlockPReg* bl = blocks->nth(n - i - 1);
      if (!bl->primFailBlockScope) theNodeGen->zapBlock(bl);
    }

    theNodeGen->branch(nlrPoint(0));
    if (haveNLR() && isMethodScope()) {
      // generate code testing whether NLR has reached its home
      Node* finalReturn = theNodeGen->testNLR(scopeID());
      // generate code for the two outcomes (NLR home reached/not reached)
      continue_NLR(); // "not reached" code
      // now do the "reached" code
      if (isTop()) {
        fint offset = theSIC->send_desc->endOffset(theSIC->L->lookupType());
        finalReturn->append(new MethodReturnNode(offset, true,
                                                 theNodeGen->nlrResultPR));
      } else {
        if (result == NULL) {
          // normal code doesn't reach end of method, and none of our
          // blocks' NLRs was inlined
          assert(endsDead, "should have a result");
        } 
        // need to take into account possible non-inlined NLR result 
        // performance bug: don't need to pollute result if all NLRs
        // were inlined
        if (nsends) addResult(new UnknownSExpr(resultPR));
        Node* nn = new AssignNode(theNodeGen->nlrResultPR, resultPR);
        finalReturn->append(nn);
        nn->append(endOfScope);
      }
      theNodeGen->current = NULL;
    } else {
      // can't be the target of any NLR
      continue_NLR(); 
    }
  }

  void SCodeScope::continue_NLR() {
    // search up the (inlined) call chain to find a possible NLR home
    SCodeScope *s;
    for (s = sender(); s && !s->nlrPoint(); s = s->sender()) ;
    if (s) {
      PReg* r = theNodeGen->nlrResultPR;
      theNodeGen->append(new InlinedNonLocalReturnNode(r, r));
      theNodeGen->branch(s->nlrPoint());
    } else {
      // none of the callers can possibly be the home of an NLR
      theNodeGen->continueNonLocalReturn();
    }
  }
  
  SExpr* SCodeScope::genPrim(SendInfo* info) {
    // primitive call or perform
    LookupType performLookupType;
    bool isPerform = checkPerformPrim(info->sel, performLookupType);
    SExpr* rcvr = info->isSelfImplicit  ? self : NULL;    
    SPrimScope* prim;
    if (isPerform) {
      prim = new SPerformPrimScope(this, performLookupType, info->sel,
                                   info->del, rcvr);
    } else if (info->sel == VMString[_RESTART] ||
               info->sel == VMString[_RESTART_IF_FAIL]) {
      prim = new SRestartPrimScope(this, info->sel, info->rcvr);
    } else {
      prim = new SPrimScope(this, info->sel, rcvr, bci() == ncodes - 1);
    }
    prim->genCode();
    return prim->result;
  }

  void SCodeScope::markLocals(PRegBList* expStk) {
    // this scope has at least one send - mark locals as debug-visible
    if (nsends == 0) {
      // first time we're called
      self->preg()->debug = true;
      receiver->preg()->debug = true;
      for (fint i = 0; i < nslots; i++) {
        if (args[i]) args[i]->preg()->debug = true;
        if (locals[i]) locals[i]->debug = true;
      }
    }
    // also mark expression stack as debug-visible (excluding arguments to
    // current send) (the args are already excluded from the CallNode's
    // expression stack, so just use that one instead of this->exprStack)
    for (fint i = 0; i < expStk->length(); i++) {
      expStk->nth(i)->debug = true;
    }
  }

  void SCodeScope::addSend(PRegBList* expStk) {
    if (!expStk) return;                    // not an exposing send
    for (SCodeScope* s = this;
         s && s->isCodeScope(); s = s->sender()) {
      // possible performance bug: currently, the expr stack elems of a prim
      // that scavenges but doesn't walk the stack are marked debug-visible.
      // In reality, the should only be allocated to non-temp regs but can
      // be freely copy-propagated etc.  But such prims are exceedingly rare.
      s->markLocals(expStk);
      s->nsends++;
    }
  }

  void SCodeScope::computeMaskFor(SAPReg* r, fint stackLocs,
                                  fint nonRegisterArgs) {
    // r belongs to our scope; mark its live range
    // (note that "belongs to this scope" doesn't mean "is an expr stack entry
    // in this scope" -- the SAPReg may have been copy-propagated from another
    // scope)
    assert(r->scope == this, "not in this scope");
    if (nsends == 0) return;                 // don't need to compute masks

#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  r->isBlockPReg() && ((BlockPReg*)r)->primFailBlockScope) {
        // a prim fail block is only live after the prim has failed
        // (in that range it will be marked because it is the receiver of
        // the fail send)
        // but the code is currently broken, so the reg is always initialized
        // (see BlockCloneNode::gen), must mark it live
        assert(r->creationScope == this, "shouldn't have CPed block");
      }
#   endif

    RegisterString mask = registerMaskBit(r->loc, stackLocs, nonRegisterArgs);
    if (mask == 0) return;
    if (r->isSplitPReg()) {
      // don't mark split regs here - each send will mark them iff the
      // reg is live at that send
      splitRegs->append(r);
      return;
    }
    
    // don't mark startBCI because r is not live at beginning of startBCI
    // (but it is at the end of startBCI)
    fint i; 
    // added min w.r.t ncodes-1 to handle endBCI == ncodes
    //  for branch bcs -- dmu
    fint end = min(r->endBCI, ncodes - 1);
    // added max for i because startBCI can be -2 and allocs starts at 0.
    // also added it two more places below.
    //   -- dmu 4/22/04
    for (i = max(0, r->startBCI + 1); i <= end; i++) {
      allocs[i] |= mask;
    }
    SCodeScope* s = r->creationScope->sender();
    if (r->creationScope != this) {
      // r was constant-propagated out of its scope; need to mark it live
      // in subscopes.  E.g. if r is created in C where A@0 calls B@0 calls C,
      // and r was propagated to A, need to mark it live in B@1 
      fint start = r->creationScope->senderBCI();
      while (s != this) {
        for (i = max(0, start + 1); i < s->ncodes; i++) {
          s->allocs[i] |= mask;
        }
        start = s->senderBCI();
        s = s->sender();
      }
      // also mark it live in creation scope ]creationStartBCI..end]
      s = r->creationScope;
      for (i = max(0, r->creationStartBCI + 1); i < s->ncodes; i++) {
        s->allocs[i] |= mask;
      }
    }
  }

  void SCodeScope::computeMasks(RegisterString base, fint stackLocs,
                                fint nonRegisterArgs) {
    // called after reg. allocation to compute register masks for sends
    // the bits corresponding to SAPRegs have already been set
    if (nsends == 0) {
      // don't need to compute masks
      allocs = NULL; // to catch bugs - shouldn't ever ask for allocs
      return;
    }

    base |= registerMaskBit(    self->preg()->loc, stackLocs, nonRegisterArgs);
    base |= registerMaskBit(receiver->preg()->loc, stackLocs, nonRegisterArgs);
    // compute base mask 
    fint i;
    for (i = 0; i < nslots; i++) {
      if (locals[i]) {
        base |= registerMaskBit(locals[i]->loc, stackLocs, nonRegisterArgs);
      } else if (args[i]) {
        // args are either covered by enclosing scope, or they are SAPRegs
        PReg* r = args[i]->preg();
        assert(r->isSAPReg() ||
               r->scope->isSenderOf(this), "oops");
        // nevertheless, we need to set the bit because prim fail block args
        // (prim name / error string) wouldn't be marked otherwise (their
        // lifetime can't reallybe expressed in bcis because it starts in
        // the middle of the prim call bci, i.e., the prim call bci has
        // two sends...sigh)
        base |= registerMaskBit(r->loc, stackLocs, nonRegisterArgs);
      }        
    }

    // add constants allocated to registers
    ConstPRegBList* constants = &constInitNode->constants;
    for (i = constants->length() - 1; i >= 0; i--) {
      base |= registerMaskBit(constants->nth(i)->loc, stackLocs, nonRegisterArgs);
    }
    
    for (i = 0; i < ncodes; i++) allocs[i] |= base;

    // make sure prim fail blocks are scavenged correctly
    for (i = 0; i < ncodes; i++) {
      PReg* r = expressions[i];
      if (r && r->isBlockPReg() &&
          ((BlockPReg*)r)->primFailBlockScope == this) {
        // mark block live from here to end of scope (should be exactly
        // two bcis: literal/arg acces, and the prim. call
        
        // Except for index bytecodes! -- dmu
        // Used to just mark allocs[i] and allocs[i+1] 
        // with following check:
        //  assert(i == ncodes - 2, "should be 2nd last byte code");
        // Now mark through next non-index bytecode:
        
        int32 mask = registerMaskBit(r->loc, stackLocs, nonRegisterArgs);
        allocs[i] |= mask;
        
        for ( int32 nextNonIndexBCI = i + 1;
              getOp( method_map->start_codes()[nextNonIndexBCI] )
                ==  INDEX_CODE;
              ++nextNonIndexBCI ) {
          if (nextNonIndexBCI >= ncodes)
            fatal("should have found a prim call");
          allocs[nextNonIndexBCI] |= mask;
        }
      }
    }

    // compute masks of subscopes
    for (i = 0; i < subScopes->length(); i++) {
      SSelfScope* s = subScopes->nth(i);
      s->computeMasks(allocs[s->senderBCI()], stackLocs, nonRegisterArgs);
    }
  }

  // register allocation: if r is being allocated to a PReg of a scope,
  // it is marked as unavailable in all subscopes and all superscopes.
  // (But it is still available in sibling scopes and their subscopes.)
  // r is the RegisterString for the register

  inline void SSelfScope::allocateUp(PReg* r, bool okToOverlap) {
    Unused(okToOverlap);
    // mark reg as taken in callers
    // (don't need to worry about lexical chain because it must be a subset
    // of the calling chain)
    SSelfScope* callee = this;
    for (SCodeScope* s = sender(); s; callee = s, s = s->sender()) {
      s->markReg(r, callee->senderBCI(), callee->senderBCI());
    }
  }
  
  inline void SAccessScope::allocateDown(PReg* r, fint start, fint end,
                                         bool okToOverlap) {
    Unused(start); Unused(end);
    assert(!regs.isAllocated(r->loc) || r->weight < 0 || okToOverlap,
           "already allocated");
    markReg(r, -1, -1);
  }

  void SAccessScope::allocateReg(PReg* r, bool okToOverlap) {
    assert(!regs.isAllocated(r->loc) || r->weight < 0 || okToOverlap,
           "already allocated");
    markReg(r, 0, 0);
    allocateUp(r, okToOverlap);
  }
  
  void SAccessScope::markReg(PReg* r, fint start, fint end) {
    Unused(start); Unused(end);
    regs.allocate(r->loc);
  }
    
  void SCodeScope::markReg(PReg* r, fint start, fint end) {
    Location l = r->loc;
    for (fint i = start; i <= end; i++) {
      regs[i]->allocate(l);
    }
  }
    
  inline void SCodeScope::getLiveRange(PReg* r, fint& start, fint& end) {
    if (r->isSAPReg()) {
      // get live range of r
      SAPReg* sr = (SAPReg*)r;
      start = max(sr->startBCI, 0);
      end = sr->endBCI < 0 ? ncodes - 1 : sr->endBCI;
    } else {
      start = 0; end = ncodes - 1;
    }
    assert(start >= 0 && start < ncodes, "invalid bci");
    assert(end   >= 0 && end   < ncodes, "invalid bci");
  }
  
  void SCodeScope::allocateReg(PReg* r, bool okToOverlap) {
    fint start, end;
    getLiveRange(r, start, end);
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      for (fint i = start; i <= end; i++) {
        assert(!regs[i]->isAllocated(r->loc) || r->weight < 0 || okToOverlap,
               "already allocated");
      }
    }
#   endif
    allocateUp(r, okToOverlap);
    allocateDown(r, start, end, okToOverlap);
  }
  
  
  // Return true if this scope is deleted.
  // Optimizations delete nodes and pseudo registers.
  // Splitting clones nodes, creating multiple copies of a scope's
  // nodes.
  // If all ways up to me are deleted, then there is no point generating
  // scope, etc info for me. And it would be impossible anyway, since
  // the pregs would not even be allocated anywhere. -- dmu 7/96
  
  bool SCodeScope::isDeleted() {
    for (ConstInitNode* n = constInitNode;  n;  n = n->nextCopy) {
      if (!n->deleted)
        return false;
    }
    return true;
  }


  void SCodeScope::allocateConst(ConstPReg* r) {
    for (ConstInitNode* n = constInitNode; n; n = n->nextCopy) {
      n->addConstant(r);
    }
  }

  // is register/stack temp l available in r's live range?
  bool SAccessScope::isRegAvailable(PReg* r, Location l, RegisterString in) {
    Unused(r); Unused(in);
    return !regs.isAllocated(l);
  }

  Location SAccessScope::findReg(PReg* r, fint firstRegisterIndexToUse,
                                 RegisterString incoming) {
    Unused(r);
    RegisterString taken = regs.regs() | incoming;
    fint i;
    for (i = firstRegisterIndexToUse;
         isAllocated(taken, CalleeSavedRegs[i]) && i < NumCalleeSavedRegs;
         i++) ;
    if (i < NumCalleeSavedRegs) {
      // found a free register 
      return CalleeSavedRegs[i];
    } else {
      return UnAllocated;
    }
  }
  
  bool SCodeScope::isRegAvailable(PReg* r, Location l, RegisterString inc) {
    if (isRegister(l) && isAllocated(inc, l)) return false;
    fint start, end;
    getLiveRange(r, start, end);
    for (fint i = start; i <= end; i++) {
      if (regs[i]->isAllocated(l)) return false;
    }
    return true;
  }
  
  Location SCodeScope::findReg(PReg* r, fint firstRegisterIndexToUse,
                               RegisterString incmng) {
    fint start, end;
    getLiveRange(r, start, end);
    RegisterString taken = incmng;
    fint i;
    for (i = start; i <= end; i++) taken |= regs[i]->regs();
    for (i = firstRegisterIndexToUse;
         isAllocated(taken, CalleeSavedRegs[i]) && i < NumCalleeSavedRegs;
         i++) ;
    if (i < NumCalleeSavedRegs) {
      // found a free register 
      return CalleeSavedRegs[i];
    } else {
      return UnAllocated;
    }  
  }
  
  Location SAccessScope::findStackTemp(PReg* r) {
    Unused(r);
    LongRegisterString* rs = &regs;
    return (Location)findFirstUnusedTemp(&rs, 1);
  }
  
  Location SCodeScope::findStackTemp(PReg* r) {
    fint start, end;
    getLiveRange(r, start, end);
    return (Location)findFirstUnusedTemp(&regs[start], end - start + 1);
  }
  
  
  SExpr* SCodeScope::genLocalSend( bool isRead, fint lexicalLevel,
                                   fint index) {
    assert(UseLocalAccessBytecodes, "harbinger of the future");
    slotDesc* sd = method_map->getLocalSlot(lexicalLevel, index);

    SScope* s = this;
    for (fint i = 0;  i < lexicalLevel;  ++i)
      s = s->parent();

    SExpr* res = genLocalSend( isRead ? sd->name : sd->assignment_slot_name(),
                               sd,
                               s);
    assert(res, "is local");
    return res;
  }
  
  
  SExpr* SCodeScope::genSend( bool isReceiverImplicit,
                              bool isUndirectedResend,
                              stringOop sel,
                              stringOop del,
                              fint arg_count) {
                            
    SendInfo* info = new SendInfo(NULL, 0, 
                                  isReceiverImplicit,
                                  isUndirectedResend,
                                  sel,
                                  del);
    info->computeNSends(rscope, _bci);
    if (sel->is_string() && sel->is_prim_name()) {
      return genPrim(info);
    }
    
    LookupType lookupType;
    if (del != NULL) {
      assert(isReceiverImplicit, "directed resend must be to implicit self");
      lookupType = DirectedResendLookupType;
    }
    else if (isUndirectedResend) {
      lookupType = ResendLookupType;
    }
    else if (isReceiverImplicit) {
      if (!UseLocalAccessBytecodes) {
        SExpr* res = genLocalSend(sel);
        if (res) return res;        // was local send
      }
      lookupType = ImplicitSelfLookupType;
    }
    else {
      lookupType = NormalLookupType;
    }
    info->l = lookupType;
    info->rcvr =   isReceiverImplicit
      ? self
      : exprStack->nth(exprStack->length() - arg_count - 1);
    return inlineSend(info);
  }

  SExpr* SCodeScope::genRealSend(SendInfo* info) {
    if (PrintInlining) {
      lprintf("%*s*sending %s\n", (void*)depth, "",
              selector_string(info->sel));
    }
    if (info->rcvr->isUnknownSExpr()) {
      theSIC->registerUninlinable(info, NormalFnLimit, 0);
    } else {
      // known receiver - registerUninlinable already handled
    }
    LookupType lookupType = withCountBits(info->l, info->countType);
    if (theRecompilation && SICUseOptimizedSDFlag && !info->primFailure) {
      lookupType |= OptimizedSendMask;
      if (info->uninlinable) {
        // don't count
      } else {
        lookupType = withCountBits(lookupType, Comparing);
      }
    } else {
      // either we have only one compiler (so the flag is redundant), or we're
      // prematurely compiling with the SIC (e.g. isCriticalSend), or this
      // nmethod is from a sd with OptimizedSendMask; don't propagate
      // Optimized flag
    }
    if (info->uninlinable) {
      lookupType |= UninlinableSendMask;
    }
    if (nstages == 1 || (recompilee && recompilee->version() >= MaxVersions-1)) {
      // don't use any counting code
      lookupType = withCountBits(lookupType, NonCounting);
    } else if (currentProcess->isUncommon() && !info->uninlinable) {
      // make it counting so uncommon version gets replaced after a while
      lookupType = withCountBits(lookupType, Comparing);
    }
    
    // push args and generate call
    fint argc = info->sel->arg_count();
    fint top = exprStack->length();
    theSIC->allocateArgs(argc, false);
    PReg* rcvr = info->rcvr->preg();

    // materialize memoized blocks
    PRegBList* blks = new PRegBList(5);
    fint i;
    for (i = 1; i <= argc; i++) {
      PReg* l = exprStack->nth(top - i)->preg();
      theNodeGen->materializeBlock(l, sig, blks);
    }
    theNodeGen->materializeBlock(rcvr, sig, blks);

    // Make sure NLR points are created - we need one if there are
    // blocks to kill or if an superscope has NLRs.  Since the latter
    // is hard to test, just assume it's so.
    _haveNLRPoint |= blocks->length() > 0 || isInlined();

    // move args to arg regs
    for (i = 1; i <= argc; i++) {
      PReg* l = exprStack->nth(top - i)->preg();
      theNodeGen->loadArg(argc - i, l, false);
    }

    fint exclude = argc;
    if (!info->isSelfImplicit) exclude++;
    theNodeGen->loadArg(-1, rcvr, false);        // load receiver
    Node* call = theNodeGen->selfCall(this, lookupType, self->preg(),
                                      info->sel, info->del,
                                      methodHolder_or_map(), 
                                      nlrPoint(), argc,
                                      currentExprStack(exclude), sig);
    theNodeGen->move(call->dest(), info->resReg);  
    return new UnknownSExpr(info->resReg, theNodeGen->current);
  }
  
  SScope* SMethodScope::lookup(stringOop sel, slotDesc*& sd) {
    sd = method()->find_slot(sel);
    return sd ? this : NULL;
  }
  
  SScope* SBlockScope::lookup(stringOop sel, slotDesc*& sd) {
    sd = method()->find_slot(sel);
    return sd ? this : parent()->lookup(sel, sd);
  }
  
  SScope* SDeadBlockScope::lookup(stringOop sel, slotDesc*& sd) {
    Unused(sel);
    sd = NULL; return NULL;
  }
  
  SScope* SVFrameMethodScope::lookup(stringOop sel, slotDesc*& sd) {
    sd = _vf->method()->find_slot(sel);
    return sd ? this : NULL;
  }
  
  SScope* SVFrameBlockScope::lookup(stringOop sel, slotDesc*& sd) {
    sd = _vf->method()->find_slot(sel);
    return sd ? this : parent()->lookup(sel, sd);
  }
  
  SExpr* SCodeScope::genLocalSend( stringOop sel, slotDesc* sd, SScope* s) {
    if (sd == NULL) {
      s = lookup(sel, sd);
    }
    if (s != NULL) {
      // found a lexically-scoped slot with this name
      // s is the scope containing the slot
      fint argc = sel->arg_count();
      assert(argc == 0 || argc == 1, "wrong number of args");
      NameDesc* nd = NULL;
      if (sd->is_map_slot() ||
          (s->isVFrameScope() &&
          (nd = s->vf()->get_name_desc(sd, true), nd && nd->isValue()))) {
        // load value of constant slot or of unallocated (constant) data slot
        assert(argc == 0, "must be an access");
        oop p = nd ? nd->value() : sd->data;
        PReg* reg = new_ConstPReg(this, p);
        return new ConstantSExpr(p, reg, theNodeGen->current);
      } else {
        // generate access code
        PReg* reg = s->isVFrameScope() ? new TempPReg(this) : NULL; 
        return genLocal(s, this, sd, argc, reg);
      }
    } else {
      return NULL;
    }
  }
  
  SExpr* SCodeScope::local(SCodeScope* sender, slotDesc* sd,
                           fint argc, PReg* t) { 
    Unused(t);
    oop m = method();
    if (sd->is_map_slot()) {
      // load constant slot
      assert(argc == 0, "must be an access");
      PReg* reg = new_ConstPReg(sender, sd->data);
      return new ConstantSExpr(sd->data, reg, theNodeGen->current);
    } else {
      // slot access or assignment
      bool isArg = sd->is_arg_slot();
      if (isArg) {
        return args[sd - m->map()->slots()];
        // don't need to actually assign anything
      } 
      assert(sd->is_obj_slot(), "unexpected slot type");
      PReg* l = locals[sd - m->map()->slots()];
      SExpr* expr;
      if (argc == 0) {
        // slot access
        SAPReg* res = new SAPReg(sender);
        theNodeGen->move(l, res);
        if (SICRegTargeting) new RegisterPair(res, l);
        expr = new UnknownSExpr(res, theNodeGen->current);
        sender->pushedLocal[sender->_bci] = l;
      } else {
        // assignment
        PReg* arg = sender->exprStack->pop()->preg();
        theNodeGen->materializeBlock(arg, sig, new PRegBList(5));
        theNodeGen->loadArg(arg, l);
        expr = self;
      }
      return expr;
    }
  }

  
  // The genLocal calls generate a sequence of instructions to load the
  // SP of the frame containing the local.  The last scope then loads
  // (or stores) the value.
  
  SExpr* SMethodScope::genLocal(SScope* target, SCodeScope* sender,
                                slotDesc* sd, fint argc, PReg* t) {
    Unused(sender); Unused(target);
    assert(target == this, "should have found the slot");
    return SCodeScope::local(sender, sd, argc, t);
  }
  
  SExpr* SBlockScope::genLocal(SScope* target, SCodeScope* sender,
                               slotDesc* sd, fint argc, PReg* t) {
    if (target == this) {
      // normal local slot access
      return SCodeScope::local(sender, sd, argc, t);
    } else {
      // load parent scope's SP, then continue
      loadParentScope(t);
      return parent()->genLocal(target, sender, sd, argc, t);
    }
  }
  
  void SBlockScope::loadParentScope(PReg* t) {
    // load first block's parent scope
    if (_parent->isVFrameScope()) {
      theNodeGen->loadBlockParent(receiver->preg(), t);
    } else {
      // inlined block - access will be handled by enclosing scope
    }
  }
  
  SExpr* SVFrameScope::genLocal(SScope* target, SCodeScope* sender,
                                slotDesc* sd, fint argc, PReg* t) {
    assert(target == this, "should have found the slot");
    NameDesc* nd = _vf->get_name_desc(sd);
    assert(!nd->isValue(), "shouldn't call genLocal for constants");
    if (argc == 0) {
      PReg* r = new SAPReg(sender);
      theNodeGen->loadSaved(t, _vf->code, nd, r, sd->name);
      return new UnknownSExpr(r, theNodeGen->current);
    } else {
      PReg* arg = sender->exprStack->pop()->preg();
      theNodeGen->storeSaved(arg, t, _vf->code, nd, sd->name);
      return sender->self;
    }
  }  
  
  SExpr* SVFrameBlockScope::genLocal(SScope* target, SCodeScope* sender,
                                     slotDesc* sd,  fint argc, PReg* t) {
    if (target == this) {
      return SVFrameScope::genLocal(target, sender, sd, argc, t);
    } else {
      if (_vf->fr != parent()->vf()->fr) {
        loadParentScope(t);
      } else {
        // inlined in parent's frame, don't need to walk up the stack
      }
      return parent()->genLocal(target, sender, sd, argc, t);
    } 
  }
  
  void SVFrameBlockScope::loadParentScope(PReg* t) {
    if (_vf->fr != _parent->vf()->fr) {
      // load block's parent scope
      NameDesc* nd = _vf->get_block_name();
      theNodeGen->loadSaved(t, _vf->code, nd, t,
                            VMString[LEXICAL_PARENT]);
      theNodeGen->loadBlockParent(t, t);
    } else {
      // inlined in parent's frame, don't need to walk up the stack
    }
  }
  
  bool SCodeScope::checkPerformPrim(stringOop selector,
                                    LookupType& lookupType) {
    if (!selector->is_string()) return false;
    
    fint len = selector->length();
    char* sel = selector->bytes();
    if (selector->has__Perform__prefix()) {
      return checkPerform(sel, len, 9 /* _Perform: */, lookupType);
    } else if (selector->has__PerformResend__prefix()) {
      return checkPerform(sel, len, 15 /* _PerformResend: */, lookupType);
    }
    return false;
  }
  
  
  bool SCodeScope::checkPerform(const char* sel, fint len, fint prefix,
                                LookupType& performLookupType) {
    // check if this is really a perform primitive; if so, also
    // return perform type
    
    if (str_has_IfFail(sel, len)) len -= 7;
    
    fint argc = 0;
    while (strncmp(sel + len - 5, "With:", 5) == 0) {
      argc ++;
      len -= 5;
    }
    
    if (len == prefix) {
      if (prefix == 9) {
        performLookupType = NormalPerformType;
      } else {
        performLookupType = ResendPerformType;
      }
    } else if (prefix == 9 && len == prefix + 13 &&
               strncmp(sel + prefix, "DelegatingTo:", 13) == 0) { 
      performLookupType = DelegatedPerformType;
    } else {
      // not a recognized perform primitive name; don't replace with perform
      return false;
    }
    return true;
  }
  
  void SPrimScope::initialize() {
    stringOop selector = (stringOop)_selector;
    nargs = selector->arg_count();
    if (selector->has_IfFail()) {
      _selector = selector->without_IfFail();
      failSelector = VMString[VALUE_WITH_];
      hasFailBlock = true;
      failBlock = exprStack()->pop();
      nargs--;
    } else {
      failSelector = VMString[PRIMITIVE_FAILED_ERROR_NAME_];
      hasFailBlock = false;
      failBlock = NULL;
    }
    pd = getPrimDescOfSelector((stringOop)_selector);
    args = new SExprBList(nargs);
    // npop: keep track of how many elems we need to pop from expr stack;
    // need to keep expr stack intact until end of call so that e.g. uncommon
    // branches can mark elems as visible
    npop = nargs;
    fint top = exprStack()->length() - 1;
    for (fint i = 0; i < nargs; i++) args->append(exprStack()->nth(top - i));
    if (receiver == NULL) {
      receiver = exprStack()->nth(top - nargs);
      npop++;
    }
    needZap = false;
    if (pd->canFail()) {
      // mark failure block as memoized
      if (failBlock && failBlock->preg()->isBlockPReg()) {
        BlockPReg* bl = (BlockPReg*)failBlock->preg();
        bool primFailBlock = lastBCI && !pd->canWalkStack();
        for (SCodeScope* s = _sender; s != bl->scope && primFailBlock;
             s = s->sender()) {
          if (s->bci() < s->ncodes - 1) primFailBlock = false;
        }
        if (primFailBlock) {
          // this block is never needed/visible unless the primitive fails
          // because the primitive call is the last byte code of the method
          // and all scopes between this one and the block's home are at the
          // last bci.
          bl->primFailBlockScope = sender();
          bl->memoized = false;
          needZap = true;
        } else {
          bl->memoize();
        }
      }
    }

  }
  
  SExpr* SPerformPrimScope::genCall() {
    Node* n = theNodeGen->perform(_sender, lookupType, self()->preg(), nargs,
                                  delegatee(), methodHolder_or_map(),
                                  _sender->nlrPoint(),
                                  currentExprStack(0), _sender->sig);
            // args already popped, thus currentExprStack(0)
    theNodeGen->move(n->dest(), resultPR);
    successNode = theNodeGen->current;
    return NULL;
  }
  
  void SPrimScope::genCode() {
    initialize();
    result = tryConstantFold();
    if (!result) result = tryTypeCheck();
    if (!result) result = tryInline();
    if (result) {
       // don't need to generate code for call
    } else {
      loadArgs();
      theNodeGen->loadArg(-1, receiver->preg(), true); // load receiver
      SExpr* failRes = genCall();
      assert(successNode, "should have successNode");
      Node* c = successNode;          // for better formatting of switch
      PReg* r = resultPR;
      switch(pd->etype()) {
       case ReceiverPrimType:
        result = receiver->shallowCopy(r, c); break;
       case ReceiverMapPrimType:
        result = receiver->mapify(r, c); break;
       case IntegerPrimType:
        result = new MapSExpr(Memory->smi_map->enclosing_mapOop(), r, c); break;
       case FloatPrimType:
        result = new MapSExpr(Memory->float_map->enclosing_mapOop(), r, c); break;
       case StringPrimType:
        result = new MapSExpr(Memory->stringObj->map()->enclosing_mapOop(), r, c);
        break;
       case ObjVectorPrimType:
        result = new MapSExpr(Memory->objVectorObj->map()->enclosing_mapOop(), r, c);
        break;
       case ByteVectorPrimType:
        result = new MapSExpr(Memory->byteVectorObj->map()->enclosing_mapOop(), r, c);
        break;
       case BooleanPrimType:
        result = (new ConstantSExpr(Memory->trueObj,  r, NULL))->copyMergeWith(
          new ConstantSExpr(Memory->falseObj, r, NULL),
          r, c);
        break;
       case NoReturnPrimType:
        result = new NoResultSExpr;
       case UnknownPrimType:
       default:
        result = new UnknownSExpr(r, c); break;
      }
      if (failRes) result = result->mergeWith(failRes, theNodeGen->current);
    }
    // pop args 
    while (npop-- > 0) exprStack()->pop();
  }

  void SRestartPrimScope::genCode() {
    theNodeGen->restart(_sender->startOfScope, currentExprStack(0),
                        _sender->sig);
    result = new NoResultSExpr;
  }

  SExpr* SPrimScope::genCall() {
    BlockPReg* failReg = NULL;
    if (needZap && pd->canScavenge() && failBlock->preg()->isBlockPReg()) {
      BlockPReg* r = (BlockPReg*)failBlock->preg();
      if (r->primFailBlockScope) failReg = r;
    }
    MergeNode* nlr = pd->needsNLRCode() ? _sender->nlrPoint() : NULL;
    PRegBList* es = currentExprStack(npop); 
    PrimNode* call =
      theNodeGen->primCall(_sender, pd, nlr, nargs, es, _sender->sig, failReg);

    // failure handling
    SExpr* failResult;
    if (pd->canFail()) {
      Node* test = NULL;
      MergeNode* done = NULL;
      failResult = genPrimFailure(call, NULL, test, done, resultPR);
      // move success result to right place and merge with fail branch
      successNode = test->append1(new AssignNode(call->dest(), resultPR));
      if (done) {
        successNode->append(done);
        theNodeGen->current = done;
      }
      else
        theNodeGen->current = successNode;
    } else {
      failResult = NULL;
      theNodeGen->move(call->dest(), resultPR);
      successNode = theNodeGen->current;
    }

    return failResult;
  }
  
  void SPrimScope::loadArgs(PRegBList* blocks) {
    theSIC->allocateArgs(nargs, true);

    // materialize block args
    if (!blocks) blocks = new PRegBList(5);
    fint i;
    for (i = 0; i < nargs; i++) {
      PReg* l = args->nth(i)->preg();
      theNodeGen->materializeBlock(l, _sender->sig, blocks);
    }
    theNodeGen->materializeBlock(receiver->preg(), _sender->sig, blocks);
    
    // move args to arg registers
    for (i = 1; i <= nargs; i++) {
      PReg* l = args->nth(i - 1)->preg();
      theNodeGen->loadArg(nargs - i, l, true);
    }
  }
  
  void SPerformPrimScope::initialize() {
    bool explicitReceiver = receiver == NULL;
    if (explicitReceiver) receiver = (SExpr*)1; // fake for SPrimSc::init
    SPrimScope::initialize();
    while (npop-- > 0) exprStack()->pop();
    if (explicitReceiver) receiver = exprStack()->pop();
    sel = args->pop(); nargs--; // subtract selector
    del = NULL;
    if (needsDelegatee(lookupType)) {
      del = args->pop(); nargs--; // subtract delegatee
    }
  }

  void SPerformPrimScope::loadArgs(PRegBList* blocks) {
    // materialize possible blocks
    if (!blocks) blocks = new PRegBList(5);
    theNodeGen->materializeBlock(sel->preg(), _sender->sig, blocks);
    if (del) theNodeGen->materializeBlock(del->preg(), _sender->sig, blocks);
    SPrimScope::loadArgs(blocks);
    
    if (needsDelegatee(lookupType)) {
      theNodeGen->loadArg(del->preg(), theNodeGen->delPR); // push delegatee
    }
    theNodeGen->loadArg(sel->preg(), theNodeGen->selPR);   // push selector
  }
  
  void SMethodScope::genCode() {
    prologue();
    SCodeScope::genCode();
    epilogue();
  }
  
  void SBlockScope::genCode() {
    prologue();
    SCodeScope::genCode();
    epilogue();
  }
  
  
  void SBlockScope::postPrologue() {
    // load self
    if (parent()->isVFrameScope()) {
      SAPReg* selfPR = new SAPReg(this, PrologueBCI, ncodes-1);
      self = new MapSExpr(mapOop(selfMapOop()), selfPR , NULL);

      compiled_vframe* vf = parent()->vf();
      NameDesc* selfName = vf->get_self_name();
      PReg* t = new TempPReg(this);
      loadParentScope(t);
      if (selfName->hasLocation()) {
        theNodeGen->loadSaved(t, vf->code, selfName, self->preg(), VMString[SELF]);
        // make sure this load isn't optimized away (to trap dead blocks)
        theNodeGen->current->hasSideEffects_now = true;
      } else {
        // do dummy load to test for dead block
        theNodeGen->comment("dummy load for dead block test");
        theNodeGen->loadSaved(t, vf->code, NULL, theNodeGen->noPR, VMString[SELF]);
        theNodeGen->current->hasSideEffects_now = true;
        theNodeGen->loadOop(selfName->value(), self->preg());
      }
    } else {
      assert(parent()->isCodeScope(), "just checkin'");
      self = ((SCodeScope*)parent())->self;
    }
  }
  
  void SDeadBlockScope::postPrologue() {
    // don't load parent scope - not necessary
  }
  
  SVFrameScope::SVFrameScope(compiled_vframe* f) { _vf = f; }
  
  SVFrameBlockScope::SVFrameBlockScope(compiled_vframe* f) : SVFrameScope(f) {
    abstract_vframe* parent = f->parent();
    if (parent) {
      _parent = new_SVFrameScope(parent->as_compiled());
    } else {
      _parent = NULL;
    }
  }

  SExpr* SVFrameScope::receiverExpr() {
    SExpr* descSelf= _vf->desc->selfExpr();
    // get the map info from the runtime, because
    // 1. The scopeDesc in the nmethod may be invalid, or
    // 2. If reusing nmethods, it might just be plain wrong.
    if (ReuseNICMethods || true || _vf->code->isInvalid()) {
      oop self= _vf->self();
      if (descSelf->isMapSExpr() && descSelf->map() != self->map()) {
        MapSExpr* m= (MapSExpr*)descSelf;
        m->_myMapOop= self->map()->enclosing_mapOop();
        LOG_EVENT1("SIC: counterfactual inlining %#lx",
        m->_myMapOop);
      }
    }
    return descSelf;
  }
  oop SVFrameScope::method() { return _vf->desc->method(); }
  oop SVFrameScope::methodHolder_or_map() {
    return _vf->methodHolder_or_map(); }
  
  SDeadBlockScope::SDeadBlockScope(oop m, SendInfo* info)
  : SBlockScope(m, NULL, NULL, new RNullScope(NULL), info) {}
  
  void SDeadBlockScope::genCode() {
    prologue();                // to create stack frame
    theNodeGen->nonLifoTrap(currentExprStack(0), sig);    
    endOfScope = new MergeNode("SDeadBlockScope::genCode endOfScope");
    theNodeGen->append(endOfScope);
    fint offset = theSIC->send_desc->endOffset(theSIC->L->lookupType());
    theNodeGen->append(new MethodReturnNode(offset, true, theNodeGen->noPR));
    // ignore rest of method - will never execute
  }
  
  SVFrameScope* new_SVFrameScope(compiled_vframe* vf) {
    MethodKind kind = vf->method()->kind();
    switch (kind) {
     case OuterMethodType:
      return new SVFrameMethodScope(vf);
     case BlockMethodType:
      return new SVFrameBlockScope(vf);
     default:
      fatal("unexpected byte code kind");
      return NULL;
    }
  }


  SPrimScope::SPrimScope(SCodeScope* s, oop sel,
                         SExpr* rcvr, bool last)
  : SSelfScope(s, s->rscope, NULL) {
    _selector = sel; receiver = rcvr; lastBCI = last; successNode = NULL;
    // for correct register allocation, result has to belong to sender scope
    if (!resultPR) {
      assert(s, "must have sender");
      SAPReg* r = new SAPReg(s, s->bci(), s->bci());
      resultPR = r;
    }
  }
  
  SPerformPrimScope::SPerformPrimScope(SCodeScope* s,
                                       LookupType l, oop selector, oop delgt,
                                       SExpr* rcvr)
    : SPrimScope(s, selector, rcvr, false) {
    lookupType = l; _delegatee = delgt; 
  }

  SRestartPrimScope::SRestartPrimScope(SCodeScope* s, oop sel, SExpr* r)
    : SPrimScope(s, sel, r, false) { }

  mapOop SAccessScope::receiverMapOop() { return L->receiverMapOop(); }
  oop SAccessScope::selector()                 { return L->selector(); }
  
  void SConstantSlotAccessScope::genCode() {
    slotDesc* s = L->result()->as_real()->desc;
    assert(s->is_map_slot(), "should be constant slot");
    theNodeGen->loadOop(s->data, resultPR);
    result = new ConstantSExpr(s->data, resultPR, theNodeGen->current);
  }

  SDataSlotAssignScope::SDataSlotAssignScope(SICLookup* L1, SCodeScope* s,
                                             RScope* rs, SExpr* r, SExpr* a,
                                             SendInfo* info)
  : SAccessScope(L1, s, rs, r, info) { arg = a; }

  void SDataSlotAccessScope::genCode() {
    theNodeGen->pathLookup(L->result()->as_real(), receiver->preg(), resultPR);
    result = new UnknownSExpr(resultPR, theNodeGen->current);
  }

  void SDataSlotAssignScope::genCode() {
    assert(L->result()->is_real(), "real lookup should have real result");
    realSlotRef* ref = (realSlotRef*) L->result();
    slotDesc* dataSlot = ref->holder->map()->find_slot(ref->desc->name);
    realSlotRef*  dataRef  = new realSlotRef(ref->holder, dataSlot);
    PReg* rcvrPR = receiver->preg();
    PReg* argPR = arg->preg();
    bool needCheckStore = true;
    if (arg->hasMap()) {
      Map* map = arg->map();
      if (map == Memory->smi_map || map == Memory->float_map)
        needCheckStore = false;
    }
    if (arg->hasConstant()) {
      oop constant = arg->constant();
      needCheckStore = constant->is_new();          // ints/floats aren't new
    } else if (arg->hasMap()) {
      Map* m = arg->map();
      if (m == Memory->smi_map || m == Memory->float_map)
        needCheckStore = false;
    }
    if (argPR->isBlockPReg()) {
      theNodeGen->materializeBlock(argPR, _sender->sig, new PRegBList(1));
    }
    theNodeGen->pathAssign(rcvrPR, dataRef, argPR, needCheckStore);
    if (resultPR != rcvrPR) {
      theNodeGen->move(rcvrPR, resultPR);
      result = receiver->shallowCopy(resultPR, theNodeGen->current);
    } else {
      result = receiver;
    }
  }

  // debugging information

  fint SSelfScope::descOffset() {
    return scopeInfo ? theSIC->rec->offset(scopeInfo) : IllegalDescOffset;
  }

  void SCodeScope::fixupBlocks() {
    if (!CheckAssertions  &&  isLite()) return;         // nothing to do
    fint i;
    for (i = 0; i < ncodes; i++) {
      PReg* r = expressions[i];
      if (r && r->isBlockPReg()) {
        // can't use BlockPReg::parent - slot may have been fixed up already
        oop block = ((BlockPReg*)r)->block;
        blockMap *map= (blockMap*)block->map();
        smiOop desc= map->desc();
        assert_smi(desc, "should be an integer");
        assert(desc != BLOCK_PROTO_DESC, "should have been changed");
        if (desc == as_byte_count_smiOop((int32) this)) {
#          if GENERATE_DEBUGGING_AIDS
            if (CheckAssertions  &&  isLite()) {
              // don't actually get the offset because it's undefined
              assert(r->isUnused(), "should be unused");
              assert(!((BlockPReg*)r)->escapes, "shouldn't escape");
              continue;
            }
#          endif
          map->setDesc(as_smiOop(descOffset()));
        }
      }
    }
    for (i = 0; i < subScopes->length(); i++) {
      subScopes->nth(i)->fixupBlocks();
    }
  }
  
  void SCodeScope::describe() {
    if (isDeleted())   return;  // totally unreachable, can't do anything for me
      
    genScopeInfo();

    if (isLite()) {
      // don't decribe locals & expr stack -- can't ever ask for this info
    } else {
      // locals
      ScopeDescRecorder* rec = theSIC->rec;
      ScopeInfo info = scopeInfo;
      assert(nslots == method()->map()->length_slots(), "just checkin'");
      FOR_EACH_SLOTDESC_N(method()->map(), s, n) {
        if (s->is_vm_slot()) {
          // ignore
        } else if (s->is_arg_slot()) {
          //  I changed it from:
          //    rec->addSlot(info, n, args[n]->preg()->nameNode());
          //    because it was failing for constants (ConstantSExpr) -- dmu
          rec->addSlot(info, n, args[n]->nameNode());
        } else if (s->is_obj_slot()) {
          // local data slot
          rec->addSlot(info, n, locals[n]->nameNode());
        } else {
          // constant or assignment slot - don't describe
          assert(s->is_map_slot(), "unexpected slot type");
        }
      }
      
      // expression stack
      for (fint i= 0; i < exprStackElems->length(); i++) {
        SExpr* e = exprStackElems->nth(i);
        if (e) {
          rec->addExprStack(info, i, e->nameNode(false));
        } else {
          // dead code was omitted, still need to write dummy expr stack elem
          assert(endsDead, "should have expr stack element");
          rec->addExprStack(info, i, new IllegalName);
        }
      }
      
      // live blocks
      for (fint i= 0;  i < blockElems->length();  i++) {
        SExpr* e = blockElems->nth(i);
        rec->addBlock(info, i, e->nameNode(false));
      }
    }

    // subscopes
    for (fint i = 0; i < subScopes->length(); i++) {
      subScopes->nth(i)->describe();
    }
  }
  
  void SMethodScope::genScopeInfo() {
    bool lite = isLite();
    scopeInfo = theSIC->rec->
      addMethodScope(_key,
                     _method,
                     self->nameNode(!lite),
                     self->myMapOop(),
                     _methodHolder_or_map, lite, scopeID(),
                     _sender ? _sender->scopeInfo : NULL,
                     _senderBCI,
                     predicted);
  }
  
  void SBlockScope::genScopeInfo() {
    bool top = isTop() || parent()->isVFrameScope();
    bool lite = isLite();
    if (top) {
      scopeInfo = theSIC->rec->
        addTopLevelBlockScope(_key,
                              _method,
                              _key->receiverMapOop(),
                              self->nameNode(!lite),
                              self->nameNode(!lite),     // woud be different for PPC
                              self->myMapOop(),
                              methodHolder_or_map(),
                              receiver->nameNode(!lite),
                              receiver->nameNode(!lite), // woud be different for PPC   
                              lite,
                              scopeID(),
                              _sender ? _sender->scopeInfo : NULL,
                              _senderBCI,
                              predicted);
    } else {
      assert(parent()->isCodeScope(), "must be method scope");
      SCodeScope* p = (SCodeScope*)parent();
      scopeInfo = theSIC->rec->
        addBlockScope(_key, _method, _key->receiverMapOop(),
                      receiver->nameNode(!lite), p->scopeInfo, lite,
                      scopeID(), _sender ? _sender->scopeInfo : NULL,
                      _senderBCI, predicted);
    }
  }

  void SDeadBlockScope::genScopeInfo() {
    bool lite = isLite();
    scopeInfo = theSIC->rec->
      addDeadBlockScope(_key, _method, receiver->nameNode(!lite), lite,
                        scopeID(), _sender ? _sender->scopeInfo : NULL,
                        _senderBCI, predicted);
  }

  void SDataSlotAccessScope::describe() {
    // describe access scope only if receiver type isn't known without
    // referring to recompilee
    if (predicted) {
      MethodLookupKey* k= new_MethodLookupKey(L->key);
      scopeInfo =
        theSIC->rec->addDataAccessScope(k, receiver->nameNode(false),
                                        receiverMapOop(), methodHolder_or_map(),
                                        _sender->scopeInfo, _senderBCI,
                                        predicted);
    }
  }

  void SDataSlotAssignScope::describe() {
    // describe access scope only if receiver type isn't known without
    // referring to recompilee
    if (predicted) {
      MethodLookupKey* k= new_MethodLookupKey(L->key);
      scopeInfo =
        theSIC->rec->addDataAssignmentScope(k, receiver->nameNode(false),
                                            receiverMapOop(), methodHolder_or_map(),
                                            _sender->scopeInfo, _senderBCI,
                                            predicted);
    }
  }

  void SConstantSlotAccessScope::describe() {
    // describe access scope only if receiver type isn't known without
    // referring to recompilee
    if (predicted) {
      MethodLookupKey* k= new_MethodLookupKey(L->key);
      scopeInfo =
        theSIC->rec->addDataAccessScope(k, receiver->nameNode(false),
                                        receiverMapOop(), methodHolder_or_map(),
                                        _sender->scopeInfo, _senderBCI,
                                        predicted);
    }
  }

  ValueLocationNameDesc* SCodeScope::nameDescFor(PReg* r) {
    // r is a register that is live at the restarting marker node; find its
    // run-time value and create a NameDesc for it
    assert(vscope, "not on current stack");
    oop val;
    blockOop blk = (blockOop)badOop;
    if (r->isConstPReg()) {
      val = ((ConstPReg*)r)->constant;
    } else {
      SExpr* expr;
      val = valueFor(r, expr);
      if (val == badOop) return NULL;
      if (val->is_block()) {
        if (expr && expr->hasMap()) {
          // fabricate a dummy block with the right map
          blk = blockOop(val)->copy();
          blk->addr()->_map = expr->myMapOop();
          blk->kill();
        } else {
          // Couldn't find new map for the block; maybe the block belongs
          // to a caller nmethod?  Find out.
          frame* home = blockOop(val)->scope(true);
          if (home == NULL ||
              (theSIC->vscopes &&
              home > theSIC->vscopes->last()->vf->fr->block_scope_of_home_frame())) {
            // ok, don't need to remap the block
          } else {
            if (PrintRecompilation) {
              lprintf("*couldn't find block map for %s (oop %#lx)\n",
                      r->name(), blk);
            }
            return NULL;            // sorry, need map and couldn't find it
          }
        }
      }
    }
    LOG_EVENT3("ValueLocationNameDesc %s%d %s",
               r->prefix(), r->id(), locationName(r->loc));
    return new ValueLocationNameDesc(r->loc, val, blk);
  }

  oop SCodeScope::valueFor(PReg* r, SExpr*& expr) {
    // try to find r's run-time value and its SExpr
#   define RETURN(val, exp) { expr = exp; return val; }
    compiled_vframe* vf = vscope->vf;

    // try receiver & self
    if (r->isCPEquivalent(receiver->preg()))
      RETURN(vf->get_contents(vf->get_receiver_name(), true, true), receiver);
    if (r->isCPEquivalent(self->preg()))
      RETURN(vf->get_contents(vf->get_self_name(), true, true), self);
    { // args and locals
      FOR_EACH_SLOTDESC_N(_method->map(), sd, i) {
        if (args[i] && args[i]->preg()->isCPEquivalent(r)) {
          RETURN(vf->get_slot2(sd, true, true), args[i]);
        } else if (locals[i] && locals[i]->isCPEquivalent(r)) {
          RETURN(vf->get_slot2(sd, true, true), NULL);
        }
      }
    }
    // live blocks (try *before* expr stack to create eliminated blocks)
    IntList* bcs = method_map->blocks_upto(vf->bci());
    IntListElem* e;
    for ( e = bcs->head(); e; e = e->next()) {
      int32 bci2 = e->data();
      if (expressions[bci2]->isCPEquivalent(r)) {
        NameDesc* n = vf->desc->blockElem(bci2);
        oop block = vf->get_contents(n, true, true);
        Map* bm = block->map();
        fint i;
        for (i= blocks->length() - 1;
             i >= 0 && !bm->equal(blocks->nth(i)->block->map());
             i--) ;
        assert(i >= 0, "should have found block");
        RETURN(block, new MapSExpr(blocks->nth(i)->block->map()->enclosing_mapOop(),
                                   blocks->nth(i), NULL));
      }
    }

    // expr stack
    oop* stack;
    smi len;
    vf->get_expr_stack(stack, len, true);
    SExprStack* es = marker->exprStack;
    for (fint i= len - 1; i >= 0; i--) {
      if (es->nth(i)->preg()->isCPEquivalent(r)) RETURN(stack[i], es->nth(i));
    }

    // not a source-level name - may be an outgoing arg
    if (r->isArgSAPReg() && isArgRegister(r->loc)) {
      RETURN(vf->register_contents(r->loc), NULL);
    }
    RETURN(badOop, NULL);           // not found
#   undef RETURN
  }

  // printing code

  void SendInfo::print() {
    lprintf("SendInfo %#lx \"%s\" (rcvr = %#lx, nsends = %ld)\n",
           (unsigned long)this, selector_string(sel), (unsigned long)rcvr,
           long(nsends));
  }
  
  void SCodeScope::print() {
    lprintf(" method: %#lx\n\tdel: %#lx  id: %ld",
           (unsigned long)method(), (unsigned long)delegatee(),
           long(scopeID()));
    lprintf("  args: x/%ldx %#lx", long(nslots), (unsigned long)args); 
    lprintf("\n\tlocals: x/%ldx %#lx", long(nslots), (unsigned long)locals); 
    lprintf("  stack: "); exprStack->print_short_zero();
    lprintf("  blocks: "); blocks->print_short_zero();
    lprintf("\n\tnlrPoints: x/%ldx %#lx\n", long(blocks->length()),
           (unsigned long)nlrPoints);
    lprintf("  subScopes: "); subScopes->print();
    lprintf("\n");
  }
  
  void SMethodScope::print_short() {
    lprintf("(SMethodScope*)%#lx (", (unsigned long)this);
    selector()->print_oop(); lprintf(")");
  }
  
  void SMethodScope::print() {
    print_short();
    SCodeScope::print();
    lprintf("\tmh: %#lx", (unsigned long)methodHolder_or_map()); 
    lprintf("  sender: "); sender()->print_short_zero();
    lprintf(" @ %ld\n", long(senderBCI()));
  }
  
  void SBlockScope::print() {
    print_short();
    SCodeScope::print();
    lprintf("\tparent: "); parent()->print_short_zero();
    lprintf("  sender: "); sender()->print_short_zero();
    lprintf(" @ %ld\n", long(senderBCI()));
  }
  
  void SBlockScope::print_short() {
    lprintf("(SBlockScope*)%#lx (", (unsigned long)this);
    selector()->print_oop(); 
    lprintf(" %#lx)", (unsigned long)method());
  }
  
  void SDeadBlockScope::print_short() {
    lprintf("SDeadBlockScope %#lx (%#lx)", (unsigned long)this,
           (unsigned long)method()); }
  
  void SVFrameScope::print() {
    print_short();
    lprintf("  _vf = %#lx", (unsigned long)_vf);
  }
  
  void SVFrameMethodScope::print_short() {
    lprintf("(SVFrameMethodScope*)%#lx (", (unsigned long)this);
    _vf->selector()->print_oop();
    lprintf(")");
  }
  
  void SVFrameMethodScope::print() {
    SVFrameScope::print();
    lprintf("\n");
  }
  
  void SVFrameBlockScope::print() {
    SVFrameScope::print();
    lprintf("\n    parent: "); parent()->print_short_zero();
    lprintf("\n");
  }
  
  void SVFrameBlockScope::print_short() {
    lprintf("(SVFrameBlockScope*)%#lx (", (unsigned long)this);
    _vf->selector()->print_oop();
    lprintf(" = %#lx)", (unsigned long)method());
  }
  
  void SPrimScope::print_short() {
    lprintf("(SPrimScope*)%#lx (", (unsigned long)this);
    _selector->print_oop();
    lprintf(")");
  }

  void SPerformPrimScope::print_short() {
    lprintf("(SPerformPrimScope*)%#lx (", (unsigned long)this);
    _selector->print_oop();
    lprintf(")");
  }
  void SRestartPrimScope::print_short() {
    lprintf("(SRestartPrimScope*)%#lx (", (unsigned long)this);
    _selector->print_oop();
    lprintf(")");
  }

  void SDataSlotAccessScope::print_short() {
    lprintf("(SDataSlotAccessScope*)%#lx (", (unsigned long)this);
    L->selector()->print_oop();
    lprintf(")");
  }
  
  void SConstantSlotAccessScope::print_short() {
    lprintf("(SConstantSlotAccessScope*)%#lx (", (unsigned long)this);
    L->selector()->print_oop();
    lprintf(")");
  }
  
  void SDataSlotAssignScope::print_short() {
    lprintf("(SDataSlotAssignScope*)%#lx (", (unsigned long)this);
    L->selector()->print_oop();
    lprintf(")");
  }
  
  
# endif
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "defUse.hh"
# pragma implementation "defUse_inline.hh"
# include "_defUse.cpp.incl"



  // rcvr is definition of a PReg (the arg) which we are trying to
  //  apply global copy propagation to

  bool PDef::canCopyPropagate(PReg* definedPr) {

    // don't propagate if the defNode isn't assignment

    if (!node->isAssignmentLike())
      return false;
    assert(node->dest() == definedPr, "not assignment-like");


    // don't propagate if source has incorrect def info, does not
    // survive calls (i.e. is local to BB), or is uplevelW (incorr. defs)

    PReg* defSrc = node->src();

    if ( defSrc->incorrectD() 
    ||   isTrashedReg(defSrc->loc)
    ||   defSrc->uplevelW)
      return false;

    return true;
  }


  void DUInfo::propagateTo(BB* useBB, PUse* use, Node* fromNode, PReg* src,
                           Node* toNode, bool global) {
    // r1 := r2; ...; r3 := op(r1)  -->  r1 := r2; ...; r3 := op(r2)
    bool ok = toNode->copyPropagate(useBB, use, src);
    if (PrintSICCopyPropagation) {
      lprintf("*%s cp:%s propagate %s from N%ld (%#lx) to N%ld (%#lx)\n",
              global ? "global" : "local",
              ok ? "" : " couldn't",
              src->name(),
              (void*)(fromNode ? fromNode->id() : 0),
              fromNode,
              (void*)toNode->id(), toNode);
    }
  }


  void DUInfo::propagateTo(BB* useBB, PReg* r, PDef* def, PUse* use,
                           bool global) {
    // def reaches use; try to eliminate r's use by using copy propagation
    Node* fromNode = def->node;
    bool isAssignment = fromNode->isAssignmentLike();
    Node* toNode = use->node;
    bool hasSrc = fromNode->hasSrc();
    PReg* fromPR = hasSrc ? fromNode->src() : NULL;
    bool isConst = hasSrc && fromPR->isConstPReg();
    if (  isAssignment 
    &&    isConst 
    &&    toNode->canCopyPropagateOop()) {
      // loadOop r1, oop; ...; r2 := op(r1)    --->
      // loadOop r1, oop; ...; r2 := op(oop)
      bool ok = toNode->copyPropagate(useBB, use, fromPR);
      if (PrintSICCopyPropagation) {
        lprintf("*%s cp:%s %s from N%ld (%#lx) to N%ld (%#lx)\n",
               global ? "global" : "local", 
               ok ? "" : " couldn't propagate",
               fromPR->name(),
               (void*)fromNode->id(), fromNode,
               (void*)toNode->id(), toNode);
      }
    } else if (  isAssignment 
           &&   !isConst 
           &&    hasSrc 
           &&    toNode->canCopyPropagate()
           &&    toNode->canCopyPropagateFrom(fromPR)) {
      // r1 := r2; ...; r3 := op(r1)  -->  
      // r1 := r2; ...; r3 := op(r2)
      propagateTo(useBB, use, fromNode, fromPR, toNode, global);
    } 
    else if (  r->nuses() == 1 
         &&    toNode->isAssignNode() 
         &&   !toNode->hasSideEffects() 
         &&    fromNode->canChangeDest() 
         &&    r->canEliminateAndStillDebug()) {
      // fromNode: r := ...;  ... ; toNode: x := r    --->
      // fromNode: x := ...;
      if (PrintSICCopyPropagation) {
        lprintf("*%s cp: changing dest of N%ld (%#lx) to %s to match use at N%ld (%#lx)\n",
                global ? "global" : "local", 
                (void*)fromNode->id(),
                fromNode, r->name(),
                (void*)toNode->id(), toNode);
      }
      assert(!r->incorrectDU(), "shouldn't try CP on this");
      assert(!global || r->ndefs() == 1, "not safe with >1 defs");
      assert(fromNode->dest() == r, "unexpected dest");
      fromNode->setDest(fromNode->bb(), toNode->dest());
      toNode->eliminateNodeAndUsedPRs(toNode->bb(), NULL, false);
    } else {
      if (PrintSICCopyPropagation) {
        lprintf("*%s cp: can't propagate N%ld (%#lx) to N%ld (%#lx)\n",
               global ? "global" : "local",
               (void*)fromNode->id(), fromNode,
               (void*)toNode->id(), toNode);
      }
    }
  }
  
  void PUse::print()      { lprintf("PUse %#lx (N%ld)", this,
                                    (void*)node->id()); }
  void PSoftUse::print()  { lprintf("PSoftUse %#lx (N%ld)", this,
                                    (void*)node->id());}
  void PDef::print()      { lprintf("PDef %#lx (N%ld)", this,
                                    (void*)node->id()); }

  void BBDUTable::print() {
    if (!info) return;      // not built yet
    print_short();
    for (fint i = 0; i < info->length(); i++) {
      lprintf("%3ld: ", (void*)i); info->nth(i)->print();
    }
  }
  
  void DUInfo::getLiveRange(int32& firstNodeNum, int32& lastNodeNum) {
    // IN:  first & last node ID of BB
    // OUT: first & last node ID where this PReg is live
    bool firstSet = false;
    PDefListElem* d = defs.head();
    PUseListElem* u = uses.head();
    while (u || d) {
      assert(!u || !u->next() ||
             u->data()->node->id() <= u->next()->data()->node->id(),
             "should be ordered by increasing node number");
      assert(!d || !d->next() ||
             d->data()->node->id() <= d->next()->data()->node->id(),
             "should be ordered by increasing node number");
      if (u && (!d || u->data()->node->id() <= d->data()->node->id())) {
        firstSet = true;                // use before def --> live from start
        lastNodeNum = u->data()->node->num();
        u = u->next();
      } else if (d) {
        if (!firstSet) {
          firstSet = true;              // first definition is here
          firstNodeNum = d->data()->node->num();
        }
        d = d->next();
      }
    }
  }

  void DUInfo::print_short() { lprintf("DUInfo %#lx", this); }
  void DUInfo::print() {
    print_short(); lprintf(" for "); reg->print_short(); lprintf(": ");
    uses.print(); lprintf("; "); defs.print(); lprintf("\n");
  }

  void PRegBBIndex::print_short() {
    lprintf("PRegBBIndex [%ld] for", (void*)index); bb->print_short(); 
  }

  void PRegBBIndex::print() {
    print_short(); lprintf(": "); bb->duInfo.info->nth(index)->print();
  }

  static bool cpCreateFailed = false;

  CPInfo* CPInfo::new_CPInfo(Node* n) {
    CPInfo* cpi = new CPInfo(n);
    if (cpCreateFailed) {
      cpCreateFailed = false;
      return NULL;
    }
    return cpi;
  }
  
  CPInfo::CPInfo(Node* n) {
    def = n;
    if (n->hasSrc()) {
      r = n->src();
    } else {
      // can't use this def
      cpCreateFailed = true;
    }
  }

  bool CPInfo::isConstant()     { return r->isConstPReg(); }
  oop  CPInfo::constant()       {
    assert(isConstant(), "not constant");
    return ((ConstPReg*)r)->constant;
  }

  void CPInfo::print() {
    lprintf("*(CPInfo*)%#lx : def %#lx, %s\n", this, def, r->name());
  }
  
  static void printNodeFn(PDefUse* du) { lprintf("N%ld ",
                                                 (void*)du->node->id()); }
  
  void printDefsAndUses(PRegBBIndexBList* l) {
    lprintf("defs: ");
    forAllDefsDo(l, (defDoFn)printNodeFn);
    lprintf("; uses: ");
    forAllUsesDo(l, (useDoFn)printNodeFn);
  }

  static defDoFn theDefIteratorFn;
  static void doDefsFn(PRegBBIndex* p) {
    DUInfo* info = p->bb->duInfo.info->nth(p->index);
    info->defs.apply(theDefIteratorFn);
  }
  
  void forAllDefsDo(PRegBBIndexBList* l, defDoFn f) {
    theDefIteratorFn = f;
    l->apply(doDefsFn);
  }

  static useDoFn theUseIteratorFn;
  static void doUsesFn(PRegBBIndex* p) {
    DUInfo* info = p->bb->duInfo.info->nth(p->index);
    info->uses.apply(theUseIteratorFn);
  }
  
  void forAllUsesDo(PRegBBIndexBList* l, useDoFn f) {
    theUseIteratorFn = f;
    l->apply(doUsesFn);
  }
  
# endif
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "nodeGen.hh"
# include "_nodeGen.cpp.incl"

# define APPEND(node)      current = current->append (node)
# define APPEND1(node)     current = current->append1(node)
# define APPENDN(n, node)  current = current->append(n, node)
# define COMMENT(s)        current = current->append(new CommentNode(s))

  NodeGen* theNodeGen;

  NodeGen::NodeGen(compilingLookup* l, sendDesc* sd, nmln* d) {
    initPRegs();
    initNodes();
    scopeStack = new SSelfScopeBList(30);
    start = current = NULL;
    L =l; send_desc = sd; diLink = d;
    haveStackFrame = false;
    delPR = new PReg(NULL, PerformDelegateeLoc, true, true);
    selPR = new PReg(NULL, PerformSelectorLoc, true, true);
    nlrHomePR = new PReg(NULL, NLRHomeReg, true, true);
    nlrHomeIDPR = new PReg(NULL, NLRHomeIDReg, true, true);
    nlrResultPR = new PReg(NULL, NLRResultReg, true, true);
    nlrTempPR = new PReg(NULL, NLRTempReg, true, true);
    framePR = new PReg(NULL, FrameReg, true, true);
    noPR = new NoPReg;
    theNodeGen = this;
  }

  void NodeGen::enterScope(SSelfScope* s) { scopeStack->push(s); }
  void NodeGen::exitScope (SSelfScope* s) {
    assert(currentScope() == s, "exiting wrong scope");
    scopeStack->pop();
  }

  void NodeGen::prologue(bool needToFlushRegWindow, bool isAccessMethod,
                         fint nargs) {
    assert(current == NULL, "prologue must be first");
    current = start = new PrologueNode(needToFlushRegWindow, isAccessMethod,
                                       nargs, L);
  }

  Node* NodeGen::append(Node* n) { 
    return APPEND(n); 
  }
  
  Node* NodeGen::comment(const char* s) { return APPEND(new CommentNode(s)); }

  void NodeGen::testStackOverflow(PRegBList* exprStack, SplitSig* sig) {
    APPEND(new InterruptCheckNode(exprStack, sig));
    ((SCodeScope*)currentScope())->addSend(exprStack);
  }

  void NodeGen::loadBlockParent(PReg* block, PReg* dst) {
    // given a block, load the frame pointer of its enclosing scope
    APPEND(new LoadOffsetNode(block, scope_offset(), dst));
  }

# ifdef UNUSED
  void NodeGen::loadSender(PReg* sp, PReg* dest) {
    // load frame's sender
    APPEND(new LoadOffsetNode(sp, frame_offset * oopSize, dest));
  }
# endif

  void NodeGen::loadSaved(PReg* frame, nmethod* nm, NameDesc* nd, PReg* dest, oop name) {
    assert(!nd || !nd->isValue(), "should have a location");
    APPEND(new LoadStackNode(frame, nm, nd, dest, name));
  }

  void NodeGen::storeSaved(PReg* val, PReg* frame, nmethod* nm, NameDesc* nd, oop name) {
    assert(!nd->isValue(), "should have a location");
    APPEND(new StoreStackNode(val, frame, nm, nd, name));
  }

  void NodeGen::pathAssign(PReg* rcvr,
                           realSlotRef* path,
                           PReg* val,
                           bool checkStore) {
    COMMENT("Begin slot assignment");
    int32 offset = smiOop(path->desc->data)->byte_count() - Mem_Tag;
    if (path->holder->is_object_or_map()) {
      PReg* t = new TempPReg(currentScope());
      t = loadPath(path->holder, rcvr, t);
      APPEND(new StoreOffsetNode(val, t, offset, checkStore));  
    } else {
      fatal("don't support vframe lookups");
    }
  }

  PReg* NodeGen::loadPath(lookupTarget* target,
                          PReg* receiver,
                          PReg* dest) {
    // returns register to use for base address of load
    if (target->is_receiver()) {
      return receiver;
    } else {
      assert(target->is_object(), "must be an object path search");
      objectLookupTarget* otarget = (objectLookupTarget*) target;
      if (otarget->prevTargetSlot) {
        pathLookup(otarget->prevTargetSlot, receiver, dest);
      } else {
        loadOop(otarget->obj, dest);
      }
      return dest;
    }
  }
    
  void NodeGen::pathLookup(realSlotRef* path, PReg* receiver, PReg* dest) {
    if (path->holder->is_object_or_map()) {
      PReg* base = loadPath(path->holder, receiver, dest);
      fint offset = smiOop(path->desc->data)->byte_count() - Mem_Tag;
      APPEND(new LoadOffsetNode(base, offset, dest));
    } else {
      fatal("don't support vframe lookups");
    }
  }
    
  Node* NodeGen::restart(MergeNode* loopStart,
                         PRegBList* exprStack, SplitSig* s) {
    Node* n = APPEND(new RestartNode(exprStack, s, loopStart));
    // reset current so that any code generated after the restart will
    // be ignored in later phases since there is no path reaching it

    // Next line added by dmu 4/26/07 to try to fix bug:
    //  inlined [3] loop had nsends of 0, but needed mask for call to InterruptCheck
    ((SCodeScope*)currentScope())->addSend(exprStack);
    
    current = new NopNode;
    return n;
  }

  Node* NodeGen::selfCall(SCodeScope* sc, LookupType l, PReg* self,
                          oop sel, oop del, oop methodHolder,
                          MergeNode* nlrPoint, fint argc,
                          PRegBList* exprStack, SplitSig* sig) {
    Unused(self);
    APPEND(new SendNode(l, sel, del, methodHolder, nlrPoint, argc,
                        exprStack, sig));
    sc->addSend(exprStack);
    return current;
  }

  Node* NodeGen::perform(SCodeScope* sc, LookupType l, PReg* self,
                         fint argc, oop del, oop mh, MergeNode* nlrPoint,
                         PRegBList* exprStack, SplitSig* sig) {
    return selfCall(sc, l, self, as_smiOop(argc), del, mh, nlrPoint, argc,
                    exprStack, sig);
  }

  PrimNode* NodeGen::primCall(SCodeScope* sc, PrimDesc* p,
                              MergeNode* nlrPoint, fint argc,
                              PRegBList* exprStack, SplitSig* sig,
                              BlockPReg* failBlock) {
    PrimNode* pn;
    APPEND(pn = new PrimNode(p, nlrPoint, argc, exprStack, sig, failBlock));
    if (pn->exprStack) sc->addSend(pn->exprStack);
    return pn;
  }

  Node* NodeGen::uncommonBranch(PRegBList* exprStack, bool restartSend) {
    assert(SICDeferUncommonBranches, "shouldn't use uncommon traps");
    Node* n = APPEND(new UncommonNode(exprStack, restartSend));
    assert(currentScope()->isCodeScope(), "must be non-access");
    ((SCodeScope*)currentScope())->addSend(exprStack);
    current = NULL;
    return n;
  }

  void NodeGen::nonLifoTrap(PRegBList* exprStack, SplitSig* s) {
    APPEND(new DeadBlockNode(exprStack, s));
  }


  void NodeGen::deadEnd() {
    APPEND(new DeadEndNode);
    current = NULL;
  }
  void NodeGen::loadOop(oop p, PReg* dest) {
    APPEND(new AssignNode(new_ConstPReg(currentScope(), p), dest));
  }
  void NodeGen::loadBlockOop(BlockPReg* b, SplitSig* s) {
    APPEND(new BlockCloneNode(b, s)); }
  void NodeGen::move(PReg* from, PReg* to) { APPEND(new AssignNode(from, to));}
  void NodeGen::zapBlock(BlockPReg* block) { 
    assert(current, "should not generate unreachable zaps");
    APPEND(new BlockZapNode(block)); }

  static PRegBList* mlist;
  static SplitSig* msig;
  static void materializeHelper(PReg*, PReg* r, bool) {
    theNodeGen->materializeBlock(r, msig, mlist, true);
  }

  void NodeGen::materializeBlock(PReg* r, SplitSig* sig,
                                 PRegBList* materialized, bool recursive) {
    Unused(recursive);
    if (r->isBlockPReg() && !materialized->contains(r)) {
      BlockPReg* bpr = (BlockPReg*)r;
      bpr->isMaterialized = true;

      // make sure the block is created
      APPEND(new BlockCreateNode(bpr, sig));
      
      // flush if uplevel-accessed
      // done by BlockCreateNode
      // if (recursive) APPEND(new FlushNode(currentScope(), bpr, true));
      materialized->append(bpr);
      
      // also make sure all uplevel-accessed blocks exist
      msig = sig; mlist = materialized; // params for materializeHelper
      bpr->nscope()->doUplevelAccesses(bpr, materializeHelper);
    }
  }
      
  void NodeGen::loadArg(PReg* from, PReg* to) {
    APPEND(new AssignNode(from, to));
  }
      
  void NodeGen::loadArg(fint argNo, PReg* from, bool isPrimCall) {
    Unused(isPrimCall);
    assert(currentScope()->isCodeScope(), "oops");
    SCodeScope* s = (SCodeScope*)currentScope();
    fint bci = s->bci();
    Location l = argNo == -1 ? ReceiverReg : ArgLocation(argNo);
                // weird arg numbering - 0 is 1st arg, not receiver
    // uses aren't right yet (call should have use) -fix this
    loadArg(from, new ArgSAPReg(s, l, true, false, bci, bci));
  }

  void NodeGen::prepareNLR(PReg* result, PReg* scope, smi homeID) {
    Unused(scope);
    // set up NLR registers
    loadArg(result, nlrResultPR);
    APPEND(new LoadIntNode(homeID, nlrHomeIDPR));
    // (nlrHomePR is loaded by caller)
  }

  Node* NodeGen::testNLR(smi homeID) {
    // test if NLR has reached home; the node returned is the success
    // branch (i.e. the home has been reached), current is the other
    // branch
    Node* homeIDTest = NULL;
    if (homeID) {       // note: will be 0 if no inlining
      if (isImmediate(smiOop(homeID))) {
        APPEND(new ArithRCNode(SubCCArithOp, nlrHomeIDPR, homeID, noPR));
      } else {
        APPEND(new LoadIntNode(homeID, nlrTempPR));
        APPEND(new ArithRRNode(SubCCArithOp, nlrHomeIDPR, nlrTempPR, noPR));
      }
      homeIDTest = APPEND(new BranchNode(EQBranchOp));
      APPEND1(new ArithRRNode(SubCCArithOp, nlrHomePR, framePR, noPR));
    } else {
      APPEND (new ArithRRNode(SubCCArithOp, nlrHomePR, framePR, noPR));
    }
    Node* homeFrameTest = APPEND(new BranchNode(EQBranchOp));

    // home & homeID match
    Node* finalReturn = new NopNode;
    APPEND1(finalReturn);

    // no match, continue NLR
    Node* cont = new MergeNode("testNLR cont"); 
    if (homeIDTest) homeIDTest->append(cont);
    homeFrameTest->append(cont);
    current = cont;
    return finalReturn;
  }

  void NodeGen::continueNonLocalReturn() {
    // continue NLR (return through caller's inline cache)
    APPEND(new NonLocalReturnNode(NULL, NULL));
  }

  void NodeGen::branch(MergeNode* target) {
    // connect current with target
    if (current != NULL && current != target) {
      current->append(target);
    }
    current = target;
  }

  void NodeGen::branchCode( MergeNode*           targetNode,
                            bool                 isBackwards,
                            PReg*                targetPR,
                            SExpr*               testExpr,
                            BranchBCTargetStack* targetStack,
                            SExprStack*          exprStack,
                            PRegBList*           exprStackPRs, 
                            SplitSig*            s ) {
    // gen nodes for all but indexed branch bytecode
    // branch if top of stack == target_oop, uncond if PRs NULL
    
    if ( targetPR != NULL  &&  SICBranchSplitting ) {
      const char* whyNot =  splitCondBranch( targetNode,
                                             isBackwards,
                                             targetPR,
                                             testExpr,
                                             targetStack,
                                             exprStack,
                                             exprStackPRs,
                                             s );
      if (PrintSICBranchSplitting) {
        if (!whyNot)
          lprintf("branch splitting succeeded\n");
        else
          lprintf("branch splitting failed: %s\n", whyNot);
      }
      
      if ( !whyNot )
        return;
    }
      
    BranchOpCode op;
    if (targetPR == NULL)
      op = ALBranchOp;
    else {
      APPEND( new ArithRRNode( SubCCArithOp, targetPR, testExpr->preg(), noPR));
      op = EQBranchOp;
    }

    Node* condBranch = new BranchNode(op);
    append( condBranch );
    appendBranchCodeNodes( 1,
                           isBackwards,
                           targetNode,
                           targetStack,
                           exprStack,
                           exprStackPRs,
                           s );
    current = condBranch;
  }
  

  const char* NodeGen::splitCondBranch( MergeNode*           targetNode,
                                        bool                 isBackwards,
                                        PReg*                targetPR,
                                        SExpr*               testExpr,
                                        BranchBCTargetStack* targetStack,
                                        SExprStack*          exprStack,
                                        PRegBList*           exprStackPRs, 
                                        SplitSig*            s ) {
    // try to split a conditional branch bc to avoid materializing
    // the boolean
    
    // local splitting only for now
  
    assert(targetPR->isConstPReg(), 
           "cond branch must be testing for constant");
    oop targetOop = ((ConstPReg*)targetPR)->constant;
    
    if (!testExpr->isMergeSExpr()) 
      return "testExpr not MergeSExpr";
    if (!((MergeSExpr*)testExpr)->isSplittable()) 
      return "textExpr not splittable";
    SExprBList* exprs = ((MergeSExpr*)testExpr)->exprs;
    assert(testExpr->node(), "splittable implies node()");
    Node* preceedingMerge = testExpr->node();
    if (current != preceedingMerge)
      return "not local"; // local only for now
    
    if ( preceedingMerge->nPredecessors() != exprs->length() )
      return "would have to iterate over predecessors";
      
    fint i;
    for ( i = 0;  
          i < exprs->length(); 
          ++i) {
       SExpr* e = exprs->nth(i);
       Node* n = e->node();
       if ( !preceedingMerge->isPredecessor(n) )
         return "merge not immediately after expr node";
       if ( !e->isConstantSExpr() )
         return "merge contains non-constant expression";
    }
    MergeNode* mergeForBranching      = new MergeNode("for branching");
    MergeNode* mergeForFallingThrough = new MergeNode("for falling through");
    mergeForBranching     ->setScope(currentScope());
    mergeForFallingThrough->setScope(currentScope());
    for ( i = 0;  
          i < exprs->length();  
          ++i) {
      SExpr* e = exprs->nth(i);
      Node* n = e->node();
      MergeNode* mn = e->constant() == targetOop
        ?  mergeForBranching
        :  mergeForFallingThrough;
      mn->setPrev(n);
      n->moveNext(preceedingMerge, mn);
    }     
    while (preceedingMerge->nPredecessors())
      preceedingMerge->removePrev(preceedingMerge->prev(0));
       
    current = mergeForBranching;
    branchCode( targetNode,
                isBackwards,
                NULL,
                NULL,
                targetStack,
                exprStack,
                exprStackPRs,
                s);
                
    append(mergeForFallingThrough);
    return NULL;
  }
  
  
  void NodeGen::branchIndexedCode( 
                       int32                 nCases,
                       MergeNode**           targetNodes,
                       bool*                 isBackwards,
                       PReg*                 testPR,
                       BranchBCTargetStack** targetStacks,
                       SExprStack*           exprStack,
                       PRegBList*            exprStackPRs, 
                       SplitSig*             s ) {
    // generate indexed branch
        
    IndexedBranchNode* ib = new IndexedBranchNode(testPR, nCases);
    current->append(ib); 
    
    for (int32 i = 0;  i < nCases;  ++i) {
      current = ib;

      appendBranchCodeNodes( i + 1, 
                             isBackwards[i], 
                             targetNodes[i], 
                             targetStacks[i],
                             exprStack,
                             exprStackPRs, 
                             s );
    }
    current = ib;
  }
  
  
  void NodeGen::appendBranchCodeNodes( int32                whichSucc, 
                                       bool                 isBackwards,
                                       MergeNode*           target,
                                       BranchBCTargetStack* targetStack,
                                       SExprStack*          exprStack,
                                       PRegBList*           exprStackPRs,
                                       SplitSig*            s ) {
  // use nop nodes to avoid indexed br node being pred twice of
  // same merge node if two cases goto same place
  APPENDN(whichSucc, new NopNode);
  
  // move stack values to targetStack
  targetStack->mergeInExprsFromStack(exprStack, target, isBackwards);
  
 // append to current for a fwd or back branch for a branch bc
  Node* n =  isBackwards
    ?  (Node*)  new RestartNode(exprStackPRs, s, target)
    :           target;
  append( n);
}


  void NodeGen::print_short() { lprintf("NodeGen %#lx", (unsigned long)this); }
      
# endif

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "aNode.hh"
# include "_aNode.cpp.incl"

# endif


/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "regAlloc.hh"
# include "_regAlloc.cpp.incl"



SICAllocator* theAllocator;

static int compare_pregs(const void* p1,  const void* p2) {
  PReg** r1 = (PReg**) p1;
  PReg** r2 = (PReg**) p2;
  return (*r1)->weight - (*r2)->weight;
}

SICAllocator::SICAllocator() {
  theAllocator = this; 
  globals = NULL;
  pairs = new RegisterPairList(100);
  regClasses = new RegisterEqClassBList(50);
  regClasses->append(NULL);     // make first index be 1, not 0
}

void SICAllocator::allocateToStack(PReg* r) {
  Location l = r->scope->findStackTemp(r);
  r->allocateTo(l);
  if (l >= StackLocations + stackTemps) {
    assert(l == StackLocations + stackTemps, "should increase by one");
    stackTemps++;
  }
  if (PrintSICRegAlloc) {
    lprintf("*SIC-allocating %s (weight %ld, %ld uses/%ld defs)\n",
            r->name(), (void*)r->weight, (void*)r->nuses(), (void*)r->ndefs());
  }
}

bool SICAllocator::allocateConst(ConstPReg* pr, Location preferred) {
  // allocate pr to a register if possible; if preferred != UnAllocated,
  // must allocate to that register
  if (!pr->needsRegister()) return false;
  
  // allocate only to register - allocating to stack makes code slower
  // also, try to move initialization out of loops
  fint depth = pr->scope->loopDepth;
  SCodeScope **scopes= NEW_RESOURCE_ARRAY( SCodeScope*, depth + 1);
  assert(pr->scope->isCodeScope(), "expected non-access scope");
  SCodeScope* s = (SCodeScope*)pr->scope;
  fint i;
  for (i = 0; i < depth && s; i++) {
    assert(s->isCodeScope(), "should be non-access");
    scopes[i] = (SCodeScope*)s;
    fint d = s->loopDepth;
    do { s = s->sender(); } while (s && s->loopDepth == d);
  }
  if (s) {
    scopes[i] = s;
    assert(s->loopDepth == 0, "should have reached top");
  } else {
    // loop wasn't completely inlined
    scopes[i] = theSIC->topScope;
  }
  if (preferred == UnAllocated) {
    for ( ; i >= 0; i--) {
      Location l = scopes[i]->findReg(pr, firstRegisterIndexToUse, incoming);
      if (l != UnAllocated) {
        pr->scope = scopes[i];          // promote to higher scope
        pr->allocateTo(l);
        if (PrintSICRegAlloc) {
          lprintf("*SIC-allocating %s (weight %ld, %ld uses/%ld defs%s)\n",
                  pr->name(),
                  (void*)pr->weight,
                  (void*)pr->nuses(),
                  (void*)pr->ndefs(),
                  i < pr->scope->loopDepth ? "; moved out of loop" : "");
        }
        break;
      }
    }
    if (pr->loc == UnAllocated && PrintSICRegAlloc) {
      lprintf("*not allocating %s (weight %ld, %ld uses/%ld defs)\n",
              pr->name(),
              (void*)pr->weight,
              (void*)pr->nuses(),
              (void*)pr->ndefs());
    }
  } else if (isRegister(preferred)) {
    for ( ; i >= 0; i--) {
      if (scopes[i]->isRegAvailable(pr, preferred, incoming)) {
        pr->allocateTo(preferred);
        return true;
      }
    }
  }
  return pr->loc != UnAllocated;
}

void SICAllocator::allocate(PRegBList* g, RegisterString inc) {
  // simple count-based register allocation
  globals = g; 
  if (SICRegTargeting) handlePairs();

  qsort(globals->data_addr(), globals->length(), sizeof(PReg*),
        compare_pregs);
  assert(globals->isEmpty() ||
         globals->first()->weight <= globals->last()->weight,
         "wrong sort order");

  stackTemps = 0;
  firstRegisterIndexToUse = 0;
  incoming = inc;
  while (nthBit(CalleeSavedRegs[firstRegisterIndexToUse]) & incoming) firstRegisterIndexToUse++;

  while (globals->nonEmpty()) {
    PReg* r = globals->pop();
    assert(r->ndefs() + r->nuses() > 0, "PReg is unused");
    if (r->loc != UnAllocated) {
      assert(r->weight < 0 || r->regClass, "must be targeted register");
      if (isRegister(r->loc)) continue;
    }
    bool wasAllocatedHere = false;      // to avoid extra calls to targetUses()
    if (r->isConstPReg()) {
      allocateConst((ConstPReg*)r);
      wasAllocatedHere = (r->loc != UnAllocated);
    } else if (  r->uplevelW
           ||   (keepUplevelRPRegsInMemory && r->uplevelR)
           ||   (SICDontUseRegs && !(r->isSAPReg() || r->isTempPReg()))) {
      // allocate uplevel-written _local_ vars to stack.
      allocateToStack(r);
      wasAllocatedHere = true;
    } else {
      Location loc = r->scope->findReg(r, firstRegisterIndexToUse, incoming);
      if (loc != UnAllocated) {
        // found a free register 
        r->allocateTo(loc);
        wasAllocatedHere = true;
        if (PrintSICRegAlloc) {
          lprintf("*SIC-allocating %s (weight %ld, %ld uses/%ld defs)\n",
                  r->name(),
                  (void*)r->weight,
                  (void*)r->nuses(),
                  (void*)r->ndefs());
        }
      } else if (r->loc == UnAllocated) {
        allocateToStack(r);
        wasAllocatedHere = true;
      } else {
        // already stack-allocated via register targeting, and we didn't
        // find a free register in our scope
      }
    }
    if (r->loc != UnAllocated && wasAllocatedHere) {
      if (SICRegTargeting) r->targetUses(incoming, false);
      if (r->regClass) {
        // co-allocate other PRegs
        for (PReg* pr = regClasses->nth(r->regClass)->first;
             pr; pr = pr->regClassLink) {
          if ( pr->loc == UnAllocated 
          &&   !(keepUplevelRPRegsInMemory && pr->uplevelR)
          &&   pr->scope->isRegAvailable(pr, r->loc, incoming)) {
            // NB: test for isRegAvailable is necessary; in general, r
            // covers pr, but pr may have been propagated to a higher scope
            pr->loc = r->loc;
            theSIC->check_flushability(pr);
            if (PrintSICRegAlloc) {
              lprintf("*SIC-coallocating %s (weight %ld, %ld uses/%ld defs)\n",
                      pr->name(),
                      (void*)pr->weight,
                      (void*)pr->nuses(),
                      (void*)pr->ndefs());
            }
          }
        }
      }
      // allocate *after* targeting so that register doesn't appear
      // to be taken
      r->scope->allocateReg(r);
      if (r->regClass) {
        // possibly extend range where r->loc is busy
        for (PReg* pr = regClasses->nth(r->regClass)->first;
             pr; pr = pr->regClassLink) {
          if (pr->loc != UnAllocated) pr->scope->allocateReg(pr, true);
        }
      }
    }
  }
}


// PReg methods for register allocation

static PReg* thePReg;           // for targetUse
static PRegBList* targetedRegs;
static RegisterString inc;

// can't easily pass extra args to list iterators - arggh!
class RecursionHack {
 public:
  PReg* saved;
  RecursionHack(PReg* r) { saved = thePReg; thePReg = r; }
  ~RecursionHack() { thePReg = saved; }
};

static void targetUse(PUse* u) {
  Node* n = u->node;
  if (n->isAssignmentLike() && n->hasDest()) {
    assert(n->src() == thePReg, "wrong source");
    PReg* r = n->dest();
    // thePReg is assigned to r; if r is singly-defined, try to allocate
    // it to the same register
    if ( r->loc == UnAllocated 
    &&  !r->uplevelW
    &&  !(SICAllocator::keepUplevelRPRegsInMemory && r->uplevelR)
    &&   r->isSinglyAssigned()
    &&   r->checkEquivalentDefs()) {
      assert(!r->isConstPReg(), "shouldn't be const");
      // ok, we can use the same register if it is available in r's
      // live range
      if (PrintSICRegTargeting) {
        lprintf("*SIC-targetingU %s to %s ", r->name(),
                locationName(thePReg->loc));
      }
      if (r->scope->isRegAvailable(r, thePReg->loc, inc)) {
        r->allocateTo(thePReg->loc);
        if (PrintSICRegTargeting) lprintf("- ok.\n");
        targetedRegs->append(r);
        thePReg->weight = r->weight = -1;
        r->targetUses(inc, true);
      } else {
        if (PrintSICRegTargeting) lprintf("- not available.\n");
      }
    }
  }
}

static void targetU(PRegBBIndex* p) {
  DUInfo* info = p->bb->duInfo.info->nth(p->index);
  info->uses.apply(targetUse);
}



void PReg::targetDef() {
  // try to target the receiver's single definition to the receiver's reg
  DUInfo* info = dus.first()->defUseInfo();
  PDefListElem* e = info->defs.head();
  if (!e) {
    // info not in first elem - would have to search
    if (PrintSICRegTargeting) 
      lprintf("*not targeting %s: def not in first info\n", name());
    return;
  }
  Node* defNode = e->data()->node;
  if (defNode->isAssignmentLike() && defNode->hasSrc()) {
    assert(defNode->dest() == this, "wrong preg");
    PReg* r = defNode->src();
    // r defines the receiver; if r is also singly-assigned, we can
    // target it to our register
    if (r->loc == UnAllocated &&
        r->isSinglyAssigned() && r->checkEquivalentDefs()) {
      if (PrintSICRegTargeting) {
        lprintf("*SIC-targetingD %s to %s ", r->name(), locationName(loc));
      }
      bool ok;
      if (r->isConstPReg()) {
        // need to treat const pregs specially - propagate scope, or
        // maybe doesn't need a register
        ConstPReg* pr = (ConstPReg*)r;
        if (!pr->needsRegister()) {
          if (PrintSICRegTargeting) lprintf("- not necessary.\n");
          return;
        } else {
          ok = theAllocator->allocateConst(pr, loc);
        }
      } else if (r->scope->isRegAvailable(r, loc, inc)) {
        r->allocateTo(loc);
        ok = true;
      } else {
        if (PrintSICRegTargeting) lprintf("- not available.\n");
        ok = false;
      }
      if (ok) {
        if (PrintSICRegTargeting) lprintf("- ok.\n");
        targetedRegs->append(r);
        thePReg->weight = r->weight = -1;
        r->targetUses(inc, true);
      } else {
        if (PrintSICRegTargeting) lprintf("- not available.\n");
      }
    }
  }
}

void PReg::targetUses(RegisterString incoming, bool recursive) {
  // receiver has been register-allocated; try to target other PRegs so that
  // register moves will be eliminated
  if (isSinglyAssigned() && checkEquivalentDefs()) {
    FlagSetting fs(PrintSICRegTargeting,
                   PrintSICRegAlloc || PrintSICRegTargeting);
    RecursionHack h(this);
    if (!recursive) {
      inc = incoming;
      targetedRegs = new PRegBList(10);
    }
    targetDef();        // try targeting my defining PReg
    dus.apply(targetU);         // try targeting my uses
    if (!recursive) {
      // mark the targeted regs as allocated
      while (targetedRegs->nonEmpty()) {
        PReg* r = targetedRegs->pop();
        r->scope->allocateReg(r);
      }
      targetedRegs = NULL;
    }
  }
}

RegisterEqClass::RegisterEqClass(PReg* f) {
  first = last = f;
  assert(last->regClassLink == NULL, "should be empty");
}

void RegisterEqClass::append(PReg* other) {
  assert(first->regClass && first->regClass == last->regClass,
         "bad regClass");
  assert(last->regClassLink == NULL, "should be empty");
  last->regClassLink = other;
  fint c = first->regClass;
  for ( ; last->regClassLink; last = last->regClassLink) {
    last->regClass = c;
  }
  last->regClass = c;
}

void RegisterPair::add() { theAllocator->pairs->append(this); }

static SAPReg* check_lhs = NULL;
static bool check_ok = false;

static void checkDef(PDef* u) {
  Node* n = u->node;
  if (check_lhs->slow_isLiveAt(n)) check_ok = false;
}

static void checkDefs(PRegBBIndex* p) {
  DUInfo* info = p->bb->duInfo.info->nth(p->index);
  info->defs.apply((defDoFn)checkDef);
}

void SICAllocator::handlePairs() {
  RegisterPair* p; // moved dcl out of loop so ?#!$ gdb would show it--dmu
  while (pairs->nonEmpty()) {
    p = pairs->pop();
    PReg* rhs = p->rhs;
    SAPReg* lhs = p->lhs;
    if (lhs->loc != UnAllocated || rhs->loc != UnAllocated) continue;
    if (lhs->isUnused()) continue;

    // The following assert used to be present, but tripped
    // on code that seems to work fine, 
    // assert(!rhs->isUnused(), "if rhs is unused, lhs should be, too");
    // so I am replacing it with
    // the following if-test -- dmui 2/97
    if (rhs->isUnused()) continue;

    if (lhs->regClass == rhs->regClass && lhs->regClass)
      continue;  // already handled
    // see if rhs has any defs during lhs' life time
    // rhs has defs if uplevel-assigned and lhs has sends
    // but since uplevelW are stack-allocated anyway... fix this
    if (rhs->uplevelW) continue;
    check_lhs = lhs;
    check_ok = true;
    rhs->dus.apply(checkDefs);
    if (check_ok) {
      // no definitions - allocate to same reg
      if (PrintSICRegTargeting)
        lprintf("*co-allocating %s and %s\n", rhs->name(), lhs->name());
      // note: rhs's live range covers lhs's live range, so make sure allocate
      // rhs first
      rhs->makeSameRegClass(lhs, regClasses);
      // increase weight of rhs to reflect joint use
      rhs->weight += lhs->weight;
    }
  }
}

# endif
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "basicBlock.hh"
# pragma implementation "basicBlock_inline.hh"
# include "_basicBlock.cpp.incl"

  void BasicBlockBase::init(Node* f, Node* l, fint n) {
    if (f->isMergeNode()) {
      ((MergeNode*)f)->didStartBB = true;     // for debugging
    }
    first = f; last = l; backwardBranches = 0; nnodes = n;
    _id = 0; 
  }
  

  void BB::localCopyPropagate() {
    fint len = duInfo.info->length();
    RegisterString usedTwice = compute_usedTwice();
    fint i;
    
    for (i = 0; i < len; i++) {
      DUInfo* dui = duInfo.info->nth(i);
      localCopyPropagateOnePReg(dui, usedTwice);
    }
  }
  
  
  RegisterString BB::compute_usedTwice() {
    fint len = duInfo.info->length();
    RegisterString used = 0;
    RegisterString usedTwice = 0;
    fint i;
    for (i = 0; i < len; i++) {
      PReg* r = duInfo.info->nth(i)->reg;
      if (r->loc != UnAllocated  &&  isAllocated(used, r->loc)) {
        // two PRegs have same preallocated reg - algorithm below can't handle
        // this
        usedTwice = allocateRegister(usedTwice, r->loc);
      }
      used = allocateRegister(used, r->loc);
    }
    return usedTwice;
  }


  void BB::localCopyPropagateOnePReg(DUInfo* dui, RegisterString usedTwice) {
    PReg* r = dui->reg;
    if (r->loc != UnAllocated  &&  isAllocated(usedTwice, r->loc)) {
      // this preallocated PReg has aliases - don't do CP
      return;
    }    
    PUseListElem* u = dui->uses.head();
    PDefListElem* nextd;
    for (PDefListElem* d = dui->defs.head(); 
                       d && u; 
                       d = nextd) {
      nextd     = d->next();
      PDef* def = d->data();

      if (isSourceOfDefUsedTwice(def, usedTwice))
        continue; // r := f(r2), and r2 is aliased preallocated - can't handle
      u = findFirstUseOfDef(def, u);
      if (u == NULL)
        break; // done with this chain (or PReg)
      u = localCopyPropagateEachUseOfThisDef(dui, d, u, usedTwice);
    }
  }   
  
  
  
  PUseListElem* BB::localCopyPropagateEachUseOfThisDef(DUInfo* dui, 
                                                       PDefListElem* d,
                                                       PUseListElem* firstUseToConsider,
                                                       RegisterString usedTwice) {
    PDef* def = d->data();

    // init loop vars:
    PUseListElem* u = firstUseToConsider;
    fint     u_id = u->data()->node->num();
    fint     d_id =       def->node->num();
    fint nextd_id = IDOfDefAfter(d);
    
    // for all uses of the def at d
    while (d_id < u_id  &&  u_id <= nextd_id) {
      // the def at d_id reaches the use at u_id
      dui->propagateTo(this, dui->reg, def, u->data(), false);
      
      u = u->next();  
      if (u == NULL) return NULL; 
      u_id = u->data()->node->num();
    }
    return u;
  }
  
  
  bool BB::isSourceOfDefUsedTwice(PDef* def, RegisterString usedTwice) {
    Node* definingNode = def->node;
    return  definingNode->hasSrc()  &&  isAllocated(usedTwice, definingNode->src()->loc);
  }

  
  PUseListElem* BB::findFirstUseOfDef(PDef* def, PUseListElem* startingUse) {
    fint d_id = def->node->num();
    for ( PUseListElem* u = startingUse;
                        u != NULL;
                        u = u->next() ) {
      fint u_id = u->data()->node->num();
      if (u_id >= d_id)
        return u;
    }
    return NULL;
  }
  
  
  fint BB::IDOfDefAfter(PDefListElem* d) {
    PDefListElem* nextd = d->next();
    const fint BIG = ~(fint(1) << fint(sizeof(fint) * BitsPerByte  -  1));
    assert(sizeof(fint) != 4  ||  BIG == 0x7fffffff, "just checking");
    return nextd == NULL  ?  BIG  : nextd->data()->node->num();
  }


  void BB::makeUses() {
    // collect defs and uses for all pregs (and fill pregTable in the process)
    assert(duInfo.info == NULL, "shouldn't be set");
    duInfo.info = new DUInfoBList(nnodes + 10);
    // note: BB may grow during loop below (inserting FlushNodes)
    // Or new BBs may be added
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted) continue;
      n->makeUses(this);
    }
  }

  void BB::renumber() {
    fint count = 0;
    for (Node* n = first; n != last->next(); n = n->next()) n->setNum(count++);
    nnodes = count;
  }
    
  void BB::remove(Node* n) {
    // remove this node and its defs & uses
    // NB: nodes aren't actually removed from the graph but just marked as
    // deleted.  This is much simpler because the topology of the flow graph
    // doesn't change this way
    assert(contains(n), "node isn't in this BB");
    n->removeUses(this);
    n->deleted = true;
  }

  void BB::addAfter(Node* prev, Node* newNode) {
    // prev == NULL means add as first node
    assert(nnodes, "shouldn't add anything to this BB");
    assert(prev == NULL || contains(prev), "node isn't in this BB");
    BB* newNodeBB = this;
    if (prev) {
      prev->insertNext(newNode);
      // cannot just set last because prev may have multiple successors
      //   if (prev == last) last = newNode;
      // Instead add flush node to NEXT bb -- dmu 10/96
      // Nope, cannot do that if following node is MERGE, have to add a bb!
      // See the long fatal elsewhere in this file where MergeNodes are created.
      
      if (prev == last) {
        // cannot use node::newBB(), would disturb successor node BB's
        newNodeBB = new BB(newNode, newNode, 1);
        theSIC->bbIterator->addBBPostHoc( newNodeBB, 
                                          this, 
                                          newNode->next()->bb());
      }
    } else {
      first->insertPrev(newNode);
      first = newNode;
    }
    if (theSIC->bbIterator->usesBuilt) {
      newNode->makeUses(newNodeBB);
    } else {
      newNode->setBB(newNodeBB);
    }
    if ( newNodeBB == this )
      renumber();
    assert(newNode->bb() == newNodeBB, "should point to me now");
  }

  static BB* thisBB;
  static void duChecker(PRegBBIndex* p) {
    if (p->bb == thisBB) fatal("should not be in middle of list");
  }

  static bool findMyBB(PRegBBIndex* bb, PRegBBIndex* p) {
    return p->bb == (BB*)bb; // hack around type system, find fn would want PRegBBIndex
  }

  fint BB::addUDHelper(PReg* r) {
    // we're computing the uses block by block, and the current BB's
    // PRegBBIndex is always the last entry in the preg's list.
    assert(nnodes, "shouldn't add anything to this BB");
    theSIC->bbIterator->pregTable->nthPut(r->id(), r, true);
    PRegBBIndex* p;
    if (theSIC->bbIterator->usesBuilt) {
      // find entry for the PReg
      fint i = r->dus.find((PRegBBIndex*)this, findMyBB);
      if (i >= 0) {
        p = r->dus.nth(i);
      } else {
        // create new entry
        duInfo.info->append(new DUInfo(r));
        r->dus.append(p = new PRegBBIndex(this, duInfo.info->length() - 1, r));
      }
    } else {
      // while building the defs & uses, the PReg's entry must be the last
      // one in the list (if there is an entry for this BB)
      if (r->dus.isEmpty() || r->dus.last()->bb != this) {
        // PReg doesn't yet have an entry for this block
#       if GENERATE_DEBUGGING_AIDS
          if (CheckAssertions) {
            thisBB = this;
            r->dus.apply(duChecker);
          }
#       endif
        duInfo.info->append(new DUInfo(r));
        r->dus.append(new PRegBBIndex(this, duInfo.info->length() - 1, r));
      }
      p = r->dus.last();
    }
    assert(p->bb == this, "wrong BB");
    assert(duInfo.info->nth(p->index)->reg == r, "wrong PReg");
    return p->index;
  }

  PUse* BB::addUse(Node* n, PReg* r, bool soft) {
    assert(contains(n), "node isn't in this BB");
    if (r->isNoPReg()) return NULL;
    PUse* u = soft ? new PSoftUse(n) : new PUse(n);
    r->incUses(u);
    fint index = addUDHelper(r);
    duInfo.info->nth(index)->uses.append(u);
    return u;
  }
    
  PDef* BB::addDef(Node* n, PReg* r) {
    assert(contains(n), "node isn't in this BB");
    if (r->isNoPReg()) return NULL;
    PDef* d = new PDef(n);
    r->incDefs(d);
    fint index = addUDHelper(r);
    duInfo.info->nth(index)->defs.append(d);
    if (r->uplevelR) {
      // flush uplevel-read pregs to stack after each definition
      n->addFlushNode(new FlushNode(r, n->scope(), n->bci()));
    }
    return d;
  }

  void BB::allocateTempRegisters(BitVector** hardwired, PRegBList* tempRegs,
                                 BitVectorBList* lives) {
    if (!nnodes) return;            // empty BB

    RegisterEqClassBList regClasses(nnodes + 1);
    regClasses.append(NULL);        // first reg class has index 1

    fint  use_count[NumRegisters], def_count[NumRegisters];
    for (fint i = 0; i < NumRegisters; i++) use_count[i] = def_count[i] = 0;

    allocate_to_preferred_candidates_if_possible(use_count, def_count);

    // allocate other temp regs (using the untouched temp regs of this BB)
    fint temp = 0;
    for (int i = 0; i < duInfo.info->length(); i++) {
      // collect temp regs 
      PReg* r = duInfo.info->nth(i)->reg;
      if (r->loc == UnAllocated && !r->isUnused() && r->isLocalTo(this)) {
        assert(r->dus.first()->index == i, "should be the same");
        for ( ; temp < NumTempRegs &&
             use_count[TempRegs[temp]] + def_count[TempRegs[temp]] > 0;
             temp++) ;
        if (temp == NumTempRegs) break;     // ran out of regs
        // ok, allocate TempRegs[temp] to the preg and equivalent pregs
        Location t = TempRegs[temp++];
        PReg* frst = r->regClass ? regClasses.nth(r->regClass)->first : r;
        for (PReg* pr = frst; pr; pr = pr->regClassLink) {
          doAlloc(pr, t);
          pr->regClass = 0;
        }
      }
      r->regClass = 0;
    }

    if (temp == NumTempRegs) {
      // ran out of temp regs with the simple strategy - try using slow
      // allocation algorithm
      slowAllocateTempRegisters(hardwired, tempRegs, lives);
    }
  }
  
  void BB::allocate_to_preferred_candidates_if_possible(fint use_count[], fint def_count[]) {
    RegCandidateBList cands(nnodes);
 
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted) continue;
      n->markAllocated(use_count, def_count);
      if (n->isAssignNode())
        pick_candidates_for_assignment_node(n, use_count, def_count, cands);
    }

    // now examine all candidates and allocate them to preferred register
    // if possible
    while (cands.nonEmpty()) {
      RegCandidate* c = cands.pop();
      if (def_count[c->loc] == c->ndefs) {
        doAlloc(c->r, c->loc);
      }
    }
  }
  
  
  void BB::pick_candidates_for_assignment_node(Node* n, fint use_count[], fint def_count[],
                                               RegCandidateBList &cands) {
    PReg* src = n->src();
    PReg* dest = n->dest();
    bool localSrc  = src ->isLocalTo(this);
    bool localDest = dest->isLocalTo(this);
    if ( isRegister(src->loc)) {
      if (dest->loc == UnAllocated && localDest) {
        // PR = PR2(reg)
        // allocate dest->loc to src->loc, but only if src->loc
        // isn't defined again
        cands.append(new RegCandidate(dest, src->loc, def_count[src->loc]));
      }
    } else if ( isRegister(dest->loc)) {
      if (src->loc == UnAllocated && localSrc) {
        // PR2(reg) = PR
        // should allocate src->loc to dest->loc, but only if dest->loc
        // has single definition (this one) and isn't used before
        // this point   [simplification]
        if (def_count[dest->loc] != 1 || use_count[dest->loc]) {
          // not eligible for special treatment
        } else {
          cands.append(new RegCandidate(src, dest->loc, 1));
        }
      }
    } else if (localSrc && localDest) {
      // both regs are local and unallocated - put them in same
      // equivalence class
      // fix this - should check for overlapping live ranges
      //        needed to say "if nonoverlapping live ranges(src, dest)"
      //        src->makeSameRegClass(dest, &regClasses);
      //        if (WizardMode) warning("basicBlock: happens");
    } else {
      // non-local registers - skip
    }
  }

    
  // allocate PRegs that are used & defined solely within this BB
  void BB::slowAllocateTempRegisters(BitVector** hardwired, PRegBList* tempRegs,
                                     BitVectorBList* lives) {
    // clear temporary data structures
    tempRegs->clear();
    lives->clear();
    fint i;
    for (i = 0; i < NumTempRegs; i++) {
      hardwired[i]->setLength(nnodes);
      hardwired[i]->clear();
    }

    for (i = 0; i < duInfo.info->length(); i++) {
      // collect temp regs and hardwired temp regs
      PReg* r = duInfo.info->nth(i)->reg;
      if (r->isLocalTo(this)) {
        assert(r->dus.first()->index == i, "should be the same");
        if (r->isUnused()) {
          // unused register - ignore
        } else {
          DUInfo* info = duInfo.info->nth(r->dus.first()->index);
          tempRegs->append(r);
          BitVector* bv = new BitVector(nnodes);
          lives->append(bv);
          fint firstUse = 0, lastUse = nnodes - 1;
          duInfo.info->nth(i)->getLiveRange(firstUse, lastUse);
          bv->addFromTo(firstUse, lastUse);
        }
      } else if (isTempReg(r->loc)) {
        fint firstUse = 0, lastUse = nnodes - 1;
        if (!r->incorrectDU()) {
          duInfo.info->nth(i)->getLiveRange(firstUse, lastUse);
        } else {
          // can't really compute live range since the temp might be non-local
          // so assume it's live from first node til the end
        }
        hardwired[RegToTempNo[r->loc]]->addFromTo(firstUse, lastUse);
      }
    }

    // now, tempRegs holds all temp regs, and lives contains each register's
    // live range (one bit per node, 1 = reg is live); hardwired contains
    // the ranges where temp regs are already taken (e.g. for NLR, calls, etc)

    // cycle through the temp registers to (hopefully) allow more optimizations
    // later (e.g. scheduling)
    fint lastTemp = 0;
#   define nextTemp(n) (n == NumTempRegs - 1) ? 0 : n + 1

    for (i = 0; i < tempRegs->length(); i++) {
      // try to allocate tempRegs[i] to a temp register
      PReg* r = tempRegs->nth(i);
      if (r->loc != UnAllocated) {
        assert(r->regClass == 0, "should have been cleared");
        continue;
      }
      BitVector* liveRange = lives->nth(i);
      for (fint tempNo = lastTemp, ntries = 0; ntries < NumTempRegs;
           tempNo = nextTemp(tempNo), ntries++) {
        if (liveRange->isDisjointFrom(hardwired[tempNo])) {
          Location temp = TempRegs[tempNo];
          doAlloc(r, temp);
          hardwired[tempNo]->unionWith(liveRange);
          lastTemp = nextTemp(tempNo);
          break;
        }
      }
      if ( r->loc == UnAllocated
      && (PrintSICTempRegisterAllocation   
          ||   (WizardMode  &&  TARGET_ARCH != I386_ARCH) /* happens normally in I386; few regs */ )) {
        lprintf("*could NOT find temp assignment for local %s in BB%ld\n",
               r->name(), (void*)id());
      } else if (r->loc == UnAllocated) {
        if (PrintSICTempRegisterAllocation) lprintf("out of temp regs");
      }
      r->regClass = 0;
    }
  }

  void BB::doAlloc(PReg* r, Location l) {
    assert( !r->cpInfo,
            "should not allocate to a PReg that has been copy-propagated");
    if (PrintSICTempRegisterAllocation) {
      lprintf("*assigning %s to temp %s in BB%ld\n",
             locationName(l), r->name(), (void*)id());
    }
    assert(!r->debug, "should not allocate to temp reg");
    r->loc = l;
    theSIC->check_flushability(r);
  }
  
  void BB::eliminateUnreachableNodes() {
    for (Node* n = first;  n != last->next();  n = n->next()) {
      n->eliminateUnreachableNodes();
    }
  }

  void BB::computeExposedBlocks(BlockPRegBList* l) {
    // add all escaping blocks to l
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted) continue;
      n->computeExposedBlocks(l);
    }
  }
  
  void BB::gen() {
    if (!nnodes) return;            // empty BB
    
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->l && !n->l->isDefined()) n->l->define();
      if (n->deleted) continue;
      n->gen();
    }
  }

  static fint prevsBBLen;
  static void printPrevBBs(AbstractBB* b) {
    lprintf("BB%ld%s", (void*)b->id(), --prevsBBLen > 0 ? ", " : "");
  }
  
  void BB::print_short() {
    lprintf("BB%-3ld %#lx [%ld..%ld]; prevs ",
            (void*)id(), this, (void*)first->id(), (void*)last->id());
    prevsBBLen = _prevs->length();
    _prevs->apply(printPrevBBs);
    lprintf("; ");
    if (next ()) lprintf("next BB%ld", (void*)next ()->id());
    if (next1()) lprintf(    " BB%ld", (void*)next1()->id());
  }

  void BB::print() {
    print_short(); lprintf("(%ld nodes):\n", (void*)nnodes);
    print_code(false);
    lprintf("duInfo: "); duInfo.print();
  }

  void BB::print_code(bool suppressTrivial) {
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted && !(n == first || n == last)) continue;
      if (suppressTrivial && n->isTrivial()) {
        // don't print
      } else {
        n->printID(); n->print_short(); lprintf("\n");
      }
    }
  }

  void BB::print_vcg_nodes(FILE* f, bool suppressTrivial) {
    fprintf(f, "graph: { title: \"BB%d (%d..%d)\" folding:1\n",
            id(), first->id(), last->id());
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted && !(n == first || n == last)) continue;
      if (suppressTrivial && n->isTrivial()) {
        // don't print
      } else {
        char b[256], nname[256];
        char* buf = b;
        mysprintf(buf, "node: { title:\"%d\" label: \"%s\" }\n",
                (void*)n->id(), n->print_string(nname, false));
        fputs(b, f);
      }
    }
    fprintf(f, "}\n");
  }

  void BB::print_vcg_edges(FILE* f, bool suppressTrivial) {
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n->deleted && !(n == first || n == last)) continue;
      if (suppressTrivial && n->isTrivial()) {
        // don't print
      } else {
        for (fint i = 0; i < n->nSuccessors(); i++) {
          Node* nnext = n->nexti(i);
          while (nnext &&
                 (nnext->deleted || (suppressTrivial && nnext->isTrivial())))
            nnext = nnext->next();
          if (nnext) 
            fprintf(f, "edge:{ sourcename:\"%d\" targetname: \"%d\" }\n",
                    n->id(), nnext->id());
        }
      }
    }
  }

  bool BB::contains(Node* which) {
    for (Node* n = first; n != last->next(); n = n->next()) {
      if (n == which) return true;
    }
    return false;
  }

  void BB::verify() {
    fint count = 0;
    for (Node* n = first; n != last->next(); n = n->next()) {
      count++;
      n->verify();
      if (n->bb() != this)
        error2("BB %#lx: Node %#lx doesn't point back to me", this, n);
      // endsBB() is true if the block isn't memoized.  But when a block
      // is memoized after the basic block structure was created, the
      // assertion will complain because the BB structure is never updated
      // (as a matter of principle).
      // if (n == last && !n->endsBB() &&
      //     !(n->next() && n->next()->isMergeNode() &&
      //     ((MergeNode*)(n->next()))->didStartBB)) {
      //   error2("BB %#lx: last Node %#lx isn't endsBB()", this, n);
      //   fatal("compiler error");     // so it bombs when hit
      // }
      if (n->endsBB() && n != last)
        error2("BB %#lx: Node %#lx ends BB but isn't last node", this, n);
    }
    if (count != nnodes) error1("incorrect nnodes in BB %#lx", this);
  }


  void BBIterator::build(Node* first) {
    assert(bbTable == NULL, "already built");
    bbCount = 0;
    pregTable = globals = NULL;
    buildBBs(first);
  }

  void BBIterator::buildBBs(Node* first) {
    // build basic blocks
    // first, sort them topologically (ignoring backward arcs)
    // some things (e.g. SplitPReg::isLiveAt) depend on correct ordering
    BBBList* list = new BBBList(max(BasicNode::currentID >> 3, 10));
    first->newBB()->dfs(list);
    // now, the list contains the BBs in reverse order
    bbCount = list->length();
    bbTable = new BBBList(bbCount);
    for (fint i = bbCount-1; i >= 0; i--) {
      BB* bb = list->nth(i);
      bb->_id = bbCount - i - 1;
      bbTable->append(bb);
    }
  }
  

  void BBIterator::addBBPostHoc(BB* newBB, BB* predBB,  BB* nextBB) {
    // have just created a new BB after whole BB structure was built,
    // need to fix up iterator -- dmu 10/96
    // Have to keep BBs in order, so they get generated in
    // bytecode order, so SAPReg lifetime analysis works. -- dmu
    // nextBB may be earlier than predBB, so insert new BB after
    // predBB (which must preceede newBB).
    
    fint newBBId = predBB->id() + 1;
    bbTable->append(NULL); // create place holder
    for ( fint i = bbCount - 1;  i >= newBBId;  --i ) {
      BB* b = bbTable->nth(i);
      assert(b->id() == i,  "bb id must be same as bb index");
      ++b->_id;
      bbTable->nthPut(i+1, b);
    }
    newBB->_id = newBBId;
    bbTable->nthPut(newBBId, newBB);
    ++bbCount;
    
    // Now need to restitch bb next, pred pointers
    predBB->moveNext(nextBB, newBB);
    nextBB->movePrev(predBB, newBB);
    newBB->setPrev(predBB);
    newBB->setNext(nextBB);
  }


  void BB::dfs(BBBList* list) {
    // build BB graph with depth-first traversal
    _id = 1;            // mark as visited
    if (last->isRestartNode()) {
      // don't follow backward branch, but add backwards arc
      MergeNode* loopHeadNode = ((RestartNode*)last)->loopStart;
      BB* loopHeadBB = loopHeadNode->bb();
      
      bool wasNull = loopHeadBB == NULL; // added for branch bcs
      if ( wasNull ) {                   // added for branch bcs
        loopHeadBB = loopHeadNode->newBB();
      }
      loopHeadBB->backwardBranches++;
      append(loopHeadBB);

      if (loopHeadBB->id() == 0)  { // added for branch bcs
        loopHeadBB->dfs(list);
      }
    } else {
      fint n = last->nSuccessors();
      fint i;
      for (i = 0;  i < n;  i++) {
        Node* next = last->nexti(i);
        if (next == NULL) continue;     // for printing incomplete graphs
        BB* nextBB = next->newBB();
        append(i, nextBB);
      } 
      for (i = nSuccessors() - 1; i >= 0; i--) {
        BB* nextBB = (BB*)nexti(i);
        // only follow the link if next->bb hasn't been visited yet
        if (nextBB->id() == 0) nextBB->dfs(list);
      }
    }
    list->append(this);
  }
  
  static void clearNodes(BB* bb) {
    for (Node* n = bb->first; n != bb->last->next(); n = n->next()) {
      n->setBB(NULL);
    }
  }
  
  void BBIterator::clear() {
    apply(clearNodes);
    bbTable = NULL; pregTable = globals = NULL;
}

  void BBIterator::makeUses() {
    // a few PRegs may be created during makeUses (e.g. for deadBlockObj,
    // true/false etc), so leave a bit of room
    const fint ExtraPRegs = 10;
    fint n = PReg::currentNo + ExtraPRegs;
    pregTable = new PRegBList(n);
    for (fint i = 0; i < n; i++) pregTable->append(NULL);
    for (fint i = 0; i < bbCount; i++) { bbTable->nth(i)->makeUses(); }
    usesBuilt = true;
  }

  void BBIterator::allocateTempRegisters() {
    // speed/space optimization: allocate hardwired et al only once, not for
    // every BB
    BitVector* hardwired[NumTempRegs];
    PRegBList tempRegs(BasicNode::currentID);
    BitVectorBList lives(BasicNode::currentID);    
    fint i;
    for (i = 0; i < NumTempRegs; i++) {
      hardwired[i] = new BitVector(roundTo(BasicNode::currentID, BitsPerWord));
    }
    
    for (i = 0; i < bbCount; i++) {
      bbTable->nth(i)->allocateTempRegisters(hardwired, &tempRegs, &lives);
    }
    
    fint done = 0;
    globals = new PRegBList(pregTable->length());
    for (i = 0; i < pregTable->length(); i++) {
      PReg* r = pregTable->nth(i);
      if (r) {
        if (r->isUnused()) {
          pregTable->nthPut(i, NULL);           // no longer needed
        } else if (r->loc == UnAllocated) {
          globals->append(r);
        } else {
          done++;                               // allocated to a temp register
        }
      }
    }
    if (PrintSICTempRegisterAllocation) {
      fint total = globals->length() + done;
      float f = 100.0 * done / total;
      lprintf("*temporary reg. allocations done; %ld out of %ld = (%3.1f%%).\n",
              (void*)done, (void*)total, *(void**)&f);
    }
  }

  void BBIterator::print() {
    fint i;
    for ( i = 0; i < bbCount; i++) {
      lprintf("  "); bbTable->nth(i)->print_short(); lprintf("\n");
    }
  }

  void BBIterator::localCopyPropagate() {
    fint i;
    for ( i = 0; i < bbCount; i++) { bbTable->nth(i)->localCopyPropagate(); }
  }

  void BBIterator::computeExposedBlocks() {
    // escape analysis for blocks: compute initial approximation (lower bound)
    exposedBlks = new BlockPRegBList(BlockPReg::numBlocks);
    fint i;
    for ( i = 0; i < bbCount; i++) {
      bbTable->nth(i)->computeExposedBlocks(exposedBlks);
    }
  }

  static void addUplevel(PReg* exposer, PReg* exposed, bool isWritten) {
    // exposed is uplevel-accessed by exposer
    Unused(exposer);
    if (exposed->isBlockPReg()) {
      BlockPReg* block = (BlockPReg*)exposed;
      BlockPRegBList* l = theSIC->bbIterator->exposedBlks;
      if (!block->escapes) {
        assert(!l->contains(block), "should not be in list");
        l->append(block);
        block->markEscaped();
      }
    }
    exposed->uplevelR = true;
    if (isWritten)
      exposed->uplevelW = true;
    
    theSIC->cope_with_uplevel_access_to(exposed);
  }
    
  void BBIterator::computeUplevelAccesses() {
    // mark uplevel-accessed variables and compute closure of exposedBlks
    // NOTE: exposedBlks may grow during the iteration because blocks that
    // are uplevel-accessed by blocks in the initial exposedBlks list are
    // added to the list until closure is reached
    fint i;
    for ( i = 0; i < exposedBlks->length(); i++) {
      BlockPReg* r = exposedBlks->nth(i);
      r->markEscaped();
      assert (r->parent() == r->scope, "block preg was copied/changed scope");
      assert(r->scope->isCodeScope(), "must be code scope");
      r->nscope()->doUplevelAccesses(r, addUplevel);
    }
  }

  void BBIterator::gen() {
    fint i;
    for (i = 0; i < bbCount; i++) {
      bbTable->nth(i)->gen();
      fint next;
      for (next = i + 1;            // find next nonempty BB
           next < bbCount && bbTable->nth(next)->nnodes == 0;
           next++) ;
      if (     (next == bbCount 
           &&  bbTable->nth(i)->next()) 
      ||       (next < bbCount 
           &&  bbTable->nth(i)->next() 
           &&  bbTable->nth(i)->next() != bbTable->nth(next) 
           && !bbTable->nth(i)->last->isRestartNode()) /* zzz or branch? */ ) {
        // non-sequential control flow - insert a branch
        Node* n = bbTable->nth(i)->next()->first;
        n->genBranch();
      }
    }
  }

  void BBIterator::print_code(bool suppressTrivial) {
    fint i;
    for ( i = 0; i < bbCount; i++) {
      bbTable->nth(i)->print_code(suppressTrivial);
      int32 next = i + 1;
      if ( ( next == bbCount && bbTable->nth(i)->next() )  ||
           ( next <  bbCount && bbTable->nth(i)->next() &&
                                bbTable->nth(i)->next() != bbTable->nth(next) &&
                               !bbTable->nth(i)->last->isRestartNode())) {
        // non-sequential control flow - insert a branch
        Node* n = bbTable->nth(i)->next()->first;
        lprintf("\tgoto N%ld\n", (void*)n->id());
      }
      if (!suppressTrivial) lprintf("\n");
    }
  }

  void BBIterator::print_vcg_code(FILE* f, bool suppressTrivial) {
    fint i;
    for (i = 0; i < bbCount; i++) {
      bbTable->nth(i)->print_vcg_nodes(f, suppressTrivial);
    }
    for (i = 0; i < bbCount; i++) {
      bbTable->nth(i)->print_vcg_edges(f, suppressTrivial);
      int32 next = i + 1;
      if ( ( next == bbCount && bbTable->nth(i)->next() )   ||
           ( next <  bbCount && bbTable->nth(i)->next() &&
                                bbTable->nth(i)->next() != bbTable->nth(next) &&
                               !bbTable->nth(i)->last->isRestartNode())) {
        // non-sequential control flow - insert a branch
        Node* n = bbTable->nth(i)->next()->first;
      }
    }
  }

  void BBIterator::eliminateUnreachableNodes() {
    // eliminateUptoMerge stops at a merge with an extra
    //  predecessor. But if that is an unreachable node, it
    //  is stopping too soon. While it might be best if the SIC
    //  never generated such unreachable paths, it does do it and
    //  it is so hard to eliminate that I am adding a pass to do it
    //  post hoc. -- dmu
    fint i;
    for (i = 1;  i < bbCount;  i++) {
      bbTable->nth(i)->eliminateUnreachableNodes();
    }
  }

  void BBIterator::globalCopyPropagate() {

    // do global copy propagation of singly-assigned PRegs

    fint i;
    for ( i = 0; i < pregTable->length(); i++) {
      PReg* r = pregTable->nth(i);
      if (r)
        r->attemptGlobalCopyPropagate();
    }
  }

  void BBIterator::eliminateUnneededResults() {
    // eliminate nodes producing results that are never used
    // (except for preallocated regs...their uses aren't completely right now)
    fint i;
    for ( i = 0; i < pregTable->length(); i++) {
      PReg* r = pregTable->nth(i);
      if (r && r->hardUses() == 0) {
        r->eliminatePR( false);
      }
    }
  }

  void BBIterator::computeMasks(fint stackLocs, fint nonRegisterArgs) {
    // mark bits for SAPRegs that are live at sends (i.e. expr stack temps)
    fint i;
    for ( i = 0; i < pregTable->length(); i++) {
      PReg* r = pregTable->nth(i);
      if (r && r->isSAPReg() && r->loc != UnAllocated && !isTrashedReg(r->loc))
        r->scope->computeMaskFor((SAPReg*)r, stackLocs, nonRegisterArgs);
    }
  }

  void BBIterator::computeDominators() {
    // Dragon book p. 671
    dominators = new BitVector*[bbCount];
    fint i;
    for (i = 0; i < bbCount; i++) {
      dominators[i] = new BitVector(bbCount);
      dominators[i]->addFromTo(0, bbCount - 1);
    }
    dominators[0]->clear();
    dominators[0]->add(0);

    bool changed = true;
    // since BBs are topologically sorted, one iteration (+ one to check)
    // is usually enough
    while (changed) {
      changed = false;
      for (i = 1; i < bbCount; i++) {
        BB* bb = bbTable->nth(i);
        BitVector* dom = dominators[i];
        dom->remove(i); // so changed doesn't give false alerts
        fint j;
        for ( j = bb->nPredecessors() - 1; j >= 0; j--) {
          changed |= dom->intersectWith(dominators[bb->prev(j)->id()]);
        }
        dom->add(i);
      }
    }
    if (PrintSICDominators) printDominators();
  }

  void BBIterator::printDominators() {
    fint i;
    for ( i = 0; i < bbCount; i++) {
      lprintf("BB%d: ", (void*)i); dominators[i]->print(); lprintf("\n");
    }
  }

  static Node** ott_tt;
  static fint ott_dominated;

  static void optimTT(fint index) {
    // the type test ott_tt[iindex] dominates ott_tt[ott_dominated]; see if the
    // dominated node can be eliminated
    Node* dominator = ott_tt[index];
    Node* dominated = ott_tt[ott_dominated];
    if (dominated == dominator || dominator == NULL) return;

    if (!dominator->isFailureUncommon()) return;

    fint i;
    for ( i = dominator->numTested() - 1; i >= 0; i--) {
      PReg* r = dominator->testedReg(i);
      if (r->isSinglyAssigned()) {
        dominated->simplify(r, dominator->testedType(i));
      }
    }
  }

  void BBIterator::optimizeTypeTests() {
    // a typetest node can be eliminated if it is dominated by another
    // type test for the same value, unless the value could have been
    // assigned between the two nodes

    // not fully debugged yet!  don't use  -Urs 8/94
    FlagSetting fs1(PrintSICTypeTestOpt, true);
    FlagSetting fs2(PrintSICEliminateUnneededNodes, true);
    SICDebug=WizardMode= true;

    Node* tt[bbCount];  // maps BB --> type test in that BB
    ott_tt = tt;
    fint i;
    for (i = 0; i < bbCount; i++) {
      BB* bb = bbTable->nth(i);
      if (bb->last->isTypeTestLike()) {
        tt[i] = bb->last;
      } else {
        tt[i] = NULL;
      }
    }

    // forall type tests
    for (i = 0; i < bbCount; i++) {
      if (tt[i]) {
        ott_dominated = i;
        // forall type tests dominating tt[i]
        dominators[i]->doForAllOnes(optimTT);
      }
    }
  }
    
  void BBIterator::apply(BBDoFn f) {
    for (int32 i = 0; i < bbCount; i++) { f(bbTable->nth(i)); }
  }

  void BBIterator::verify(bool theWoiks) {
    if (!bbTable)
      return;
    if (theWoiks) {
      int32 i;
      for (i = 0; i < bbCount; i++) { bbTable->nth(i)->verify(); }
      if (pregTable) {
        for (i = 0; i < pregTable->length(); i++) {
          if (pregTable->nth(i)) pregTable->nth(i)->verify();
        }
      }
    }
    checkTail();
    checkReturnSomewhere();
  }
  
  void BBIterator::checkTail() {
    // check for code at end that falls into space
    BB* bb = bbTable->last();
    Node* n = bb->last;
    if (n->isExitNode()  ||  n->isRestartNode())
      return;
    if (n->next())
      return;
    fatal("last node has no successor");
  }
  
  void BBIterator::checkReturnSomewhere() {
    // ensure that a return exists somwhere
    // check for code at end that falls into space
    fint i;
    for (i = 0; i < bbCount; i++) {
      BB* bb = bbTable->nth(i);
      for (Node* n = bb->first; n;  n = n->next()) {
        if ( n->deleted  ) {
  }
  else if ( (n->isExitNode()  &&  !n->isDeadEndNode())
             ||   n->isRestartNode())
          return; // found return!
        if (n == bb->last)
          break;
      }
    }
    fatal("no live real return node");
  }
    
    

# endif
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "inlining.hh"
# include "_inlining.cpp.incl"

# ifdef SIC_COMPILER

/* JAD - 4/15/92.  Checks to see if the given selector is one of a list
   which is recognized as both common and cheap.  This is kind of hack.   */

  bool isCheapMessage(stringOop selector) {
    return
        selector == VMString[OR] || selector == VMString[AND] ||
        selector == VMString[XOR] ||
        selector == VMString[IF_TRUE_] ||
        selector == VMString[IF_FALSE_] ||
        selector == VMString[IF_TRUE_FALSE_] ||
        selector == VMString[IF_FALSE_TRUE_] ||
        selector == VMString[NOT] ||
        selector == VMString[PLUS] || selector == VMString[MINUS] ||
        selector == VMString[TIMES] || selector == VMString[DIVIDE] ||
        selector == VMString[PERCENT] ||
        selector == VMString[EQUAL] ||
        selector == VMString[NOT_EQUAL] ||
        selector == VMString[LESS_THAN] ||
        selector == VMString[LESS_EQUAL] || 
        selector == VMString[GREATER_EQUAL] ||
        selector == VMString[GREATER_THAN] ||
        selector == VMString[COMPARE_LESS_EQUAL_GREATER_] ||
        selector == VMString[TO_DO_] || 
        selector == VMString[TO_BY_DO_] || 
        selector == VMString[UP_TO_DO_] || 
        selector == VMString[UP_TO_BY_DO_] || 
        selector == VMString[DOWN_TO_DO_] || 
        selector == VMString[DOWN_TO_BY_DO_] || 
        selector == VMString[SUCCESSOR] ||
        selector == VMString[SUCC] ||
        selector == VMString[PREDECESSOR] ||
        selector == VMString[PRED] ||
        selector == VMString[NEGATE] ||
        selector == VMString[COMPLEMENT] ||
        selector == VMString[INVERSE] ||
        selector == VMString[AS_FLOAT] ||
        selector == VMString[ASSMALLINTEGER] ||
        selector == VMString[AT_] ||
        selector == VMString[AT_PUT_] ||
        selector == VMString[SIZE];
  }

  bool isCheapMessage(stringOop selector, mapOop map) {
    // same as above, but checks receiver map
            if (
                selector == VMString[OR] || selector == VMString[AND] ||
                selector == VMString[XOR] ||
                selector == VMString[IF_TRUE_] ||
                selector == VMString[IF_FALSE_] ||
                selector == VMString[IF_TRUE_FALSE_] ||
                selector == VMString[IF_FALSE_TRUE_] ||
                selector == VMString[NOT]) {
      return map == Memory->true_mapOop() || map == Memory->false_mapOop();
    } else if (
               selector == VMString[PLUS] || selector == VMString[MINUS] ||
               selector == VMString[TIMES] || selector == VMString[DIVIDE] ||
               selector == VMString[PERCENT] ||
               selector == VMString[EQUAL] ||
               selector == VMString[NOT_EQUAL] ||
               selector == VMString[LESS_THAN] ||
               selector == VMString[LESS_EQUAL] || 
               selector == VMString[GREATER_EQUAL] ||
               selector == VMString[GREATER_THAN] ||
               selector == VMString[COMPARE_LESS_EQUAL_GREATER_] ||
               selector == VMString[TO_DO_] || 
               selector == VMString[TO_BY_DO_] || 
               selector == VMString[UP_TO_DO_] || 
               selector == VMString[UP_TO_BY_DO_] || 
               selector == VMString[DOWN_TO_DO_] || 
               selector == VMString[DOWN_TO_BY_DO_] || 
               selector == VMString[SUCCESSOR] ||
               selector == VMString[SUCC] ||
               selector == VMString[PREDECESSOR] ||
               selector == VMString[PRED] ||
               selector == VMString[NEGATE] ||
               selector == VMString[COMPLEMENT] ||
               selector == VMString[INVERSE] ||
               selector == VMString[AS_FLOAT] ||
               selector == VMString[ASSMALLINTEGER]) {
      Map* m = map->map_addr();
      return m == Memory->smi_map;
    } else if (
               selector == VMString[AT_] ||
               selector == VMString[AT_PUT_] ||
               selector == VMString[SIZE]) {
      Map* m = map->map_addr();
      return m->is_objVector() || m->is_byteVector();
    } else {
      return false;
    }
  }

  bool isReallyCheapMessage(stringOop selector, mapOop map) {
    // a stripped-down version of isCheapMessage that is safer (i.e., checks
    // the map as well as the selector)
    // it is used to figure out whether to allow further inlining even if
    // total size limit for nmethod has been reached
           if (
               selector == VMString[OR] || selector == VMString[AND] ||
               selector == VMString[XOR] ||
               selector == VMString[IF_TRUE_] ||
               selector == VMString[IF_FALSE_] ||
               selector == VMString[IF_TRUE_FALSE_] ||
               selector == VMString[IF_FALSE_TRUE_] ||
               selector == VMString[NOT]) {
      return map == Memory->true_mapOop() || map == Memory->false_mapOop();
    } else if (
               selector == VMString[PLUS] || selector == VMString[MINUS] ||
               selector == VMString[TIMES] || selector == VMString[DIVIDE] ||
               selector == VMString[PERCENT] ||
               selector == VMString[EQUAL] ||
               selector == VMString[NOT_EQUAL] ||
               selector == VMString[LESS_THAN] ||
               selector == VMString[LESS_EQUAL] || 
               selector == VMString[GREATER_EQUAL] ||
               selector == VMString[GREATER_THAN] ||
               selector == VMString[COMPARE_LESS_EQUAL_GREATER_] ||
               
               selector == VMString[SUCCESSOR] ||
               selector == VMString[SUCC] ||
               selector == VMString[PREDECESSOR] ||
               selector == VMString[PRED] ||
               selector == VMString[NEGATE] ||
               selector == VMString[COMPLEMENT] ||
               selector == VMString[INVERSE] ||
               selector == VMString[ASSMALLINTEGER]) {
      Map* m = map->map_addr();
      return m == Memory->smi_map;
    } else if (
               selector == VMString[AT_] ||
               selector == VMString[AT_PUT_] ||
               selector == VMString[SIZE]) {
      Map* m = map->map_addr();
      return m->is_objVector() || m->is_byteVector();
    } else {
      return false;
    }
  }

  // If these methods are NIC_compiled, the invocation counts of the subsequent
  // "value" sends make it harder to recognize what to recompile.  Essentially,
  // the problem is that the top few block methods below loop belong to the
  // control structure (whileTrue), and part of them to the actual user code.
  // Inlining these blocks away eliminates the distortion (and makes loops go
  // somewhat faster).
  // Strictly speaking, only whileTrue is critical; the rest are here for
  // better speed / less block cloning.
  bool isCriticalMessage(oop selector) {
    return
        selector == VMString[IF_TRUE_] ||
        selector == VMString[IF_FALSE_] ||
        selector == VMString[WHILETRUE_] ||
        selector == VMString[WHILEFALSE_] ||
        selector == VMString[UNTILTRUE_] ||
        selector == VMString[UNTILFALSE_] ||
        selector == VMString[TO_BY_DO_] ||
        selector == VMString[TO_BYPOSITIVE_DO_] ||
        selector == VMString[TO_BYNEGATIVE_DO_] ||
        selector == VMString[UP_TO_DO_] ||
        selector == VMString[UP_TO_BY_DO_] ||
        selector == VMString[DOWN_TO_DO_] ||
        selector == VMString[DOWN_TO_BY_DO_];
  }

  bool isSmalltalkInlined(oop selector) {
    // for comparison with PP ST; everything *whithin* these messages is
    // also inlined if possible
    return
        selector == VMString[IF_TRUE_] ||
        selector == VMString[IF_FALSE_] ||
        selector == VMString[IF_TRUE_FALSE_] ||
        selector == VMString[IF_FALSE_TRUE_] ||
        selector == VMString[WHILETRUE_] ||
        selector == VMString[WHILEFALSE_] ||
        selector == VMString[TO_BY_DO_] ||
        selector == VMString[PLUS] || selector == VMString[MINUS];
  }

  static MapBList* methodStack;

  static bool checkLocalSlot(stringOop sel, checkLocalSendFn isLocal) {
    assert(!UseLocalAccessBytecodes, "relic of the past");
    for (fint i = methodStack->length() - 1; i >= 0; i--) {
      if (methodStack->nth(i)->find_slot(sel) != NULL) return true;
    }
    return isLocal(sel);
  }
  
  
  class methodAppraiser: public abstract_interpreter {
   public:
   
    checkLocalSendFn isLocal;
    CostParam*       cp;
    fint             cutoff;
    BoolBList        penaltyStack;  // to keep track of block args
    fint                                           cost;
    fint                   failLen;
   
    methodAppraiser( oop meth,
                     checkLocalSendFn isLoc,
                     CostParam* cp_arg,
                     fint cutoff_arg);
                     
    void interpret_method();
                     
    void do_SELF_CODE() { penaltyStack.push(false);  failLen= 0; }
    void do_POP_CODE()  { penaltyStack.pop();        failLen= 0; }
    void do_DELEGATEE_CODE() { failLen= 0; }
    void do_UNDIRECTED_RESEND_CODE() { failLen= 0; }
    void do_read_write_local_code(bool isWrite);
    void do_send_code( bool isSelfImplicit, stringOop selector, fint arg_count);
    void do_literal_code( oop lit);
    
    void do_BRANCH_CODE()          { failLen= 0; }
    void do_BRANCH_TRUE_CODE()     { penaltyStack.pop();  failLen= 0; }
    void do_BRANCH_FALSE_CODE()    { penaltyStack.pop();  failLen= 0; }
    void do_BRANCH_INDEXED_CODE()  { penaltyStack.pop();  failLen= 0; }

    
    void check_penalty_stack();      
  };


  methodAppraiser::methodAppraiser( oop meth,
                                    checkLocalSendFn isLoc,
                                    CostParam* cp_arg,
                                    fint cutoff_arg) 
  : abstract_interpreter(meth), 
    penaltyStack( mi.length_codes) {
    isLocal= isLoc;
    cp= cp_arg;
    cutoff= cutoff_arg;
    cost= failLen= 0;
  }
  
  
  void methodAppraiser::interpret_method() {
    methodStack->push(mi.map());
    abstract_interpreter::interpret_method();
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        check_penalty_stack();
      }
#   endif
    methodStack->pop();    
  }
  
  
  void methodAppraiser::do_read_write_local_code( bool isWrite) {
    if (isWrite)
      penaltyStack.pop();
      
    cost += cp->localSendCost;
    failLen= 0;
    penaltyStack.push(false);
  }
  
  
  void methodAppraiser::do_literal_code( oop lit ) {
    bool isBlk= lit->is_block_with_code();
    penaltyStack.push(isBlk);
    if (isBlk) {
      failLen= methodCost( blockOop(lit)->value(), isLocal, cp, cutoff);
      cost += failLen;
    }
  }
  
  
  void methodAppraiser::do_send_code( bool isSelfImplicit, stringOop sel, fint argc) {
                if (!isSelfImplicit) ++argc; // Explicit receiver
                bool penaltyFlag = false;
                for (fint argnum = 0; argnum < argc; argnum++) {
                  penaltyFlag |= penaltyStack.pop();
                }
                if (sel->is_prim_name()) {
                  // a primitive; don't count the previous fail block in total length
                  cost -= failLen;
                  cost += cp->primCallCost;
                } 
                else if (isCheapMessage(sel)) {
                  cost += cp->cheapSendCost;
                } 
                else if (!UseLocalAccessBytecodes
                       &&  isSelfImplicit
                       &&  !( is.is_undirected_resend || is.delegatee )
                       &&  checkLocalSlot(sel, isLocal)) {
      // a local slot access
                  cost += cp->localSendCost;
                } else {
                  cost += isSelfImplicit ? cp->selfSendCost : cp->unknownSendCost;
                  if (penaltyFlag)  cost += cp->blockArgPenalty;
                }
                failLen = 0;
                penaltyStack.push(false);  // penalties already taken into account
  }
    
    
  void methodAppraiser::check_penalty_stack() {
    assert(penaltyStack.length() == 1, 
           "Penalty stack should have only last expr.");
  }
  
  
  // remove isLocal argument when UseLocalAccessBytecodes is always true

  fint methodCost(oop meth, checkLocalSendFn isLocal,
                  CostParam* cp, fint cutoff) {
    methodAppraiser ma(meth, isLocal, cp, cutoff);
    ma.interpret_method();
    return ma.cost;
  }

  CostParam* costP;
  CostParam* failCostP;

  static SCodeScope* checkLocal_scope; // remove when UseLocalAccessBytecodes is always true
  static bool checkLocal(stringOop sel) {
    assert(!UseLocalAccessBytecodes, "relic of the past");
    slotDesc* sd;
    return checkLocal_scope->lookup(sel, sd) != NULL;
  }

  fint sicCost(oop meth, SCodeScope* s, CostParam* cp) {
    checkLocal_scope = s;
    return methodCost(meth, checkLocal, cp, 9999);
  }
    

  void inlining_init() {
    methodStack = new MapBList(100, true);
      costP     = new CostParam(0, 1, 1, 1, 2, 1);            // normal sends
      failCostP = new CostParam(1, 2, 2, 4, 2, 2);            // prim failure blks
  }

# else // SIC_COMPILER
 void inlining_init() {}
# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "longRegString.hh"

# include "_longRegString.cpp.incl"


# ifdef SIC_COMPILER

  inline fint tempToIndex(Location temp) { return index_for_StackLocation(temp) + 32; }
  inline Location indexToTemp(fint temp) {
    return Location(StackLocation_for_index(temp - 32));
  }

  LongRegisterString::LongRegisterString() {
    bv = new BitVector(128);
  }

  void LongRegisterString::doAllocate(Location l) {
    if (isRegister(l)) {
      bv->add(l);
    } else {
      assert(isStackRegister(l), "should be stack reg");
      fint i = tempToIndex(l);
      if (i >= bv->length) grow(i);
      bv->add(i);
    }
  }

# ifdef UNUSED
  void LongRegisterString::deallocate(Location l) {
    if (isRegister(l)) {
      bv->remove(l);
    } else {
      assert(isStackRegister(l), "should be stack reg");
      fint i = tempToIndex(l);
      bv->remove(i);
    }
  }
# endif
    
  bool LongRegisterString::isAllocated(Location l) {
    if (isRegister(l)) {
      return bv->includes(l);
    } else {
      assert(isStackRegister(l), "should be stack reg");
      fint i = tempToIndex(l);
      if (i < bv->length) {
        return bv->includes(i);
      } else {
        return false;
      }
    }
  }

  RegisterString LongRegisterString::regs() {
    return bv->bits[0];
  }

  void LongRegisterString::grow(fint desiredIndex) {
    if (desiredIndex < bv->maxLength) {
      bv->setLength(desiredIndex + 1);
      return;
    }
    fint newMaxLength = bv->maxLength * 2;
    while (desiredIndex >= newMaxLength)  newMaxLength *= 2;
    bv = bv->copy(newMaxLength);
    bv->setLength(desiredIndex + 1);
  }

  void LongRegisterString::print() {
  }


  // find the first bit >= start that is unused in all strings[0..len-1]
  fint findFirstUnused(LongRegisterString** strings, fint len,
                       fint start) {
    // currently quite unoptimized
    BitVector* b = strings[0]->bv->copy(strings[0]->bv->maxLength);
    fint i;
    for (i = 1; i < len; i++) {
      b->unionWith(strings[i]->bv);
    }
    for (i = start; i < b->length; i++) {
      if (!b->includes(i)) break;
    }
    return i;
  }

  Location findFirstUnusedTemp(LongRegisterString** strings, fint len) {
    fint i = findFirstUnused(strings, len, tempToIndex(StackLocation_for_index(0)));
    return indexToTemp(i);
  }

# endif // SIC_COMPILER
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "sicInline.hh"

# include "_sicInline.cpp.incl"

  static fint msgCost = 0;      // estimated cost of last inlining candidate
  
  SExpr* SCodeScope::inlineSend(SendInfo* info) {
    stringOop sel = info->sel;
    SExpr* res = NULL;
    info->resReg = new SAPReg(this);
    MergeNode* merge = NULL;
    fint argc = sel->arg_count();

    if (!Inline && !InlineSTMessages) {
      // don't do any inlining
      info->needRealSend = true;
    } else {
      info->rcvr = picPredict(info);
      UnknownSExpr* u = info->rcvr->findUnknown();
      if (u && !u->isUnlikely()) {
        info->rcvr = typePredict(info);
      }
      
      if (info->rcvr->really_hasMap(this)) {
        // single map - try to inline this send
        SSelfScope* s = tryLookup(info, info->rcvr);
        if (s) {
          SExpr* r = doInline(s, info->rcvr, theNodeGen->current, NULL);
          if (r->isNoResultSExpr()) {
            res = r;
          } else {
            theNodeGen->append(new NopNode());   // to get right scope for r
            res = r->shallowCopy(r->preg(), theNodeGen->current);
          }
        } else {
          if (PrintInlining) {
            lprintf("%*s*marking %s send ReceiverStatic\n",
                    (void*)depth,"", selector_string(sel));
          }
          // receiver type is constant (but e.g. method was too big to inline)
          info->l |= ReceiverStaticBit;
          info->needRealSend = true;
        }
      } else if (info->rcvr->isMergeSExpr()) {
        res = inlineMerge(info, merge);
      } else {
        // unknown receiver
        // NB: *must* use uncommon branch if marked unlikely because
        // future type tests won't test for unknown
        if (info->rcvr->findUnknown()->isUnlikely()) {
          // generate an uncommon branch for the unknown case, not a send
          theNodeGen->current = theNodeGen->uncommonBranch(currentExprStack(0),
                                                           info->restartPrim);
          info->needRealSend = false;
          if (PrintInlining) {
            lprintf("%*s*making %s uncommon\n", (void*)depth,"",selector_string(sel));
          }
        } else {
          info->needRealSend = true;
        }
      }
    }
    
    if (info->needRealSend) {
      SExpr* r = genRealSend(info);
      res = res ? res->mergeWith(r, merge) : r;
    }
    if (merge && res && !res->isNoResultSExpr()) theNodeGen->branch(merge);

    // now pop expr stack
    for (fint i = 0; i < argc; i++) exprStack->pop();
    if (!info->isSelfImplicit) exprStack->pop();
    if (!res) res = new NoResultSExpr;
    return res;
  }
  
  
  SExpr* SCodeScope::inlineMerge(SendInfo* info, MergeNode*& merge) {
    // inline the send by type-casing; return uninlined cases in others list
    // If merge has no predecessors, return NULL for merge ref.
    SExpr* res = NULL;
    assert(info->rcvr->isMergeSExpr(), "must be a merge");
    MergeSExpr* r = (MergeSExpr*)info->rcvr;
    stringOop sel = info->sel;
    merge = NULL;

    if (r->isSplittable() && shouldSplit(info)) {
      return splitMerge(info, merge);
    }
                    
    fint ncases = r->exprs->length();
    if (ncases > SICTypeCaseLimit) {
      info->needRealSend = true;
      if (PrintInlining) {
        lprintf("%*s*not type-casing %s (%ld > SICTypeCaseLimit)\n",
                (void*)depth, "", selector_string(sel), (void*)ncases);
      }
      return res;
    }
    assert( merge == NULL, "I assume merge is out param only");
    merge = new MergeNode("inlineMerge merge");

    if (SICDebug) {
      char* s = NEW_RESOURCE_ARRAY(char, 200);
      sprintf(s, "begin type-case of %s (ends at node N%ld)",
              sel->copy_null_terminated(), long(merge->id()));
      theNodeGen->comment(s);
    }
    if (PrintInlining) {
      lprintf("%*s*type-casing %s\n", (void*)depth, "", selector_string(sel));
    }

    // build list of cases to inline
    // (add only immediate maps at first, collect others in ...2 lists
    SSelfScopeBList* slist  = new SSelfScopeBList(ncases);
    SSelfScopeBList* slist2 = new SSelfScopeBList(ncases);
    SExprBList* elist  = new SExprBList(ncases);
    SExprBList* elist2 = new SExprBList(ncases);
    SExprBList* others = new SExprBList(ncases);
    OopBList* mlist  = new OopBList(ncases);
    OopBList* mlist2 = new OopBList(ncases);
    bool needMapLoad = false;
    fint i;
    for (i = 0; i < ncases; i++) {    
      SExpr* nth = r->exprs->nth(i);
      assert(!nth->isConstantSExpr() || nth->next == NULL ||
             nth->constant() == nth->next->constant(),
             "shouldn't happen: merged consts - convert to map");
      SSelfScope* s;

      if (!nth->hasMap()  ||  (s = tryLookup(info, nth)) == NULL) {
        // cannot inline
        others->append(nth);
        info->needRealSend = true;
        continue;
      }
      // can inline this case

      // Notice that for immediates, instead of putting the constants in the mlist,
      // we put the maps. No point in optimizing just for 17. -- dmu 6/05
      Map* map = nth->map();
      if (map == Memory->smi_map  ||  map == Memory->float_map) {
        slist ->append(s);        // immediate maps go first
        // Bug fix: instead of nth->shallowCopy, must generalize to any 
        // with same map, not just the same constant, because other ints (for example)
        // will pass the type test, too. -- dmu 6/05
        elist ->append(new MapSExpr(map->enclosing_mapOop(), r->preg(), NULL));
        mlist ->append(map->enclosing_mapOop());
        continue;
      }
      // can inline but not immediate map
      slist2->append(s);        // append later
      elist2->append(nth->shallowCopy(r->preg(), NULL)); // use preg of merge
      if (nth->isConstantSExpr()) {
        mlist2->append(nth->constant());
      }
      else {
        needMapLoad = true; // will need to load map of testee
        mlist2->append(map->enclosing_mapOop());
      }
    }

    mlist->appendList(mlist2);
    elist->appendList(elist2);
    slist->appendList(slist2);
        
    // now do the type test and inline the individual cases
    if (slist->length() > 0) {
      memoizeBlocks(sel);
      
      Node* typeCase =
        theNodeGen->append(new TypeTestNode(r->preg(), mlist, needMapLoad,
                                            info->needRealSend));
      Node* fallThrough = typeCase->append(new NopNode);
      for (i = 0; i < slist->length(); i++) {
        theNodeGen->current = typeCase->append(i + 1, new NopNode);
        SExpr* e = doInline(slist->nth(i), elist->nth(i), theNodeGen->current, merge);
        if (!e->isNoResultSExpr()) {
          theNodeGen->append(new NopNode);
          e = e->shallowCopy(info->resReg, theNodeGen->current);
          res = res ? res->mergeWith(e, merge) : e;
        }
        theNodeGen->branch(merge);
      }
      theNodeGen->current = fallThrough;
    }
    if (res && res->isMergeSExpr()) 
      res->setNode(merge, info->resReg);
      
    assert( info->needRealSend &&  others->length() ||
           !info->needRealSend && !others->length(), "inconsistent");
           
    // NB: *must* use uncommon branch if marked unlikely because
    // future type tests won't test for unknown
                      
    if (others->isEmpty()) {
      // typecase cannot fail
      theNodeGen->deadEnd();
    }
    else if ( others->length() == 1
         &&   others->first()->isUnknownSExpr()
         &&   ((UnknownSExpr*)others->first())->isUnlikely()) {
            // generate an uncommon branch for the unknown case, not a send
            theNodeGen->uncommonBranch(currentExprStack(0), info->restartPrim);
            info->needRealSend = false;
            if (PrintInlining)
              lprintf("%*s*making %s uncommon (2)\n", (void*)depth,"",selector_string(sel));
    }
    return res;
  }

  void SCodeScope::memoizeBlocks(stringOop sel) {
    // memoize block args so they aren't created for inlined cases
    fint top = exprStack->length();
    fint argc = sel->arg_count();
    for (fint i = 1; i <= argc; i++) {
      PReg* r = exprStack->nth(top - i)->preg();
      if (r->isBlockPReg()) ((BlockPReg*)r)->memoize();
    }
  }

  bool SMethodScope::isRecursiveCall(oop meth, oop rcvrMap, fint n) {
    if (meth == method()) {
      if (receiverMapOop() == rcvrMap) {
        if (n <= 1) return true; else n--;
      }
    }
    SScope* s = sender();
    return s && s->isRecursiveCall(meth, rcvrMap, n);
  }

  bool SBlockScope::isRecursiveCall(oop meth, oop rcvrMap, fint n) {
    // skip lexically nested frames -- otherwise, get false recursion
    // e.g. with nested ifTrue's
    return _parent->isRecursiveCall(meth, rcvrMap, n);
  }
  
  bool SVFrameMethodScope::isRecursiveCall(oop meth, oop rcvrMap, fint n) {
    // potential performance bug (bad interaction with block cloning)
    // fix this
    if (meth == method()) {
      if (receiverMapOop() == rcvrMap) {
        return true; 
      }
    }
    SScope* s = sender();
    return s && s->isRecursiveCall(meth, rcvrMap, n);
  }

  fint SSelfScope::calleeSize(RScope* rs) {
    // try to get the callee's size (in bytes)
    if (!rs->isPICScope()) return 0;    // no info
    RPICScope* ps = (RPICScope*)rs;
    nmethod* inlinee= ps->callee;
    if (inlinee->compiler() == NIC) {
      return 0;         // can't say much about the real code size
    }
    fint size = inlinee->instsLen() - oopSize * PrologueSize;
    if (inlinee->isUncommonRecompiled()) {
      // uncommon nmethods are bigger because the contain many more non-
      // inlined sends for all the uncomon cases
      size /= 2;
    }
    return size;
  }
  
  bool SCodeScope::calleeTooBig(SendInfo* info, RScope* rs,
                                InlineLimitType limitType) {
    // try to see if the potential inlinee is too big
    fint size = calleeSize(rs);
    // NB: continue even if size == 0 to bring current estimated size into play
    assert(limitType <= BlockFnLimit, "bad limit");
    limitType = InlineLimitType(limitType + NormalFnInstrLimit-NormalFnLimit);
    fint cutoff = theSIC->inlineLimit[limitType];
    fint estimated = theSIC->estimatedSize();
    fint limit = theSIC->inlineLimit[NmInstrLimit];
    // reject if inlinee too large, but correct for well-known cheap messages
    bool bad = size > cutoff &&
      !(info->rcvr->hasMap() &&
        isCheapMessage(info->sel, info->rcvr->myMapOop()));
    if (bad && estimated + size < limit / 2) {
      // allow inlining if recompilee itself is small (i.e., a forwarder)
      if (recompilee &&
          recompilee->instsLen() - oopSize * PrologueSize < cutoff) {
        bad = false;
      }
    }
    // also reject if est. total size too large (except for ifTrue et al)
    // but don't trust the isCheap thing if we're way over the limit
    bad = bad || estimated + size >= limit;
    if (bad &&
        estimated + size < 2 * limit &&
        (size == 0 || size < cutoff / 8) &&
        info->rcvr->hasMap() &&
        isReallyCheapMessage(info->sel, info->rcvr->myMapOop())) {
      bad = false;
    }
    if (bad && PrintInlining) {
      lprintf("%*s*not inlining %s: callee too big (%d/%d/%d/%d)\n", (void*)depth, "",
              selector_string(info->sel),
              (void*)size, (void*)cutoff, (void*)estimated, (void*)limit);
    }
    if (SICDebug && bad && estimated > limit)
      warning4("SIC: (while compiling %s/%s) estimated nmethod size > limit (%ld > %ld)",
               selector_string(theSIC->L->selector()),
               selector_string(info->sel), estimated, limit);
    return bad;
  }
  
  bool SCodeScope::calleeIsSmall(SendInfo* info, RScope* rs,
                                 InlineLimitType limitType) {
    // try to see if the potential inlinee is small
    fint size = calleeSize(rs);
    if (!size) return false;    // no size info
    assert(limitType <= BlockFnLimit, "bad limit");
    limitType = InlineLimitType(limitType + NormalFnInstrLimit-NormalFnLimit);
    fint cutoff = theSIC->inlineLimit[limitType];
    fint estimated = theSIC->estimatedSize();
    fint limit = theSIC->inlineLimit[NmInstrLimit];
    bool ok = size <= cutoff && estimated + size < limit;
    if (ok && PrintInlining) {
      lprintf("%*s*inlining %s anyway: callee is small (%d/%d/%d/%d)\n",
              (void*)depth, "", selector_string(info->sel),
              (void*)size, (void*)cutoff, (void*)estimated, (void*)limit);
    }
    return ok;
  }
  
  bool SCodeScope::shouldInlineSend(SendInfo* info, RScope* rs, SExpr* rcvr,
                                    oop m, InlineLimitType limitType) {
    MethodLookupKey* k = info->key;
    if (!k->selector->is_string()) return false;

    if (isRecursiveCall(m, k->receiverMapOop(), MaxRecursionUnroll)) {
      info->uninlinable = true;
      return false;
    }
    
    if (!Inline) {
      assert(InlineSTMessages, "shouldn't be here");
      // NB: works only with rewritten whileTrue/False (using _Restart)
      if (isSmalltalkInlined(k->selector)) return true;
      if (isSmalltalkInlined(selector())) return true;
      return false;
    }

    if (limitType == NormalFnLimit) {
      // check args to see if any of them is a block; if so, increase limit
      fint top = exprStack->length();
      fint argc = stringOop(k->selector)->arg_count();
      for (fint i = argc; i > 0; i--) {
        if (exprStack->nth(top - i)->preg()->isBlockPReg()) {
          limitType = BlockArgFnLimit;
          goto done;
        }
      }
      // check receiver
      if (lookupReceiverIsSelf(k->lookupType)) {
        if (self->preg()->isBlockPReg()) limitType = BlockArgFnLimit;
      } else if (exprStack->nth(top - argc - 1)->preg()->isBlockPReg()) {
        limitType = BlockArgFnLimit;
      }
    }
    
   done:
    if (calleeTooBig(info, rs, limitType)) {
      // register this send as uninlinable
      theSIC->registerUninlinable(info, limitType, 9999);
      return false;
    }

    // NB: this test comes after calleeTooBig to prevent forced inlining of
    // e.g. a really complicated user-defined '+' for matrices
    if (isCheapMessage(stringOop(k->selector))) {
      msgCost = costP->cheapSendCost;
      return true;
    }

    fint cutoff = theSIC->inlineLimit[limitType];
    msgCost = sicCost( m, this, costP);
    if (info->primFailure &&
        info->nsends < MinPrimFailureInvocations) {
      if (rs->isPICScope() && ((RPICScope*)rs)->sd->isOptimized()) {
        // the fail block send is optimized, it's probably executed frequently
      } else if (rs->isSelfScope()) {
        // was inlined in previous version, so do it again
      } else {
        // don't inline error block unless trivial or taken often
        if (msgCost > MaxTrivialPrimFailureCost) return false;
        // should also look at block method, not default value:With:
        Map* map = rcvr->map();
        if (map->is_block()) {
          slotsOop method = ((blockMap*)map)->value();
          msgCost = sicCost( method, this, failCostP);
          // bug: should estimate real length of prim failure; e.g. could
          // have single send (cost 1) but that method sends more and more
          // msgs...i.e. need concept of "being in fail branch" so that
          // no further inlining takes place -- fix this
          if (msgCost > MaxTrivialPrimFailureCost) return false;
        }
      }
    }
    if (msgCost > cutoff) {
      if (calleeIsSmall(info, rs, limitType)) return true;
      theSIC->registerUninlinable(info, limitType, msgCost);
      return false;
    }
    return true;
  }

  bool SCodeScope::shouldInlineBlock(SendInfo* info, RScope* rs,
                                     SExpr* rcvr, oop method) {
    return shouldInlineSend(info, rs, rcvr, method, BlockFnLimit);
  }

  bool SCodeScope::shouldInlineMethod(SendInfo* info, RScope* rs,
                                      SExpr* rcvr, oop meth) {
    return shouldInlineSend(info, rs, rcvr, meth, NormalFnLimit);
  }
  
  SSelfScope* SCodeScope::notify(stringOop selector, const char* msg) {
    if (PrintInlining) {
      lprintf("%*s*cannot inline %s, cost = %ld (%s)\n", (void*)depth, "",
              selector_string(selector), (void*)msgCost, msg);
    }
    return NULL;        // cheap trick to make calls more convenient
  }

  SSelfScope* SCodeScope::tryLookup(SendInfo* info, SExpr* rcvr) {
    // info->rcvr is the overall receiver (e.g. a merge expr), rcvr is
    // a particular branch
    // nmln* depsTop = theSIC->L->deps->top;
    assert(rcvr->hasMap(), "should have a map");
    
    SICLookup* L = new SICLookup(info->l,
                                 rcvr->isConstantSExpr()
                                   ? rcvr->asConstantSExpr()->constant()
                                   : rcvr->map()->enclosing_mapOop(),
                                 info->sel,
                                 info->del,
                                 theSIC->L->deps,
                                 this );

    L->perform_lookup();
    info->key= new_MethodLookupKey(L->key);
    
    if (L->result() == NULL) {
      // nothing found statically
      // should remove unused deps here
      if (L->status == foundNone) {
        // next recompilation might get it via PICs
        theSIC->registerUninlinable(info, NormalFnLimit, 0);
      } else {
        info->uninlinable = true;
      }
      char msg[80];
      sprintf(msg, "lookup failed: %s", lookupStatusString(L->status));
      return notify(info->sel, msg);
    }

    slotDesc* s = L->result()->as_real()->desc;
    assert(! s->is_vm_slot(), "shouldn't be a vm slot");
    RScope* rs = rscope->subScope(_bci, L);
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      bool isNullRScope = rs->isNullScope(); // for breakpoints
    }
#   endif
    
    if (s->is_map_slot()) {
      // handle constant slots first
      
      if ( !s->data->has_code()) {
        // inline value of a constant slot
        msgCost = 0;
        return new SConstantSlotAccessScope(L, this, rs, rcvr, info);
      } 
      switch ( s->data->kind() ) {
       case BlockMethodType:
        return tryLookupOfBlockMethod(info, rcvr, rs, s->data, L);

       case OuterMethodType:
        return tryLookupOfOuterMethod(info, rcvr, rs, s->data, L);

       default:
         fatal("unexpected kind of code in slot");
      }
    } 

    assert(s->is_obj_slot(), "unexpected slot type");
    // is not map slot
    // found a slot in the receiver or one of its ancestors
    msgCost = 0;

    return
     info->sel->is_1arg_keyword()
       ?  // inline assignment to an instance variable
          (SSelfScope*) new SDataSlotAssignScope(L, this, rs, rcvr, exprStack->top(),
                                   info) 
       :  // inline access of an instance variable
          (SSelfScope*) new SDataSlotAccessScope(L, this, rs, rcvr, info);
  }


  SSelfScope* SCodeScope::tryLookupOfOuterMethod( SendInfo* info,
                                                  SExpr* rcvr,
                                                  RScope* rs, 
                                                  oop methodObj,
                                                  SICLookup* L) {
    oop mh = L->result()->as_real()->generalized_methodHolder_or_map(L->receiver);
    if (mh == MH_TBD) {
      // method holder is not known statically; cannot inline
      return notify(info->sel, "method holder unknown");
    }
    if ( !shouldInlineMethod(info, rs, rcvr, methodObj))
      return notify(info->sel, "rejected");

    // fix rs here
    return new SMethodScope(methodObj, mh, this, rs, info);
  }


  SSelfScope* SCodeScope::tryLookupOfBlockMethod( SendInfo* info, 
                                                  SExpr* rcvr, 
                                                  RScope* rs, 
                                                  oop methodObj,
                                                  SICLookup* L) {
    // inline block method (value, value: etc)
    if (!shouldInlineBlock(info, rs, rcvr, methodObj))
      return notify(info->sel, "rejected");

    SScope* parentScope;
    if (rcvr->isClonedBlockSExpr()) {
      // result of a BlockClone node
      parentScope = ((ClonedBlockSExpr*)rcvr)->blockScope();
      assert(!parentScope->isVFrameScope(), "shouldn't be a vf scope");
      assert(parentScope->isCodeScope(), "shouldn't be access");
    }
    else if ( L->result()->as_real()->holder->is_map() 
         &&   L->result()->as_real()->holder->map() == theSIC->L->receiverMap()) {
      // block is in same clone family as receiver block; therefore,
      // must have same parent scope as receiver block
      // example: value:With: sends value:
      assert_block(theSIC->L->receiver, "expecting a block literal");
      blockOop block = (blockOop) theSIC->L->receiver;
      compiled_vframe* vf = block->parentVFrame(NULL)->as_compiled();
      parentScope = new_SVFrameScope(vf);
    }
    else {
      // can't find nmethod for this block
      assert(! (L->result()->as_real()->holder->is_map() &&
                L->result()->as_real()->holder->map()->equal(
                    theSIC->L->receiverMap())),
             "block map cloning got us again");
      return notify(info->sel, "unknown block");
    }
    //    fix rs here
    return new SBlockScope(methodObj, parentScope, this, rs, info);
  }


  SExpr* SCodeScope::doInline(SSelfScope* s, SExpr* rcvr, Node* start,
                              MergeNode* end) {
    s->receiver = rcvr->asReceiver();
    if (PrintInlining) {
      lprintf("%*s*inlining %s, cost %ld/size %ld (%#lx)%s\n", (void*)depth, "",
              (s->isCodeScope() 
                ? sprintName((methodMap*)s->method()->map(), s->selector())
                : selector_string(s->selector())), // was: selector_string(s->selector()),
               (void*)msgCost, (void*)calleeSize(s->rscope),
              s, s->rscope->isNullScope() ? "" : "*");
    }
    Node* next = start->next();
    if (next) start->removeNext(next);
    theNodeGen->current = start;
    s->genCode();
    if (next && end) theNodeGen->branch(end);
    return s->result;
  }
    
  SExpr* SCodeScope::picPredictUnlikely(SendInfo* info,
                                        RUntakenScope* uscope) {
    if (theSIC->useUncommonTraps &&
        info->primFailure && uscope->isUnlikely()) {
      // this send was never executed in the recompilee
      // only make the send unlikely if it had a chance to execute
      // (If the send isn't a prim failure, don't trust the info --
      // it's unlikely that the method just stops executing in the middle.
      // What probably happened is that recompilation happens before the
      // rest of the method got a chance to execute (e.g. recursion), or it
      // always quit via NLR.  In any case, the compiler can't handle this
      // yet - need to treat it specially similar to endsDead.)
      info->nsends = 0;
      const fint MinSends = MinPrimFailureInvocations;
      fint count = uscope->caller->invocationCount();
      if (count <= 0) {
        // hack: optimized method has no count, so assume it's > MinSends
        count = MinSends + 1;
      }
      bool makeUncommon = count >= MinSends && uscope->sd->wasNeverExecuted();
      if (PrintInlining) {
        lprintf("%*s*%sPIC-type-predicting %s as never executed\n",
                (void*)depth, "", makeUncommon ? "" : "NOT ",
                info->sel->copy_null_terminated());
      }
      if (makeUncommon) {
        return new UnknownSExpr(info->rcvr->preg(), NULL, true);
      }
    }
    return info->rcvr;
  }

  SExpr* SCodeScope::picPredict(SendInfo* info) {
    // check PICs for information
    if (!UsePICRecompilation) return info->rcvr;
    bool canBeUnlikely = theSIC->useUncommonTraps;
    if (rscope->hasSubScopes(_bci)) {
      RScopeBList* l = rscope->subScopes(_bci);
      if (l->first()->isUntakenScope() && l->length() == 1) {
        return picPredictUnlikely(info, (RUntakenScope*)l->first());
      } else if (info->rcvr->containsUnknown()) {
        if (PrintInlining) {
          lprintf("%*s*PIC-type-predicting %s (%ld maps)\n", (void*)depth, "",
                  info->sel->copy_null_terminated(), (void*)l->length());
        }
        for (fint i = 0; i < l->length(); i++) {
          RScope* r = l->nth(i);
          SExpr* expr = r->receiverExpr();
          if (expr->isUnknownSExpr()) {
            // untaken real send (from PIC)
          } else if (expr->map()->is_block()) {
            // for now, doesn't make sense to predict block maps because of
            // map cloning
            if (PrintInlining) {
              lprintf("%*s*not predicting block map\n", (void*)depth, ""); 
            }
            canBeUnlikely = false;
          } else {
            SExpr* alreadyThere = info->rcvr->findMap(expr->myMapOop());
            if (alreadyThere) {
              // generalize to map if only have constant
              if (alreadyThere->isConstantSExpr())
                info->rcvr = info->rcvr->mergeWith(expr, NULL);
            } else {
              // add map only if type isn't already present (for splitting)
              info->predicted = true;
              info->rcvr = info->rcvr->mergeWith(expr, NULL);
              if (expr->hasConstant() && l->length() == 1) {
                // check to see if single predicted receiver is true or false;
                // if so, add other boolean to prediction.  Reduces the number
                // of uncommon branches; not doing so appears to be overly
                // aggressive (as observed experimentally)
                oop c = expr->constant();
                if (c == Memory->trueObj &&
                    !info->rcvr->findMap(Memory->false_mapOop())) {
                  SExpr* f = new ConstantSExpr(Memory->falseObj, NULL, NULL);
                  info->rcvr = info->rcvr->mergeWith(f, NULL);
                } else if (c == Memory->falseObj &&
                           !info->rcvr->findMap(Memory->true_mapOop())) {
                  SExpr* t = new ConstantSExpr(Memory->trueObj, NULL, NULL);
                  info->rcvr = info->rcvr->mergeWith(t, NULL);
                }
              }
            }
          }
        }
      } else {
        // know receiver type precisely
        return info->rcvr;
      }
      // mark unknown branch as unlikely
      UnknownSExpr* u = info->rcvr->findUnknown();
      if (u && canBeUnlikely && theSIC->useUncommonTraps && 
          rscope->isUncommonAt(_bci, false)) {
        info->rcvr = info->rcvr->makeUnknownUnlikely(this);
      }
    } else if (theSIC->useUncommonTraps &&
               info->primFailure &&
               rscope->isUncommonAt(_bci, true)) {
      // this is the failure send of a primitive, and the failure was made
      // uncommon in the recompilee, and it was never taken, so keep it
      // uncommon
      if (PrintInlining) {
        lprintf("%*s*PIC-type-predicting %s as never executed (2)\n",
                (void*)depth, "", info->sel->copy_null_terminated());
      }
      info->rcvr = new UnknownSExpr(info->rcvr->preg(), NULL, true);
    }
     
    assert(info->rcvr->preg(), "should have a preg");
    return info->rcvr;
  }

  SExpr* SCodeScope::typePredict(SendInfo* info) {
    // try static type prediction
    PReg* r = info->rcvr->preg();
    stringOop sel = info->sel;
    if (sel == VMString[IF_TRUE_] ||
        sel == VMString[IF_FALSE_] ||
        sel == VMString[IF_TRUE_FALSE_] ||
        sel == VMString[IF_FALSE_TRUE_] ||
        sel == VMString[OR]  || sel == VMString[AND] || sel == VMString[NOT]) {
      // boolean message
      if (PrintInlining) {
        lprintf("%*s*type-predicting %s\n", (void*)depth, "",
               sel->copy_null_terminated());
      }
      info->predicted = true;
      bool allowUnlikely = theSIC->useUncommonTraps;
      if (SICDeferUncommonBranches &&
          (sel == VMString[IF_TRUE_] ||
           sel == VMString[IF_FALSE_] ||
           sel == VMString[IF_TRUE_FALSE_] ||
           sel == VMString[IF_FALSE_TRUE_])) {
        // these bets are really safe - make uncommon even when recompiling
        // due to uncommon trap (if the ifTrue itself caused the trap,
        // rscope->isUncommonAt will be false, so this is safe)
        allowUnlikely = true;
      }
      if (allowUnlikely) {
        if (rscope->isUncommonAt(_bci, false)) {
          // ok, no uncommon trap here
        } else if (rscope->hasSubScopes(_bci)) {
          // has real send for ifTrue et al. -- must be NIC-compiled
          // make uncommon unlikely if no non-true/false receiver present
          RScopeBList* subs = rscope->subScopes(_bci);
          for (fint i = subs->length() - 1; i >= 0; i--) {
            RScope* s = subs->nth(i);
            SExpr* rcvr = s->receiverExpr();
            if (rcvr->hasMap()) {
              Map* m = rcvr->map();
              if (m != Memory-> true_map() &&
                  m != Memory->false_map()) {
                allowUnlikely = false;
                break;
              }
            }
          }
          if (WizardMode && !allowUnlikely)
            warning("SIC: non-bool receiver for ifTrue: et al. detected");
        }
        if (allowUnlikely) {
          // make unknown case unlikely despite 
          info->rcvr = info->rcvr->makeUnknownUnlikely(this);
        }
      }
      SExpr* rcvr = info->rcvr;
      SExpr* t = new ConstantSExpr(Memory->trueObj , r, NULL);
      SExpr* f = new ConstantSExpr(Memory->falseObj, r, NULL);
      // make sure we don't destroy splitting info; only add types if not
      // already present
      if (rcvr->findMap(Memory-> true_mapOop()) == NULL)
        rcvr = rcvr->mergeWith(t, NULL);
      if (rcvr->findMap(Memory->false_mapOop()) == NULL)
        rcvr = rcvr->mergeWith(f, NULL);
      return rcvr;
    }
    
    if (// binary integer arithmetic messages
        sel == VMString[PLUS] ||
        sel == VMString[MINUS] ||
        sel == VMString[PERCENT] ||
        sel == VMString[LESS_THAN] ||
        sel == VMString[LESS_EQUAL] || 
        sel == VMString[GREATER_EQUAL] ||
        sel == VMString[GREATER_THAN] ||
        // integer looping messages
        sel == VMString[TO_DO_] ||
        sel == VMString[UP_TO_DO_] ||
        sel == VMString[DOWN_TO_DO_] ||
        sel == VMString[TO_BY_DO_] ||
        sel == VMString[UP_TO_BY_DO_] ||
        sel == VMString[DOWN_TO_BY_DO_] ||
        // unary integer arithmetic selectors
        sel == VMString[SUCCESSOR] ||
        sel == VMString[SUCC] ||
        sel == VMString[PREDECESSOR] ||
        sel == VMString[PRED]) {
      if (info->rcvr->findMap(Memory->smi_map->enclosing_mapOop())) return info->rcvr;
      if (PrintInlining) {
        lprintf("%*s*type-predicting %s\n", (void*)depth, "",
                sel->copy_null_terminated());
      }
      info->predicted = true;
      SExpr* res =
        info->rcvr->mergeWith(new MapSExpr(Memory->smi_map->enclosing_mapOop(),
                                           r, NULL), NULL);
      if (theSIC->useUncommonTraps && rscope->isUncommonAt(_bci, false)) {
        info->rcvr = res = res->makeUnknownUnlikely(this);
      }
      return res;
    }
    
    return info->rcvr;
  }

# else // !SIC_COMPILER
  void sicInline_init() {}
# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "bitVector.hh"
# include "_bitVector.cpp.incl"

  bool BitVector::unionWith(BitVector* other) {
    if (length < other->length) {
      if (other->length <= maxLength)
        setLength(other->length);
      else {
        *this = *copy(other->length);
      }
    }
    bool changed = false;
    for (fint i = indexFromNumber(other->length-1); i >= 0; i--) {
      int32 old = bits[i];
      bits[i] |= other->bits[i];
      changed |= old != bits[i];
    }
    return changed;
  }

  bool BitVector::intersectWith(BitVector* other) {
    bool changed = false;
    for (fint i = indexFromNumber(min(length, other->length)-1); i >= 0; i--) {
      int32 old = bits[i];
      bits[i] &= other->bits[i];
      changed |= old != bits[i];
    }
    return changed;
  }

  bool BitVector::isDisjointFrom(BitVector* other) {
    for (fint i = indexFromNumber(min(length, other->length)-1); i >= 0; i--) {
      if ((bits[i] & other->bits[i]) != 0) return false;
    }
    return true;
  }

  void BitVector::addFromTo(int32 first, int32 last) {
    // mark bits [first..last]
    assert(first >= 0 && first < length, "wrong index");
    assert(last >= 0 && last < length, "wrong index");
    fint startIndex = indexFromNumber(first);
    fint   endIndex = indexFromNumber(last);
    if (startIndex == endIndex) {
      assert(last - first < BitsPerWord, "oops");
      int32 mask = nthMask(last - first + 1);
      bits[startIndex] |= mask << offsetFromNumber(first);
    } else {
      bits[startIndex] |= AllBits << offsetFromNumber(first);
      for (fint i = startIndex + 1; i < endIndex; i++) bits[i] = AllBits;
      bits[endIndex] |= nthMask(offsetFromNumber(last) + 1);
    }
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions)
        for (fint i = first; i <= last; i++) {
          assert(includes(i), "bit should be set");
        }
#   endif
  }

#ifdef UNUSED
  void BitVector::removeFromTo(int32 first, int32 last) {
    assert(first >= 0 && first < length, "wrong index");
    assert(last >= 0 && last < length, "wrong index");
    fint startIndex = indexFromNumber(first);
    fint   endIndex = indexFromNumber(last);
    if (startIndex == endIndex) {
      assert(last - first < BitsPerWord, "oops");
      int32 mask = ~nthMask(last - first + 1);
      bits[startIndex] &= mask << offsetFromNumber(first);
    } else {
      bits[startIndex] &= ~(AllBits << offsetFromNumber(first));
      for (fint i = startIndex + 1; i < endIndex; i++) bits[i] = 0;
      bits[endIndex] &= ~nthMask(offsetFromNumber(last) + 1);
    }
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions)
        for (fint i = first; i <= last; i++)
          assert(!includes(i), "bit shouldn't be set");
#   endif
  }
#endif

  void BitVector::print_short() { lprintf("BitVector %#lx", this); }
  
  void BitVector::doForAllOnes(intDoFn f) {
    for (fint i = indexFromNumber(length-1); i >= 0; i--) {
      int32 b = bits[i];
      for (fint j = 0; j < BitsPerWord; j++) {
        if (isSet(b, j)) {
          f(i * BitsPerWord + j);
          clearNth(b, j);
          if (!b) break;
        }
      }
    }
  }
  
  void BitVector::print() {
    print_short();
    lprintf(": {");
    fint i, last = -1;
    for (i = 0; i < length; i++) {
      if (includes(i)) {
        if (last < 0) {
          lprintf(" %ld", (void*)i);    // first bit after string of 0s
          last = i;
        }
      } else {
        if (last >= 0) lprintf("..%ld", (void*)(i - 1)); // ended a group
        last = -1;
      }
    }
    if (last >= 0) lprintf("..%ld", (void*)(i - 1));
    lprintf(" }");
  }
  
# endif
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER
  
# pragma implementation "preg.hh"
# include "_preg.cpp.incl"
  
  int32 PReg::currentNo = 0;
  int32 BlockPReg::numBlocks = 0;
  static ConstPRegBList* constants = 0;
  static PReg* dummyPR;
  const fint PReg::AvgBBIndexLen = 10;
  const fint PReg::VeryNegative = -99999999;
  
  // weights indexed by loop depth
  static fint udWeight[] = { 1, 8, 8*8, 8*8*8, 8*8*8*8 };
  const  fint udWeightLen = sizeof(udWeight) / sizeof(fint) - 1;
  
  void initPRegs() {
    PReg::currentNo = 0; BlockPReg::numBlocks = 0;
    constants = new ConstPRegBList(50);
    dummyPR = new PReg(NULL);
  }

  SAPReg::SAPReg(SCodeScope* s, fint st, fint en) : PReg(s) {
    creationStartBCI = startBCI = st == IllegalBCI ? s->bci() : st;
    endBCI                      = en == IllegalBCI ? s->bci() : en;
    creationScope = s;
  }

  bool PReg::isLocalTo(BB* bb) {
    // is this a preg local to bb? (i.e. can it be allocated to temp regs?)
    // treat ConstPRegs as non-local so they don't get allocated prematurely
    // (possible performance bug)
    // (Note: uplevelW -> uplevelR, so this tests either uplevel -- dmu 12/02)
    return
      loc == UnAllocated && !uplevelR && !debug && !incorrectDU() &&
        !isConstPReg() && dus.length() == 1 && dus.first()->bb == bb;
  }


  PDef* PReg::getDefinitionForGlobalCopyPropagation() {
    fint dulength = dus.length();
    fint e;
    DUInfo* info;
    for (e = 0; ; ++e) {
      if (e >= dulength)
        fatal("getDefinitionForGlobalCopyProagation could not find one");
      info = dus.nth(e)->defUseInfo();
      if (info->defs.length())
        break;
    }
    
    assert(info->defs.length() == 1, "should be one definition only");
    PDef* def = info->defs.first();

#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        for ( ++e; e < dulength; ++e) {
          if (dus.nth(e)->defUseInfo()->defs.length())
              fatal("already have def!?!");
        }
      }
#   endif

    assert(def, "should have found the definition");
    return def;
  }


  void PReg::attemptGlobalCopyPropagate() {

    // do global copy propagation of this PReg
    //  only really do it if singly-assigned PRegs

    if ( isConstPReg() || !canCopyPropagate()) 
      return;

    PDef* def = getDefinitionForGlobalCopyPropagation();
      
    if (!def->canCopyPropagate(this))
      return;

    // ok, everything is fine - propagate the def to the uses

    fint dulength = dus.length();
    fint e;
    for (e = 0; e < dulength; e++) {
      PRegBBIndex* index = dus.nth(e);
      DUInfo* info = index->defUseInfo();
      // caution: propagateTo may eliminate nodes and thus shorten
      // info->uses
      fint j = 0;
      BB* bb = index->bb;
      while (j < info->uses.length()) {
        fint oldLen = info->uses.length();
        info->propagateTo(bb, info->reg, def, info->uses.nth(j), true);
        if (info->uses.length() == oldLen) {
          // propagate didn't eliminate this use; try next one
          j++;
        }
      }
    }
  }


  // check basic conditions for global CP
  bool PReg::canCopyPropagate() {
    if (nuses() == 0 || ndefs() != 1) return false;
    // don't propagate if register has incorrect def info or does not
    // survive calls (i.e. is local to BB)
    if (incorrectD() || isTrashedReg(loc)) return false;

    // don't propagate uplevel-assigned (defs aren't correct)
    if (uplevelW) return false;
      
    return true;        // looks good
  }
  
  void PReg::makeSameRegClass(PReg* other, RegisterEqClassBList* classes) {
    if (regClass && !other->regClass) {
      classes->nth(regClass)->append(other);
    } else if (!regClass && other->regClass) {
      classes->nth(other->regClass)->append(this);
    } else if (!regClass && !other->regClass) {
      // non has a regClass - create new one
      regClass = classes->length();
      RegisterEqClass* newClass = new RegisterEqClass(this);
      classes->append(newClass);
      newClass->append(other);
    } else {
      // both already have regClasses - combine the two classes
      classes->nth(regClass)->append(other);
      classes->nthPut(other->regClass, NULL);
    }
  }
  
  ConstPReg* ConstPReg::new_ConstPReg(SSelfScope* s, oop c) {
    for (fint i = 0; i < constants->length(); i++) {
      ConstPReg* r = constants->nth(i);
      if (r->constant == c) {
        r->extendLiveRangeForScope(s);
        return r;
      }
    }
    // constant not found, create new ConstPReg*
    ConstPReg* r = new ConstPReg(s, c);
    constants->append(r);
    r->_ndefs = 1;      // fake def
    return r;
  }
  
# ifdef UNUSED
  ConstPReg* PReg::bfindConstPReg(Node* n, oop c) {
    // return const preg for oop or NULL if none exists
    for (fint i = 0; i < constants->length(); i++) {
      ConstPReg* r = constants->nth(i);
      if (r->constant == c) {
        return r->covers(n) ? r : NULL;
      }
    }
    return NULL;                    // constant not found
  }
# endif
  
  bool ConstPReg::needsRegister() {
    // register only pays off if we're used more than once and aren't a
    // small immediate constant
# if TARGET_ARCH == SPARC_ARCH
    return SICCSEConstants && weight > 1 && 
      (int32(constant) > maxImmediate || int32(constant) < -maxImmediate);
# elif TARGET_ARCH == PPC_ARCH
    return SICCSEConstants && weight > 1 && !fits_within_si(int32(constant));
# elif TARGET_ARCH == I386_ARCH
    return false; // regs tight, use immediates
# endif
  }

  void ConstPReg::allocateTo(Location reg) {
    assert(isRegister(reg), "should be a register");
    loc = reg;
    assert(scope->isCodeScope(), "expected non-access scope");
    ((SCodeScope*)scope)->allocateConst(this);
  }

  inline fint computeWeight(SSelfScope* s) {
    const fint scale = 16;      // normal use counts scale, uncommon use is 1
    if (s && s->isCodeScope() && ((SCodeScope*)s)->primFailure) {
      return 1 * udWeight[min(udWeightLen, s->loopDepth)];
    } else {
      return scale * udWeight[min(udWeightLen, s ? s->loopDepth : 0)];
    }
  }
  
  void PReg::incUses(PUse* use) {
    _nuses++;
    if (use->isSoft()) _nsoftUses++;
    SSelfScope* s = use->node->scope();
    weight += computeWeight(s);
    assert(weight >= _nuses + _ndefs || isConstPReg(), "weight too small");
  }     
  void PReg::decUses(PUse* use) {
    _nuses--;
    if (use->isSoft()) _nsoftUses--;
    SSelfScope* s = use->node->scope();
    weight -= computeWeight(s);
    assert(weight >= _nuses + _ndefs || isConstPReg(), "weight too small");
  }
  
  void PReg::incDefs(PDef* def) {
    _ndefs++;
    SSelfScope* s = def->node->scope();
    weight += computeWeight(s);
    assert(weight >= _nuses + _ndefs || isConstPReg(), "weight too small");
  }

# ifdef UNUSED
  void PReg::decDefs(PDef* def) {
    _ndefs--;
    SSelfScope* s = def->node->scope();
    weight -= computeWeight(s);
    assert(weight >= _nuses + _ndefs || isConstPReg(), "weight too small");
  }
# endif
  
  void PReg::removeUse(DUInfo* info, PUse* use) {
    assert(info->reg == this, "wrong reg");
    info->uses.remove(use);
    decUses(use);
  }
  
  void PReg::removeUse(BB* bb, PUse* use) {
    if (use == NULL) return;
    for (fint i = 0; i < dus.length(); i++) {
      PRegBBIndex* index = dus.nth(i);
      if (index->bb == bb) {
        DUInfo* info = bb->defUseInfo(index);
        removeUse(info, use);
        return;
      }
    }
    ShouldNotReachHere(); // info not found
  }
  
  void PReg::removeDef(DUInfo* info, PDef* def) {
    assert(info->reg == this, "wrong reg");
    info->defs.remove(def);
    _ndefs--;
    SSelfScope* s = def->node->scope();
    weight -= computeWeight(s);
    assert(weight >= _nuses + _ndefs, "weight too small");
  }
  
  void PReg::removeDef(BB* bb, PDef* def) {
    if (def == NULL) return;
    for (fint i = 0; i < dus.length(); i++) {
      PRegBBIndex* index = dus.nth(i);
      if (index->bb == bb) {
        DUInfo* info = bb->defUseInfo(index);
        removeDef(info, def);
        return;
      }
    }
    ShouldNotReachHere(); // info not found
  }
  
  void PReg::addDUHelper(Node* n, PDefUseList* l, PDefUse* el) {
    int32 myNum = n->num();
    PDefUseListElem* prev = NULL;
    for (PDefUseListElem* e = l->head();
         e && e->data()->node->num() < myNum;
         prev = e, e = e->next()) ;
    l->insertAfter(prev, el);
  }
  
  PUse* PReg::addUse(DUInfo* info, Node* n) {
    assert(info->reg == this, "wrong reg");
    PUse* u = new PUse(n);
    addDUHelper(n, (PDefUseList*)&info->uses, u);
    incUses(u);
    return u;
  }
  
  PUse* PReg::addUse(BB* bb, Node* n) {
    for (fint i = 0; i < dus.length(); i++) {
      PRegBBIndex* index = dus.nth(i);
      if (index->bb == bb) {
        DUInfo* info = bb->defUseInfo(index);
        return addUse(info, n);
      }
    }
    return bb->addUse(n, this);
  }
  
  PDef* PReg::addDef(DUInfo* info, Node* n) {
    assert(info->reg == this, "wrong reg");
    PDef* d = new PDef(n);
    addDUHelper(n, (PDefUseList*)&info->defs, d);
    incDefs(d);
    return d;
  }
  
  PDef* PReg::addDef(BB* bb, Node* n) {
    for (fint i = 0; i < dus.length(); i++) {
      PRegBBIndex* index = dus.nth(i);
      if (index->bb == bb) {
        DUInfo* info = bb->defUseInfo(index);
        return addDef(info, n);
      }
    }
    return bb->addDef(n, this);
  }

  bool SAPReg::extendLiveRange(Node* n) {
    // the receiver is being copy-propagated to n; try to extend its live range
    assert(startBCI != IllegalBCI && creationStartBCI != IllegalBCI &&
           endBCI != IllegalBCI, "live range not set");
    bool ok = true;
    if (n->scope() == scope) {
      fint bci = n->bci();
      assert(bciGE(bci, startBCI), "can't propagate backwards");
      if (bciGT(bci, endBCI)) endBCI = bci;
    } else if (n->scope()->isSenderOf(scope)) {
      // propagating upwards - promote receiver to higher scope
      SSelfScope *s, *nscope = n->scope();
      for (s = scope; s->sender() != nscope; s = s->sender());
      scope = nscope;
      startBCI = s->senderBCI();
      endBCI = n->bci();
    } else if (scope->isSenderOf(n->scope())) {
      // scope is caller; check if already covered
      SSelfScope *s;
      for (s = n->scope(); s->sender() != scope; s = s->sender()) ;
      fint bci = s->senderBCI();
      assert(bciGE(bci, startBCI), "can't propagate backwards");
      if (bciGT(bci, endBCI)) endBCI = bci;
    } else {
      // can't propagate between siblings yet
      ok = false;
    }
    assert(bciLE(startBCI, endBCI) && startBCI != IllegalBCI &&
           bciLT(endBCI, nscope()->ncodes), "invalid start/endBCI");
    return ok;
  }

  bool SplitPReg::extendLiveRange(Node* n) {
    // currently can't extend split regs live range without screwing up
    // register allocation
    assert(startBCI != IllegalBCI && creationStartBCI != IllegalBCI &&
           endBCI != IllegalBCI, "live range not set");
    if (n->scope() == scope) {
      fint bci = n->bci();
      assert(bciGE(bci, startBCI), "can't propagate backwards");
      if (bciLE(bci, endBCI)) return true;      // already covered
    } else if (scope->isSenderOf(n->scope())) {
      // scope is caller; check if already covered
      SSelfScope *s;
      for (s = n->scope(); s->sender() != scope; s = s->sender()) ;
      fint bci = s->senderBCI();
      assert(bciGE(bci, startBCI), "can't propagate backwards");
      if (bciLE(bci, endBCI)) return true;      // already covered
    } 
    return false;
  }

  void ConstPReg::extendLiveRangeForScope(SSelfScope* s) {
    // make sure the constant reg is in a high enough scope
    if (scope->isSenderOrSame(s)) {
      // scope is caller of s
    } else if (s->isSenderOf(scope)) {
      // s is caller of scope, so promote receiver to s
      scope = s;
    } else {
      // scope and s are siblings of some sort - go up to common sender
      do { s = s->sender(); } while (!s->isSenderOf(scope));
      scope = s;
    }
  }

# ifdef UNUSED
  bool ConstPReg::covers(Node* n) {
    // does receiver cover node n (is it live at n)?
    SSelfScope* s = n->scope();
    if (scope->isSenderOrSame(s)) {
      // ok, scope is caller of s
      return true;
    } 
    return false;
  }
# endif

  bool ConstPReg::extendLiveRange(Node* n) {
    extendLiveRangeForScope(n->scope());
    return true;
  }
  
  bool PReg::checkEquivalentDefs() {
    // check if all defs are equivalent, i.e. assign the same preg
    if (ndefs() == 1) return true;
    PReg* rhs = NULL;
    for (fint i = 0; i < dus.length(); i++) {
      DUInfo* info = dus.nth(i)->defUseInfo();
      for (PDefListElem* e = info->defs.head(); e; e = e->next()) {
        Node* n = e->data()->node;
        if (!n->isAssignmentLike()) return false;
        if (rhs) {
          if (rhs != n->src()) return false;
        } else {
          rhs = n->src();
        }
      }
    }
    // yup, rhs is the only preg ever assigned to me
    return true;
  }

  bool PReg::canEliminateAndStillDebug() {
    // can this PReg be eliminated without compromising the debugging info?
    //  (used to take withUses arg, but am always planning to elim uses, too)

    if (isUnused()) return if_why(false, "nothing to eliminate");
    
    // check if reg can be eliminated
    if ( incorrectU() ) 
      return if_why(false, "don't elim if uses are incorrect (hardwired pregs");
      
    if ( uplevelW  ) 
      return if_why(false, "uses also incorrect for uplevel-written vars");
    
    assert(!_nsoftUses || debug, "nsoftUses should imply debug");

    if (isBlockPReg()) { 
      assert( hardUses() == 0, "only called when no hard uses of me");
      return if_why(true, 
        "blocks can always be eliminated - can describe with BlockValueDesc");
    }
    
    if (!debug)
      return if_why(true, "not needed for debugging");
      
    return canRuntimeValueBeReconstructed();
  }
  
  
  bool PReg::if_why(bool b, const char* why) {
    if (PrintSICEliminateUnneededNodes)
      lprintf("*%s eliminate %s: %s\n", 
              b ? "ok to" : "uncool to", why, name());
    return b;
  }
  

  // debug-visible: eliminate only if run-time value
  // can be reconstructed
  
  bool PReg::canRuntimeValueBeReconstructed() {
      
    if ( _ndefs > 1
    &&   !isBlockPReg()            // we know all defs of a block are equivalent
    &&   !checkEquivalentDefs())   // ok if all defs are the same
      return if_why(false, ">1 def && debug-visible");
    
    DUInfo* info = dus.first()->defUseInfo();
    PDefListElem* e = info->defs.head();
    if (!e)
      return if_why(false, 
        "info not in first elem - would have to search");

    Node* defNode = e->data()->node;
    if (defNode->hasConstantSrc())
      return if_why(true, "constant assignment - easy to handle");
    
    // can substitute defSrc if its lifetime encompasses ours and if
    // it is singly-assigned and not a temp reg (last cond. is necessary to
    // prevent e.g. result of a send (in o0) to be used as the receiver of
    // subsequent scopes that have nsends > 0; fix this if it becomes a
    // performance problem)
      
    if ( !defNode->hasSrc() )
      return if_why(false, "can't recover debug info, no defSrc");

    PReg* defSrc = defNode->src();
    if (!defSrc->isSAPReg())
      return if_why(false, "can't recover debug info, def not SAPReg");

    if (isTempReg(defSrc->loc)) 
      return if_why(false, "can't recover debug info, def isTempReg");
    
    if ( defSrc->scope->isSenderOf(scope) )
      return if_why(true, 
        "definition scope is sender of my eliminatee's scope");
        
    if ( defSrc->scope != scope )
      return if_why(false, "def scope not my scope");
    
    if (!isSAPReg())
      return if_why(false, "not SAPReg");
      
    if (bciGE(((SAPReg*)defSrc)->endBCI, ((SAPReg*)this)->endBCI))
      return if_why(true, "def in same scope, SAPReg, lifetime encompasses");
    
    return if_why(false, "def ends too soon");
  }
  

  bool BlockPReg::canEliminateAndStillDebug() {
    if (!PReg::canEliminateAndStillDebug()) return false;
    if (!escapes) return true;
    if (uplevelR) return false;  // uplevelW -> uplevelW -- dmu 12/02
    
    // escaping, unused block; since it has no real uses, the only time
    // it might be created (might escape) is when a nested block is created
    oop value = block->value();
    if (!value->has_code()) {   // empty block
      escapes = false; 
      isEliminated = true;
      return true; 
    }
    oop* literals = value->literals()->objs();
    for (fint i = value->literals()->length() - 1; i >= 0; i--) {
      if (literals[i]->is_block()) {
        // can't eliminate, but at least memoize it
        // fix this - should find nested BlockPR and check if it escapes
        if (PrintSICEliminateUnneededNodes) 
          lprintf("*not eliminating block %s (has nested blocks)\n", name());
        memoize();
        return false;
      }
    }
    // ok - all exposing nodes were optimized away, the block doesn't really
    // escape
    escapes = false;
    isEliminated = true;
    return true;
  }
  
    
  // eliminate all nodes defining me
  // if removing is true, then we are removing unreachable code,
  //   else removing reg cause it is never used
  
  void PReg::eliminatePR( bool removingUnreachableCode) {
    assert( hardUses() == 0, "only called when no hard uses of me");
    if ( !canEliminateAndStillDebug()) return;
    for (fint i = 0; i < dus.length(); i++) {
      PRegBBIndex* index = dus.nth(i);
      DUInfo* info = index->defUseInfo();
      BB*       bb = index->bb;
      PDefListElem* e = info->defs.head();
      while (e) {
#       if GENERATE_DEBUGGING_AIDS
          fint oldlen;
          if (CheckAssertions) {
            oldlen = info->defs.length();
          }
#       endif
        Node* n = e->data()->node;
        if (!removingUnreachableCode  &&   n->hasSideEffects()) {
          // cannot eliminate this node, its effect (though not its)
          //  result is needed
          // But, its destination must be redirected to a dummy register.
          // Otherwise, the allocator will allocate a location for it,
          //  and an assertion will trip, since this PR has a cpinfo
          n->eliminateDest(bb);
          // eliminateDest is a noop for a call node, but is OK
          //   to leave this def of o0, shouldn't hurt anything
          // elminateDest is also a noop for a block zap node
          //  this should not be a problem says Urs, -- dmu 8/96
          if (PrintSICEliminateUnneededNodes) {
            char buf[1024];
            lprintf("*not eliminating node N%ld: %s\n", (void*)n->id(),
                    n->print_string(buf)); 
          }
          assert(nuses() == 0, "cannot eliminate this?");
          e = e->next();
        }
        else {
          if (cpInfo) {
            if (debug) {
              // canEliminateAndStillDebug assures that all defs are equivalent
#             if GENERATE_DEBUGGING_AIDS
                if (CheckAssertions) {
                  CPInfo* cpi = new_CPInfo(n);
                  assert(  cpi && cpi->r->cpReg() == cpReg(),
                         "can't handle this");
                }
#             endif
            } 
            else {
              // can't really handle CP w/multiple defs; make sure we don't use
              // bad information
              cpInfo->r = dummyPR;
            }
          } 
          else {
            cpInfo = new_CPInfo(n);
            assert(!debug || cpInfo || isBlockPReg(), "couldn't create info");
            if (cpInfo) {
              PReg* r = cpInfo->r;
              // if we're eliminating a debug-visible PReg, the replacement
              // must be debug-visible, too (so that it isn't allocated to
              // a temp reg)
              r->debug |= debug;
              if (r->cpRegs == NULL) r->cpRegs = new PRegBList(5);
              r->cpRegs->append(this);
            }
          }
          
          n->eliminateNodeAndUsedPRs(bb, this, removingUnreachableCode);
          assert(info->defs.length() < oldlen, "didn't remove def");
          e = info->defs.head();
          // simple, but may rescan some uneliminatable nodes
        }
      }
      eliminateUses(info, bb, removingUnreachableCode);
    }
  }
  
  void PReg::eliminateUses(DUInfo* info, BB* bb, bool removingUnreachableCode) {
    PUseListElem* ue = info->uses.head();
    while (ue) {
#     if GENERATE_DEBUGGING_AIDS
      fint oldlen;
      if (CheckAssertions) {
        oldlen = info->uses.length();
      }
#     endif
      Node* n = ue->data()->node;
      if (PrintSICEliminateUnneededNodes) {
        char buf[1024];
        lprintf("*eliminating node N%ld: %s\n", 
               (void*)n->id(),
               n->print_string(buf)); 
      }
      assert(!n->hasSideEffects(), "must be able to eliminate this");
      n->eliminateNodeAndUsedPRs(bb, this, removingUnreachableCode);
      assert(info->uses.length() < oldlen, "didn't remove use");
      ue = info->uses.head();
    }
 }

  // for efficiency, node n in isLiveAt() must be "plausible", i.e. in a
  // scope somewhere below the receiver's scope
  // otherwise, use slow_isLiveAt

  bool PReg::slow_isLiveAt(Node* n) {
    if (scope->isSenderOrSame(n->scope())) {
      return isLiveAt(n);
    } else {
      return false;
    }
  }

  bool PReg::isLiveAt(Node* n) {
    SSelfScope* s = n->scope();
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      SScope *ss;
      for (ss = s; ss != scope && ss; ss = ss->sender()) ;
      assert(ss == scope, "s is not below my scope");
    }
#   endif
    return true;
  }

  SCodeScope* PReg::findAncestor(SSelfScope* s1, fint& bci1,
                                 SSelfScope* s2, fint& bci2) {
    // find closest common ancestor of s1 and s2, and the
    // respective sender bcis in that scope
    if (s1->depth > s2->depth) {
      while (s1->depth > s2->depth) {
        bci1 = s1->senderBCI(); s1 = s1->sender();
      }
    } else {
      while (s2->depth > s1->depth) {
        bci2 = s2->senderBCI(); s2 = s2->sender();
      }
    }
    assert(s1->depth == s2->depth, "oops");
    while (s1 != s2) {
      bci1 = s1->senderBCI(); s1 = s1->sender();
      bci2 = s2->senderBCI(); s2 = s2->sender();
    }
    assert(s1->isCodeScope(), "oops");
    return (SCodeScope*)s1;
  }

  bool SplitPReg::isLiveAt(Node* n) {
    if (!SAPReg::isLiveAt(n)) return false;
    SplitSig* sig2 = n->splitSig();
    return sig->contains(sig2);
  }

  SplitPReg* SAPReg::regCovering(SCodeScope* s1, fint bci1,
                                 SCodeScope* s2, fint bci2, SplitSig* sig) {
    // return a PReg covering both s1 and s2; use s2 as the creation scope
    fint b1 = bci1, b2 = bci2;
    SCodeScope* ss = PReg::findAncestor(s1, b1, s2, b2);
    assert(b1 >= b2, "use (b1) should come after def (b2)");
    SplitPReg* r = new SplitPReg(ss, b2, b1, sig);
    r->creationScope = s2;
    r->creationStartBCI = bci2;
    return r;
  }

  bool SAPReg::isLiveAt(Node* n) {
    // first check if receiver is live in source-level terms; if that says
    // dead it really means dead
    SSelfScope* s = n->scope();
    assert(s->isCodeScope(), "oops");
    bool live = basic_isLiveAt((SCodeScope*)s, n->bci());
    if (!live || !isTempReg(loc)) return live;
    // tempReg-allocated PRegs can only be live in the BB's where they appear;
    // use this as an additional filter
    // this check helps reduce the number of "inconsistent reg contents"
    // failures in MarkerNode::dexcribe
    // (this code only works because tempReg-allocated PRegs currently span
    // only one BB - see isLocalTo)
    BB* bb = n->bb();
    for (fint i = 0; i < dus.length(); i++) {
      BB* bb2 = dus.nth(i)->bb;
      if (bb == bb2) return true;
    }
    return false;       // not live despite what basic_isLiveAt says
  }
  
  bool SAPReg::basic_isLiveAt(SCodeScope* s, fint bci) {
    assert(bciLT(bci, s->ncodes), "bci too high");
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      SScope* s1;
      for (s1 = s; s1 != scope && s1; s1 = s1->sender()) ;
      assert(s1 == scope, "s is not below my scope");
    }
#   endif
    // check out SCodeScope::computeMaskFor before/after changing the
    // code below - is marks the sendDesc masks using the same algorithm

    // find closest common ancestor of s and creationScope, and the
    // respective bcis in that scope
    fint bs = bci;
    fint bc = creationStartBCI;
    SSelfScope* ss = findAncestor(s, bs, creationScope, bc);
    if (!scope->isSenderOrSame(ss)) fatal("bad scope arg in basic_isLiveAt");
    if (ss == scope) {
      // live range = ]startBCI, endBCI]
      assert(startBCI == bc ||
             ss == creationScope && creationStartBCI == bc, "oops");
      return bciLT(startBCI, bs) && bciLE(bs, endBCI);
    } else {
      // live range = ]bc, end of scope]
      return bciLT(bc, bs);
    }
  }

  bool PReg::isCPEquivalent(PReg* r) {
    // is receiver in same register as argument?
    if (this == r) return true;
    // try receiver's CP info
    CPInfo* i;
    for (i = cpInfo; i && i->r; i = i->r->cpInfo) {
      if (i->r == r) return true;
    }
    // now try the other way
    for (i = r->cpInfo; i && i->r; i = i->r->cpInfo) {
      if (i->r == this) return true;
    }
    return false;
  }
  
  NameNode* PReg::locNameNode(bool mustBeLegal) {
    UsedOnlyInAssert(mustBeLegal);
    assert(!isTempReg(loc) || !mustBeLegal, "shouldn't be in temp reg");
    if (isTempReg(loc) && !debug) {
      return new IllegalName;
    } else {
      // debug-visible PRegs may have temp regs if they're only visible 
      // from uncommon branches
      return new LocationName(loc);
    }
  }

  SCodeScope* BlockPReg::parent() {
    return scopeFromBlockMap(block->map()->enclosing_mapOop());
  }

  SCodeScope* BlockPReg::scopeFromBlockMap(mapOop block_map) {
    blockMap *b= (blockMap*)block_map->map_addr();
    smiOop desc= b->desc();
    assert_smi(desc, "should be an integer");
    assert(desc != BLOCK_PROTO_DESC, "should have been changed");
    SCodeScope* s = (SCodeScope*)desc;
    // s may be invalid (receiver block) - make educated guess
    if (int(s) < 256 * K) s = NULL;     // not a valid address
    assert(s == NULL || s->isCodeScope(), "should be non-access");
    return s;
  }
  
  NameNode* BlockPReg::locNameNode(bool mustBeLegal) {
    Unused(mustBeLegal);
    assert(!isTempReg(loc), "shouldn't be in temp reg");    
    if (memoized || primFailBlockScope) {
      assert(parent()->nsends > 0, "should have exposing send for this block");
    }
    // always describe blocks with MemoizedNames so we can find all blocks
    // in a vframe (see Recompilation::checkForRemappedBlocks)
    return new MemoizedName(loc, block);
  }
  
  NameNode* PReg::nameNode(bool mustBeLegal) {
    PReg* r = cpReg();
    if (r->loc != UnAllocated) {
      return r->locNameNode(mustBeLegal);
    } else if (r->isConstPReg()) {
      return r->nameNode(mustBeLegal);
    } else if (r->isBlockPReg()) {
      return new BlockValueName(((BlockPReg*)r)->block);
    } else {
      // commented out for the time being, miw 6/14/96
      // assert(!debug, "couldn't recover debug info");
      return new IllegalName;
    }
  }

  NameNode* ConstPReg::nameNode(bool mustBeLegal) {
    Unused(mustBeLegal);
    return newValueName(constant); }

  PReg* PReg::cpReg() {
    assert(!cpInfo || loc == UnAllocated,
           "allocated regs shouldn't have cpInfo");
    if (cpInfo == NULL) {
      return this;
    } else {
      PReg* r;
      for (CPInfo* i = cpInfo; i; r = i->r, i = r->cpInfo) ;
      return r;
    }
  }
  
  void BlockPReg::memoize() {
    if (primFailBlockScope == NULL) memoized = true;
  }
  
  void BlockPReg::markEscaped() {
    if (!escapes) {
      escapes = true;
      // must describe parent scope because block escapes
      assert(scope->isCodeScope(), "should be non-access");
      nscope()->addSend(new PRegBList(1));
      if (PrintSICExposed) lprintf("*exposing %s\n", name());
    }
  }
      
  
  const char* PReg::name() {
    char* n = NEW_RESOURCE_ARRAY(char, 25);
    if (loc == UnAllocated) { 
      sprintf(n, "%s%d%s%s%s", prefix(), id(),
              uplevelR || uplevelW ? "^" : "",
              uplevelR ? "R" : "", uplevelW ? "W" : "");
    } else {
      sprintf(n, "%s%d(%s)%s%s%s", prefix(), id(), locationName(loc),
              uplevelR || uplevelW ? "^" : "",
              uplevelR ? "R" : "", uplevelW ? "W" : "");
    }
    return n; 
  }
  
  void PReg::print() {
    print_short(); lprintf(": "); printDefsAndUses(&dus);
  }
  
  const char* BlockPReg::name() {
    char* n = NEW_RESOURCE_ARRAY(char, 25);
    sprintf(n, "%s <%#lx>%s", PReg::name(), (unsigned long)block,
            memoized ? "#" : (primFailBlockScope ? "#F" : ""));
    return n;
  }
  
  const char* ConstPReg::name() {
    char* n = NEW_RESOURCE_ARRAY(char, 25);
    sprintf(n, "%s <%#lx>", PReg::name(), (unsigned long)constant);
    return n;
  }
  
  const char* SplitPReg::name() {
    char* n = NEW_RESOURCE_ARRAY(char, 25);
    char buf[MaxSplitDepth+1];
    sprintf(n, "%s <%s>", PReg::name(), sig->prefix(buf));
    return n;
  }
  
  bool PReg::verify() {
    bool ok = true;
    if (_id < 0 || _id >= currentNo) {
      ok = false;
      error2("PReg %#lx: invalid ID %ld", this, _id);
    }
    int32 uses = 0, defs = 0;
    for (fint i = 0; i < dus.length(); i++) {
      DUInfo* info = dus.nth(i)->defUseInfo();
      defs += info->defs.length();
      uses += info->uses.length();
    }
    if (defs != _ndefs && !incorrectD() && !isConstPReg()) {
      // ConstPRegs have fake def
      ok = false;
      error3("PReg %#lx: wrong def count (%ld instead of %ld)",
             this, _ndefs, defs);
    }
    if (uses != _nuses && !incorrectU()) {
      ok = false;
      error3("PReg %#lx: wrong use count (%ld instead of %ld)",
             this, _nuses, uses);
    }
    if (!incorrectDU() && _ndefs == 0 && _nuses > 0) {
      ok = false;
      error1("PReg %#lx: used but not defined", this);
    }
    if (uplevelW && !uplevelR) {
      ok = false;
      error1("PReg %#lx: uplevelW should imply uplevelR", this);
    }
#   ifdef fixthis  // fix this - may still be needed
    if (debug && !incorrectDU() && isTrashedReg(loc)) {
      ok = false;
      error1("PReg %#lx: debug-visible but allocated to temp reg", this);
    }
#   endif
    
    return ok;
  }
  
  bool SAPReg::verify() {
    bool ok = PReg::verify();
    if (ok) {
      if (startBCI == IllegalBCI) {
        if (creationStartBCI != IllegalBCI || endBCI != IllegalBCI) {
          ok = false;
          error1("SAPReg %#lx: live range only partially set", this);
        }
      } else if (scope->isCodeScope()) {
        fint ncodes = nscope()->ncodes;
        if (creationStartBCI < EpilogueBCI
        ||  creationStartBCI >= creationScope->ncodes) {
          ok = false;
          error2("SAPReg %#lx: invalid creationStartBCI %ld",
                 this, creationStartBCI);
        }
        if (startBCI < EpilogueBCI || startBCI >= ncodes) {
          ok = false;
          error2("SAPReg %#lx: invalid startBCI %ld", this, startBCI);
        }
        if (endBCI < EpilogueBCI || endBCI >= ncodes) {
          ok = false;
          error2("SAPReg %#lx: invalid endBCI %ld", this, endBCI);
        }
      }
    }
    return ok;
  }

  bool BlockPReg::verify() {
    bool ok = SAPReg::verify() && block->verify();
    if (!block->is_block()) {
      ok = false;
      error2("BlockPReg %#lx: oop %#lx is not a block", this, block);
    }
    if (isMaterialized &&  !escapes  &&  !isEliminated) {
      // blocks are materialized when generating code;
      //  a subsequent pass asks each node for escaping blocks to
      //   ensure their locals are stack-allocated.
      //  So the computeExposedBlocks routines must be consistent with
      //   the code generation routines.
      //  Catch one inconsistency here. -- dmu
      //  (Must check has_code because canEliminateAndStillDebug above falsifies escapes).
      ok = false;
      error2("BlockPReg %#lx: "
             "oop %#lx has been materialized but has not been caught escaping",
             this, block);
    }
    return ok;
  }
  
  bool NoPReg::verify() { return true; }
  
  bool ConstPReg::verify() {
    bool ok = (PReg::verify() && constant->is_map()) || constant->verify();
# if TARGET_ARCH == SPARC_ARCH
    if (int32(constant) < maxImmediate && int32(constant) > -maxImmediate
        && loc != UnAllocated) {
# elif TARGET_ARCH == PPC_ARCH
    if (fits_within_si(int32(constant)) && loc != UnAllocated) {
# elif TARGET_ARCH == I386_ARCH
    if (loc != UnAllocated) {
# endif
      error2("ConstPReg %#lx: could use load immediate to load oop %#lx",
             this, constant);
      ok = false;
    }
    if (loc != UnAllocated && !isRegister(loc)) {
      error1("ConstPReg %#lx: was allocated to stack", this);
      ok = false;
    }
    if (loc != UnAllocated && isTrashedReg(loc)) {
      error1("ConstPReg %#lx: was allocated to trashed reg", this);
      ok = false;
    }
    return ok;
  }
  
# endif
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "sicExpr.hh"
# include "_sicExpr.cpp.incl"

  // merge expressions have the following properties:
  // - they never have more than MaxPICSize+1 elements
  // - they're flat (i.e don't contain other MergeSExprs)

  SExpr* SExpr::copyMergeWith(SExpr* other, PReg* p, Node* nod) {
    SExpr *m = new MergeSExpr(p, nod);
    if (!p) m->_preg = preg();
    m = m->mergeWith(this, nod);
    m = m->mergeWith(other, nod);
    return m;
  }

  MergeSExpr::MergeSExpr(PReg* p, Node* nod) : SExpr(p, nod) { initialize(); }

  void MergeSExpr::initialize() {
    exprs = new SExprBList(MaxPICSize + 1); setSplittable(_node != NULL);
  }

  NoResultSExpr::NoResultSExpr(Node* n) : SExpr(theNodeGen->noPR, n) {}

  MapSExpr::MapSExpr(mapOop m, PReg* p, Node* n) : SExpr(p, n) {
    assert_map(m, "must be a map");
    _myMapOop = m;
  }
  
  ClonedBlockSExpr::ClonedBlockSExpr(mapOop m, PReg* p, Node* n)
  : MapSExpr(m, p, n) {
    assert(m->map_addr()->is_block(), "not a block");
    assert(p->isBlockPReg(), "not a block preg");
    assert(n, "must have a node");
    _blockScope = (SCodeScope*)p->scope;
  }

  const fint UnknownSExpr::UnlikelyBit = 1;

  bool UnknownSExpr::equals(SExpr* other) {
    return other->isUnknownSExpr();
  }

  bool NoResultSExpr::equals(SExpr* other) {
    return other->isNoResultSExpr();
  }

  bool MapSExpr::equals(SExpr* other) {
    return (other->isMapSExpr() || other->isConstantSExpr()) &&
           other->myMapOop() == myMapOop();
  }

  bool ClonedBlockSExpr::equals(SExpr* other) {
    return other->isClonedBlockSExpr() && other->myMapOop() == myMapOop();
  }

  bool ConstantSExpr::equals(SExpr* other) {
    return (other->isConstantSExpr() && other->constant() == constant()) ||
           (other->     isMapSExpr() && other->myMapOop() == myMapOop());
  }

  const fint MergeSExpr::SplittableBit        = 2;
  const fint MergeSExpr::UnknownSetBit        = 4;
  const fint MergeSExpr::ContainingUnknownBit = 8;

  bool MergeSExpr::equals(SExpr* other) {
    Unused(other);
    return false; // for now
  }  

  SExpr* UnknownSExpr::mergeWith(SExpr* other, Node* n) {
    if (other->isNoResultSExpr()) return this;
    if (other->isUnknownSExpr()) {
      if (n && node() && other->node()) {
        // preserve splitting info
        SExpr* e = copyMergeWith(other, preg(), n);
        return e;
      } else {
        _node = NULL;   // prevent future splitting
        return this;
      }
    } else {
      PReg* r = _preg == other->preg() ? _preg : NULL;
      return copyMergeWith(other, r, n);
    }
  }

  SExpr* NoResultSExpr::mergeWith(SExpr* other, Node* n) {
    Unused(n);
    return other; }

  SExpr* MapSExpr::mergeWith(SExpr* other, Node* n) {
    if (other->isNoResultSExpr()) return this;
    if ((other->isMapSExpr() || other->isConstantSExpr())
        && other->myMapOop() == myMapOop()) {
      // generalize map + constant in same clone family --> map
      _node = NULL;     // prevent future splitting
      return this;
    } else {
      PReg* r = _preg == other->preg() ? _preg : NULL;
      return copyMergeWith(other, r, n);
    }
  }

  SExpr* ClonedBlockSExpr::mergeWith(SExpr* other, Node* n) {
    if (other->isNoResultSExpr()) return this;
    if (equals(other)) {
      if (n && node() && other->node()) {
        // preserve splitting info
        SExpr* e = copyMergeWith(other, preg(), n);
        return e;
      } else {
        _node = NULL;   // prevent future splitting
        return this;
      }
    } else {
      PReg* r = _preg == other->preg() ? _preg : NULL;
      return copyMergeWith(other, r, n);
    }
  }

  SExpr* ConstantSExpr::mergeWith(SExpr* other, Node* n) {
    // NB: be careful not to mapify true & false consts
    if (other->isNoResultSExpr()) return this;
    if (other->isConstantSExpr()
        && other->constant() == constant()) {
      if (n && node() && other->node()) {
        // preserve splitting info
        SExpr* e = copyMergeWith(other, preg(), n);
        return e;
      } else {
        _node = NULL;   // prevent future splitting
        return this;
      }
    } else if (other->isMapSExpr()) {
      return other->mergeWith(this, n);
    } else {
      PReg* r = _preg == other->preg() ? _preg : NULL;
      return copyMergeWith(other, r, n);
    }
  }

  SExpr* MergeSExpr::mergeWith(SExpr* other, Node* n) {
    if (other->isNoResultSExpr()) return this;
    setUnknownSet(false);
    if (n == NULL) setSplittable(false);
    _node = n;
    if (other->isMergeSExpr()) {
      MergeSExpr* o = other->asMergeSExpr();
      if (o->isSplittable() && !isSplittable()) return o->mergeWith(this, n);
      for (fint i = 0; i < o->exprs->length(); i++) {
        // must be careful when adding splittable exprs (e->next != NULL)
        // to avoid creating loops in the ->next chain
        SExpr* e = o->exprs->nth(i);
        SExpr* nexte;
        for ( ; e; e = nexte) {
          nexte = e->next;
          e->next = NULL;
          add(e);
        }
      }
    } else {
      add(other);
    }
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        fint len = exprs->length();
        for (fint i = 0; i < len; i++) {
          SExpr* e = exprs->nth(i);
          for (fint j = i + 1; j < len; j++) {
            SExpr* e2 = exprs->nth(j);
            assert(! e->equals(e2), "duplicate expr");
            assert(! (e->hasMap() && e2->hasMap() && e->myMapOop() == e2->myMapOop()),
                     "duplicate maps");
          }
        }
      }
#   endif
    assert(!isSplittable() || _node, "splittable mergeExpr must have a node");
    return this;
  }

  void MergeSExpr::add(SExpr* e) {
    assert(e->next == NULL, "shouldn't be set");
    setUnknownSet(false);
    if (exprs->isFull()) return;
    if (!e->node()) setSplittable(false);
    for (fint i = 0; i < exprs->length(); i++) {
      SExpr* e1 = exprs->nth(i);
      if ((e->hasMap()  &&  e1->hasMap()  &&  e->myMapOop() == e1->myMapOop()) ||
          e->equals(e1)) {
        // an equivalent expression is already in our list
        // if unsplittable we don't need to do anything except
        // if e is a map and the expr we already have is a constant
        // (otherwise: might later make unknown unlikely and rely on
        // constant value)
        if (!isSplittable() && !e1->isConstantSExpr()) return;
        
        // even though the map is already in our list, we care about
        // the new entry because we might have to copy the nodes between
        // it and the split send
        // Therefore, we keep lists of equivalent SExprs (linked via the
        // "next" field).
#       if GENERATE_DEBUGGING_AIDS
          if (CheckAssertions) {
            for (SExpr* e2 = exprs->nth(i); e2; e2 = e2->next)
              assert(!e->node() || e2->node() != e->node(),
                       "node alrady in list");
          }
#       endif
        // generalize different constants to maps
        if (e->isConstantSExpr() && e1->isConstantSExpr() &&
            e->constant() == e1->constant()) {
          // leave them as constants
        } else {
          if (e->isConstantSExpr()) {
            e = e->mapify(e->preg(), e->node());
          }
          if (e1->isConstantSExpr()) {
            // mapify e1 and replace it in receiver
            SExpr* ee = e1->mapify(e1->preg(), e1->node());
            ee->next = e1->next;
            exprs->nthPut(i, ee);
          }
        }
        if (!isSplittable()) return;
        // append e at end of e1's next chain
        for (e1 = exprs->nth(i); e1->next; e1 = e1->next) ;
        e1->next = e;
        return;
      }
    }
    if (exprs->length() == MaxPICSize) {
      // our capacity overflows, so make sure we've got at least one Unknown
      // type in our set
      if (findUnknown() == NULL)
        exprs->append(new UnknownSExpr(e->preg(), NULL));
    } else {
      exprs->append(e);
    }
  }


  void MergeSExpr::resetTo(Node* n, SExprBList* e) {
    // called for multiple splitting; does the unorthodox
    // (after all exprs should be immutable)
    // step of resetting my node and exprs to account for a split
    // that has just been done.
    // keep the same preg because it was good before the split and
    // the split should not remove assignments to it.

   if (exprs->length() == 0)
     fatal("merge must have one old expr");
   PReg* pr = exprs->nth(0)->preg();

#  if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
     for (fint i = 0;  i < exprs->length();  ++i)
       assert(pr == exprs->nth(i)->preg(), "incoming exprs must have same preg");
    }
#  endif
     
   setNode(n, pr);
    initialize(); // empty out exprs
    for (fint i = 0;  i < e->length();  ++i)
      add( e->nth(i) );
  }


  bool MergeSExpr::really_hasMap(SSelfScope* s) {
    // Check if receiver really has only one map.  Specifically, if we're
    // at the place that made the receiver's unknown part unlikely, the
    // receiver should *not* be considered a MapSExpr because the unknown
    // part hasn't been tested yet.
    return hasMap() && !(s == unlikelyScope && s->bci() <= unlikelyBCI);
  }

  bool MergeSExpr::hasMap() {
    // treat a merge expr like a single map if it contains only one map and
    // possibly an unlikely unknown
    if (exprs->length() > 2) return false;
    SExpr* e1 = exprs->nth(0);
    bool haveMap1 = e1->hasMap();
    if (exprs->length() == 1) return haveMap1;  // only one expr
    UnknownSExpr* u1 = e1->findUnknown();
    if (u1 && !u1->isUnlikely()) return false;  // 1st = likely unknown
    SExpr* e2 = exprs->nth(1);
    bool haveMap2 = e2->hasMap();
    UnknownSExpr* u2 = e2->findUnknown();
    if (u2 && !u2->isUnlikely()) return false;  // 2nd = likely unknown
    if (haveMap1 && haveMap2) return false;     // 2 maps
    // success!  one expr may have map, one is unlikely unknown
    return haveMap1 || haveMap2;
  }

  mapOop MergeSExpr::myMapOop() {
    assert(hasMap(), "don't have a map");
    SExpr* e = exprs->nth(0);
    if (e->hasMap()) return e->myMapOop();
    return exprs->nth(1)->myMapOop();
  }

  SExpr* MergeSExpr::asReceiver() {
    return new MapSExpr(myMapOop(), _preg, _node);
  }

  bool MergeSExpr::hasConstant() {
    // see hasMap()...also, must be careful about different constants with
    // same map (i.e. expr->next is set)
    return false;
  }

  oop MergeSExpr::constant() {
    ShouldNotCallThis();
    return 0;
  }

  SExpr* ConstantSExpr::mapify(PReg* p, Node* n) {
    return new MapSExpr(_c->map()->enclosing_mapOop(), p, n);
  }

  MapSExpr* ConstantSExpr::asMapSExpr() {
    return new MapSExpr(_c->map()->enclosing_mapOop(), _preg, _node);
  }

  SExpr* MergeSExpr::mapify(PReg* p, Node* n) {
    MergeSExpr* e = new MergeSExpr(p, n);
    for (fint i = 0; i < exprs->length(); i++) {
      SExpr* expr = exprs->nth(i)->mapify(p, n);
      e->add(expr);
    }
    // result is non-splittable because nodes aren't correct and expr->next
    // is ignored for the components of the receiver
    setSplittable(false);
    return e;
  }
  
  bool MergeSExpr::containsUnknown() {
    if (isUnknownSet()) {
      UnknownSExpr* u;
      assert((u = findUnknown()) == NULL && !isContainingUnknown() ||
             u != NULL && isContainingUnknown(), "isContainingUnknown wrong");
      return isContainingUnknown();
    }
    setUnknownSet(true);
    for (fint i = 0; i < exprs->length(); i++) {
      if (exprs->nth(i)->isUnknownSExpr()) {
        setContainingUnknown(true);
        return true;
      }
    }
    setContainingUnknown(false);
    return false;
  }
  
  UnknownSExpr* MergeSExpr::findUnknown() {
    for (fint i = 0; i < exprs->length(); i++) {
      if (exprs->nth(i)->isUnknownSExpr()) return (UnknownSExpr*)exprs->nth(i);
    }
    return NULL;
  }

  SExpr* MergeSExpr::findMap(mapOop map) {
    for (fint i = 0; i < exprs->length(); i++) {
      SExpr* e = exprs->nth(i);
      if (e->hasMap() && e->myMapOop() == map) return e;
    }
    return NULL;
  }

  SExpr* UnknownSExpr::makeUnknownUnlikely(SSelfScope* s)    {
    Unused(s);
    assert(SICDeferUncommonBranches, "shouldn't make unlikely");
    // called on an UnknownSExpr itself, this is a no-op; works only
    // with merge exprs
    return this;
  }

  SExpr* MergeSExpr::makeUnknownUnlikely(SSelfScope* s) {
    assert(SICDeferUncommonBranches, "shouldn't make unlikely");
    unlikelyScope = s; unlikelyBCI = s->bci();
    for (fint i = 0; i < exprs->length(); i++) {
      SExpr* e;
      if ((e = exprs->nth(i))->isUnknownSExpr()) {
        if (!((UnknownSExpr*)e)->isUnlikely()) {
          // must make a copy to avoid side-effecting e.g. incoming arg
          UnknownSExpr* u = (UnknownSExpr*)e;
          UnknownSExpr* new_u = new UnknownSExpr(u->preg(), u->node(), true);
          exprs->nthPut(i, new_u);
          for (u = (UnknownSExpr*)u->next; u;
               u = (UnknownSExpr*)u->next, new_u = (UnknownSExpr*)new_u->next){
            new_u->next = new UnknownSExpr(u->preg(), u->node(), true);
          }
        }
        return this;
      }
    }
    ShouldNotReachHere(); // contains no unknown expression
    return NULL;
  }

  SExpr* ConstantSExpr::findMap(mapOop m) { return myMapOop() == m ? this : NULL; }
  
  SExpr* UnknownSExpr::shallowCopy(PReg* p, Node* n) {
    return new UnknownSExpr(p, n);
  }

  SExpr* NoResultSExpr::shallowCopy(PReg* p, Node* n) {
    Unused(p); Unused(n);
    return new NoResultSExpr();
  }

  SExpr* MapSExpr::shallowCopy(PReg* p, Node* n) {
    return new MapSExpr(myMapOop(), p, n);
  }

  SExpr* ClonedBlockSExpr::shallowCopy(PReg* p, Node* n) {
    return new ClonedBlockSExpr(myMapOop(), p, n, _blockScope);
  }

  SExpr* ConstantSExpr::shallowCopy(PReg* p, Node* n) {
    return new ConstantSExpr(_c, p, n);
  }

  SExpr* MergeSExpr::shallowCopy(PReg* p, Node* n) {
    MergeSExpr* e = new MergeSExpr(p, n);
    e->exprs = exprs;
    e->setSplittable(isSplittable());
    return e;
  }

  SSelfScope* SExpr::scope() { return _preg->scope; }

  NameNode* SExpr::nameNode(bool mustBeLegal) {
    return preg()->nameNode(mustBeLegal); }
  
  NameNode* ConstantSExpr::nameNode(bool mustBeLegal) {
    Unused(mustBeLegal);
    return newValueName(constant()); }

  void SExpr::print_expr(const char* type) {
    lprintf(" (Node %#lx)", (unsigned long)node());
    if (next) lprintf(" (next %#lx)", (unsigned long)next);
    lprintf("    p *(%s*)%#lx\n", type, (unsigned long)this);
  }
  
  void UnknownSExpr::print() {
    lprintf("UnknownSExpr %s", isUnlikely() ? "unlikely" : "");
    SExpr::print_expr("UnknownSExpr");
  }

  void NoResultSExpr::print() { SExpr::print_expr("NoResultSExpr"); }

  void ConstantSExpr::print() {
    lprintf("ConstantSExpr "); constant()->print_oop(); SExpr::print_expr("ConstantSExpr");
  }

  void MapSExpr::print() {
    lprintf("MapSExpr "); myMapOop()->print_oop(); SExpr::print_expr("MapSExpr"); 
  }

  void ClonedBlockSExpr::print() {
    lprintf("ClonedBlockSExpr "); myMapOop()->print_oop(); SExpr::print_expr("ClonedBlockSExpr"); 
  }

  void MergeSExpr::print() {
    lprintf("MergeSExpr %s(", isSplittable() ? "splittable " : "");
    for (fint i = 0; i < exprs->length(); i++) {
      lprintf("%#lx%s ", (unsigned long)(exprs->nth(i)),
             exprs->nth(i)->next ? "*" : "");
    }
    lprintf(")");
    SExpr::print_expr("MergeSExpr");
  }

  void SExprStack::push(SExpr* e) {
    // set r's startBCI if it is an expr stack entry and not already set
    PReg* r = e->preg();
    if (r->isSAPReg()) {
      SAPReg* sr = (SAPReg*)r;
      if (sr->scope == scope) {
        if (sr->startBCI == IllegalBCI)
          sr->startBCI = sr->creationStartBCI = scope->bci();
      }
    }
    SExprBList::push(e);
  }

  SExpr* SExprStack::pop() {
    SExpr* e = SExprBList::pop();
    PReg* r = e->preg();
    if (r->isSAPReg()) {
      SAPReg* sr = (SAPReg*)r;
      if (sr->scope == scope) {
        // endBCI may be assigned several times, e.g. in "x foo: 1" where
        // foo is an assignment slot
        fint newBCI =
          scope->bci() == EpilogueBCI ? scope->ncodes - 1 : scope->bci();
        if (bciLT(sr->endBCI, newBCI)) sr->endBCI = newBCI;
      } else {
        assert(sr->scope->isSenderOf(scope), "preg scope too low");
      }
    }
    return e;
  }
  
  
  BranchBCTargetStack* BranchBCTargetStack::new_BranchBCTargetStack(
              SCodeScope* s,
              bool  isTargetOfBackwardsBranch,
              int32 len,
              int32 targetBCI ) {
    if ( len == 0 ) {
      // empty: no problem
    } 
    else if ( s->bci() >= s->ncodes ) {
      // branch to return: no problem
    }
    else if ( s->method_map->start_codes()[s->bci()]
                 == BuildCode( NO_OPERAND_CODE, NONLOCAL_RETURN_CODE) ) {
      // branch to non-local return: no problem
    } 
    else {
      warning1("SIC found non-empty expression stack at branch target"
               "in method %s; "
               "The debug information will be incorrect.--dmu 9/96",
               stringOop(s->selector())->copy_null_terminated());
    }
    return  ( isTargetOfBackwardsBranch
             ?  (BranchBCTargetStack*) new BBranchBCTargetStack( len)
             :  (BranchBCTargetStack*) new FBranchBCTargetStack( len)
             )->initialize(len, s, targetBCI);
  }


  BranchBCTargetStack::BranchBCTargetStack(int32 len)
    : SExprBList(len) {}
    
    
  BranchBCTargetStack* BranchBCTargetStack::initialize(
                         int32 len,
                         SCodeScope* s,
                         int32 targetBCI) {
    // no virts in constructor, argh!
    assert(length() == 0, "should be empty");
    for ( int32 i = 0;  i < len;  ++i)
      append( new_expr(s, targetBCI) );
    return this;
  }


  void BranchBCTargetStack::mergeInExprsFromStack( 
              SExprStack* stk,
              Node*       mergeNode,
              bool        isBackwards ) {
    // bring values from exprStack into me
    assert(stk->length() == length(), "lengths must match");
    for (int32 i = 0;  i < length();  ++i)
      nthPut( i,  mergeInExpr( nth(i), stk->nth(i), mergeNode, isBackwards ) );
  }
  

  SExpr* FBranchBCTargetStack::mergeInExpr( 
             SExpr*  myExpr, 
             SExpr*  stkExpr,
             Node*   mergeNode,
             bool    ) {
    theNodeGen->move(stkExpr->preg(), myExpr->preg());
    return myExpr->copyMergeWith( stkExpr, myExpr->preg(), mergeNode);
  }

  
  // recall that splitting cannot handle backwards branches
  
  SExpr* BBranchBCTargetStack::mergeInExpr( 
             SExpr*  myExpr, 
             SExpr*  stkExpr,
             Node*   ,
             bool    forBackwardsBranch ) {
    theNodeGen->move(stkExpr->preg(), myExpr->preg());
    return forBackwardsBranch
      ?  myExpr // too late to change myExpr, but already put in UnknownSExpr
      :  myExpr->copyMergeWith(stkExpr,
                               myExpr->preg(),
                               NULL); // NULL so is not splittable
  }

  
  SExpr* FBranchBCTargetStack::new_expr(SCodeScope* s, int32 targetBCI) {
    // I changed lifetime of new SAPReg from default (s->bci())
    // to targetBCI.  
    // Actually targetBCI - 1 because lifetime is ]startBCI, endBCI].
    // This fixed a bug: for indexed bytecodes
    // the default lifetimes makes the SIC think register is
    // live (allocated) at the indexed-branch bytecode.
    // But it is not assigned-to unless the branch for that case
    // is taken. The result is that the SIC thinks the register is live
    // when it has not been assigned to. It is marked live in the
    // register mask of an inline-cache, and it scavenged, even though
    // it has garbage in it. Changing the lifetime fixes it. 
    // -- dmu 11/17/96
    return new MergeSExpr(
                 new SAPReg(s, targetBCI - 1 ), 
                 // SExpr for type of res of this bc
                 s->branchTargets[s->bci()] );
  }
  
  
  SExpr* BBranchBCTargetStack::new_expr(SCodeScope* s, int32 targetBCI) {
    // Here put in UnknownSExpr, cause will not know
    // types of values coming in backwards in time for
    // node generation.
    // Later, in mergeInExpr, will make
    // a merge expr with NULL node to turn off splitting.
    // Urs says splitting is unprepared to deal with the topologies
    // of a backwards branch (a branch below the center of the Y). -- dmu
    
    // Even though this is a backwards branch STACK,
    // it could be a stack for a branch target that is reached
    // both by a forwards indexed branch and a backwards whatever
    // branch. So do the same lifetime setting as above. -- dmu
    return new UnknownSExpr(new SAPReg(s, targetBCI - 1));
  }
  
  
  void BranchBCTargetStack::moveExprsToStack( SExprStack* stk ) {
    // move my exprs to the stk
    for (fint i = 0;  i < length();  ++i)
      stk->pop();
    for (fint j = 0;  j < length();  ++j)
      stk->push(nth(j));
  }
  
# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

#ifdef SIC_COMPILER

# pragma implementation "slist.hh"
# include "_slist.cpp.incl"
  
  SListElem* SList::findL(void* p) {
    for (SListElem* e = headL(); e; e = e->nextL()) {
      if (e->dataL() == p) return e;
    }
    return NULL;
  }
  
  SListElem* SList::findL(void* token, slistFindFn f) {
    for (SListElem* e = headL(); e; e = e->nextL()) {
      if (f(token, e->dataL())) return e;
    }
    return NULL;
  }
  
  void* SList::nthL(fint n) {
    assert(n < length(), "non-existing element");
    SListElem* e = headL();
    for (fint i = 0; i < n; i++, e = e->nextL()) ;
    return e->dataL();
  }
  
  void SList::insertAfterL(SListElem* e, void* d) {
    if (e == tailL()) {
      appendL(d);
    } else if (e == NULL) {
      prependL(d);
    } else {
      SListElem* newe = new SListElem(d, e->nextL());
      e->setNextL(newe);
      _len++;
    }
  }

  void SList::removeAfterL(SListElem* e) {
    if (e == NULL) {
      removeHeadL();
    } else {
      SListElem* deletee = e->nextL();
      e->_next = deletee->nextL();
      if (deletee == tailL()) _tail = e;
      _len--;
    }
  }

  void SList::removeL(void* p) {
    SListElem *e, *prev = NULL;
    for (e = headL(); e && e->dataL() != p;
         prev = e, e = e->nextL()) ;
    if (e == NULL) fatal("not in list");
    removeAfterL(prev);
    assert(!includesL(p), "remove doesn't work");
  }
  
  void SList::applyL(void f(void*)) {
    SListElem* nexte;   // to permit removing during iteration
    for (SListElem* e = headL(); e; e = nexte) {
      nexte = e->nextL();
      f(e->dataL());
    }
  }

  void SList::print_short() { lprintf("SList %#lx", (unsigned long)this); }

  static void print_them(void* p) {
    ((VMObj*)p)->print_short(); lprintf(" ");
  }
  
  void SList::print() {
    print_short(); lprintf(": ");
    applyL(print_them);
  }
# endif
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef SIC_COMPILER

# pragma implementation "tracing.hh"
# include "_tracing.cpp.incl"

  TraceControl trc;

  static bool have_ranges = false;

  static void all_mode() {
    // trace all instructions
    trc.niranges = 1;
    trc.irange[0] = const_cast<char*>( "all");
    trc.irange[1] = NULL;
    trc.irange[2] =( char*)-1;
  }
  
  static void self_mode() {
    // trace only Self & stubs
    trc.niranges = 0;

    // turn off tracing everywhere
    trc.irange[trc.niranges * 3    ] = NULL;
    trc.irange[trc.niranges * 3 + 1] = NULL;
    trc.irange[trc.niranges * 3 + 2] = (char*)-1;
    trc.niranges++;

    // nmethods
    trc.irange[trc.niranges * 3    ] = const_cast<char*>( "Self");
    trc.irange[trc.niranges * 3 + 1] = Memory->code->iZone->startAddr();
    trc.irange[trc.niranges * 3 + 2] = Memory->code->iZone->  endAddr();
    trc.niranges++;

    // PICs, count stubs
    trc.irange[trc.niranges * 3    ] = const_cast<char*>( "stubs");
    trc.irange[trc.niranges * 3 + 1] = Memory->code->stubs->zone()->startAddr();
    trc.irange[trc.niranges * 3 + 2] = Memory->code->stubs->zone()->  endAddr();
    trc.niranges++;
  }
  
#  ifndef NOASM

  oop itrace_prim(oop fncode, oop mode) {
    ResourceMark rm;
    if (!fncode->is_smi())
      return ErrorCodes::general_prim_error("fn code isn't a smi");
    smi fn = smiOop(fncode)->value();
    if (fn >= 0 && fn <= TraceFinish) {
      // ok
    } else {
      return ErrorCodes::general_prim_error("fn code has bad value");
    }

    switch (fn) {
     case TracePrint:
      // "mode" really is a string to print
      if (mode->is_byteVector()) {
        trc.irange[0] = byteVectorOop(mode)->copy_null_terminated();
      } else {
        return ErrorCodes::general_prim_error("arg must be a byte vector");
      }
      break;
     default:
      if (mode == as_smiOop(0)) {
        all_mode();
      } else if (mode == as_smiOop(1)) {
        self_mode();
      } else {
        return ErrorCodes::general_prim_error("arg must be 0 or 1");
      }
    }
    itrace(fn);
    return fncode;
  }

  void itrace(fint fn) {
    trc.magic = MAGIC;
    trc.fn = TraceFun(fn);
    trc.ndranges = 0;

    if (!have_ranges && fn == TraceOn) {
      // old space
      trc.drange[trc.ndranges * 3    ] = const_cast<char*>( "old space");
      trc.drange[trc.ndranges * 3 + 1] = Memory->old_gen->low_boundary;
      trc.drange[trc.ndranges * 3 + 2] = Memory->old_gen->high_boundary;
      trc.ndranges++;

      // survivor spaces
      trc.drange[trc.ndranges * 3    ] = const_cast<char*>( "from space");
      trc.drange[trc.ndranges * 3 + 1] = Memory->new_gen->from_space->spaceStart();
      trc.drange[trc.ndranges * 3 + 2] = Memory->new_gen->from_space->spaceEnd();
      trc.ndranges++;
      trc.drange[trc.ndranges * 3    ] = const_cast<char*>( "to space");
      trc.drange[trc.ndranges * 3 + 1] = Memory->new_gen->to_space->spaceStart();
      trc.drange[trc.ndranges * 3 + 2] = Memory->new_gen->to_space->spaceEnd();
      trc.ndranges++;

      // eden
      trc.drange[trc.ndranges * 3    ] = const_cast<char*>( "eden space");
      trc.drange[trc.ndranges * 3 + 1] = Memory->new_gen->eden_space->spaceStart();
      trc.drange[trc.ndranges * 3 + 2] = Memory->new_gen->eden_space->spaceEnd();
      trc.ndranges++;
      have_ranges = true;
    }
    
    // send data to analyzer and wait for ack
    trc.ack = 0;
    for (fint i = 0; !trc.ack && i < 2 * NTRBUF; i++) {
# if TARGET_ARCH == SPARC_ARCH
      asm volatile ("taddcctv %0, %%g0, %%g0"
                    : /* no output */
                    : "r" (&trc)
                    : "memory", "cc");
# else
      fatal("shade tracing unimplemented on this platform");
# endif
      currentSPLimit();    // gcc bug - ignores "memory" in asm directive (2.3.3)
    }
    if (!trc.ack) {
      error("didn't get any ACK from analyzer - no analyzer running?");
    }
  }

#  endif
# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "expr.hh"
# include "_expr.cpp.incl"

Expr* Expr::AddArg(ArgSlot* arg, Parser* parser) {
  Unused(arg); Unused(parser);
  ShouldNotReachHere(); // turning a constant expression into a method: no longer supported
  return 0;
}

Expr* Expr::AddArgs(ArgSlotList* args, Parser* parser) {
  Unused(args); Unused(parser);
  ShouldNotReachHere(); // turning a constant expression into a method: no longer supported
  return 0;
}

oop Expr::Eval(bool printing, bool inSlot, bool mustAllocate) {
  Unused(inSlot);
  
  slotsOop doItMethod;
  { ResourceMark m;             // release resources after compile
    
    if (PrintParsedExpressions) {
      Print();
      lprintf("\n");
    }
    
    ByteCode b(mustAllocate);
    if (! GenByteCodes(&b, 0, false) || NLRSupport::have_NLR_through_C()) return badOop;
    if (!b.Finish(source_file, source_line, source_start, source_length)) {
      ErrorMessage(b.errorMessage);
      return badOop;
    }

    doItMethod = create_outerMethod(0, &b);

    if (Trace) {
      lprintf("evaluating expr in context of lobby\n");
    }
  }
  
  oop res = currentProcess->runDoItMethod(Memory->lobbyObj, doItMethod);

  if (printing && res != badOop && !postReadSnapshot) {
    res->print();
  }
  return res;
}


void Expr::ErrorMessage(const char* msg) {
  parser->syntaxError(msg, source_line, source_column);
}

void Pop::Print() {
  lprintf("pop");
}

bool Pop::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  b->GenPopByteCode(position_in_method(parent), source_length); 
  return true; 
}


void Self::Print() {
  lprintf("self");
}

bool Self::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  b->GenSelfByteCode(position_in_method(parent), source_length); 
  return true; 
}


bool Constant::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  b->GenLiteralByteCode( position_in_method(parent), source_length, value);
  return true;
}


void Integer::Print() {
  lprintf("%ld", (void*)long(smiOop(value)->value()));
}

void Float::Print() {
  char buf[100];
  sprintf(buf, "%g", floatOop(value)->value());
  lprintf(buf);
}

void StringLiteral::Print() {
  lprintf("'");
  stringOop(value)->string_print();
  lprintf("'");
}

Object::Object(SlotList*   slots_arg, 
               ExprList*   body_arg, 
               bool        isDeferred_arg,
               const char* source_file_arg, 
               fint        source_line_arg,
               fint        source_column_arg,
               const char* source_start_arg,
               fint        source_length_arg,
               fint        source_body_line_arg,
               fint        source_body_column_arg,
               const char* source_body_start_arg,
               fint        source_body_length_arg,
               Parser*     p)
   : Constant(0, p) {
     slots      = slots_arg;
     body       = body_arg;
     isDeferred = isDeferred_arg; 
     annotation = "";

     //including slots and parenthesis
     source_file   = source_file_arg; 
     source_line   = source_line_arg;
     source_column = source_column_arg;
     source_start  = source_start_arg;
     source_length = source_length_arg;

     //only body part
     source_body_start  = source_body_start_arg;
     source_body_length = source_body_length_arg;
     source_body_line   = source_body_line_arg;
     source_body_column = source_body_column_arg;
}

Expr* Object::AddArg(ArgSlot* arg, Parser* parser) {
  slots = slots->Append(arg, parser);
  return slots ? this : NULL;
}

Expr* Object::AddArgs(ArgSlotList* args, Parser* parser) { 
  return args->AddArgs(this, parser);
}


void Object::oops_do(oopsDoFn f) {
  Constant::oops_do(f);
  slots->oops_do(f);
  body->oops_do(f);
}

bool Object::GenBody(Object* parent) {
  // Make the slot list
  slotList* s;
  if (! slots->make_list(s)) { // this can run Self code!
    LOG_EVENT("couldn't construct object literal");
    ErrorMessage("couldn't construct object literal");
    return false;
  }

  if (body->Length()) {
    // Create block or method.
    preservedObj ps(s); // since GenByteCodes can cause a gc,
                        // preserve the generated slotList.
    ByteCode b(true); // I am not sure that this method can handle space failure
    
    if ( !body->GenByteCodes(&b, this, false)
    ||  NLRSupport::have_NLR_through_C()) 
      return false;

    if (isDeferred) {
      if (parent) {
        if ( !b.Finish(source_file, source_body_line, 
                       source_body_start - parent->source_body_start,
                       source_body_length)) {
           parser->syntaxError( b.errorMessage, 
                                source_body_line, source_body_column);
           return false;
         }
      }
      else {
        if ( !b.Finish(source_file, source_body_line, 
                       source_body_start, source_body_length)) {
           parser->syntaxError( b.errorMessage, 
                                source_body_line, source_body_column);
           return false;
        }
      }
      value = create_block(create_blockMethod(s,  &b, annotation));
    } else {
      if ( !b.Finish(source_file, source_body_line, 
                     source_body_start, source_body_length)) {
           parser->syntaxError( b.errorMessage, 
                                source_body_line, source_body_column);
        return false;
      }
      value = create_outerMethod(s, &b, annotation);
    }
  } else {
    // Create plain object.
    value = create_slots(s, annotation);
    assert(value->arg_count() == 0,
           "plain objects cannot have arguments");
    if (isDeferred) {
      // a deferred object (e.g. [])
      s = new slotList(VMString[PARENT], parent_map_slotType,
                       Memory->blockTraitsObj);
      s = s->add(VMString[VALUE], map_slotType, value);
      value = create_slots(s, annotation);
    }
  }

  return true;
}


bool Object::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  if (NLRSupport::have_NLR_through_C()) return false;
  if (IsMethod()) {
    if (   ! body->GenByteCodes(b, isExpr ? parent : this)
          || NLRSupport::have_NLR_through_C()) 
            return false;
  } else {
    // This is a Self object add the object to the literal vector
    // Do not make a real block for the position table version--
    //   That is why GenBody is conditionally called.
    if ( b->isPositionTable())
      value= Memory->objVectorObj->cloneSize(0);
    else if ( !GenBody(parent))
      return false;
    b->GenLiteralByteCode( position_in_method(parent), source_length, value );
  }
  return true;
}


void Object::Print() {
  lprintf(isDeferred ? "[ " : "( ");
  if (slots->Length()) {
    lprintf("| ");
    slots->Print();
    lprintf("| ");
  }
  
  body->Print();
  lprintf(isDeferred ? "]" : ")");
}

void Return::Print() {
  lprintf("^ ");
  result->Print();
}

bool Return::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  if (! result->GenByteCodes(b, parent) || NLRSupport::have_NLR_through_C()) return false;
  b->GenNonLocalReturnByteCode( position_in_method(parent), source_length);
  return true;
}

bool Object::ContainsMethod() {
  SlotListElement* s;
  for (s = slots->Head(); s && s->Data(); s = s->Next()) {
    if (s->Data()->IsMethodSlot()) return true;
  }
  assert(!s || !s->Next(), "empty element in list");
  return false;
}

static void print_comment_warning(const char* msg, 
                                  Slot* s, Token* comment, const char* file) {
  Unused(s);
  lprintf("%s\n", msg);
  lprintf("%s:%d \"", file, comment->line);
  comment->string->Print(); lprintf("\"\n");
}

static char* extend_annotation(const char* anno, const char* keyword, const char* str, int len) {
  const char* separator = "\177";
  fint prefix_len = strlen(anno) ? strlen(anno) + strlen(separator) : 0;
  char *new_anno = NEW_RESOURCE_ARRAY( char, 
                                      prefix_len
                                      + strlen(keyword) 
                                      + len + 1);
  char *buf = NEW_RESOURCE_ARRAY(char, len + 1);
  lsprintf_string(buf, len, str);
  sprintf(new_anno, "%s%s%s%s", 
          anno, prefix_len ? separator : "" , keyword, buf);
  return new_anno;
}

static char* extend_annotation(const char* anno, const char* keyword, Token* string) {
  return extend_annotation(anno, keyword, 
                           string->string->AsCharP(), string->string->len);}
char* extend_annotation_with_string(const char* annotation, Token* string) {
  return extend_annotation(annotation, "", string); }
char* extend_annotation_with_comment(const char* annotation, Token* comment) {
  return extend_annotation(annotation, "Comment: ", comment); }
char* extend_annotation_with_filename(const char* annotation, const char* name) {
  return extend_annotation(annotation, "File: ", name, strlen(name)); }

void Object::addCommentAnnotations(Scanner* scanner) {
  TokenList* commentList = scanner->commentList;
  for(SlotListElement* s = slots->Head(); s && s->Data(); s = s->Next()) {
    Slot* sl =  s->Data();

    if (AddFileAnnotationsToSlots) {
      sl->annotation = 
        extend_annotation_with_filename(sl->annotation, scanner->fileName());
    }

    if (ConvertCommentsIntoAnnotations) {
      if (sl->startToken) {
        if (WizardMode) {
          // Print out the slot information
          lprintf("%d>> ", sl->startToken->line); 
          sl->name->Print();
          lprintf("\n");
        }
        Token* t = commentList->isEmpty() ? NULL :commentList->first();
        while (t && t->line <= sl->startToken->line) {
          // Check if the comment token should be promoted to a slot annotation.
          if (t->line < sl->startToken->line) {
            if (t->string->line_count() + t->line == sl->startToken->line) {
              // ex. "Yet another method"
              //     fisk = (fugl * flodhest).
              sl->annotation = extend_annotation_with_comment(sl->annotation, t);
            } else { 
              // In case we cannot find a slot for the comment a warning is
              // printed out and the comment is added to the object annotation.
              print_comment_warning(
                                    "Warning: comment is added as the object annotation",
                                    sl, t, source_file);
              annotation = extend_annotation_with_comment(annotation, t);
            }
          } else {
            // ex. "_" fisk = (12 + 12). "Return an awesome number"
            sl->annotation = extend_annotation_with_comment(sl->annotation, t);
          }
          commentList->remove(t);
          t = commentList->isEmpty() ? NULL :commentList->first();
        }
      }
    }
    sl->addCommentAnnotations(scanner);
  }
  body->addCommentAnnotations(scanner);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stringUtil.hh"
# include "_stringUtil.cpp.incl"

fint str_arg_count(const char* s) {
  char c = *s++;
  assert(c != '\0', "shouldn't be testing the empty string");
  if (!is_id_alpha(c)) {
    return 1;
  }
  fint argc = 0;
  for (;;) {
    c = *s++;
    if (c == '\0') {
      break;
    } else if (c == ':') {
      argc ++;
    }
  }
  return argc;
}

fint str_arg_count(const char* s, fint len) {
  assert(len > 0, "should have a positive length");
  char c = *s;
  if (!is_id_alpha(c)) {
    return 1;
  }
  if ( s[ len-1 ] != ':' )  return 0; // an optimization
  fint argc = 1;
  for (const char* ss = s + len - 3; // last is :, next-to-last is alpha
       ss > s;                 // do not need to look at first one
       --ss ) {
    if (*ss == ':') {
      argc ++;
    }
  }
  return argc;
}

bool str_is_slot_name(const char* s, fint len) {
  assert(len >= 0, "shouldn't be negative length");
  if (len == 0) {
    return false;
  }
  char c = *s;
  if (!is_lower(c)) {
    if (!is_punct(c))     return false;
    switch (c) {
     case '^':      case '|':       case '\\':   case '.':
       if (len == 1) return false;
    }
    for (int i = 0;  i < len; ) {
      c = s[i++];
      if (! is_punct(c)) return false;
      switch (c) {
       case '(':  case ')':  case '\'':  case '\"':  case ':':  
       case '[': case ']':
        return false;
      }
    }
    return true;
  }
  for (int i = 1;  i < len;  ) {
    c = s[i++];
    if (is_id_char(c))   continue;
    if (c != ':')        return false;
    if (i == len)        return true;  // this was final ":"
    if (!is_upper(s[i])) return false; // after ":" must be uppercase
    if (s[len-1] != ':') return false; // one ":" -> last is ":"
  }
  return true;
}

bool str_is_prim_name(const char* s) { return str_is_prim_name(s, strlen(s)); }

bool str_is_prim_name(const char* s, fint len) {
  return len >= 1  &&  s[0] == '_';
}

bool str_has_IfFail(const char* s) { return str_has_IfFail(s, strlen(s)); }

bool str_has_IfFail(const char* s, fint len) {
  return len >= 7  &&  strncmp(s + len - 7, "IfFail:", 7) == 0;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "str.hh"
# include "_str.cpp.incl"

void String::Print() {
  smi   n = 0, i;
  const char *s = str;
  while (n < len) {            // loop to handle all '\0' in string.
    lprintf("%s", s);
    int r = strlen(s);
    i = r + 1;
    n += i;
    s += i;
    if (n < len)
      lprintf("\\0");
  }
}

fint String::line_count() {
  fint count = 1;
  for (const char *s = str;  *s;  s++)   if (*s == '\n') count++;
  return count;
}

String* StringList::AsSelector() {
  smi len = 0;
  StringListElement* e;
  for (e = Head(); e; e = e->Next()) {
    len += strlen(e->Data()->AsCharP());
  }
  char* buffer = NEW_RESOURCE_ARRAY( char, len+1);
  char* b = &buffer[0];
  for (e = Head(); e; e = e->Next()) {
    for (const char* d = e->Data()->AsCharP();  *d;  b++, d++) {
      *b = *d;
    }
  }
  *b = '\0';
  return new String(buffer);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "labelSet.hh"
# pragma implementation "labelSet_inline.hh"
# include "_labelSet.cpp.incl"


LabelSet::LabelSet() {
  maxIndex = 32; 
  index = 0;
  allocateArrays();
}


LabelInfo* LabelSet::at( oop label) {
  for ( int32 i = 0;  i < index; ++i)
    if ( labels[i]->label == label )
      return labels[i];
  return NULL;
}


// called by label def, simple branch, indexed branch
// defines label record, allocates literal if needed,
//  checks stack_depth
// returns literal index (or 0 if not needed) or -1 for error
 
int32 LabelSet::RecordLabelInfo( AbstractByteCode* b, oop label, int32 sd, 
                              LabelOccurrence why,  int32 bci) {
  static char buf[BUFSIZ];                       
  LabelInfo* li= at(label);
  if ( li == NULL ) {
    if (index == maxIndex)
      grow();
    labels[index++] = li= new LabelInfo(label, sd, bci, -1);
    li->isDefined = why == defineLabel;
  }
  else {
    if ( li->stack_depth != sd ) {
      b->errorMessage = buf;
      char psb[BUFSIZ];
      label->print_string(psb);
      sprintf(buf,
              "Label %s has stack depth %d at bytecode %d, "
              "but has differing stack depth %d at bytecode %d",
              psb,
              li->stack_depth, li->bci,
              sd, bci);
      return -1;
    }
    if ( why == defineLabel ) {
      if ( li->isDefined ) { 
        b->errorMessage = buf;
        char psb[BUFSIZ];
        label->print_string(psb);
        sprintf(buf,
          "Label %s is multiply defined at bytecodes %d and %d",
          psb,
          li->bci, bci);
        return -1;
      }
      li->bci = bci;
      li->isDefined = true;
    }
  }
  // dont alloc lit for label def, might be used in case only
  if (li->literalIndex == -1  &&  why == simpleBranch)
    li->literalIndex= b->GenLabelLiteral();
  return why == simpleBranch  ?  li->literalIndex  :  0;
}


// called for indexed branch; check labels, gen pc vector

int32 LabelSet::RecordLabelVectorInfo( AbstractByteCode* b, objVectorOop labels,
                                       int32 sd, int32 bci) {
  objVectorOop pcs= labels->clone(); // want same size as labels
  for (int32 i = 0,  n = labels->length();  i < n;  ++i)
    if ( RecordLabelInfo( b, labels->obj_at(i), sd, indexedBranch, bci ) == -1)
      return -1;
  return b->GenLiteral(pcs);
}  


void LabelSet::grow() {
  maxIndex <<= 1;
  LabelInfo** ol = labels;
  allocateArrays();
  for ( int32 i = 0;  i < index; ++i ) {
    labels[i] = ol[i];
  }
}



bool LabelSet::ResolveLabel( AbstractByteCode* b,
                             oop lbl, objVectorOop pcs, int32 i) {
  static char buf[BUFSIZ];
  LabelInfo* li = at(lbl);
  if ( li == NULL  ||  !li->isDefined) { 
      char psb[BUFSIZ];
      lbl->print_string(psb);
      b->errorMessage = buf;
      sprintf(buf, "Label %s is undefined", psb);
    return false;
  }
  pcs->obj_at_put(i, as_smiOop(li->bci));
  return true;
}


void LabelSet::oops_do(oopsDoFn f) {     
  for (int32 i = 0;  i < index;  ++i)
    (*f)(&labels[i]->label); 
  resort();
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "send.hh"
# include "_send.cpp.incl"

# define check(e)                                                             \
    if ((e) == false) return false
    
# define checkMsg(b, e) \
  if (!(e)) { \
    ErrorMessage(b->errorMessage); \
    return false; \
  } else /* else is for semicolon */ \
   (void)0 /* silence unwanted ; warning */
  
  
void Send::PrintDelegatee() {
  if (del != 0) lprintf("%s.", del->AsCharP());
}

bool Send::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  // this is a pain in the ***
  // if (receiver && receiver->IsSelf()) {
  //  warning("sending messages to explicit \"self\" is considered bad style");
  // }
  const char* msg= message->AsCharP();
  
  stringOop selector = new_string(msg);
  
  if ( del  &&   receiver ) {
    ErrorMessage("resends must be to implicit self");;
    return false;
  }

  bool isUndirectedResend = false;
  stringOop delegatee = NULL;
  if (!del)
    ;
  else if ( del->len == 0  ||  strcmp(del->AsCharP(), "resend") == 0 )
      isUndirectedResend = true;
  else
    delegatee = new_string(del->AsCharP());
    
  b->GenSendByteCode( position_in_method(parent),
                      source_length,
                      selector,
                      receiver == 0,
                      isUndirectedResend,
                      delegatee );
  return true;
}


void Send::addCommentAnnotations(Scanner* scanner) {
  if (receiver) receiver->addCommentAnnotations(scanner);
}


// The branches need a value to be the result
// (for the sake of consistency with everything else).
// Let the receiver expression be the result.
// If elided, generate a push self bytecode.

bool Send::GenReceiverForPseudoPrim(AbstractByteCode* b, Object* parent) {
  if ( receiver )
    return  receiver->GenByteCodes(b, parent);
  b->GenSelfByteCode(position_in_method(parent), 0);
  return true;
}


void Unary::Print() {
  if (receiver) {
    bool parens = receiver->IsBinary() || receiver->IsKeyword();
    if (parens) lprintf("(");
    receiver->Print();
    if (parens) lprintf(")");
    lprintf(" ");
  }
  PrintDelegatee();
  message->Print();
}

bool Unary::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  const char* msg= message->AsCharP();
  if (strncmp(msg, "__", 2) == 0) {
    ErrorMessage("illegal pseudo-primitive");
    return false;
  }
  if (receiver) check(receiver->GenByteCodes(b, parent));
  return Send::GenByteCodes(b, parent);
}

void Binary::addCommentAnnotations(Scanner* scanner) {
  Send::addCommentAnnotations(scanner);
  arg->addCommentAnnotations(scanner);
}

void Binary::Print() {
  bool parens;
  if (receiver) {
    parens = receiver->IsKeyword();
    if (parens) lprintf("(");
    receiver->Print();
    if (parens) lprintf(")");
    lprintf(" ");
  }
  PrintDelegatee();
  message->Print();
  lprintf(" ");
  parens = arg->IsKeyword() || arg->IsBinary();
  if (parens) lprintf("(");
  arg->Print();
  if (parens) lprintf(")");
}

bool Binary::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  if (receiver) check(receiver->GenByteCodes(b, parent));
  check(arg->GenByteCodes(b, parent));
  return Send::GenByteCodes(b, parent);
}

void Keyword::Print() {
  if (receiver) {
    bool parens = receiver->IsKeyword();
    if (parens) lprintf("(");
    receiver->Print();
    if (parens) lprintf(")");
    lprintf(" ");
  }
  PrintDelegatee();
  StringListElement* s = keywords->Head();
  ExprListElement* e = args->Head();
  for (; s; s = s->Next(), e = e->Next()) {
    s->Data()->Print();
    lprintf(" ");
    bool parens = e->Data()->IsKeyword();
    if (parens) lprintf("(");
    e->Data()->Print();
    if (parens) lprintf(")");
    if (s->Next()) lprintf(" ");
  }
}


bool Keyword::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  const char* msg= message->AsCharP();
   
  if (strcmp(msg, "__DefineLabel:") == 0) {
    return  GenLabelDefinition(b, parent);
  }
  if (strcmp(msg, "__BranchTo:") == 0) {
    return  GenBranch(b, parent, false, NULL, args->Tail()->Data());
  }
  if (strcmp(msg, "__BranchIfTrue:To:") == 0) {
    return GenBranch(b, parent, true,  args->Head()->Data(), 
                         args->Tail()->Data());
  }
  if (strcmp(msg, "__BranchIfFalse:To:") == 0) {
    return GenBranch(b, parent, false, args->Head()->Data(), 
               args->Tail()->Data());
  }
  static const char* casePrefix= "__BranchIndexedBy:To:";
  if (strncmp(msg, casePrefix, strlen(casePrefix)) == 0) {
    return GenIndexedBranch(b, parent);
  }
  if (strncmp(msg, "__", 2) == 0) {
    ErrorMessage("illegal pseudo-primitive");
    return false;
  }
  if (receiver) check(receiver->GenByteCodes(b, parent));
  check(  args->GenByteCodes(b, parent));
  return  Send::GenByteCodes(b, parent);
}


bool Keyword::GenLabelDefinition(AbstractByteCode* b, Object* parent) {
  check( GenReceiverForPseudoPrim(b, parent) );
  Expr* labelE= args->Head()->Data();
  oop labelOop= labelE->get_label();
  if ( labelOop == badOop)
    return false;
  checkMsg(b, b->GenLabelDefinition(labelOop));
  return true;
}


bool Keyword::GenBranch(AbstractByteCode* b, Object* parent,
                        bool cond, Expr* testMe, Expr* dst) {
  check( GenReceiverForPseudoPrim(b, parent) );
  oop dstLabel = dst->get_label();
  if (dstLabel == badOop) {
    return false;
  }
  if (!testMe) {
    checkMsg(b,  b->GenBranchByteCode( position_in_method(parent),
                                 source_length,
                                 dstLabel));
    return true;
  }
  checkMsg( b, testMe->GenByteCodes( b, parent));
  checkMsg( b,
         cond
            ?  b->GenBranchTrueByteCode( position_in_method(parent),
                                         source_length,
                                         dstLabel)
            :  b->GenBranchFalseByteCode( position_in_method(parent),
                                          source_length,
                                          dstLabel));
  return true;
}


bool Keyword::GenIndexedBranch(AbstractByteCode* b, Object* parent) {
  check( GenReceiverForPseudoPrim(b, parent) );
  int32 n = args->Length() - 1;
  if ( n < 1 ) {
    ErrorMessage("indexed branch needs index argument and at least one label argument");
    return false;
  }
  objVectorOop v= Memory->objVectorObj->cloneSize(n);
  Expr* testMe= args->Head()->Data();
  check( testMe->GenByteCodes( b, parent));
  ExprListElement* elm= args->Head()->Next();
  for (int32 i = 0;  i < n;  ++i, elm=elm->Next()) {
    oop dstLabel = elm->Data()->get_label();
    if (dstLabel == badOop) {
      return false;
    }
    v->obj_at_put(i, dstLabel);
  }
  checkMsg(b,  b->GenBranchIndexedByteCode( position_in_method(parent),
                                      source_length,
                                      v));
  return true;
}



void Keyword::addCommentAnnotations(Scanner* scanner) {
  Send::addCommentAnnotations(scanner);
  args->addCommentAnnotations(scanner);
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "byteCodes.hh"
# include "_byteCodes.cpp.incl"

// generation routines

void AbstractByteCode::GenLiteralByteCode(fint offset, fint length,
                                          oop literal) { 
  GenCode(offset, length,
          BuildCode(LITERAL_CODE, GenIndex(offset, length,
                                           GenLiteral(literal)))); 
        ++stack_depth;
}


void AbstractByteCode::GenUndirectedResendByteCode(fint offset, fint length) {
  GenCode(offset, length,
          BuildCode( NO_OPERAND_CODE, UNDIRECTED_RESEND_CODE));
}


void AbstractByteCode::GenDelegateeByteCode(fint offset, fint length, stringOop delegatee) {
  GenCode(offset, length,
          BuildCode(DELEGATEE_CODE, GenIndex(offset, length,
                                           GenLiteral(delegatee)))); 
}


void AbstractByteCode::GenSendByteCode( fint offset, fint length,
                                        stringOop selector, 
                                        bool isSelfImplicit,
                                        bool isUndirectedResend, 
                                        stringOop resendTarget) {
                                        
  if (resendTarget != NULL) {
    GenDelegateeByteCode(offset, length, resendTarget);
  }
  else if ( isUndirectedResend )
     GenUndirectedResendByteCode(offset, length);

  fint argc = selector->arg_count();
  if (GenArgCountBytecode  &&  argc != 0)
    GenCode(offset, length, 
            BuildCode( ARGUMENT_COUNT_CODE, 
                       GenIndex(offset, length, argc)));
    
  GenCode(offset, length,
          BuildCode( isSelfImplicit ? IMPLICIT_SEND_CODE : SEND_CODE, 
                     GenIndex(offset, length,
                              GenLiteral(selector))));
                              
  if (!isSelfImplicit)
    --stack_depth;
  stack_depth -= argc;
  assert(stack_depth >= 0, "negative stack?");
  ++stack_depth;
}


void AbstractByteCode::GenRWLocalByteCode( fint offset, fint length,
                                           bool isRead,
                                           int32 lexicalLevel,
                                           int32 index) {
  assert(lexicalLevel >= 0  && index >= 0, "just checkin'");
  if ( lexicalLevel > 0 )
    GenCode(offset, length,
            BuildCode(LEXICAL_LEVEL_CODE,
                      GenIndex( offset, length, lexicalLevel )));
  
  GenCode(offset, length,
          BuildCode(isRead ? READ_LOCAL_CODE : WRITE_LOCAL_CODE,
                    GenIndex( offset, length, index )));
                    
  if ( isRead )
    ++stack_depth;
}
  

void AbstractByteCode::GenSelfByteCode(fint offset, fint length) {
  GenCode(offset, length, BuildCode(NO_OPERAND_CODE, SELF_CODE));
  ++stack_depth;
}

void AbstractByteCode::GenPopByteCode(fint offset, fint length) {
  GenCode(offset, length, BuildCode(NO_OPERAND_CODE, POP_CODE));
  --stack_depth;
  assert(stack_depth >= 0, "negative stack?");
}

void AbstractByteCode::GenNonLocalReturnByteCode(fint offset, fint length) {
  GenCode(offset, length, BuildCode(NO_OPERAND_CODE, NONLOCAL_RETURN_CODE));
}       


bool AbstractByteCode::GenSimpleBranchByteCode( fint offset,
                                                fint length,
                                                oop label,
                                                ByteCodeKind op ) {
  int32 literalIndex = labelSet->RecordLabelInfo( this, label, 
                                                  stack_depth, simpleBranch,
                                                  bci() );
  if ( literalIndex == -1 )
    return false;
  branchSet->RecordBranch( false, literalIndex, label );
  GenCode(offset, length, BuildCode(op, 
    GenIndex( offset, length, literalIndex)));
  return true;
}
                                       

bool AbstractByteCode::GenBranchByteCode( fint offset, 
                                          fint length,
                                          oop label) {
  bool r= GenSimpleBranchByteCode( offset, length, label, BRANCH_CODE);
  return r;
}


bool AbstractByteCode::GenBranchTrueByteCode( fint offset, 
                                              fint length,
                                              oop label) {
  
  --stack_depth;
  assert(stack_depth >= 0, "negative stack?");
  if ( !GenSimpleBranchByteCode( offset, length, label, BRANCH_TRUE_CODE))
    return false;
  return true;
}


bool AbstractByteCode::GenBranchFalseByteCode( fint offset, 
                                               fint length,
                                               oop label) {
  --stack_depth;
  assert(stack_depth >= 0, "negative stack?");
  if ( !GenSimpleBranchByteCode( offset, length, label, BRANCH_FALSE_CODE))
    return false;
  return true;
}


bool AbstractByteCode::GenBranchIndexedByteCode( fint offset, 
                                                 fint length,
                                                 objVectorOop labels) {
                                                 
  --stack_depth;
  assert(stack_depth >= 0, "negative stack?");
  int32 literalIndex = 
    labelSet->RecordLabelVectorInfo( this, labels, stack_depth, bci() );
  if ( literalIndex == -1 )
    return false;
  branchSet->RecordBranch( true, literalIndex, literals->obj_at(literalIndex) );
  GenCode(offset, length, BuildCode(BRANCH_INDEXED_CODE, 
    GenIndex( offset, length, literalIndex)));
  return true;
}
 

void AbstractByteCode::GenInstructionSetSelectionByteCode( fint offset, fint length,
                                                           InstructionSetKind k) {
  assert(0 <= k  &&  k <= LAST_INSTRUCTION_SET,  "bad instruction set");
  GenCode(offset, length, BuildCode(INSTRUCTION_SET_SELECTION_CODE, k));
}


bool AbstractByteCode::GenLabelDefinition( oop label) {
  return 
    -1  !=  labelSet->RecordLabelInfo( this, label, stack_depth, 
                                       defineLabel, bci());
}


bool AbstractByteCode::ResolveBranches() {
  return branchSet->ResolveBranches(this, labelSet, literals);
}


// generation helpers

fint AbstractByteCode::GenIndex(fint offset, fint length, fint x) {
  return x <= MAXINDEX ? x : GenExtendedIndex(offset, length, x);
}
  
void ByteCode::GenCode(fint offset, fint length, fint c) {
  Unused(offset); Unused(length);
  if (codeIndex == maxCodeIndex) {
    maxCodeIndex *= 2;
    codes= codes->cloneSize(maxCodeIndex);
  }
  codes->byte_at_put(codeIndex++, (char)c);
}

fint AbstractByteCode::GenLiteral(oop p) {
  for (fint i = 0; i < literalIndex; i ++) {
    if (literals->obj_at(i) == p) {
      return i;
    }
  }
  if (literalIndex == maxLiteralIndex) {
    maxLiteralIndex *= 2;
    literals= literals->cloneSize(maxLiteralIndex);
  }
  literals->obj_at_put(literalIndex, p);
  return literalIndex++;
}


fint AbstractByteCode::GenExtendedIndex(fint offset, fint length, fint x) {
  assert(x > MAXINDEX, "shouldn't be calling");
  GenCode(offset, length, BuildCode(INDEX_CODE, GenIndex(offset, length,
                                                         x >> INDEXWIDTH)));
  return x & MAXINDEX;
}

oop ByteCode::create_outer_method_prim(oop ignore,
                                       byteVectorOop bv,
                                       objVectorOop lits,
                                       stringOop file,
                                       smiOop line,
                                       stringOop source) {
  Unused(ignore);
  ResourceMark rm;

  if (bv->length() == 0)
    return ErrorCodes::general_prim_error("Error: empty byte code vector");

  smi errorIndex;
  IntBList* stack_deltas;
  const char* errorMsg = methodMap::check_byteCodes_and_literals( errorIndex, 
                                                                  stack_deltas,
                                                                  bv, lits );
  if (errorMsg) {
    char buf[BUFSIZ];
    (void) sprintf(buf, "Error: bad byte code at: %d <%s>", 
                   errorIndex, errorMsg);
    return ErrorCodes::general_prim_error(buf);
  }
  // clone bv & lits for cheap insurance
  // also must clone literals because create_outerMethod will set
  //   the methodPointer in it
  ByteCode b(false, new_string(bv->bytes(), bv->length()),
             lits->clone(), file, line, source);
  oop m = create_outerMethod(0, &b, "", stack_deltas);
  return m->as_mirror();
}


bool ByteCode::Finish() {
  if ( !ResolveBranches())
    return false;
  codes= codes->cloneSize(codeIndex, mustAllocate);
  literals= literals->cloneSize(literalIndex, mustAllocate);
  if ( codes == failedAllocationOop ) {
    errorMessage= "out of memory for codes in ByteCode::Finish";
    ranOutOfMemory= true;
    return false;
  }
  if ( literals == failedAllocationOop) {
    errorMessage= "out of memory for literals in ByteCode::Finish";
    ranOutOfMemory= true;
    return false;
  }
  return true;
}


bool ByteCode::Finish(const char* fname, fint sourceLine,
                      const char* srcStart, fint srcLen) {
  if (!Finish())
    return false;
  file= mustAllocate ? new_string(fname) : new_string_or_fail(fname);
  line= as_smiOop(sourceLine);
  source= new_string(srcStart, srcLen, mustAllocate);
  if ( file == failedAllocationOop ) {
    errorMessage= "out of memory for file in ByteCode::Finish";
    ranOutOfMemory= true;
    return false;
  }
  if ( source == failedAllocationOop ) {
    errorMessage= "out of memory for source in ByteCode::Finish";
    ranOutOfMemory= true;
    return false;
  }
  return true;
}


bool ByteCode::Finish(const char* fname, const char* src) {
  return Finish(fname, 1, src, strlen(src));
}


bool ByteCode::Finish(const char* fname, fint sourceLine, fint srcOffset, fint srcLen) {
  bool r = Finish();
  if (!r) return false;
  if ((file= new_string(fname)) == failedAllocationOop) {
    errorMessage= "out of memory for file in ByteCode::Finish";
    ranOutOfMemory = true;
    return false;
  }
  line= as_smiOop(sourceLine);
  source= Memory->stringObj;
  sourceOffset= as_smiOop(srcOffset);
  sourceLen=    as_smiOop(srcLen);
  return r;
}


oop ByteCode::create_block_method_prim(oop ignore,
                                       byteVectorOop bv,
                                       objVectorOop lits,
                                       stringOop file,
                                       smiOop line,
                                       stringOop source) {
  Unused(ignore);
  ResourceMark rm;

  if (bv->length() == 0)
    return ErrorCodes::general_prim_error("Error: empty byte code vector");

  smi errorIndex;
  IntBList* stack_deltas;
  const char* errorMsg = methodMap::check_byteCodes_and_literals( errorIndex, 
                                                                  stack_deltas,
                                                                  bv, lits );
  if ( errorMsg ) {
    char buf[BUFSIZ];
    (void) sprintf(buf, "Error: bad byte code at: %d <%s>",
                   errorIndex, errorMsg);
    return ErrorCodes::general_prim_error(buf);
  }
  // clone bv & lits for cheap insurance
  ByteCode b(false, new_string(bv->bytes(), bv->length()),
             lits->clone(), file, line, source);
  oop m = create_blockMethod(NULL, &b, "", stack_deltas);

  return m->as_mirror();
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "parser.hh"
# include "_parser.cpp.incl"

DO_NOT_CROSS_COMPILE 
  
# define assertToken(t, c, s)                                                 \
    if ((t)->type != (c)) { expecting(t, s); return 0; }

# define CHECK_TOKEN(t)                                                       \
    if (t->type == Token::ERROR_TOKEN) {                                      \
      syntaxError( t->sourceStart, t);                                        \
      return NULL; }

Parser::Parser(Scanner* scanr, bool silnt) {
  scanner        = scanr;
  silent         = silnt;

  implicitSelf         = NULL;

  error                = noParseErr;
  parse_error_message  = NULL;
  parse_error_line     = -1;
  parse_error_column   = -1;
  parse_error_expected = "";
}


void Parser::prematureEOF(Token *t) {
  if (!parse_error_message) {
    error                = prematureEndErr;
    parse_error_message  = "premature end of input";
    parse_error_line     = t->line;
    parse_error_column   = t->column;
  }
}

void Parser::syntaxErrorExpecting(Token*t , const char *expected) {
  if (!silent) scanner->ErrorMessage("unexpected token", t->line, t->column);
  if (!parse_error_message) {
    error                = parseErr;
    parse_error_message  = "unexpected token";
    parse_error_line     = t->line;
    parse_error_column   = t->column;
    parse_error_expected = expected;
  }
}


void Parser::syntaxError(const char* msg, Token* t) {
  syntaxError(msg, t->line, t->column);
}


void Parser::syntaxError(const char* msg, int32 l, int32 c) {
  if (!silent) scanner->ErrorMessage(msg, l, c);
  if (!parse_error_message) {
    error               = parseErr;
    parse_error_message = msg;
    parse_error_line     = l;
    parse_error_column   = c;
  }
}

bool Parser::noParseError()             { return error == noParseErr;      }
bool Parser::prematureEndOfInputError() { return error == prematureEndErr; }

void Parser::innerMethodError(Token* t) {
  // Error message when slots are present in sub expression.
  syntaxError("inner methods are no longer supported, "
                          "slot-list within a sub expression is not legal", t);
}


String* Parser::OpStringFromNum(Token* t) {
  assert(isPossibleOp(t) && !isOp(t), "negative number expected");
  if (t->type == Token::INTEGER) {
    t->integer = -t->integer;
  } else {
    t->floating = -t->floating ;
  }
  scanner->push_token(t);
  return new String("-");
}

String* Parser::StringFromOp(Token* t) {
  switch (t->type) {
   // TODO (topa 2012-05-06):  = and ~ are no TokenTypes in Token...
   case '=':             return new String("=");
   case '~':             return new String("~");
   case Token::ARROW:    return new String("<-");
   case Token::OPERATOR: return t->string;
   default:    error("parser: should not be here"); return NULL;
  }
}

Expr* Parser::parseExpr(Token* t, bool methodCandidate) {
  Expr* expr;
  Token* delegate = NULL;
  Token* tStart = t;
  if (t->type == Token::DELEGATE || t->type == Token::RESEND_TOKEN) {
    delegate = t;
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (t->type != Token::KEYWORD) {
      scanner->push_token(t);
      t = delegate;
      delegate = NULL;
    }
  }
  if (t->type == Token::KEYWORD || t->type == Token::PRIMKEYWORD) {
    expr = implicitSelf;
  } else {
    expr = parseBinary(t, methodCandidate);
    if (! expr) return NULL;
    t = scanner->get_token(); CHECK_TOKEN(t);
  }
  if (t->type == Token::KEYWORD || t->type == Token::PRIMKEYWORD) {
    StringList* keywords = new StringList(t->string);
    Expr* arg = parseExpr();
    if (! arg) return NULL;
    ExprList* args = new ExprList(arg);
    t = scanner->get_token(); CHECK_TOKEN(t);
    while (t->type == Token::Token::CAPKEYWORD) {
      keywords = keywords->Append(t->string);
      arg = parseExpr();
      if (! arg) return NULL;
      args = args->Append(arg);
      t = scanner->get_token(); CHECK_TOKEN(t);
    }
    if (delegate) {
      switch (delegate->type) {
       case Token::DELEGATE:
        expr = new Keyword(expr, keywords, args, delegate->string,
                           tStart->sourceStart,
                           scanner->lastWhiteSpaceBefore(t->sourceStart),
                           scanner->fileName(), tStart->line, tStart->column,
                           this);
        break;
       case Token::RESEND_TOKEN:
        expr = new Keyword(expr, keywords, args, new String("resend"),
                           tStart->sourceStart,
                           scanner->lastWhiteSpaceBefore(t->sourceStart),
                           scanner->fileName(), tStart->line, tStart->column,
                           this);
        break;
       default:
        fatal1("incorrect delegate token type %ld", delegate->type);
      }
    } else {
      expr = new Keyword(expr, keywords, args, 0,
                         tStart->sourceStart,
                         scanner->lastWhiteSpaceBefore(t->sourceStart),
                         scanner->fileName(), tStart->line, tStart->column,
                         this);
    }
  }
  scanner->push_token(t);
  return expr;
}

bool Parser::checkSubExpression(ExprList* body, Token* s) {
  // This must be a sub-expression, check length of code == 1 and
  // absence of '^'.
  if (body->Length() > 1) {
    syntaxError("cannot have multiple expressions inside a sub-expression", s);
    return false;
  }
  if (s->type == '^') {
    syntaxError("return must be last statement", s);
    return false;
  }
  return true;
}

Expr* Parser::parseBinary(Token* t, bool methodCandidate) {
  Expr* expr;
  Token* delegate = NULL;
  String* old_op = NULL;
  Token* tStart = t;
  if (t->type == Token::DELEGATE || t->type == Token::RESEND_TOKEN) {
    delegate = t;
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (! isOp(t)) {
      scanner->push_token(t);
      t = delegate;
      delegate = NULL;
    }
  }
  if (isOp(t)) {
    expr = implicitSelf;
  } else {
    Token* s;
    if (t->type == '(') {
      s = scanner->get_token(); CHECK_TOKEN(s);
      scanner->push_token(s);
    }
    expr = parseUnary(t, methodCandidate);
    if (! expr) return NULL;
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (methodCandidate && expr->IsMethod() && isPossibleOp(t)) {
     Object* obj = (Object *) expr;
     if (!obj->slots->IsEmpty()) {
       innerMethodError(s);
       return NULL;
     } else
       if (!checkSubExpression(obj->body,s))
         return NULL;
   }
  }
  while (isPossibleOp(t)) {
    String* op;
    if (isOp(t)) {
      op = StringFromOp(t);
    } else {
      op = OpStringFromNum(t);
    }
    if (old_op && strcmp(old_op->AsCharP(), op->AsCharP())) {
      syntaxError("no precedence for binary operator - please use parentheses", t);
      return NULL;
    }
    old_op = op;
    Expr* arg;
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (t->type == Token::KEYWORD) {
      arg = parseExpr(t);
    } else {
      arg = parseUnary(t);
    }
    if (! arg) return NULL;
    if (delegate) {
      switch (delegate->type) {
       case Token::DELEGATE:
        expr = new Binary(expr, op, arg, delegate->string,
                          tStart->sourceStart, arg->source_end(),
                          scanner->fileName(), tStart->line, tStart->column,
                          this);
        delegate = NULL;
        break;
       case Token::RESEND_TOKEN:
        expr = new Binary(expr, op, arg, new String("resend"),
                          tStart->sourceStart, arg->source_end(),
                          scanner->fileName(), tStart->line, tStart->column,
                          this);
        delegate = NULL;
        break;
       default:
        fatal1("incorrect delegate token type %ld", delegate->type);
      }
    } else {
      expr = new Binary(expr, op, arg, 0,
                        tStart->sourceStart,  arg->source_end(),
                        scanner->fileName(), tStart->line, tStart->column,
                        this);
    }
    t = scanner->get_token(); CHECK_TOKEN(t);
  }
  scanner->push_token(t);
  return expr;
}

Expr* Parser::parseUnary(Token* t, bool methodCandidate) {
  Expr* expr;
  Token* delegate = NULL;
  Token* tStart = t;
  switch (t->type) {
   case Token::INTEGER:
    expr = parseInteger(t);
    if (! expr) return NULL;
    t = scanner->get_token(); CHECK_TOKEN(t);
    break;
   case Token::FLOAT: 
    expr = parseFloat(t);
    if (! expr) return NULL;
    t = scanner->get_token(); CHECK_TOKEN(t);
    break;
   case Token::STRING:
    expr = parseString(t);
    if (! expr) return NULL;
    t = scanner->get_token(); CHECK_TOKEN(t);
    break;
   case Token::ANNOTATION_START:
   case '(':
   case '[':
    { bool enclosed_in_annotation = t->type == Token::ANNOTATION_START;
      Token* anno = NULL;

      if (enclosed_in_annotation) {
        t = scanner->get_token(); CHECK_TOKEN(t);
        // Skip annoying ACCEPT tokens.
        while(t->type == Token::ACCEPT && !scanner->is_done()) {
          t = scanner->get_token(); CHECK_TOKEN(t);
        }
        
        if (t->type != Token::STRING) {
          expecting(t, "a string containing an annotation");
          return NULL;
        }
        anno = t;
        t = scanner->get_token(); CHECK_TOKEN(t);

        // Skip annoying ACCEPT tokens.
        while(t->type == Token::ACCEPT && !scanner->is_done()) {
          t = scanner->get_token(); CHECK_TOKEN(t);
        }
      }
      if ( t->type != '(' && t->type != '[') {
          expecting(t, "'(' or '['");
          return NULL;
      }
      expr = parseObject(t->type == '(' ? ')' : ']', t, 0, 
                         methodCandidate, anno);
      if (! expr) return NULL;
      t = scanner->get_token(); CHECK_TOKEN(t);
      if (enclosed_in_annotation) {
        // Skip annoying ACCEPT tokens.
        while(t->type == Token::ACCEPT && !scanner->is_done()) {
          t = scanner->get_token(); CHECK_TOKEN(t);
        }

        if (t->type != Token::ANNOTATION_END) {
          expecting(t, "'}' at end of annotated slot list");
          return NULL;
        }
        t = scanner->get_token(); CHECK_TOKEN(t);
      }
    }
    break;
   case Token::SELF_TOKEN:
    expr = new Self(t->sourceStart, scanner->sourceAddr(),
                    scanner->fileName(), tStart->line, tStart->column,
                    this);
    t = scanner->get_token(); CHECK_TOKEN(t);
    break;
   case Token::NAME:
    // Check for capitalized name
    if (isupper(t->string->AsCharP()[0])) {
      syntaxError("unary name must begin with a lowercase letter", t);
      return NULL;
    }
   case Token::PRIMNAME:
    expr = implicitSelf;
    break;
   case Token::DELEGATE:
   case Token::RESEND_TOKEN:
    expr = implicitSelf;
    delegate = t;
    t = scanner->get_token(); CHECK_TOKEN(t);
    assertToken(t, Token::NAME, "a name token");
    break;
   default:
    expecting(t, "a token beginning a unary expression");
    return NULL;
  }
  if (delegate) {
    switch (delegate->type) {
     case Token::DELEGATE:
      expr = new Unary(expr, t->string, delegate->string,
                       tStart->sourceStart, scanner->sourceAddr(),
                       scanner->fileName(), tStart->line, 
                       tStart->column, this);
      t = scanner->get_token(); CHECK_TOKEN(t);
      break;
     case Token::RESEND_TOKEN:
      expr = new Unary(expr, t->string, new String("resend"),
                       tStart->sourceStart, scanner->sourceAddr(),
                       scanner->fileName(), tStart->line,  
                       tStart->column, this);
      t = scanner->get_token(); CHECK_TOKEN(t);
      break;
     default:
      fatal1("incorrect delegate token type %ld", delegate->type);
    }
  }
  while (t->type == Token::NAME || t->type == Token::PRIMNAME) {
    if (t->type == Token::NAME && isupper(t->string->AsCharP()[0])) {
      syntaxError("unary name must begin with a lowercase letter", t);
      return NULL;
    }
    expr = new Unary(expr, t->string, 0,
                     tStart->sourceStart, scanner->sourceAddr(),
                     scanner->fileName(), tStart->line,  
                     tStart->column, this);
    t = scanner->get_token(); CHECK_TOKEN(t);
  }
  scanner->push_token(t);
  return expr;
}

Integer* Parser::parseInteger(Token* t) {
  assertToken(t, Token::INTEGER, "an integer literal token");
  return new Integer(t->integer, t->sourceStart, scanner->sourceAddr(),
                     scanner->fileName(), t->line, t->column, this);
}

Float* Parser::parseFloat(Token* t) {
  assertToken(t, Token::FLOAT, "a float literal token");
  return new Float(t->floating, t->sourceStart, scanner->sourceAddr(),
                   scanner->fileName(), t->line, t->column, this);
}

StringLiteral* Parser::parseString(Token* t) {
  assertToken(t, Token::STRING, "a string literal token");
  return new StringLiteral(t->string, t->sourceStart, scanner->sourceAddr(),
                           scanner->fileName(), t->line, t->column, this);
}

Expr* Parser::parseExpr() {
  Token *t = scanner->get_token(); CHECK_TOKEN(t);
  return parseExpr(t);
}

Object* Parser::parseObject(char match, Token* t, fint nargs,
                            bool methodCandidate, Token* anno) {

  if (match == ')') {
    assertToken(t, '(', "a '(' token");
  } else {
    assertToken(t, '[', "a '[' token");
    nargs = -1;                    // blocks can have arbitrary # args
  }
  SlotList* slots;
  ExprList* code;

  fint        source_body_line;
  fint        source_body_col;
  const char* source_body_start;
  fint        source_body_length;
  Token*      object_annotation = NULL;

  fint        source_line   = t->line;
  fint        source_column = t->column;
  const char* source_start  = t->sourceStart;
  fint        source_length;

  t = scanner->get_token(); CHECK_TOKEN(t);

  bool ok = parseBody(slots, code, t, nargs, 
                      source_body_start, source_body_line, source_body_col,
                      object_annotation);
  if (! ok) return NULL;

  if ( !methodCandidate && (match == ')') && !code->IsEmpty() ) {
    if (!slots->IsEmpty()) {
      innerMethodError(t);
      return NULL;
    } else
      if (!checkSubExpression(code,t))
        return NULL;
  }

  // If this object has code make sure its slots are non methods

  // - 1 to omit closing paren
  source_body_length = scanner->sourceAddr() - source_body_start - 1;
  Token *closing_token = scanner->get_token(); CHECK_TOKEN(closing_token);
  if (match == ')') {
    assertToken(closing_token, ')', "a ')' token");
  } else {
    assertToken(closing_token, ']', "a ']' token");
    if (code->Length() == 0 && slots->ArgCount() > 0) {
      syntaxError("blocks without code cannot have arguments", t);
      return NULL;
    }
  }
  source_length = scanner->sourceAddr() - source_start;

  Object *o = new Object(slots, code, match == ']',
                         scanner->fileName(),
                         source_line,
                         source_column,
                         source_start,
                         source_length,
                         source_body_line,
                         source_body_col,
                         source_body_start,
                         source_body_length,
                         this);

  if (object_annotation) {
    o->annotation = extend_annotation_with_string("", object_annotation);
  } else {
    if (anno) o->annotation = extend_annotation_with_string("",anno);
  }

  if (!o->body->IsEmpty()) {
    Token* tok = scanner->commentList->isEmpty() 
      ? NULL : scanner->commentList->last();
    while (tok && tok->sourceStart >= source_start) {
      scanner->commentList->pop();
      tok = scanner->commentList->isEmpty() 
        ? NULL :  scanner->commentList->last();
    }
  }
  

  if (o->isDeferred && !o->body->IsEmpty()) {
    // check existence of local methods in blocks with code
    if (o->ContainsMethod()) {
      syntaxError("sorry - local methods not implemented yet", t);
      return NULL;
    }    
  }
  return o;
}

bool Parser::parseBody(SlotList*& slots, ExprList*& code, Token* t, fint nargs,
                       const char*& sourceStart, fint& line, fint& col,
                       Token*& object_annotation) {
  if (t->type == '|') {
    slots = parseObjectSlots(t, object_annotation);
    if (! slots) return false;
    t = scanner->get_token(); CHECK_TOKEN(t);
  } else {
    slots = new SlotList;
  }
  sourceStart = scanner->lastWhiteSpaceBefore(t->sourceStart);
  line = t->line;
  col  = t->column;
  
  if (slots->ArgCount() != nargs && nargs >= 0) {
    syntaxError("wrong number of argument slots", t);
    return false;
  }

  scanner->start_allowing_comments();
  code = new ExprList;
  Token* periodTok = NULL;
  for (;;) {
    if (t->type == ')' || t->type == ']' || t->type == Token::ACCEPT) {
      break;
    }
    if ( periodTok != NULL)
      code = code->Append(new Pop( periodTok->sourceStart, 
                                   periodTok->sourceStart + 1,
                                   scanner->fileName(),
                                   periodTok->line, 
                                   periodTok->column,
                                   this));
    if (t->type == '^') {
      Expr* expr = parseExpr();
      if (! expr) return false;
      expr = new Return(expr, t->sourceStart, expr->source_end(),
                        scanner->fileName(), t->line, t->column, this);
      code = code->Append(expr);
      t = scanner->get_token(); CHECK_TOKEN(t);
      if (t->type == '.') { 
        t = scanner->get_token(); CHECK_TOKEN(t);
      }
      break;
    }
    Expr* expr = parseExpr(t);
    if (! expr) return false;
    code = code->Append(expr);
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (t->type != '.') break;
    periodTok = t;
    t = scanner->get_token(); CHECK_TOKEN(t);
  }
  scanner->push_token(t);
  scanner->stop_allowing_comments();

  return true;
}


SlotList* Parser::parseObjectSlots(Token* t, Token*& object_annotation) {
  assertToken(t, '|', "a '|' token");
  Token* finalT;
  SlotList* slots = new SlotList;
  slots = parseSlots(slots, "", finalT, object_annotation);
  if (!slots) {
    return NULL;
  }
  if (finalT->type != '|') {
    expecting(finalT, "'|' at end of slot list");
    return NULL;
  }
  return slots;
}


SlotList* Parser::parseAnnotatedSlots(Token* t, SlotList* slots, const char* annos,
                                      Token*& object_annotation) {
  assertToken(t, Token::ANNOTATION_START, "an annotation start token");

  t = scanner->get_token(); CHECK_TOKEN(t);
  if (t->type != Token::STRING) {
    expecting(t, "a string containing an annotation");
    return NULL;
  }

  Token* finalT;
  slots = parseSlots(slots, extend_annotation_with_string(annos, t), finalT,
                     object_annotation);
  if (!slots) {
    return NULL;
  }
  if (finalT->type != Token::ANNOTATION_END) {
    expecting(finalT, "'}' at end of annotated slot list");
    return NULL;
  }
  return slots;
}

SlotList* Parser::parseSlots(SlotList* slots, const char* annos, Token*& finalT,
                             Token*& object_annotation) {
  Token* t = scanner->get_token(); CHECK_TOKEN(t);
  for (;;) {
    while (t->type == Token::ANNOTATION_START) {
      Token* t1 = scanner->get_token(); CHECK_TOKEN(t1);
      if (t1->type == Token::ANNOTATION_END) {
        // Parse: {} = 'xxx' as object annotation.
        t = scanner->get_token(); CHECK_TOKEN(t);
        if (t->type != '=') {
          expecting(t, "'=' when defining an object annotation");
          return NULL;
        }
        t = scanner->get_token(); CHECK_TOKEN(t);
        if (t->type != Token::STRING) {
          expecting(t, "a string containing an annotation");
          return NULL;
        }
        object_annotation = t;
      } else {
        scanner->push_token(t1);
        slots = parseAnnotatedSlots(t, slots, annos, object_annotation);
        if (slots == EMPTY) break;
      }
      t = scanner->get_token(); CHECK_TOKEN(t);
      if (t->type == '.') {
        t = scanner->get_token(); CHECK_TOKEN(t);
      }
    }
    if (t->type == '|' || t->type == Token::ANNOTATION_END) {
      finalT = t;
      break;
    }
    slots = parseSlot(slots, t, annos);
    if (slots == EMPTY) {
      break;
    }
    t = scanner->get_token(); CHECK_TOKEN(t);
    if (t->type == '.') {
      t = scanner->get_token(); CHECK_TOKEN(t);
    } else {
      finalT = t;
      return slots;
    };
  }
  return slots;
}


SlotList* Parser::parseSlot(SlotList* slots, Token* t, const char* anno) {
  switch (t->type) {
   case Token::ARG:
    return parseArgSlot(slots, t, anno);
   case Token::NAME:
   case Token::PRIMNAME:
    return parseUnarySlot(slots, t, anno);
   case Token::OPERATOR:
   case '=':
   case '~':
   case Token::ARROW:
    return parseBinarySlot(slots, t, anno);
   case Token::KEYWORD:
   case Token::PRIMKEYWORD:
    return parseKeywordSlot(slots, t, anno);
   default:
    expecting(t, "a slot name expression");
    return NULL;
  }
}

SlotList* Parser::parseArgSlot(SlotList* slots, Token* t, const char* anno) {
  assertToken(t, Token::ARG, "an argument token");
  if (!isValidSlotName(t)) return NULL;
  return slots->Append(new ArgSlot(t->string, t, anno), this);
}

SlotList* Parser::parseUnarySlot(SlotList* slots, Token* t0, const char* anno) {
  if (t0->type != Token::NAME && t0->type != Token::PRIMNAME) {
    expecting(t0, "a name token");
    return NULL;
  }
  if (!isValidSlotName(t0)) return NULL;
  String* name = t0->string;
  Token* t = scanner->get_token(); CHECK_TOKEN(t);
  bool is_parent_slot = false;
  if (t->type == Token::OPERATOR) {
    // check token for '*'
    const char* s = StringFromOp(t)->AsCharP();
    if (*s == '*') {
      int tokLen= strlen(s);
      if (tokLen > 1) {
        if (s[1] == '*') {
          syntaxError("only a single * is allowed for a parent slot", t);
          return NULL;
        } else {
          while (--tokLen > 0)
            scanner->push_char(s[tokLen]);
        }
      }
      is_parent_slot = true;
      t = scanner->get_token(); CHECK_TOKEN(t);
    }
  }
  if (t->type == '=') {                  // read-only slot or unary method
    Expr* expr = parseExpr(scanner->get_token(), true); 
    if (! expr) return NULL;
    if (expr->IsMethod() && expr->ContainsMethod()) {
      syntaxError("sorry - local methods not implemented yet", t0);
      return NULL;
    }    
    return slots->Append(new DataSlot(name, expr, t0, anno, is_parent_slot),
                         this);
  } else {                              // assignable slot
    Expr* expr;
    if (t->type == Token::ARROW) {
      expr = parseExpr();          
      if (! expr) return NULL;
      if (expr->IsMethod()) {
        syntaxError("sorry - assignable methods not implemented", t0);
        return NULL;
      }
    } else {
      scanner->push_token(t);
      expr = NULL;
    }
    slots = slots->Append(new DataSlot(name, expr, t0, 
                                       anno, is_parent_slot, true), this);
    return slots;
  }
}


Object* Parser::parseMethodDecl(Token* t, fint nargs, fint nargsSeen) {
  Object* obj;
  assertToken(t, '=', "a '=' token");
  t = scanner->get_token(); CHECK_TOKEN(t);
  if (t->type != '(' && t->type != '[') {
    if (t->type == Token::ACCEPT) {
      // don't print out error message
      prematureEOF(t);
    } else {
      syntaxError("object expected", t);
    }
    return NULL;
  }
  obj = parseObject(t->type == '(' ? ')' : ']', t, nargs - nargsSeen, true);
  if (! obj) return NULL;
  if (nargs > 0 && !obj->IsMethod()) {
    syntaxError("empty objects cannot have arguments", t);
    return NULL;
  }
  if (obj->ContainsMethod()) {
    syntaxError("sorry - local methods not implemented yet", t);
    return NULL;
  }    

  return obj;
}


SlotList* Parser::parseBinarySlot(SlotList* slots, Token* t, const char* anno) {
  Token* t0 = t;
  if (! isOp(t)) {
    expecting(t, "a binary operator token");
    return NULL;
  }
  String* name = StringFromOp(t);
  t = scanner->get_token(); CHECK_TOKEN(t);
  Expr* expr;
  if (t->type == Token::NAME) {                    // argument
    if (!isValidSlotName(t)) return NULL;
    Token* argument_token = t;
    t = scanner->get_token(); CHECK_TOKEN(t);
    expr = parseMethodDecl(t, 1, 1);        // no arg slot
    if (! expr) return NULL;
    expr = expr->AddArg(new ArgSlot(argument_token->string, argument_token), this);
  } else {
    expr = parseMethodDecl(t, 1, 0);        // should have 1 arg slot
    if (! expr) return NULL;
  }
  return slots->Append(new DataSlot(name, expr, t0, anno), this);
}

SlotList* Parser::parseKeywordSlot(SlotList* slots, Token* t, const char* anno) {
  Token* t0 = t;
  if (t->type != Token::KEYWORD && t->type != Token::PRIMKEYWORD) {
    expecting(t, "a keyword token");
    return NULL;
  }
  StringList* keywords = new StringList(t->string);
  if (!isValidSlotName(t)) return NULL;
  t = scanner->get_token(); CHECK_TOKEN(t);
  Expr* expr;
  if (t->type == Token::NAME) {                    // args in selector
    ArgSlotList* args = new ArgSlotList(new ArgSlot(t->string, t));
    t = scanner->get_token(); CHECK_TOKEN(t);
    while (t->type == Token::CAPKEYWORD) {
      keywords = keywords->Append(t->string);
      t = scanner->get_token(); CHECK_TOKEN(t);
      assertToken(t, Token::NAME, "a name token");
      args = args->Append(new ArgSlot(t->string, t));
      t = scanner->get_token(); CHECK_TOKEN(t);
    }
    expr = parseMethodDecl(t, args->Length(), args->Length());      
  // no more arg slots, please
    if (! expr) return NULL;
    expr = expr->AddArgs(args, this);
    if (! expr) return NULL;
  } else {                                // args slot names in body
    fint nargs = 1;
    while (t->type == Token::CAPKEYWORD) {
      nargs++;
      keywords = keywords->Append(t->string);
      t = scanner->get_token(); CHECK_TOKEN(t);
    }
    expr = parseMethodDecl(t, nargs, 0);  // must have arg slots
    if (! expr) return NULL;
  }
  String* s = keywords->AsSelector();
  return slots->Append(new DataSlot(s, expr, t0, anno), this);
}

void Parser::expecting(Token* t, const char* c) {
  if (t->type == Token::ACCEPT) {
    // don't print the error message
    prematureEOF(t);
  } else if (t->type != Token::ERROR_TOKEN) {
    syntaxErrorExpecting(t, c);
  }
}

bool Parser::isValidSlotName(Token* name) {
  if (str_is_prim_name(name->string->AsCharP(), name->string->len)) {  
    syntaxError("slot names cannot begin with '_'", name);
    return false;
  } else if (isupper(name->string->AsCharP()[0])) {
    syntaxError("slot names must begin with a lowercase letter", name);
    return false;
  } else return true;
}

bool Parser::isOp(Token* t) {
  return    t->type == Token::OPERATOR
         || t->type == '='
         || t->type == '~'
         || t->type == Token::ARROW;
}

bool Parser::isPossibleOp(Token* t) {
  return isOp(t)
         || (t->type == Token::INTEGER && t->integer < 0)
         || (t->type == Token::FLOAT && t->floating < 0.0);
}

bool Parser::testDone() {
  Token* t = scanner->get_token();
  assertToken(t, Token::ACCEPT, "the end of the expression");
  return true;
}

Expr* Parser::readExpr(fint& line, const char*& sourceStart, fint& sourceLength) {
  Token* t = scanner->get_token();
  line = t->line;
  Expr* e = NULL;
  if (t->type != Token::ACCEPT) {
    if (t->type == Token::ERROR_TOKEN) {
      sourceStart= scanner->sourceAddr();
      sourceLength= 0;
      return NULL;
    }
    sourceStart = scanner->lastWhiteSpaceBefore(t->sourceStart);
    e = parseExpr(t, true);
    t = scanner->get_token();           // eat extra period at end of expr
    if (t->type != '.') scanner->push_token(t);
    if (!e) {
      testDone();
      if (prematureEndOfInputError()) {
        scanner->ErrorMessage(parse_error_message, 
                              parse_error_line,
                              parse_error_column);
      }
      if (VMScanner == scanner) {       
        scanner->SuppressErrors(true);
        assert(scanner->is_interactive(), "wrong scanner");
        scanner->discardInput();
        scanner->SuppressErrors(false);
        scanner->reset();
      }
    }
  }
  sourceLength= scanner->sourceAddr() - sourceStart - 1;
  return e;
}

Object* Parser::readBody(fint& line, fint& col,
                         const char*& sourceStart, fint& sourceLength) {
  Token* t = scanner->get_token();
  line = t->line;
  col  = t->column;

  fint        source_line  = t->line;
  fint        source_column = t->column;
  const char* source_start = t->sourceStart;
  fint        source_length;
  
  sourceStart = scanner->lastWhiteSpaceBefore(t->sourceStart);

  if (t->type == Token::ACCEPT) {
    prematureEOF(t);
    return NULL;
  }

  if (t->type == Token::ERROR_TOKEN) {
    syntaxError("syntax error", t);
    return NULL;
  }

  SlotList* slots;
  ExprList* code;
  Token* object_annotation = NULL;

  if (parseBody(slots, code, t, -1, sourceStart, line, col, object_annotation)) {
    testDone();
  }

  if (noParseError()) {
    sourceLength  = scanner->sourceAddr() - sourceStart;
    source_length = scanner->sourceAddr() - source_start;
    Object* e = new Object(slots, code, false, 
                           scanner->fileName(),
                           source_line,
                           source_column,
                           source_start,
                           source_length,
                           line,
                           col,
                           sourceStart,
                           sourceLength, 
                           this);
    
    if (!e->body->IsEmpty() && e->ContainsMethod()) {
      syntaxError("sorry - local methods not implemented yet", t);
      return NULL;
    }
    return e;
  }
  assert(parse_error_message, "Parse error missing");
  return NULL;
}

bool fill_data_slot(oop obj, const char* slot_name, oop value) {
  stringOop name = Memory->string_table->lookup(slot_name, strlen(slot_name));
  slotDesc* sd   = obj->find_slot(name);
  if (sd && sd->type->is_obj_slot()) {
    smi offset = smiOop(sd->data)->value();
    Memory->store(oopsOop(obj)->oops(offset), value);
    return true;
  }
  return false;
}

void Parser::fillErrorObj(oop errorObj) {
  assert(parse_error_message, "Error is not set");
  fill_data_slot(errorObj, "message",       new_string(parse_error_message));
  fill_data_slot(errorObj, "errorLine",     as_smiOop(parse_error_line));
  fill_data_slot(errorObj, "errorColumn",   as_smiOop(parse_error_column));
  fill_data_slot(errorObj, "scannerLine",   as_smiOop(scanner->get_line_number()));
  fill_data_slot(errorObj, "scannerColumn", as_smiOop(scanner->get_column()));
  fill_data_slot(errorObj, "expecting",     new_string(parse_error_expected));
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "kinds.hh"
# include "_kinds.cpp.incl"

void printLookupType(LookupType l) {
  lprintf(lookupTypeName(l));
}

static void addFlag(bool& flag, char* name, const char* add) {
  if (!flag) strcat(name, " { ");
  flag = true;
  strcat(name, add);
}

char* lookupTypeName(LookupType l) {
  char* name = NEW_RESOURCE_ARRAY(char, 80);
  switch (l) {
   case NormalLookupType:
    strcpy(name, "NormalLookup");
    break;
   case StaticNormalLookupType:
    strcpy(name, "StaticNormalLookup");
    break;
   case ImplicitSelfLookupType:
    strcpy(name, "ImplicitSelfLookup");
    break;
   case ResendLookupType:
    strcpy(name, "ResendLookup");
    break;
   case DirectedResendLookupType:
    strcpy(name, "DirectedResendLookup");
    break;
   case NormalPerformType:
    strcpy(name, "NormalPerform");
    break;
   case ResendPerformType:
    strcpy(name, "ResendPerform");
    break;
   case DelegatedPerformType:
    strcpy(name, "DelegatedPerform");
    break;
   default:
    const char* baseName;
    switch (baseLookupType(l)) {
     case NormalBaseLookupType:
      baseName ="NormalBaseLookup "; break;
     case ResendBaseLookupType:
      baseName ="ResendBaseLookup "; break;
     case DirectedResendBaseLookupType:
      baseName ="DirectedResendBaseLookup "; break;
     case DelegatedBaseLookupType:
      baseName ="DelegatedBaseLookup "; break;
     default: fatal("unexpected base lookup type");
    }
    strcpy(name, baseName);
    if (l & SelectorStaticBit) {
      strcat(name, "<selector static> ");
    }
    if (l & DelegateeStaticBit) {
      strcat(name, "<delegatee static> ");
    }
    if (l & ReceiverStaticBit) {
      strcat(name, "<receiver map static> ");
    }
  }
  bool hasFlag = false;
  switch (countType(l)) {
   case NonCounting: break;
   case Counting:    addFlag(hasFlag, name, "counting "); break;
   case Comparing:   addFlag(hasFlag, name, "comparing "); break;
   default:          fatal1("invalid count type %ld", countType(l));
  }
  if (isSet(l, DirtySendBit)) addFlag(hasFlag, name, "dirty ");
  if (isSet(l, OptimizedSendBit)) addFlag(hasFlag, name, "optimized ");
  if (isSet(l, UninlinableSendBit)) addFlag(hasFlag, name, "uninlinable ");
  if (hasFlag) strcat(name, "}");
    
  return name;
}

void kinds_init() {
  assert((LookupTypeMask >> LookupTypeSize) == 0, "wrong LookupTypeSize");
  assert((CountTypeMask >> CountTypeSize) == 0, "wrong CountTypeSize");
}  
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "scanner.hh"
# include "_scanner.cpp.incl"

unsigned char c_type[] = {
  /* EOF  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^@  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^A  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^B  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^C  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^D  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^E  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^F  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^G  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^H  */  _O | _O | _O | _O | _O | _O | _O,
  /*  \t  */  _O | _O | _O | _O | _S | _O | _O,
  /*  \n  */  _O | _O | _O | _O | _S | _O | _O,
  /*  ^K  */  _O | _O | _O | _O | _O | _O | _O,
  /*  \f  */  _O | _O | _O | _O | _S | _O | _O,
  /*  \r  */  _O | _O | _O | _O | _S | _O | _O,
  /*  ^N  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^O  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^P  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^Q  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^R  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^S  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^T  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^U  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^V  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^W  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^X  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^Y  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^Z  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^[  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^\  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^]  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^^  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^_  */  _O | _O | _O | _O | _O | _O | _O,
  /*     */  _O | _O | _O | _O | _S | _O | _O,
  /*  !  */  _O | _O | _O | _O | _O | _P | _O,
  /*  "  */  _O | _O | _O | _O | _O | _O | _O,
  /*  #  */  _O | _O | _O | _O | _O | _P | _O,
  /*  $  */  _O | _O | _O | _O | _O | _P | _O,
  /*  %  */  _O | _O | _O | _O | _O | _P | _O,
  /*  &  */  _O | _O | _O | _O | _O | _P | _O,
  /*  '  */  _O | _O | _O | _O | _O | _O | _O,
  /*  (  */  _O | _O | _O | _O | _O | _O | _O,
  /*  )  */  _O | _O | _O | _O | _O | _O | _O,
  /*  *  */  _O | _O | _O | _O | _O | _P | _O,
  /*  +  */  _O | _O | _O | _O | _O | _P | _O,
  /*  ,  */  _O | _O | _O | _O | _O | _P | _O,
  /*  -  */  _O | _O | _O | _O | _O | _P | _O,
  /*  .  */  _O | _O | _O | _O | _O | _O | _O,
  /*  /  */  _O | _O | _O | _O | _O | _P | _O,
  /*  0  */  _O | _O | _O | _N | _O | _O | _X,
  /*  1  */  _O | _O | _O | _N | _O | _O | _X,
  /*  2  */  _O | _O | _O | _N | _O | _O | _X,
  /*  3  */  _O | _O | _O | _N | _O | _O | _X,
  /*  4  */  _O | _O | _O | _N | _O | _O | _X,
  /*  5  */  _O | _O | _O | _N | _O | _O | _X,
  /*  6  */  _O | _O | _O | _N | _O | _O | _X,
  /*  7  */  _O | _O | _O | _N | _O | _O | _X,
  /*  8  */  _O | _O | _O | _N | _O | _O | _X,
  /*  9  */  _O | _O | _O | _N | _O | _O | _X,
  /*  :  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ;  */  _O | _O | _O | _O | _O | _P | _O,
  /*  <  */  _O | _O | _O | _O | _O | _P | _O,
  /*  =  */  _O | _O | _O | _O | _O | _P | _O,
  /*  >  */  _O | _O | _O | _O | _O | _P | _O,
  /*  ?  */  _O | _O | _O | _O | _O | _P | _O,
  /*  @  */  _O | _O | _O | _O | _O | _P | _O,
  /*  A  */  _U | _O | _A | _O | _O | _O | _X,
  /*  B  */  _U | _O | _A | _O | _O | _O | _X,
  /*  C  */  _U | _O | _A | _O | _O | _O | _X,
  /*  D  */  _U | _O | _A | _O | _O | _O | _X,
  /*  E  */  _U | _O | _A | _O | _O | _O | _X,
  /*  F  */  _U | _O | _A | _O | _O | _O | _X,
  /*  G  */  _U | _O | _A | _O | _O | _O | _O,
  /*  H  */  _U | _O | _A | _O | _O | _O | _O,
  /*  I  */  _U | _O | _A | _O | _O | _O | _O,
  /*  J  */  _U | _O | _A | _O | _O | _O | _O,
  /*  K  */  _U | _O | _A | _O | _O | _O | _O,
  /*  L  */  _U | _O | _A | _O | _O | _O | _O,
  /*  M  */  _U | _O | _A | _O | _O | _O | _O,
  /*  N  */  _U | _O | _A | _O | _O | _O | _O,
  /*  O  */  _U | _O | _A | _O | _O | _O | _O,
  /*  P  */  _U | _O | _A | _O | _O | _O | _O,
  /*  Q  */  _U | _O | _A | _O | _O | _O | _O,
  /*  R  */  _U | _O | _A | _O | _O | _O | _O,
  /*  S  */  _U | _O | _A | _O | _O | _O | _O,
  /*  T  */  _U | _O | _A | _O | _O | _O | _O,
  /*  U  */  _U | _O | _A | _O | _O | _O | _O,
  /*  V  */  _U | _O | _A | _O | _O | _O | _O,
  /*  W  */  _U | _O | _A | _O | _O | _O | _O,
  /*  X  */  _U | _O | _A | _O | _O | _O | _O,
  /*  Y  */  _U | _O | _A | _O | _O | _O | _O,
  /*  Z  */  _U | _O | _A | _O | _O | _O | _O,
  /*  [  */  _O | _O | _O | _O | _O | _O | _O,
  /*  \  */  _O | _O | _O | _O | _O | _P | _O,
  /*  ]  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ^  */  _O | _O | _O | _O | _O | _P | _O,
  /*  _  */  _O | _O | _A | _O | _O | _O | _O,
  /*  `  */  _O | _O | _O | _O | _O | _P | _O,
  /*  a  */  _O | _L | _A | _O | _O | _O | _X,
  /*  b  */  _O | _L | _A | _O | _O | _O | _X,
  /*  c  */  _O | _L | _A | _O | _O | _O | _X,
  /*  d  */  _O | _L | _A | _O | _O | _O | _X,
  /*  e  */  _O | _L | _A | _O | _O | _O | _X,
  /*  f  */  _O | _L | _A | _O | _O | _O | _X,
  /*  g  */  _O | _L | _A | _O | _O | _O | _O,
  /*  h  */  _O | _L | _A | _O | _O | _O | _O,
  /*  i  */  _O | _L | _A | _O | _O | _O | _O,
  /*  j  */  _O | _L | _A | _O | _O | _O | _O,
  /*  k  */  _O | _L | _A | _O | _O | _O | _O,
  /*  l  */  _O | _L | _A | _O | _O | _O | _O,
  /*  m  */  _O | _L | _A | _O | _O | _O | _O,
  /*  n  */  _O | _L | _A | _O | _O | _O | _O,
  /*  o  */  _O | _L | _A | _O | _O | _O | _O,
  /*  p  */  _O | _L | _A | _O | _O | _O | _O,
  /*  q  */  _O | _L | _A | _O | _O | _O | _O,
  /*  r  */  _O | _L | _A | _O | _O | _O | _O,
  /*  s  */  _O | _L | _A | _O | _O | _O | _O,
  /*  t  */  _O | _L | _A | _O | _O | _O | _O,
  /*  u  */  _O | _L | _A | _O | _O | _O | _O,
  /*  v  */  _O | _L | _A | _O | _O | _O | _O,
  /*  w  */  _O | _L | _A | _O | _O | _O | _O,
  /*  x  */  _O | _L | _A | _O | _O | _O | _O,
  /*  y  */  _O | _L | _A | _O | _O | _O | _O,
  /*  z  */  _O | _L | _A | _O | _O | _O | _O,
  /*  {  */  _O | _O | _O | _O | _O | _O | _O,
  /*  |  */  _O | _O | _O | _O | _O | _P | _O,
  /*  }  */  _O | _O | _O | _O | _O | _O | _O,
  /*  ~  */  _O | _O | _O | _O | _O | _P | _O,
  /*  ^?  */ _O | _O | _O | _O | _O | _O | _O,
  /*  M-^@  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^A  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^B  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^C  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^D  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^E  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^F  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^G  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^H  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^I  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^J  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^K  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^L  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^M  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^N  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^O  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^P  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^Q  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^R  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^S  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^T  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^U  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^V  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^W  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^X  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^Y  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^Z  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^[  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^\  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^]  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^^  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^_  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-   */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-!  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-"  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-#  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-$  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-%  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-&  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-'  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-(  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-)  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-*  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-+  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-,  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M--  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-.  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-/  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-0  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-1  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-2  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-3  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-4  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-5  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-6  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-7  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-8  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-9  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-:  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-;  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-<  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-=  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M->  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-?  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-@  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-A  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-B  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-C  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-D  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-E  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-F  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-G  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-H  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-I  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-J  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-K  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-L  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-M  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-N  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-O  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-P  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-Q  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-R  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-S  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-T  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-U  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-V  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-W  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-X  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-Y  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-Z  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-[  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-\  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-]  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-_  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-`  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-a  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-b  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-c  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-d  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-e  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-f  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-g  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-h  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-i  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-j  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-k  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-l  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-m  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-n  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-o  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-p  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-q  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-r  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-s  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-t  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-u  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-v  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-w  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-x  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-y  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-z  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-{  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-|  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-}  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-~  */  _O | _O | _O | _O | _O | _O | _O,
  /*  M-^? */  _O | _O | _O | _O | _O | _O | _O,
};

DO_NOT_CROSS_COMPILE
  
// upped this size to read in long addSlots string in modules part of
//   xlib_wrappers -- dmu 3/93
  const int32 ScannerBufferSize = 50 * K;
  static char buffer[ScannerBufferSize];

InteractiveScanner *VMScanner;

// C++ 2.0 complains about assigning a char to a TokenType--
//  and it's right!  I think it's dangerous, but rather than redo it now,
//  I will put in as_TokenType so we can find it later.  dmu 7/91

inline Token::TokenType as_TokenType(char c) {
  # if !GCC3 // compiler warns that this will never fail
    assert((unsigned)c <= (unsigned)Token::LAST_CHAR, "TokenType clash!");
  # endif
  return Token::TokenType(c);
}


inline fint asnum(fint c) {
  return is_digit(c) ? c - '0' : is_lower(c) ? c - 'a' + 10 : c - 'A' + 10;
}


SourceBuffer::SourceBuffer(FILE* source_file) {
  if (!source_file) return;
  // Find the size of the file
  if (fseek(source_file, 0, SEEK_END) < 0) {
    perror("seek failed");
    fatal("cannot seek in file");
  }
  length =  ftell(source_file);
  rewind(source_file);
  
  first = OS::map_or_read_source_file(source_file, length);

  next = const_cast<char*>(first);
  last = const_cast<char*>(first + length);
  
  Files->closeFile(source_file);
} 


FileScanner::~FileScanner() {
  if (!fileError) {
    OS::unmap_source_file((caddr_t) sourceBuf->first, sourceBuf->length);
  }
}


void Scanner::initialize() {
  initSourcePos();
  commentsOK = 0;
  commentList = new TokenList(100);
  suppress = false;
}

Scanner::Scanner() {
  sourceBuf= NULL;
  initialize();
}

Scanner::Scanner(long fsz) {
  sourceBuf = new SourceBuffer(fsz);
  initialize();
}

Scanner::Scanner(FILE* source_file) {
  sourceBuf = new SourceBuffer(source_file);
  initialize();
}

InteractiveScanner::InteractiveScanner() : Scanner(ScannerBufferSize) {}

fint InteractiveScanner::read_next_char() {
  char c;
  while (true) {
    if (fread(&c, sizeof(char), 1, stdin)) return c;
    if (feof(stdin)) return EOF;
  }
}

void InteractiveScanner::discardInput() {
  fint c;
  do {
    c = read_next_char();
  } while (c != EOF  &&  c != '\n');
}

FILE* FileScanner::openFileAndReturnFile(const char* fn) {
  _fullName = NULL;
  file = Files->openSelfFile(fn, &_fullName);
  fileError= file ? 0 : errno;
  if (file == NULL) {
    if (fileError == 0) // yes, Virginia, some unix ops can fail to set errno
      fileError= ENOENT;  // such as fopen("", "r")
    return 0;
  }
  _fileName = copy_string(fn);
  return file;
}

FileScanner::FileScanner(const char* fn) : Scanner(openFileAndReturnFile(fn)) {}

StringScanner::StringScanner(const char* s, fint len, const char* fn, fint l, fint c) {
  sourceBuf= new SourceBuffer(s, len);
  _fileName = fn;
  line = l;
  column = c;
}

void InteractiveScanner::start_line(const char* prompt) {
  initSourcePos();
  column += strlen(prompt);
  lprintf("%s", prompt);
}

fint Scanner::get_char() {
  fint c;
  if (chars) {
    c = chars->c;
    chars = chars->prev;
  } else {
    c = read_next_char();
    if (c == '\n' || c == '\r') {
      line ++;
      column = 1;
    } else {
      column ++;
    }
  }
  if (c != EOF) {
    is_buffer_filled() ? sourceBuf->advance() : sourceBuf->nextPut(char(c));
  }
  return c;
}

bool Scanner::is_done() {
  fint c = get_char();
  if (c == EOF) return true;
  push_char(c);
  return false;
}

void Scanner::push_char(fint c) {
  if (c != EOF) sourceBuf->pop();
  if (!is_buffer_filled())
    chars= new CharPushback(c, chars);
  else if (c != EOF) {
    if (c == '\n' || c == '\r') {
      line--;
    } else {
      column--;
    }
    assert(char(c) == sourceBuf->current_char(), "wierd pushback");
  }
}

void Scanner::initSourcePos() {
  line = 1;
  column = 1;
  depth = 0;
  tokens = EMPTY;
  chars = EMPTY;
  if (sourceBuf) sourceBuf->reset();
}

void Scanner::resetTokenList() {
  assert(   tokens == NULL
         || tokens->prev == NULL && tokens->token->type == Token::ACCEPT,
         "unexpected token");
  tokens= NULL;
}


// syntax errors:

void Scanner::SuppressErrors(bool b) { suppress = b; }

Token* Scanner::TokenizingError(const char* msg) {
  if (suppress) {
    return NULL;
  } else {
    return new Token(Token::ERROR_TOKEN, line, column - 1, msg);
  }
}

void Scanner::ErrorMessage(const char* msg, fint l, fint c) {
  if (suppress)
    return;
  error4("%s on line %ld, character %ld of \"%s\"", msg, l, c, fileName());
}

void InteractiveScanner::ErrorMessage(const char* msg, fint l, fint c) {
  if (suppress)
    return;
  for (fint i = 1; i < c; i ++) putchar(' ');
  lprintf("^\n");
  error3("%s on line %ld, character %ld", msg, l, c);
}

const char* SourceBuffer::lastWhiteSpaceBefore(const char* start) {
  // return last (most distant) whitespace character preceding start
  // include comments in start of source

  if ( !(first <= start && start <= last) )
    // start outside of source buffer, this happens for error tokens
    // -- dmu 8/95
    return start;

  // first find the opening paren or trailing bar
  do {
    if (--start < first) return first;
    if (*start == '"') { // "skip comments"
      do {
        if (--start < first) return first;
      } while (*start != '"');
      if (--start < first) return first;
    }
  } while (is_space(*start));
  
  return start + 1;
}
    

Token* Scanner::read_numeric_escape(fint base, fint howmany, char& ch) {
  fint value = 0;
  for (fint count = 0; count < howmany; count ++) {
    fint b = get_char();
    fint digit = asnum(b);
    if (digit >= base || digit < 0) {
      return TokenizingError("invalid digit in character escape");
    }
    value = value * base + digit;
    if (value > 255) {
      return TokenizingError("invalid character escape value (>255)");
    }
  }
  ch = char(value);
  return NULL;
}

// read next char and store in into buf; read 2 chars if first char is \'
Token* Scanner::read_char(char*& buf, bool& cannot_be_a_delimeter) {
  char c = get_char();
  Token* t = NULL;
  cannot_be_a_delimeter = false;
  
  switch (c) {
   default:  *buf = c;  return NULL;
    
    // all self code assumes annotation strings have newlines in them
   // but mac reads them as \r -- dmu 6/99
   case '\r':  *buf = '\n';  return NULL;

   case '\\':  return read_escaped_char(buf, cannot_be_a_delimeter);
  }
}
     

Token* Scanner::read_escaped_char(char*& buf, bool& cannot_be_a_delimeter) {
  fint c1 = get_char();
  char c;
  Token* t = NULL;
  switch (c1) {
   // An escaped newline or CR translates to nothing, 
   // recurse to force a read of whatever is next -- dmu 6/99
   case '\n': return read_char(buf, cannot_be_a_delimeter);
   case '\r': return read_char(buf, cannot_be_a_delimeter);

   case EOF:  c = '\\'; break;
   case 'n':  c = '\n'; break;
   case 't':  c = '\t'; break;
   case 'f':  c = '\f'; break;
   case 'r':  c = '\r'; break;
   case 'v':  c = '\v'; break;
   case 'b':  c = '\b'; break;
   case 'a':  c = '\a'; break;
   
   case '?':  c =  '?'; break;
   case '\\': c = '\\'; break;
    
   // An escaped quote means to include the quote in the string.
   // Set cannot_be_a_delimeter to prevent this quote from being taken as a delimeter.
   case '\'':   c = '\'';  cannot_be_a_delimeter = true;  break;
   case  '"':   c =  '"';  cannot_be_a_delimeter = true;  break;
    
   case '0':  c = '\0'; break;
   
   // Set cannot_be_a_delimeter because a hex sequence that is equivalent to a quote
   // does NOT terminate the string.
   case 'x':  t = read_numeric_escape(16, 2, c); cannot_be_a_delimeter = true; break;
   case 'o':  t = read_numeric_escape( 8, 3, c); cannot_be_a_delimeter = true; break;
   case 'd':  t = read_numeric_escape(10, 3, c); cannot_be_a_delimeter = true; break;
   
   default:
    return TokenizingError("unknown escape sequence");
  }
  *buf = c;
  return t;
}


Token* Scanner::skip_comment() {
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;

  int len = 0;

  fint c;
  bool isEscaped = false;
  do {
    c = get_char();
    buffer[len++] = char(c);
    if (c == '"'  &&  !isEscaped)
      break;
    isEscaped =  c == '\\'  ?  !isEscaped  : false;
  } while (c != EOF);

  if (c == EOF) return TokenizingError("missing trailing \" of comment");
  if (!commentsOK) {
    if (ReportLostComments)
      TokenizingError("this comment is not in code and will be lost");

    buffer[--len] = '\0';
    String* s = new String(copy_string(buffer));
    commentList->append( new Token(Token::COMMENT, s, l, col, ss));
  }
  return NULL;
}


Token* Scanner::read_name(fint c) {
  Token::TokenType t;
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;
  fint len;
  if (c == ':') {
    t = Token::ARG;
    len = 0;
  } else {
    t = c == '_' ? Token::PRIMNAME : Token::NAME;
    len = 1;
    buffer[0] = char(c);
  }
  while (c = get_char(), is_id_char(c)) {
    buffer[len++] = char(c);
  }
  if (c == ':' && (t == Token::NAME || t == Token::PRIMNAME)) {
    buffer[len++] = char(c);
    if (is_upper((fint)*buffer)) t = Token::CAPKEYWORD;
    else
      t = c == '_' ? Token::PRIMKEYWORD : Token::KEYWORD;
  } 
  else {
    push_char(c);
  }
  buffer[len] = '\0';
  if (t == Token::ARG && len == 0) {
    t = as_TokenType(':');
  } else if (t == Token::NAME || t == Token::PRIMNAME) {
    c = get_char();
    if (c == '.') {
      c = get_char();
      push_char(c);
      if (is_id_alpha(c) || is_punct(c)) {
        t = Token::DELEGATE;
      } else {
        push_char('.');
      }
    } else {
      push_char(c);
    }
  }
  if (strcmp(buffer, "self") == 0) {
    if (t == Token::NAME) {
      t = Token::SELF_TOKEN;
    } else {
      return TokenizingError(
                             "using \"self\" as a parent name for a directed resend");
    }
  } else if (strcmp(buffer, "resend") == 0) {
    if (t == Token::DELEGATE) {
      t = Token::RESEND_TOKEN;
    } else {
      return TokenizingError("not using \"resend\" in a resend");
    }
  }
  String* s;
  if (t == Token::NAME    || t == Token::PRIMNAME    || t == Token::ARG || t == Token::DELEGATE ||
      t == Token::KEYWORD || t == Token::PRIMKEYWORD || t == Token::CAPKEYWORD) {
    s = new String(copy_string(buffer));
  } else {
    s = NULL;
  }
  return new Token(t, s, l, col, ss);
}

Token* Scanner::read_op(fint c) {
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;
  buffer[0] = char(c);
  fint len = 1;
  while (is_punct(c = get_char())) buffer[len++] = char(c);
  push_char(c);
  buffer[len] = '\0';
  Token::TokenType t;
  String* s = NULL;
  if (strcmp(buffer, "<-") == 0) t = Token::ARROW;
  else if (strcmp(buffer, "=") == 0) t = as_TokenType('=');
  else if (strcmp(buffer, "|") == 0) t = as_TokenType('|');
  else if (strcmp(buffer, "^") == 0) t = as_TokenType('^');
  else if (strcmp(buffer, "\\") == 0 && (c == '\n' || c == '\r')) {
    get_char();
    return get_token();
  } else {
    t = Token::OPERATOR;
    s = new String(copy_string(buffer));
  }
  return new Token(t, s, l, col, ss);
}

Token* Scanner::read_number(fint c) {
  
  const int MAXSELFINT = smiOop_max->value() + 1; // maximum Self integer + 1
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;
  bool neg = false;
  Token::TokenType t = Token::INTEGER;
  if (c == '-') {
    neg = true;
    c = get_char();
    if (!is_digit(c)) {
      push_char(c);
      return read_op('-');
    }
  }
  int32 i = asnum(c);
  fint base;
  double f = 0;
  
  if (c == '0') {
    if (c = get_char(), c == 'x' || c == 'X') {
      return TokenizingError("invalid numeric constant (use 16r...)");
    } else if (is_digit(c)) {
      return TokenizingError("C-style octals are not supported (use 8r...)");
    } else {
      push_char(c);
    }
  } 
  
  while (is_digit(c = get_char())) {            // read decimal number
    /* The following test is necessary; else i*10 may overflow. */
    if (i > MAXSELFINT / 10)
      return TokenizingError("numeric constant too large");
    i *= 10;
    i += asnum(c);
    if (i > MAXSELFINT || (i == MAXSELFINT && !neg)) {
      return TokenizingError("numeric constant too large");
    }
  }
  
  if (c == 'r' || c == 'R') {                   // base specification ?
    c = get_char();
    if (i < 2 || i > 36) {
      return TokenizingError("illegal base for numeric literal");
    }
    base = i;
    i = 0;
    if (is_alphadigit(c)) {
      do {
        fint v = asnum(c);
        if (v >= base) {
          if (is_digit(c)) {
            return TokenizingError("digit too large for given base");
          } else {
            return TokenizingError("need spaces between numeric literals and selectors\n\t(letter too large for given base)");
          }
        }
        int32 newi = i * base + v;
        int32 maxi = MAXSELFINT / base;
        if ((i > maxi || (i == maxi && !neg)) ||
            (newi > MAXSELFINT || (newi == MAXSELFINT && !neg))) {
          return TokenizingError("numeric constant too large");
        }
        i = newi;
        c = get_char();
      } while (is_digit(c) || (base > 10 && is_alphadigit(c)));
    } else {
      return TokenizingError("expecting a number after the base specifier");
    }
  } else {
    if (is_alpha(c) && c != 'e' && c != 'E') {
      return TokenizingError("need spaces between numeric literals and selectors");
    }
    if (c == '.') {                     // read fraction part
      c = get_char();  
      if (is_digit(c)) {
        t = Token::FLOAT;      
        double place = 1.0;
        f = (double) i;
        do {
          fint v = asnum(c);
          f += v * (place /= 10.0);
        } while (c = get_char(), is_digit(c));
      } else {
        push_char(c);
        c = '.';
      }
    }
    if (c == 'e' || c == 'E') {         // read exponent
      if (t == Token::INTEGER) {
        t = Token::FLOAT;
        f = (double) i;
      }
      bool sign = false;
      c = get_char();
      if (c == '+') {
        c = get_char();
      } else if (c == '-') {
        c = get_char();
        sign = true;
      }
      fint exp = asnum(c);
      if (! is_digit(c)) {
        return TokenizingError("illegal exponent");
      }
      while (is_alphadigit(c = get_char())) {
        if (! is_digit(c)) {
          return TokenizingError("illegal exponent");
        }
        exp *= 10;
        exp += asnum(c);
      }
      if (sign) {
        while (exp --) f /= 10.0;
      } else {
        while (exp --) f *= 10.0;
      }
    }
  }
  push_char(c);
  
  if (neg) {
    if (t == Token::INTEGER) i = - i; else f = - f;
  }
  if (t == Token::INTEGER) {
    return new Token(t, ss, i, l, col);
  } else {
    assert(t == Token::FLOAT, "unexpected token type");
    return new Token(t, f, l, col, ss);
  } 
}

Token* Scanner::read_string() {
  return read_general_string('\'', Token::STRING);
}


Token* Scanner::read_general_string(char delimiter, Token::TokenType tokenType) {
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;
  char* b = buffer;
  fint c;
  bool cannot_be_a_delimeter;
  do {
    Token* t = read_char(b, cannot_be_a_delimeter);
    if (t) return t;     // Error return.
    c = *b++;
    if (b >= &buffer[ScannerBufferSize]) {
      return TokenizingError("string literal or comment too long");
    }
  } while (cannot_be_a_delimeter  ||  (c != delimiter && c != EOF));
  if (c == EOF) {
    return TokenizingError("missing trailing ' of string literal or comment");
  }
  b[-1] = '\0';
  return new Token(tokenType, 
                   new String(copy_string(buffer, b-buffer), b-buffer-1), 
                   l, col, ss);
}


Token* Scanner::read_dot() {
  fint l = line;
  fint col = column - 1;
  const char* ss = sourceAddr() - 1;
  return new Token(as_TokenType('.'), l, col, ss);
}

Token* Scanner::get_token() {
  Token* t;
  if (tokens) {
    t = tokens->token;
    tokens = tokens->prev;
  } else {
    t = NULL;
    while (t == NULL) {
      fint c = get_char();
      switch (c) {
       case EOF:
        depth = 0;
        t = new Token(Token::ACCEPT, line, column, sourceAddr());
        break;
       case '\n':
       case '\r':
        if (depth <= 0 && !is_string_scanner()) {
          t = new Token(Token::ACCEPT, line, column - 1, sourceAddr() - 1);
          depth = 0;
        }
        break;
       case ' ':
       case '\t':
       case '\v':
       case '\b':
       case '\f':
        break;
       case '"':
        t = skip_comment();
        break;
       case '(':
        depth ++;
        t = new Token(as_TokenType('('), line, column - 1, sourceAddr() - 1);
        break;
       case ')':
        depth --;
        t = new Token(as_TokenType(')'), line, column - 1, sourceAddr() - 1);
        break;
       case '[':
        depth ++;
        t = new Token(as_TokenType('['), line, column - 1, sourceAddr() - 1);
        break;
       case ']':
        depth --;
        t = new Token(as_TokenType(']'), line, column - 1, sourceAddr() - 1);
        break;
       case '.':
        t = read_dot();
        break;
       case '\'':
        t = read_string();
        break;
       case '\\':
        c = get_char();
        if (c == '\n' || c == '\r') {
          // an escaped newline; ignore
        } else {
          push_char(c);
          c = '\\';
          t = read_op(c);
        }
        break;
       case '{':
        t = new Token(Token::ANNOTATION_START, line, column - 1, sourceAddr() - 1);
        break;
       case '}':
        t = new Token(Token::ANNOTATION_END,   line, column - 1, sourceAddr() - 1);
        break;
       default:
        if (is_digit(c) || c == '-') t = read_number(c);
        else if (is_id_alpha(c) || c == ':') t = read_name(c);
        else if (is_punct(c)) t = read_op(c);
        else t = TokenizingError("unknown character in input");
      }
    }
  }
  if (t && PrintTokens) t->print();
  return t;
}

void Token::print() {
  switch (type) {
   case ERROR_TOKEN:
    lprintf("an lexical error token");
    break;
   case ACCEPT:
    lprintf("the end-of-expression token");
    break;
   case INTEGER:
    lprintf("an integer literal token: %ld", long(integer));
    break;
   case FLOAT:
    lprintf("a float literal token: %g", floating);
    break;
   case STRING:
    lprintf("a string literal token: '");
    string->Print();
    lprintf("'");
    break;
   case SELF_TOKEN:
    lprintf("the self token");
    break;
   case RESEND_TOKEN:
    lprintf("the resend token");
    break;
   case NAME:
    lprintf("a name token: "); 
    string->Print();
    break;
   case PRIMNAME:
    lprintf("a primitive name token: "); 
    string->Print();
    break;
   case DELEGATE:
    lprintf("a delegatee token: "); 
    string->Print();
    break;
   case OPERATOR:
    lprintf("a binary operator token: ");
    string->Print();
    break;
   case KEYWORD:
    lprintf("a keyword token: ");
    string->Print();
    break;
   case PRIMKEYWORD:
    lprintf("a primitive keyword token: ");
    string->Print();
    break;
   case CAPKEYWORD:
    lprintf("a capitalized keyword token: ");
    string->Print();
    break;
   case ARG:
    lprintf("an argument token: ");
    string->Print();
    break;
   case ARROW:
    lprintf("the <- token");
    break;
   default:
    lprintf("a '%c' token", type);
    break;
  }
  lprintf("\n");
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "branchSet.hh"
# pragma implementation "branchSet_inline.hh"
# include "_branchSet.cpp.incl"


bool BranchInfo::ResolveBranch( AbstractByteCode* b, 
                                LabelSet* labelSet, objVectorOop literals) {
  if (!isIndexed) {
    return labelSet->ResolveLabel(b, labelOrLabels,
                                  literals, literalIndex);
  }
  assert_objVector(labelOrLabels, "just checking");
  objVectorOop lbls= objVectorOop(labelOrLabels);
  for (int32 i = 0,  n = lbls->length();  i < n; ++i)
    if ( !labelSet->ResolveLabel( b, lbls->obj_at(i), lbls, i))
      return false;
  return true;
}


bool BranchSet::ResolveBranches(AbstractByteCode* b, LabelSet* labelSet, 
                                objVectorOop literals) {
  for ( int32 i = 0;  i < index;  ++i) {
    if (!branches[i]->ResolveBranch( b, labelSet, literals))
      return false;
  }
  return true;
}


void BranchSet::RecordBranch( bool isIndexed, int32 literalIndex, 
                              oop labelOrLabels) {
  if (index == maxIndex) {
    grow();
  }
  branches[index] = new BranchInfo(isIndexed, literalIndex, labelOrLabels );
  ++index;
}


void BranchSet::grow() {
  maxIndex <<= 1;
  BranchInfo** b = branches;
  allocateArrays();
  for ( int32 i = 0;  i < index; ++i ) {
    branches[i] = b[i];
  }
}


void BranchSet::oops_do(oopsDoFn f) {     
  for (int32 i = 0;  i < index;  ++i)
    (*f)(&branches[i]->labelOrLabels); 
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "list.hh"
# include "_list.cpp.incl"


List* List::Append(ParseNode* e) {
  ListElement* le = new ListElement(e);
  if (tail) {
    tail->SetNext(le);
  } else {
    head = le;
  }
  tail = le;
  return this;
}

fint List::Length() {
  fint count = 0;
  for (ListElement* e = Head(); e; e = e->Next()) {
    count ++;
  }
  return count;
}

void List::PrintSeparatedBy(const char* sep) {
  for (ListElement* e = Head(); e; e = e->Next()) {
    e->Print();
    if (sep) lprintf("%s", sep);
  }
}

bool List::GenByteCodes(AbstractByteCode* b, Object* parent, bool isExpr) {
  Unused(isExpr);
  for (ListElement* e = Head(); e; e = e->Next()) {
    if (! e->Data()->GenByteCodes(b, parent)) return false;
  }
  return true;
}


void List::addCommentAnnotations(Scanner* scanner) {
  for (ListElement* e = Head(); e; e = e->Next())
   e->Data()->addCommentAnnotations(scanner);
}

void List::oops_do(oopsDoFn f) {
  for (ListElement* e = Head(); e; e = e->Next())
    e->oops_do(f);
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slot.hh"
# include "_slot.cpp.incl"

# define CHECK(e)                                                             \
    if ((e) == 0) return 0
  
  slotList* Slot::make_list(slotList* slots, fint& index) {
    preservedObj ps(slots);                  // for scavenging!
    oop contents = slot_contents(index);     // this may run Self!!
    if (!contents->is_mark()) {
      return slots->add(new_string(name->AsCharP()), 
                        slot_type(),
                        contents, 
                        *annotation == '\0'
                         ? Memory->slotAnnotationObj
                         : new_string(annotation));
    } else {
      return NULL;
    }
  }


void Slot::Print() {
  name->Print();
}

fint SlotList::ArgCount() {
  fint n = 0;
  for (SlotListElement* e = Head(); e; e = e->Next()) {
    if (e->Data()->IsArgSlot()) n++;
  }
  return n;
}

bool SlotList::make_list(slotList* &s) {    // turns the Slots into oops
  fint index = 0;
  s = EMPTY;
  for (SlotListElement* e = Head(); e; e = e->Next()) {
    s = e->Data()->make_list(s, index);
    CHECK(s);
  }
  return true;
}


SlotList* SlotList::Append(Slot* elem, Parser* parser) {
  for (SlotListElement* e = Head();  e;  e = e->Next()) {
    if (elem->collides(e->Data())) {
      char errmsg[255];
      sprintf(errmsg, "slot %s already defined", elem->name->AsCharP());
      parser->syntaxError(errmsg, elem->startToken);
      return NULL;
    }
  }
  return (SlotList*) List::Append(elem);
}


void ArgSlot::Print() {
  lprintf(":");
  name->Print();
}


Object* ArgSlotList::AddArgs(Object* b, Parser* parser) {
  for (ArgSlotListElement* e = Head(); e && b; e = e->Next()) {
    if (b->AddArg(e->Data(), parser) == NULL) return NULL;
  }
  return b;
}


void DataSlot::Print() {
  Slot::Print();
  if (is_parent) lprintf("*");
  if (expr) {
    lprintf(assignable ? " <- " : " = ");
    expr->Print();
  }
}


void DataSlot::addCommentAnnotations(Scanner* scanner) {
  if (expr) expr->addCommentAnnotations(scanner);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "positionTable.hh"
# include "_positionTable.cpp.incl"

void ByteCodePositionTable::GenCode(fint offset, fint length, fint c) {
  Unused(c);
  if (maxCodeIndex <= codeIndex + 1) {
    maxCodeIndex *= 2;
    positions= positions->cloneSize(maxCodeIndex);
  }
  positions->obj_at_put(codeIndex++, as_smiOop(offset));
  positions->obj_at_put(codeIndex++, as_smiOop(length));
}
/* Sun-$Revision: 30.17 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "codeSlotsMap.hh"
# include "_codeSlotsMap.cpp.incl"


slotsOop methodMap::basic_create_method(slotList* slots,
                                         ByteCode* b,
                                         methodMap &m1,
                                         const char* annotation,
                                         bool isBlock) {

  slotsOop method;
  methodMap* m= (methodMap*) create_map(isBlock
                                        ? sizeof(blockMethodMap)
                                        : sizeof(methodMap),
                                        slots, &m1, (oop*)&method);
  m->setFile(b->file);
  m->setLine(b->line);
  m->setLiterals(b->literals);
  m->setSource(b->source);
  m->setCodes(b->codes);
  if (isBlock) {
    blockMethodMap *bmm= (blockMethodMap*)m;
    Memory->store((oop*)&bmm->_sourceOffset, b->sourceOffset);
    Memory->store((oop*)&bmm->_sourceLen,    b->sourceLen);
  }    

  if (*annotation != '\0') m->set_annotation(new_string(annotation));

  return method;
}


oop codeSlotsMap::mirror_names(oop ignored) {
  Unused(ignored);
  objVectorOop r= Memory->objVectorObj->cloneSize(length_nonVM_slots());
  fint i= 0;
  {  // arguments go first, in order of their declaration
    FOR_EACH_SLOTDESC(this, sd) {
      if (sd->is_arg_slot()) {
        r->obj_at_put(smiOop(sd->data)->value(), sd->name);
        ++i;
      }
    }
  }
  {  // other slots
    FOR_EACH_SLOTDESC(this, sd) {
      if (!sd->is_vm_slot() && !sd->is_arg_slot()) {
        r->obj_at_put(i++, sd->name);
        if (sd->is_obj_slot())
          r->obj_at_put(i++, sd->assignment_slot_name());
      }
    }
  }
  return r;
}


// return mirror for enclosing method

oop blockMethodMap::mirror_parent(oop obj) {
  Unused(obj);
  return get_lexical_link()->as_mirror();
}


// set the outer method link in literals

void methodMap::set_outer_method_link_in_literals( slotsOop outerMethod ) {
  // set literals
  // can get here of outerMethod is a blockmethod when fixing up
  //  block method bytecodes after adding/rming slot to block
  
  if ( outerMethod->kind() != OuterMethodType)
    return;

  objVectorOop lits = literals();
  lits = (objVectorOop)lits->copy_add_slot( VMString[METHOD_POINTER], 
                                            vm_obj_slotType,
                                            outerMethod,
                                            Memory->slotAnnotationObj,
                                            true);

  assert(lits->is_objVector(), "copy_add_slot failed!");
  assert(lits != literals(), "need fresh copy of literals");
  setLiterals(lits); 
}


// set my object's lexical link (functional), return new method

slotsOop blockMethodMap::set_lexical_link( slotsOop block_method, 
                                           slotsOop enclosingMethod ) {
  oop block_meth_or_mark=
    block_method->copy_add_slot( VMString[LEXICAL_PARENT],
                                 vm_parent_map_slotType,
                                 enclosingMethod,
                                 Memory->slotAnnotationObj,
                                 true);
  assert_slots(block_meth_or_mark, "add_slot failed");
  slotsOop new_block_meth = slotsOop(block_meth_or_mark);
  blockMethodMap* new_map = (blockMethodMap*) new_block_meth->map();    
  assert(this != new_map,  "must have new map");
  
  if (source() == Memory->stringObj) {
    // set the source offset and length of the old version back to
    // zero as it doesn't have a source string, and we don't want
    // it to trip the verify in blockMethodMap.
    _sourceOffset= _sourceLen= smiOop_zero;
  }
  return new_block_meth;
}


// Walk recursively through blocks and set lexical links
//  Precondition: I am a new method map
//  Postcondition, unless isOKToBashLiteralVector,
//    my literals will be new and bashable, for me and for other routines
//    that follow

void methodMap::set_lexical_links( slotsOop   enclosingMethod,
                                   slotsOop       outerMethod,
                                   stringOop  src,
                                   fint       srcOffset,
                                   bool       isOKToBashLiteralVector) {



  // Setting lexical links will create new blocks to store in the literals
  // Before mutating literals, be sure no other method is sharing it.
  // Only have to worry about outermost method's literals here:
  //  blocks are changed functionally, and create_block creates new lit vector.
  //  (of course copy_add_slots can be used on a block, but then its
  //   method IS the outermost method here)
  // -- dmu 4/96

  if (!isOKToBashLiteralVector)
    setLiterals(literals()->copy());

  // handle blocks referred to by the literal vector
  objVectorOop lits = literals();
  for (fint index = 0; index < lits->length(); index++) {
    oop obj = lits->obj_at(index);
    if (obj->is_block())  {
      slotsOop v = blockOop(obj)->value();
      if (v->has_code()) {
        blockMethodMap* bmm = (blockMethodMap*) v->map();
        slotsOop new_block_meth = bmm->set_lexical_link( v, enclosingMethod );

        // at this stage, have new block_meth, but its literals
        //  point to wrong outer method, and its blocks point to
        //  wrong parent method

        blockOop new_block = create_block(new_block_meth);
        lits->obj_at_put( index, new_block);

        // now fix the block's method, better clone literals because they
        //  may be shared with old block

        ((blockMethodMap*)new_block_meth->map()) 
            -> set_lexical_links( new_block_meth, 
                                  outerMethod, 
                                  src, 
                                  srcOffset, 
                                  false);
      }
    }
  }
}


// pass down src recursively, since for run scripts,
//  the outer method does not have the source, but the enclosing
//  block does -- dmu 12/95

void blockMethodMap::set_lexical_links( slotsOop  enclosingMethod,
                                        slotsOop      outerMethod,
                                        stringOop src,
                                        fint      srcOffset, 
                                        bool      isOKToBashLiteralVector) {
  if (source() == Memory->stringObj) {
    // fix up source and source offset to refer to source of outerMethod
    assert(enclosingMethod->kind() == BlockMethodType,
           "was expecting a nested block method");
    setSource(src);
    srcOffset += _sourceOffset->value();
    _sourceOffset= as_smiOop(srcOffset);
  }
  else
    src = source();

  methodMap::set_lexical_links( enclosingMethod, 
                                outerMethod, 
                                src, 
                                srcOffset, 
                                isOKToBashLiteralVector);
}



// called from create_outer_method_prim, and also
// by VM for creating the prototype outer method,
// by VM for Eval'ing expressions,
// by Object::GenBody for parsing expressions,
// by lookupErrorCode for creating lookup error methods,
// by Process::initialize to create method to start a process
// by evalExpressions in shell for top-level read-eval-print loop

slotsOop methodMap::create_outerMethod( slotList*   slots, 
                                        ByteCode*   b,
                                        const char* annotation,
                                        IntBList*   stack_deltas ) {
  slots = slots->add(VMString[SELF], vm_parent_map_slotType, NULL);
  outerMethodMap m;
  slotsOop method = methodMap::basic_create_method(slots, b, m, annotation, false);
  return (slotsOop) method->fix_up_method(NULL, true, true, stack_deltas);
}


slotsOop blockMethodMap::create_blockMethod( slotList*   slots, 
                                             ByteCode*   b, 
                                             const char* annotation,
                                             IntBList*   stack_deltas) {
  slots = slots->add(VMString[LEXICAL_PARENT], vm_parent_map_slotType, 
                     Memory->lobbyObj);
  blockMethodMap m;
  // since starting from slot list do not have to fix it up
  slotsOop method = methodMap::basic_create_method(slots, b, m, annotation, true);
  // except for stack_deltas!
  return stack_deltas 
    ?  (slotsOop) method->fix_up_method(NULL, true, true, stack_deltas) 
    :             method;
    
}


bool methodMap::verify(oop obj) {
  bool flag = slotsMap::verify(obj);
  if (flag) {
    /* cannot verify this anymore, because
     * methods are now created with byteVectors, 
     * since fix_up copies and canonizes--dmu 11/95
     *
     * if (! obj->codes()->is_string()) {
     *  error1("codes of method oop 0x%lx is not a string", obj);
     *  flag = false;
     * }
     *
     */
    if (! obj->literals()->is_objVector()) {
      error1("literals of method oop 0x%lx is not an objVectorOop", obj);
      flag = false;
    }
    if (! obj->file()->is_string()) {
      error1("file of method oop 0x%lx is not a  string", obj);
      flag = false;
    }
    if (! obj->line()->is_smi()) {
      error1("line of method oop 0x%lx is not a smiOop", obj);
      flag = false;
    }
  }
  return flag;
}

bool outerMethodMap::verify(oop obj) {
  bool flag = methodMap::verify(obj);
  if (flag) {
    if (! obj->source()->is_string()) {
      error1("source of method oop 0x%lx is not a string", obj);
      flag = false;
    }
  }
  return flag;
}

bool blockMethodMap::verify(oop obj) {
  bool flag = methodMap::verify(obj);
  if (flag) {
    if (! source()->is_string()) {
      error1("source of block method oop 0x%lx is not a string", obj);
      flag = false;
    }
    if (! _sourceOffset->is_smi())  {
      error1("source offset of block method oop 0x%lx is not a smallInt", obj);
      flag = false;
    }
    if (! _sourceLen->is_smi())  {
      error1("source length of block method oop 0x%lx is not a smallInt", obj);
      flag = false;
    }
    if (   _sourceOffset->value() < 0
        || _sourceOffset->value() > source()->length())  {
      error1("source offset of block method oop 0x%lx is out of bounds", obj);
      flag = false;
    }
    if (   _sourceLen->value() < 0
        || _sourceOffset->value() + _sourceLen->value() > source()->length()) {
      error1("source length of block method oop 0x%lx is out of bounds", obj);
      flag = false;
    }
  }
  return flag;
}
  
oop methodMap::mirror_codes(oop r)    { Unused(r); return _codes;     }
oop methodMap::mirror_literals(oop r) { Unused(r); return literals(); }
oop methodMap::mirror_source(oop r)   { Unused(r); return source();   }
oop methodMap::mirror_file(oop r)     { Unused(r); return file();     }
oop methodMap::mirror_line(oop r)     { Unused(r); return line();     }



//----------------------------------------------------------------------

// Used to return bci at beginning of statement that arg bci is in.
// Finds points where stack is empty.
// Branch-naive (for now)


class StatementBoundaryBytecodeScanner : public stack_depth_interpreter {
 public:
  fint target_bci;
  fint statement_start;
  fint prev_stack_depth;
  
  StatementBoundaryBytecodeScanner( methodMap* mm, fint tbci )
  : stack_depth_interpreter(mm) {
    target_bci = tbci;
    statement_start = mi.firstBCI();
    prev_stack_depth = 0;
  }
  
  void interpret_method() {
    for (  ;  pc < target_bci;  ++pc ) {
      interpret_bytecode();
      if ( get_error_msg() )
        return;
      if ( get_stack_depth() == 0  
      &&   prev_stack_depth  != 0 )
        statement_start = pc + 1;
      prev_stack_depth = get_stack_depth();
    }
  }
};

fint methodMap::beginningOfStatement(fint bci) {
  StatementBoundaryBytecodeScanner s(this, bci);
  s.interpret_method();
  return s.statement_start;
}


// ----------------------------------------------------------------------


void methodMap::print_string(oop obj, char* buf) {
  Unused(obj);
  sprintf(buf, "<a method>");
}


void methodMap::print_oop(oop obj) {
  lprintf("<a method");
  if (PrintOopAddress) {
    lprintf(" (0x%lx)", (void*)obj);
  }
  lprintf(">");
}


void methodMap::print_source() {
  source()->string_print(); }


void blockMethodMap::print_source() {
  if (_sourceOffset == smiOop_zero)
    source()->string_print();
  else
    source()->string_print(_sourceOffset->value(), _sourceLen->value());
}



// returns the literal index for a bytecode at given bytecode pc
// including any preceeding index bytecodes

fint methodMap::get_index_at(fint byteCodeIndex) {
  u_char* start = start_codes();
  u_char* here = start + byteCodeIndex;
  u_char* p = here;
  for (p = here;
       p > start  &&  getOp(p[-1]) == INDEX_CODE;
       p--)
    ;
  fint x = 0;
  while (p <= here)
    x = x << INDEXWIDTH  |  getIndex(*p++);
  return x;
}

stringOop methodMap::get_selector_at(fint byteCodeIndex) {
  // determine whether this send is a primitive or not
  fint code = start_codes()[byteCodeIndex];
  fint op = getOp(code);
  assert(op == SEND_CODE || op == IMPLICIT_SEND_CODE,
         "expecting a send");
  fint x = get_index_at(byteCodeIndex);
  assert(x >= 0 && x < length_literals(), "literal index out of bounds");
  stringOop sel = (stringOop) start_literals()[x];
  assert_string(sel, "expecting a string message name");
  return sel;
}

int32 methodMap::debug_size(oop p) {
  int32 s = (p->size() + size()) * oopSize;             // method obj and map
  s += codes()->size() * oopSize + codes()->length();   // byte codes
  s += literals()->size()* oopSize;                     // literal vector
  for (fint i = literals()->length() - 1; i >= 0; i--) {
    oop l = literals()->obj_at(i);
    if (l->is_mem()) s += literals()->obj_at(i)->size() * oopSize;
  }
  return s;
}


fint codeSlotsMap::arg_count() {
  fint argc = 0;
  FOR_EACH_SLOTDESC(this, slot) {
    if (slot->is_arg_slot()) argc++;
  }
  assert(has_code() || argc == 0, "only objects with code can have arg slots");
  return argc;
}


// a helper

class oldMapList: public ResourceObj {
 public:
  methodMap*  method_map;
  oldMapList* lexicalParent;

  oldMapList(methodMap* m) {
    method_map = m;
    if (m == NULL) {
      lexicalParent = NULL;
      return;
    }
    methodMap* mm = m->get_lexical_link_map();
    if (mm == NULL) 
      lexicalParent = NULL;
    else
      lexicalParent = new oldMapList(mm);
  }
  
  oldMapList(methodMap* m, oldMapList* n) {
    method_map = m;
    lexicalParent = n;
  }
  methodMap* ancestor(fint n) {
    assert(this != NULL, "ancestors past null");
    return n == 0 ? method_map : lexicalParent->ancestor(n - 1);
  }
};
  

// called after doing a functional programming primitive
//  to fix up the backpointers, and local access bytecodes;
// obj had better be a brand-new method never executed
//  cause we change the map without invalidating code or stack frames
//
// Pass in old optimized method (or NULL) to fix up local access bytecodes.
//  -- dmu

oop methodMap::fix_up_method( oop       new_obj,
                              oop       old_optimized_method,
                              bool      isOKToBashLiteralVector,
                              bool      mustAllocate,
                              IntBList* stack_deltas ) {
  // get a new map in case some other method uses the same map
  // set_lexical_links will be changing it, I think
  // do not rely on lexical links of old method, because
  //  e.g. copy_remove_slots does not preserve them

  assert_slots(new_obj, "obj must have this for its map");
  slotsOop new_meth = slotsOop(new_obj);

  methodMap* new_map = (methodMap*)copy(mustAllocate);
  if (new_map == NULL) return failedAllocationOop;
  new_map->enclosing_mapOop()->init_mark();

  new_meth->set_map(new_map);

  new_map->set_lexical_links( new_meth, new_meth, new_meth->source(), 0,
                              isOKToBashLiteralVector);
  // set_lexical_links has insured that literals in new_meth
  //   and all its contained block-methods are new
  //   (if isOKToBaseLiteralVector then literals in top-level method
  //    were new and need not have been cloned)

  methodMap* old_map = NULL;
  if (old_optimized_method != NULL) {
    Map* m = old_optimized_method->map();
    assert(m->has_code(), "must be a methodMap");
    old_map = (methodMap*)m;
  }
  ResourceMark rm;
  oldMapList* old_maps = new oldMapList(old_map);
  new_map->fix_local_bytecodes_and_links( old_optimized_method == NULL
                                            ? NULL
                                            : old_maps,
                                          new_meth,
                                          stack_deltas);
  return new_meth;
}


// a slot has been added or removed to a method;
//  given the method's old map, replace the local access bytecodes
//  for the old map with ones for me. (oldMap may be null)

void methodMap::fix_local_bytecodes_and_links( oldMapList* old_maps,
                                               slotsOop   outerMethod,
                                               IntBList*  stack_deltas ) {
  fix_local_bytecodes_and_links_of_my_blocks( old_maps, outerMethod);
  fix_local_bytecodes_and_links_in_myself(    old_maps, outerMethod, 
                                              stack_deltas);
}


class BytecodeFixerUpper : public abstract_interpreter {
  oldMapList* old_maps;
  BoolBList* branchTargets;
  IntBList* stack_deltas;
public:
  ByteCode bcode;

  BytecodeFixerUpper(methodMap *mm, oldMapList *oml, IntBList* sds) 
    : abstract_interpreter(mm), bcode(true) {
    old_maps= oml;
    bool gotOne;
    mm->branch_targets(gotOne, &branchTargets); 
    stack_deltas= sds;
  }
    
 protected:
  void define_label() {
    if (stack_deltas) {
      int32 d = stack_deltas->nth(pc);
      for ( ; d > 0;  --d )  do_SELF_CODE();
      for ( ; d < 0;  ++d )  do_POP_CODE();
    }
    if ( pc < branchTargets->length()  &&  branchTargets->nth(pc)) {
      bool ok = bcode.GenLabelDefinition(as_smiOop(pc));
      if (!ok)
        fatal1("The parser and bytecode verifier should have screened out"
               " the label error below:\n%s\n", bcode.errorMessage);
    }
  }
  
 public:
    
  void interpret_method() {
    abstract_interpreter::interpret_method();
    define_label();
  }

  void interpret_bytecode() {
    define_label();
    abstract_interpreter::interpret_bytecode();
  }

protected:
  void do_SELF_CODE() { bcode.GenSelfByteCode(0, 0); }
  void do_POP_CODE()  { bcode.GenPopByteCode( 0, 0); }
  void do_NONLOCAL_RETURN_CODE() { bcode.GenNonLocalReturnByteCode(0, 0); }
  
  void do_BRANCH_CODE()          { 
    bool ok = bcode.GenBranchByteCode( 0, 0, as_smiOop(get_branch_pc()));
    assert(ok, "should not have any branch errors here"); 
  }
  void do_BRANCH_TRUE_CODE()     { 
    bool ok = bcode.GenBranchTrueByteCode(  0, 0, as_smiOop(get_branch_pc()));
    assert(ok, "should not have any branch errors here"); 
  }
  void do_BRANCH_FALSE_CODE()    { 
    bool ok = bcode.GenBranchFalseByteCode(  0, 0, as_smiOop(get_branch_pc())); 
    assert(ok, "should not have any branch errors here"); 
  }
  void do_BRANCH_INDEXED_CODE()  { 
    bool ok = bcode.GenBranchIndexedByteCode(0, 0, get_branch_vector()); 
    assert(ok, "should not have any branch errors here"); 
  }

  void do_literal_code(oop lit) { bcode.GenLiteralByteCode(0, 0, lit); }
  
  void do_read_write_local_code(bool isWrite) {
    slotDesc *sd= old_maps->ancestor(is.lexical_level)->getLocalSlot(0, is.index);
    assert(sd->name != VMString[SELF],  "never really send self");
    stringOop selector=
      isWrite  ?  sd->assignment_slot_name()  :  sd->name;
    mi.map()->GenSendOrLocal(&bcode, selector); }
    
  void do_SEND_CODE() {
    bcode.GenSendByteCode( 0, 0, get_selector(), false, false, NULL); }
    
  void do_IMPLICIT_SEND_CODE() {
    if ( is.is_undirected_resend || is.delegatee ) {
      bcode.GenSendByteCode( 0, 0, get_selector(), true,
                           is.is_undirected_resend, is.delegatee);
    }
    else {
      mi.map()->GenSendOrLocal( &bcode, get_selector());
    }
  }
};



void methodMap::fix_local_bytecodes_and_links_in_myself( oldMapList* old_maps,
                                                         slotsOop outerMethod,
                                                         IntBList* stack_deltas )
{
  if (old_maps != NULL) {
    assert(codes() == old_maps->method_map->codes(),
           "old map should have same codes");
  }

  BytecodeFixerUpper b(this, old_maps, stack_deltas);
  b.interpret_method();
  if ( !b.bcode.Finish() )
    fatal("should have passed parser already");
  setLiterals(b.bcode.literals);
  set_outer_method_link_in_literals(outerMethod);
  setCodes(new_string(b.bcode.codes->bytes(), b.bcode.codes->length()));
}




void methodMap::fix_local_bytecodes_and_links_of_my_blocks(oldMapList* old_maps,
                                                           slotsOop   outerMethod) {
  
  oop* new_litp = start_literals();
  oop* new_lit_end = new_litp + length_literals();

  for ( ;  new_litp < new_lit_end;  ++new_litp ) {
    if ((*new_litp)->is_block_with_code()) {

      blockMethodMap* nm = 
        (blockMethodMap*) blockOop(*new_litp)->value()->map();
        
      // since block code is fixed when the block method was created,
      //  dont have to pass down old maps here

      oldMapList* old_maps_out = new oldMapList(nm, old_maps);
      nm->fix_local_bytecodes_and_links(old_maps_out, outerMethod);
    }
  }
}


void methodMap::GenSendOrLocal( ByteCode *bc,
                                stringOop sel) {
  stringOop selOfSlot = NULL;
  methodMap* m = this;

  if ( UseLocalAccessBytecodes  &&  sel->is_unary() )
    selOfSlot = sel;  // reading
  else if ( UseLocalAccessBytecodes  &&  sel->is_1arg_keyword() )
    selOfSlot = sel->unary();  // writing
  else
    m = NULL; // don't even try

  fint lexicalLevel = 0;
  for ( ;
        m != NULL;
        ++lexicalLevel,  m = m->get_lexical_link_map()) {
    slotDesc* sd = m->find_nonVM_slot(selOfSlot);
    if ( sd != NULL 
         // check for write of const slot
         && (sel == selOfSlot  ||  sd->is_obj_slot())) {
      bc->GenRWLocalByteCode( 0, 0,
                              sel == selOfSlot,
                              lexicalLevel,
                              sd - m->slots());
      return;
    }
  }
  bc->GenSendByteCode(0, 0, sel, true, false, NULL);
}
  

// return contents of my lexical link
slotsOop blockMethodMap::get_lexical_link() {
  slotDesc* lps = find_slot(VMString[LEXICAL_PARENT]);
  assert(lps != NULL, "block must have lexical link");
  assert(lps->is_map_slot(), "only have map");
  oop r = lps->data;
  assert_slots(r, "lexical parent must be a method");
  return slotsOop(r);
}
  

// return contents of my lexical link
methodMap* blockMethodMap::get_lexical_link_map() {
  Map* m = get_lexical_link()->map();
  return m->has_code() ? (methodMap*)m : NULL;
}
  

slotDesc* methodMap::getLocalSlot( fint lexicalLevel, fint index ) {
  methodMap* m = this;
  for (fint i = 0;  i < lexicalLevel; i++) {
    m = m->get_lexical_link_map();
    assert(m != NULL, "bad lexical level");
  }
  return m->slot(index);
}


//----------------------------------------------------------------------

// need only for simplistic stack alloc nic hack

class BytecodeBranchScanner : public abstract_interpreter {
public:
  bool hasBranch;

  BytecodeBranchScanner(methodMap *m) : abstract_interpreter(m) {
    hasBranch= false; }

protected:
  void do_branch_code( int32 /* target_PC */, oop /* target_oop */ = badOop ) {
    hasBranch= true;
    pc= mi.length_codes; // look no further
  }
  void do_BRANCH_INDEXED_CODE() {
    hasBranch= true;
    pc= mi.length_codes; // look no further
  }
};


bool methodMap::containsBranch() {
  BytecodeBranchScanner sc(this);
  sc.interpret_method();
  return sc.hasBranch;
}


//----------------------------------------------------------------------

class BytecodeLoopScanner : public abstract_interpreter {
public:
  bool hasLoop;

  BytecodeLoopScanner(methodMap *m) : abstract_interpreter(m) {
    hasLoop= false; }

protected:
  void do_branch_code( int32 target_PC, oop target_oop = badOop ) {
    Unused(target_oop);
    if (target_PC <= pc) {
      hasLoop= true;
      pc= mi.length_codes; // look no further
    }
  }
  void do_BRANCH_INDEXED_CODE() {
    objVectorOop v= get_branch_vector();
    for ( int32 i = 0, n = v->length();  i < n;  ++i )
      if ( smiOop( v->obj_at(i) )->value() <= pc) {
        hasLoop= true;
        pc= mi.length_codes;
        return;
      }
  }
};


bool methodMap::containsLoop() {
  BytecodeLoopScanner sc(this);
  sc.interpret_method();
  return sc.hasLoop;
}


//----------------------------------------------------------------------


class BytecodeNLRScanner : public abstract_interpreter {
public:
  bool hasNLR;

  BytecodeNLRScanner(methodMap *m) : abstract_interpreter(m) {
    hasNLR= false; }

protected:
  void do_NONLOCAL_RETURN_CODE() {
    if (mi.map()->kind() == BlockMethodType)
      hasNLR= true; }

  void do_literal_code(oop lit) {
    if (lit->is_block_with_code()) {
      blockOop b= blockOop(lit);
      BytecodeNLRScanner sub((methodMap*)b->value()->map());
      sub.interpret_method();
      if (sub.hasNLR) {
        hasNLR= true;
        pc= mi.length_codes; // look no further
      }
    }
  }
};


bool methodMap::containsNLR() {
  BytecodeNLRScanner sc(this);
  sc.interpret_method();
  return sc.hasNLR;
}


//----------------------------------------------------------------------

// Used to find selectors of all sends that could uplevel-access local slots 
// (i.e. all implicit-self sends with 0 or 1 args that don't match
// a local slot); includes nested blocks

class AccessedLocalsBytecodeScanner : public abstract_interpreter {
  bool includeMySends;
public:
  StringOopList *selectors;

  AccessedLocalsBytecodeScanner(methodMap *m) : abstract_interpreter(m) {
    assert(!UseLocalAccessBytecodes, "relic of the past");
    includeMySends= m->kind() != OuterMethodType;
    selectors= EMPTY; }

protected:
  void do_SELF_CODE() {
    if (includeMySends) selectors= selectors->add(VMString[SELF]);
  }

  void do_literal_code(oop lit);

  void do_send_code(bool isImplicitSelf, stringOop selector, fint argc) {
    if (   !isImplicitSelf
        || !includeMySends
        || selector->is_prim_name())
      return;
    if (argc == 0  ||  (argc == 1 && !is_punct(selector->bytes()[0]))) {
      addName(selector, mi.map());
    }
  }

  void addName(stringOop selector, Map *map);

};



void AccessedLocalsBytecodeScanner::do_literal_code(oop lit) {
 
  if (!lit->is_block_with_code()) return;

  blockOop b= blockOop(lit);
  blockMethodMap* bmm= (blockMethodMap*)b->value()->map();
  AccessedLocalsBytecodeScanner sub(bmm);
  sub.interpret_method();

  Map *bm= b->map();

  for (StringOopListElem* e= sub.selectors->head(); e; e= e->next()) {
    stringOop selector= e->data();
    addName(selector, bm);
  }
}


void AccessedLocalsBytecodeScanner::addName(stringOop selector, Map *map) {
  // add selector to list if it is not already there and if it does not
  // match a local slot
  assert(!selector->is_prim_name(), "should not have one of these");

  if (selectors->includes(selector))
    return;

  FOR_EACH_SLOTDESC(map, slot) {
    if (slot->is_vm_slot()) continue;
    if (   slot->name == selector
        || slot->is_assignment_slot_name(selector))
      return;
  }
  selectors= selectors->append(selector);
}


IntList* methodMap::accessedSlots(blockMethodMap *bmm) {
  // get locals accessed by bmm
  AccessedLocalsBytecodeScanner sc(bmm);
  sc.interpret_method();

  // translate list of names to list of slot indices
  IntList* slotIndices= EMPTY;
  for (StringOopListElem* e= sc.selectors->head(); e; e= e->next()) {
    stringOop selector= e->data();
    FOR_EACH_SLOTDESC_N(this, s, i) {
      if (s->is_vm_slot()) continue;
      if (s->name == selector || s->is_assignment_slot_name(selector)) {
        slotIndices= slotIndices->append(i);
        break;
      }
    }
  }
  return slotIndices;
}

//----------------------------------------------------------------------

// Used to find indices of slots in enclosing block/method of a block that are
// uplevel-accessed.

class UplevelAccessedLocalsBytecodeScanner : public abstract_interpreter {
  fint targetLexicalLevel;
  methodMap *parentMap;

public:
  IntList *indices;

  UplevelAccessedLocalsBytecodeScanner(blockMethodMap *bmm, methodMap *pm,
                                       fint tll = -1);

protected:
  void do_literal_code(oop lit);
  void do_read_write_local_code(bool isWrite);
};




UplevelAccessedLocalsBytecodeScanner::UplevelAccessedLocalsBytecodeScanner(
  blockMethodMap *bmm, methodMap *pm, fint tll)
  : abstract_interpreter(bmm) {

  assert(UseLocalAccessBytecodes, "relic of the future");

  parentMap= pm;
  indices= EMPTY;

  if (tll >= 0) {
    targetLexicalLevel= tll;
  }
  else {
    targetLexicalLevel= 0;
    for (methodMap* m= mi.map(); 
         m != parentMap;
         m= m->get_lexical_link_map()) {
      ++targetLexicalLevel;
      if (m == NULL)
        ShouldNotReachHere(); // could not find parent
    }
  }
}

void UplevelAccessedLocalsBytecodeScanner::do_literal_code(oop lit) {
  if (!lit->is_block_with_code()) return;

  blockMethodMap *bmm= (blockMethodMap*) blockOop(lit)->value()->map();
  UplevelAccessedLocalsBytecodeScanner sub(bmm, parentMap,
                                           targetLexicalLevel + 1);
  sub.interpret_method();
  for (IntListElem *e= sub.indices->head(); e; e= e->next()) {
    fint index= e->data();
    if (!indices->includes(index))
      indices= indices->append(index);
  }
}

void UplevelAccessedLocalsBytecodeScanner::do_read_write_local_code(bool isW) {
  Unused(isW);
  if (   is.lexical_level == targetLexicalLevel
      && !indices->includes(is.index))
    indices= indices->append(is.index);
}


IntList* blockMethodMap::uplevel_accessed_slots(methodMap* parentMap) {
  UplevelAccessedLocalsBytecodeScanner sc(this, parentMap);
  sc.interpret_method();
  return sc.indices;      
}

//----------------------------------------------------------------------

// I record the bci's of expression stack values
// that are used, not just popped.
// If not debugMode, don't record "trivial" ones.
// hack: push negative PCs for debug-only values

class ExprStackBCIsBytecodeScanner : public stacking_interpreter {
  bool debugMode;
  bool onlyForDebug;
  IntList* bcis;
  char* used_bci_flags; // need flags cause used_bcis must be sorted
  IntList* used_bcis;
  
 public:

  ExprStackBCIsBytecodeScanner(methodMap *mm, bool debug)
    : stacking_interpreter(mm) {
    debugMode= debug; 
    bcis= EMPTY;
    used_bci_flags = NEW_RESOURCE_ARRAY(char, mi.length_codes);
    for (fint i = 0,  n = mi.length_codes;  
              i < n;  
              used_bci_flags[i++] = '\0')  { }
    used_bcis= EMPTY;
  }

 protected:

  void push() { 
    bcis= bcis->push(onlyForDebug ? -pc : pc); }
      
  void* pop(fint n = 1) { 
    assert(n >= 0, "just checking");
    for ( ; n > 0;  --n ) {
      int bci= bcis->pop(); 
      if ( bci < 0  &&  !debugMode ) {
        // toss it, debugOnly bci but not in debugMode
      }
      else
        used_bci_flags[ bci < 0  ?  -bci  :  bci] = '\1';
    }
    return NULL; 
  }
  
  fint get_stack_depth() { return bcis->length(); }

  void do_SELF_CODE() {
    onlyForDebug = true;
    stacking_interpreter::do_SELF_CODE();
  }
  void do_POP_CODE() {
    onlyForDebug = true; 
    stacking_interpreter::do_POP_CODE();
  }
  // don't do anything for nonlocal return code;
  //   previous push will have genererated expr bci
  void do_literal_code(oop lit);
  void do_read_write_local_code(bool isWrite);
  void do_send_code(bool isImp, stringOop sel, fint arg_count);
  
 public:
  IntList* result_bcis() { 
    for (fint i = mi.firstBCI(),  n = mi.length_codes;  
              i < n;  
              ++i ) {
      if (used_bci_flags[i])
        used_bcis= used_bcis->append(i);
    }
    return used_bcis; 
  }
  
  void interpret_method() {
    stacking_interpreter::interpret_method();
    check_and_pop(); // want bci of bytecode computing result that method returns
  }
};


void ExprStackBCIsBytecodeScanner::do_literal_code(oop lit) {
  if (lit->is_block_with_code()) {
    // even if blocks are just popped, the old version of this method
    // reported them, so keep that behavior -- dmu
    used_bci_flags[ pc ] = '\1';
  }
  onlyForDebug = !lit->is_block_with_code();
  stacking_interpreter::do_literal_code(lit);
}

void ExprStackBCIsBytecodeScanner::do_read_write_local_code(bool isWrite) {
  if (isWrite) {
    onlyForDebug = true;
  }
  else {
    // do not need to duplicate arg slot name descs
    // handled by CodeScopeDesc::exprStackElem
    onlyForDebug =
        is.lexical_level == 0
    &&  mi.map()->getLocalSlot(is.lexical_level, is.index)->is_arg_slot();
  }
  stacking_interpreter::do_read_write_local_code(isWrite);
}

void ExprStackBCIsBytecodeScanner::do_send_code(bool isImp, stringOop sel, fint arg_count) {
  slotDesc* s;
  
  // don't need to duplicate arg slot name descs
  onlyForDebug = 
      !UseLocalAccessBytecodes  
  &&   isImp  
  &&  (s= mi.map()->find_slot(sel)) != NULL
  && s->is_arg_slot();

  stacking_interpreter::do_send_code(isImp, sel, arg_count);
}


// return a list of bcis that push things on the stack that
//  will need to be recorded/visible for debugging

IntList* methodMap::expression_stack_bcis(bool debugMode) {
  ExprStackBCIsBytecodeScanner sc(this, debugMode || containsBranch() );
  sc.interpret_method();
  return sc.result_bcis();
}



//----------------------------------------------------------------------

class ExprStackBytecodeScanner : public stacking_interpreter {
  fint bci;
  bool keepArgs;
public:
  IntList *stack;

  ExprStackBytecodeScanner(methodMap *mm, fint b, bool keep, fint startBCI)
    : stacking_interpreter(mm) {
    bci= b;
    keepArgs= keep;
    pc= startBCI;
    stack= EMPTY;

    if ( /* !CheckAssertions  ||  */ 
         startBCI <= bci  ||  (bci == 0  &&  startBCI <= 1) )
      ;
    else
      fatal2("start(%d) > target(%d)  is OK only for first bytecode -- dmu 5/03",
              startBCI, bci);
    }

  void interpret_method() {
    while ( pc < mi.length_codes ) {
      interpret_bytecode();
      ++pc;
    }
  }


 protected:

  int   get_stack_depth() { return stack->length(); }
  void  push() { stack= stack->push(pc); }
  void* pop(fint n = 1) { stack->pop(n); return NULL; }
  
  void interpret_bytecode() {
    // at bci, stop if not a send
    //    if it is a send, do send then stop
    
    fetch_and_decode_bytecode();
    if (pc < bci)
      dispatch_bytecode();
    else if ( pc == bci
         &&   ( bc.op == SEND_CODE  ||  bc.op == IMPLICIT_SEND_CODE ) ) {
      dispatch_bytecode();
      pc= mi.length_codes;
    }
    else
      pc= mi.length_codes;
  }

  void do_send_code(bool isImp, stringOop sel, fint arg_count);
};

void ExprStackBytecodeScanner::do_send_code(bool isImp, stringOop sel, fint arg_count) {
  if (pc < bci) {
    stacking_interpreter::do_send_code(isImp, sel, arg_count);
  }
  else if (!keepArgs) {
    // pop off args (do not want args anyway)
    stacking_interpreter::do_send_code(isImp, sel, arg_count);
    pop();  // but pop result, send is suspended
  }
}



IntList* methodMap::expression_stack(fint bci, bool keepArgs,
                                     fint startBCI) {
  // returns the byte code indices making up the current expression stack
  // bci = current position; except for sends, it means we are suspended just 
  // before executing bytecode bci.  For sends, we are "during" the send, and
  // keepArgs determines whether the send's arguments should be included
  // or not.
  // oldest expr stack elem is first in list, most recent is last
  
  ExprStackBytecodeScanner sc(this, bci, keepArgs, startBCI);
  sc.interpret_method();
  return sc.stack->reverse();
}


//----------------------------------------------------------------------


// In order to make it easier to generate bytecodes,
// the checker allows the bytecodes after an unconditional branch
// to have a different expression stack height, as long at it matches
// all branches to the point after the unconditional branch.
// Since the compilers require the height to match,
// the bytecode checker returns a list of integers, indexed by pc.
// This list is the deltas required.
// In other words, a 1 means that a push self must be added before this
// code when fixing up the codes, a -1 means a pop must be added, etc.

// How the method is traversed:
//  when a branch is encountered, its destination(s) are
//  added to bb_heads_to_do.
//  If the branch is conditional, traversal continues in a straight line.

class BytecodeChecker : public stack_depth_interpreter {
 protected:
  IntBList* stack_depths; // for checking branch target stack depths
  IntBList* bb_heads_to_do; // for nonlinear traversal to compute deltas
  bool*     bcs_done;       // true if already checked this bc
  
public:
  IntBList* stack_deltas;
public:

  BytecodeChecker(byteVectorOop codes, objVectorOop literals)
  : stack_depth_interpreter(codes, literals) {
    stack_depths   = new IntBList( mi.length_codes + 1);
    stack_deltas   = new IntBList( mi.length_codes + 1);
    bb_heads_to_do = new IntBList( mi.length_codes );
    bcs_done = NEW_RESOURCE_ARRAY( bool, mi.length_codes );
    memset(bcs_done, 0, sizeof(bool) * mi.length_codes );
    
    for (fint i = 0;  i <= mi.length_codes;  ++i) {
      stack_depths->nthPut(i, -1, true); // set to do not care
      stack_deltas->nthPut(i,  0, true);
     }
    if (mi.length_codes > 0)
      bb_heads_to_do->push(mi.firstBCI());
  }

    
  
protected:
  
  bool check(aiCheckFn fn, oop p = NULL) {
    (*fn)(this, p);
    return !error_msg;
  }

  void check_stack_depth(int32 tpc) {
    int32 d= stack_depths->nth(tpc);
    if (d == -1) { // haven't checked it yet
      // was prev code an uncond branch?
      // if so, calc stack delta
      if ( tpc > 0  
      &&  getOp(mi.codes[tpc-1]) == BRANCH_CODE
      &&  stack_depths->nth(tpc-1) != -1 )
        stack_deltas->nthPut(tpc, get_stack_depth() - stack_depths->nth(tpc-1));
      stack_depths->nthPut(tpc, get_stack_depth());
    }
    else if (d != get_stack_depth()) {
      static char buf[BUFSIZ];
      sprintf(buf, "stack depth mismatch: %d vs %d, pcs %d vs %d",
              get_stack_depth(), d, pc, tpc);
      set_error_msg(buf);
    }
    if ( tpc == mi.length_codes  // ensure stack had entry at end of method
    &&   get_stack_depth() != 1) {
      static char buf[BUFSIZ];
      sprintf( buf, 
               "stack must be one deep at end of method, depth is %d",
               get_stack_depth());
      set_error_msg(buf);
    }
  }
  
    
  void do_BRANCH_CODE() {
    stacking_interpreter::do_BRANCH_CODE();
    if (error_msg)
      return;
    // handle successor of uncond branch
    // this even works if this bc is the last one
    // because can have branches to one past last bytecode
    int32 d = stack_depths->nth(pc+1);
    if (d != -1)
      stack_deltas->nthPut(pc+1, d - get_stack_depth());
    pc = mi.length_codes; // NOT mi.length_codes-1
  }
  
  
  void do_branch_code( int32 target_PC, oop target_oop = badOop ) {
    bb_heads_to_do->push( target_PC );
    stacking_interpreter::do_branch_code( target_PC, target_oop);
    if (error_msg)
      return;
    check_stack_depth( target_PC );
  }  
  
  
  void do_BRANCH_INDEXED_CODE()  { 
    stacking_interpreter::do_BRANCH_INDEXED_CODE();
    if (error_msg)
      return;
    objVectorOop v = get_branch_vector(); 
    for (int32 i = 0,  n = v->length();  i < n;  ++i) {
      assert_smi(v->obj_at(i), "should have been checked by get_branch_vector");
      int tpc= smiOop(v->obj_at(i))->value();
      bb_heads_to_do->push( tpc );
      check_stack_depth( tpc );
      if (error_msg)
        return;
    }
  }
  
  void do_NONLOCAL_RETURN_CODE() {
    if (pc + 1 != mi.length_codes) 
      set_error_msg("return is not last bytecode"); 
  }
   
  // self-level defined methods should not have these local codes in them;
  //  at self level cannot know the required indices, so VM adds these codes later
  void do_read_write_local_code(bool isWrite) { 
    if (!AllowLocalAccessInCreatedMethods)
      set_error_msg("read/write local are illegal (VM will insert them automatically)");
    stack_depth_interpreter::do_read_write_local_code(isWrite);
  }
  void do_LEXICAL_LEVEL_CODE()   { 
    if (!AllowLocalAccessInCreatedMethods)
      set_error_msg("illegal lexical level code"); 
    stack_depth_interpreter::do_LEXICAL_LEVEL_CODE();
  }
  void do_illegal_code()         { set_error_msg("undefined byte code"); }

  
 public:
  void interpret_method() {
    // must check stack depth before each code and after last one
    // NOT sequential
    while ( bb_heads_to_do->nonEmpty()) {
      for (  pc = bb_heads_to_do->pop(),
             set_stack_depth( 
               pc == mi.firstBCI()
                 ? 0                       // first bc starts w/ empty stack
                 : stack_depths->nth(pc)); // was set by branch to me
             pc < mi.length_codes
             &&  !bcs_done[pc];  // stop if already been here
             ++pc ) {
        check_stack_depth(pc);
        interpret_bytecode();
        bcs_done[pc] = true;
        if (get_error_msg())
          return;
      }
      if (pc == mi.length_codes) {
        // just fell off end of method, check stack
        check_stack_depth(pc);
        pre_NONLOCAL_RETURN_CODE(); // checks zero index, etc.
      }
    }
  }
  
};


// Returns null iff the byte codes and literals are OK -- dmu 2/93
// Sets the errorIndex to the position of the bad bytecode, or -1.
  
const char* methodMap::check_byteCodes_and_literals( smi&            errorIndex,
                                                     IntBList*&      stack_deltas,
                                                     byteVectorOop   codes,
                                                     objVectorOop    literals ) {
  BytecodeChecker ch(codes, literals);
  ch.interpret_method();
  errorIndex= ch.pc;
  stack_deltas= ch.stack_deltas;
  if (ch.get_error_msg())
    return ch.get_error_msg();
  return NULL;
}


//----------------------------------------------------------------------

class BlockBytecodeScanner : public abstract_interpreter {
  fint bci;
public:
  IntList* blks;
  OopList* blkLiterals;

  BlockBytecodeScanner(methodMap *mm, fint b)
    : abstract_interpreter(mm) {
    blks= EMPTY;
    blkLiterals= EMPTY;
    bool hasLoop = mm->containsLoop();
    // If method contains a backwards branch, must consider
    // all blocks as live everywhere, not just up to the pc.
    // (The NIC will allocate locations and init them in prologue.)
    // Sic does not do branches yet. -- dmu
    bci=  hasLoop ? mi.length_codes : b;
  }

  void interpret_method() {
    for (pc= mi.firstBCI();  pc < bci;  ++pc)
      interpret_bytecode(); 
  }

protected:
  void do_literal_code(oop lit) {
    if (lit->is_block_with_code()) {
      blks= blks->append(pc);
      blkLiterals= blkLiterals->append(lit);
    }
  }
};


IntList* methodMap::blocks_upto(fint bci,  OopList** literals) {

  // if method has a branch, all block locs will be
  //  initialized at start, therefore, must include all blocks
  //  lest frame copying neglect to zero out an inited
  //  but not cloned block -- dmu 10/96
  if (  bci != length_codes()
  &&    containsBranch() )
    bci = length_codes();
    
  BlockBytecodeScanner sc(this, bci);
  sc.interpret_method();
  if (literals != NULL)
    *literals = sc.blkLiterals;
  return sc.blks;
}

IntList* methodMap::all_blocks(OopList** literals) {
  return blocks_upto(length_codes(), literals);
}


//----------------------------------------------------------------------


// return a blist 
// that contains the true for every branch bytecode
// it goes one past the end for branches to returns -- dmu

class BranchTargetFinder: public abstract_interpreter {
 public:
  BoolBList*  branch_targets;
  BoolBList*  backwards_branch_targets;
  bool        separateDirections;
  bool        got_one;
  
  BranchTargetFinder(methodMap *mm, bool sepDirs) : abstract_interpreter(mm) {
    separateDirections = sepDirs;
    branch_targets = new BoolBList(mi.length_codes + 1);
    backwards_branch_targets =   separateDirections
      ?  new BoolBList(mi.length_codes + 1)
      :  NULL;
    for (int32 i = 0;  i < mi.length_codes + 1;  ++i) {
      branch_targets->push(NULL);
      if (separateDirections)
        backwards_branch_targets->push(NULL);
    }
    got_one= false;
  }
  
  void target(int32 t) {
    got_one= true;
   ( separateDirections &&  t <= pc  
        ?  backwards_branch_targets  
        :  branch_targets )
     ->nthPut(t, true, true);
  }
  
  void do_branch_code( int32 target_PC, oop target_oop = badOop ) {
    Unused(target_oop);
    target(target_PC);
  }
  
  void do_BRANCH_INDEXED_CODE() {
    objVectorOop v= get_branch_vector();
    for (int32 i = 0,  n = v->length();  i < n;  ++i)
      target(smiOop(v->obj_at(i))->value());
  }
};



// if bacwkards_branch_targets is omitted (NULL)
// return bcis that are branch bc targets in branch_targets.
// If not null, return targets of fwd branches in branch_targets
// and targets of backwards branches in last argument.

void methodMap::branch_targets( bool& got_one,  
                                BoolBList** branch_targets,
                                BoolBList** backwards_branch_targets) {
  bool discriminateDirection = backwards_branch_targets != NULL;
  BranchTargetFinder btf(this, discriminateDirection);
  btf.interpret_method();
  *branch_targets=  btf.branch_targets;
  if (discriminateDirection)
    *backwards_branch_targets= btf.backwards_branch_targets;
  got_one= btf.got_one;
}



//----------------------------------------------------------------------

class BytecodePrinter : public abstract_interpreter {
public:

  BytecodePrinter(methodMap *mm, fint startBCI = 0)
    : abstract_interpreter(mm) {
    pc= startBCI == 0 ? mi.firstBCI() : startBCI;
    is.index= mm->get_index_at(startBCI) >> INDEXWIDTH;
    do_print = true;
  }

  void interpret_method() {
    for ( pc= mi.firstBCI();  pc < mi.length_codes;  ++pc) {
      lprintf("\t\t%d: ", pc);
      interpret_bytecode();
      lprintf("\n");
      if (error_msg) {
        lprintf( "*** %s in preceeding bytecode\n", error_msg);
      }
      error_msg = NULL;
    }
  }

  void setup_state() {
    // sets index, arg count, etc.
    do_print = false;
    fint startBCI = pc;
    for (pc= mi.firstBCI();  pc < startBCI;  ++pc) {
      interpret_bytecode();
      error_msg = NULL;
    }
    pc = startBCI;
    do_print = true;
  }
    
protected:

  bool do_print;

  bool check(aiCheckFn fn, oop p = NULL) {
    (*fn)(this, p);
    return !error_msg;
  }

  void do_SELF_CODE()               { if (do_print)  lprintf("self"); }
  void do_POP_CODE()                { if (do_print)  lprintf("pop"); }
  void do_NONLOCAL_RETURN_CODE()    { if (do_print)  lprintf("non-local return"); }
  void do_UNDIRECTED_RESEND_CODE()  { if (do_print)  lprintf("undirected-resend"); }
  void do_INDEX_CODE()              { if (do_print)  lprintf("index %d", bc.x); }

  void do_BRANCH_CODE()          {
    if (do_print) {
      lprintf("branch: ");
      get_literal()->print_oop(); 
    }
  }
  void do_BRANCH_TRUE_CODE()     {
    if (do_print) {
      lprintf("branch if true: ");
      get_literal()->print_oop(); 
    }
  }
  void do_BRANCH_FALSE_CODE()    {
    if (do_print) {
      lprintf("branch if false: ");
      get_literal()->print_oop(); 
    }
  }
  void do_BRANCH_INDEXED_CODE()  {
    if (do_print) {
      lprintf("branch indexed: ");
      get_literal()->print_oop();
    }
  }

  void do_LEXICAL_LEVEL_CODE()      { if (do_print)  lprintf("lexical level %d",  is.index); }
  void do_ARGUMENT_COUNT_CODE()     { if (do_print)  lprintf("argument count %d", is.index); }

  void do_send_code(bool isSelfImplicit, stringOop sel, fint arg_count) {
    if (do_print) {
      lprintf("%ssend: ", isSelfImplicit ? "implicit self " : "");
      sel->string_print();
      lprintf(" %d arguments", arg_count);
    }
  }
  void do_read_write_local_code(bool isWrite) {
    if (do_print)
      lprintf("%s local: %d",
             isWrite ? "write" : "read",
             is.index); 
  }
  void do_DELEGATEE_CODE() {
    if (do_print) {
      lprintf("delegatee: ");
      get_literal()->print_oop(); 
    }
  }

  void do_literal_code(oop lit) {
    if (do_print) {
      lprintf("literal: ");
      lit->print_oop(); 
    }
  }
};


void methodMap::print_byteCode_at(fint bci) {
  BytecodePrinter sc(this, bci);
  sc.setup_state();
  sc.interpret_bytecode();
}


void methodMap::print_code(oop obj) {
  Unused(obj);
  lprintf("\n");
  stringOop f = file();
  smiOop ln = line();
  if (f->length() > 0) {
    lprintf("\tFile (%#lx):\t", f);
    f->string_print();
    lprintf(", line: %ld\n", long(ln->value()));
  }
  lprintf("\tSource (%#lx):\t", source());
  print_source();
  lprintf("\n");
  lprintf("\tLiterals (%#lx)\n", literals());
  lprintf("\tByte codes (%#lx):\n", codes());
  
  BytecodePrinter sc(this);
  sc.interpret_method();
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "memOop.hh"
# pragma implementation "memOop_inline.hh"

# include "_memOop.cpp.incl"


Map* memOopClass::map() { return addr()->_map->map_addr(); }

void memOopClass::set_map(Map* m, bool cs) {
  mapOop mp = m->enclosing_mapOop();
  if (cs) Memory->store((oop*)&addr()->_map, mp);
  else    addr()->_map = mp; 
}


void memOopClass::print_string(char* buf) {
  if (this == memOop(badOop)) {
    sprintf(buf, "badOop");
  }
  else
    map()->print_string(this, buf);
}

void memOopClass::print_oop() {
  if (this == memOop(badOop)) {
    lprintf("badOop");
  }
  else
    map()->print_oop(this);
}

int32 memOopClass::compute_derived_offset() {
  int32 i = 1;
  oop* p = ((oop*) addr()) - i;
  while (!is_object_start(*p)) {
    i ++, p --;
  }
  return i;
}

oop memOopClass::scavenge() {
  if (! Memory->should_scavenge(this)) {
    return this;
  } else if (this->is_forwarded()) {
    return oop(this->forwardee());
  } else {
    return map()->scavenge(this);
  }
}

bool memOopClass::verify_oop(bool expectErrorObj) {
  bool flag = Memory->verify_oop(this, expectErrorObj);
  if (flag && ! mark()->is_mark()) {
    error1("memOop 0x%lx doesn't point to a mark", this);
    flag = false;
  }
  return flag;
}

bool memOopClass::verify() {
  bool flag = verify_oop(this == Memory->errorObj);
  if (flag) {
    markOop m = mark();
    if (! oop(m)->is_mark()) {
      error1("mark of memOop 0x%lx isn't a markOop", this);
      if (! m->verify_oop())
        error1(" mark of memOop 0x%lx isn't even a legal oop", this);
      flag = false;
    }
    else if (is_objectMarked()) {
      error1("memOop 0x%lx is marked!", this);
      flag = false;
    }
    mapOop p = map()->enclosing_mapOop();
    if (! oop(p)->verify_oop()) {
      error1("map of memOop 0x%lx isn't a legal oop", this);
      flag = false;
    } else if (! p->is_map()) {
      error1("map of memOop 0x%lx isn't a mapOop", this);
      flag = false;
    } else if (map()->should_canonicalize()) {
      // put this test here so we only check maps actually used in objs--dmu
      Memory->map_table->verify_map((slotsMapDeps*)(map()));
    }
  }
  return flag;
}

void memOopClass::set_canonical_map(Map* new_map) {
  // check if there is an existing equivalent map.
  // if so, set the map to that instead of the current map.

  // Note that we don't check if the map has dependents.
  // If it does, we rely on garbage collection for cleaning up.

  set_map(new_map->should_canonicalize()
          ? (Map*) (Memory->map_table->canonical_map((slotsMapDeps*)new_map))
          : new_map);
}

void memOopClass::canonicalize_map() {
  // Called when reading a snapshot to get all maps in objects
  // in the map table & to canonicalize the objects' map fields.
  // We use an optimization: if the map is already canonical,
  //  don't have to do anything.  -- dmu 12/9/93

  Map* m = map();
  if (m->should_canonicalize() && ((slotsMapDeps*)m)->map_chain()->isEmpty())
    set_canonical_map(m);
}

void memOopClass::set_identity_hash(smi h) {
  set_mark(mark()->set_hash(h));
}
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stringOop.hh"
# include "_stringOop.cpp.incl"


stringOop new_string(const char* bytes, int32 len, bool mustAllocate) {
  assert(bytes != NULL, "new_string: null bytes");
  return Memory->string_table->lookup(bytes, len, mustAllocate); }

bool stringOopClass::is_unary(fint upToLen) {
  char *s= bytes();
  char c= *s++;
  if (!is_id_alpha(c)) return false;
  for (upToLen--; upToLen > 0; upToLen--)
    if (*s++ == ':')
      return false;
  return true;
}

stringOop stringOopClass::create_string(fint size) {
  char* nb;
  oop *p= Memory->old_gen->alloc_objs_and_bytes(size, 0, nb);
  stringOop obj= as_stringOop(p);
  obj->init_mark();
  obj->set_length(0);
  obj->set_bytes(nb);
  return obj;
}

stringOop stringOopClass::make_string(const char* value, fint len,
                                      bool mustAllocate) {
  fint s= size();
  fint l= ::lengthWords(len);
  char* nb;
  oop* x= Memory->old_gen->alloc_objs_and_bytes(s, l, nb, mustAllocate);
  if (x == NULL)
    return stringOop(failedAllocationOop);
  copy_oops(oops(), x, s);
  copy_bytes(value, nb, len);   // value maybe not word-aligned
  stringOop r = as_stringOop(x);
  assert(r->is_old(), "should be allocated in old space");
  r->init_mark();
  r->set_length(len);
  r->set_bytes(nb);
  r->fix_generation(s);
  return r;
}

stringOop stringOopClass::scavenge() {
  ShouldNotCallThis(); // shouldn't be scavenging canonical strings
                       // (should be tenured);
  return NULL;
}

bool stringOopClass::verify() {
  bool flag = byteVectorOopClass::verify();
  if (flag) {
    if (!is_old()) {
      error1("stringOop 0x%lx isn't tenured", this);
      flag = false;
    }
    stringOop s = Memory->string_table->lookup(bytes(), length());
    if (s != this) {
      error1("stringOop 0x%lx isn't canonical", this);
      flag = false;
    }
    if (!flag) {
      lprintf("It contains: ");
      string_print();
      lprintf("\n");
    }
  }
  return flag;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */



# pragma implementation "processOop.hh"
# include "_processOop.cpp.incl"

oop processOopClass::NewProcess_prim(smi stackSize, oop rcvr, stringOop sel, 
                                     objVectorOop args, void *FH) {
  ResourceMark rm;
  char *selector = sel->copy_null_terminated();
  if (stackSize <= 0 || str_arg_count(selector) > args->length()) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return NULL;
  }
  Process* p;
  p = new Process(this, stackSize, rcvr, sel, args);
  return p->processObj();
}

inline Process* checkProcess(oop p) {
  assert(p->is_process(), "not a process");
  Process* proc = processOop(p)->process();
  if (!proc || proc->processObj() != p) {
    return NULL;                // dead process or a clone of a process obj
  } else {
    return proc;
  }
}


void processOopClass::kill_process() {
  set_process(NULL);
  vframeOop sentinel = vframeList();
  sentinel->set_next(NULL);
}


bool processOopClass::is_live_process() {
  return checkProcess(this) != NULL;
}


oop processOopClass::AbortProcess_prim(void *FH) {
  Process* proc = checkProcess(this);
  if (!proc) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  if (proc == twainsProcess)  {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return NULL;
  }
  preserved pres(this);
  proc->abort();
  if (proc->state == defunct) fatal("process already deleted");
  delete proc;
  return pres.value;
}


oop Yield_prim(oop rcvr, oop arg) {
  if (twainsProcess) {
    if (currentProcess->isSingleStepping()) {
      // yielding counts as a step
      assert(preemptCause == cSingleStepped, "wrong preemptCause");
    } 
    else {
      preemptCause = cYielded;
      yieldRcvr = rcvr;
      yieldArg  = arg;
      { preserved p(rcvr);
        // Self processes that willingly yielded the CPU are suspended here;
        // the other suspension point is in handlePreemption (process.c)
        if (currentProcess->profiler) { 
          currentProcess->profiler->suspend();
        }
        twainsProcess->transfer();
        rcvr = p.value;
      }
      if (currentProcess->isKillingOrDeoptimizing()) {
        // continue non-local return (for KillActivations primitive)
        NLRSupport::continue_NLR_into_Self(false);
      }
    }
  } 
  else {
    // this is a no-op
  }
  assert(!processSemaphore, "processSemaphore should not be set");
  return rcvr;
}

oop ThisProcess_prim(oop rcvr) {
  Unused(rcvr);
  return currentProcess->processObj();
}

oop processOopClass::PrintProcessStack_prim(void *FH) {
  Process* p = checkProcess(this);
  if (!p) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  switch (p->state) {
   case ready:
   case stopped:
    p->stack()->print();
    break;
   case initialized:
    lprintf("\nProcess has not started yet.\n");
    break;
   case aborting:
    lprintf("\nProcess is aborting.\n");
    break;
   case defunct:
   default:
    fatal("illegal process status");
  };
  return this;
}

smi TWAINSResultSize_prim() {
  return PendingSelfSignals::Self_result_size();
}

// twains helpers:

Process* processOopClass::TWAINS_receiver_check(void* FH) {
  Process* proc = checkProcess(this);
  if (!proc  || !proc->isRunnable()) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  return proc;
}

bool processOopClass::TWAINS_result_vector_check(objVectorOop resultArg,
                                                 void* FH) {
  if (resultArg->length_obj_array() >= PendingSelfSignals::Self_result_size()) {
    return true;
  } else {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return false;
  }
}

vframeOop processOopClass::TWAINS_stop_activation_check(Process* proc,
                                                        oop stop, void* FH) {
  vframeOop stop_vfo = NULL;
  if (stop == Memory->nilObj) {
    // no stopping (common case)
  } else if (stop->is_mirror() && mirrorOop(stop)->reflectee()->is_vframe()) {
    // make sure it's live and on proc's stack
    stop_vfo = vframeOop(mirrorOop(stop)->reflectee());
    if (!stop_vfo->is_live()) {
      prim_failure(FH, NOACTIVATIONERROR);
      return vframeOop(badOop);
    }
    if (!proc->contains((char*)stop_vfo->locals())) {
      prim_failure(FH, NOACTIVATIONERROR);
      return vframeOop(badOop);
    }
  } else {
    prim_failure(FH, BADTYPEERROR);
    return vframeOop(badOop);
  }
  return stop_vfo;
}

bool processOopClass::TWAINS_parallel_check(void* FH) {
  if (twainsProcess) {
    prim_failure(FH, PARALLELTWAINSERROR);
    return false;
  } else {
    return true;
  }
}

void processOopClass::TWAINS_transfer_to_another_process(
                        Process* proc,
                        objVectorOop& resultArg, 
                        bool stepping,
                        vframeOop stop_vfo) {
                        
  if (proc->hasStack() || proc->allocate()) {
    // transfer to the other process
    preserved pres1(resultArg);
    if (stepping) {
      proc->setSingleStepping();
      preemptCause = cSingleStepped;    // so it isn't overridden by signals
    }
    if (stop_vfo) proc->setStopPoint(stop_vfo);
    LOG_EVENT3("TWAINS: transfer to %#lx %s stop=%#lx", proc, 
           stepping ? "stepping" : "", stop_vfo); 
    proc->transfer();           // run the other process
    resultArg = objVectorOop(pres1.value);
    if (stepping) {
      assert(preemptCause == cSingleStepped ||
         isFatalCause(preemptCause), "wrong preemptCause");
      proc->resetSingleStepping();
    }
    if (proc->stopping) {
      // returned from stopActivation (but could be way past it, e.g.
      // when process was aborted)
      proc->resetStopping();
      if (!isFatalCause(preemptCause)) preemptCause = cFinishedActivation;
    }
    proc->setStopPoint(NULL);
  } else {
    // couldn't allocate stack
    preemptCause= cCouldntAllocateStack;
    proc->state= aborting;
  }
}


void processOopClass::TWAINS_await_signal() {
  // nothing to do; just wait for next signal from Unix
  processes->idle = true;
  do {
    // wait for any interrupt
    SignalInterface::wait_for_any();
    // The while loop is necessary for debugging: sigpause
    // ends after a ^C even if gdb doesn't pass the signal.
  } while ( preemptCause == cNoCause );
  processes->idle = false;
}


oop processOopClass::TWAINS_prim(objVectorOop resultArg, 
                                 bool stepping, oop stop, void *FH) {
  if (SignalInterface::are_self_signals_blocked())
    warning("_TWAINS: signals are blocked (with _BlockSignals)");
  
  Process* proc= TWAINS_receiver_check(FH);
  if (proc == NULL) return NULL;
  if (!TWAINS_result_vector_check( resultArg, FH)) return NULL;
  vframeOop stop_vfo= TWAINS_stop_activation_check(proc, stop, FH);
  if (stop_vfo == vframeOop(badOop)) return NULL;
  if (!TWAINS_parallel_check(FH)) return NULL;

  preemptCause = cNoCause;
  twainsProcess = currentProcess;
  if (PendingSelfSignals::are_any_pending() && !SignalInterface::are_self_signals_blocked()) {
    // return immediately - have unhandled signals
    preemptCause = cSignal;
  }
  else if (proc != twainsProcess) {
    TWAINS_transfer_to_another_process(proc, resultArg, stepping, stop_vfo);
  }
  else {
    TWAINS_await_signal();
  }
  twainsProcess = NULL;
  
  oop res = get_result(resultArg);
  LOG_EVENT3("TWAINS: res = %#lx { %#lx, %#lx, ... }", res,
             resultArg->obj_at(0), resultArg->obj_at(1));
  preemptCause = cNoCause;
  return res;
}


oop processOopClass::ProcessReturnValue_prim(void *FH) {
  if (process()) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  return get_return_oop();
}

oop processOopClass::get_result(oop resultArg) {
  // compute result and fill in result vector if necessary
  oop res;
  switch (preemptCause) {
   case cTerminated:
   case cAborted:
    res = VMString[causeString[preemptCause]];
    delete prevProcess;
    break;
   case cStackOverflowed:
   case cNonLIFOBlock:
    res = VMString[causeString[preemptCause]];
    assert(prevProcess->state == stopped, "wrong state");
    break;
   case cSingleStepped:
   case cFinishedActivation:
   case cCouldntAllocateStack:
   case cLowOnSpace:
    res = VMString[causeString[preemptCause]];
    break;
   case cYielded:
    res = VMString[causeString[preemptCause]];
    resultArg->obj_at_put(0, yieldRcvr);
    resultArg->obj_at_put(1, yieldArg);
    break;
   case cSignal:
    res = VMString[SIGNAL];
    PendingSelfSignals::pass_to_Self(resultArg);
    break;
   case cNoCause:
   case cLast:
   default:
    fatal1("unknown/illegal preemptCause %ld", preemptCause);
  }
  return res;
}

// debugging primitives
smi processOopClass::StackDepth_prim(void *FH) {
  Process* p = checkProcess(this);
  if (!p) {
    prim_failure(FH, NOPROCESSERROR);
    return smi(NULL);
  }
  int32 vdepth = p->stack()->vdepth();
  if (p != vmProcess && vdepth > 0) vdepth--;   // hide dummy doIt vframe
  return vdepth;
}

typedef BoundedListTemplate<abstract_vframe*> VFrameBList;

static VFrameBList*  vframeBList;
static void addVFrameToList(abstract_vframe* vf) { vframeBList->append(vf); }

vframeOop processOopClass::clone_vframeOop(abstract_vframe* vf, Process* p,
                                           bool mustAllocate) {
  vframeOop vfo;
  MethodKind k = vf->method()->kind();
  switch (k) {
   case OuterMethodType:
    vfo= Memory->outerActivationObj->basic_clone(mustAllocate); break;
   case BlockMethodType:
    vfo= Memory->blockActivationObj->basic_clone(mustAllocate); break;
   default:
    fatal("unexpected kind");
  }
  if (oop(vfo) != failedAllocationOop) {
    vfo->set_process(p);
    vfo->set_locals(vf->fr->vfo_locals_of_home_frame());
    vfo->set_method(vf->method());
    vfo->set_descOffset(vf->descOffset());
  }
  return vfo;
}

oop processOopClass::ActivationStack_prim(void *FH) {
  ResourceMark rm;
  Process* p = checkProcess(this);
  if (p == currentProcess)
    p->killVFrameOopsAndSetWatermark( p->last_self_frame(false));
  if (!p) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  FlushRegisterWindows();

  vframeBList = new VFrameBList(1000);

  p->stack()->vframes_do(addVFrameToList);

  bool hideFirst = p != vmProcess;
  if (vframeBList->nonEmpty() && hideFirst) (void) vframeBList->pop();

  smi len= vframeBList->length();
  objVectorOop resultVec= Memory->objVectorObj->cloneSize(len, CANFAIL);
  if (oop(resultVec) == failedAllocationOop) {
    out_of_memory_failure(FH, Memory->objVectorObj->size() + len);
    return NULL;
  }

  oop*         resultp   = resultVec->objs();

  // Build the result as a merge of vframeBList and existing vframeOops
  // and save mirrors of all the resulting vframeOops.
  vframeOop prev  = vframeList();
  vframeOop merge = prev->next();
  for (int i = 0; i < vframeBList->length(); i++) {
    abstract_vframe* vf = vframeBList->nth(i);
    mirrorOop mirror;
    if (merge && merge->is_equal(vf)) {
      mirror= merge->as_mirror_or_fail();
      if (oop(mirror) == failedAllocationOop) {
        out_of_memory_failure(FH);
        return NULL;
      }
      prev  = merge;
      merge = merge->next();
    } else {
      vframeOop vfo= clone_vframeOop(vf, p, CANFAIL);
      if (oop(vfo) == failedAllocationOop) {
        out_of_memory_failure(FH);
        return NULL;
      }
      mirror= vfo->as_mirror_or_fail();
      if (oop(mirror) == failedAllocationOop) {
        out_of_memory_failure(FH);
        return NULL;
      }
      vfo->insertAfter(prev);
      prev = vfo;
    }
    Memory->store(resultp++, mirror);
  }
  if (p == currentProcess)
    p->killVFrameOopsAndSetWatermark( p->last_self_frame(false));
  return resultVec;
}


abstract_vframe* processOopClass::vframe_at(smi index, void *FH, Process*& p,
                                   frame*& last) {
  p = checkProcess(this);
  if (!p) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  if (index < 0 || !p->inSelf()) {
    prim_failure(FH, BADINDEXERROR);
    return NULL;
  }
  FlushRegisterWindows();
  last = p->last_self_frame(false);
  abstract_vframe* vf = new_vframe(last);
  if (vf->is_prologue()) vf = vf->sender();
  bool hideFirst = p != vmProcess;
  int32 i = 0;
  for ( ;; ) {
    if (i == index) break;
    vf = vf->sender();
    i++;
    if (!vf) break;
  }
  if (!vf || (vf->is_first_self_vframe() && hideFirst)) {
    prim_failure(FH, BADINDEXERROR);
    return NULL;
  }
  return vf;
}

oop processOopClass::ActivationAt_prim(smi index, void *FH) {
  ResourceMark rm;
  Process* p;
  frame* last;
 abstract_vframe* vf = vframe_at(index, FH, p, last);
  if (!vf) return NULL; 
  oop vfo = new_vframeOop(p, vf)->as_mirror();
  if (p == currentProcess) p->killVFrameOopsAndSetWatermark(last);
  return vfo;
}

oop processOopClass::KillUpTo_prim(smi index, void *FH) {
  ResourceMark rm;
  preserved pres1(this);
  Process* p;
  frame* last;
  abstract_vframe* vf = vframe_at(index, FH, p, last);
  if (!vf) return NULL; 
  if (!p->isRunnable()) {
    failure(FH, "process is not runnable");
    return 0;
  }
  if (p == currentProcess) {
    failure(FH, "cannot operate on current process");
    return 0;
  }
  if (p == twainsProcess) {
    failure(FH, "cannot operate on scheduler process");
    return 0;
  }
  p->killFrames(vf);
  assert(!p->isKilling(), "shouldn't be in killing mode anymore");
  return pres1.value;
}

oop processOopClass::GotoByteCode_prim(smi bci, objVectorOop exprStack,
                                       void *FH) {
  ResourceMark rm;
  preserved pres1(this);
  Process* p;
  frame* last;
  abstract_vframe* vf = vframe_at(0, FH, p, last);
  if (!vf) return NULL; 
  if (!p->isRunnable()) {
    failure(FH, "processes is not runnable");
    return 0;
  }
  if (p == currentProcess) {
    failure(FH, "cannot operate on current process");
    return 0;
  }
  if (p == twainsProcess) {
    failure(FH, "cannot operate on scheduler process");
    return 0;
  }
  p->gotoByteCode(vf, bci, exprStack, FH);
  return pres1.value;
}


    
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "fctProxyOop.hh"
#include "_fctProxyOop.cpp.incl"



smi fctProxyOopClass::get_noOfArgs_prim(void *FH) {
  if (!is_live()) {
    prim_failure(FH, DEADPROXYERROR);
    return 0;
  }
  return get_noOfArgs();
}


oop fctProxyOopClass::set_noOfArgs_prim(smi n, void *FH) {
  if (!is_live()) {
    prim_failure(FH, DEADPROXYERROR);
    return NULL;
  }
  set_noOfArgs(n);
  return this;
}


# if  GENERATE_DEBUGGING_AIDS
bool fctProxyOopClass::verify() {
  bool flag = proxyOopClass::verify();
  if (!addr()->noOfArgs->is_smi()) {
    error1("fctProxyOop %#lx: noOfArgs isn't a smi", this);
    flag = false;
  }
  return flag;
}
# endif


#define CALL_TEMPLATE(name, comma, declList, argList, nargs)                  \
  inline oop fctProxyOopClass::name(declList) {                               \
    if (!is_fctProxy())                                                       \
      return ErrorCodes::vmString_prim_error(BADTYPEERROR);                                        \
    if (!is_live())                                                           \
      return ErrorCodes::vmString_prim_error(DEADPROXYERROR);                                      \
    if (get_noOfArgs() != nargs && get_noOfArgs() != unknownNoOfArgs)         \
      return ErrorCodes::vmString_prim_error(WRONGNOOFARGSERROR);                                  \
    oop res = (*get_pointer()) (argList);                                     \
    assert(res->verify_oop_mark_ok(),"should be an oop");                     \
    return res;                                                               \
  }                                                                           \
                                                                              \
  oop name(oop rcvr comma declList) {                                         \
    return fctProxyOop(rcvr)->name(argList); }                                \


// what a hack
#define C ,

CALL_TEMPLATE(call0_prim, , , , 0)

CALL_TEMPLATE(call1_prim, C, 
              oop a1, 
              a1, 
              1)

CALL_TEMPLATE(call2_prim, C, 
              oop a1 C oop a2, 
              a1 C a2,
              2)

CALL_TEMPLATE(call3_prim, C, 
              oop a1 C oop a2 C oop a3,
              a1 C a2 C a3,
              3)

CALL_TEMPLATE(call4_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4,
              a1 C a2 C a3 C a4,
              4)

CALL_TEMPLATE(call5_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5, 
              a1 C a2 C a3 C a4 C a5,
              5)

CALL_TEMPLATE(call6_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5 C oop a6, 
              a1 C a2 C a3 C a4 C a5 C a6,
              6)

CALL_TEMPLATE(call7_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5 C oop a6 C oop a7, 
              a1 C a2 C a3 C a4 C a5 C a6 C a7,
              7)

CALL_TEMPLATE(call8_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5 C oop a6 C oop a7 C oop a8, 
              a1 C a2 C a3 C a4 C a5 C a6 C a7 C a8,
              8)

CALL_TEMPLATE(call9_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5 C oop a6 C oop a7 C oop a8 C oop a9, 
              a1 C a2 C a3 C a4 C a5 C a6 C a7 C a8 C a9,
              9)

CALL_TEMPLATE(call10_prim, C, 
              oop a1 C oop a2 C oop a3 C oop a4 C oop a5 C oop a6 C oop a7 C oop a8 C oop a9 C oop a10, 
              a1 C a2 C a3 C a4 C a5 C a6 C a7 C a8 C a9 C a10,
              10)


# define C_C_TEMPLATE(declList, argList)                                      \
  oop fctProxyOopClass::declList {                                            \
    if (!is_live())                                                           \
      return ErrorCodes::vmString_prim_error(DEADPROXYERROR);                                      \
    byteVectorOop res= Memory->byteVectorObj->cloneSize(sizeof(void*));       \
    *(void**) res->bytes() =  (*get_pointer()) argList;                       \
    return res;                                                               \
  }

C_C_TEMPLATE(call_and_convert0(), ())
C_C_TEMPLATE(call_and_convert1(void* a1), (a1))
C_C_TEMPLATE(call_and_convert2(void* a1, void* a2), (a1, a2))
C_C_TEMPLATE(call_and_convert3(void* a1, void* a2, void* a3), 
             (a1, a2, a3))
C_C_TEMPLATE(call_and_convert4(void* a1, void* a2, void* a3, void* a4),
             (a1, a2, a3, a4))
C_C_TEMPLATE(call_and_convert5(void* a1, void* a2, void* a3, void* a4, 
                               void* a5),
             (a1, a2, a3, a4, a5))
C_C_TEMPLATE(call_and_convert6(void* a1, void* a2, void* a3, void* a4, 
                               void* a5, void* a6), 
             (a1, a2, a3, a4, a5, a6))
C_C_TEMPLATE(call_and_convert7(void* a1, void* a2, void* a3, void* a4, 
                               void* a5, void* a6, void* a7), 
             (a1, a2, a3, a4, a5, a6, a7))
C_C_TEMPLATE(call_and_convert8(void* a1, void* a2, void* a3, void* a4, 
                               void* a5, void* a6, void* a7, void* a8), 
             (a1, a2, a3, a4, a5, a6, a7, a8))
C_C_TEMPLATE(call_and_convert9(void* a1, void* a2, void* a3, void* a4, 
                               void* a5, void* a6, void* a7, void* a8,
                               void* a9), 
             (a1, a2, a3, a4, a5, a6, a7, a8, a9))
C_C_TEMPLATE(call_and_convert10(void* a1, void* a2, void* a3, void* a4, 
                                void* a5, void* a6, void* a7, void* a8,
                                void* a9, void* a10), 
             (a1, a2, a3, a4, a5, a6, a7, a8, a9, a10))
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "byteVectorOop.hh"
# include "_byteVectorOop.cpp.incl"


byteVectorOop byteVectorOopClass::copy(fint s, bool mustAlloc,
                                       oop genObj, bool cs) {
  fint l= lengthWords();
  char* nb;
  oop* x= genObj
    ? genObj->my_generation()->alloc_objs_and_bytes(s, l, nb, mustAlloc)
      : Memory->alloc_objs_and_bytes(s, l, nb, mustAlloc);
  if (x == NULL)
    return byteVectorOop(failedAllocationOop);
  copy_oops(oops(), x, s);
  copy_words((int32*) bytes(), (int32*) nb, l);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_bytes(nb);
  if (cs) r->fix_generation(s);
  return r;
}

byteVectorOop byteVectorOopClass::grow_bytes(fint delta, bool mustAllocate) {
  fint s = size();
  fint nlen = length() + delta;
  fint l= lengthWords();
  fint nl= ::lengthWords(nlen);
  char* nb;
  oop* x= Memory->alloc_objs_and_bytes(s, nl, nb, mustAllocate);
  if (x == NULL)
    return byteVectorOop(failedAllocationOop);
  copy_oops(oops(), x, s);
  copy_words((int32*) bytes(), (int32*) nb, l);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_length(nlen);
  r->set_bytes(nb);
  r->fix_generation(s);
  return r;
}

byteVectorOop byteVectorOopClass::shrink_bytes(fint delta, bool mustAllocate) {
  fint s = size();
  fint nlen = length() - delta;
  assert(nlen >= 0, "shrinking below size zero");
  fint nl= ::lengthWords(nlen);
  char* nb;
  oop* x= Memory->alloc_objs_and_bytes(s, nl, nb, mustAllocate);
  if (x == NULL)
    return byteVectorOop(failedAllocationOop);
  copy_oops(oops(), x, s);
  copy_words((int32*) bytes(), (int32*) nb, nl);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_length(nlen);
  r->set_bytes(nb);
  r->fix_generation(s);
  return r;
}

byteVectorOop byteVectorOopClass::insert(fint s, fint change_point,
                                         fint delta, bool mustAllocate,
                                         bool sameGen) {
  assert(s == size(), "should be the right size");
  fint ns = s + delta;
  fint l= lengthWords();
  char* nb;
  oop* x= sameGen
    ? my_generation()->alloc_objs_and_bytes(ns, l, nb, mustAllocate)
      : Memory->alloc_objs_and_bytes(ns, l, nb, mustAllocate);
  if (x == NULL) 
    return byteVectorOop(failedAllocationOop);
  oop* p = oops();
  copy_oops(p, x, change_point);
  copy_oops(p + change_point, x + change_point + delta, s - change_point);
  copy_words((int32*) bytes(), (int32*) nb, l);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_bytes(nb);
  // can't do fix_generation(), since inserted space isn't initialized yet
  return r;
}

byteVectorOop byteVectorOopClass::remove(fint s, fint change_point,
                                         fint delta, bool mustAllocate,
                                         bool sameGen) {
  assert(s == size(), "should be the right size");
  fint ns = s - delta;
  assert(ns >= sizeof(byteVectorOopClass)/oopSize,
         "shrinking below empty byte vector size");
  fint l= lengthWords();
  char* nb;
  oop* x= sameGen
    ? my_generation()->alloc_objs_and_bytes(ns, l, nb, mustAllocate)
      : Memory->alloc_objs_and_bytes(ns, l, nb, mustAllocate);
  if (x == NULL)
    return byteVectorOop(failedAllocationOop);
  oop* p = oops();
  copy_oops(p, x, change_point);
  copy_oops(p + change_point + delta, x + change_point,
            s - change_point - delta);
  copy_words((int32*) bytes(), (int32*) nb, l);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_bytes(nb);
  r->fix_generation(ns);
  return r;
}

byteVectorOop byteVectorOopClass::create_byteVector(oop parent) {
  slotList* slist = new slotList(VMString[PARENT],
                                 parent_map_slotType,
                                 parent); 
  return ::create_byteVector(slist);
}

byteVectorOop byteVectorOopClass::create_byteVector(fint size) {
  char* nb;
  oop *p= Memory->alloc_objs_and_bytes(size, 0, nb);
  byteVectorOop obj= as_byteVectorOop(p);
  obj->init_mark();
  obj->set_length(0);
  obj->set_bytes(nb);
  return obj;
}

byteVectorOop byteVectorOopClass::scavenge(fint size) {
  assert(Memory->should_scavenge(this) && !is_forwarded(), 
         "shouldn't be scavenging"); 
  bool is_new;
  fint l= lengthWords();
  space *copySpace= Memory->survivor_space(this, size, l, is_new);
  char *nb;
  oop *x= copySpace->alloc_objs_and_bytes_local(size, l, nb);
  if (x == NULL) fatal("out of space in scavenge");
  copy_oops(oops(), x, size);
  copy_words((int32*) bytes(), (int32*) nb, l);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_bytes(nb);
  if (is_new) { 
    r->set_mark(r->mark()->incr_age()); 
    Memory->age_table->add(r); 
  } 
  forward_to(r); 
  return r;
}

void byteVectorOopClass::relocate_bytes(space* s) {
  set_bytes(s->relocate_bytes(bytes()));
}

bool byteVectorOopClass::verify() {
  bool flag = slotsOopClass::verify();
  if (flag) {
    int32 l = length();
    if (l < 0) {
      error1("byteVectorOop 0x%lx has negative length", this);
      flag = false;
    }
    if (bytes() == NULL) {
      error1("byteVectorOop 0x%lx has a zero bytes part", this);
      flag = false;
    }
  }
  return flag;
}

bool byteVectorOopClass::verifyBytesPart(char*& b) {
  bool flag = true;
  char* bs = bytes();
  if (bs + roundTo(length(), oopSize) == b) {
    // adjust last bytes part
    b = bs;
  } else {
    error1("byteVectorOop 0x%lx has out-of-order bytes part", this);
    flag = false;
  }
  return flag;
}


char* byteVectorOopClass::copy_null_terminated(int &Clength) {
  // Copy the bytes() part. Always add trailing '\0'. If byte vector
  // contains '\0', these will be escaped in the copy, i.e. "....\0...".
  // Clength is set to length of the copy (may be longer due to escaping).
  // Presence of null chars can be detected by comparing Clength to length().

  assert_byteVector(this, "should be a byte vector");
  Clength = length();
  char *res = copy_string(bytes(), Clength);
  if (strlen(res) == Clength) 
    return res;                   // Simple case, no '\0' in byte vector.
  
  // Simple case failed ...
  smi t = length();               // Copy and 'escape' null chars.
  smi i;
  for (i = length()-1; i >= 0; i--) 
    if (byte_at(i) == '\0') t++; 
  // t is total length of result string.
  res = NEW_RESOURCE_ARRAY( char, t + 1);
  res[t--] = '\0';
  Clength  = t;
  for (i = length()-1; i >= 0; i--) {
    if (byte_at(i) != '\0') {
      res[t--] = byte_at(i);
    } else {
      res[t--] = '0';
      res[t--]   = '\\';
    }
  }
  assert(t == -1, "sanity check");
  return res;
}

char* byteVectorOopClass::copy_c_heap_null_terminated() {
  // Copy the bytes() part. Always add trailing '\0'. If byte vector
  // contains '\0', these will be escaped in the copy, i.e. "....\0...".
  // NOTE: The resulting string is allocated in Cheap
  
  assert_byteVector(this, "should be a byte vector");
  smi t = length();               // Copy and 'escape' null chars.
  smi i;
  for (i = length()-1; i >= 0; i--) 
    if (byte_at(i) == '\0') t++; 
  // t is total length of result string.
  char* res = NEW_C_HEAP_ARRAY( char, t + 1);
  res[t--] = '\0';
  for (i = length()-1; i >= 0; i--) {
    if (byte_at(i) != '\0') {
      res[t--] = byte_at(i);
    } else {
      res[t--] = '0';
      res[t--]   = '\\';
    }
  }
  assert(t == -1, "sanity check");
  return res;
}

// byte vector primitives

oop byteVectorOopClass::bv_size_prim(oop rcvr) {
  if (!rcvr->is_byteVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_smiOop(byteVectorOop(rcvr)->length());
}

oop byteVectorOopClass::bv_compare_prim(oop rcvr,  oop arg) {
  if (!rcvr->is_byteVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!arg ->is_byteVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_smiOop(
                   compare_bytes(
                                 byteVectorOop(rcvr)->bytes(),
                                 byteVectorOop(rcvr)->length(),
                                 byteVectorOop(arg )->bytes(),
                                 byteVectorOop(arg )->length()));
}

oop byteVectorOopClass::bv_concatenate_prim(byteVectorOop arg,
                                            byteVectorOop proto, void *FH) {
  if (proto->is_string())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi len1= this->length();
  smi len2= arg->length();
  // the code below is similar to copy_size
  smi len= len1 + len2;
  fint l= ::lengthWords(len);
  // NB: copy proto, not receiver -- otherwise, would create a
  // string if rcvr is a string (but then the result should be old and
  // canonicalized, and that's undesirable)
  fint s= proto->size();

  char* nb;
  oop* x= Memory->alloc_objs_and_bytes(s, l, nb, false);
  if (x == NULL) {
    out_of_memory_failure(FH, s, l);
    return NULL;
  }
  copy_oops(proto->oops(), x, s);
  copy_bytes(this->bytes(), nb,        len1);
  copy_bytes(arg ->bytes(), nb + len1, len2);
  byteVectorOop r = as_byteVectorOop(x);
  r->set_length(len);
  r->set_bytes(nb);
  r->fix_generation(s);
  return r;
}

oop byteVectorOopClass::bv_at_prim(oop rcvr, oop indexOop) { 
  if (!rcvr->is_byteVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!indexOop->is_smi())    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi index = smiOop(indexOop)->value();
  if (unsigned(index) >= unsigned(byteVectorOop(rcvr)->length()))
    return ErrorCodes::vmString_prim_error(BADINDEXERROR);
  return as_smiOop(u_char(byteVectorOop(rcvr)->byte_at(index)));
}


oop byteVectorOopClass::bv_at_put_prim(oop rcvr, oop indexOop, oop contents) {
  if (!rcvr->is_byteVector() || rcvr->is_string())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!indexOop->is_smi()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi index = smiOop(indexOop)->value();
  if (unsigned(index) >= unsigned(byteVectorOop(rcvr)->length()))
    return ErrorCodes::vmString_prim_error(BADINDEXERROR);
  if (!contents->is_smi()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi cval = smiOop(contents)->value();
  if (unsigned(cval) > 255) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  byteVectorOop(rcvr)->byte_at_put(index, char(cval));
  return rcvr;
}


oop byteVectorOopClass::bv_clone_prim(smi size, u_char filler, void *FH) {
  assert(size >= 0, "bad size");
  if (is_string())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  oop c= cloneSize(size, CANFAIL, as_smiOop(filler));
  if (c == failedAllocationOop) {
    out_of_memory_failure(FH, this->size(), size);
    return NULL;
  }    
  // test for scavenging
  if (ScavengeInPrimitives && Memory->needs_scavenge()) {
    return Memory->scavenge(c);
  } else {
    return c;
  }
}

// string-like primitives

oop byteVectorOopClass::string_canonicalize_prim() {
  return new_string(bytes(), length());
}

oop byteVectorOopClass::string_print_prim() {
  string_print();
  return this;
}

oop byteVectorOopClass::run_script_prim() {
  ResourceMark rm;
  oop res;
  { preserved pres(this);

    FileScanner scanner(copy_null_terminated());
  
    if (scanner.fileError)
      return ErrorCodes::os_prim_error(scanner.fileError);

    if (PrintPeriod) {
      lprintf(".");
    }
    if (PrintScriptName) {
      lprintf("reading %s...\n", scanner.fullName());
    }
      
    res = evalExpressions(&scanner);
    currentProcess->cleanup_after_eval_prim(res);
    return res;
  }
}

oop byteVectorOopClass::parseObject_prim(char* fn, oop errorObj, void *FH) {
  ResourceMark rm;
  oop res = NULL;
  fint line, col, len;

  // Check to see if allocation would fail
  // This is a crude check based on the size of the source string,
  // and is only designed to catch evaluation of giant strings.
  // A thorough-going fix would be to have all the inner ops fail if an
  // allocation failed, and propagate this out to the Self level.
  if (Memory->old_gen->bytes_free() - Memory->old_gen->get_VM_reserved_mem()
      < 4 * length()) {
    out_of_memory_failure(FH, 2 * size(), 2 * length());
    return NULL;
  }    

  // need a copy because Eval can cause GC before source is captured.
  const char* source= copy_null_terminated(len);
  
  StringScanner scanner(source, len, fn, 1, 1);
  Parser parser(&scanner, true);
  Object* o = parser.readBody(line, col, source, len);
  if (o == NULL) {
    res = badOop;
  } 
  else {
    preservedObj x(o);
    if (ScavengeInPrimitives && Memory->needs_scavenge()) {
      Memory->scavenge();
    }
    res = o->Eval(false, true, false);
  }

  if (res == failedAllocationOop) {
    out_of_memory_failure(FH, 4*length()); // wild guess
    return NULL;
  }
  if (res == badOop && !NLRSupport::have_NLR_through_C()) {
    parser.fillErrorObj(errorObj);
    prim_failure(FH, PRIMITIVEFAILEDERROR); // receiver had syntax errors
    return NULL;
  }

  currentProcess->cleanup_after_eval_prim(res);
  if (!res->is_mark()) {
    res = res->as_mirror();
  }
  return res;
}


oop byteVectorOopClass::parseObjectIntoPositionTable_prim() {
  ResourceMark rm;
  fint line, col, len;

  // need a copy because Eval can cause GC.
  const char* source= copy_null_terminated(len);

  StringScanner scanner(source, len, "<position table primitive>");
  Parser parser(&scanner, true); // make parser silent (for Klein) 7/04 dmu

  Object* o = parser.readBody(line, col, source, len);
  if (!o)
    return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);

  // Do scavenge if necessary
  preservedObj x(o);
  if (ScavengeInPrimitives && Memory->needs_scavenge()) {
    Memory->scavenge();
  }
  ByteCodePositionTable p(false);
  o->body->GenByteCodes(&p, o);
  oop r = p.Finish();
  if (p.ranOutOfMemory)
    return ErrorCodes::vmString_prim_error(OUTOFMEMORYERROR);

  return r;
}


oop byteVectorOopClass::write_snapshot_prim(void *FH) {
  return full_write_snapshot_prim(Memory->nilObj, this,
                                  Memory->nilObj, Memory->nilObj,
                                  false, FH); }

oop byteVectorOopClass::primitive_documentation_prim(void *FH) {
  ResourceMark rm;
  oop result = primitive_documentation(copy_null_terminated());
  if (result == NULL) prim_failure(FH, PRIMITIVEFAILEDERROR);
  return result;
}


byteVectorOop byteVectorOopClass::verify_opts_prim() {
  return Memory->verifyOpts(copy_c_heap_null_terminated());
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotsMapDeps.hh"
# include "_slotsMapDeps.cpp.incl"


# define FOR_EACH_SLOT_DEP(d)                                           \
  if (dependents)                                                       \
    for (nmln* d= dependents, *dend= d + length_slots(); d < dend; d++) \


oop slotsMapDeps::define(oop obj, oop contents) {
  if (VerifyZoneOften)
    Memory->code->verify();
  oop result= slotsMap::define(obj, contents);
  if (result->is_mark()) return result;
  Memory->nonCombiningMode();
  FOR_EACH_SLOT_DEP(d) {
    d->invalidate();
  }
  deallocate_slot_dependents();
  add_slot_dependents()->invalidate();
  if (VerifyZoneOften)
    Memory->code->verify();
  return result;
}  


void slotsMapDeps::switch_pointer_in_map_slot(slotDesc *s, oop* where, oop to)
{
  Memory->store(where, to);
  if (dependents)
    dependents_for_slot(s)->invalidate();
}


void slotsMapDeps::shift_obj_slots(smiOop offset, fint delta) {
  FOR_EACH_SLOTDESC(this, slot) {
    if (slot->is_obj_slot() && smiOop(slot->data) >= offset) {
      Memory->store(&slot->data,
                   as_smiOop(smiOop(slot->data)->value() + delta));
      if (   dependents
          && dependents_for_slot(slot)->notEmpty()) {
        Memory->nonCombiningMode();
        dependents_for_slot(slot)->invalidate();
      }
    }
  }
}


slotsOop slotsMapDeps::create_slots(slotList* slots, const char* annotation) {
  slotsMapDeps m1;
  slotsOop obj;
  slotsMapDeps* sm= (slotsMapDeps*) create_map(sizeof(slotsMapDeps),
                                               slots, &m1, (oop*)&obj);
  if (*annotation != '\0') sm->set_annotation(new_string(annotation));
  return obj;
}


oop slotsMapDeps::fill_in_slots(slotList* slist, fint slotCount) {
  add_slot_dependents()->init();
  dependents= NULL;
  return slotsMap::fill_in_slots(slist, slotCount);
}



// Shift the links, as the map is moving during compaction.
// Note that the new_map list is being rebuilt from scratch,
// so we can assume that the target is not in the list.
void slotsMapDeps::shift_map(Map* target) {
  slotsMapDeps *m= (slotsMapDeps*)target;
  Memory->new_gen->add_map(m);
  fint delta= (char*) m - (char*) this;
  add_slot_dependents()->shift(delta);
  // make next map point to shifted (current) map
  map_chain()->shift(delta);
  if (dependents)
    Memory->setDepsMap(dependents, m);
}


void slotsMapDeps::forward_map(Map* oldMap) {
  // make next map point to forwarded (current) map
  fint delta= (char*) this - (char*) oldMap;
  map_chain()->forward(delta);
  add_slot_dependents()->forward(delta);
  if (dependents)
    Memory->setDepsMap(dependents, this);
}


void slotsMapDeps::delete_map() {
  add_slot_dependents()->remove();
  map_chain()->remove();
  FOR_EACH_SLOT_DEP(d) {
    d->remove();
  }
  deallocate_slot_dependents();
}


nmln* slotsMapDeps::dependents_for_slot(slotDesc *s) {
  assert(s >= slots() && s < slot(length_slots()),
         "slotDesc not part of map");
  if (dependents == NULL) {
    dependents= Memory->allocateSlotDeps(this);
    FOR_EACH_SLOT_DEP(d) {
      d->init();
    }
  }    
  return &dependents[s - slots()];
}

void slotsMapDeps::deallocate_slot_dependents() {
  if (dependents) {
    Memory->deallocateSlotDeps(dependents, length_slots());
    dependents= NULL;
  }
}

void slotsMapDeps::moveDeps(nmln* newDeps, int32 delta) {
  FOR_EACH_SLOT_DEP(d) {
    d->shift(delta);
  }
  dependents= newDeps;
}

// receiver should be a new map
void slotsMapDeps::init_dependents() {
  dependents= NULL;
  add_slot_dependents()->init();
}

bool slotsMapDeps::verify_dependents() {
  mapOop m= enclosing_mapOop();
  bool flag= true;
  if (! add_slot_dependents()->verify_map_deps()) {
    lprintf("\tof map 0x%lx\n", m);
    flag = false;
  }
  FOR_EACH_SLOT_DEP(d) {
    flag= flag && d->verify_map_deps();
  }
  if (dependents && *(((slotsMapDeps**)dependents)-1) != this) {
    lprintf("slot map dependency backpointer inconsistent in map %#lx\n",
           (long unsigned)m);
    flag= false;
  }
  if (m->is_new() != Memory->new_gen->has_map(this)) {
    lprintf("map %#lx and new map list are inconsistent\n", m);
    flag= false;
  }
  return flag;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "floatMap.hh"
# include "_floatMap.cpp.incl"

DO_NOT_CROSS_COMPILE 
  
Map* floatMap::create_floatMap(oop parent) {
  slotList *s= new slotList(VMString[PARENT], 
                            parent_map_slotType,
                            parent,
                            Memory->slotAnnotationObj);
  floatMap m1;
  oop ignored;
  return create_map(sizeof(floatMap), s, &m1, &ignored);
}

void floatMap::print_string(oop obj, char* buf) {
  floatOop(obj)->print_string(buf);
}

void floatMap::print_oop(oop obj) {
  floatOop(obj)->print_oop();
}

void floatMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("float ");
  } else {
    floatOop(obj)->print_oop();
    lprintf(": ");
  }
  immediateMap::print(obj);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "objVectorMap.hh"
# include "_objVectorMap.cpp.incl"

oop objVectorMap::clone(oop obj, bool mustAllocate, oop genObj) {
  assert_objVector(obj, "not an obj vector");
  objVectorOop v= objVectorOop(obj)->copy(mustAllocate, genObj);
  if (oop(v) != failedAllocationOop) v->init_mark();
  return v;
}

oop objVectorMap::cloneSize(oop obj, fint len, bool mustAllocate, oop filler) {
  assert_objVector(obj, "not an obj vector");
  fint l = objVectorOop(obj)->length();
  objVectorOop v;
  if (l < len) {
    // growing array
    v= objVectorOop(obj)->grow(len - l, mustAllocate);
    if (oop(v) != failedAllocationOop) {
      set_oops(v->objs(l), len - l, filler);
      if (v->is_old()) {
        if (filler->is_new()) {
          // do all of object
          Memory->remembered_set->record_multistores(v->oops(), v->objs(len));
        } else {
          // just do beginning of object; filler isn't new
          Memory->remembered_set->record_multistores(v->oops(), v->objs(l));
        }
      }
    }
  } else if (l > len) {
    // shrinking array
    v= objVectorOop(obj)->shrink(l - len, mustAllocate);
  } else {
    // copying array
    v= objVectorOop(obj)->copy(mustAllocate);
  }
  if (oop(v) != failedAllocationOop) v->init_mark();
  return v;
}

objVectorOop objVectorMap::create_objVector(slotList* slots) {
  objVectorOop ov;
  objVectorMap m1;
  (void)create_map(sizeof(objVectorMap), slots, &m1, (oop*)&ov);
  return ov;
}

oop objVectorMap::scavenge(oop obj) {
  // use :: to avoid another virtual function call
  return objVectorOop(obj)->scavenge(objVectorMap::object_size(obj)); 
}

bool objVectorMap::verify(oop obj) {
  return objVectorOop(obj)->verify();
}

fint objVectorMap::empty_object_size() {
  return sizeof(objVectorOopClass)/oopSize;
}

// do not change literal backpointer
void objVectorMap::switch_pointer(oop obj, oop* where, oop to) {
  slotDesc* s = find_slot(VMString[METHOD_POINTER]);
  if (s  &&  obj->get_slot_data_address(s) == where) 
    return;
  slotsMap::switch_pointer(obj, where, to);
}

void objVectorMap::dummy_initialize(oop obj, oop filler) { 
  Unused(filler);
  assert_objVector(obj, "not an obj vector");
  objVectorOop(obj)->set_length(0);
}

void objVectorMap::print_objVector(oop obj) {
  lprintf("object array: {");
  if (obj->is_map()) {
    lprintf("...");
  } else {
    bool first = true;
    oop* p = obj_array(obj);
    oop* end = p + length_obj_array(obj);
    oop* end2 = p + VectorPrintLimit < end ? p + VectorPrintLimit : end;
    for (; p < end2; p ++) {
      if (first) first = false;
      else lprintf(", ");
      (*p)->print_oop();
    }
    if (end != end2) {
      lprintf(", ... (%d more elements) ", end - end2);
    }
  }
  lprintf("} ");
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "immediateMap.hh"
# include "_immediateMap.cpp.incl"

oop immediateMap::fill_in_slots(slotList *slist, fint slotCount) {
  UsedOnlyInAssert(slotCount);
  assert(slist->obj_count() == 0, "immediates can't have obj slots");
  assert(slotCount == 1, "only space for a single map slot");

  slots()->init(slist->name(),
                slist->type(),
                slist->contents(),
                slist->annotation());

  return badOop; // nothing to create
}
/* Sun-$Revision: 30.8 $ $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "profilerMap.hh"
# include "_profilerMap.cpp.incl"

profilerOop profilerMap::create_profiler() {
  slotList* slots = new slotList(VMString[PARENT], 
                                 parent_map_slotType,
                                 create_slots((slotList*)NULL));
  profilerMap pm;
  profilerOop prof;
  profilerMap* m= (profilerMap*)create_map(sizeof(profilerMap), slots,
                                           &pm, (oop*)&prof);
  Unused(m); // profiling
  prof->initialize();
  return prof;
}

void profilerMap::kill(oop p) {
  profilerOop(p)->initialize();
}

bool profilerMap::is_live(oop p) {
 return profilerOop(p)->get_profiler() != NULL;
}

fint profilerMap::empty_object_size() {
  return sizeof(profilerOopClass) / oopSize; 
}

bool profilerMap::verify(oop obj) {
  Unused(obj);
  return true;
}

void profilerMap::print(oop obj) {
  ResourceMark rm;
  lprintf("profiler ");
  if (WizardMode && !obj->is_map()) {
    lprintf("(FILL OUT)");
  }
  slotsMapDeps::print(obj);
}

void profilerMap::print_string(oop, char* buf) {
  sprintf(buf, "<a profilerOop>");
}

void profilerMap::print_oop(oop obj) {
  lprintf("<a profilerOop");
  lprintf(" (%#lx) ", (void*)obj);
  lprintf(">");
}

// enumerating
// bool profilerMap::is_enumerable(oop obj,  oop* matching_cell) { }

void profilerMap::dummy_initialize(oop obj, oop filler) {
  Unused(filler);
  assert(obj->is_profiler(), "a profiler object is expected");
  profilerOop(obj)->initialize();
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "markMap.hh"
# include "_markMap.cpp.incl"

Map* markMap::create_markMap() {
  markMap m1;
  oop ignored;
  return create_map(sizeof(markMap), NULL, &m1, &ignored);
}

void markMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("mark ");
    Map::print(obj);
  } else {
    markOop(obj)->print();
  }
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "objVectorOop.hh"
# include "_objVectorOop.cpp.incl"

objVectorOop objVectorOopClass::grow(fint delta, bool mustAllocate) {
  objVectorOop v= (objVectorOop) slotsOopClass::grow(size(), delta,
                                                     mustAllocate);
  if (oop(v) != failedAllocationOop) v->set_length(length() + delta);
  return v;
}

objVectorOop objVectorOopClass::shrink(fint delta, bool mustAllocate) {
  objVectorOop v= (objVectorOop) slotsOopClass::shrink(size(), delta,
                                                       mustAllocate);
  if (oop(v) != failedAllocationOop) v->set_length(length() - delta);
  return v;
}

objVectorOop objVectorOopClass::create_objVector(oop parent) {
  slotList* slist = new slotList(VMString[PARENT],
                                 parent_map_slotType,
                                 parent); 
  return ::create_objVector(slist);
}

objVectorOop objVectorOopClass::create_objVector(fint size) {
  objVectorOop obj = (objVectorOop) create_slots(size);
  obj->set_length(0);
  return obj;
}

bool objVectorOopClass::verify() {
  bool flag = slotsOopClass::verify();
  if (flag) {
    int32 l = length();
    if (l < 0) {
      error1("objVectorOop 0x%lx has negative length", this);
      flag = false;
    }
  }
  return flag;
}

oop objVectorOopClass::ov_size_prim(oop rcvr) {
  if (!rcvr->is_objVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_smiOop(objVectorOop(rcvr)->length());
}

oop objVectorOopClass::ov_at_prim(oop rcvr, oop indexOop) {
  if (!rcvr->is_objVector())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!indexOop->is_smi())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi index = smiOop(indexOop)->value();
  if (unsigned(index) >= unsigned(objVectorOop(rcvr)->length()))
    return ErrorCodes::vmString_prim_error(BADINDEXERROR);
  return objVectorOop(rcvr)->obj_at(index);
}

oop objVectorOopClass::ov_at_put_prim(oop rcvr, oop indexOop, oop contents) {
  if (!rcvr->is_objVector()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!indexOop->is_smi()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi index = smiOop(indexOop)->value();
  if (unsigned(index) >= unsigned(objVectorOop(rcvr)->length()))
    return ErrorCodes::vmString_prim_error(BADINDEXERROR);
  objVectorOop(rcvr)->obj_at_put(index, contents);
  return rcvr;
}

oop objVectorOopClass::ov_clone_prim(smi size, oop filler, void *FH) {
  oop c= cloneSize(size, CANFAIL, filler);
  if (c == failedAllocationOop) {
    out_of_memory_failure(FH, map()->empty_object_size() + size);
    return NULL;
  }    
  // test for scavenging
  if (ScavengeInPrimitives && Memory->needs_scavenge()) {
    return Memory->scavenge(c);
  } else {
    return c;
  }
}

oop objVectorOopClass::methodPointer() {
  slotDesc* methodPointerSlot = find_slot( VMString[METHOD_POINTER]);
  return methodPointerSlot != NULL
    ? get_slot(methodPointerSlot)
    : badOop;
}

oop objVectorOopClass::ov_methodPointer_prim() {
  oop mp = methodPointer();
  return mp != badOop
    ? mp->as_mirror()
    : (oop)ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}

oop objVectorOopClass::ov_references_prim(oop limit) {
  return referencesEnumeration::enumerate_vector_references( this, limit);
}

oop objVectorOopClass::ov_implementors_prim(oop limit) {
  return implementorsEnumeration::enumerate_vector_implementors( this, limit);
}

int32* objVectorOopClass::convertIntArray() {
  oop*   src    = objs();
  oop*   end    = src + length();
  int32* result = NEW_RESOURCE_ARRAY( int32, length());
  int32* dst    = result;
  while (src < end) {
    oop s = *src++;
    if (!s->is_smi())
      return NULL;
    *dst++ = smiOop(s)->value();
  }
  return result;
}


short* objVectorOopClass::convertShortArray() {
  oop*   src    = objs();
  oop*   end    = src + length();
  short* result = NEW_RESOURCE_ARRAY( short, length());
  short* dst    = result;
  while (src < end) {
    oop s = *src++;
    if (!s->is_smi())
      return NULL;
    int32 v = smiOop(s)->value();
    if (int32(short(v)) != v)
      return NULL;
    *dst++ = short(v);
  }
  return result;
}


unsigned short* objVectorOopClass::convertUnsignedShortArray() {
  oop*   src    = objs();
  oop*   end    = src + length();
  unsigned short* result = NEW_RESOURCE_ARRAY( unsigned short, length());
  unsigned short* dst    = result;
  while (src < end) {
    oop s = *src++;
    if (!s->is_smi())
      return NULL;
    int32 v = smiOop(s)->value();
    if (int32((unsigned short)v) != v)
      return NULL;
    *dst++ = (unsigned short)v;
  }
  return result;
}


float* objVectorOopClass::convertFloatArray() {
  oop*   src    = objs();
  oop*   end    = src + length();
  float* result = NEW_RESOURCE_ARRAY( float, length());
  float* dst    = result;
  while (src < end) {
    oop s = *src++;
    if (s->is_smi())
      *dst++ = float(smiOop(s)->value());
    else if (s->is_float())
      *dst++ = floatOop(s)->value();
    else
      return NULL;
  }
  return result;
}


void* objVectorOopClass::convertProxyArray(const void* seal) {
  oop*   src    = objs();
  oop*   end    = src + length();
  void** result = NEW_RESOURCE_ARRAY( void*, length());
  void** dst    = result;
  while (src < end) {
    oop s = *src++;
    proxyOop p = proxyOop(s);
    if ( !s->is_proxy()  
    ||   !p->is_live()  
    ||    p->get_type_seal() != seal)
      return NULL;

    *dst++ = proxyOop(s)->get_pointer();
  }
  return result;
}
/* Sun-$Revision: 30.8 $ $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "profilerOop.hh"
# include "_profilerOop.cpp.incl"


void profilerOopClass::initialize() {
  set_process(illegal_process());
  set_profiler(NULL);
}

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

# define TRACE(str) \
    // fprintf(stderr, "(profiler-trace %s)\n", str)

oop profilerOopClass::ProfilerProcess_prim(void *FH) {
  TRACE("process");
  if (!is_active()) {
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }
  return get_process(); 
}

oop profilerOopClass::ProfilerEngage_prim(oop process, void *FH) {
  TRACE("engage");
  // Fail if argument it is not a process
  if(!process->is_process() ) {
    prim_failure(FH, BADTYPEERROR);
    return NULL;
  } 
  processOop p = processOop(process);

  // Fail if argument has no real process (see processOop.c, checkProcess)
  if(!p->process() || p->process()->processObj() != p ) {
    // dead process or a clone of a process obj
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }

  if (get_profiler() == NULL) {
    set_profiler(new Profiler(this));
  }
  
  set_process(p);
  get_profiler()->engage(p->process());

  return (oop) this;
}

oop profilerOopClass::ProfilerDisengage_prim(void *FH) {
  TRACE("disengage");
  processOop process = get_process();

  if (!is_active()) {
    // No profiler active for this 
    prim_failure(FH, NOPROCESSERROR);
    return NULL;
  }

  assert(process->process()->profiler, "Profiler should be present");

  process->process()->profiler->disengage();
  process->process()->profiler = NULL;

  set_process(illegal_process());

  return (oop) this;
}

oop profilerOopClass::ProfilerReset_prim() {
  TRACE("reset");
  Profiler* prof = get_profiler();
  if (prof) delete prof;
  initialize();
  return (oop) this;
}


oop profilerOopClass::ProfilerTicks_prim(oop tick_pt, void *FH) {
  TRACE("ticks");

  if (!get_profiler()) {
    prim_failure(FH, NOPROFILINGINFOERROR);
    return NULL;
  }
  if (!tick_pt->is_slots()) {
    prim_failure(FH, BADTYPEERROR);
    return NULL;
  }
  oop tick_node = get_profiler()->copy_tick_info(tick_pt);
  if (tick_node == failedAllocationOop) {
    out_of_memory_failure(FH);
    return NULL;
  }
  return tick_node;
}

oop profilerOopClass::ProfilerTimes_prim(oop time_pt, void *FH) {
  TRACE("timesd");

  if (!get_profiler()) {
    prim_failure(FH, NOPROFILINGINFOERROR);
    return NULL;
  }
  if (!time_pt->is_slots()) {
    prim_failure(FH, BADTYPEERROR);
    return NULL;
  }
  oop time_node = get_profiler()->copy_time_info(time_pt);
  if (time_node == failedAllocationOop) {
    out_of_memory_failure(FH);
    return NULL;
  }
  return time_node;
}

oop profilerOopClass::ProfilerPrint_prim(float cutoff) {
  TRACE("call-print");
  get_profiler()->printProfile(cutoff);
  return (oop) this;
}

oop profilerOopClass::ProfilerCopyGraph_prim(oop method_pt, oop block_pt, 
                                             oop access_pt, oop prim_pt,
                                             oop leaf_pt,   oop fold_pt,
                                             oop unknown_oop, smi cutoff,
                                             void *FH) {
  TRACE("call-graph");

  if (!get_profiler()) {
    prim_failure(FH, NOPROFILINGINFOERROR);
    return NULL;
  }

  // check method_pt, block_pt, and prim_pt are vector objects and
  // access_t, leaf_pt and fold_pt are slot objects.
  if (! method_pt->is_objVector() ||
      ! block_pt->is_objVector()  ||
      ! prim_pt->is_objVector()   ||
      ! access_pt->is_slots()     ||
      ! leaf_pt->is_slots()       ||
      ! fold_pt->is_slots()) {
    prim_failure(FH, BADTYPEERROR);
    return NULL;
  }

  oop call_graph = get_profiler()->
                      copy_call_graph(method_pt, block_pt, access_pt, 
                                      prim_pt,   leaf_pt,  fold_pt,
                                      unknown_oop, cutoff);

  if (call_graph == failedAllocationOop) {
    out_of_memory_failure(FH);
    return NULL;
  }
  return call_graph;
}

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)

oop profilerOopClass::ProfilerProcess_prim(void *FH) {
  Unused(FH);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerEngage_prim(oop process, void *FH) {
  Unused(process); Unused(FH);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerDisengage_prim(void *FH) {
  Unused(FH);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerReset_prim() {
  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerTicks_prim(oop tick_pt, void *FH) {
  Unused(tick_pt); Unused(FH);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerTimes_prim(oop time_pt, void *FH) {
  Unused(time_pt); Unused(FH);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerPrint_prim(float cutoff) {
  Unused(cutoff);  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}
oop profilerOopClass::ProfilerCopyGraph_prim(oop method_pt, oop block_pt, 
                                             oop access_pt, oop prim_pt,
                                             oop leaf_pt,   oop fold_pt,
                                             oop unknown_oop, smi cutoff, 
                                             void *FH) {
  Unused(method_pt); Unused(block_pt); Unused(access_pt);
  Unused(prim_pt);   Unused(leaf_pt);  Unused(fold_pt);
  Unused(unknown_oop); Unused(FH);  
  return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "oop.hh"
# pragma implementation "oop_inline.hh"
# pragma implementation "assignmentOop.hh"
# pragma implementation "bits.hh"
# pragma implementation "config.hh"
# pragma implementation "macros.hh"

# include "_oop.cpp.incl"

oop* oopClass::get_slot_data_address_if_present(const char* name, bool &inObj) {
  return get_slot_data_address_if_present(new_string(name), inObj); }

smi oopClass::objectID_prim() {
  return getObjectID(this);
}

oop oopClass::clone_prim(void *FH) {
  oop p= clone(CANFAIL);
  if (p == failedAllocationOop) {
    out_of_memory_failure(FH, size());
    return NULL;
  }
  if (ScavengeInPrimitives && Memory->needs_scavenge()) {
    oop r = Memory->scavenge(p);
    return r;
  } else {
    return p;
  }
}

oop oopClass::print_prim() {
  print();
  return Memory->nilObj; // dont return ``this'', printing it may fail!
}

oop oopClass::all_prim(oop lim){ 
  return allObjEnumeration::enumerate_all_objs(lim); 
}

void oopClass::print_string(char* buf) {
  fint t = tag();
  if (t == Int_Tag) {
    smiOop(this)->print_string(buf);
  } else if (t == Mem_Tag) {
    memOop(this)->print_string(buf);
  } else if (t == Float_Tag) {
    floatOop(this)->print_string(buf);
  } else {
    assert(t == Mark_Tag, "unknown tag");
    markOop(this)->print_string(buf);
  }
}

void oopClass::print_oop() {
  fint t = tag();
  if (t == Int_Tag) {
    smiOop(this)->print_oop();
  } else if (t == Mem_Tag) {
    memOop(this)->print_oop();
  } else if (t == Float_Tag) {
    floatOop(this)->print_oop();
  } else {
    assert(t == Mark_Tag, "unknown tag");
    markOop(this)->print_oop();
  }
}

char* oopClass::debug_print() {
  const int MaxLen = 30;
  fint t = tag();
  char* s = NEW_RESOURCE_ARRAY(char, MaxLen);
  if (t == Int_Tag) {
    sprintf(s, "%ld", long(smiOop(this)->value()));
  } else if (t == Mem_Tag) {
    if      (this == Memory->trueObj)           sprintf(s, "true");
    else if (this == Memory->falseObj)          sprintf(s, "false");
    else if (this == Memory->nilObj)            sprintf(s, "nil");
    else if (this == Memory->lobbyObj)          sprintf(s, "lobby");
    else if (this == Memory->errorObj)          sprintf(s, "!errorObj!");
    else if (this == Memory->deadBlockObj)      sprintf(s, "deadBlock");
    else if (this == Memory->true_mapOop())
      sprintf(s, "true map");
    else if (this == Memory->false_mapOop())
      sprintf(s, "false map");
    else if (this == Memory->nilObj->map()->enclosing_mapOop())
      sprintf(s, "nil map");
    else if (this == Memory->stringObj->map()->enclosing_mapOop() )
      sprintf(s, "string map");
    else if (this == Memory->objVectorObj->map()->enclosing_mapOop() )
      sprintf(s, "objVector map");
    else if (this == Memory->byteVectorObj->map()->enclosing_mapOop() )
      sprintf(s, "byteVector map");
    else if (this == Memory->smi_map->enclosing_mapOop())    sprintf(s, "smi map");
    else if (this == Memory->float_map->enclosing_mapOop())  sprintf(s, "float map");
    else if (is_string()) 
      sprintf(s, "\"%-.*s\"", MaxLen-2,
              stringOop(this)->copy_null_terminated());
    else sprintf(s, "%#lx", long(this));
  } else if (t == Float_Tag) {
    sprintf(s, "%f", floatOop(this)->value());
  } else {
    assert(t == Mark_Tag, "unknown tag");
    sprintf(s, "Mark#%#lx", long(this));
  }
  return s;
}

void oopClass::print_real_oop() {
  FlagSetting f(PrintOopAddress, true);
  print_oop();
}

void oopClass::print() {
  if (is_mark()) {
    markOop(this)->print();
  } else {
    map()->print(this);
  } 
} 

void oopClass::print_real() {
  FlagSetting f(PrintOopAddress, true);
  print();
}

oop scavenge_prim(oop x) {
  BlockProfilerTicks bpt(include_type);
  return Memory->scavenge(x);
}

oop oopClass::garbage_collect_prim() {
  BlockProfilerTicks bpt(include_type);
  return Memory->garbage_collect(this);
}
oop oopClass::tenure_prim() {
  return Memory->tenure(this);
}

oop oopClass::verify_prim() {
  return Memory->verify() ? this : ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
}

 oop oopClass::compact_prim() {
  Memory->code->compactAll();
  return this;
}

oop oopClass::flush_prim() {
  BlockProfilerTicks bpt(include_type);
  Memory->code->flush();
  return this;
}

oop oopClass::flushUnused_prim() {
  Memory->code->flushUnused();
  return this;
}

oop oopClass::markAllUnused_prim() {
  Memory->code->markAllUnused();
  return this;
}

oop oopClass::flush_inline_cache_prim() {
  Memory->code->flush_inline_cache();
  return this;
}

oop oopClass::print_memory_histogram_prim(smi size) {
  Memory->objectSizeHistogram(min(10000, max(size, 0)));
  return this;
}

oop oopClass::print_nmethod_histogram_prim(smi size) {
  Memory->code->print_nmethod_histogram(min(10000, max(size, 0)));
  return this;
}  

oop oopClass::print_memory_prim() {
  printIndent();
  lprintf("Virtual Machine:\n");
  OS::print_memory();
  Memory->print();
  return this;
}

oop oopClass::print_slot_stats_prim() {
  Memory->printSlotDescStats();
  return this;
}

oop oopClass::print_zone_stats_prim() {
  Memory->code->print_stats();
  return this;
}

oop oopClass::manufacturer_prim() {
  return new_string(OS::get_manufacturer_name());
}

oop oopClass::operating_system_prim() {
  ResourceMark rm;
  return new_string(OS::get_operating_system());
}

oop oopClass::credits_prim() {
  lprintf("\nSelf is a community driven, open source project and includes this VM\n");
  lprintf("objects, documentation and materials.\n");
  lprintf("\nIt is a continuation of the Self system as developed at\n");
  lprintf("Xerox Parc, Stanford University and Sun Microsystems.\n");
  lprintf("\nThe first version of the Self language was designed in 1986 by\n");
  lprintf("David Ungar and Randall B. Smith at Xerox PARC. \n");
  lprintf("\nA series of Self implementations and a graphical programming environment\n"); 
  lprintf("were built at Stanford University by Craig Chambers, Urs Hlzle, \n");
  lprintf("Ole Agesen, Elgin Lee, Bay-Wei Chang, and David Ungar. \n");
  lprintf("\nThe project continued at Sun Microsystems Laboratories, where it benefited \n");
  lprintf("from the efforts of Randall B. Smith, Mario Wolczko, John Maloney, and \n");
  lprintf("Lars Bak under the joint leadership of Smith and Ungar. \n");
  lprintf("\nThe Self Group would like to thank Elgin Lee, James \"kjx\" Noble,\n");
  lprintf("Christine Ahrens, Jeff Dean, Erik Ernst, Ivan Moore, and\n");
  lprintf("Michael Abd-El-Malek for their contributions.\n\n");
  lprintf("Finally, thanks go to Sun Microsystems Laboratories, Stanford University,\n");
  lprintf("and all other organizations that supported Self (in alphabetical order):\n");
  lprintf("Aarhus University, Apple, Cray, the Danish Research Academy, DEC, IBM,\n");
  lprintf("the National Science Foundation, NCR, the Swiss Nationalfonds, Tandem,\n");
  lprintf("Texas Instruments, and Xerox.\n\n");
  lprintf("Copyright 1992-2012 AUTHORS.\n");
  lprintf("See the LICENSE file for license information.\n\n");
  return new_string("Thanks!");
}




static void print_option_primitives(bool);

oop oopClass::print_option_primitives_prim() { 
  print_option_primitives(false); 
  return this;
}

oop oopClass::print_changed_option_primitives_prim() {
  print_option_primitives(true); 
  return this;
}

void print_option_primitives(bool changedOnly) {
# define ListPrimName(                                                        \
    flagName, flagType, flagTypeName, primReturnType,                         \
    initialValue, getCurrentValue, checkNewValue, setNewValue,                \
    explanation, wizardOnly)                                                  \
                                                                              \
    if ( changedOnly                                                          \
      ? ( flagName != initialValue )                                   \
      : (!wizardOnly || WizardMode )         ) {                              \
      lprintf("_%s: %s = %s\n", XSTR(flagName), flagTypeName, explanation);    \
      lprintf("_%s => ", XSTR(flagName));                                      \
    (getCurrentValue)->print_oop();                                           \
    lprintf("\n\n");                                                           \
  }
    
  FOR_ALL_DEBUG_PRIMS(ListPrimName)
#     undef ListPrimName
  }



oop oopClass::quit_prim() {
# if TARGET_IS_PROFILED
    lprintf("Writing profile statistics...\n");
# endif
  OS::terminate(0);
  return NULL;
}

oop bad_prim(oop rcvr) {
  Unused(rcvr);
  return ErrorCodes::vmString_prim_error(PRIMITIVENOTDEFINEDERROR);
}

mirrorOop oopClass::as_mirror(bool mustAllocate) {
  mirrorOop mirror= mirrorOop(map()->mirror_proto()->clone(mustAllocate));
  if (oop(mirror) != failedAllocationOop) mirror->set_reflectee(this);
  return mirror;
}

# define DefineDebugPrim(                                                     \
    flagName, flagType, flagTypeName, primReturnType,                         \
    initialValue, getCurrentValue, checkNewValue, setNewValue,                \
    explanation, wizardOnly)                                                  \
                                                                              \
    oop CONC3(get_,flagName,_prim)(oop rcvr)     {                            \
      Unused(rcvr);                                                           \
      return getCurrentValue;                                                 \
    }                                                                         \
                                                                              \
    oop CONC3(set_,flagName,_prim)(oop rcvr, oop flag) {                      \
      breakpoint();                                                           \
      Unused(rcvr); Unused(flag);                                             \
      if (! (checkNewValue)) {                                                \
        return ErrorCodes::vmString_prim_error(BADTYPEERROR);                 \
      }                                                                       \
      oop oldValue = getCurrentValue;                                         \
      setNewValue;                                                            \
      return oldValue;                                                        \
    }

  FOR_ALL_DEBUG_PRIMS(DefineDebugPrim)

# undef DefineDebugPrim

// eval method as if self is receiver

oop oopClass::evaluate_in_context_prim(oop method) {
  if ( (!NakedMethods && is_method_like())
  ||   is_vframe())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  
  if (!method->is_method_like()) return method; // like running 17
  if (method->map()->kind() != OuterMethodType) 
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (method->arg_count() != 0) return ErrorCodes::vmString_prim_error(ARGUMENTCOUNTERROR);
  
  oop res = currentProcess->runDoItMethod(this, method);
  currentProcess->cleanup_after_eval_prim(res);
  return res;
}

oop oopClass::unwind_protect_prim(oop doBlock, oop protectBlock) {
  // send "value" to doBlock;
  //   if this msg returns V normally, then return V as result of primitive.
  //   otherwise, msg returns V with non-local return or abort;
  //     send "value: V" to protectBlock;
  //     if this msg returns V', normally or non-locally, then
  //       continue the original nlr/abort with V' as the value being returned.
  //     if this msg aborts, then continue the abort
  //
  //  I think this function may have a BUG! It compiles without switching
  //  stacks. -- dmu

  { // start a scope for the resource mark
    ResourceMark rm;

    assert(NakedMethods || !doBlock->is_method_like(), "shouldn't be a method");
    assert(NakedMethods || !protectBlock->is_method_like(), "shouldn't be a method");

    // this should be valid across calls to self, right?
    abstract_vframe* vf = new_vframe(currentProcess->last_self_frame(false));

    // lookup nmethod for 1st value message send

#   if defined (FAST_COMPILER) || defined(SIC_COMPILER)
#   define makeALookup(theLookup, r, s) \
      cacheProbingLookup theLookup(r, s, NULL, MH_TBD, vf, sendDesc::first_sendDesc(), \
                                   NULL, false)
#   else
#   define makeALookup(theLookup, r, s) \
      simpleLookup theLookup(NormalLookupType, MH_NOT_A_RESEND, r, s, NULL)
#   endif
        
    makeALookup( L, doBlock, VMString[VALUE] );
    
    nmethod* nm = NULL;
    if (Interpret)
      L.perform_full_lookup_n(0);
    else {
#     if defined (FAST_COMPILER) || defined(SIC_COMPILER)
        nm= L.lookupNMethod();
#     else
        ShouldNotReachHere(); // no compilers: must interpret
#     endif
    }
   oop res, arg;
    {
      preserved p(protectBlock);

      // send the first value message
      
      res = L.evaluateResult(&arg, 0, nm);

      protectBlock = p.value;
    }

    if (!NLRSupport::have_NLR_through_C()) {
      // no nlr; just return
      return res;
    }

    // had a NLR; need to invoke protect block with result as arg

    // first, save target of original NLR
    int32 OriginalNLRHomeFromC   = NLRSupport::NLR_home_from_C();
    int32 OriginalNLRHomeIDFromC = NLRSupport::NLR_home_ID_from_C();

    NLRSupport::reset_have_NLR_through_C();    // forget the old NLR, for now

    bool original_aborting = res->is_mark();
    assert( res == badOop  ||  res == 0  ||  OriginalNLRHomeFromC != NULL,
           "if not aborting, must have a how frame");
    
    // lookup nmethod for 2nd value: message send
    makeALookup( Ltwo, protectBlock, VMString[VALUE_] );
                            
    nmethod* nm2 = NULL;
    if (Interpret)
      Ltwo.perform_full_lookup_n(1);
    else {
#     if defined (FAST_COMPILER) || defined(SIC_COMPILER)
      nm2= Ltwo.lookupNMethod();
#     else
        ShouldNotReachHere(); // no compilers: must interpret
#     endif
    }
    oop res2;
    {
      preserved p(res);

      // send the 2nd value: message
      arg =  original_aborting ? Memory->nilObj : res;
      preservedArray p1(&arg, 1); // interp does not scav args
      res2 = Ltwo.evaluateResult(&arg, 1, nm2);
                
      res = p.value;
    }

    // determine target of nlr
    if (NLRSupport::have_NLR_through_C() && NLRSupport::NLR_home_from_C() == 0) {
      // aborting; use that pseudo nlr target
    } 
    else {
      // use original nlr target
      NLRSupport::set_have_NLR_through_C();
      NLRSupport::set_NLR_home_from_C( OriginalNLRHomeFromC );
      NLRSupport::set_NLR_home_ID_from_C ( OriginalNLRHomeIDFromC );
    }

    // determine result value of nlr
    // use original aborting result or new result
    NLRSupport::set_NLR_result_from_C(original_aborting ? res : res2);

    LOG_EVENT2("*3* continuing: NLR=%#lx, ID=%d",
               (unsigned long)NLRSupport::NLR_home_from_C(),
               NLRSupport::NLR_home_ID_from_C());

  } // end scope for the resource mark
  // return non-locally
  NLRSupport::continue_NLR_into_Self(false);

  ShouldNotReachHere(); // NLRSupport::continue_NLR_into_Self() shouldn't return
  return badOop;
}

void oopClass::switch_pointer(oop* where, oop to) {
  map()->switch_pointer(this, where, to);
}

oop oopClass::define(oop contents) {
  return map()->define(this, contents);
}


// 2 ways of looking at define:
//   1. make receiver be a copy of the argument
//   2. redirect all pointers to receiver to point to a copy of the argument

oop oopClass::define_prim(oop contents, void *FH) {
  oop result= define(contents);
  if (result == failedAllocationOop) {
    out_of_memory_failure(FH, contents->size());
    return NULL;
  }
  if (bootstrapping || result->is_mark()) {
    // can't call convert() while creating lobby!
    // (and don't bother incrementing timestamp, either)
    // not worth calling it on failure, either
  } else {
    processes->convert();
    Memory->increment_programming_timestamp(); 
  }
  return result;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "foreignMap.hh"
# pragma implementation "proxyMap.hh"

# include "_proxyMap.cpp.incl"


proxyOop proxyMap::create_proxy() {
  slotList* slots = new slotList(VMString[PARENT], 
                                 parent_map_slotType,
                                 create_slots((slotList*)NULL));
  proxyMap mm;
  proxyOop mr;
  (void)create_map(sizeof(proxyMap), slots, &mm, (oop*)&mr);
  mr->set_pointer(NULL);
  mr->set_type_seal(NULL);
  mr->kill();
  assert(mr->is_killable() && mr->is_foreign() && !mr->is_live(), 
         "should be dead foreigner");
  return mr;
}


fint proxyMap::empty_object_size() {
  return sizeof(proxyOopClass) / oopSize; 
}


void proxyMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("proxy ");
  } else {
    if (proxyOop(obj)->is_live()) {
      lprintf("proxy(live) ");
    } else {
      lprintf("proxy(dead) ");
    }
    if (WizardMode) 
      lprintf("ptr = %#lx, type_seal = %#lx ",
              proxyOop(obj)->get_pointer(),
              proxyOop(obj)->get_type_seal());
  }
  slotsMap::print(obj);
}

void proxyMap::dummy_initialize(oop obj, oop filler){
  foreignMap::dummy_initialize(obj, filler);
  proxyOop(obj)->set_type_seal(NULL);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "codeLikeSlotsMap.hh"
# include "_codeLikeSlotsMap.cpp.incl"

fint codeLikeSlotsMap::arg_count() {
  fint argc = 0;
  FOR_EACH_SLOTDESC(this, slot) {
    if (slot->is_arg_slot()) argc++;
  }
  assert(has_code() || argc == 0, "only objects with code can have arg slots");
  return argc;
}


/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "blockOop.hh"
# pragma implementation "blockOop_inline.hh"

# include "_blockOop.cpp.incl"

frame* blockOopClass::scope(bool orNone) {
  UsedOnlyInAssert(orNone);
  blockOopClass *b= addr();
  frame* f= b->homeFr();
  if (NLRSupport::is_bad_home_reference((char*)f)) {
    // prototype block or non-LIFO block
    f = NULL;
  }
  assert(f || orNone, "blockMap::scope: block is dead");
  return f;
}


void blockOopClass::setScope(frame* newScope) {
  blockOopClass *b= addr();
  frame* f= b->homeFr();
  if (NLRSupport::is_bad_home_reference((char*)f)) {
    // prototype block or non-LIFO block
    if (newScope != NULL)
      ShouldNotReachHere(); // shouldn't try to set scope pointer
  } else {
    assert_smi(newScope, "should be a word-aligned pointer");
    b->setHomeFr(smiOop(newScope));
  }
}



// an optimized version
inline oop blockOopClass::really_clone_block(smiOop fp) {
  assert_block(this, "not a block");
  assert_smi(fp, "not a smallInt or pointer");
  NumberOfBlockClones++;
  const int32 size = sizeof(blockOopClass)/oopSize;

  // would be cleaner (but slightly slower in the fast case) to do
  //  b= Memory->alloc_objs(size);
  //  if (Memory->new_gen->eden_space->contains(b)) ...

  blockOopClass* b=
    (blockOopClass*) Memory->new_gen->eden_space->alloc_objs_local(size);
  if (b) {
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  b == (blockOopClass*)catchThisOne) {
      warning1("blockOopClass::really_clone_block caught 0x%lx", b);
    }
#   endif
    // allocated in eden; don't do check stores or scavenging
    b->_map = addr()->_map;
    b->setHomeFr(fp);
    blockOop b1 = as_blockOop(b);
    b1->init_mark();
    return b1;
  } else {
    // overflowed eden; do check stores when done
    b = (blockOopClass*) Memory->alloc_objs(size);
    Memory->store((oop*) &b->_map, addr()->_map);
    b->setHomeFr(fp);
    blockOop b1 = as_blockOop(b);
    b1->init_mark();
    return b1;
  }
}

oop blockOopClass::clone_block(smiOop fp) { return really_clone_block(fp); }

oop blockOopClass::clone_block_for_interpreter(frame* f) {
  return clone_block(smiOop(f));
}


// internal primitive to clone block literal and set scope

oop clone_block_prim(oop rcvr, smiOop fp) {
  return blockOop(rcvr)->clone_block(fp);
}


  
frame* blockOopClass::parentFrame(frame* currentFrame, bool orNone) {
  // currentFrame, if non-NULL, is a hint (where to start searching for
  // the full frame for this block)
  frame* sc = scope(orNone);
  if (sc == NULL) return NULL;
  if (currentFrame) {
    // the hint may be wrong (rarely) if the two frames are on different
    // stacks)
    Stack* stk = processes->stackFor(currentFrame);
    if (!stk->contains(sc)) currentFrame = NULL;
  }
  frame* pf = sc->home_frame_of_block_scope(currentFrame);
  assert(pf->block_scope_of_home_frame() == sc, "wrong frame");
  assert(desc()->value() != IllegalDescOffset, "parent vframe not visible");
  # if GENERATE_DEBUGGING_AIDS
    if ( !CheckAssertions )
      ;
    else if ( pf->is_interpreted_self_frame() )
      assert( desc() == BLOCK_PROTO_DESC, "interpreted blocks have original desc" );
    else
      assert( desc() != BLOCK_PROTO_DESC, "compiled blocks do not have original desc" );
  # endif

  return pf;
}


abstract_vframe* blockOopClass::parentVFrame(frame* currentFrame, bool orNone) {
  frame* pf= parentFrame(currentFrame, orNone);
  return pf == NULL  ?  NULL  :  new_vframe(pf, desc());
}

stringOop blockOopClass::outermostMethodSelector() {
  oop selector = NULL;
  for ( abstract_vframe* pp = parentVFrame(NULL, true); 
                         pp != NULL;
                         pp = pp->parent())
    selector = pp->selector();
    
  if (selector == NULL) { // block was zapped  
    /* should really get name from lexical linkes
       here's a start:
        slotsOop v = value();
        if (v->map()->has_code()) {
          slotsOop ll = v->map()->get_lexical_link();
          ....
    */
    selector = new_string("zappedBlock");
  }

  assert_string(selector, "should be a selector");
  return stringOop(selector);
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


void blockOopClass::remap(nmethod* nm, frame* fr) {
  // the receiver's enclosing nmethod has been recompiled; update the
  // map and scope pointer
  // frame* home = scope();   // caller ensures liveness
  // Map* oldMap = map();
  Map* newMap = nm->blockMapFor(this);
  // assert(oldMap != newMap, "maps should be different");
  // can be equal for receiver block, or could have been remapped in
  // one of the sending vframes
  if (!newMap) ShouldNotReachHere(); // should have found new block map
  remap(newMap, fr);
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void blockOopClass::remap(Map* newMap, frame* newHome) {
  LOG_EVENT3("remapping block %#lx from %#lx/%#lx...",
             this, map(), scope(true));
  LOG_EVENT2("...to %#lx/%#lx", newMap, newHome);
  set_map(newMap);
  setScope(newHome->block_scope_of_home_frame());
  // debugging aid: set block's hash (almost all blocks have an uninitialized
  // hash value, so if a block has a hash it's likely to be a remapped block)
  identity_hash();
}


blockOop blockOopClass::clone_and_set_desc(smiOop descOffset) {
  assert_smi(descOffset, "must be an int");
  blockMap *newMap= ((blockMap*)map())->clone_and_set_desc(descOffset);
  blockOop newBlock= copy();
  newBlock->set_map(newMap);
  return newBlock;
}


#if defined(FAST_COMPILER) || defined(SIC_COMPILER)

# if  GENERATE_DEBUGGING_AIDS
static mapOop map_to_find;
static void findBlkFn(nmethod* nm) {
  nmethodScopes* s = nm->scopes;
  for (oop* p = s->oops(), *end = p + s->oops_size(); p < end; p++) {
    if (*p == map_to_find || (*p)->map()->enclosing_mapOop() == map_to_find) {
      lprintf("nmethod* %#lx\n", nm);
      return;
    }
  }
}

void findNMethodsFor(oop b) {
  // for debugging
  if (!b->is_block()) {
    lprintf("(warning: not a block)\n");
  }
  map_to_find = b->map()->enclosing_mapOop();
  Memory->code->nmethods_do(findBlkFn);
}
#endif

#endif

oop catch_interprocess_returns(oop blk) {
  // for debugging: catch nlrs to blocks in other processes
  // (if CatchInterprocessReturns is on)
  // lprintf("blk 0x%x,  scope 0x%x,  scopeID 0x%x\n",
  //         blk, scope, scopeID);
  if (!blk->is_block())
    return blk;
  blockOop b = blockOop(blk);
  if (!b->is_live())
    return b;
  frame* myScope = b->scope(true);
  if (myScope == NULL)
    return b;
  if (!currentProcess->contains(myScope)) {
    b->kill();
  }
  return b;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "vframeMap.hh"
# include "_vframeMap.cpp.incl"

// all routines that call this macro that are called by primitives must
// have the canWalkStack flag set in the primitive table so the
// nonvols will be saved for the vframe creation -- dmu 6/99

# define TEST                                                                 \
    ResourceMark rm;                                                          \
    vframeOop vfo = vframeOop(obj);                                           \
    if (!vfo->is_live()) return ErrorCodes::vmString_prim_error(NOACTIVATIONERROR);  \
    /* must check for dead vfos if current process */                         \
    if (vfo->process() == currentProcess)                                     \
      currentProcess->killVFrameOopsAndSetWatermark(NULL);

# define SLOTS_PRIM(name)                                                     \
    oop vframeMap::name(oop obj,  stringOop nam) {                            \
    TEST;                                                                     \
    return vfo->method()->name(nam);                                          \
  }                                                                           \
    
oop vframeMap::mirror_names(oop obj) {
  TEST;
  return vfo->method()->mirror_names();
}

oop vframeMap::mirror_name_at(oop obj, smi inx) {
  TEST;
  return vfo->method()->mirror_name_at(inx);
}

SLOTS_PRIM(mirror_is_parent_at)
SLOTS_PRIM(mirror_is_argument_at)
SLOTS_PRIM(mirror_is_assignable_at)

void vframeMap::kill(oop vfp)    { vframeOop(vfp)->kill_vframe(); }
bool vframeMap::is_live(oop vfp) { return vframeOop(vfp)->is_live_vframe(); }

bool vframeMap::verify(oop obj) { return vframeOop(obj)->verify(); }

oop vframeMap::mirror_codes(oop obj) {
  return vframeOop(obj)->method()->mirror_codes(); }
oop vframeMap::mirror_literals(oop obj) {
  return vframeOop(obj)->method()->mirror_literals(); }
oop vframeMap::mirror_source(oop obj) {
  return vframeOop(obj)->method()->mirror_source(); }
oop vframeMap::mirror_source_length(oop obj) {
  return vframeOop(obj)->method()->mirror_source_length(); }
oop vframeMap::mirror_source_offset(oop obj) {
  return vframeOop(obj)->method()->mirror_source_offset(); }
oop vframeMap::mirror_file(oop obj) {
  return vframeOop(obj)->method()->mirror_file(); }
oop vframeMap::mirror_line(oop obj) {
  return vframeOop(obj)->method()->mirror_line(); }

oop vframeMap::mirror_contents_at(oop obj, stringOop name) {
  TEST;
  slotDesc* sd = vfo->method()->find_nonVM_slot(name);
  if (sd == NULL) return ErrorCodes::vmString_prim_error(SLOTNAMEERROR);
  if (sd->is_obj_slot() && name->is_1arg_keyword())
    return Memory->assignmentMirrorObj;
  abstract_vframe* vf = vfo->as_vframe();
  oop contents = vf->get_slot(sd);
  return contents->as_mirror();
}

# define VFRAME_PRIM(name, what)                                              \
  oop vframeMap::name(oop obj) {                                              \
    TEST;                                                                     \
    abstract_vframe* vf = vfo->as_vframe();                                           \
    return what;                                                              \
  }                                                                           \
    
VFRAME_PRIM(mirror_bci, as_smiOop(vf->bci()))
VFRAME_PRIM(mirror_methodHolder, vf->methodHolder_object()->as_mirror())

oop vframeMap::mirror_receiver(oop obj) {
  TEST;
  return vfo->as_vframe()->receiver()->as_mirror();
}

oop vframeMap::mirror_parent(oop obj) {
  TEST;
  abstract_vframe* parent = vfo->as_vframe()->parent();
  if (!parent) return ErrorCodes::vmString_prim_error(NOPARENTERROR);
  assert(vfo->process() == vmProcess || !parent->is_first_self_vframe(),
         "doIt of a process cannot have blocks!");
  vframeOop nvfo = new_vframeOop(vfo->process(), parent);
  return nvfo->as_mirror();
}

oop vframeMap::mirror_sender(oop obj) {
  TEST;
  abstract_vframe* vf = vfo->as_vframe();
  abstract_vframe* sender = vf->sender();
  if (! sender ||
      (vfo->process() != vmProcess && sender->is_first_self_vframe()))
    return ErrorCodes::vmString_prim_error(NOSENDERERROR);
  
  vframeOop nvfo = new_senderVFrameOop(vfo->process(), vfo, sender);

  return nvfo->as_mirror();
}

oop vframeMap::mirror_expr_stack(oop obj) {
  TEST;
  smi len;
  oop* vec;
  vfo->as_vframe()->get_expr_stack(vec, len);
  objVectorOop objVec= Memory->objVectorObj->cloneSize(len);
  for (smi i = 0; i < len; i++) objVec->obj_at_put(i, vec[i]);    
  return objVec;
}

oop vframeMap::mirror_selector(oop obj) {
  TEST;
  return vfo->as_vframe()->selector();
}

oop vframeMap::clone(oop obj, bool mustAllocate, oop genObj) {
  vframeOop theClone= vframeOop(obj)->basic_clone(mustAllocate, genObj);
  if (oop(theClone) != failedAllocationOop && theClone->is_live()) {
    // link clone into list after the original
    vframeOop(obj)->set_next(theClone);
  }
  return theClone;
}

oop vframeMap::cloneSize(oop obj, fint length, bool mustAllocate, oop filler) {
  return slotsMapDeps::cloneSize(obj, length, mustAllocate, filler); }

fint vframeMap::empty_object_size() {
  return sizeof(vframeOopClass) / oopSize; 
}

void vframeMap::print(oop obj) {
  ResourceMark rm;
  lprintf("vframe ");
  if (WizardMode && !obj->is_map()) {
    vframeOop vfo = vframeOop(obj);
    if (vfo->is_live()) {
      abstract_vframe* vf = vfo->as_vframe();
      const char* sel = vf->selector() ?
        stringOop(vf->selector())->copy_null_terminated() : "...";
      lprintf("(%s / %#lx %#lx) ", sel, vfo->fr(),
             vfo->locals());
    } else {
      lprintf("(DEAD)");
    }
  }
  slotsMapDeps::print(obj);
}

void vframeMap::print_string(oop obj, char* buf) {
  bool live = vframeOop(obj)->is_live();
  sprintf(buf, "<a%s activation object>", live ? "n" : " dead");
}


void vframeMap::print_oop(oop obj) {
  bool live = vframeOop(obj)->is_live();
  lprintf("<a%s activation object", live ? "n" : " dead");
  if (WizardMode && live) {
    ResourceMark rm;
    abstract_vframe* vf = vframeOop(obj)->as_vframe();
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      if (vf->is_compiled()) {
        compiled_vframe* v = vf->as_compiled();
        lprintf(" <%#lx @ %#lx # %ld>", 
               vf->fr,
               v->code,
               (void*)long(v->descOffset()->value()));
      }
#   endif
    methodMap *mm = (methodMap*) vf->method()->map();
    stringOop file = mm->file();
    if (file->length() > 0) {
      lprintf(" (");
      file->string_print();
      lprintf(":%ld): ", mm->line()->value());
    } else {
      lprintf(" ");
    }
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      if (vf->is_compiled()) {
        compiled_vframe* v = vf->as_compiled();
        if (v->desc->key.selector->is_string()) {
          stringOop(v->desc->key.selector)->string_print();
        } else {
          v->desc->key.selector->print_oop();
        }
      }
#   endif
  } else if (PrintOopAddress) {
    lprintf(" (%#lx) ", obj);
  }
  lprintf(">");
}

oop vframeMap::mirror_annotation_at(oop obj, stringOop name) {
  return vframeOop(obj)->method()->map()->
    mirror_annotation_at(obj, name);
}

// enumerating
bool vframeMap::is_enumerable(oop obj,  oop* matching_cell) {
  return  matching_cell >=     vframeOop(obj)->oops(empty_object_size())
    ||   *matching_cell ==     vframeOop(obj)->method()
    ||   *matching_cell == oop(vframeOop(obj)->process());
};

void vframeMap::dummy_initialize(oop obj, oop filler) {
  assert( obj->is_vframe(), "a vframeOop is expected");
  vframeOop(obj)->set_next(NULL);
  vframeOop(obj)->set_method(filler);
  vframeOop(obj)->set_locals(NULL);
  vframeOop(obj)->set_process(NULL);
  vframeOop(obj)->set_descOffset(0);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mirrorMap.hh"
# include "_mirrorMap.cpp.incl"

mirrorOop mirrorMap::create_mirror(oop reflectee) {
  slotList* slots = new slotList(VMString[PARENT], 
                                 parent_map_slotType,
                                 create_slots((slotList*)NULL));
  mirrorMap mm;
  mirrorOop mr;
  (void)create_map(sizeof(mirrorMap), slots, &mm, (oop*)&mr);
  mr->set_reflectee(reflectee);
  return mr;
}

fint mirrorMap::empty_object_size() {
  return sizeof(mirrorOopClass) / oopSize;
}

void mirrorMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("mirror ");
  } else {
    lprintf("mirror <reflectee = ");
    mirrorOop(obj)->reflectee()->print_oop();
    lprintf("> ");
  }
  slotsMapDeps::print(obj);
}

void mirrorMap::switch_pointer(oop obj, oop* where, oop to) {
  assert_mirror(obj, "obj must be mirror");
  mirrorOop(obj)->switch_reflectee(where, to);
  slotsMapDeps::switch_pointer(obj, where, to);
}

void mirrorMap::dummy_initialize(oop obj, oop filler) {
  assert_mirror(obj, "obj must be mirror");
  mirrorOop(obj)->set_reflectee(filler);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stringMap.hh"
# include "_stringMap.cpp.incl"

void stringMap::create_initial_strings(oop parent) {
  // create string map
  slotList* slist =
    new slotList(NULL, parent_map_slotType, parent); 
  stringMap m1;
  stringOop s;
  stringMap* m= (stringMap*) create_map(sizeof(stringMap), slist, &m1,
                                        (oop*)&s);
  assert(Memory->old_gen->objs_contains(s),
         "should be allocated in old space");

  s->set_length(0);
  s->set_bytes((char*) Memory->old_gen->empty_bytes_addr());
  Memory->stringObj = s;
  
  // add in initial empty string object
  Memory->string_table->add(s);

  // initialize vm strings -- can only do this after stringObj has been made
  vmStrings_init();
  
  // fill in empty parent slot name with "parent" VM string
  slotDesc* slot = m->slots();
  Memory->store((oop*)&slot->name, oop(VMString[PARENT]));
}

oop stringMap::clone(oop obj, bool mustAllocate, oop genObj) {
  Unused(mustAllocate); Unused(genObj);
  assert_string(obj, "object isn't a string");
  return obj;
}

oop stringMap::cloneSize(oop obj, fint len, bool mustAllocate, oop filler) {
  Unused(obj); Unused(len); Unused(mustAllocate); Unused(filler);
  ShouldNotCallThis(); // shouldn't be cloning/resizing a string
  return NULL;
}

oop stringMap::scavenge(oop obj) {
  Unused(obj);
  ShouldNotCallThis(); // shouldn't need to scavenge canonical strings
                       // (should be tenured)
  return NULL;
}

bool stringMap::verify(oop obj) {
  return stringOop(obj)->verify();
}

void stringMap::print_string(oop obj, char* buf) {
  /* sprintf breaks (why?)
  sprintf(buf, "'%*s'", 
          stringOop(obj)->length(), 
          stringOop(obj)->bytes());
  */
  buf[0] = '\'';
  fint n = stringOop(obj)->length();
  n = min(n, BUFSIZ - 3);
  strncpy(buf + 1, stringOop(obj)->bytes(), n);
  buf[ 1 + n ] = '\'';
  buf[ 2 + n ] = '\0';
}

void stringMap::print_oop(oop obj) {
  lprintf("'");
  stringOop(obj)->string_print();
  lprintf("'");
}

void stringMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("string ");
  } else {
    lprintf("'");
    stringOop(obj)->string_print();
    lprintf("' ");
  }
  byteVectorMap::print(obj);
}

int32 stringMap::debug_size(oop p) {
  return p->size() * oopSize + stringOop(p)->length();
}
  
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "fctProxyMap.hh"
# include "_fctProxyMap.cpp.incl"


fctProxyOop fctProxyMap::create_fctProxy() {
  slotList* slots = new slotList(VMString[PARENT], 
                                 parent_map_slotType,
                                 create_slots((slotList*)NULL));
  fctProxyMap mm;
  fctProxyOop mr;
  (void)create_map(sizeof(fctProxyMap), slots, &mm, (oop*)&mr);
  mr->kill();
  assert(mr->is_killable() && mr->is_foreign() && !mr->is_live(), 
         "should be dead foreigner");
  return mr;
}


fint fctProxyMap::empty_object_size() {
  return sizeof(fctProxyOopClass) / oopSize; 
}


void fctProxyMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("fctProxy ");
  } else {
    lprintf("fctProxy(%s) ", fctProxyOop(obj)->is_live() ? "live" : "dead");
    if (WizardMode)
      lprintf("ptr = %#lx, type_seal = %#lx, argc = %d ", 
             fctProxyOop(obj)->get_pointer(),
             fctProxyOop(obj)->get_type_seal(),
             fctProxyOop(obj)->get_noOfArgs());
  }
  slotsMap::print(obj);
}

bool fctProxyMap::is_enumerable(oop obj,  oop* matching_cell) {
  return          matching_cell  >= fctProxyOop(obj)->oops(empty_object_size())
    ||   smiOop(*matching_cell)  == as_smiOop(fctProxyOop(obj)->get_noOfArgs());
}

void fctProxyMap::dummy_initialize(oop obj, oop filler) {
  assert( obj->is_fctProxy(), "a fctProxyOop is expected");
  proxyMap::dummy_initialize(obj, filler);
  fctProxyOop(obj)->set_noOfArgs(0);
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "floatOop.hh"
# include "_floatOop.cpp.incl"

DO_NOT_CROSS_COMPILE
  
/* float formats:
   IEEE_float = { int sign:1; int fract: 23; int exp: 8 }

   Normally:
   Self_float = { int sign:1; int fract: 23; int exp: 6; int tag: 2}

   if FAST_FLOATS is defined
   Self_float = { int sign:1; int fract: 21; int exp: 8; int tag: 2}
*/
  
static const fint fractSize = 23;
static const fint expSize = 8;
static const fint expOffset = fractSize;

# ifdef FAST_FLOATS
  floatOop infinityOop = floatOop(nthMask(expSize) << expOffset
                                  |  Float_Tag);
# else

  static const fint signSize = 1;
  static const fint signOffset = fractSize + expSize;
  
  static const fint selfExpSize   = fint(expSize   - Tag_Size);
  static const fint selfExpOffset = fint(expOffset + Tag_Size);
  
  static const fint bias = nthBit(expSize) / 2  -  1;
  static const fint selfBias = nthBit(selfExpSize) / 2  -  1;
  
  floatOop infinityOop = floatOop(nthMask(selfExpSize) << selfExpOffset
                                  |  Float_Tag);
  
  floatOop floatOopClass::as_floatOop(float value) {
    union { float f; uint32 i; } x;
    x.f = value;
    uint32 i = x.i;
    int32 exp     = i >> expOffset  &  nthMask(expSize);
    int32 selfExp = exp - bias + selfBias;
    int32 fract   = i & nthMask(fractSize);
    int32 r       = (i & (nthMask(signSize) << signOffset)) | Float_Tag;
    
    if (selfExp <= 0) {
      // underflow - round towards zero
    } else if (selfExp >= nthMask(selfExpSize)) {
      if (exp < nthMask(expSize)) {
        // warning1("converting float %g to Inf", value);
        r |= nthMask(selfExpSize) << selfExpOffset;
      } else if (fract == 0) {
        r |= int32(infinityOop);
      } else { // NaN
        r |= (nthMask(selfExpSize) << selfExpOffset) | (fract << Tag_Size);
      }
    } else {
      r |= (selfExp << selfExpOffset) | (fract << Tag_Size);
    }
    return floatOop(r);
  }
  
  
  float floatOopClass::value() {
    uint32 i = uint32(this);
    int32 selfExp = i >> selfExpOffset  &  nthMask(selfExpSize);
    int32 fract   = i >> Tag_Size  &  nthMask(fractSize);
    int32 exp     = selfExp - selfBias + bias;
    int32 r       = i & (nthMask(signSize) << signOffset);
    
    if (selfExp == 0) {
      assert(fract == 0, "self float is denormalized");
      // zero
    } else if (selfExp >= nthMask(selfExpSize)) {
      if (fract == 0) {
        r |= nthMask(expSize) << expOffset; // infinity
      } else {
        r |= (nthMask(expSize) << expOffset) | fract; // NaN
      }
    } else {
      r |= (exp << expOffset) | fract;
    }
    
    union { float f; uint32 i; } x;
    x.i = r;
    return x.f;
  }
# endif


# define CHECK_XY                                                             \
    if (!x->is_float() || !y->is_float()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);

# define FLOAT_PRIM(name, op)                                                 \
  oop name(floatOop x, floatOop y) {                                          \
    CHECK_XY;                                                                 \
    return as_floatOop(x->value() op y->value());                             \
  }

FLOAT_PRIM(float_add_prim, +)
FLOAT_PRIM(float_sub_prim, -)
FLOAT_PRIM(float_mul_prim, *)
      
oop float_div_prim(floatOop x, floatOop y) {
  CHECK_XY;
  float xv = x->value();
  float yv = y->value();
  if (yv == 0)
    return ErrorCodes::vmString_prim_error(DIVISIONBYZEROERROR);

  float quo = xv / yv;
  return as_floatOop(quo);
}

oop float_mod_prim(floatOop x, floatOop y) {
  CHECK_XY;
  const float xv = x->value();
  const float yv = y->value();
  if (yv == 0) {
    return ErrorCodes::vmString_prim_error(DIVISIONBYZEROERROR);
  }
  double res= fmod(xv, yv);
  return as_floatOop(res);
}

oop as_int_prim(floatOop x) {
  if (!x->is_float()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  float xv = x->value();
  if (xv < smiOop_min->value() || xv > smiOop_max->value()) {
    return ErrorCodes::vmString_prim_error(OVERFLOWERROR);
  } else {
    return as_smiOop((smi)rint(xv));
  }
}

oop as_float_prim(smiOop x) {
  if (!x->is_smi()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_floatOop(x->as_float_prim());
}

inline float float_trunc(float x) {
  return x > 0 ? floor(x) : -(floor(-x));
}

# define FLOAT_FUN_PRIM(name, fun)                                            \
  oop name(floatOop x) {                                                      \
    if (!x->is_float()) return ErrorCodes::vmString_prim_error(BADTYPEERROR); \
    return as_floatOop(fun(x->value()));                                      \
  }

FLOAT_FUN_PRIM(float_floor_prim, floor)
FLOAT_FUN_PRIM(float_round_prim, rint)
FLOAT_FUN_PRIM(float_ceil_prim,  ceil)
FLOAT_FUN_PRIM(float_truncate_prim, float_trunc)


# define FLOAT_CMP_PRIM(name, op)                                             \
  oop name(floatOop x, floatOop y) {                                          \
    CHECK_XY;                                                                 \
    return (x->value() op y->value()) ? Memory->trueObj : Memory->falseObj;   \
  }

FLOAT_CMP_PRIM(float_lt_prim, <)
FLOAT_CMP_PRIM(float_le_prim, <=)
FLOAT_CMP_PRIM(float_eq_prim, ==)
FLOAT_CMP_PRIM(float_ne_prim, !=)
FLOAT_CMP_PRIM(float_gt_prim, >)
FLOAT_CMP_PRIM(float_ge_prim, >=)



# define MaxFloatString 20

void floatOopClass::make_print_string(char* buf, const char* format) {
  sprintf(buf, format, value());
  char c = buf[0];
  if ((c >= '0' && c <= '9') || (c == '-' && buf[1] >= '0' && buf[1] <= '9')) {
    char *p;
    for (p = &buf[0]; c= *p, c != '\0' ; p++)
      if (c == '.' || c == 'e' || c == 'E')
        return;
    *p++ = '.';
    *p++ = '0';
    *p = '\0';
    assert(p < &buf[MaxFloatString], "too many digits");
  }
}

void floatOopClass::print_string(char* buf) {
  make_print_string(buf);
}

void floatOopClass::print_oop() {
  char buf[MaxFloatString];
  make_print_string(buf);
  lprintf("%s", buf);
}

char *floatOopClass::print_string_prim() {
  static char buf[MaxFloatString];
  make_print_string(buf);
  return buf;
}

char *floatOopClass::print_string_precision_prim(smi precision) {
  char format[15];
  sprintf(format, "%%.%df", precision);
  static char buf[MaxFloatString];
  sprintf(buf, format, value());        // don't append .0 if 0 precision
  return buf;
}
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mapMap.hh"
# include "_mapMap.cpp.incl"

Map* mapMap::create_mapMap() {
  oop ignored;
  mapMap m1;
  Map* m = create_map(sizeof(mapMap), NULL, &m1, &ignored);
  m->enclosing_mapOop()->set_map(m); // mapMap is its own map
  return m;
}

oop mapMap::clone(oop obj, bool mustAllocate, oop genObj) {
  Unused(mustAllocate); Unused(genObj);
  ShouldNotCallThis(); // cannot clone a mapOop
  return obj;
}

oop mapMap::cloneSize(oop obj, fint length, bool mustAllocate, oop filler) {
  Unused(length); Unused(mustAllocate); Unused(filler);
  ShouldNotCallThis(); // cannot clone a mapOop
  return obj;
}

oop mapMap::scavenge(oop obj) {
  // use :: to avoid another virtual function call
  return mapOop(obj)->scavenge(mapMap::object_size(obj)); 
}


bool mapMap::verify(oop obj) {
  if (!oopsOop(obj)->verify())
    return false;
  bool isOK = true;
  Map* m = mapOop(obj)->map_addr();
  
  if (!Vtbls->contains(m->vtbl_value()) )  {
    lprintf("Vtable of map %#lx is not registered in Vtbls\n", m);
    isOK = false;
  }
  if (m->length_slots() < 0) {
    error1("map 0x%lx has a weird slot length", obj);
    isOK = false;
  }
  slotDesc *prev= NULL;
  fint offset= m->is_slots() ? m->empty_object_size() : 0;
  FOR_EACH_SLOTDESC(m, slot) {
    if (! slot->verify(m)) {
      lprintf("\tof map 0x%lx\n", obj);
      isOK = false;
    }
    if (slot->is_obj_slot()) {
      if (smiOop(slot->data)->value() != offset) {
        lprintf("error in offset of obj slot in map %#lx\n", (long unsigned)m);
        isOK= false;
      }
      ++offset;
    }
    if (prev && prev->name->cmp(slot->name) >= 0) {
      lprintf("slot order error in map %#lx\n", m);
      isOK= false;
    }
    prev= slot;
  }
  if (m->can_have_dependents()) {
    ((slotsMapDeps*)m)->verify_dependents();
  }
  /*  disable this; only check maps actually in objects
      Memory->map_table->verify_map(m);
   */
  return isOK;
}


void mapMap::print(oop obj) {
  Map* m = mapOop(obj)->map_addr();
  if (m == Memory->map_map) {
    lprintf("map map\n");
  } else {
    m->print_map();
  }
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "processMap.hh"
# include "_processMap.cpp.incl"

processOop processMap::create_process(Process* process) {
  slotList* slots = new slotList(VMString[PARENT],
                                 parent_map_slotType,
                                 create_slots((slotList*)NULL));
  processMap mm;
  processOop mr;
  (void)create_map(sizeof(processMap), slots, &mm, (oop*)&mr);
  mm.dummy_initialize(mr, smiOop_zero); // for verify
  mr = processOop(mr->clone()); // clone for processOops inits them
  mr->set_process(process);
  return mr;
}

oop processMap::clone(oop obj, bool mustAllocate, oop genObj) {
  processOop p= processOop(slotsMapDeps::clone(obj, mustAllocate, genObj));
  vframeOop m= Memory->outerActivationObj->basic_clone(mustAllocate, genObj);
  if (oop(p) != failedAllocationOop  &&  oop(m) != failedAllocationOop) {
    assert(!m->is_live(), "shouldn't be live");
    assert(m->next() == NULL, "should be empty");
    Memory->store((oop*)&p->addr()->_vframeList, m);
    p->set_return_oop(smiOop_zero);
  }
  return p;
}

oop processMap::cloneSize(oop obj, fint length, bool mustAllocate,
                          oop filler) {
  return slotsMapDeps::cloneSize(obj, length, mustAllocate, filler); }

fint processMap::empty_object_size() {
  return sizeof(processOopClass) / oopSize; 
}

void processMap::kill(oop p)    { processOop(p)->kill_process(); }
bool processMap::is_live(oop p) { return processOop(p)->is_live_process(); }


// If process is alive, so are stacks etc.
void processMap::gc_mark_contents(oop p) {
  processOop procObj= processOop(p);
  assert(procObj->is_forwarded(), "otherwise how did we get here?");
  Process *proc= procObj->process();
  if (proc == NULL // not a live process
      || proc == currentProcess || proc == twainsProcess) // already done
    return;
  if (proc->processObj() == procObj) // not a clone
    proc->gc_mark_contents();
}

void processMap::print(oop obj) {
  lprintf("process ");
  if (WizardMode && !obj->is_map())
    lprintf("%#lx ", processOop(obj)->process());
  slotsMapDeps::print(obj);
}

void processMap::switch_pointer(oop obj, oop* where, oop to) {
  assert_process(obj, "obj must be process");
  processOop(obj)->switch_return_oop(where, to);
  slotsMapDeps::switch_pointer(obj, where, to);
}

void processMap::dummy_initialize(oop obj, oop filler) {
  assert_process(obj, "obj must be process");
  processOop(obj)->set_process(NULL);
  Memory->store((oop*)&processOop(obj)->addr()->_vframeList, filler);  
  Memory->store((oop*)&processOop(obj)->addr()->_return_oop, filler);  
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "oopsOop.hh"
# pragma implementation  "oopsOop_inline.hh"

# include "_oopsOop.cpp.incl"

oopsOop oopsOopClass::scavenge(fint size) {
  assert(Memory->should_scavenge(this) && !is_forwarded(), 
         "shouldn't be scavenging"); 
  bool is_new;
  space *copySpace= Memory->survivor_space(this, size, 0, is_new);
  oop *x= copySpace->alloc_objs_local(size);
  if (x == NULL) fatal("out of space in scavenge");
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  x == (oop*)catchThisOne) {
    warning1("oopsOopClass::scavenge caught 0x%lx", x);
  }
# endif
  copy_oops(oops(), x, size);
  oopsOop p= as_oopsOop(x);
  if (is_new) { 
    p->set_mark(p->mark()->incr_age()); 
    Memory->age_table->add(p); 
  } 
  forward_to(p); 
  return p;
}

void oopsOopClass::gc_mark_referents() {
  oop* p = oops();
  oop* end = p + size();
  for (p ++; p < end; p ++)
    MARK_TEMPLATE(p);
}

bool oopsOopClass::verify() {
  bool flag = memOopClass::verify();
  if (flag) {
    oop* p = oops() + 1;
    oop* end = oops() + size();
    if (p > end) {
      error1("oopsOop 0x%lx has incorrect size", this);
      flag = false;
    }
    for (; p < end; p ++) {
      if (! (*p)->verify_oop()) {
        lprintf("\tof oopsOop 0x%lx\n", long(this));
        flag = false;
      }
    }
  }
  return flag;
}

void oopsOopClass::record_promotion(fint size) {
  Memory->record_multistores((oop*)addr(), (oop*)addr() + size);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotType.hh"
# include "_slotType.cpp.incl"

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "vframeOop.hh"
# include "_vframeOop.cpp.incl"

// vframeOops (vfos for short) contain a pointer to the corresponding stack
// frame.  Unfortunately, we can't just store the frame* pointer bacause
// it is not constant (on the Sparc, the frame* is really the sp of the frame
// below our frame).
// Thus we store the halfFrame* of our activation (SPARC: sp) which is
// constant EXCEPT for the bottommost frame: if we switch to an uncommon
// branch, the frame size may change (and also the descOffset).
// Process::killVFrameOopsAndSetWatermark checks for this situation and 
// adjusts the values accordingly.


frame* vframeOopClass::fr() {
  frame* r = locals()->home_frame_of_vfo_locals();
  assert( r->is_self_frame(), "must be self frame");
  return r;
}

abstract_vframe* vframeOopClass::as_vframe() {
  assert(is_live()  ||  (lprintf("vframeOop 0x%x is dead\n", this), false),
         "not live");
  return new_vframe(fr(), descOffset());
}

void vframeOopClass::kill_vframe() {
  if (traceV) lprintf("*** killing vframeOop %#lx\n", this);
  LOG_EVENT2("killing vframe %#lx (locals = %#lx)", this, locals());
  set_locals(NULL); 
}

bool vframeOopClass::is_below(frame* fr) {
  return locals() < fr->vfo_locals_of_home_frame();
}

bool vframeOopClass::is_equal(frame* fr) {
  return locals() == fr->vfo_locals_of_home_frame();
}

bool vframeOopClass::is_above(frame* fr) {
  return locals() > fr->vfo_locals_of_home_frame();
}

#ifdef UNUSED
bool vframeOopClass::is_below(abstract_vframe* vf) {
  if (is_below(vf->fr)) return true;
  if (is_above(vf->fr)) return false;
  return descOffset() > vf->descOffset();
}
#endif

bool vframeOopClass::is_equal(abstract_vframe* vf){
  return is_equal(vf->fr) && descOffset() == vf->descOffset();
}

#ifdef UNUSED
bool vframeOopClass::is_above(abstract_vframe* vf){
  if (is_above(vf->fr)) return true;
  if (is_below(vf->fr)) return false;
  return descOffset() < vf->descOffset();
}
#endif
  

vframeOop vframeOopClass::new_senderVFrameOop(Process* p, vframeOop from, abstract_vframe* vf)
{
  vframeOop vfo;
  // vframeOops are canonical, so check first for existing vframeOop

  if (from->next() && from->next()->as_vframe()->EQ(vf)) { 
      return from->next();
  }

  MethodKind k = vf->method()->kind();
  switch (k) {
   case OuterMethodType:
    vfo = Memory->outerActivationObj->basic_clone(); break;
   case BlockMethodType:
    vfo = Memory->blockActivationObj->basic_clone(); break;
   default:
    fatal("unexpected kind");
  }
  vfo->set_process(p);
  vfo->set_locals(vf->fr->vfo_locals_of_home_frame());
  vfo->set_method(vf->method());
  vfo->set_descOffset(vf->descOffset());
    
  // Insert the new vframeOop after from
  vfo->insertAfter(from);
    
  if (p == currentProcess) {
    // make sure the current stack is updated
    p->killVFrameOopsAndSetWatermark(p->last_self_frame(true));
  }
  return vfo;
}

vframeOop vframeOopClass::new_vframeOop(Process* p, abstract_vframe* vf) {
  vframeOop vfo;
  // vframeOops are canonical, so check first for existing vframeOop
  vfo = p->findVFrameOop(vf);
  if (vfo) {
    if (traceV) lprintf("*** suppressing duplicate of vframeOop %#lx\n",
                       (long unsigned)vfo);
    return vfo;
  }

  MethodKind k = vf->method()->kind();
  switch (k) {
   case OuterMethodType:
    vfo = Memory->outerActivationObj->basic_clone(); break;
   case BlockMethodType:
    vfo = Memory->blockActivationObj->basic_clone(); break;
   default:
    fatal("unexpected kind");
  }
  vfo->set_process(p);
  vfo->set_locals(vf->fr->vfo_locals_of_home_frame());
  vfo->set_method(vf->method());
  vfo->set_descOffset(vf->descOffset());
  vfo = p->insertVFrameOop(vfo);
  if (p == currentProcess) {
    // make sure the current stack is updated
    p->killVFrameOopsAndSetWatermark(p->last_self_frame(true));
  }
  return vfo;
}

static int32 createdVFrameProtos = 0;

vframeOop vframeOopClass::create_vframeOop(oop method) {
  if (++createdVFrameProtos > 3)
    ShouldNotReachHere(); // should create only three vframe prototypes
  vframeMap *vf;
  switch (method->kind()) {
   case OuterMethodType: { ovframeMap m; vf= &m; break; }
   case BlockMethodType: { bvframeMap m; vf= &m; break; }
   default: ShouldNotReachHere();
  }
  assert(sizeof(vframeMap) == sizeof(bvframeMap) &&
         sizeof(vframeMap) == sizeof(ovframeMap), "should be the same");
  vframeOop p;
  vframeMap* m = (vframeMap*)create_map(sizeof(vframeMap), NULL, vf, (oop*)&p);
  p->set_next(NULL);
  p->set_method(method);
  p->set_locals(NULL);
  p->set_process(NULL);
  p->set_descOffset(0);
  return p;
}

bool vframeOopClass::equal(vframeOop v) {
  bool eq =
        method()     == v->method()
    &&  locals()     == v->locals()
    &&  descOffset() == v->descOffset();
  assert(!eq  ||  process() == v->process(), "should be the same");
  return eq;
}

bool vframeOopClass::verify() {
  bool flag = slotsOopClass::verify();
  if (flag && is_live()) {
    if (next() != NULL && !next()->is_vframe()) {
      error1("vframeOop %#lx: next isn't a vframeOop", this);
      flag = false;
    }
    if (!method()->has_code()) {
      error1("vframeOop %#lx: method doesn't have code", this);
      flag = false;
    }
    if (!oop(locals())->is_smi()) {
      error1("vframeOop %#lx: locals ptr isn't a smi", this);
      flag = false;
    }
    frame* f = fr();
    if (!oop(f)->is_smi()) {
      error1("vframeOop %#lx: frame ptr isn't a smi", this);
      flag = false;
    }
    if (!descOffset()->is_smi()) {
      error1("vframeOop %#lx: desc isn't a smi", this);
      flag = false;
    }
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      if (f->is_compiled_self_frame()) {
        nmethod* nm = f->code();
        ScopeDesc* d = nm->scopes->at(descOffset());
        if (!nm->scopes->includes(d)) {
          error1("vframeOop %#lx: desc isn't a valid scope offset", this);
          flag = false;
        }
      }
#   endif
  }
  return flag;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "proxyOop.hh"
#include "_proxyOop.cpp.incl"

// Magic number, stored in proxyOop's cObject field, when it is killed.
// safer to make it smiOop -- dmu
const void *deadProxyObject = (void *)0x109b1500; 


bool proxyOopClass::verify() {
  return foreignOopClass::verify() && addr()->type_seal.verify();
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "markOop.hh"
# include "_markOop.cpp.incl"

void markOopClass::print() {
  lprintf("Mark#0x%lx: hash: %ld; age: %ld",
         this,  hash(),  age());
  
  if (is_objectMarked()) lprintf("; object_is_marked");
  lprintf("\n");
}

int32 assign_hash(markOop& m) {
  m = m->set_hash(currentProcess->current_hash++);
  return m->hash();
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "smiOop.hh"
# include "_smiOop.cpp.incl"

oop smiOopClass::as_object_prim() {
  int32 id = value();
  if (id < CurrentObjectID - NumObjectIDs ||
      id >= CurrentObjectID ||
      id < 0)
    return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
  return Memory->objectIDArray->obj_at(id % NumObjectIDs);
}

oop smiOopClass::address_as_oop_prim() {
  oop p = oop(value());
  if (p->is_mark() || 
      (p->is_mem() && !Memory->really_contains(p)) ||
      !p->verify_oop())
    return ErrorCodes::vmString_prim_error(PRIMITIVEFAILEDERROR);
  return p;
}

oop smi_div_prim(smiOop rcvr,  smiOop arg) {
  if (!rcvr->is_smi())   return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (! arg->is_smi())   return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi r = rcvr->value();
  smi a = arg->value();
  if ((r == smiOop_min->value()) &&  a == -1) return ErrorCodes::vmString_prim_error(OVERFLOWERROR);
  if (a == 0)                                 return ErrorCodes::vmString_prim_error(DIVISIONBYZEROERROR);
  smi quo = (r / a);
  assert( abs(quo) == abs(r) / abs(a), "smi_div_prim is wrong on this platform");
  return as_smiOop(quo);
}

oop smi_mod_prim(smiOop rcvr,  smiOop arg) {
  if (!rcvr->is_smi())   return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (! arg->is_smi())   return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  smi r = rcvr->value();
  smi a = arg->value();
  smi aa = a >= 0  ?  a  :  -a;
  if (a == 0) return ErrorCodes::vmString_prim_error(DIVISIONBYZEROERROR);
  int32 res = r % a;
  assert( smiOop(smi_div_prim(rcvr, arg))->value() * a   +  res  ==  r,
          "smi_mod_prim incorrect on this platform");
  return as_smiOop(res);
}

# define relational(name, op)                                                 \
  oop name(oop rcvr, oop arg) {                                               \
    if (rcvr->is_smi() && arg->is_smi()) {                                    \
      return ((smi)rcvr op (smi)arg) ? Memory->trueObj : Memory->falseObj;    \
    } else {                                                                  \
      return ErrorCodes::vmString_prim_error(BADTYPEERROR);                 \
    }                                                                         \
  }                                                                           \

relational(smi_lt_prim, <)
relational(smi_le_prim, <=)
relational(smi_eq_prim, ==)
relational(smi_ne_prim, !=)
relational(smi_ge_prim, >=)
relational(smi_gt_prim, >)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "blockMap.hh"
# include "_blockMap.cpp.incl"

void blockMap::init(stringOop name, slotsOop method) {
  Memory->store((oop*)&valueMethodName, oop(name));
  Memory->store((oop*)&valueMethod, oop(method));
}

blockOop blockMap::create_block(slotsOop meth) {
  stringOop name;
  fint arg_count = meth->map()->arg_count();
  switch (arg_count) {
   case 0: name= VMString[VALUE]; break;
   case 1: name= VMString[VALUE_]; break;
   case 2: name= VMString[VALUE_WITH_]; break;
   case 3: name= VMString[VALUE_WITH_WITH_]; break;
   case 4: name= VMString[VALUE_WITH_WITH_WITH_]; break;
   default:
    char* slot =  NEW_RESOURCE_ARRAY( char, arg_count * 5 + 2);
    char* s = slot;
    strncpy(s, "value:With:With:With:With:", 26);
    for (s += 26, arg_count -= 5; arg_count > 0; arg_count --, s += 5) {
      strncpy(s, "With:", 5);
    }
    *s = '\0';
    name= new_string(slot);
  }
  
  blockMap m1;
  oop ignored;
  blockMap* b= (blockMap*)create_map(sizeof(blockMap), NULL, &m1, &ignored);
  b->setDesc(BLOCK_PROTO_DESC);
  b->init(name, meth);
  blockOop block= (blockOop)create_slots(b->empty_object_size());
  slotsOop(block)->set_map(b);
  ((blockOopClass*)block->addr())->setHomeFr(BLOCK_PROTO_SCOPE);
  return block;
}


slotDesc* blockMap::find_slot(stringOop name) {
  if (name == valueMethodName) {
    setSlots();
    return &block_slots[1];
  }
  if (name == VMString[PARENT])
    return &block_slots[0];
  return NULL;
}
    

void blockMap::switch_pointer_in_map(oop* where,  oop to) {
  if (where == (oop*)&valueMethod) {
    Memory->store((oop*)&valueMethod, to);
    return;
  }
}


void blockMap::kill(oop p) {
  blockOop(p)->kill_block();
}
 
bool blockMap::is_live(oop p) {
  return blockOop(p)->is_live_block();
}

oop blockMap::scavenge(oop obj) {
  // use :: to avoid another virtual function call
  return slotsOop(obj)->scavenge(blockMap::object_size(obj)); 
}

// BUG: verifying block homes is optional because the current system doesn't
// always kill blocks correctly.  Known bugs are:
// - dead blocks are returned by vframe::get_contents if the block was
//   completely optimized away (BlockValueDesc)  (all compilers except
//   NIC which doesn't have such blocks)
// - recompilation & on-stack replacement may leave unkilled blocks
//   (should only happen for "primFailBlock" blocks)
// Urs 4/93

static bool VerifyBlockHomes = false;

bool blockMap::verify(oop obj) {
  bool flag= slotsOop(obj)->verify();
  if (flag) {
    frame* bs = blockOop(obj)->scope(true);
    if (! oop(bs)->is_smi()) {
      error1("scope of blockOop %#lx isn't a smiOop", obj);
      flag = false;
    }
    // verify scope bs
    if (flag && bs && VerifyBlockHomes) {
      Stack* stk = processes->stackFor(bs);
      if (stk == NULL) {
        error1("scope of blockOop %#lx isn't in any stack", obj);
        flag = false;
      } else {
        frame* f = stk->last_self_frame(false);
        while (f && f != bs)  f = f->selfSender();
        if (f == NULL) {
          error1("scope of blockOop %#lx isn't a valid frame pointer", obj);
          flag = false;
        }
      }
    }
    if (! blockOop(obj)->desc()->is_smi()) {
      error1("desc of blockOop %#lx isn't a smiOop", obj);
      flag = false;
    }
  }
  return flag;
}

void blockMap::print(oop obj) {
  if (obj->is_map()) {
    blockMap *m= (blockMap*)(mapOop(obj)->map_addr());
    lprintf("block map (desc=%d, ", m->desc()->value());
    m->valueMethodName->print();
    lprintf(" method %#lx\n", (unsigned long)(m->valueMethod));
    return;
  }
  printObjectID(obj);
  lprintf(": ");
  Map::print(obj);
}


oop blockMap::mirror_parent(oop obj) {
  ResourceMark rm;
  frame* scope = blockOop(obj)->scope(true);
  if (scope) {
    abstract_vframe* vf = new_vframe(
      scope->home_frame_of_block_scope(),
      blockOop(obj)->desc());
    Process* p = processes->stackFor(scope)->process;
    return new_vframeOop(p, vf)->as_mirror();
  } else {
    return ErrorCodes::vmString_prim_error(NOPARENTERROR);
  }
}

void blockMap::dummy_initialize(oop obj, oop filler) {
  Unused(filler);
  assert( obj->is_block(), "a blockOop is expected");
  kill(obj);
}

oop blockMap::dummy_obj(oop filler) {
  slotsOop obj= create_slots(empty_object_size());
  obj->set_map(this);
  dummy_initialize(obj, filler);
  return obj;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "foreignOop.hh"
# include "_foreignOop.cpp.incl"


const smi  C_pointer::shifts   = sizeof(void *) * BitsPerByte / 2;
const long C_pointer::low_mask = (1L << shifts) - 1;

bool C_pointer::verify() {
  if (!hi->is_smi())
    error1("C_pointer %#lx: hi isn't a smi", this);
  if (!hi->is_smi())
    error1("C_pointer %#lx: hi isn't a smi", this);
  return hi->is_smi() && lo->is_smi();
}
    

bool foreignOopClass::verify() {
    return slotsOopClass::verify() && addr()->cObject.verify();
}
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "map.hh"

# include "_map.cpp.incl" 

Map* Map::create_map(fint size, slotList *slots, Map* mm, oop *obj) {
  fint slotCount= slots ? slots->length() : 0;
  mapOop mo = as_mapOop(
    Memory->alloc_objs(   enclosed_map_offset + (size + slotCount * sizeof(slotDesc))/oopSize)  );
  mo->init_mark();
  Memory->store((oop*)&mo->addr()->_map, Memory->map_map->enclosing_mapOop());
  
  Map* m= mo->map_addr();
  m->set_vtbl_value(mm->vtbl_value());
  m->initialize();
  m->set_annotation(Memory->objectAnnotationObj);
  if (m->is_slots() || slotCount > 0)
    *obj= m->fill_in_slots(slots, slotCount);
  return m;
}


FindSlotCache findSlotCache;

inline bool FindSlotCache::checkFor1ArgKW(stringOop name, stringOop &unary) {
  if (name == last1ArgKW) {
    unary= lastUnary;
    return true;
  }
  if (name->is_1arg_keyword()) {
    last1ArgKW= name;
    lastUnary= unary= name->unary();
    return true;
  }
  return false;
}

bool FindSlotCache::verify() {
  if (last1ArgKW == stringOop(badOop) && lastUnary == stringOop(badOop))
    return true;
  bool flag= last1ArgKW->verify() && lastUnary->verify();
  if (!last1ArgKW->is_1arg_keyword() || !lastUnary->is_unary()) {
    error("bad findSlotCache");
    flag= false;
  }
  return flag;
}
  

// we can merge search for foo and foo: because stringOop::cmp
// orders foo: before foo0
fint Map::find_slot_index_binary_for(stringOop name, bool &found) {
  stringOop shortName;
  if (!findSlotCache.checkFor1ArgKW(name, shortName)) shortName= name;
  slotDesc *s= slots();
  fint low= 0;
  fint high= length_slots() - 1;
  while (low <= high) {
    fint i= (low + high) / 2;
    stringOop sin= s[i].name;
    if (sin == name
    ||  (sin == shortName && s[i].is_obj_slot())) {
      found= true;
      return i;
    }
    if (sin->cmp(name) < 0) {
      low= i + 1; 
    } else {
      high= i - 1;
    }
  }
  found= false;
  return low;
}

inline slotDesc* Map::find_slot_binary(stringOop name) {
  bool found;
  fint i= find_slot_index_binary_for(name, found);
  return found ? slot(i) : NULL;
}

inline slotDesc* Map::find_slot_linear(stringOop name) {
  stringOop shortName;
  if (findSlotCache.checkFor1ArgKW(name, shortName)) {
    FOR_EACH_SLOTDESC(this, slot) {
      if (slot->name == name
          || (slot->is_obj_slot() && slot->name == shortName))
        return slot;
    }
  } else {
    FOR_EACH_SLOTDESC(this, slot) {
      if (slot->name == name)
  return slot;
    }
  }
  return NULL;
}


fint Map::find_slot_index_linear_for(stringOop name, bool &found) {
  stringOop shortName;
  if (findSlotCache.checkFor1ArgKW(name, shortName)) {
    FOR_EACH_SLOTDESC_N(this, slot, i) {
      if (slot->name == name
          || (slot->is_obj_slot() && slot->name == shortName)) {
        found= true;
        return i;
      }
      if (slot->name->cmp(name) > 0) {
        found= false;
        return i;
      }
    }
  } else {
    FOR_EACH_SLOTDESC_N(this, slot, i) {
      if (slot->name == name) {
        found= true;
        return i;
      }
      if (slot->name->cmp(name) > 0) {
        found= false;
        return i;
      }
    }
  }
  found= false;
  return length_slots();
}


static fint big_map_thresh; // set in findSlot_init, below

fint Map::find_slot_index_for(stringOop name, bool &found) {
  if (length_slots() > big_map_thresh)
    return find_slot_index_binary_for(name, found);
  else
    return find_slot_index_linear_for(name, found);
}


slotDesc* Map::find_slot(stringOop name) {
  if (length_slots() > big_map_thresh)
    return find_slot_binary(name);
  else
    return find_slot_linear(name);
}


slotDesc* Map::find_nonVM_slot(stringOop name) {
  slotDesc* sd = find_slot(name);
  return  sd != NULL  &&  !sd->is_vm_slot()  ?  sd  :  NULL;
}


// given "foo:", find "foo" (return NULL if not found)
slotDesc* Map::find_assignee_slot(stringOop assigner_name) {
  return find_slot(new_string(assigner_name->bytes(),
                              assigner_name->length() - 1));
}



// Allow fast defines of methods when the generated code does not change.
// This check must be recursive because the literals 
// (i.e. the actual literal vectors) won't be the same objects because
// of the backpointer that must be embedded from the vector to the method.

static bool recursive_method_similarity_check(oop o1, oop o2) {
  if (!FastMethodDefines) return false;
  
  if (o1 == o2) return true;
  if (o1->is_block()  &&  o2->is_block())
    return recursive_method_similarity_check(
                            blockOop(o1)->value(),
                            blockOop(o2)->value());

  if (!o1->has_code()  ||  !o2->has_code())
    return false;
  
  if ( o1->kind()  != o2->kind() )
    return false;

  if (o1->codes() != o2->codes()) return false;

  // arg count must be same
  if (o1->arg_count() != o2->arg_count()) return false;


  // slots must be same
  Map *m1= o1->map();
  Map *m2= o2->map();
  fint nSlots = m1->length_slots();
  if (m2->length_slots() != nSlots) return false;

  SlotIterator *it= m1->slotIterator();
  FOR_EACH_SLOTDESC(m2, s2) {
    slotDesc *s1= it->slot_desc();

    if (    s1->name != s2->name
        ||  s1->type != s2->type
        ||  s1->data != s2->data) {
      if (s1->is_vm_slot())
        assert(   s1->name == VMString[SELF]
               || s1->name == VMString[LEXICAL_PARENT],
               "Found a new VM slot in a method that I may not be able to ignore");
      else
        return false;
    }

    it->next();
  }

  objVectorOop l1 = o1->literals();
  objVectorOop l2 = o2->literals();
  if ( l1->length() != l2->length() ) return false;
  for (fint i = 0;  i < l1->length();  i++)
    if ( ! recursive_method_similarity_check( l1->obj_at(i), l2->obj_at(i)) )
      return false;

  return true;
}
    

// The cloning semantics allow the optimization included here.
// I am restricting this to slotsOops to keep it simple.
// Otherwise would have to worry about changing smallInts such as
//  slot offsets, etc.
// The problem is that 1. could not keep the hash, and 2. the zone code
//  is not smart enough to change slotsOops to smiOops.
// If the hash were not propagated, the maps would need to be
//  recanonicalized.
// Mirrors will do a recursive define so that they can change their
//  maps, etc when the reflectee changes type.
//  -- dmu 1/93
// Can return failedAllocationOop if out of space due to copy of large object

oop Map::define(oop obj, oop contents) {
  if (   !obj     ->is_programmable_slots() 
      || !contents->is_programmable_slots()) {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }

  // cannot change a non-method to a method cause the non-method
  //  might be in an obj_slot instead of a map_slot
  // I think this is OK for NakedMethods -- dmu 3/02
  if (!NakedMethods && !obj->is_method_like() && contents->is_method_like()) {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }

  slotsOop sobj = slotsOop(obj);
  slotsOop scon = slotsOop(contents);

  slotsOop result;
  fint sobjSize = sobj->size();

  // assume as_mirror() will not fail
  if (    scon->size() == sobjSize
      && !scon->is_byteVector()  
      && !sobj->is_byteVector()
      && (
               // contents not method, mirror check will ensure recvr
               //  not method either, both not methods is OK
               !scon->is_method_like()
          ||
                recursive_method_similarity_check(sobj, scon)
         )        
      // cannot do fast one if mirrors need to change; must visit mirrors 
      &&  sobj->as_mirror()->map() == scon->as_mirror()->map()) { 
    // fast define: just overwrite lhs with right (excluding mark)
    oop* sobjOops = sobj->oops();
    // don't overwrite mark word (don't change hash/age of object)
    copy_oops(scon->oops() + 1, sobjOops + 1, sobjSize - 1);
    Memory->record_multistores(sobjOops + 1, sobjOops + sobjSize);
    result = sobj;
  } else {
    // slow define: switch pointers from lhs to a clone of the rhs            

    BlockProfilerTicks bpt(include_type);

    result = slotsOop(scon->clone_into_same_gen(sobj, CANFAIL));
    if (oop(result) == failedAllocationOop) return failedAllocationOop;
    assert(result != scon, // see switch_pointer_in_map (dmu)
           "switch_pointers needs a new object to avoid creating redundant maps");

    // copy over hash to new object
    // used to be:
    //  int32 hash = sobj->mark()->hash_in_place();
    //  result->set_mark(result->mark()->set_hash_in_place(hash));
    // which avoided assigning a hash just for this.
    // However, I think it is cleaner, to just use the external interface.
    // That way the callee can simply check for the same identity_hash
    //  on old & new objects.
    // It has to check so that switching pointers in a map will not change
    //  the map's hash in the canonicalization table (mapTable) -- dmu 11/93.

    result->set_identity_hash(sobj->identity_hash());

    Memory->switch_pointers(sobj, result);
  }
  // too late to fail
  result = (slotsOop)result->fix_up_method( contents, false, !CANFAIL);
  assert_slots(result, "just checking");

  return result;
}


// Add all slots of contents to the receiver.  
// If new_only, then add slot only if it doesn't already exist.

oop Map::add_slots_to(oop src, oop dst, bool new_only, void *FH) {
  assert(src->map() == this, "oops!");

  if (    dst->is_vframe()
      ||  dst->is_method_like()
      || !dst->is_programmable_slots()) {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }

  oop result = dst;

  {
    FOR_EACH_SLOTDESC(this, s) {
      slotDesc *old;
      oop result2;
      if (new_only
          && (old= dst->find_slot(s->name), old != NULL)) {
        if (s->is_obj_slot() && !old->is_obj_slot()) {
          // must add assignment slot
          assert_slots(result, "can only add slots to a slotsOop");
          result2= slotsOop(result)->
            copy_add_slot_fixing_up_method( s->assignment_slot_name(),
                                            MAP_SLOT(s->type),
                                            Memory->assignmentObj,
                                            old->annotation);
        } else {
          continue; // do nothing
        }
      } else {
        assert_slots(result, "can only add slots to a slotsOop");
        result2= slotsOop(result)->
          copy_add_slot_fixing_up_method( s->name,
                                          s->type,
                                          src->get_slot(s),
                                          s->get_annotation());
      }
      if (result2 == failedAllocationOop) {
        out_of_memory_failure(FH, result->size() + 1);
        return NULL;
      }
      result= result2;
      if (result->is_mark()) return result;
    }
  }

  // Update the annotation of the dst object.
  oop anno = get_annotation();
  if (anno != Memory->objectAnnotationObj) {
    oop result2= result->mirror_copy_set_annotation(anno);
    if (result2 == failedAllocationOop) {
      out_of_memory_failure(FH, result->size());
      return NULL;
    }
    result = result2;
  }
  oop r = dst->define_prim(result, FH);
  return r;
}

     
oop Map::copy_add_slot(oop obj, stringOop name, slotType t, oop contents,
                       oop anno,
                       bool mustAllocate) {
  Unused(obj); Unused(name); Unused(t); Unused(contents);
  Unused(anno);
  Unused(mustAllocate);
  return ErrorCodes::vmString_prim_error(BADTYPEERROR);
}


oop Map::copy_remove_slot(oop obj, stringOop name, bool mustAllocate) {
  Unused(obj); Unused(name); Unused(mustAllocate);
  return ErrorCodes::vmString_prim_error(BADTYPEERROR);
}


// Called when Define does a switch_pointers and it finds a match in map.
//  It could be something we should not change, so only change it
//  if it is contained in a slot -- dmu 1/93
void Map::switch_pointer_in_map(oop* where,  oop to) {
  fint i= ((char*)where - (char*)slots()) / sizeof(slotDesc);

  if (i < 0 || i >= length_slots()) return;

  slotDesc *s= slot(i);

  if (&s->annotation == where) {
    s->set_annotation(to);
  }
  else if (&s->data == where  &&  s->is_map_slot()  && !s->is_vm_slot()) {
    switch_pointer_in_map_slot(s, where, to);
  }
}

void Map::switch_pointer_in_map_slot(slotDesc *s, oop *where, oop to) {
  Unused(s);
  Memory->store(where, to);
}


oop Map::fix_up_method( oop obj, 
                        oop old_optimized_method, 
                        bool isOKToBashLiteralVector,
                        bool mustAllocate,
      IntBList* stack_deltas ) {
  Unused(mustAllocate); 
  Unused(isOKToBashLiteralVector);
  Unused(old_optimized_method);
  Unused(stack_deltas);
  // except for methods, is noop
  return obj;
}

fint Map::length_nonVM_slots() {
  fint i = 0;
  FOR_EACH_SLOTDESC(this, slot) {
    if (!slot->is_vm_slot())
      i += slot->is_obj_slot() ? 2 : 1;
  }
  return i;
}

fint Map::length_obj_slots() {
  fint slotCount = 0;
  FOR_EACH_SLOTDESC(this, s) {
    if (s->is_obj_slot()) slotCount++;
  }
  return slotCount;
}


oop Map::cloneSize(oop obj, fint length, bool mustAllocate, oop filler) {
  Unused(length); Unused(mustAllocate); Unused(filler);
  ShouldNotCallThis(); // object isn't a vector
  return obj;
}

bool Map::verifyBytesPart(oop obj, char*& b) {
  // nothing more to do
  Unused(obj); Unused(b);
  return true;
}

bool Map::compare(Map* m) {
  // INPUT: another map object, its slots and its length
  // OUTPUT: true if this map is equivalent to m, else false

  // make sure it's not mapMap
  assert(!m->is_map(), "map can't be mapMap");

  // check if they are the same type
  if (m->vtbl_value() != vtbl_value()) return false;
      
  // check if they have the same number of slots
  fint length = length_slots();
  if (m->length_slots() != length) return false;

  // check object annotations
  if (m->get_annotation() != get_annotation()) return false;
          
  // check slots are equivalent.  
  slotDesc *m_slot= m->slots();
  FOR_EACH_SLOTDESC(this, slot) {
    if (slot->name != m_slot->name) return false;
    if (!m_slot->compare(slot)) return false;
    ++m_slot;
  }
  
  // check if one has code and the other doesn't
  if (has_code()) {
    assert(m->has_code(), "one compared map has code, other doesn't");
    return false; // code comparison not implemented
  } else {
    assert(!m->has_code(), "one compared map has code, other doesn't");
  }

  // if got this far, then equivalent
  return true;
}

bool Map::equal(Map* other) {
  return enclosing_mapOop()->equal(other->enclosing_mapOop());
}

int32 getObjectID(oop obj) {
  int32 i = CurrentObjectID % NumObjectIDs;
  if (i) i--; else i = NumObjectIDs - 1;
  int32 id = CurrentObjectID - 1;
  int32 m = max(0, CurrentObjectID - NumObjectIDs);
  while (id >= m && Memory->objectIDArray->obj_at(i) != obj) {
    id--;
    if (--i < 0) i += NumObjectIDs;
  }
  if (id >= m) {
    assert(Memory->objectIDArray->obj_at(i) == obj, "not found");
  } else {
    Memory->objectIDArray->obj_at_put(CurrentObjectID % NumObjectIDs, obj);
    id = CurrentObjectID++;
  }
  return id;
}

void printObjectID(oop obj) {
  int32 id = bootstrapping ? -1 : getObjectID(obj);
  lprintf("<%ld", (void*)id);
  if (PrintOopAddress) {
    lprintf(" (0x%lx)", obj);
  }
  lprintf(">");
}

void SetNumObjectIDs(int32 length) {
  if (length > 1) {
    objVectorOop newArray= Memory->objectIDArray->cloneSize(length, CANFAIL);
    // make sure old objects are released
    for (int32 i = 0;  i < length;  ++i)
      newArray->obj_at_put(i, as_smiOop(0));
    if (oop(newArray) != failedAllocationOop) {
      Memory->objectIDArray= newArray;
      NumObjectIDs = length;
    }
  }
}

void Map::print_string(oop obj, char* buf) {
  if (obj == (oop) Memory->trueObj) {
    sprintf(buf, "true");
  } 
  else if (obj == (oop) Memory->falseObj) {
    sprintf(buf, "false");
  } 
  else if (obj == (oop) Memory->nilObj) {
    sprintf(buf, "nil");
  } 
  else if (obj == (oop) Memory->lobbyObj) {
    sprintf(buf, "lobby");
  } 
  else if (obj == (oop) Memory->errorObj) {
    sprintf(buf, "<!error object!>");
  } 
  else {
    sprintf(buf, "<%d>", obj->objectID_prim());
  }
}

void Map::print_oop(oop obj) {
  if (obj == (oop) Memory->trueObj) {
    lprintf("true");
    if (PrintOopAddress) {
      lprintf(" (0x%lx)", obj);
    }
  } else if (obj == (oop) Memory->falseObj) {
    lprintf("false");
    if (PrintOopAddress) {
      lprintf(" (0x%lx)", obj);
    }
  } else if (obj == (oop) Memory->nilObj) {
    lprintf("nil");
    if (PrintOopAddress) {
      lprintf(" (0x%lx)", obj);
    }
  } else if (obj == (oop) Memory->lobbyObj) {
    lprintf("lobby");
    if (PrintOopAddress) {
      lprintf(" (0x%lx)", obj);
    }
  } else if (obj == (oop) Memory->errorObj) {
    lprintf("<!error object!>");
    if (PrintOopAddress) {
      lprintf(" (0x%lx)", obj);
    }
  } else {
    printObjectID(obj);
  }
}

void Map::print(oop obj) {
  if (obj->is_map()) {
    lprintf("map ");
  }
  lprintf("( ");
  fint length = length_slots();
  if (length || obj->is_block()) {
    lprintf("| ");
    FOR_EACH_SLOTDESC(this, slot) {
      if (slot->is_vm_slot()) {
        if (! WizardMode) continue;
        lprintf("{");
      }
      slot->printAugmentedName();
      if (slot->is_arg_slot()) {
        lprintf(" = <arg %ld>", smiOop(slot->data)->value());
      } else if (slot->is_map_slot()) {
        lprintf(" = ");
        slot->data->print_oop();
      } else {
        assert(slot->is_obj_slot(), "unexpected slot type");
        if (obj->is_map()) {
          // just printing a map; there isn't an object to print
          lprintf(" = <offset %ld>", slot->data);
        } else {
          // printing a real object; fetch its slot
          lprintf(" <- ");
          obj->get_slot(slot)->print_oop();
        }
      }
      if (slot->is_vm_slot()) lprintf("}");
      lprintf(". ");
    }
    lprintf("| ");
  }
  print_objVector(obj);
  print_byteVector(obj);
  print_code(obj);
  lprintf(")\n");
}

bool Map::has_assignment_slots() {
  FOR_EACH_SLOTDESC(this, s) {
    if (!s->is_vm_slot() && s->is_obj_slot())
      return true;
  }
  return false;
}

bool Map::matching_slots_data(oop match) {
  FOR_EACH_SLOTDESC(this, s) {
    if (!s->is_vm_slot() && s->is_map_slot() && s->data == match)
      return true;
  }
  return false;
}

// used for enumerating implementors
bool Map::matching_slots_name(oop match) {
  FOR_EACH_SLOTDESC(this, s) {
    if (!s->is_vm_slot() && s->name == match)
      return true;
  }
  return false;
}

// used for enumerating implementors
bool Map::matching_slots_assignment_name(oop match) {
  FOR_EACH_SLOTDESC(this, s) {
    if (!s->is_vm_slot() && s->name == match)
      return s->is_obj_slot();
  }
  return false;
}


# define GET_SLOT_DESC                                                        \
    slotDesc* sd = find_nonVM_slot(name);                                     \
    if (sd == NULL) return ErrorCodes::vmString_prim_error(SLOTNAMEERROR)                          

oop Map::mirror_is_parent_at(oop r, stringOop name) {
  Unused(r);
  GET_SLOT_DESC;
  return sd->is_assignment_slot_name(name) ? Memory->falseObj
    : sd->is_parent() ? Memory->trueObj : Memory->falseObj;
}

oop Map::mirror_is_assignable_at(oop r, stringOop name) {
  Unused(r);
  GET_SLOT_DESC;
  return sd->is_obj_slot() && sd->name == name
    ? Memory->trueObj : Memory->falseObj;
}

oop Map::mirror_is_argument_at(oop r, stringOop name) {
  Unused(r);
  GET_SLOT_DESC;
  return sd->is_arg_slot() ? Memory->trueObj : Memory->falseObj;
}

oop Map::mirror_names(oop ignored) {
  Unused(ignored);
  objVectorOop r= Memory->objVectorObj->cloneSize(length_nonVM_slots());
  fint i= 0;
  FOR_EACH_SLOTDESC(this, sd) {
    if (!sd->is_vm_slot()) {
      r->obj_at_put(i++, sd->name);
      if (sd->is_obj_slot())
        r->obj_at_put(i++, sd->assignment_slot_name());
    }
  }
  return r;
}


// get name of selector at index inx;
//  used to decode local access bytecodes

oop Map::mirror_name_at(oop obj, smi inx) {
  Unused(obj);
  if (inx < 0 || inx >= length_slots()) 
    return ErrorCodes::vmString_prim_error(BADINDEXERROR);
  return slot(inx)->name;
}


oop Map::mirror_contents_at(oop r, stringOop name) {
  GET_SLOT_DESC;
  if (sd->is_obj_slot() && name->is_1arg_keyword())
    return Memory->assignmentMirrorObj;
  if (sd->is_arg_slot())
    return Memory->nilObj->as_mirror();
  return r->get_slot(sd)->as_mirror();
}

oop Map::mirror_get_annotation(oop r) {
  Unused(r);
  return get_annotation();
}

oop Map::mirror_copy_set_annotation(oop r, oop a, bool mustAllocate) {
  Unused(r); Unused(a); Unused(mustAllocate);
  return ErrorCodes::vmString_prim_error(BADTYPEERROR);
}

oop Map::mirror_annotation_at(oop r, stringOop name) {
  Unused(r);
  GET_SLOT_DESC;
  return sd->get_annotation();
}


# define CODE_PRIM(name)                                                      \
    oop Map::name(oop r) { Unused(r); return ErrorCodes::vmString_prim_error(REFLECTTYPEERROR); }  \
    
CODE_PRIM(mirror_codes)
CODE_PRIM(mirror_literals)
CODE_PRIM(mirror_source)
CODE_PRIM(mirror_source_length)
CODE_PRIM(mirror_source_offset)
CODE_PRIM(mirror_file)
CODE_PRIM(mirror_line)
CODE_PRIM(mirror_sender)
CODE_PRIM(mirror_parent)
CODE_PRIM(mirror_selector)
CODE_PRIM(mirror_bci)
CODE_PRIM(mirror_receiver)
CODE_PRIM(mirror_expr_stack)
CODE_PRIM(mirror_methodHolder)

static int time_find_slot(slotDesc* (Map::*searchFn)(stringOop),
        stringOop s, Map* m) {
  ProcessInfo::update();
  int t0= ProcessInfo::user_time().milli_secs();
  for (fint i= 0; i < 100000; ++i)
    (m->*searchFn)(s); // C++ is amazing, isn't it?
  ProcessInfo::update();
  int t1= ProcessInfo::user_time().milli_secs();
  return t1 - t0;
}

static Map* build_map_of_size(fint i) {
  slotList* slist= NULL;
  char nm[3];
  nm[0]= nm[1]= 'a'; nm[2]= 0;
  fint incCh= i > 26 ? 1 : 0;
  while (i-- > 0) {
    stringOop s= new_string(nm);
    slist= slist->add(s, map_slotType, Memory->nilObj);
    nm[incCh]++;
    if (nm[incCh] > 'z') {
      nm[0]++; nm[1]= 'a';
    }
  }
  oop obj= create_slots(slist);
  return obj->map();
}

// figure out map size at which binary search is better than linear
void findSlot_init() {
  fint i= 1;
  int t_linear, t_binary;
  do {
    i *= 2;
    if (i > 26*26) break; 
    Map *m= build_map_of_size(i);
    stringOop s= m->slot(i / 2 - 1)->name;
    // preceeding ampersands needed for MetroWorks
    t_linear= time_find_slot(&Map::find_slot_linear, s, m);
    t_binary= time_find_slot(&Map::find_slot_binary, s, m);
  } while (t_linear <= t_binary);
  big_map_thresh= i / 2;
  LOG_EVENT1("setting find slot threshold to %d", big_map_thresh);
}

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotsMap.hh"
# include "_slotsMap.cpp.incl"

// Add a slot to a copy, respecting all invariants.
// Return mark on error, failedAllocationOop if ran out of space and 
// mustAllocate arg was false.

// copy_add_slot has many complex possible errors.
// That's why I have chosen to handle errors a bit differently for it.
// It may return a mark on error and its caller needs to check

// many interesting cases are here:
// The worst is when adding an assignment slot or replacing an assignment slot.
// Aside from that, the world divides into adding a brand new slot and
//  replacing an old one. When replacing an old one, if replacing a
//  variable data slot with a method, the corresponding assignment slot
//  must also go.
// When adding an assignment slot, the data must be put into the object.
// When replacing an assignment slot, the data must be put into the map.

// If type is a VM slot, assume the caller knows what he's doing.

oop slotsMap::copy_add_slot(oop obj,
                            stringOop name,
                            slotType type,
                            oop contents,
                            oop anno,
                            bool mustAllocate) {

  if (!type->is_vm_slot()) {
    if (   !obj->is_programmable_slots()  // cannot add slot to some objects
        || contents->is_vframe())         // cannot put vframe in a slot
      return ErrorCodes::vmString_prim_error(BADTYPEERROR);
    else if (type->is_parent()  &&  contents->is_method_like())
      return ErrorCodes::vmString_prim_error(PARENTERROR);
  }

  slotsOop sobj = slotsOop(obj);

  if ( type->is_arg_slot() )
    return  copy_add_argument_slot(sobj, name, type, contents, anno,
                                   mustAllocate);

  if (contents->is_assignment())
    return  copy_add_assignment_slot(sobj, name, type, anno, mustAllocate);

  if (contents->is_method_like())
    return  copy_add_method_slot(sobj, name, type, contents, anno,
                                 mustAllocate);

  return    copy_add_data_slot(  sobj, name, type, contents, anno,
                                 mustAllocate);
}


oop slotsMap::copy_add_argument_slot(slotsOop obj,
                                     stringOop name, 
                                     slotType type,
                                     oop contents,
                                     oop anno,
                                     bool mustAllocate) {
  assert_smi(contents, "arg data must be position");

  if (!name->is_unary())
    return ErrorCodes::vmString_prim_error(ARGUMENTCOUNTERROR);

  slotDesc* old = find_slot(name);
  slotsOop result;
  if (old == NULL)
    result= obj;
  else if (old->is_arg_slot()) {
    // No need to remove and reinsert because order is the same.
    // Only the annotation might be really different.
    // The index will be off by one (assumes that added slot is new)
    assert(smiOop(contents)->value() == smiOop(old->data)->value() + 1,
           "arg index wrong");
    return change_slot(obj, old, type, old->data, anno, mustAllocate);
  } else {
    result= (slotsOop)copy_remove_slot(obj, name, mustAllocate);
    if (oop(result) == failedAllocationOop || result->is_mark()) return result;
    assert(result->is_slots(), "just checking");
  }
  assert(smiOop(contents)->value() == arg_count(), "arg index wrong");
  return ((slotsMap*)result->map())->copy_add_new_slot(result, 
                                                       name, 
                                                       type, 
                                                       contents,
                                                       anno,
                                                       mustAllocate);
}


// called to add a slot whose contents is assignmentObj

oop slotsMap::copy_add_assignment_slot(slotsOop obj,
                                       stringOop name,
                                       slotType type,
                                       oop annoIgnored,
                                       bool mustAllocate) {
  Unused(annoIgnored);
  Unused(type);
                                         
  if (!name->is_1arg_keyword())
    return ErrorCodes::vmString_prim_error(ARGUMENTCOUNTERROR);

  assert(obj->is_slots() && this == obj->map(), "insecurity");

  // find the data slot

  slotDesc* ds= find_assignee_slot(name);

  if (ds == NULL)
    return ErrorCodes::vmString_prim_error(LONELYASSIGNMENTSLOTERROR);

  if (ds->is_obj_slot())
    // slot already exists -- nothing to do
    return obj->clone(mustAllocate);

  // must be a map slot
  if (!NakedMethods && ds->data->is_method_like())
    return ErrorCodes::vmString_prim_error(UNASSIGNABLESLOTERROR);

  // remove old map data slot
  slotsOop result= copy_remove_one_slot(obj, ds, mustAllocate);
  if (oop(result) == failedAllocationOop) return failedAllocationOop;
  slotsMap *new_map= (slotsMap*)result->map();

  // remove any existing method by same name
  slotDesc* old= new_map->find_slot(name);
  if (old) {
    result= new_map->copy_remove_one_slot(result, old, mustAllocate);
    if (oop(result) == failedAllocationOop) return result;
    new_map= (slotsMap*)result->map();
    assert(result->is_slots(), "just checking");
  } else {
    result= obj;
  }

  // Add in obj slot - enlarge object and put data into it from map
  result= new_map->copy_add_new_slot(result, ds->name, OBJ_SLOT(ds->type),
                                     ds->data, ds->annotation, mustAllocate);
  if (oop(result) == failedAllocationOop) return failedAllocationOop;
  assert(result->is_slots(), "should not fail");

  return result;
}


// put in vm_slot checks cause method backpointer in literals and
// parent pointer in block methods do not obey the rules

oop slotsMap::copy_add_method_slot( slotsOop obj,
                                    stringOop name, 
                                    slotType type, 
                                    oop contents,
                                    oop anno,
                                    bool mustAllocate) {

  if (type->is_vm_slot()) {
    // skip all the checks and do not try to remove it, since
    // vm slots do not change from obj to map, and rm will fetch anyway
    slotDesc* sd = obj->find_slot(name);
    return sd != NULL
     ? change_slot      (obj, sd,   type, contents, anno, mustAllocate)
     : copy_add_new_slot(obj, name, type, contents, anno, mustAllocate);
  }
  if ( contents->arg_count() != name->arg_count() )
    return ErrorCodes::vmString_prim_error(ARGUMENTCOUNTERROR);

  if (obj->is_method_like())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR); // cannot add methods to methods

  slotDesc *old= find_slot(name);
  if (old && old->is_map_slot())
    // change in situ
    return change_slot(obj, old, type, contents, anno, mustAllocate);

  slotsOop result;
  if (old) {
    // remove the old slot, then add in a new one
    result= (slotsOop)copy_remove_one_slot(obj, old, mustAllocate);
    if (oop(result) == failedAllocationOop) return failedAllocationOop;

    if (old->is_assignment_slot_name(name)) {
      // replace the data slot as a map slot
      result= ((slotsMap*)result->map())->
        copy_add_new_slot(result, old->name, MAP_SLOT(old->type),
                          obj->get_slot(old), old->annotation, mustAllocate);
      if (oop(result) == failedAllocationOop) return failedAllocationOop;
    }
  } else
    result= obj;

  return ((slotsMap*)result->map())->
    copy_add_new_slot(result, name, MAP_SLOT(type), contents, anno,
                      mustAllocate);
}


oop slotsMap::copy_add_data_slot(slotsOop obj,
                                 stringOop name, 
                                 slotType type,
                                 oop contents,
                                 oop anno,
                                 bool mustAllocate) {
  if (!name->is_unary()) {
    return ErrorCodes::vmString_prim_error(SLOTNAMEERROR);
  }

  slotDesc* old = find_slot(name);
  
  if (!old)
    return copy_add_new_slot(obj, name, type, contents, anno, mustAllocate);

  if (old->is_obj_slot())
    // change in place; if type is map_slot, just changes value
    return (slotsOop)change_slot(obj, old, OBJ_SLOT(type), contents, anno,
                                 mustAllocate);

  // remove then add obj slot
  slotsOop result= copy_remove_one_slot(obj, old, mustAllocate);
  if (oop(result) == failedAllocationOop) return result;
  slotsMap *new_map= (slotsMap*)result->map();
  
  return new_map->copy_add_new_slot(result, name, type, contents,
                                    anno, mustAllocate);
}



slotsOop slotsMap::copy_add_new_slot(slotsOop  obj, 
                                     stringOop name,
                                     slotType  slot_type,
                                     oop       contents,
                                     oop       anno,
                                     bool      mustAllocate) {
  assert_slots(obj, "object isn't a slotsOop");
  assert(!obj->is_string(), "cannot clone strings!");

  bool found;
  fint newIndex= find_slot_index_for(name, found);
  assert(!found, "I only add new slots");
  slotsMap* new_map= (slotsMap*) insert(newIndex, mustAllocate);
  if (new_map == NULL) return slotsOop(failedAllocationOop);

  slotDesc* s= new_map->slot(newIndex);
  new_map->slots_length= new_map->slots_length->increment();
  mapOop new_moop= new_map->enclosing_mapOop();
  new_moop->init_mark();
  new_map->init_dependents();

  slotsOop new_obj;
  switch (slot_type->slot_type()) {
   case obj_slot_type: {
    assert(NakedMethods || !contents->has_code() || slot_type->is_vm_slot(),
           "adding an assignable slot with code");
    // find which offset this slot should be at
    fint offset= empty_object_size();
    for (fint i= newIndex - 1; i >= 0; --i)
      if (slot(i)->is_obj_slot()) {
        offset= smiOop(slot(i)->data)->value() + 1;
        break;
      }
    new_obj= obj->is_byteVector()
              ? (slotsOop) byteVectorOop(obj) ->
                  insert(object_size(obj), offset, 1, mustAllocate, true)
              : (slotsOop) slotsOop(obj) ->
                  insert(object_size(obj), offset, 1, mustAllocate, true);
    if (oop(new_obj) == failedAllocationOop)
      return slotsOop(failedAllocationOop);
    new_map->shift_obj_slots(as_smiOop(offset), 1);
    new_map->object_length = new_map->object_length->increment();
    new_obj->at_put(offset, contents, false);
    new_obj->fix_generation(new_map->object_size(new_obj));
    contents= as_smiOop(offset);   // tagged index of slot data
    break; }
   case map_slot_type:
    new_obj= slotsOop(obj->clone(mustAllocate));
    break;
   case arg_slot_type:
    assert_smi(contents, "argument index isn't a smiOop");
    new_obj= slotsOop(obj->clone(mustAllocate));
    break;
   default:
    ShouldNotReachHere(); // unexpected slot type
  }
  if (oop(new_obj) == failedAllocationOop)
    return slotsOop(failedAllocationOop);
  s->init(name, slot_type, contents, anno, false);
  new_moop->fix_generation(new_moop->size());
  new_obj->set_canonical_map(new_map);
  
  return new_obj;
}

oop slotsMap::change_slot(oop obj,
                          slotDesc* slot,
                          slotType type, 
                          oop contents,
                          oop anno,
                          bool mustAllocate) {
  assert(slot != NULL, "cannot change the contents of a non-existent slot");
  assert(!obj->is_string(), "cannot clone strings!");
  assert_slots(obj, "object isn't a slotsOop");

  slotsOop new_obj= slotsOop(obj->clone(mustAllocate));
  if (oop(new_obj) == failedAllocationOop) return failedAllocationOop;
  switch (slot->type->slot_type()) {
   case obj_slot_type:
    assert(NakedMethods || !contents->has_code() || slot->type->is_vm_slot(),
           "adding an assignable slot with code");
    assert_smi(slot->data, "data slot contents isn't an offset");
    new_obj->at_put(smiOop(slot->data)->value(), contents);
    break;
   case map_slot_type:
    break;
   case arg_slot_type:
    assert_smi(contents, "argument index isn't a smiOop");
    break;
   default:
    ShouldNotReachHere(); // unexpected slot type
  }

  if (    slot->data == contents
      &&  slot->type == type
      &&  slot->get_annotation() == anno) {
    // no change (to the map, at least)!
    return new_obj;
  }
  
  // create a new map for this object
  slotsMap* new_map= copy_for_changing(mustAllocate);
  if (new_map == NULL) return failedAllocationOop;
  slot = slot->shift(this, new_map);
  slot->type = type;
  slot->set_annotation(anno);
  if (!slot->is_obj_slot()) {
    Memory->store(&slot->data, contents);
  }

  new_obj->set_canonical_map(new_map);
  
  return new_obj;
}


slotsMap* slotsMap::copy_for_changing(bool mustAllocate) {
  slotsMap* new_map= (slotsMap*) copy(mustAllocate);
  if (new_map == NULL) return NULL;
  new_map->init_dependents();
  mapOop new_moop = new_map->enclosing_mapOop();
  new_moop->init_mark();
  return new_map;
}


// Return clone of obj with slot removed.
// If removed slot is asg slot, must also move data slot.
// If slotDesc::data is ever used for arg slots, this routine will have
//  to do more work than today -- dmu 2/93

oop slotsMap::copy_remove_slot(oop obj, stringOop name, bool mustAllocate) {

  if (    obj->is_vframe()
      ||  obj->is_assignment()
      || !obj->is_programmable_slots()) {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }

  slotsOop sobj = slotsOop(obj);

  slotDesc* d= find_slot(name);
  if (d == NULL || d->is_vm_slot()) {
    return ErrorCodes::vmString_prim_error(BADSLOTNAMEERROR); 
  }

  slotsOop result;

  if (d->is_assignment_slot_name(name)) {
    // remove data slot then add in as map slot
    result= copy_remove_one_slot(sobj, d, mustAllocate);
    if (oop(result) == failedAllocationOop) return failedAllocationOop;

    // add data slot back in as map slot
    result = (slotsOop)
      result->copy_add_slot(d->name, MAP_SLOT(d->type), sobj->get_slot(d),
                            d->get_annotation(), mustAllocate);
    if (oop(result) == failedAllocationOop) return failedAllocationOop;
    assert(result->is_slots(), "should not fail");

  } else {
    // remove slot
    result= copy_remove_one_slot(sobj, d, mustAllocate);
  }

  return result;
}


slotsOop slotsMap::copy_remove_one_slot(slotsOop obj, slotDesc *slot,
                                        bool mustAllocate) {
  assert_slots(obj, "object isn't a slotsOop");
  assert(!obj->is_string(), "cannot clone strings!");
  assert(slot >= slots() && slot < slotsMap::slot(length_slots()),
         "slotDesc not part of map");

  slotsMap* new_map= (slotsMap*) remove(slot, 1, mustAllocate);
  if (new_map == NULL) return slotsOop(failedAllocationOop);
  new_map->slots_length = new_map->slots_length->decrement();
  new_map->init_dependents();
  mapOop new_moop = new_map->enclosing_mapOop();
  new_moop->init_mark();

  slotsOop new_obj;
  switch (slot->type->slot_type()) {
   case obj_slot_type:
    assert_smi(slot->data, "data slot contents isn't an offset");
    new_obj= obj->is_byteVector()
      ? (slotsOop) byteVectorOop(obj)->remove(object_size(obj),
                                              smiOop(slot->data)->value(), 
                                              1, mustAllocate, true)
      : (slotsOop) slotsOop(obj)->remove(object_size(obj),
                                         smiOop(slot->data)->value(), 
                                         1, mustAllocate, true);
    if (oop(new_obj) == failedAllocationOop)
      return slotsOop(failedAllocationOop);
    // check-stores done by remove already
    new_map->shift_obj_slots(smiOop(slot->data), -1);
    new_map->object_length = new_map->object_length->decrement();
    break;
   case arg_slot_type: {
    // fix up any arg slots after this one
    assert_smi(slot->data, "bad arg index");
    fint argIndex= smiOop(slot->data)->value();
    FOR_EACH_SLOTDESC(new_map, s) {
      if (s->is_arg_slot()) {
        assert_smi(s->data, "bad arg index");
        fint a= smiOop(s->data)->value();
        if (a > argIndex)
          s->data= as_smiOop(a - 1);
      }
    }
   }
   // fall through     
   case map_slot_type:
    new_obj= slotsOop(obj->clone(mustAllocate));
    if (oop(new_obj) == failedAllocationOop)
      return slotsOop(failedAllocationOop);
    break;
   default:
    ShouldNotReachHere(); // unexpected slot type;
  }

  new_obj->set_canonical_map(new_map);
  
  return new_obj;
}


void slotsMap::switch_pointer(oop obj, oop* where, oop to) {
  assert_slots(obj, "must be slotsOop");
  slotsOop sobj = slotsOop(obj);
  if (where  >=  sobj->oops(empty_object_size())) {
    Memory->store(where, to);
  }
}


void slotsMap::switch_pointer_in_map(oop* where,  oop to) {
  if (where == &annotation) {
    set_annotation(to);
    return;
  }
  Map::switch_pointer_in_map(where, to);
}


void slotsMap::shift_obj_slots(smiOop offset, fint delta) {
  FOR_EACH_SLOTDESC(this, slot) {
    if (slot->type->is_obj_slot() && smiOop(slot->data) >= offset)
      Memory->store(&slot->data,
                    as_smiOop(smiOop(slot->data)->value() + delta));
  }
}

oop slotsMap::clone(oop obj, bool mustAllocate, oop genObj) {
  assert_slots(obj, "not a slots object");
  assert(!is_byteVector(), "should override this method for a byte vector");
  slotsOop p= (slotsOop) slotsOop(obj)->copy(object_size(obj),
                                             mustAllocate, genObj);
  if (oop(p) != failedAllocationOop) p->init_mark();
  return p;
}

oop slotsMap::cloneSize(oop obj, fint length, bool mustAllocate, oop filler) {
  Unused(length); Unused(mustAllocate); Unused(filler);
  ShouldNotCallThis(); // object isn't a vector
  return obj;
}

oop slotsMap::fill_in_slots(slotList* slist, fint slotCount) {
  fint offset = empty_object_size();
  fint objCount = slist->obj_count();

  slots_length = as_smiOop(slotCount);
  object_length = as_smiOop(offset + objCount);

  slotsOop obj = create_object(offset + objCount);
  oop* p = obj->oops(offset);
  for (slotDesc* slot = slots(); slist; slist = slist->next, slot ++) {
    oop data;
    switch (slist->type()->slot_type()) {
     case obj_slot_type:
      assert(slist->contents() == NULL ||
             NakedMethods ||
             !slist->contents()->has_code() ||
             slist->type()->is_vm_slot(),
             "creating an assignable slot with code");
      data = as_smiOop(offset++);
      Memory->store(p++, slist->contents());
      break;
     case map_slot_type:
      data = slist->contents();
      break;
     case arg_slot_type:
      assert_smi(slist->contents(), "not an integer offset");
      data = slist->contents();
      break;
     default:
      ShouldNotReachHere(); // unexpected slot type
    }
    slot->init(slist->name(), slist->type(), data, slist->annotation());
  }

  slotsOop(obj)->set_canonical_map(this);
  
  return obj;
}

oop slotsMap::scavenge(oop obj) {
  // use :: to avoid another virtual function call
  return slotsOop(obj)->scavenge(slotsMap::object_size(obj)); 
}

bool slotsMap::verify(oop obj) {
  return slotsOop(obj)->verify();
}

fint slotsMap::empty_object_size() {
  return sizeof(slotsOopClass) / oopSize;
}

void slotsMap::print(oop obj) {
  if (obj->is_map()) {
    // do nothing special
  } else {
    if (obj == (oop) Memory->trueObj) {
      lprintf("true ");
    } else if (obj == (oop) Memory->falseObj) {
      lprintf("false ");
    } else if (obj == (oop) Memory->nilObj) {
      lprintf("nil ");
    } else if (obj == (oop) Memory->lobbyObj) {
      lprintf("lobby ");
    }
    printObjectID(obj);
    lprintf(": ");
  }
  Map::print(obj);
}

void slotsMap::dummy_initialize(oop obj, oop filler) {
  Unused(obj); Unused(filler); }

oop slotsMap::dummy_obj(oop filler) {
  fint offset = empty_object_size();
  fint n_slots= length_obj_slots();
  slotsOop obj= create_object(offset + n_slots);
  obj->set_map(this);

  { // fill in the data slots with filler
    oop* p = obj->oops(offset);
    for (fint n= 0; n < n_slots; ++n) {
      Memory->store(p++, filler);
    }
  }

  dummy_initialize(obj, filler);

  return obj;
}

oop slotsMap::mirror_copy_set_annotation(oop r, oop a, bool mustAllocate) {
  if (!is_programmable_slots())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);

  assert_slots(r, "better be slotsOop");
  slotsOop new_obj = slotsOop(r)->clone(mustAllocate);
  if (new_obj == slotsOop(failedAllocationOop))
    return slotsOop(failedAllocationOop);

  slotsMap* new_map= copy_for_changing(mustAllocate);
  if (new_map == NULL) return slotsOop(failedAllocationOop);
  new_map->set_annotation(a);
  new_obj->set_canonical_map(new_map);
  return new_obj->fix_up_method(r, false, mustAllocate);
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "byteVectorMap.hh"
# include "_byteVectorMap.cpp.incl"

oop byteVectorMap::clone(oop obj, bool mustAllocate, oop genObj) {
  assert_byteVector(obj, "not a byte vector");
  byteVectorOop v= byteVectorOop(obj)->copy(mustAllocate, genObj);
  if (oop(v) != failedAllocationOop) v->init_mark();
  return v;
}

oop byteVectorMap::cloneSize(oop obj, fint len, bool mustAllocate,
                             oop filler) {
  assert_byteVector(obj, "not a byte vector");
  assert_smi(filler, "not a smiOop filler");
  fint l = byteVectorOop(obj)->length();
  byteVectorOop v;
  if (l < len) {
    // growing array
    v= byteVectorOop(obj)->grow_bytes(len - l, mustAllocate);
    if (oop(v) != failedAllocationOop)
      set_bytes(v->bytes(l),  len - l,  char(smiOop(filler)->value()));
  } else if (l > len) {
    // shrinking array
    v= byteVectorOop(obj)->shrink_bytes(l - len, mustAllocate);
  } else {
    // copying array
    v= byteVectorOop(obj)->copy(mustAllocate);
  }
  if (oop(v) != failedAllocationOop) v->init_mark();
  return v;
}

byteVectorOop byteVectorMap::create_byteVector(slotList* slots) {
  byteVectorOop bv;
  byteVectorMap m1;
  (void)create_map(sizeof(byteVectorMap), slots, &m1, (oop*)&bv);
  return bv;
}

oop byteVectorMap::scavenge(oop obj) {
  // use :: to avoid another virtual function call
  return byteVectorOop(obj)->scavenge(byteVectorMap::object_size(obj)); 
}

bool byteVectorMap::verify(oop obj) {
  return byteVectorOop(obj)->verify();
}

bool byteVectorMap::verifyBytesPart(oop obj, char*& b) {
  return byteVectorOop(obj)->verifyBytesPart(b);
}

fint byteVectorMap::empty_object_size()  { 
  return sizeof(byteVectorOopClass) / oopSize;
}

void byteVectorMap::dummy_initialize(oop obj, oop filler) {
 Unused(filler);
 assert_byteVector(obj, "not a byte vector");
 byteVectorOop(obj)->set_length(0);
 byteVectorOop(obj)->set_bytes(NULL);
}

void byteVectorMap::print_byteVector(oop obj) {
  lprintf("byte array: {");
  if (obj->is_map()) {
    lprintf("...");
  } else {
    bool first = true;
    char* p = byte_array(obj);
    char* end = p + length_byte_array(obj);
    char* end2 = p + VectorPrintLimit < end ? p + VectorPrintLimit : end;
    for (; p < end2; p ++) {
      if (first) first = false;
      else lprintf(", ");
      lprintf("%ld", long(*p));
    }
    if (end != end2) {
      lprintf(", ... (%d more elements) ", end - end2);
    }
  }
  lprintf("} ");
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mirrorOop.hh"
# include "_mirrorOop.cpp.incl"

oop mirrorOopClass::as_mirror_prim(oop obj) {
  return obj->as_mirror();
}


oop reflectee_id_hash_prim(mirrorOop rcvr) {
  if (!rcvr->is_mirror()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return as_smiOop(rcvr->reflectee()->identity_hash());
}

oop reflectee_eq_prim(mirrorOop rcvr, oop anotherMirror) {
  if (!rcvr         ->is_mirror()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  if (!anotherMirror->is_mirror()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  return rcvr->reflectee() == mirrorOop(anotherMirror)->reflectee()
    ? Memory->trueObj : Memory->falseObj;
}


oop mirrorOopClass::define_prim(mirrorOop cont, void *FH) {
  oop result= reflectee()->define_prim(cont->reflectee(), FH);
  if (result == NULL) return NULL;
  return  result->is_mark() ?  result : this;
}

oop mirrorOopClass::copy_add_slot_prim(stringOop name, 
                                       mirrorOop contMirror,
                                       bool isP,
                                       bool isA,
                                       oop anno,
                                       void *FH) {
  oop result = slotsOop(reflectee())
    ->copy_add_slot_prim(name, contMirror->reflectee(), isP, isA, anno, FH);
  return result == failedAllocationOop || result->is_mark()
    ? result : result->as_mirror();
}

oop mirrorOopClass::copy_remove_prim(stringOop name, void *FH) {
  oop result = slotsOop(reflectee())->copy_remove_slot_prim(name, FH);
  return result == failedAllocationOop || result->is_mark() 
    ? result : result->as_mirror();
}


oop mirrorOopClass::evaluate_in_context_prim(mirrorOop methodMirror) {
  return reflectee()->evaluate_in_context_prim(methodMirror->reflectee());
}

// switch reflectee if appropriate
// because a mirror's map depends on the kind of reflectee it has,
// this can get difficult, since the map or size might be different
// Let's try a recursive define! It will be fast for the like-type case,
// ought to work for the weird cases. -- dmu 2/93
//
// unsuitable for primitive
//  cause doesnt call define_prim, so doesnt convert processes

void mirrorOopClass::switch_reflectee(oop* where, oop to) {
  if (where == &addr()->_reflectee) {
    // optimize to prevent alloated and running out of space -- dmu 5/6
    if (to->map()->mirror_proto() == reflectee()->map()->mirror_proto())
      set_reflectee(to);
    else {
      mirrorOop toM = to->as_mirror();
      if (toM->map() == map()) // an opt to avoid method inval
        set_reflectee(to);
      else if ( define(toM) -> is_mark() )
        fatal("define failed");
    }
  }
}

oop mirrorOopClass::copy_set_annotation_prim(oop a, void *FH) { 
  oop r= reflectee()->mirror_copy_set_annotation(a);
  if (r == failedAllocationOop) {
    out_of_memory_failure(FH, this->size());
    return NULL;
  }
  return r->is_mark() ? r : r->as_mirror();
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotsOop.hh"
# include "_slotsOop.cpp.incl"

slotsOop slotsOopClass::create_slots(fint size) {
  oop *p= Memory->alloc_objs(size);
  slotsOop obj = as_slotsOop(p);
  obj->init_mark();
  return obj;
}


// overall scheme for programming prims:
// the ones in mirrorOop just unwrap and call here
// The ones here, just check the basic types and call
// over to the map, where the real work is done.
// The map side may return marks on error.
// -- dmu 1/92

oop slotsOopClass::add_slots_prim(oop contents, void *FH) {
  return add_slots(contents, false, FH);
}

oop slotsOopClass::add_slots_if_absent_prim(oop contents, void *FH) {
  return add_slots(contents, true, FH);
}


// add a slot to the receiver: always functional


oop slotsOopClass::copy_add_slot_prim(stringOop      name,
                                      oop            contents, 
                                      bool           isP,
                                      bool           isA,
                                      oop            anno,
                                      void *FH) {
  ResourceMark rm;
  // primitives can call here with any oop

  if (!is_slots())              return ErrorCodes::vmString_prim_error(  BADTYPEERROR);
  if (!name->is_string())       return ErrorCodes::vmString_prim_error(  BADTYPEERROR);
  if (!name->is_slot_name())    return ErrorCodes::vmString_prim_error( SLOTNAMEERROR);
  
  // Cannot put a block method into a regular method slot; compilers will not work. -- dmu 7/1
  if (contents->has_code()
  &&  contents->kind() == BlockMethodType
  &&  !is_block())
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
    
  if (isA) {
    if (!is_programmable_slots())              return ErrorCodes::vmString_prim_error( BADTYPEERROR);
    if (!is_method_like() || is_assignment())  return ErrorCodes::vmString_prim_error( BADTYPEERROR);
    if (isP)                                   return ErrorCodes::vmString_prim_error(  PARENTERROR);

  }

  slotType type;
  if (isA) {
    type= arg_slotType;
    contents= as_smiOop(arg_count());
    // could be off by one if there's already an arg slot with this name,
    // but copy_add_argument_slot will deal with this.
  } else {
    // specify map slot here; the underlying map op will make it an obj
    // slot if necessary
    type= isP ? parent_map_slotType : map_slotType;
  }
    
  oop result= copy_add_slot_fixing_up_method(name, type, contents, anno);

  if (result == failedAllocationOop) {
    out_of_memory_failure(FH, size()+1);
    return NULL;
  }
  return result;
}


// do rest of prim & provide a place callable with slotType that
//   still fixes lexical_links, etc in methods

oop slotsOopClass::copy_add_slot_fixing_up_method( stringOop      name,
                                                   slotType       t,
                                                   oop            contents,
                                                   oop            anno,
                                                   bool           mustAllocate) {
  oop result=  copy_add_slot(name, t, contents, anno, mustAllocate);
  if (result == failedAllocationOop)
    return result;
  return  result->fix_up_method(this, false, mustAllocate);
}  
     


oop slotsOopClass::remove_slot_prim(stringOop name, void *FH) {
  oop result= copy_remove_slot_prim(name, FH);
  if (result == failedAllocationOop) {
    out_of_memory_failure(FH, size());
    return NULL;
  }
  if (result->is_mark()) return result;
  return define_prim(result, FH);
}

oop slotsOopClass::copy_remove_slot_fixing_up_method( stringOop name,
                                                      bool mustAllocate) {
  oop res= copy_remove_slot(name, mustAllocate);
  return res == failedAllocationOop
    ? res
    : res->fix_up_method(this, false, mustAllocate);
}


// remove all slots from me -- needed to fileOut proxies and such -- dmu 4/93

oop slotsOopClass::remove_all_slots_prim(void *FH) {
  slotsOop result = this;
  Map* m = map();
  FOR_EACH_SLOTDESC(m, slot) {
    if (slot->is_vm_slot()        ) continue; // leave vm slots alone
    oop result2= result->copy_remove_slot_fixing_up_method(slot->name, CANFAIL);
    if (result2 == failedAllocationOop) {
      out_of_memory_failure(FH, size());
      return NULL;
    }
    if (result2->is_mark()) return result2;
    assert_slots(result2, "result of removing slot must be slotsOop");
    result = slotsOop(result2);
  }
  return define_prim(result, FH);
}


oop slotsOopClass::copy_remove_slot_prim(stringOop name, void *FH) {
  if (!is_slots() || !name->is_string()) return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  oop result= copy_remove_slot_fixing_up_method(name, CANFAIL);
  if (result == failedAllocationOop) {
    out_of_memory_failure(FH, size());
    return NULL;
  }
  return result;
}

oop slotsOopClass::create_block_prim() {
   if (!is_slots()  ||  !is_method_like()  || is_assignment())
     return ErrorCodes::vmString_prim_error(BADTYPEERROR);
   return create_block(this);
 }

// ---------------------------------------------------

# define DO_0(x)
# define DO_1(x) DO_0(x) x(0)
# define DO_2(x) DO_1(x) x(1)
# define DO_3(x) DO_2(x) x(2)
# define DO_4(x) DO_3(x) x(3)
# define DO_5(x) DO_4(x) x(4)
# define DO_6(x) DO_5(x) x(5)
# define DO_7(x) DO_6(x) x(6)
# define DO_8(x) DO_7(x) x(7)
# define DO_9(x) DO_8(x) x(8)

# define COPY_TEMPLATE(x)                                                     \
    ((oop*) b)[x + EMPTY_SLOTS_OOP_SIZE] = ((oop*) p)[x + EMPTY_SLOTS_OOP_SIZE];

# if GENERATE_DEBUGGING_AIDS
#  define ALLOC_CHECK(N)                                                      \
    if (CheckAssertions  &&  b == (slotsOopClass*)catchThisOne) {                                  \
      warning1("clone" #N "_prim caught 0x%lx", b);                           \
    }
# else
#  define ALLOC_CHECK(N)
# endif


# define cloneN(N)                                                            \
  oop CONC3(clone,N,_prim)(slotsOop rcvr) {                                   \
    assert_slots(rcvr, "must clone a slotsOop");                              \
    assert(rcvr->map()->empty_object_size() == EMPTY_SLOTS_OOP_SIZE,          \
           "embedded constants assume this");                                 \
    assert(rcvr->map()->can_inline_clone(), "shouldn't use this function");   \
    slotsOopClass* p = rcvr->addr();                                          \
    const int32 size = sizeof(slotsOopClass)/oopSize + N;                     \
      /* Would be cleaner (but slightly slower in the fast case) to do */     \
      /*  b= Memory->alloc_objs(size); */                                     \
      /*  if (Memory->new_gen->eden_space->contains(b)) ... */                \
    slotsOopClass* b= (slotsOopClass*)                                        \
       Memory->new_gen->eden_space->alloc_objs_local(size);                   \
    slotsOop b1;                                                              \
    if (b) {                                                                  \
      ALLOC_CHECK(N);                                                         \
      b1 = as_slotsOop(b);                                                    \
      b1->init_mark();                                                        \
      COPY_TEMPLATE(-1)                                                       \
      CONC(DO_,N)(COPY_TEMPLATE)                                              \
    } else {                                                                  \
      b = (slotsOopClass*) Memory->alloc_objs(size);                          \
      b1 = as_slotsOop(b);                                                    \
      b1->init_mark();                                                        \
      COPY_TEMPLATE(-1)                                                       \
      CONC(DO_,N)(COPY_TEMPLATE)                                              \
      if (b1->is_old())                                                       \
        Memory->record_multistores(((oop*) b) + EMPTY_SLOTS_OOP_SIZE - 1,     \
                                   ((oop*) b) + EMPTY_SLOTS_OOP_SIZE + N);    \
    }                                                                         \
    return b1;                                                                \
  }


cloneN(0)
cloneN(1)
cloneN(2)
cloneN(3)
cloneN(4)
cloneN(5)
cloneN(6)
cloneN(7)
cloneN(8)
cloneN(9)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "slotDesc.hh"
# include "_slotDesc.cpp.incl"

stringOop slotDesc::assignment_slot_name() {
  ResourceMark rm;
  assert(is_obj_slot(), "only obj slots are assignable");
  int32 len= name->length();
  char* sn= NEW_RESOURCE_ARRAY(char, len + 2);
  copy_bytes(name->bytes(), sn, len);
  sn[len    ] = ':';
  sn[len + 1] = '\0';
  return new_string(sn, len + 1); }

bool slotDesc::is_assignment_slot_name(stringOop n) {
  if (!is_obj_slot()) return false;
  fint l1= name->length(), l2= n->length();
  if (l1 + 1 != l2  ||  n->bytes()[l2-1] != ':') return false;
  return strncmp(name->bytes(), n->bytes(), l1)==0; }

bool slotDesc::verify(Map* m) {
  bool flag = true;
  if (!bootstrapping && !name->is_string()) {
    error1("slot 0x%lx name isn't a string", this);
    flag = false;
  }
  if (!oop(type)->is_smi()) {
    error1("slot 0x%lx type isn't a smiOop", this);
    flag = false;
  } else {
    smiOop offset;
    switch (type->slot_type()) {
     case obj_slot_type:
      offset = smiOop(data);
      if (!offset->is_smi()) {
        error1("slot 0x%lx offset isn't a smiOop", this);
        flag = false;
      } else if (m && ! m->is_slots()) {
        error1("slot 0x%lx is in the object of a non-slots map", this);
        flag = false;
      } else if (offset->value() < 0 || 
                 (m && 
                 offset->value() >= ((slotsMap*) m)->object_length->value())) {
        error1("slot 0x%lx offset is out of range", this);
        flag = false;
      }
      break;
     case map_slot_type:
      break;
     case arg_slot_type:
      offset = smiOop(data);
      if (!offset->is_smi()) {
        error1("slot 0x%lx argument index isn't a smiOop", this);
        flag = false;
      } else if (offset->value() < 0) {
        error1("slot 0x%lx argument index is negative", this);
        flag = false;
      }
      break;
     default:
      error1("slot 0x%lx type isn't a legal slot type", this);
      flag = false;
    }
  }
  return flag;
}



void slotDesc::printAugmentedName() { lprintf("%s", augmentedName()); }

char* slotDesc::augmentedName() {
  char* buf = NEW_RESOURCE_ARRAY(char, 80);
  buf[0] = '\0';
  if (is_arg_slot()) strcat(buf, ":");
  strcat(buf, name->copy_null_terminated());
  if (is_parent()) strcat(buf, "*");
  return buf;
}


bool slotDesc::compare(slotDesc* sd) {
  //INPUT a slotDesc, sd
  //OUTPUT true if sd is equivalent to this, otherwise false

  assert(   name == sd->name
         ||     name->is_unary() &&     is_assignment_slot_name(sd->name)
         || sd->name->is_unary() && sd->is_assignment_slot_name(    name),
         "compared slots must have the same name");
  
  // are slot types the same?
  if (type             != sd->type            ) return false;
  if (get_annotation() != sd->get_annotation()) return false;

  switch (type->slot_type()) {
  case obj_slot_type: // assignable slot; slots match
    assert(data == sd->data, "offsets should match");
    return true;
  case map_slot_type: // constant slot:  make sure constants are same
    return data == sd->data;
  case arg_slot_type: // argument slot: shouldn't be comparing methods
  default:            // bad slot type
    break;
  }
  ShouldNotReachHere();
  return false;
}
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "mapOop.hh"
# include "_mapOop.cpp.incl" 

bool mapOopClass::equal(mapOop other) {
  if (this == other) return true;
  if (map_addr()->is_block()  &&  other->map_addr()->is_block()) {
    // blocks are the same if block method is the same
    return ((blockMap*)  map_addr())->value() ==
      ((blockMap*)other->map_addr())->value();
  } else {
    return false;
  }
}

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "smiMap.hh"
# include "_smiMap.cpp.incl"

Map* smiMap::create_smiMap(oop parent) {
  slotList *s= new slotList(VMString[PARENT], 
                            parent_map_slotType,
                            parent,
                            Memory->slotAnnotationObj);
  smiMap m1;
  oop ignored;
  return create_map(sizeof(smiMap), s, &m1, &ignored);
}

void smiMap::print_string(oop obj, char* buf) {
  smiOop(obj)->print_string(buf);
}

void smiMap::print_oop(oop obj) {
  smiOop(obj)->print_oop();
}

void smiMap::print(oop obj) {
  if (obj->is_map()) {
    lprintf("smi ");
  } else {
    smiOop(obj)->print_oop();
    lprintf(": ");
  }
  immediateMap::print(obj);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "assignmentMap.hh"
# include "_assignmentMap.cpp.incl"

static bool createdAssignment = false;

assignmentOop assignmentMap::create_assignment() {
  if (createdAssignment)
    ShouldNotReachHere(); // should create only one assignment obj
  createdAssignment= true;
  assignmentMap m1;
  assignmentOop p;
  (void)create_map(sizeof(assignmentMap), NULL, &m1, (oop*)&p);
  return p;
}

oop assignmentMap::cloneSize(oop obj, fint length, bool mustAllocate,
                             oop filler) {
  return slotsMapDeps::cloneSize(obj, length, mustAllocate, filler); }


/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerString.hh"

# include "_registerString.cpp.incl"

# ifdef FAST_COMPILER


void printAllocated(RegisterString rs) {
  lprintf("{");
  bool first = true;
  unsigned r = rs;              // safer for >>
  for (fint d = 0; r; d++, r >>= 1) {
    if (isSet(r, 0)) {
      if (first) {
        first = false;
      } else {
        lprintf(",");
      }
      lprintf("%s", RegisterNames[d]);
      Location d1 = Location(d);
      for (char c = RegisterNames[d][0];
           isSet(r, 1) && c == RegisterNames[d + 1][0];
           d ++, r >>= 1)
        ;
      if (d > d1) lprintf("-%s", RegisterNames[d]);
    }
  }
  lprintf("}");
}



Location pick(RegisterString& alloc, RegisterString mask) {
  unsigned r = mask & ~alloc;
  if (r == 0) return UnAllocated;
  fint reg;
  for (reg = 0; ! isSet(r, 0); reg ++, r >>= 1) ;
  setNth(alloc, reg);
  return Location(reg);
}
  

// ppc uses ldm, stm to save/restore nonvol regs, so must allocate
// them from top (r31 downwards)_:

Location pickReverse(RegisterString& alloc, RegisterString mask) {
  unsigned r = mask & ~alloc;
  if (r == 0) return UnAllocated;
  fint reg;
  for (reg = 31; ! isSet(r, 31); --reg, r <<= 1) ;
  setNth(alloc, reg);
  return Location(reg);
}


# endif // FAST_COMPILER
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "fcompiler.hh"
# include "_fcompiler.cpp.incl"

# ifdef FAST_COMPILER

FCompiler* theCompiler = NULL;

int32 FCompiler::verifiedOffset() { return codeGen->verifiedOffset; }
int32 FCompiler::diCheckOffset()  { return codeGen->diCheckOffset; }
int32 FCompiler::frameCreationOffset()  { return codeGen->frameCreationOffset;}
int32 FCompiler::frameSize()      { return codeGen->frameSize; }
ScopeDescRecorder* FCompiler::scopeDescRecorder() {return codeGen->scopeDescs;}
Assembler* FCompiler::instructions() { return &codeGen->a; }

FCompiler::FCompiler(compilingLookup* k, sendDesc* sd, nmln* d)
  : AbstractCompiler(k, sd, d) {
  if (VMNICProfiling) OS::profile(true);
  initialize();
}


void FCompiler::finalize() {
  theCompiler  = NULL;
  theCodeGen   = NULL;
  theAssembler = NULL;
  AbstractCompiler::finalize();
  if (VMNICProfiling) OS::profile(false);
}

void FCompiler::initialize() {
  assert(theCompiler == NULL,
         "shouldn't have but one fast compiler at a time");
  theCompiler = this;
  generateDebugCode = false;
  stackLocCount = extraArgCount = 0;
  countID = 0;
  codeGen = new CodeGen(L, send_desc, diLink);
  if (baseLookupType(L->lookupType()) == NormalBaseLookupType) {
    // ignore the receiver static bit (same nmethod covers both cases)
    L->clearReceiverStatic();
  }
  containsLoop = false;
  isImpure= false; // set to true when something is inlined
}


nmethod* FCompiler::compile() {
  EventMarker em("NIC-compiling %#lx %#lx", L->selector(), NULL);
  ElapsedTimer t(PrintCompilation || PrintCompilationStatistics);
  ShowCompileInMonitor sc(L->selector(), "NIC", false);
  // don't do any inlining when converting / single-stepping
  FlagSetting noinl(NICInlineDataAccess,
                    NICInlineDataAccess && !generateDebugCode);
  
  dispatchToCode();
  
  codeGen->scopeDescs->generate();
  if (PrintCompilationStatistics) t.stop();     // don't include nmethod alloc
  nmethod* nm = nmethod::new_nmethod(this, generateDebugCode);
  assert(theAssembler->verifyLabels(), "undefined labels");
  em.event.args[1] = nm;
  if (PrintCompilation || PrintCompilationStatistics) {
    float dt = IntervalTimer::dont_use_any_timer ? 0.0 : t.millisecs();
    if (PrintCompilation) {
      char buf[1000];
      sprintf(buf, "%#lx (%3.1f ms%s)\n", (long unsigned int)nm, dt,
              dt >= MaxCompilePause ? "!" : "");
      lprintf("%s", buf);
    }
    if (PrintCompilationStatistics) {
      lprintf("\n*NIC-time=|%2.1f| ms; to/co/sc/lo/de/bc= |%d|%d|%d|%d|%d|%d|", 
              *(void**)&dt, 
              (void*)(nm->instsLen() + nm->scopes->length() + 
                nm->locsLen() + nm->depsLen),
              (void*)nm->instsLen(), 
              (void*)nm->scopes->length(),
              (void*)nm->locsLen(), 
              (void*)nm->depsLen,
              (void*) (nm->isAccess()
                       ? 0 :
                       ((methodMap*)(nm->method()->map()))
                          ->codes()->length()));
    }
  }
  if (VerifyZoneOften) nm->verify();
  return nm;
}


void FCompiler::trace_compile(const char *s) {
  if (PrintCompilation) {
    lprintf("*NIC-compiling %s method for %s:\n",
            s, sprintName(NULL, L->selector()));
  }
}


void FCompiler::assignmentCode() {
  trace_compile("assignment");

  realSlotRef *res= L->result()->as_real();
  lookupTarget* h = res->holder;
  
  slotDesc* dataSlot = h->map()->find_slot(res->desc->name);
  realSlotRef* dataRef  = new realSlotRef(h, dataSlot);
  codeGen->assignmentCode(dataRef);

  MethodLookupKey* k= new_MethodLookupKey(L->key);
  
  ScopeInfo scope = codeGen->scopeDescs->
    addDataAssignmentScope( k,
                            new LocationName(LReceiverReg),
                            L->receiverMapOop(), 
                            methodHolder_or_map());
  codeGen->scopeDescs->addSlot(scope, 0, new LocationName(ArgLocation(0)));
}


void FCompiler::dataCode() {
  trace_compile("access");

  codeGen->prologue(true, 0);
  codeGen->lookup(Temp1, L->result()->as_real(), LReceiverReg);
  codeGen->epilogue(Temp1);

  MethodLookupKey* k= new_MethodLookupKey(L->key);
  
  (void) codeGen->scopeDescs->
    addDataAccessScope(k,
                       new LocationName(LReceiverReg),
                       L->receiverMapOop(), 
                       methodHolder_or_map());
}


void FCompiler::constantCode() {
  trace_compile("constant");

  codeGen->prologue(true, 0);
  codeGen->loadOop(Temp1, L->result()->as_real()->desc->data);
  codeGen->epilogue(Temp1);

  MethodLookupKey* k= new_MethodLookupKey(L->key);
  
  (void) codeGen->scopeDescs->
    addDataAccessScope(k,
                       new LocationName(LReceiverReg),
                       L->receiverMapOop(), 
                       methodHolder_or_map());
}


void FCompiler::methodCode() {
  trace_compile("normal");
  
  if (PrintNICSource  &&  method()  &&  method()->source() != NULL  &&  method()->source()->copy_null_terminated()) {
    lprintf( "NIC source: ");
    method()->source()->string_print();
    lprintf( "\n");
  }

  codeGen->haveStackFrame = true;
  oop slotContents= L->result()->contents();
  MethodKind kind= slotContents->kind();
  containsLoop= ((methodMap*)slotContents->map())->containsLoop();
  countID = Memory->code->nextNMethodID();
  MethodLookupKey* k= new_MethodLookupKey(L->key);
  
  FSelfScope* s;
  switch (kind) {

    case OuterMethodType:
      s = new FMethodScope(debugging, k, method(), methodHolder_or_map());
      break;

    case BlockMethodType: {
      FScope* parentScope = NULL;
      assert(ReuseNICMethods || L->receiverMap()->is_block(),
             "was expecting block");
      blockOop block = (blockOop) L->receiver;
      assert_block(block, "expecting a block literal");
      frame* parentBS = block->scope(true);
      if (parentBS) {
        // caution: parentFrame could be on conversion stack, so use
        // sending frame as a starting point
        //
        // I see that sendingVFrame can also be on conversion stack,
        //  so forget the frame hint -- dmu 1/96
        // Just to explain a bit more:
        //  for the conversion, the frames being converted are
        //  copied off the stack, and the sender is also copied.
        // So, starting with the sender is wrong, since it is not
        // on the stack, you cannot walk up the stack from it.
        // Also, the compiler wants the NEW frames anyway, so this
        // "hint" is just plain bogus. -- dmu
        
        // Guess what? Urs is here and he agrees! -- dmu 8/96

        // frame* sender =
        //  L->sendingVFrame
        //  ? L->sendingVFrame->fr
        //  : currentProcess->last_self_frame(false);

        parentVFrame = block->parentVFrame(NULL, true)->as_compiled();
        if (parentVFrame) parentScope = new_FVFrameScope(debugging, parentVFrame);
      }

      if (parentScope) {
        s = new FBlockScope(debugging, k, method(), parentScope);
      } else {
        s = new FDeadBlockScope(debugging, k, method());
      }
      break;
    }

    default:
      ShouldNotReachHere();
  }

  needRegWindowFlushes = false;

  s->genCode();
}


fint   FCompiler::incoming_arg_count() {
  return codeGen->incoming_arg_count();
}


void FCompiler::print() {
  print_short(); lprintf(":");
  L->print();
  L->result()->print();
  lprintf("\treceiver: ");
  L->receiver->print_real_oop();
  lprintf("\tcodeGen: %#lx\n",  codeGen);
}

void FCompiler::print_short() {
  lprintf("fcompiler %#lx", (unsigned long)this); }


# endif // FAST_COMPILER
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerState.hh"
# include "_registerState.cpp.incl"

# ifdef FAST_COMPILER

# undef  MAX
# define MAX ((maxDepth >> LogBitsPerWord) + 2) // added an extra 1 for I386, which has no regs


RegisterState::RegisterState(fint maxTemps) {
  allocated = permanent = 0; maxDepth = maxTemps;
  argDepth = stackDepth = curDepth = initStackTemps = 0;
  assert(sizeof(unsigned) == oopSize, "changed this");
  stackAllocs = NEW_RESOURCE_ARRAY( unsigned, MAX);
  stackPerms =  NEW_RESOURCE_ARRAY( unsigned, MAX);
  for (fint i = 0; i < MAX; i++) {
    stackAllocs[i] = stackPerms[i] = 0;
  }
  initialize_for_platform(maxTemps);
}


void RegisterState::allocatePermanent(Location r) {
  if (isRegister(r)) {
    ::allocateRegister(permanent, r);
  } 
  else if (is_StackLocation(r)) {
    setNth(stackPerms[whichMask(r)], whichBit(r));
  } 
  else {
    // extra incoming arg etc -- ignore
  }
  allocate(r);
}


Location RegisterState::pickPermanent() {
  // pick a permanent local
  RegisterString s = allocated;
  Location r = ::pickRegister(s, LocalMask);
  if (r == UnAllocated) {
    r = pickStackTemp();
  } 
  allocatePermanent(r);
  return r;
}

Location RegisterState::pickStackTemp() {
  Location r;
  fint base = StackLocations;
  for (fint i = 0; i < MAX; i++) {
    RegisterString m = stackAllocs[i];
    # if GENERATE_DEBUGGING_AIDS
      RegisterString old_m = m;
    # endif
    r = ::pick(m, AllBits);
    if (r != UnAllocated) break;
    # if GENERATE_DEBUGGING_AIDS
      assert(m == old_m, "pick did not find one, but changed m");
    # endif
    base += BitsPerWord;
  }
  assert(r != UnAllocated, "couldn't allocate stack temp");
  return Location(base + r);
}

void RegisterState::print() {
  ::printAllocated(allocated);
  ::printAllocated(permanent);
  lprintf("+%ld(%ld/%ld)+%ld:", curDepth, stackDepth,
         maxDepth, argDepth);
}
  


# endif // FAST_COMPILER
/* Sun-$Revision: 30.16 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "fscope.hh"
# include "_fscope.cpp.incl"

# ifdef FAST_COMPILER


// specializations since BlockLocation is not a pointer

template<> void BlockLocationList::grow() { ShouldNotCallThis(); }


template<> void BlockLocationList::print() {
  print_short();
  lprintf(": length %ld (max %ld) { ", (void*)long(len), (void*)long(max));
  for (fint i = 0; i < length(); i++) {
    BlockLocation bl = nth(i);
    if (bl.memoized) {
      lprintf("[%#lx] ", bl.loc);
    } else {
      lprintf("%#lx ", bl.loc);
    }
  }
  lprintf("}\n");
}


FSelfScope::FSelfScope(bool d, MethodLookupKey *key, oop method) :
  FScope(d) {
  _key    = key;
  _method = method;
  
  method_map = (methodMap *) method->map();
  fint codeLength  = method_map->length_codes();
  fint slotsLength = method_map->length_slots();
  args =   new LocationList(slotsLength);
  locals = new LocationList(slotsLength);
  exprStack = new      LocationList(codeLength);
  blocks =    new BlockLocationList(codeLength);
  nlrPoints = new LabelList(codeLength + 1);
  nlrPoints->append(NULL);
  
  bool got_one;
  method_map->branch_targets(got_one, &branchTargetFlags);
  
  if (got_one) {
    branchTargets = new LabelList(branchTargetFlags->length());
    for ( int32 i = 0,  n = branchTargetFlags->length();  i < n;  ++i)
      branchTargets->nthPut(i, new Label(theCodeGen->a.printing, NULL), true);
  }
  else {
    branchTargets = NULL;
  }
  initializedBlockLocations = NULL;
  
  frequentPreemption = theCompiler->generateDebugCode;
  // If doing consistent stack alloc for branches, must also
  //  be sure to copy fixed values when pushing -- dmu
  allowPrimitivesToAlterExpressionStack =  theCompiler->generateDebugCode;

    
  memoizeAllBlockZaps = theCompiler->generateDebugCode || got_one;
  initBlocksInPrologue = got_one;
  // VM expects to be able to create memoized blocks and put them in 
  //  locations pointed to by debugging info.
  // Since VM gets block locs from expr stack debug info, we
  //  must create all "memoized" blocks. 
  createMemoizedBlocks = theCompiler->generateDebugCode;
  
  branchTargetExprStack = new LocationList(codeLength);
  allocs = new RegisterState(slotsLength + codeLength);
  _scopeID = 0;
  scopeDescs = theCodeGen->scopeDescs;
  foundLoop = false;
  need_epilogue = true;
}


void FSelfScope::initialize() {
  assert( isTop(), "can't inline yet");
  // preallocate receiver, incoming args, locals
  self = receiver = IReceiverReg;
  allocs->allocatePermanent(receiver);
  
  { // Allocate space for arguments and count argument slots. 
    nargs = 0; 
    FOR_EACH_SLOTDESC_N(method()->map(), s, i) {
      args->append(UnAllocated);
      if (s->is_arg_slot()) {
        oop ind= s->data;
        assert_smi(ind, "bad index");
        fint argIndex= smiOop(ind)->value();
        allocs->allocatePermanent(IArgLocation(argIndex));
        args->nthPut(i, IArgLocation(argIndex));
        nargs++;
      }
    }
  }
  // This must come AFTER args so that self register does not overlap
  // arg registers. PPC assumes r3 -> r31, r4 -> r30 etc.
  if (isBlockSelfScope()) {
    // also preallocate self
    self = allocs->pickPermanent();
  }
  { // Allocate space for local slots
    FOR_EACH_SLOTDESC_N(method()->map(), s, i) {
      locals->append(UnAllocated);
      if (s->is_obj_slot()) {
        Location l = allocs->pickPermanentStackTemp();
        locals->nthPut(i, l);
      }
    }
  }
}


void FSelfScope::prologue() {
  theCodeGen->prologue(false, nargs);      // map test etc
  oop m = method();
  FOR_EACH_SLOTDESC_N(m->map(), sd, i) {
    Location l = locals->nth(i);
    if (l != UnAllocated) {
      assert(sd->is_obj_slot(), "must be an assignable slot");
      oop initialContents = m->get_slot(sd);
      theCodeGen->loadOop(l, initialContents);
    }
  }
  postPrologue();
  allocs->initStackTemps = allocs->stackDepth;
  theCodeGen->postPrologue(allocs, frequentPreemption);
  
  if (initBlocksInPrologue) {
    // reserve locations for blocks now and init to zero
    int32 n = method_map->length_literals();
    oop* lits = method_map->start_literals();
    initializedBlockLocations = new LocationList(n);
    for ( int32 j = 0;  j < n;  ++j) {
      Location loc;
      if (!lits[j]->is_block_with_code())
        loc = IllegalLocation;
      else {
        loc = allocs->pickPermanent();
        theCodeGen->loadImmediate(loc, 0);
      }
      initializedBlockLocations->nthPut( j, loc, true);
    }
  }
}


void FSelfScope::addExprStack( fint bci, NameNode* nn ) {
  scopeDescs->addExprStack( scopeDescs->root, bci, nn);
}

void FSelfScope::addBlock( fint bci, NameNode* nn ) {
  scopeDescs->addBlock( scopeDescs->root, bci, nn);
}

void  FSelfScope::addPcDesc( fint bci ) {
  scopeDescs->addPcDesc(theAssembler->offset(), scope, bci);
}


fcompiler_code_generator::fcompiler_code_generator(FSelfScope* s)  
 : abstract_interpreter(s->method()) {
  exprStackBCIs.init( 
    mi.map()->expression_stack_bcis( s->frequentPreemption), 
    -1);
  fscope= s;
}


void fcompiler_code_generator::interpret_method() {
  for ( ;    pc < mi.length_codes;  ++pc ) {
    define_branch_target(); // must define first and one past last
    interpret_bytecode();
  }
  define_branch_target();
}


void fcompiler_code_generator::interpret_bytecode() {
  abstract_interpreter::interpret_bytecode();

  assert(fscope->exprStack->isEmpty() 
      || fscope->exprStack->top() != UnAllocated,
   "invalid expr stack");

  write_expr_stack_info();
}


void fcompiler_code_generator::fetch_and_decode_bytecode() {
  abstract_interpreter::fetch_and_decode_bytecode();

  // stop when single-stepping; the code in nmethod::sendDescFor()
  // depends on the algorithm used here

  if ( fscope->frequentPreemption ) {
    if ( bc.op   == INDEX_CODE
    ||   bc.code == BuildCode(NO_OPERAND_CODE, UNDIRECTED_RESEND_CODE)
    ||   bc.op   == DELEGATEE_CODE
    // Stritly speaking, should need the following, but we don't seem to, 
    //   and it's faster this way.
    // Well, there is a mysterious infinite sendDesc finding bug, and I'm trying to fix it with this:
    //   -- dmu 5/02
    ||   (mi.instruction_set == TWENTIETH_CENTURY_PLUS_ARGUMENT_COUNT_INSTRUCTION_SET
          &&  bc.op == ARGUMENT_COUNT_CODE)
	 ) 
	  ; // do not add pc desc for these, even if in debug mode
    else {
      addPcDesc();
      theCodeGen->testStackOverflow(fscope->allocs);
    }
  }
  if (pc == mi.firstBCI()) {
    // make sure we're generating at least one pcDesc
    addPcDesc();
  }
  fscope->need_epilogue = true; // NLR code will reset this so we don't gen wasted code
}


void fcompiler_code_generator::do_SELF_CODE()              { fscope->selfCode();   }
void fcompiler_code_generator::do_POP_CODE()               { 
  Location loc = fscope->exprStack->pop();
  fscope->allocs->deallocate(loc);
}
void fcompiler_code_generator::do_NONLOCAL_RETURN_CODE()   { fscope->returnCode(); }
void fcompiler_code_generator::do_INDEX_CODE()             { }

void fcompiler_code_generator::do_literal_code(oop literal) {
  addPcDesc();

  Location loc;
  if (!literal->is_block_with_code()) {
    loc = fscope->pickStackLoc();
    theCodeGen->loadOop(loc, literal);
  }
  else {
    bool memoized = fscope->testMemoization( &mi.codes[pc], 
                                             &mi.codes[mi.length_codes], 
                                              mi.literals);
    BlockLocation bl;
    fscope->blockLiteral( is.index, blockOop(literal), memoized, bl);

    write_block_info(&bl);
    loc = fscope->pickStackLocAndMove(bl.loc);
  }
  fscope->exprStack->append(loc);
}


void fcompiler_code_generator::do_read_write_local_code( bool isWrite ) {
  if ( fscope->frequentPreemption )
    addPcDesc();
  fscope->genLocalAccess( !isWrite, is.lexical_level, is.index);
}    


void fcompiler_code_generator::do_send_code( bool isSelfImplicit, stringOop sel, fint arg_count ) {
  addPcDesc();
  fscope->genSend(isSelfImplicit, sel, arg_count, is.is_undirected_resend, is.delegatee);
}


void fcompiler_code_generator::do_branch_code( int32 target_PC, oop target_oop) {
  bool isLoop= target_PC <= pc;
  if ( isLoop )  fscope->foundLoop = true;
  if ( target_oop == badOop )
         fscope->unconditionalBranch( target_PC,             isLoop);
  else                         
         fscope->  conditionalBranch( target_PC, target_oop, isLoop);
}


void fcompiler_code_generator::do_BRANCH_INDEXED_CODE() {
  fscope->indexedBranch( get_branch_vector(), pc);
}


void fcompiler_code_generator::define_branch_target() {
  if ( fscope->branchTargetFlags->length() <= pc)
    return;
  if ( !fscope->branchTargetFlags->nth(pc))
    return;
  if (fscope->exprStack->length()) {
    warning2("NIC found non-empty expression stack at branch target "
            "at bytecode %d in method %s; "
            "The debug information will be incorrect.--dmu 9/96",
            pc, stringOop(fscope->selector())->copy_null_terminated());
  }
  // send expr stack out to registers
  fscope->materializeExprStack();
  // merge here
  fscope->branchTargets->nth(pc)->define();
}


void fcompiler_code_generator::write_block_info(BlockLocation* bl) {
  assert_block(bl->block, "should be a block");
  fscope->addBlock( pc,
    !bl->memoized  
      ?  (NameNode*) new LocationName(bl->loc)
      :  (NameNode*) new MemoizedName(bl->loc, bl->block) );
}

void fcompiler_code_generator::addPcDesc() { fscope->addPcDesc(pc); }

void fcompiler_code_generator::write_expr_stack_info() {
  if (pc != exprStackBCIs.current)
    return;

  fscope->addExprStack(  pc,  new LocationName(fscope->exprStack->top()));
  exprStackBCIs.advance();
}


void fcompiler_code_generator::print_short() {
  abstract_interpreter::print_short();
  lprintf("fscope %#lx\n", fscope);
  lprintf("exprStackBCIs: "); exprStackBCIs.print_short();
}
      
  
void FSelfScope::genCode() {
  genDescs();
  startOfScope = new DefinedLabel(theCodeGen->a.printing);

  fcompiler_code_generator cg(this);
  cg.interpret_method();
}


bool FSelfScope::testMemoization(u_char* bytes, u_char* end, oop* literals) {
  // memoize block if it is the failure block of a primitive send
  // (The test below misses some cases, but it's simple.)
  //
  // Notice that no branches can occur when a memoized block is on 
  // the stack. If this were not true, the code would fail
  // because the block could not be materialized.
  
  if (NICDelayPrimFailBlocks && bytes < end-1) {
    fint nextOp = getOp(bytes[1]);
    fint nextIndex = getIndex(bytes[1]);
    if ( (nextOp == SEND_CODE
       || nextOp == IMPLICIT_SEND_CODE
       || nextOp == ARGUMENT_COUNT_CODE)
    &&   literals[nextIndex]->is_string()) {
      char* sel = byteVectorOop(literals[nextIndex])->bytes();  
      Unused(sel); //debugging
      fint  len = byteVectorOop(literals[nextIndex])->length();
      Unused(len); //debugging
      if (stringOop(literals[nextIndex])->is_prim_name() 
      &&  stringOop(literals[nextIndex])->has_IfFail()) {
        // YES!  This block must be a failure block.
        return true;
      }
    }
  }
  return false;
}


// code for block literal (possibly memoizing)
void FSelfScope::blockLiteral(int32 litIndex, blockOop literal, 
                                  bool memoized,  BlockLocation& bl) {
  Location loc = initBlocksInPrologue 
    ? initializedBlockLocations->nth(litIndex)
    : allocs->pickPermanent();
  nlrPoints->append(NULL);

  // need to give the block a new map
  blockOop clone = literal->clone_and_set_desc(as_smiOop(descOffset()));

  assert( !clone->is_mark() && clone->map() != literal->map(), 
          "should have new map");
  bl.loc = loc;
  bl.memoized = memoized || memoizeAllBlockZaps;
  bl.block = clone;
  blocks->append(bl);
  
  // Pls note that memoizeAllBlockZaps only affects zapping;
  // a real block is created as long as memoized argument is false.
  // Otherwise, we would not know how to materialize a block.
  
  if ( !memoized  ||  createMemoizedBlocks)
    theCodeGen->loadBlockOop(loc, clone, allocs);
  else if ( !initBlocksInPrologue) {
    theCodeGen->loadImmediate(loc, 0);
    assert( !allowPrimitivesToAlterExpressionStack,
            "cannot memoize block if its canonical loc is not used"
             " in stack; debug info limitation");
  }
}



void FSelfScope::returnCode() {
  if (isMethodSelfScope()) {
    // normal return -- handled in standard epilogue
  } else {
    if (CatchInterprocessReturns) {
      loadParentScope(Temp1, IllegalLocation);
      for (FScope* s = parent();  s->isLexicalScope();  s = s->parent())
         s->loadParentScope(Temp1, Temp1);

      // check the block first
      PrimDesc* pd = getPrimDescOfFunction(
                fntype(&catch_interprocess_returns), true);
      assert(pd, "could not find it");
      theCodeGen->loadArg(-1, receiver, true);
      theCodeGen->cPrimCall(pd, allocs, false, true, 1);
    }
    // load home scope and other NLR registers
    loadParentScope(Temp1, IllegalLocation);
    for (FScope* s = parent();  s->isLexicalScope();  s = s->parent())
       s->loadParentScope(Temp1, Temp1);

    theCodeGen->prepareNLR(exprStack->pop(), Temp1, homeID());
    // jump to NLRpoint
    handleNLR( theCodeGen->branch() );
    need_epilogue = false;
  }
}


void FSelfScope::epilogue() {
  if (need_epilogue) {
    // zap all blocks
    for ( int32 i = blocks->length() - 1;  i >= 0;  i--) {
      BlockLocation bl = blocks->nth(i);
      theCodeGen->zapBlock(bl.loc, bl.memoized);
    }
    theCodeGen->epilogue( exprStack->length() > 0  ?  exprStack->pop() :  self);
  }
  genNLRPoints();
  theCodeGen->fixupFrame( allocs); // must be last to fixup code in genNLRPoints
}


void FSelfScope::handleNLR(Label* nlr) {
  // make NLR code in inline cache jump to nlrPoint n
  fint n = blocks->length();
  nlrPoints->nthPut(n, nlrPoints->nth(n)->unify(nlr));
}


void FSelfScope::genNLRPoints() {
  // generate NLR code for all inline caches in this method
  
  // If backwards-branching bytecodes are present, must zap all blocks from
  // every point.
  
  theCodeGen->a.Comment("NLR code");
  assert(nlrPoints->length() == blocks->length() + 1, "wrong length");
  if ( foundLoop ) {
    fint n = blocks->length();
    for ( ; n >= 0; n--) {
      Label* l = nlrPoints->pop();
      if (l) l->define();
    }
    n = blocks->length();
    for ( ; n > 0; n--) {  
      BlockLocation bl = blocks->pop();
      theCodeGen->zapBlock(bl.loc, bl.memoized);
    }
  }
  else {
    fint n = blocks->length();
    for ( ; n > 0; n--) {
      Label* l = nlrPoints->pop();
      BlockLocation bl = blocks->pop();
      if (l) l->define();
      theCodeGen->zapBlock(bl.loc, bl.memoized);
    }
    Label* l = nlrPoints->pop();
    if (l) l->define();
  }
  // I had tried the optimization below, but it BREAKS! I don't know why... -- dmu 5/99
  // if (blocks->length() > 0) // don't need this unless I have a block
  theCodeGen->testAndContinueNLR(scopeID());
}


void FSelfScope::genSend(bool isSelfImplicit,
       stringOop sel,
       fint argc,
       bool isUndirectedResend,
       stringOop del) {
  
  // check for perform (and pop selector if so)
  LookupType performLookupType;
  LocationList* argRegs = NULL;
  bool isPerform = checkPerformPrim_and_push_arguments(sel, performLookupType, argRegs);
  
  LookupType lookupType;
  bool isLocal = false;
  if (isPerform) {
    lookupType = performLookupType;
  }
  else if (del != NULL) {
    assert(isSelfImplicit, "directed resend must be to implicit self");
    lookupType = DirectedResendLookupType;
  }
  else if (isUndirectedResend) {
    lookupType = ResendLookupType;
  }
  else if (isSelfImplicit) {
    lookupType = ImplicitSelfLookupType;
    isLocal = !UseLocalAccessBytecodes && genLocalSend(sel, argc);
    if (!isLocal && NICInlineDataAccess) {
      isLocal = genReceiverDataAccess(sel);
      if (isLocal) theCompiler->isImpure= true;
    }
  }
  else {
    lookupType = NormalLookupType;
  }

  if (isLocal) {
    // was an access to a local / argument
    exprStack->append(result);
  } 
  else {
    Location lastArg;    // fail block (for prims)
    if (isPerform) {
      // args already pushed
      lastArg = UnAllocated;
      argc--;          // subtract selector (this argc will go into sendDesc for perform lookup)
      if (needsDelegatee(lookupType)) argc--;  // subtract delegatee
    } 
    else {
      // push args
      argRegs = new LocationList(argc + 1);
      lastArg = exprStack->isEmpty() ? UnAllocated : exprStack->top();
      bool isPrimCall = sel->is_string() && sel->is_prim_name();
      allocs->allocateArgs(argc, isPrimCall); // must do this even if argc is zero for Intel
      for (fint i = 1; i <= argc; i++) {
        Location l = exprStack->pop();
        theCodeGen->loadArg(argc - i, l, isPrimCall);
        argRegs->append(l);
      }
    }
    
    Location rcvr;
    if (isSelfImplicit) {
      rcvr = self;
    }
    else {
      rcvr = exprStack->pop();
      argRegs->append(rcvr);
    }
    
    Label* nlr = NULL;
    nlr = genCall(lookupType, rcvr, lastArg, argc, sel, del);
    // wait til here with deallocation (for scavenging)
    while (argRegs->length() > 0) {
      Location loc = argRegs->pop();
      allocs->deallocate(loc);
    }
    result = pickStackLoc();
    theCodeGen->move(result, ResultReg);
    exprStack->append(result);
    if (nlr) handleNLR(nlr);
  }
}


Location FSelfScope::pickStackLoc() {
  return allocs->pickLocal();
}


Location FSelfScope::pickStackLocAndMove(Location src ) {
  if ( !allowPrimitivesToAlterExpressionStack ) 
    return src; // no need to move to special place
    
  Location r = pickStackLoc();
  theCodeGen->move(r, src);
  return r;
}



FScope* FMethodScope::lookup(stringOop sel, slotDesc*& sd) {
  sd = method()->find_slot(sel);
  return sd ? this : NULL;
}


FScope* FLexicalScope::lookup(stringOop sel, slotDesc*& sd) {
  sd = method()->find_slot(sel);
  return sd ? this : parent()->lookup(sel, sd);
}


FScope* FVFrameMethodScope::lookup(stringOop sel, slotDesc*& sd) {
  sd = _vf->method()->find_slot(sel);
  return sd ? this : NULL;
}

  
FScope* FVFrameLexicalScope::lookup(stringOop sel, slotDesc*& sd) {
  sd = _vf->method()->find_slot(sel);
  return sd ? this : parent()->lookup(sel, sd);
}


void FSelfScope::genLocalAccess( bool isRead,
                                 fint lexicalLevel,
                                 fint index) {
  slotDesc *sd = method_map->getLocalSlot( lexicalLevel, index );
  FScope* s = this;
  for ( fint i = 0;  i < lexicalLevel;  i++)
    s = s->parent();
  bool ok = genLocalSend( isRead ? sd->name : sd->assignment_slot_name(),
        isRead ? 0 : 1,
        sd,
        s);
  assert(ok, "should always be local");
  exprStack->append(result);
}


bool FSelfScope::genLocalSend(stringOop sel, fint argc, slotDesc* sd, FScope* s) {
  if (sd == NULL) {
    s = lookup(sel, sd);
  }
  if (s != NULL) {
    // found a lexically-scoped slot with this name
    // s is the scope containing the slot

    assert(argc == 0 || argc == 1, "wrong number of args");
    NameDesc* nd = NULL;
    if (sd->is_map_slot() ||
        (s->isVFrameScope() &&
        (nd = s->vf()->get_name_desc(sd, true), nd && nd->isValue()))) {
      // load value of constant slot or of an unallocated (constant) data slot
      assert(argc == 0, "must be an access");
      result = pickStackLoc();
      theCodeGen->loadOop(result, nd ? nd->value() : sd->data);
    } else {
      // generate access code
      genLocal(s, this, sd, argc, true, Temp1);
    }
    return true;
  } 
  return false;
}


bool FSelfScope::genReceiverDataAccess(stringOop sel) {
  // Check if sel is a slot in self and:
  //   an object slot                -> load the object value,
  //   an assignment slot            -> make the assignment or
  //   a map slot (but not a method) -> load the map value.
  // Returns true if code was generated code for this send.
  
  slotDesc* sd = selfMapOop()->map_addr()->find_slot(sel);

  if (sd) {
    if (sd->is_obj_slot()) {
      if (sel->is_1arg_keyword()) {
        Location arg = exprStack->pop();
        theCodeGen->assignment(self, sd, arg);    
        allocs->deallocate(arg);
        result = pickStackLocAndMove(self);
        return true;
      } 
      else {
        // load value of a data slot 
        result = pickStackLoc();
        theCodeGen->loadOop(result, self, sd);
        return true;
      }
    } 
    else if (sd->is_map_slot() && !sd->data->is_method_like()) {
      result = pickStackLoc();
      theCodeGen->loadOop(result, sd->data);
      return true;
    }
  }
  return false;
}


void FSelfScope::selfCode() { 
  exprStack->append(pickStackLocAndMove(self)); 
}


void FSelfScope::local(slotDesc* sd, fint argc) { 
  Location l;
  oop m = method();
  if (sd->is_map_slot()) {
    // load constant slot
    assert(argc == 0, "must be an access");
    result = pickStackLoc();
    theCodeGen->loadOop(result, sd->data);
  } else {
    // slot access or assignment
    bool isArg = sd->is_arg_slot();
    if (isArg) {
      l = args->nth(sd - m->map()->slots());
    } else {
      assert(sd->is_obj_slot(), "unexpected slot type");
      l = locals->nth(sd - m->map()->slots());
    }
    if (argc == 0) {
      // access
      if (isArg) {
        // Don't need to actually move it to a register -- the expr stack
        // and args are read-only.  In debug mode, the expr stack elems
        // may be set by programming prims / conversions, so need a separate
        // location. (handed by pickStackLocAndMove)
        result = pickStackLocAndMove(l);
      } else {
        // local could be changed between here and it's use
        result = pickStackLoc();
        theCodeGen->move(result, l);
      }
    } else {
      // assignment
      Location arg = exprStack->pop();
      theCodeGen->move(l, arg);
      allocs->deallocate(arg);
      result = pickStackLocAndMove(self);
    } 
  }
}


// The genLocal calls generate a sequence of instructions to load the
// SP of the frame containing the local.  The last scope then loads
// (or stores) the value.

void FMethodScope::genLocal(FScope* target, FSelfScope* sender,
          slotDesc* sd, fint argc, bool first, Location /*temp_sp_reg*/) {
  Unused(sender); Unused(target);
  assert(target == this, "should have found the slot");
  if (first) {
    // normal local slot access
    FSelfScope::local(sd, argc);
  } else {
    ShouldNotReachHere(); // should be in VFrameMethodScope,
                          // not here (no inlining)
  }
}


void FLexicalScope::genLocal(FScope* target, FSelfScope* sender,
           slotDesc* sd, fint argc, bool first, Location temp_sp_reg) {
  if (target == this) {
    if (first) {
      // normal local slot access
      FSelfScope::local(sd, argc);
    } else {
      ShouldNotReachHere(); // should be in VFrameBlockScope, not here
    }
  } else {
    // load parent scope's SP, then continue
    theAssembler->Comment("lexically-scoped slot access");
    loadParentScope(temp_sp_reg, temp_sp_reg);
    parent()->genLocal(target, sender, sd, argc, false, temp_sp_reg);
  }
}


void FBlockScope::loadParentScope(Location dst_reg, Location /*temp_sp_reg*/) {
  // load first block's parent scope
  theCodeGen->loadBlockParent(receiver, dst_reg);
}


void FVFrameScope::genLocal(FScope* target, FSelfScope* sender,
          slotDesc* sd, fint argc, bool first, Location temp_sp_reg) {
  UsedOnlyInAssert(target); UsedOnlyInAssert(first);
  assert(!first, "just checking");
  assert(target == this, "should have found the slot");
  NameDesc* nd = _vf->get_name_desc(sd);
  assert(!nd->isValue(), "shouldn't call genLocal for constants");
  if (argc == 0) {
    sender->result = sender->pickStackLoc();
    theCodeGen->loadSaved(sender->result, nd->location(), temp_sp_reg,
        _vf);
  } else {
    Location arg = sender->exprStack->pop();
    theCodeGen->storeSaved(nd->location(), temp_sp_reg, _vf, arg);
    sender->allocs->deallocate(arg);
    sender->result = sender->pickStackLocAndMove(sender->self);
  }
}  


void FVFrameLexicalScope::genLocal(FScope* target, FSelfScope* sender,
           slotDesc* sd, fint argc, bool first, Location temp_sp_reg) {
  assert(!first, "just checking");
  if (target == this) {
    FVFrameScope::genLocal(target, sender, sd, argc, first, temp_sp_reg);
  } else {
    if (_vf->fr != parent()->vf()->fr) {
      loadParentScope(temp_sp_reg, temp_sp_reg);
    } else {
      // inlined in parent's frame, don't need to walk up the stack
    }
    parent()->genLocal(target, sender, sd, argc, false, temp_sp_reg);
  } 
}


void FVFrameBlockScope::loadParentScope(Location dst_reg, Location child_sp_reg) {
  if (_vf->fr != _parent->vf()->fr) {
    // load block's parent scope
    NameDesc* nd = _vf->get_block_name();
    theCodeGen->loadSaved(dst_reg, nd->location(), child_sp_reg, _vf);
    theCodeGen->loadBlockParent(dst_reg, dst_reg);
  } 
  else {
    // inlined in parent's frame, don't need to walk up the stack
  }
}


bool FSelfScope::checkPerformPrim_and_push_arguments(
          stringOop selector,
          LookupType& lookupType,
          LocationList*& argRegs) {
  if (!selector->is_string()) return false;
  
  fint len = selector->length();
  char* sel = selector->bytes();
  if (selector->has__Perform__prefix()) {
    return checkPerform_and_push_arguments(sel, len, 9, lookupType, argRegs);
  } 
  else if (selector->has__PerformResend__prefix()) {
    return checkPerform_and_push_arguments(sel, len, 15, lookupType, argRegs);
  }
  return false;
}


void FSelfScope::unconditionalBranch( int32 target_PC, bool isLoop) {
  materializeExprStack();
  Label* dst= branchTargets->nth(target_PC);
  Label* nlr = theCodeGen->unconditionalBranchCode( dst, 
                                                    isLoop && !frequentPreemption, 
                                                    allocs );
  if (nlr) handleNLR(nlr); // not a real NLR; used when checkInterrupt kills process
}


void FSelfScope::conditionalBranch( int32 target_PC, oop target_oop, bool isLoop) {
  Location toTest = exprStack->pop();
  materializeExprStack();
  Label* dst= branchTargets->nth(target_PC);
  Label* nlr = theCodeGen->conditionalBranchCode( toTest, 
                                                  target_oop, 
                                                  dst, 
                                                  isLoop && !frequentPreemption, 
                                                  allocs);
  allocs->deallocate(toTest);
  if (nlr) handleNLR(nlr); // not a real NLR; used when checkInterrupt kills process
}


void FSelfScope::indexedBranch( objVectorOop pcs, int32 pc ) {
  fint n= pcs->length();
  bool isLoop= false;
  LabelList* lbls = new LabelList(n);
  for (fint i = 0;  i < n;  ++i) {
    int32 tpc= smiOop(pcs->obj_at(i)) -> value();
    if ( tpc <= pc ) {
      isLoop = true;
      foundLoop = true;
    }
    lbls->nthPut(i, branchTargets->nth(tpc), true);
  }
  Location toTest = exprStack->pop();
  materializeExprStack();
  Label* nlr = theCodeGen->indexedBranchCode( toTest, 
                                              lbls, 
                                              isLoop && !frequentPreemption, 
                                              allocs);
  allocs->deallocate(toTest);
  if (nlr) handleNLR(nlr); // not a real NLR; used when checkInterrupt kills process
}


// when branching must be sure expr stack is in real locations
// Note: the carefull reader may wonder: what if compiler tries
// to move element that is already into one of the locations
// reserved for branch targets into another location reserved
// for branch target? Could overwrite?
// Well no--because once a value is materialized as stack location
//  n, it cannot later be needed as location m (m < n).
// So cannot overwrite. -- dmu

void FSelfScope::materializeExprStack() {
  for (int32 i = 0,  n = exprStack->length();  i < n;  ++ i) {
    Location dst;
    if (branchTargetExprStack->length() > i) 
      dst = branchTargetExprStack->nth(i);
    else {
      dst = allocs->pickPermanent(); // must be permanent so loc wont be deallocated
      branchTargetExprStack->append( dst );
    }
    Location whereItWas= exprStack->nth(i);
      theCodeGen->move(dst, whereItWas);
    exprStack->nthPut(i, dst);
    allocs->deallocate( whereItWas);
  }
}


bool FSelfScope::checkPerform_and_push_arguments(
            char* sel, fint len, fint prefix,
            LookupType& performLookupType,
            LocationList*& argRegs) {
  // check if this is really a perform primitive; if so, move selector
  // and delegatee to right registers and return perform type
  
  if (strncmp(sel + len - 7, "IfFail:", 7) == 0) {
    len -= 7;
    exprStack->pop();        // discard failure block
  }
    
      fint argc = 0;
  while (strncmp(sel + len - 5, "With:", 5) == 0) {
    argc ++;
    len -= 5;
  }

  fint extraArg = 0;
  if (len == prefix) {
    if (prefix == 9) {
      performLookupType = NormalPerformType;
    } else {
      performLookupType = ResendPerformType;
    }
  } else if (prefix == 9 && len == prefix + 13 
         &&  strncmp(sel + prefix, "DelegatingTo:", 13) == 0) { 
    performLookupType = DelegatedPerformType;
    extraArg = 1;
  } else {
    // not a recognized perform primitive name; don't replace with perform
    return false;
  }

  // push args
  argRegs = new LocationList(argc + extraArg + 2);  // 2 for sel + rcvr
  allocs->allocateArgs(argc + extraArg, false);
  Location l;
  for (fint i = 1; i <= argc; i++) {
    l = exprStack->pop();
    theCodeGen->loadArg(argc - i, l, false);
    argRegs->append(l);
  }
  
  if (extraArg) {
    // push delegatee
    l = exprStack->pop();
    theCodeGen->move(PerformDelegateeLoc, l);
    argRegs->append(l);
  }

  // push selector
  l = exprStack->pop();
  theCodeGen->move(PerformSelectorLoc, l);
  argRegs->append(l);

  return true;
}


Label* FSelfScope::primCall(stringOop selector, bool& canFail, fint arg_count) {
  PrimDesc* pd = getPrimDescOfSelector(selector);
  canFail = pd->canFail();
  if (selector == VMString[_RESTART]) {
    foundLoop = true;
    return theCodeGen->unconditionalBranchCode(startOfScope, true, allocs);
  } else {
    return theCodeGen->cPrimCall(pd, allocs, false, false, arg_count + 1 /* rcvr */);
  }
}


// generate code for a send; receiver hasn't been loaded yet
Label* FSelfScope::genCall(LookupType lookupType,
         Location rcvr, Location lastArg, fint argc, 
         stringOop selector, oop del) {
  // adjust lookupType if method holder isn't known
  Label* l = NULL;
  if (selector->is_prim_name()) {
    // primitive send
    if (!isPerformLookupType(lookupType)) 
      theCodeGen->loadArg(-1, rcvr, true);
    oop failSelector;
    Location failReceiver;
    if ( selector->has_IfFail() ) {
      selector = selector->without_IfFail();
      failSelector = VMString[VALUE_WITH_];
      failReceiver = lastArg;
      if (isPerformLookupType(lookupType)) argc--;    // ignore fail arg
    } else {
      failSelector = VMString[PRIMITIVE_FAILED_ERROR_NAME_];
      failReceiver = rcvr;
    }

    bool canFail;
    if (isPerformLookupType(lookupType)) {
      // perform
      canFail = false;
      l = theCodeGen->perform(allocs, lookupType, rcvr, self,
            argc, del);
    } else {
      l = primCall(selector, canFail, argc);
    }
      // failure handling
    if (canFail) {
        // make sure failure block is created if memoized
        blockOop failBlock = NULL;
        if (!blocks->isEmpty()) {
          BlockLocation bl = blocks->top();
          if (bl.loc == lastArg && bl.memoized) {
            // this memoized block is the failure block
            // (since we don't inline anything, block locations are unique)
            assert_block(bl.block, "should be a block");
            failBlock = bl.block;
          }
        }
        // gen code for failure send
        Label* failNLR =
              theCodeGen->primFailure(failReceiver, self, failSelector, selector,
                                      ResultReg,
                                    failBlock, allocs);
        l = l->unify(failNLR);
      }
  } else {
    // ordinary send
    l = theCodeGen->selfCall(allocs, lookupType, rcvr, self,
                             selector, del, argc);
  }
  return l;
}


FMethodScope::FMethodScope(bool dbg, MethodLookupKey *key, oop method, oop mh_or_map)
  : FSelfScope(dbg, key, method) {
  _methodHolder_or_map = mh_or_map;
  initialize();
}


void FMethodScope::genCode() {
  prologue();
  // self will have been saved in incoming arg location since it is first arg to a method
  // That's why we do nothing special here, unlike for the block below:
  scope = scopeDescs->addMethodScope(_key, _method, new LocationName(self),
             selfMapOop(), methodHolder_or_map());
  FSelfScope::genCode();
  epilogue();
}


void FBlockScope::genCode() {
  prologue();
  if (debugging) breakpoint();
  Location self_in_memory_for_uplevel_access         = theCodeGen->flushToStack(self,     allocs);
  Location block_in_memory_for_lexical_parent_access = theCodeGen->flushToStack(receiver, allocs);
  scope = scopeDescs->addTopLevelBlockScope(_key, _method,
              _key->receiverMapOop(),
              new LocationName( self_in_memory_for_uplevel_access ), new LocationName( self),
              selfMapOop(),
              methodHolder_or_map(), 
              new LocationName( block_in_memory_for_lexical_parent_access ), new LocationName( receiver),
              false, scopeID());
  FSelfScope::genCode();
  epilogue();
}


void FBlockScope::postPrologue() {
  // load self
  if (debugging) breakpoint();
  compiled_vframe* vf = parent()->vf();
  NameDesc* selfName = vf->get_self_name();
  if (selfName->hasLocation()) {
    loadParentScope(Temp1, IllegalLocation);
    theCodeGen->loadSaved(self, selfName->location(), Temp1, vf);
  } else {
    assert(!selfName->value()->is_block(), "must not be a block");
    theCodeGen->loadOop(self, selfName->value());
  }
}


void FDeadBlockScope::postPrologue() {
  // don't load parent scope - not necessary
}


void FSelfScope::descLocals(ScopeInfo scop) {
  fint len = args->length();
  assert(len == method()->map()->length_slots(), "just checkin'");

  FOR_EACH_SLOTDESC_N(method()->map(), s, i) {
    if (s->is_arg_slot()) {
      scopeDescs->addSlot(scop, i, new LocationName(args->nth(i)));
    } else if (!s->is_vm_slot() && s->is_obj_slot()) {
      Location l = locals->nth(i);
      assert(l != UnAllocated, "should have a location");
      scopeDescs->addSlot(scop, i, new LocationName(l));  // local data slot
    }
  }
}


void FMethodScope::genDescs() {
  descLocals(scope);
}


void FBlockScope::genDescs() {
  assert(method()->kind() == BlockMethodType, "just checkin'");
  descLocals(scope);
}


FLexicalScope::FLexicalScope(bool d, MethodLookupKey* key, oop method, FScope* p)
  : FSelfScope(d, key, method) { _parent = p; }

FBlockScope::FBlockScope(bool dbg, MethodLookupKey* key, oop method, FScope* p)
  : FLexicalScope(dbg, key, method, p) {
  initialize();
}         

FVFrameScope::FVFrameScope(bool d, compiled_vframe* f) : FScope(d) { _vf = f; }

FVFrameLexicalScope::FVFrameLexicalScope(bool dbg, compiled_vframe* f)
  : FVFrameScope(dbg, f) {
  compiled_vframe* parent = f->parent()->as_compiled();
  if (parent) {
    _parent = new_FVFrameScope(debugging, parent);
  } else {
    _parent = NULL;
  }
}

oop FVFrameScope::method() { return _vf->desc->method(); }
oop FVFrameScope::methodHolder_or_map() { return _vf->desc->methodHolder_or_map(); }

FDeadBlockScope::FDeadBlockScope(bool dbg, MethodLookupKey* key, oop method)
  : FBlockScope(dbg, key, method, NULL) {}

void FDeadBlockScope::genCode() {
  prologue();    // to create stack frame
  scope = scopeDescs->addDeadBlockScope(_key, _method, 
          new LocationName(receiver),
          false, scopeID());
  genDescs();
  scopeDescs->addPcDesc(theAssembler->offset(), scope, 0);
  theCodeGen->nonLifoTrap(allocs);
  // ignore rest of method - will never execute
  epilogue();
}


FVFrameScope* new_FVFrameScope(bool debugging, compiled_vframe* vf) {
  MethodKind kind = vf->method()->kind();
  switch (kind) {
   case OuterMethodType:
    return new FVFrameMethodScope(debugging, vf);
   case BlockMethodType:
    return new FVFrameBlockScope(debugging, vf);
   default:
    ShouldNotReachHere();
    return NULL;
  }
}
  

void FSelfScope::print() {
  lprintf(" method: %#lx\n\tdel: %#lx  id: %ld",
   method(), delegatee(), scopeID());
  lprintf("  args: "); args->print_short_zero();
  lprintf("\n\tlocals: "); locals->print_short_zero();
  lprintf("  stack: "); exprStack->print_short_zero();
  lprintf("  blocks: "); blocks->print_short_zero();
  lprintf("\n\tnlrPoints: "); nlrPoints->print_short_zero();
  lprintf("\n\branchTargets: "); branchTargets->print_short_zero();
  lprintf("\n\tallocated: "); allocs->print();
  lprintf("\n");
}


void FMethodScope::print_short() {
  lprintf("FMethodScope %#lx (", (unsigned long)this);
  selector()->print_oop();
  lprintf(")"); 
}


void FMethodScope::print() {
  print_short();
  FSelfScope::print();
  lprintf("\tmh: %#lx", methodHolder_or_map()); 
  lprintf("  sender: "); sender()->print_short_zero();
  lprintf("\n");
}


void FLexicalScope::print() {
  print_short();
  FSelfScope::print();
  lprintf("\tparent: "); parent()->print_short_zero();
  lprintf("  sender: "); sender()->print_short_zero();
  lprintf("\n");
}


void FBlockScope::print_short() {
  lprintf("FBlockScope %#lx (%#lx)", (unsigned long)this,
   (unsigned long)method()); 
}


void FDeadBlockScope::print_short() {
  lprintf("FDeadBlockScope %#lx (%#lx)", (unsigned long)this,
   (unsigned long)method()); 
}


void FVFrameScope::print() {
  print_short();
  lprintf("  _vframe_print(%#lx)", _vf);
}


void FVFrameMethodScope::print_short() {
  lprintf("FVFrameMethodScope %#lx (", (unsigned long)this);
  _vf->selector()->print_oop();
  lprintf(")");
}


void FVFrameMethodScope::print() {
  FVFrameScope::print();
  lprintf("\n");
}


void FVFrameLexicalScope::print() {
  FVFrameScope::print();
  lprintf("\n    parent: "); parent()->print_short_zero();
  lprintf("\n");
}


void FVFrameBlockScope::print_short() {
  lprintf("FVFrameBlockScope %#lx (", (unsigned long)this);
  _vf->selector()->print_oop();
  lprintf(" = %#lx)", (unsigned long)method());
}


void FVFrameBlockScope::print() {
  FVFrameLexicalScope::print();
  lprintf("\n");
}

# endif // FAST_COMPILER
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */



# pragma implementation "codeGen.hh"
# include "_codeGen.cpp.incl"

# ifdef FAST_COMPILER

  // don't make the constants below too small - the space is used only
  // during NIC-compilation, and there are some fairly big methods
  // out there -- Urs 8/93

  static const int32 FInstructionsSize  = 100 * K;
  static const int32 FInstLocationsSize =  20 * K;
  
  static const int32 FScopeDesc_size    = 20 * K;
  static const int32 FPcDesc_size       =  2 * K;
  
CodeGen::CodeGen(compilingLookup* l, sendDesc* sd, nmln* d) 
  : a(FInstructionsSize, FInstLocationsSize, PrintCompiledCode, true) {
  scopeDescs = new ScopeDescRecorder(FScopeDesc_size, FPcDesc_size);
  needToFlushRegWindow = haveStackFrame = false;
  frameSize = verifiedOffset = diCheckOffset = 0;
  L = l; send_desc = sd; diLink = d;
  theCodeGen = this;
  initialize_for_platform();
}

PrimDesc* CodeGen::intrCheck() {
  static PrimDesc* intCk = NULL;
  if (intCk == NULL)
    intCk = getPrimDescOfSelector(VMString[INTERRUPT_CHECK], true);
  return intCk;
}

PrimDesc* CodeGen::blockClone() {
  static PrimDesc* blkClone = NULL;
  if (blkClone == NULL)
    blkClone = getPrimDescOfSelector(VMString[BLOCK_CLONE], true);
  return blkClone;
}


Location CodeGen::loadPath(Location dest,
                           lookupTarget* target,
                           Location receiver,
                           Location temp_reg) {
  // *if root of path...
  //   *if holder is receiver...
  //     <move receiver, dest/t>
  //   *else...
  //     <loadOop obj, dest/t>
  //   *end
  // *else...
  //   <lookup dest/t, previousPath, receiver>
  // *end
  
  // Since I am called from CodeGen::assignment, I cannot clobber Temp2
  // Note called from codeGen::prologue, CodeGen::verifyParents, CodeGen:assignment, CodeGen::lookup
  // constrants: no frame in prologue case, 
    
  a.Comment("loadPath");
  Location t;
  t = isRegister(dest) ? dest : temp_reg;
  if (target->is_receiver()) {
    if (isRegister(receiver))     t = receiver;
    else                          move(t, receiver);
  } 
  else {
    assert(target->is_object(), "must be an object path search");
    objectLookupTarget* otarget = (objectLookupTarget*) target;
    if (otarget->prevTargetSlot) {
      lookup(temp_reg, otarget->prevTargetSlot, receiver);
      t = temp_reg;
    } 
    else {
      loadOop(t, otarget->obj);    // load oop
    }
  }
  a.Comment("end loadPath");
  return t;
}

# endif // FAST_COMPILER
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "blist.hh"
# include "_blist.cpp.incl"
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "debug.hh"
# include "_debug.cpp.incl"

# define DefineFlags(                                                         \
    flagName, flagType, flagTypeName, primReturnType,                         \
    initialValue, getCurrentValue, checkNewValue, setNewValue,                \
    explanation, wizardOnly)                                                  \
                                                                              \
    flagType flagName = initialValue;

FOR_ALL_DEBUG_PRIMS(DefineFlags)



void debug_init() {
  // initialize debug flags that use function calls
  DirPath = copy_c_heap_string("");   // so we can use free() when changing it
  SpyDisplay = copy_c_heap_string("");
  SpyFont = copy_c_heap_string("");
}

# undef DefineFlags



# if  GENERATE_DEBUGGING_AIDS

extern "C" void start_gdb_debugging() {
  // set up stuff for debugging a crashed process (call this after attach)
  // turn off timers etc. to prevent gdb crashes
  SignalBlocker sb(SignalBlocker::allow_user_int);
  static bool gdb_debugging_called = false;
  if (gdb_debugging_called) return;     // prevent multiple calls
  gdb_debugging_called = true;
  IntervalTimer::dont_use_any_timer = true;
  IntervalTimer::disable_all(true);
  TheSpy->deactivate();
  WizardMode = PrintOopAddress = true;
  flush_logFile();
}

extern "C" void redirect_stdio(char* filename) {
  // for debugging; redirect input/output to gdb window
  start_gdb_debugging();
  char devname[20];
  char* fn;
  static FILE* res;
  if (strlen(filename) == 2) {
    // shortcut: name of pseudo-tty from ps, e.g "p0" == "/dev/ttyp0"
    sprintf(devname, "/dev/tty%s", filename);
    fn = devname;
  } else {
    fn = filename;
  }
  res = freopen (fn, "a+", stdout);
  if (res) {
    lprintf("stdout redirected to %s.\n", fn);
  } else {
    lprintf("stdout couldn't be redirected to %s.\n", fn);
    perror("redirection failed");
  }
  res = freopen (fn, "a+", stderr);
  if (res) {
    lprintf("stderr redirected to %s.\n", fn);
  } else {
    lprintf("stderr couldn't be redirected to %s.\n", fn);
    perror("redirection failed");
  }
  res = freopen (fn, "r", stdin);
  if (res) {
    lprintf("stdin redirected to %s.\n", fn);
  } else {
    lprintf("stdin couldn't be redirected to %s.\n", fn);
    perror("redirection failed");
  }
  fflush(stderr);
}

#endif
/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "vframe.hh"
# include "_vframe.cpp.incl"

# define CHECK                                                                \
  if (e->is_target(obj)) {                                                    \
    Process* p = processes->stackFor(fr)->process;                            \
    e->add_obj(new_vframeOop(p, this)); return;                               \
  }                                                                           \
    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

dummy_vframe* new_dummy_vframe(frame* sender) {
  // create a dummy vframe for the callee
  if (!sender) sender = currentProcess->last_self_frame(false);
  nmethod* callee = NULL;
  ScopeDesc* desc = NULL;
  // try to fill dummy vf with decent information (get_contents will work
  // anyway, but printing etc will break)
  sendDesc* sd = sender->send_desc();
  if (sd) {
    CacheStub *pic= sd->pic();
    callee= pic ? NULL : sd->get_method();
  }
  return new dummy_vframe(sender, callee, desc, PrologueBCI);
}


void dummy_vframe::print() {
  lprintf("DUMMY ");
  if (desc) {
    compiled_vframe::print();
  } else {
    lprintf("*(dummy_vframe*)%#lx\n", this);
  }
}


void compiled_vframe::set_rl(RegisterLocator* r) {
  assert(r && r->fr() == fr, "");
  rl = r;
}


bool compiled_vframe::is_primCall() {
  // Works only if the vframe is in a call or uncommon trap.
  // Used to distinguish between prim call and failure block. 
  abstract_vframe* bottomVF = new_vframe(fr, reg_loc());
  if (this->EQ(bottomVF)) {
    sendDesc* sd = fr->send_desc();
    return sd && sd->isPrimCall();
  } else {
    // not bottom vframe in frame --> can't be a primitive call
    return false;
  }
}


bool compiled_vframe::is_uncommonTrap() {
  compiled_vframe* bottomVF = new compiled_vframe(fr, reg_loc());
  if (this->EQ(bottomVF)) {
    return fr->return_addr() == currentProcess->uncommonPC();
  } else {
    // not bottom vframe in frame --> can't be an uncommon trap
    return false;
  }
}


// first because of trouble with inlined functions
void compiled_vframe::set_contents(NameDesc* n, oop p) {
  if (n->isLocation()) {
    oop* addr = register_contents_addr(n->location());
    *addr = p;
    oop* addr2 = register_contents_secondary_addr(n->location());
    if (addr2 != NULL)  *addr2 = p;
  } else if (n->isMemoizedBlock()) {
    assert(p->is_block() &&
           blockOop(p)->scope(true) != (frame*) BLOCK_PROTO_SCOPE,
           "should be storing a real block into a memoized block location");
    oop* addr = register_contents_addr(n->location());
    *addr = p;
    oop* addr2 = register_contents_secondary_addr(n->location());
    if (addr2 != NULL)  *addr2 = p;
  } else if (n->isValue()) {
    assert(p == n->value(), "should be the same");
  } else {
    assert(n->isBlockValue(), "unexpected name desc");
    assert(p->is_block() && n->value()->is_block() &&
           blockOop(p)->value() == blockOop(n->value())->value(),
           "should be the same block");
  }
}


int32 compiled_vframe::bciFromDesc(ScopeDesc* dc) {
  PcDesc* d = code->containingPcDesc(fr->return_addr());
  int32 bci = d->byteCode;
  ScopeDesc* s = d->containingDesc(code);
  while (!s->is_equal(dc)) {
    bci = s->senderByteCodeIndex();
    assert(s->sender(), "sender must be present to continue");
    s = s->sender();
  }
  return bci;
}

void compiled_vframe::rlFromFrame() {
    set_rl(RegisterLocator::for_frame(fr));
}

bool compiled_vframe::isCallerOf(ScopeDesc* callee) {
  // true iff both are in same frame and this vframe could have
  // called the callee (note: callee may not be live anymore)
  ScopeDesc* s = callee->sender();
  while (s) {
    if (s->is_equal(desc)) return true;         // yes, I am in caller chain
    s = s->sender();
  }
  return false;
}


abstract_vframe* compiled_vframe::get_sender(bool skipC) {
  ScopeDesc* c = desc->sender();
  if (c != NULL) {
    int32 cbci = desc->senderByteCodeIndex();
    return new compiled_vframe(fr, code, c, cbci, reg_loc());
  }
  return abstract_vframe::get_sender(skipC);
}


// cannot have interp frame as parent

abstract_vframe* compiled_vframe::parent() {
  ScopeDesc* d = desc;
  ScopeDesc* c = d->parent();
  if (c == NULL) {
    if (d->isDeadBlockScope()) {
      // non-LIFO block
      return NULL;
    } else if (d->isTopLevelBlockScope()) {
      return abstract_vframe::parent();
    } else {
      assert(d->isHome(), "unexpected scope kind");
      return NULL;
    }
  } else {
    int32 cbci = bciFromDesc(c);
    return new compiled_vframe(fr, code, c, cbci, reg_loc());
  }
}



oop compiled_vframe::get_contents(NameDesc* n,
                                  bool cloneMemoizedBlock,
                                  bool cloneEliminatedBlock) {
  assert(verify_NameDesc_for_get_contents(n), "just checking");                                
  if (n->isLocation()) {
    Location loc = n->location();
    oop* addr = register_contents_addr(loc);
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
        // can report "oop is marked" during enumerations
        // (*addr)->verify();
    }
#   endif
    return *addr;
  }
  if (n->isValue()) {
    return n->value();
  } 
  if (n->isBlockValue()) {
    return
      cloneEliminatedBlock
      ? blockOop(n->value())->clone_block((smiOop)fr->block_scope_of_home_frame())
      : n->value();
  } 
  assert(n->isMemoizedBlock(), "unexpected name desc");
  oop* bp = register_contents_addr(n->location());
  oop b = *bp;
  if (b != NULL && b != Memory->deadBlockObj) {
    assert(b->is_block(), "should be a block");
    return b;
  }
  b = n->value();
  assert(b->is_block(), "should be a block"); // added for PPCSIC -- dmu 12/02
  // memoized block isn't cloned yet; do it now
  if (!cloneMemoizedBlock) {
    return b;
  }    
  b= blockOop(b)->clone_block((smiOop)fr->block_scope_of_home_frame());
  *bp = b;
  oop* bp2 = register_contents_secondary_addr(n->location());
  if (bp2 != NULL)  *bp2 = b;
  
  return b;
}


static oop* checkAddr;
static void checkAddrFn(oop* p) { if (p == checkAddr) checkAddr = NULL; }

bool compiled_vframe::verify_NameDesc_for_get_contents(NameDesc* n) {
  // check if location is marked live in register mask
      
  // NB: test for is_compiled_self_frame() is necessary because the frame's
  // return address may not have been set yet (e.g. during conversions);
  // isPrimCall covers ubDescs and (possibly non-mask) prims
  if ( ConversionInProgress 
                      // stack_locations_do may break
                      // for last copiedFrame (sender sp may be < own sp)
  ||   isDummy() 
  ||   !n->hasLocation() 
  ||   !fr->is_compiled_self_frame()
  ||   fr->send_desc() == NULL 
  ||   fr->send_desc()->isPrimCall())
          return true;

  frame* fr_to_search = NULL;
  RegisterLocator* rl_to_search = NULL;
  get_search_locations_for_liveness_check(n, fr_to_search, rl_to_search);        
  checkAddr = register_contents_addr(n->location()); 
  fr_to_search->stack_locations_do(checkAddrFn, rl_to_search);
          
  if (checkAddr != NULL) {
    n->print();
    error("location (above) isn't live; repeating for the debugger");        
    checkAddr = register_contents_addr(n->location()); 
    fr_to_search->stack_locations_do(checkAddrFn, rl_to_search);
    fatal("location (above) still isn't live");
  }

  checkAddr = register_contents_secondary_addr(n->location()); 
  if (checkAddr == NULL)
    return true;

  fr_to_search->stack_locations_do(checkAddrFn, rl_to_search);
  if (checkAddr == NULL)
    return true;

  n->print();
  error("location (above) isn't live; repeating for the debugger");        
  checkAddr = register_contents_secondary_addr(n->location()); 
  fr_to_search->stack_locations_do(checkAddrFn, rl_to_search);
  fatal("location (above) still isn't live");
  return false; // silence the compiler
}


void compiled_vframe::createBlock(NameDesc* n, OopOopTable*& blockValues) {
  // if the value described by n is an uncreated block, create it and add
  // it to the table of created blocks
  if (!(n->isBlockValue() || n->isMemoizedBlock())) return;
  oop p = get_contents(n, false);
  assert_block(p, "must be a block");

  oop clone = blockValues->lookup(p);
  if (clone == NULL) {
    if (blockOop(p)->scope(true) == NULL) {
      frame* home = fr->block_scope_of_home_frame();
      clone = blockOop(p)->clone_block(smiOop(home));
    } else {
      assert(n->isMemoizedBlock(), "must be a memoized block");
      clone = p;
    }
    blockValues = blockValues->storeNew(p, clone);
  }
}


void compiled_vframe::get_outgoing_arg_info_from_dummy_callee( 
                        bool               isReceiverExplicit,
                        compiled_vframe*   calleeOrNull,
                        compiled_vframe**  vfs,
                        NameDesc**         nds,
                        fint&              i,
                        smi                len,
                        oop                sel ) {
  // no info on outgoing args in scopeDesc - get them from callee
  // callee is a dummy vframe - must make up fake nameDescs
  // (would be cleaner to make up fake scopeDesc in new_dummy_vframe but
  // that's currently quite hard to do)
  if (isReceiverExplicit) {
    vfs[i] = calleeOrNull;
    nds[i] = new LocationNameDesc(IReceiverReg, 0);
    i++;
  }
  if (sel->is_string()) {
    if (stringOop(sel)->has__Perform__prefix()) {
      // arghhh!  performs are not primitive calls; selector is passed
      // in special register which isn't necessarily live anymore
      
      //  why not check for _PerformResend: too? -- dmu
      vfs[i] = this;
      nds[i] = new IllegalNameDesc();
      i++;
    }
  }
  for (fint argNo = 0; i < len;  i++, argNo++) {
    vfs[i] = calleeOrNull;
    nds[i] = new LocationNameDesc(IArgLocation(argNo), 0);
  }
}


// Only used for conversion...

void compiled_vframe::copyOutgoingArgs( compiled_vframe* vf,
                                        frame* oldBlkHome,
                                        OopOopTable*& blkValues,
                                        bool lastOnly ) {

  // copy the outgoing arguments of the current send; receiver/vf is/was the
  // most recent activation on the stack
  // if lastOnly, copy only the last expr stack element
  
  assert(desc->s_equivalent(vf->desc), "not the same method");

  IntListElem* e;
  bool isUncommon;
  fint startingArgNo; /* -1 for rcvr reg, 0 for first arg reg */
  fint performSelArgNo, performDelArgNo; // if a perform
  prepareToCopyOutgoingArgs(vf,
                            e, isUncommon, startingArgNo, performSelArgNo, performDelArgNo);
  
  dummy_vframe* dummy = new_dummy_vframe(NULL);

  if (lastOnly) 
    for ( ; e != NULL  &&  e->next(); 
            e = e->next(), ++startingArgNo) 
      {}

 for ( fint argNo = startingArgNo;  e;   e= e->next(),  ++argNo) {
    int32 bci2 = e->data();
    NameDesc* nd = desc->exprStackElem(bci2);
    assert(nd->hasLocation(), "cannot handle constants");
    NameDesc* from_nd = vf->desc->exprStackElem(bci2);
    
    if  (  argNo == performSelArgNo 
    ||     argNo == performDelArgNo) {
      copyValue(nd, vf, from_nd, oldBlkHome, blkValues);
      // do not advance argNo at the position of the
      //   selector or delegatee to a _Perform
      //   because will be copied to a special register
      --argNo;  --performSelArgNo;  --performDelArgNo;
    }
    else if ( !from_nd->isIllegal()
         &&   ( isUncommon  ||  argNo >= NumIArgRegisters ) ) {
      copyValue(nd, vf, from_nd, oldBlkHome, blkValues);
    }
    else {
      /* for verifying */
      if (currentProcess->isUncommon())
        fatal("should have legal nameDesc");

      copy_outgoing_arg( argNo,  from_nd,  vf,  dummy, nd,  oldBlkHome,  blkValues);
    }
  }
}


void compiled_vframe::prepareToCopyOutgoingArgs(
                         compiled_vframe* vf,
                         IntListElem*& e, bool& isUncommon,
                         fint& startingArgNo, fint& performSelArgNo, fint& performDelArgNo) {

  methodMap* mm = (methodMap*) method()->map();
  IntList* bcs = mm->expression_stack(bci(), true);
  isUncommon = vf->fr->send_desc() == NULL;
  fint op = getOp(mm->start_codes()[_bci]);
  startingArgNo = (op == SEND_CODE) ? -1 : 0;    // explicit or implicit self?
  stringOop sel= mm->get_selector_at(_bci);
  
  const fint none = -2;
  performSelArgNo= sel->has__Perform_prefix()                ?    startingArgNo + 1  :  none;
  performDelArgNo= sel->has__Perform_DelegatingTo__prefix()  ?  performSelArgNo + 1  :  none; 
    
  // nargs: # of args to copy (one more if explicit receiver)
  fint nargs= sel->arg_count() - startingArgNo;
  e= bcs->head();
  for (fint i = bcs->length() - nargs;  i > 0;  i--) {
    // pop all expr stack entries not belonging to current send
    e= e->next();
  }
}



void compiled_vframe::copyValue( NameDesc* n,
                                 compiled_vframe* fromVF,
                                 NameDesc* fromNd,
                                 frame* oldBlockHome,
                                 OopOopTable* blockValues) {
  copyValueTo(n, fromVF->copyValueFrom( this,
                                        fromNd,
                                        oldBlockHome,
                                        blockValues));
}


oop compiled_vframe::copyValueFrom( compiled_vframe* toVF,
                                    NameDesc* fromNd,
                                    frame* oldBlockHome,
                                    OopOopTable* blockValues) {
  assert(oldBlockHome, "need old block home frame");
  // NB: don't create block in get_contents because all blocks should
  // have been created at the beginning of the conversion process
  oop p = get_contents(fromNd, false);
  if (p->is_block() && !isDummy()) {
    if (fromNd->isBlockValue() || fromNd->isMemoizedBlock()) {
      // wasn't a real block in original frame; must be in blockValues
      oop clone = blockValues->lookup(p);
      assert(clone, "not found in blockValues");
      p = clone;
    } 
    frame* home = blockOop(p)->scope(true);
    if (home == NULL) {
      // dead block - don't touch it
      // should translate it's map, but it doesn't really matter so why bother
      assert(fromNd->hasLocation(), "must have a location");
    } else if (home == oldBlockHome &&
               desc->is_equal( code->scopes->at(blockOop(p)->desc()))) {
      // the block's home was in the old (now converted) frame and belonged
      // to my scope
      // need to translate block map & home pointer
      blockOop(p)->remap(toVF->code, toVF->fr);
    } else {
      // do nothing; a block from another frame
    }
  }
  return p;
}


void compiled_vframe::copyValueTo( NameDesc* n,  oop p ) {
  set_contents(n, p);
  LOG_EVENT2("compiled_vframe::copyValue %s %#lx",
             locationName(n->location()), p);
  assert(get_contents(n) == p, "contents not set correctly");
}


static frame* oldBlockHome;
static OopOopTable* blockValues;
static void copyValueFn( compiled_vframe* vf1, NameDesc* n1,
                         compiled_vframe* vf2, NameDesc* n2) {
  // copy a value from vf2 to vf1
  if (vf2) {
    vf1->copyValue(n1, vf2, n2, oldBlockHome, blockValues);
  } else {
    // must be an argument to the call that just returned - don't set
  }
}


void compiled_vframe::copyValuesFrom(
                                     compiled_vframe* vf,
                                     compiled_vframe* callee,
                                     frame* oldBlkHome,
                                     OopOopTable*& blkValues,
                                     bool wasInInterruptCheck) {
  oldBlockHome = oldBlkHome;
  blockValues  = blkValues;
  valuesDo(vf, callee, copyValueFn, wasInInterruptCheck);
  blkValues = blockValues;
}


static void createBlkFn(compiled_vframe* vf1,
                        NameDesc* n1,
                        compiled_vframe* vf2,
                        NameDesc* n2) {
  Unused(vf2); Unused(n2);
  vf1->createBlock(n1, blockValues);
}

void compiled_vframe::createBlocks(abstract_vframe* calleeOrNull,
                                   OopOopTable*& blkValues) {
  blockValues = blkValues;
  // wont work if callee is interp XXXX
  valuesDo(this, calleeOrNull->as_compiled(), 
           createBlkFn, false);
  blkValues = blockValues;
}




void compiled_vframe::valuesDo(compiled_vframe* vf,
                               compiled_vframe* calleeOrNull,
                               vfValueDoFn fn,
                               bool wasInInterruptCheck) {
  // call fn on all NameDescs in the receiver, passing along the corresponding
  // NameDesc in vf
  
  assert(desc->s_equivalent(vf->desc), "not the same method");
  fn(this, get_self_name(), vf, vf->get_self_name());
  fn(this, get_cachedSelf_name(), vf, vf->get_cachedSelf_name()); // may do redundant work on SPARC
  if (desc->isBlockScope()) {
    // copy over block receiver, too
    fn(this, get_block_name(),       vf, vf->get_block_name());
    fn(this, get_cachedBlock_name(), vf, vf->get_cachedBlock_name());
  }

  NameDesc* nd1 =     desc->getNextNameDesc(NULL);
  NameDesc* nd2 = vf->desc->getNextNameDesc(NULL);
  while (nd1 != NULL) {
     assert( nd2, "name desc length differs");
     fn(this, nd1,  vf, nd2);
     nd1 =     desc->getNextNameDesc(nd1);
     nd2 = vf->desc->getNextNameDesc(nd2);
  }
  assert( !nd2, "name desc length differs");

  // expression stack
  methodMap* mm = (methodMap*) method()->map();

  // don't visit arguments of sends if no callee is supplied; these
  // shouldn't be needed by the receiver vframe if/when resumed (extracting
  // them from optimized vframes requires looking at the called vf).
  // However: if returning from interruptCheck, still need to visit args.
  bool includeArgs = wasInInterruptCheck || calleeOrNull != NULL;
  assert(!wasInInterruptCheck || calleeOrNull==NULL,
         "interrupt check implies no callee");
  IntList* bcs = mm->expression_stack(bci(), includeArgs);
  assert(real_bci() != PrologueBCI  ||  bcs->length() == 0,
         "no expression stack elements when in prologue");
  compiled_vframe** fromVF;
  NameDesc** fromND;
  smi len;
  vf->get_exprStackInfo(calleeOrNull, fromVF, fromND, len, includeArgs);
  assert(len == bcs->length(), "oops");
  fint i = 0;
  IntListElem* e;
  for (e = bcs->head(); e; e = e->next(), i++) {
    int32 bci2 = e->data();
    NameDesc* n = desc->exprStackElem(bci2, false);
    if (n) {
      if (fromND[i]->isIllegal()) {
        // outgoing arg
      } else {
        fn(this, n, fromVF[i], fromND[i]);
      }
    } else {
      // trivial byte code, already set (e.g. incoming arg, const)
      assert(!code->isDebug(), "debug code should have real nameDesc");
    }
  }

  // live blocks
  bcs = mm->blocks_upto(bci());
  for (e = bcs->head(); e; e = e->next()) {
    int32 bci2 = e->data();
    NameDesc* n  =     desc->blockElem(bci2);
    NameDesc* n1 = vf->desc->blockElem(bci2);
    fn(this, n, vf, n1);
  }
}


void compiled_vframe::get_expr_stack(oop*& stack,
                                     smi& len,
                                     bool badOopForUnknown) {
  // Return the expression stack as seen by the user (i.e. including the
  // arguments to the current send which aren't actually stored in the
  // debugging info expression stack).

  // Don't return an objVector because this is used during _PrintProcessStack
  // and we don't want to disturb the Self heap when printing the stack.
  // stack[0] = oldest elem, stack[len-1] = most recent (top) elem
  compiled_vframe** vfs;
  NameDesc** nds;
  
  compiled_vframe* s = sendeeOrNULL_for_get_expr_stack();
  get_exprStackInfo(s, vfs, nds, len);

  stack = NEW_RESOURCE_ARRAY(oop, len);

  for (fint i = 0; i < len; i++) {
    compiled_vframe* vf = vfs[i];
    NameDesc*        nd = nds[i];
    stack[i] =  vf && !nd->isIllegal()  ?  vf->get_contents(nd)
              : badOopForUnknown        ?  oop(badOop)
                                        :  new_string("<unknown>");
  }
}
  
  
bool  compiled_vframe::is_top()  { return desc->isTop(); }
int32 compiled_vframe::scopeID() { return desc->scopeID(); }


NameDesc* compiled_vframe::get_name_desc(slotDesc* s, bool canFail) {
    return desc->slot(s, canFail); 
}
oop compiled_vframe::get_slot(slotDesc* s) {
  return get_contents(get_name_desc(s)); }

oop compiled_vframe::get_slot2(slotDesc* s, bool clone1, bool clone2) { 
  return get_contents(get_name_desc(s), clone1, clone2);
}
void compiled_vframe::set_slot(slotDesc* s,  oop x) {
    set_contents(get_name_desc(s), x); 
}
NameDesc* compiled_vframe::get_self_name() { return desc->self(); }
NameDesc* compiled_vframe::get_cachedSelf_name() { return desc->cachedSelf(); }
NameDesc* compiled_vframe::get_receiver_name() { return desc->receiver(); }
NameDesc* compiled_vframe::get_block_name() { return desc->block(); }
NameDesc* compiled_vframe::get_cachedBlock_name() { return desc->cachedBlock(); }


void compiled_vframe::get_exprStackInfo(compiled_vframe* calleeOrNull,
                                        compiled_vframe**& vfs,
                                        NameDesc**& nds,
                                        smi& len,
                                        bool includeArgs) {
  // return vframes and corresponding NameDescs needed to recover the
  // expression stack; on return, vfs[i] is either the receiver or the
  // vframe below the receiver (for outgoing args); if an expr stack
  // entry is unknown (e.g. for arguments to primitives), the corresponding
  // NameDesc is an IllegalNameDesc
  // elem 0 = oldest elem, elem len-1 = most recent (top) elem
  
  methodMap* mm = (methodMap*) method()->map();

  // bcs marks the descs which the debugging info can handle; bcsArgs
  // is the "logical" stack (including the args of the current send
  // if desired)
  IntList* bcs = mm->expression_stack(bci(), code->isDebug());
  bool same = code->isDebug() == includeArgs;
  IntList* bcsArgs =  same  ?  bcs  
                            :  mm->expression_stack(bci(), includeArgs);

  len             = bcsArgs->length();
  vfs             = NEW_RESOURCE_ARRAY(compiled_vframe*, len);
  nds             = NEW_RESOURCE_ARRAY(NameDesc*, len);
  IntListElem* e  = bcs->head();
  IntListElem* e2 = bcsArgs->head();
  int32 nargs     = same ? 0 : len - bcs->length();

  // get base expr stack from debugging info
  fint i;
  for (i = 0;  i < len && e;  e = e->next(), e2 = e2->next(), i++) {
    int32 bci2 = e->data();
    nds[i] = desc->exprStackElem(bci2);
    vfs[i] = this;
  }

  // push the args of the current send (if any)
  if (nargs > 0) {
    get_exprStackInfo_outgoing_args( mm,
                                     calleeOrNull,                                     
                                     vfs,
                                     nds,
                                     i,                                     
                                     len,
                                     e2);

  }
  else if (i != len) fatal("expr stack: wrong length?");
} 


void compiled_vframe::get_exprStackInfo_outgoing_args(
                        methodMap*          mm,
                        compiled_vframe*    calleeOrNull,
                        compiled_vframe**   vfs,
                        NameDesc**          nds,
                        fint                i,
                        smi                 len,
                        IntListElem*        e2) {
                        
  oop sel = mm->start_literals()[mm->get_index_at(bci())];
  ByteCodeKind op = getOp(mm->start_codes()[bci()]);
  bool isReceiverExplicit = op == SEND_CODE;
  
  if (   !currentProcess->isUncommon()
  &&     calleeOrNull
  &&     calleeOrNull->desc == NULL) {
  
    get_outgoing_arg_info_from_dummy_callee( isReceiverExplicit, 
                                             calleeOrNull,
                                             vfs, nds, i, len, sel );
  } 
  else if ( calleeOrNull == NULL
       ||   calleeOrNull->desc == NULL
       ||   calleeOrNull->desc->is_lite()
       ||   (isSendOp(op)  &&  sel->is_string() 
                           &&  stringOop(sel)->is_prim_name())) {
                          
    get_outgoing_arg_info_no_sendee( vfs, nds, i, len, e2 );
  } 
  else {
    get_outgoing_arg_info_from_sendee( isReceiverExplicit, 
                                       calleeOrNull,
                                       vfs, nds, i );
  }
  if (i != len) fatal("expr stack: wrong number of args?");
}


void compiled_vframe::get_outgoing_arg_info_no_sendee( 
                        compiled_vframe**  vfs,
                        NameDesc**         nds,
                        fint&              i,
                        smi                len,
                        IntListElem*       e2 ) {
  // prim call / conversion / ucommon / recursive lookup error -- no sendee
  // [NB: calleeOrNull == NULL isn't sufficient because some prims call
  // Self]
  // can't read the args out of O registers because (1) C code might reuse
  // these regs, and (2) they're not scavenged, and (3) they might not
  // contain what we want (e.g. uncommon branches have no callee)
  for (; i < len;  e2 = e2->next(), i++) {
    vfs[i] = this;
    nds[i] = desc->exprStackElem(e2->data());
  }
}


void compiled_vframe::get_outgoing_arg_info_from_sendee( 
                        bool               isReceiverExplicit, 
                        compiled_vframe*   calleeOrNull,
                        compiled_vframe**  vfs, 
                        NameDesc**         nds, 
                        fint&              i ) {
  // extract args from sendee
  if (isReceiverExplicit) {
    vfs[i] = calleeOrNull;
    nds[i] = calleeOrNull->get_receiver_name();
    i++;
  }
  const fint base= i;
  FOR_EACH_SLOTDESC(calleeOrNull->method()->map(), s) {
    if (s->is_arg_slot()) {
      oop ind= s->data;
      assert_smi(ind, "bad index");
      fint argIndex= smiOop(ind)->value();
      vfs[base+argIndex]= calleeOrNull;
      nds[base+argIndex]= calleeOrNull->get_name_desc(s);
      i++;
    }
  }
}


bool compiled_vframe::print_frame(fint curFrame) {
  if (desc->isDataAccessScope() || desc->isDataAssignmentScope()) return false;
  return abstract_vframe::print_frame(curFrame);
}
  

void compiled_vframe::enumerate_references(enumeration* e) {
  if (desc->isDataAccessScope() || desc->isDataAssignmentScope()) return;

  abstract_vframe::enumerate_references(e);

  oop meth = method();
  methodMap* mm = (methodMap*) meth->map();

  IntList* bcs = mm->expression_stack(bci(), code->isDebug());
  for (IntListElem* el = bcs->head(); el; el = el->next()) {
    int32 bci2 = el->data();
    NameDesc* n = desc->exprStackElem(bci2);
    oop obj = get_contents(n);
    CHECK;
  }

  IntList* bbcs = mm->blocks_upto(bci());
  for (IntListElem* b = bbcs->head();  b;  b = b->next()) {
    int32 bci2 = b->data();
    NameDesc* n = desc->blockElem(bci2);
    oop obj = get_contents(n);
    CHECK;
  }
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

interpreted_vframe::interpreted_vframe(frame* f) {
  assert(f->is_interpreted_self_frame(), "must be right frame");
  fr = f;
}

bool interpreted_vframe::is_primCall() {
  return interp()->selToSend->is_string() && stringOop(interp()->selToSend)->is_prim_name();
}


fint abstract_vframe::bci() {
  methodMap* mm = (methodMap*) method()->map();
  if (real_bci() == PrologueBCI) {
    return abstract_interpreter_method_info(mm).firstBCI();
  } else if (real_bci() == EpilogueBCI) {
    return mm->length_codes();
  } else {
    return real_bci();
  }
}

bool abstract_vframe::is_prologue() {
  return real_bci() == PrologueBCI;
}


abstract_vframe* abstract_vframe::get_sender(bool skipC) {
  frame* f = skipC ? fr->selfSender() : fr->sender();
  if ( f == NULL  ||  !f->is_self_frame() )
    return NULL;
  return  new_vframe(f, is_compiled() ? as_compiled()->reg_loc()->climb_to_frame(f) : NULL);
}

// factor out common case

abstract_vframe* abstract_vframe::parent() {
  blockOop b = (blockOop) block();
  assert(b->is_block(), "should be a block");
  frame* bs = b->scope(true);
  return
    bs == NULL
      ?  NULL
      :  new_vframe( bs->home_frame_of_block_scope(),
                     b->desc(),
                     NULL); // do NOT pass in rl of this vframe; may not be same stack
}


abstract_vframe* interpreted_vframe::parent() {
  if (interp()->mi.map()->kind() != BlockMethodType)
    return NULL;
  return interp()->parentVF();
}

    
// returns NULL if not found

abstract_vframe* abstract_vframe::sendee(abstract_vframe* last) {
  if (!last) {
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    if (    RecompilationInProgress 
        &&  (     currentProcess->stack()->contains((char*)fr->vfo_locals_of_home_frame()) 
             ||  resources.contains((char*)fr->vfo_locals_of_home_frame()))
        && this->EQ(  new_vframe( currentProcess->last_self_frame(false) )   )) {
      // special case to cover expr. stack of youngest vframe (incl.
      // outgoing args)
      // create a dummy vframe for the nmethod overflowing its counter;
      // the assembly glue (Recompile_stub)
      // guarantees that incoming args are ok
      frame* vmfr = currentProcess->stack()->first_VM_frame();
      last = new_dummy_vframe(vmfr);
      return last;
    }
#   endif //  defined(FAST_COMPILER) || defined(SIC_COMPILER)

    frame* f = processes->stackFor(fr)->last_self_frame(true);
    last = new_vframe(f);
  }
  // find right frame first
  frame* next = NULL;
  frame* lastFr;
  for ( lastFr = last->fr;
       lastFr && lastFr != fr && (next = lastFr->selfSender()) != fr;
       lastFr = next) ;
  if (lastFr == NULL) return NULL; // rcvr must be in copied frame (conversion)
  if (lastFr != last->fr) last = new_vframe(lastFr);
  // now find right vframe
  abstract_vframe* vf = last;
  abstract_vframe* vfs;
  for ( vfs = vf->sender();
       vfs && !vfs->EQ(this);
       vf = vfs, vfs = vfs->sender()) ;
  return vfs == NULL ? NULL : vf;
}



abstract_vframe* abstract_vframe::home() {
  abstract_vframe* v = this;
  for (abstract_vframe* p = v->parent(); p; p = p->parent()) {
    v = p;
  }
  return v;
}

abstract_vframe* abstract_vframe::top() {
  abstract_vframe* topVF = this;
  while (!topVF->is_top()) topVF = topVF->sender();
  return topVF;
}


oop interpreted_vframe::get_slot(slotDesc* sd) {
  return interp()->get_slot(sd);
}

void interpreted_vframe::set_slot(slotDesc* sd, oop x) {
  interp()->set_slot(sd, x);
}

void interpreted_vframe::createBlocks(abstract_vframe* calleeOrNull,
                                      OopOopTable*& blkValues) {
  Unused(calleeOrNull);
  for ( int32 i = 0;  i < interp()->mi.length_literals;  ++i) {
    oop b = interp()->mi.literals[i];
    if (!b->is_block()) continue;
    oop* p = &interp()->cloned_blocks[i];
    if (*p == NULL) {
      frame* home = fr->block_scope_of_home_frame();
      *p = blockOop(b)->clone_block(smiOop(home));
    }
    blkValues = blkValues->storeNew(b, *p);
  }
}

void interpreted_vframe::get_expr_stack(oop*& stack,
                                     smi& len,
                                     bool badOopForUnknown) {
  // Return the expression stack as seen by the user.

  // Don't return an objVector because this is used during _PrintProcessStack
  // and we don't want to disturb the Self heap when printing the stack.
  // stack[0] = oldest elem, stack[len-1] = most recent (top) elem

  Unused(badOopForUnknown);
  len = interp()->sp;
  stack = NEW_RESOURCE_ARRAY(oop, len);
  copy_oops(interp()->stack, stack, len);
}



bool abstract_vframe::print_frame(fint curFrame) {
  oop meth = method();
  methodMap* mm = (methodMap*) meth->map();
  print_code(curFrame);
  
  stringOop file = mm->file();
  if (file->length() > 0) {
    lprintf(" (");
    file->string_print();
    lprintf(":%ld): ", long(mm->line()->value()));
  } else {
    lprintf(" ");
  }
  
  if (selector()->is_string()) {
    stringOop(this->selector())->string_print();
  } else {
    selector()->print_oop();
  }
  
  lprintf(" = ");

  print_contents();
  
  if ( WizardMode
      && (    is_interpreted()  
#       if defined(FAST_COMPILER) || defined(SIC_COMPILER)
           || !as_compiled()->desc->is_lite()
#       endif
     )) {
    smi len;
    oop* stack;
    get_expr_stack(stack, len);
    if (len) {
      lprintf("    expression stack: { ");
      for (smi i = 0; i < len; i++) {
        stack[i]->print_oop();
        if (i < len - 1) lprintf(", ");
      }
      lprintf(" }\n");
    }
    lprintf("    current byte code:\n\t%ld: ", long(real_bci()));
    if (bci() == real_bci()) mm->print_byteCode_at(bci());
    lprintf("\n");
  }  
  lprintf("\n");
  
  OS::check_events();  // Try to catch interruptions sooner 
  return true;
}


void abstract_vframe::print_contents() {
  bool first = true;
  methodMap* mm = (methodMap*) method()->map();
  if (mm->kind() != OuterMethodType) {
    lprintf("[ ");
    if (WizardMode) {
      first = false;
      lprintf("| []* = ");
      if (parent() == NULL ) {
        lprintf("<dead>. ");
      } else {
        if (ConversionInProgress) {
          // cannot walk the stack to get parent
          lprintf(" N/A ");
        } else {
          abstract_vframe* v = parent();
          if (v) {
            lprintf("<%#lx", (v->fr));
#           if defined(FAST_COMPILER) || defined(SIC_COMPILER)
              if (v->is_compiled())
                lprintf(" # %lx",
                       (long unsigned)(v->as_compiled()->desc->offset()));
#           endif
            lprintf(">. ");
          } else {
            // shouldn't really happen, but be robust for better debugging
            lprintf("<dead>. ");
          }
        }
      }
    }
  } else {
    lprintf("( ");
    if (mm->kind () == OuterMethodType) {
      slotDesc *s= mm->find_slot(VMString[SELF]);
      if (s) {
        first = false;
        lprintf("| ");
        print_slot(s, method());
      }
    }
  }
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (is_compiled()  &&  as_compiled()->desc->is_lite()) {
    lprintf("<# LITE #>");
  }
  else
# endif
  {
    // print arg slots in order
    fint argc= mm->arg_count();
    for (fint argIndex=0;  argIndex < argc;  ++argIndex) {
      FOR_EACH_SLOTDESC(mm, s) {
        if (s->is_arg_slot() && argIndex == smiOop(s->data)->value()) {
          if (first) {
            first= false;
            lprintf("| ");
          }
          print_slot(s, method());
          break;
        }
      }
    }
      
    FOR_EACH_SLOTDESC(mm, s) {
      if (s->is_vm_slot() || s->is_arg_slot()) continue;
      if (first) {
        first = false;
        lprintf("| ");
      }
      print_slot(s, method());
    }
    if (! first) lprintf("| ");
  }
  
  mm->print_source();
  
  if (mm->kind() != OuterMethodType) {
    lprintf(" ]\n");
  } else {
    lprintf(" )\n");
  }
  lprintf("\n");
}
  

void abstract_vframe::print_slot(slotDesc* s, oop meth) {
  s->printAugmentedName();
  lprintf(s->is_obj_slot() ? " <- " : " = ");
  oop p = get_slot(s);
  p->print_oop();
  if (s->is_obj_slot()) {
    oop orig_p = meth->get_slot(s);
    if (orig_p != p) {
      lprintf(" \"");
      orig_p->print_oop();
      lprintf("\"");
    }
  }
  lprintf(". ");
}

void abstract_vframe::enumerate_references(enumeration* e) {
  oop meth = method();
  methodMap* mm = (methodMap*) meth->map();
  
  {
    oop obj = methodHolder_object();
    CHECK;
    obj = receiver();
    CHECK;
    obj = selector();
    CHECK;
    if (delegatee() != NULL) {
      obj = delegatee();
      CHECK;
    }
    obj = as_smiOop(bci());
    CHECK;
  }
  FOR_EACH_SLOTDESC(mm, s) {
    if (s->is_obj_slot()) {
      if (e->targets_has_assignments()) {
        Process* p = processes->stackFor(fr)->process;
        e->add_obj(new_vframeOop(p, this));
        return;
      }
    } else if (!s->is_vm_slot()) {
      oop obj = get_slot(s);
      CHECK;
    }
  }
}


void interpreted_vframe::enumerate_references(enumeration* e) {

  abstract_vframe::enumerate_references(e);

  for (oop* p = interp()->stack;  p < &interp()->stack[interp()->sp];  ++p) {
    oop obj = *p;
    CHECK;
  }
}

void abstract_vframe::enumerate_families(enumeration* e) {
  mapOop m      = method()->map()->enclosing_mapOop();
  oop* maps     = e->get_maps();
  smi  num_maps = e->get_num_maps();
  smi  hit_num;

  maps[num_maps] = m; // Place sentinel
  oop* matching_cell = (oop*) vectorfind_next_match((int32*) maps, 
                                                    (int32*) &m, 1,
                                                    (int32*) &hit_num);
  assert( matching_cell <= &maps[num_maps], "match out of area");
  if (matching_cell != &maps[num_maps]) {
    Process* p = processes->stackFor(fr)->process;
    e->add_obj(new_vframeOop(p, this));
  }
}

abstract_vframe* new_vframe(frame* f, RegisterLocator* r) {
  if ( f->is_interpreted_self_frame() )
    return  new interpreted_vframe(f);

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  assert(f->is_compiled_self_frame(), "f must be a self frame");
  if ( r == NULL )
    r = RegisterLocator::for_frame(f);
  return  new compiled_vframe(f, r);

# else

  ShouldNotReachHere();
  return (abstract_vframe*)NULL;

# endif
}


abstract_vframe* new_vframe(frame* f, smiOop offset, RegisterLocator* r) {
  if ( f->is_interpreted_self_frame() )
    return  (new interpreted_vframe(f) ) -> as_abstract();

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    assert(f->is_compiled_self_frame(), 
           "must not call with in-between C frame");
    if ( r == NULL )
      r = RegisterLocator::for_frame(f);
    RegisterLocator* rr = r == NULL  ?  RegisterLocator::for_frame(f)  :  r->climb_to_frame(f);
    return (new compiled_vframe(f, offset, rr)) -> as_abstract();

# else

    ShouldNotReachHere();
    Unused(offset);
    return NULL;

# endif
}


int32 interpreted_vframe::real_bci() { return interp()->pc; }

oop  interpreted_vframe::selector()     { return interp()->selector; }
oop  interpreted_vframe::delegatee()    { return interp()->delegatee; }
oop  interpreted_vframe::method()       { return interp()->method_object; }
oop  interpreted_vframe::methodHolder_or_map() {
  abstract_vframe* p = parent();
  return p == NULL ? interp()->methodHolder() : p->home()->methodHolder_or_map(); }

oop interpreted_vframe::self()     { return interp()->self; }
oop interpreted_vframe::receiver() { return interp()->receiver; }
oop interpreted_vframe::block()    { return interp()->receiver; }
oop interpreted_vframe::methodHolder_object(){ return methodHolder_or_map(); }
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "table.hh"
# include "_table.cpp.incl"

TableElem* Table::find(CListEntry* k) {
  assert(k, "shouldn't be comparing zeros");
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    assert(d->key, "shouldn't have a zero key");
    if (d->key->EQ(k)) {
      return d;
    }
  }
  return NULL;
}

TableElem* Table::identityFind(CListEntry* k) {
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    if (d->key == k) {
      return d;
    }
  }
  return NULL;
}

TableElem* Table::findContents(CListEntry* v) {
  assert(v, "shouldn't be comparing zeros");
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    if (d->value && d->value->EQ(v)) {
      return d;
    }
  }
  return NULL;
}

TableElem* Table::identityFindContents(CListEntry* v) {
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    if (d->value == v) {
      return d;
    }
  }
  return NULL;
}

void Table::remove(CListEntry* k) {
  assert(k, "shouldn't be comparing zeros");
  for (TableListElem* e = head(), *pe = NULL; e; pe = e, e = e->next()) {
    TableElem* d = e->data();
    assert(d->key, "shouldn't have a zero key");
    if (d->key->EQ(k)) {
      spliceOutNext(pe);
      break;
    }
  }
}

void Table::identityRemove(CListEntry* k) {
  for (TableListElem* e = head(), *pe = NULL; e; pe = e, e = e->next()) {
    TableElem* d = e->data();
    if (d->key == k) {
      spliceOutNext(pe);
      break;
    }
  }
}

Table* Table::copy() {
  Table* t = EMPTY;
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    t = t->append(new TableElem(d->key, d->value));
  }
  return t;
}

CListEntry* Table::realDeepCopy() {
  Table* t = EMPTY;
  for (TableListElem* e = head(); e; e = e->next()) {
    TableElem* d = e->data();
    t = t->append(new TableElem(d->key->realDeepCopy(), 
                                d->value->realDeepCopy()));
  }
  return t;
}
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "callgraph.hh"
# include "_callgraph.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

// Generalize receiver object merge:
//  add primitive _ProfilerMergeReceivers: <vectorOfVectors>

inline bool is_boolean_map(oop m) {
  if (Memory-> true_mapOop() == m) return true;
  if (Memory->false_mapOop() == m) return true;
  return false;
}

bool is_equal(oop m, oop n) {
  if (is_boolean_map(m) && is_boolean_map(n) ) return true;
  return mapOop(m)->equal( mapOop(n));
}

call_graph_node::call_graph_node() {
  edges = NULL;
  my_new_list = NULL;

  block_clones    = 0;
  bytes_allocated = 0;

  user_time = 0.0;
  optimized_user_time = 0.0;
  # if TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
    uncertain_user_time = 0.0;
  # endif
}

call_graph_node::~call_graph_node() {
  if (my_new_list)  my_new_list->remove(this);
  if (edges) delete edges;
}

void call_graph_node::add_edge(call_graph_edge* e) {
  e->next = edges;
  edges = e;
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  profilers->current_for_debugging()->root_edge->callee->edges->next) {
       lprintf("creating multiple roots; will not work\n");
       // graph_dumper(profilers->current_for_debugging()->root_edge, 0.0);
       fatal("Profiler problem: attempting to create multiple roots which will cause profiler to return only a partial graph");
    }
  # endif
}

# define FOREACH_EDGE for(call_graph_edge* e = edges; e; e = e->next)

call_graph_edge* call_graph_node::find_matching(int bci, call_graph_node* n) {
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->is_matching_node(bci, n))
      return e;
  }
  return NULL;
}

call_graph_edge* call_graph_node::find_matching(int bci, stringOop sel) {
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->is_matching_sel(bci, sel))
      return e;
  }
  return NULL;
}

call_graph_edge* call_graph_node::find_leaf(int bci) {
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->is_matching_leaf(bci))
      return e;
  }
  return NULL;
}

fold_edge* call_graph_node::find_fold_edge(int bci, call_graph_node* n) {
  FOREACH_EDGE {
    if (e->is_fold_edge() && e->is_matching_node(bci, n))
      return (fold_edge*) e;
  }
  return NULL;
}    

int call_graph_node::num_of_edges(float cutoff) {
  int i = 0;
  FOREACH_EDGE { if (e->is_significant(cutoff))  ++i; }
  return i;
}

static int compare_edges(const void* p1,  const void* p2) {
  call_graph_edge* a = *(call_graph_edge**)p1;
  call_graph_edge* b = *(call_graph_edge**)p2;

  // longest time first
  if (a->callee->user_time > a->callee->user_time)  return -1;
  if (a->callee->user_time < a->callee->user_time)  return  1;

  if (a->bci < b->bci)           return -1;
  if (a->bci > b->bci)           return  1;
  if (a->callee->is_leaf_node()) return -1;
  if (b->callee->is_leaf_node()) return  1;  
  return 0;
}

// Memory management

static void scavenge_oop(oop* p) { SCAVENGE_TEMPLATE(p); }
void call_graph_node::scavenge_contents() {
  oops_do(scavenge_oop);
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->callee) 
      e->callee->scavenge_contents();
  }
}
void call_graph_node::scavenge_contents_quickly() {
  oops_do(scavenge_oop);
}

static void mark_oop(oop* p) { MARK_TEMPLATE(p); }
void call_graph_node::gc_mark_contents() {
  oops_do(mark_oop);
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->callee) 
      e->callee->gc_mark_contents();
  }
}

static void unmark_oop(oop* p) { UNMARK_TEMPLATE(p); }
void call_graph_node::gc_unmark_contents() {
  oops_do(unmark_oop);
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->callee) 
      e->callee->gc_unmark_contents();
  }
}

static oop s_from, s_to;
static void switch_oop(oop *p) { if (*p == s_from) *p = s_to; }
void call_graph_node::switch_pointers(oop from, oop to) {
  s_from = from; s_to = to;
  oops_do(switch_oop);
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->callee) 
      e->callee->switch_pointers(from, to);
  }
}

static bool verify_oop_result = true;
static void verify_oop(oop *p) { bool verify_result = true; VERIFY_TEMPLATE(p); verify_oop_result = verify_result; }
bool call_graph_node::verify() {
  oops_do(verify_oop);
  bool r = verify_oop_result;
  FOREACH_EDGE {
    if (!e->is_fold_edge() && e->callee)
      r &= e->callee->verify();
  }
  return r;
}

// ---- prim_node

prim_node::prim_node(oop sel) : call_graph_node() {
  _selector = (stringOop) sel;
}

prim_node::prim_node(prim_node* pn) : call_graph_node() {
  _selector = pn->selector();
} 

bool prim_node::is_new() {
  return selector()->is_new();
}

bool prim_node::is_matching(call_graph_node* n) {
  if (!n->is_prim_node()) return false;
  return selector() == ((prim_node*) n)->selector();
}

call_graph_node* prim_node::clone() {
  call_graph_node* r = (call_graph_node*) new prim_node(this);
  if (my_new_list) my_new_list->insert(r);
  return r;
} 

void prim_node::print_node(bool verbose) {
  Unused(verbose);
  lprintf("%s", selector_string(selector()));
}

void prim_node::oops_do(oopsDoFn f) {
  OOPS_DO_TEMPLATE(&_selector,f);
}

// ---- leaf_node

leaf_node::leaf_node() : call_graph_node() {}

call_graph_node* leaf_node::clone() {
  call_graph_node* r = (call_graph_node*) new leaf_node();
  if (my_new_list) my_new_list->insert(r);
  return r;
} 

void leaf_node::print_node(bool verbose) {
  Unused(verbose);
  lprintf("<send>");
}

// ---- method_node

method_node::method_node(ScopeDesc* sd) : call_graph_node() {
  _selector          = (stringOop) sd->key.selector;
  _receiverMapOop    = sd->selfMapOop();
  _method            = sd->method();

  // If methodHolder is an oop replace it with the map.
  oop mh            = sd->methodHolder_or_map();
  _methodHolder_map = mh->is_map() ? mh : mh->map()->enclosing_mapOop();
}

method_node::method_node(method_node* mn) : call_graph_node() {
  _selector        = mn->selector();
  _receiverMapOop  = mn->receiverMapOop();
  _methodHolder_map   = mn->methodHolder_map();
  _method          = mn->method();
}

bool method_node::is_new() {
  return    selector()->is_new()
         || receiverMapOop()->is_new()
         || methodHolder_map()->is_new()
         || method()->is_new();
}

bool method_node::is_matching(call_graph_node* n) {
  if (!n->is_method_node()) return false;

  method_node* mn = (method_node*) n;
  if (selector()      != mn->selector())      return false;
  if (!is_equal(receiverMapOop(), mn->receiverMapOop()))  return false;
  if (methodHolder_map() != mn->methodHolder_map()) return false;
  return true;  
}

call_graph_node* method_node::clone() {
  call_graph_node* r = (call_graph_node*) new method_node(this);
  if (my_new_list) my_new_list->insert(r);
  return r;
}

void method_node::print_node(bool verbose) {
  oop m = method();
  if (m) {
    printName((methodMap*) m->map(), selector());
  } else {
    lprintf("method %s", selector_string(selector()));
  }
  if (verbose) {
    lprintf(" {rec = 0x%lx, mh = 0x%lx}", (receiverMapOop()),
           (long unsigned)(methodHolder_map()));
  }
}

void method_node::oops_do(oopsDoFn f) {
  OOPS_DO_TEMPLATE(&_selector,f);
  OOPS_DO_TEMPLATE(&_receiverMapOop,f);
  OOPS_DO_TEMPLATE(&_methodHolder_map,f);
  OOPS_DO_TEMPLATE(&_method,f);
}

// ---- access_node

access_node::access_node(ScopeDesc* sd) : call_graph_node() {
  _selector        = (stringOop) sd->key.selector;
  _receiverMapOop  = sd->selfMapOop();
  _methodHolder_map   = sd->methodHolder_or_map();
  // If methodHolder is an oop replace it with the map.
  if (!_methodHolder_map->is_map())
    _methodHolder_map = _methodHolder_map->map()->enclosing_mapOop();
}

access_node::access_node(access_node* an) : call_graph_node() {
  _selector        = an->selector();
  _receiverMapOop  = an->receiverMapOop();
  _methodHolder_map   = an->methodHolder_map();
}

bool access_node::is_new() {
  return    selector()->is_new()
         || receiverMapOop()->is_new()
         || methodHolder_map()->is_new();
}

bool access_node::is_matching(call_graph_node* n) {
  if (!n->is_access_node()) return false;

  access_node* an = (access_node*) n;
  if (selector()      != an->selector())       return false;
  if (!is_equal(receiverMapOop(), an->receiverMapOop()))   return false;
  if (methodHolder_map() != an->methodHolder_map()) return false;
  return true;  
}

call_graph_node* access_node::clone() {
  call_graph_node* r = (call_graph_node*) new access_node(this);
  if (my_new_list) my_new_list->insert(r);
  return r;
}

void access_node::print_node(bool verbose) {
  Unused(verbose);
  lprintf("access %s", selector_string(selector()));
}

void access_node::oops_do(oopsDoFn f) {
  OOPS_DO_TEMPLATE(&_selector,f);
  OOPS_DO_TEMPLATE(&_receiverMapOop,f);
  OOPS_DO_TEMPLATE(&_methodHolder_map,f);
}

// ---- block_node

block_node::block_node(ScopeDesc* sd) : call_graph_node() {
  _selector          = (stringOop) sd->key.selector;
  _method            = sd->method();
}

block_node::block_node(block_node* bn) : call_graph_node() {
  _selector          = bn->selector();
  _method            = bn->method();
}

bool block_node::is_new() {
  return selector()->is_new() || method()->is_new();
}

bool block_node::is_matching(call_graph_node* n) {
  if (!n->is_block_node()) return false;

  block_node* bn = (block_node*) n;
  if (selector()  != bn->selector()) return false;
  if (method()    != bn->method())   return false;
  return true;
}

call_graph_node* block_node::clone() {
  call_graph_node* r = (call_graph_node*) new block_node(this);
  if (my_new_list) my_new_list->insert(r);
  return r;
}

oop block_node::outer_method() {
  methodMap* mm = (methodMap*) method()->map();
  if (!mm->has_code()) return Memory->nilObj;
  objVectorOop lit = mm->literals();
  slotDesc* methodPointerSlot = lit->find_slot(VMString[METHOD_POINTER]);
  if (methodPointerSlot)
    return lit->get_slot(methodPointerSlot);
  else
    return Memory->nilObj;
}

void block_node::print_node(bool verbose) {
  printName((methodMap*) method()->map(), selector());
  if (verbose) {
    lprintf(" {method = 0x%lx}", (unsigned long)method());
  }
}

void block_node::oops_do(oopsDoFn f) {
  OOPS_DO_TEMPLATE(&_selector,f);
  OOPS_DO_TEMPLATE(&_method,f);
}

// ---- call_graph_edge

call_graph_edge::call_graph_edge(int bci_arg, call_graph_node* callee_arg) {
  bci    = bci_arg;
  callee = callee_arg;
  next   = NULL;
}

call_graph_edge::~call_graph_edge() {
  if (callee) delete callee;
  if (next)   delete next;
}

bool call_graph_edge::is_matching_node(fint bci_arg, call_graph_node* n) {
  return bci == bci_arg && callee->is_matching(n);
}

bool call_graph_edge::is_matching_sel(fint bci_arg, stringOop sel) {
  if (!callee->is_prim_node()) return false;
  return bci == bci_arg && ((prim_node*) callee)->is_matching_sel(sel);
}

bool call_graph_edge::is_matching_leaf(fint bci_arg) {
  return bci == bci_arg && callee->is_leaf_node();
}

// ---- fold_edge

fold_edge::fold_edge(fint bci_arg, call_graph_node* callee_arg) 
: call_graph_edge(bci_arg, callee_arg) {
  _counter = _num = _min = _max = _sum = 0;
}

fold_edge::~fold_edge() { 
  // callee is cleared before _call_graph_edge is magically executed
  // the C++ destructor semantic is bogus. LB.
  callee = NULL;
}

void fold_edge::print_node(bool verbose) {
  lprintf("FOLD: "); 
  callee->print_node(verbose);
  lprintf(" avg=%2.1f [%d,%d]",average(), min(), max());
}

float fold_edge::average() {
  return _sum > 0 ?  (float) _sum / _num : 0.0;
}

void fold_edge::update() {
  if (_num == 0 || _counter < _min) _min = _counter;
  if (_counter > _max) _max = _counter;
  _sum += _counter;
  _num++;
  _counter = 0;
}

// ---- graph_iterator

graph_iterator::graph_iterator(call_graph_edge* e) {
  _top   = e;
  _depth = 0;
  cutoff = -1.0;
}

void graph_iterator::do_it() {
  do_edge(top());
  if (!top()->is_fold_edge()) do_sub_edges(top());
}

int graph_iterator::do_sub_edges(call_graph_edge* e) {
  call_graph_node* n = e->callee;    
  _depth++;

  int sons = n->num_of_edges(cutoff);
  if (sons > 0) { 
    ResourceMark rm;
    call_graph_edge** es = NEW_RESOURCE_ARRAY(call_graph_edge*, sons);
    
    int i = 0; 
    for(call_graph_edge* s = n->edges; s; s = s->next) {
      if (s->is_significant(cutoff))
        es[i++] = s;
    }
    qsort(es, sons, sizeof(call_graph_edge*), compare_edges);

    for (i = 0; i < sons; i++) {
      call_graph_edge* ss = es[i];
      do_edge(ss);
      if (!ss->is_fold_edge()) do_sub_edges(ss);
    }
  }
  _depth--;
  return sons;
}

graph_dumper::graph_dumper(call_graph_edge* r, float cut_off, bool verbose)
 : graph_iterator(r) {
   _cut_off = cut_off;
   _verbose = verbose;
   user_time       = r->callee->user_time;
   block_clones    = r->callee->block_clones;
   bytes_allocated = r->callee->bytes_allocated;
   do_it();
}

void graph_dumper::do_edge(call_graph_edge* e) {
  for (fint i = 0; i < depth(); i++)
    lprintf((i % 10) != 9 ? " " : ".");

  if (e->bci == PrologueBCI) {
    lprintf("prologue,  %2.1f%%", 100.0 * e->callee->user_time/user_time);
  } else {
    lprintf("%d ", e->bci);
    if (e->is_fold_edge()) {
      ((fold_edge*) e)->print_node(verbose());
    } else {
      e->callee->print_node(verbose());
      lprintf(", %2.1f%% ", 100.0 * e->callee->user_time/user_time);
      // lprintf(" %dKb,#%dK", bytes_allocated/1024, block_clones/1024); 
    }
  }
  lprintf("\n");
}

int graph_dumper::do_sub_edges(call_graph_edge* e) {
  if (e->callee->user_time/user_time >= cut_off()) {
    return graph_iterator::do_sub_edges(e);
  }
  return 0;
}

# endif //  defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "eventlog.hh"
# include "_eventlog.cpp.incl"

EventLog* eventLog;

void eventlog_init() { eventLog = new EventLog; }

static const char* noEvent = "no event";

void EventLog::init() {
  buf = next = NEW_C_HEAP_ARRAY( Event, EventLogLength);
  bufEnd = buf + EventLogLength;
  for (Event* e = buf; e < bufEnd; e++) e->name = noEvent;
}

EventLog::EventLog() {
  nesting = 0;
  init();
}

void EventLog::resize() {
  Event* oldBuf = buf;
  Event* oldEnd = bufEnd;
  Event* oldNext = next;
  init();
  // copy events
  for (Event* e = nextEvent(oldNext, oldBuf, oldEnd); e != oldNext;
       e = nextEvent(e, oldBuf, oldEnd), next = nextEvent(next, buf, bufEnd)) {
    *next = *e;
  }
  FreeHeap( oldBuf); // cant use delete because this is an array -- dmu
}

void EventLog::printPartial(int32 n) {
  Event* e = next;
  // find starting point
  if (n >= EventLogLength) n = EventLogLength - 1;
  int32 i;
  for (i = 0; i < n; i++, e = prevEvent(e, buf, bufEnd)) ;
  
  // skip empty entries
  for (i = 0;  e != next && e->name == noEvent; i++, e = nextEvent(e, buf, bufEnd)) ;

  int32 indent = 0;
  lprintf("Printing events from earliest to most recent:\n");
  for (; i < n && e != next; i++, e = nextEvent(e, buf, bufEnd)) {
    const char* s;
    switch (e->status) {
     case starting: s = "{ "; break;
     case ending:   s = "} "; indent--; break;
     case atomic:   s = "= "; break;
    }
    lprintf("%*s%s", (void*)(2*indent), " ", s);
    lprintf(e->name, e->args[0], e->args[1], e->args[2]);
    lprintf("\n");
    if (e->status == starting) indent++;
  }
  if (indent != nesting)
    lprintf("Actual event nesting is %ld greater than shown.\n",
           (void*)long(nesting - indent));
}
    
oop printEvent_prim(oop rcvr, oop arg) {
  if (!arg->is_smi()) return VMString[BADTYPEERROR];
  eventLog->printPartial(smiOop(arg)->value());
  return rcvr;
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "lprintf.hh"
# include "_lprintf.cpp.incl"

# include <stdarg.h>

static FILE* logFile = NULL;
static char fname[80];

// dont't use include files for the thing below because this would include
// the conflicting defs of lprintf et al.
extern bool PrintVMMessages;
extern bool LogVMMessages;
extern "C" {
  void breakpoint();
  void error_breakpoint();
  volatile void fatal_handler();
}

void lprintf_exit() {
  if (logFile) {
    fclose(logFile);
    logFile = NULL;
    unlink(fname);
  }
}


void vlprintf(const char* msg, va_list ap) {
  char buf[10000];
  vsprintf(buf, msg, ap);
    
  if (LogVMMessages && !logFile) {
    logFile = fopen(OS::log_file_name(), "w");
  }
  if (LogVMMessages) {
    fputs(buf, logFile);
    fflush(logFile);
  }
  if (PrintVMMessages) {
    # if TARGET_OS_VERSION == MACOSX_VERSION \
      || TARGET_OS_VERSION ==  LINUX_VERSION
      // Mac OS X does not patch write: to restart when signals arrive
      SignalBlocker sb; // ensure nothing is lost on OS X -- dmu 2/03
    # endif
    fflush(stdout);
    fputs(buf, stderr);
    fflush(stderr);
  }
}


extern "C" void lprintf(lprint_format_t msg, ...) {
  va_list ap;
  va_start(ap, msg);
  vlprintf(msg, ap);
  va_end(ap);
}


extern "C" void lprint_error(lprint_format_t msg, ...) {
  bool saved = PrintVMMessages; PrintVMMessages = true;
  lprintf("Self VM error: " );
  va_list ap;
  va_start(ap, msg);
  vlprintf(msg, ap);
  va_end(ap);
  lprintf("\n" );
  error_breakpoint();
  PrintVMMessages = saved;
}

extern "C" void lprint_warning(lprint_format_t msg, ...) {
  bool saved = PrintVMMessages; PrintVMMessages = true;
  lprintf("Self VM warning: ", 0);
  va_list ap;
  va_start(ap, msg);
  vlprintf(msg, ap);
  va_end(ap);
  lprintf("\n" );
  breakpoint();
  PrintVMMessages = saved;
}

extern "C" volatile void lprint_fatal(const char* file, int line, lprint_format_t msg, ...) {
  bool saved = PrintVMMessages; PrintVMMessages = true;
  lprintf("Self VM fatal error (%s, line %ld): ", file, (void*)line );
  va_list ap;
  va_start(ap, msg);
  vlprintf(msg, ap);
  va_end(ap);
  lprintf("\n");
  PrintVMMessages = saved;
  fatal_handler();
}

extern "C" volatile void lprint_fatalNoMenu(const char* file, int line, lprint_format_t msg, ...) {
  Unused(file);
  Unused(line);
  bool saved = PrintVMMessages; PrintVMMessages = true;
  lprintf("\n\nSelf: fatal error: ");
  va_list ap;
  va_start(ap, msg);
  vlprintf(msg, ap);
  va_end(ap);
  lprintf("\n");
  PrintVMMessages = saved;
  breakpoint();
  OS::terminate(1);
}

void flush_logFile() { if (logFile) fflush(logFile); }

extern "C" void mysprintf(char*& buf, lprint_format_t fmt, ...) {
  // like sprintf, but updates the buf pointer so that subsequent
  // sprintfs append to the string
  va_list ap;
  va_start(ap, fmt);
  vsprintf(buf, fmt, ap);
  va_end(ap);
  buf += strlen(buf);
}


volatile void fatal_handler() {
  static bool in_fatal = false;
  if (!in_fatal) {
    in_fatal = true;
  }
  else
    return;
  error_breakpoint();
  SignalInterface::simulate_fatal_signal();
  OS::terminate(-1);
}

static const int max_len = 500; // really 509 but I'll be conservative
// I wrote these becase MetroWerks crashes on a %.*s where length is > 509 or something
void volatile lprintf_string(int len, const char* bytes) {
  for (char *p = (char*)bytes, *end = (char*)bytes + len;  p  <  end;  p += max_len)
    lprintf("%.*s", min(max_len,  end - p), p);
}

void volatile lsprintf_string(char* buf, int len, const char* bytes) {
  for (char *p = (char*)bytes, *end = (char*)bytes + len, *dst = buf;  p  <  end;  p += max_len,  dst += max_len)
    sprintf(dst, "%.*s", min(max_len,  end - p), p);
}
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "nprofiler.hh"
# include "_nprofiler.cpp.incl"


bool profilerSemaphore = false;
bool profilerCollectStackSemaphore = false;
static fint block_level = 0;


# ifdef PROFILER


// Issues still to be addressed:
// - filtering in the VM
//    lexical filtering is difficult since a lexical parent can not
//    be found without the actual activation. This problem could be solved
//    by collecting the receiver in all block actications.
// - histogram for stack size instead of (min, average, max)

const fint Profiler::max_frames  = 150000;
const fint Profiler::max_vframes = 500000;

const fint Profiler::max_pc     = 4000000;
const fint Profiler::max_stacks = 80000;

const fint Profiler::max_activations_in_context = 4;

# if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
class ProfilerTime {
 public:
  Profiler* prof;
  ProcessTime t;
  ProfilerTime(Profiler* p)  {
    prof = p;
    ProcessInfo::update();
    t = ProcessInfo::user_time();
  }
  ~ProfilerTime() {
    ProcessInfo::update();
    prof->prof_time = prof->prof_time + ProcessInfo::user_time() - t;
  }
};

class CollectTime {
 public:
  Profiler* prof;
  ProcessTime t;
  CollectTime(Profiler* p) {
    prof = p;
    ProcessInfo::update();
    t = ProcessInfo::user_time();
  }
  ~CollectTime() {
    ProcessInfo::update();
    prof->collect_time = prof->collect_time + ProcessInfo::user_time() - t;
  }
};
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
class ProfilerTime {
 public:
  Profiler* prof;
  ProfilerTime(Profiler* p) { prof = p; prof->prof_timer.start(); }
  ~ProfilerTime() { prof->prof_timer.stop();  }
};
class CollectTime {
 public:
  Profiler* prof;
  CollectTime(Profiler* p) { prof = p; prof->collect_timer.start(); }
  ~CollectTime() { prof->collect_timer.stop(); }
};
# else
class ProfilerTime { 
 public:
  ProfilerTime(Profiler*) {}
};
class CollectTime { 
 public:
  CollectTime(Profiler*) {}
};
# endif

class RusageEvent {
 public:
  ProcessTime user_time;    //  user time used
  ProcessTime elapsed_time; //  elapsed time used

  char* ident;
};

// ---- EventBuffer

class EventBuffer : public CHeapObj {
 private:
  FILE *file;
  ProcessTime start_elapsed_time;
 public:
  EventBuffer()  {
    file = fopen("nprof.events", "w");
    start_elapsed_time = ProcessTime::get_real_time();
  };
  ~EventBuffer() { if (file) fclose(file); }
# ifdef UNUSED
  void event(char* ident);
# endif
  void time(const char* ident, ProcessTime t);
};

#ifdef UNUSED
void EventBuffer::event(char* ident) {
  ProcessInfo::update();
  fprintf(file, "%-20s %6.3f %6.3f\n",
          ident,
          ProcessInfo::user_time().milli_secs_as_float(),
          (ProcessTime::get_real_time() - start_elapsed_time).milli_secs_as_float());
}
#endif

void EventBuffer::time(const char* ident, ProcessTime t) {
  ProcessInfo::update();
  fprintf(file, "%-20s %6.3f %6.3f %6.3f\n",
          ident,
          ProcessInfo::user_time().milli_secs_as_float(),
          (ProcessTime::get_real_time() - start_elapsed_time).milli_secs_as_float(),
          t.milli_secs_as_float());
}

# ifdef USE_LOG
  # define CREATE_LOG event_buffer = PrintProfiling ? new EventBuffer() : NULL;
  # define DELETE_LOG   if (event_buffer) delete event_buffer;
  # define LOG_REC(C)   if (event_buffer) event_buffer->event(C);
  # define LOG_TIME(C,T) if (event_buffer) event_buffer->time(C,T);
# else
  # define CREATE_LOG   event_buffer = NULL;
  # define DELETE_LOG
  # define LOG_REC(C)
  # define LOG_TIME(C,T)
# endif

// ---- NewNodeList


NewNodeList::NewNodeList(fint initial_size) {
  size = initial_size;
  end  = 0;
  list = NEW_C_HEAP_ARRAY(call_graph_node*, size);
}

NewNodeList::~NewNodeList() {
  if (list) { delete list; list= NULL; }
}

void NewNodeList::nodes_do(call_graph_nodeDoFn f) {
  for (int i = 0; i < end; )
    if (list[i]->is_new())  f(list[i++]);         // do this one and advance
    else                    removeAt(i);          // remove this one, and recheck this slot
}

int NewNodeList::num() {
  return end;
}

bool NewNodeList::verify() {
  bool r = true;
  for (int i = 0;  i < end;  ++i)
    r &= list[i]->verify();

  return r;
}

void NewNodeList::insert(call_graph_node* n) {
  assert( n->my_new_list == NULL, "node is already on list");
    
  if (!n->is_new())
    return; // only keep nodes with new oops
    
  // add n to new_list
  if (end >= size) {
    // sigh, allocate a new and larger array
    call_graph_node** new_list;
    new_list = NEW_C_HEAP_ARRAY(call_graph_node*, size*2);
    for (int i = 0; i < end; i++) new_list[i] = list[i];
    size = size*2;
    delete list;
    list = new_list;
    if (PrintProfiling) lprintf("new_list extend %d\n", size);
  }
  appendNode(n);
}


bool NewNodeList::member(call_graph_node* n) {
  for (int i = 0; i < end; i++) {
    if (n == list[i])  return true;
  }  
  return false;
}


void NewNodeList::remove(call_graph_node* n) {
  for (int i = 0; i < end;  i++)
    if (n == list[i])  {
      removeAt(i);
      return;
    }
  fatal("node was not on list");  
}

void NewNodeList::appendNode(call_graph_node* n) {
  assert(end < size, "checking");
  list[end++] = n;
  n->my_new_list = this;
}  

void NewNodeList::removeAt(int i) {
  assert(end > 0, "checking");
  list[i]->my_new_list = NULL;
  list[i] = list[--end];
}  

// ---- NodeCache

class NodeCacheEntry {
 public:
  nmethod*         nm;
  fint             offset;
  call_graph_node* nodes;
  void clear() { nm = NULL; offset = 0; nodes = NULL; }
# if  GENERATE_DEBUGGING_AIDS
  bool verify(nmethod *m);
# endif
};

# if  GENERATE_DEBUGGING_AIDS
bool NodeCacheEntry::verify(nmethod* m) {
  if (nm == NULL) return false;
  if (nm != m) return false;
  if (nodes->is_method_node()) {
    method_node* nn = (method_node*) nodes;
    return nm->scopes->root()->key.selector == nn->selector();
  }
  return true;
}
#endif

class NodeCache : public CHeapObj {
  // Cache for the function:
  //   (nmethod, offset) -> list of call graph nodes
 private:
  Profiler* prof;
  static const fint size_of_table;
  NodeCacheEntry *table;
  fint hash(nmethod* nm, fint offset) { return offset + (fint) nm; }
  NodeCacheEntry* entryFor(fint hashValue) {
    return &table[hashValue % size_of_table];
  }
 public:
  NodeCache(Profiler* p);
  ~NodeCache() { delete [] table; }

  // Insert a new entry in the cache
  void insert(nmethod* nm, fint offset, call_graph_node* nodes);
  
  // Find the nmethod in the cache. Returns NULL if no entry found.
  call_graph_node* find(nmethod* nm, fint offset);

  // remove all entries and delete call graph nodes
  void flush();

  // memory functions
  void scavenge_contents();
  void gc_mark_contents();
  void gc_unmark_contents();
  void switch_pointers(oop from, oop to);
  bool verify();
};

const fint NodeCache::size_of_table = 3001;

NodeCache::NodeCache(Profiler* p) {
  prof = p;
  table= new NodeCacheEntry[size_of_table];
  for (int i = 0; i < size_of_table; i++) {
    table[i].clear();
  }
}

void NodeCache::insert(nmethod* nm, fint offset, call_graph_node* nodes) {
  NodeCacheEntry* entry = entryFor(hash(nm,offset));
  if (entry->nm && entry->nodes) delete entry->nodes;
  entry->nm     = nm;
  entry->offset = offset;
  entry->nodes  = nodes;
  assert( find(nm, offset), "Cache conflict");
}
  
call_graph_node* NodeCache::find(nmethod* nm, fint offset) {
  NodeCacheEntry* entry = entryFor(hash(nm,offset));
  return (entry->nm == nm && entry->offset == offset) ? entry->nodes : NULL;
}

void NodeCache::flush () {
  for (int i = 0; i < size_of_table; i++) {
    if (table[i].nm && table[i].nodes) delete table[i].nodes;
    table[i].clear();
  }
}

# define FOR_EACH_NODE(E) \
  call_graph_node* E; \
  for (int i = 0; i < size_of_table; i++) \
    if(table[i].nm && (E= table[i].nodes,  E))

void NodeCache::scavenge_contents() {
  FOR_EACH_NODE(n) n->scavenge_contents(); }
void NodeCache::gc_mark_contents() {
  FOR_EACH_NODE(n) n->gc_mark_contents(); }
void NodeCache::gc_unmark_contents() {
  FOR_EACH_NODE(n) n->gc_unmark_contents(); }
void NodeCache::switch_pointers(oop from, oop to) {
  FOR_EACH_NODE(n) n->switch_pointers(from,to); }
bool NodeCache::verify() {
  bool r = true;
  FOR_EACH_NODE(n) r &= n->verify(); 
  return r;
}

static fint num_of_profilers = 0;

nmethod* Pc::my_nmethod() {
  // Taken from frame.c, nmethod* frame::code() [dates from antiquity]
  // But, I don't understand the subtraction. 12/03, dmu
  nmethod* r = nmethod::nmethodContaining(value - sizeof(class nmethod), NULL);
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
           if (r->contains(value))  ;
      else if (nmethod::nmethodContaining(value, NULL)->contains(value))
              fatal("should not have subtracted");
      else    fatal3("value 0x%x is not in any nmethod, either 0x%x, or 0x%x",
                     value, r, nmethod::nmethodContaining(value, NULL));
    }
  # endif
  return r;
}

PcDesc* Pc::pcDesc(nmethod *nm) {
  nmethod *nm2= nm ? nm : my_nmethod();
  PcDesc *p= nm2->contains(value) ? nm2->containingPcDescOrNULL(value) : NULL;
  return p ? p : nm2->pcs();
}

bool Pc::in_self() {
  return Memory->code->iZone->contains(value);
}

bool Pc::in_pics() {
  return Memory->code->stubs->contains(value);
}

stringOop Pc::current_selector() {
  // get selector from sendDesc instead of literal vector.
  nmethod*   nm  = my_nmethod();
  PcDesc*    pd  = pcDesc(nm);

  while(pd >=  nm->pcs() && pd->byteCode == IllegalBCI) pd--;

  ScopeDesc* sd  = pd->containingDesc(nm);
  methodMap* mm  = (methodMap*) sd->method()->map();

  fint code = mm->start_codes()[pd->byteCode];
  fint op   = getOp(code);
  if (op == SEND_CODE || op == IMPLICIT_SEND_CODE)
    return mm->get_selector_at(pd->byteCode);
  else
    return NULL;
}

bool Pc::in_block_clone() {
  nmethod*   nm  = my_nmethod();
  PcDesc*    pd  = pcDesc(nm);

  if (nm->isAccess()) return false;

  while(pd >=  nm->pcs() && pd->byteCode == IllegalBCI) pd--;

  ScopeDesc* sd  = pd->containingDesc(nm);
  methodMap* mm  = (methodMap*) sd->method()->map();

  // pd->byteCode could be either a primitive send or a
  // push of a block literal (doing block_clone)
  assert( pd->byteCode >= 0, "bci should be valid");

  fint code = mm->start_codes()[pd->byteCode];
  return getOp(code) == LITERAL_CODE;
}

bool Pc::is_calling_prim() {
  stringOop selector = current_selector();
  return    selector ? selector->is_prim_name() : false;
}

void Profiler::update_callee(fint bci, call_graph_node* n,
                             float optimized_user_time) {
  call_graph_node* top = path->top()->callee;
  // Let's check if folding is possible
  if (ProfilerUseFold) {
    if (start_of_match) {
      // stack [...start_of_match, ... start_of_fold .... path->length()-1 ]
      fint fold_size  =  start_of_fold  - start_of_match;
      fint match_size =  path->length() - start_of_fold;
      
      if (fold_size == match_size + 1) {
        // n matches the [start_of_fold - 1] 
        // create the cycle, disable the edge top start_of_fold and 
        // adjust path
        call_graph_edge* last_to_match = path->nth(start_of_fold - 1);
        if (last_to_match->callee->is_matching(n)) {
          // fold the stack
          call_graph_node* back_node = path->nth(start_of_match)->callee;
          
          call_graph_edge* fold_e = path->nth(start_of_fold);
          
          call_graph_node* insert_node =
            path->nth(start_of_fold-1)->callee;
          
          fold_edge* e = insert_node->find_fold_edge(fold_e->bci, back_node);
          if (e != NULL) {
            e->inc();
          } else {
            // Create new fold edge 
            e = new fold_edge(fold_e->bci, back_node);
            insert_node->add_edge(e);
          }
          update_back_edges_visited(e);
          
          // chop of the until start_of_fold
          for (fint i = 0; i < match_size; i++) path->pop();
          
          start_of_match = start_of_fold = 0;
          return;
        } else {
          // no match reset the match (or do back tracking)
          start_of_match = start_of_fold = 0;
        }
      }
    }
  }

  call_graph_edge* e;
  if (! (e = top->find_matching(bci, n))) {
    top->add_edge(e = new call_graph_edge(bci, new_clone(n)));
    // Adding a block node require adjustment of outer_node
  }
  assert(e->callee,"callee must exist");

  e->callee->add_optimized(optimized_user_time);
  path->push(e);

  if (ProfilerUseFold) {
    if (start_of_match) {
      // match the next in fold
      fint match_size =  path->length() - 1 - start_of_fold;
      // check start_of_fold + match_size against n
      call_graph_edge* edge =  path->nth(start_of_match + match_size);
      if (!edge->callee->is_matching(n)) {
        start_of_match = start_of_fold = 0;
      }
    } else {
      // match path()->top with something down the stack.
      for (fint i = path->length()-2; i >= 0; i--) {
        call_graph_edge* edge =  path->nth(i);
        if (edge->callee->is_matching(n)) {
          start_of_match = i;
          start_of_fold  = path->length()-1;
          return;
        }
      }
    }
  }
}

void Profiler::update_callee(fint bci, stringOop sel) {
  // if (bci, sel) is present update trace and return.
  call_graph_node* top = path->top()->callee;
  call_graph_edge* e = top->find_matching(bci, sel);
  if (!e) {
    // create (bci, sel) and update trace.    
    e = new call_graph_edge(bci, new_prim_node(sel));
    top->add_edge(e);
  }
  assert(e->callee,"callee must exist");
  path->push(e);
}

static void profilerTick() { 
  // Should be called at each tick if at least one profile is active.
  if (currentProcess->profiler) currentProcess->profiler->tick();
}

Profiler::Profiler(profilerOop prof) :
  stack_size("\tStack size (frames)"), collect_ticks("\tSlice (num)") {
  status = inactive;

  _profilerOop = prof;

  _process     = NULL;

  new_list  = new NewNodeList(10000);
  use_new_list = true;
  cache     = new NodeCache(this);
  root      = new_leaf_node();
  root_edge = new call_graph_edge(0, root);

  last_stack= new pc_state[max_frames];
  last_stack_end = 0;
  last_path = new call_graph_edge *[max_vframes];
  last_path_end = 0;
  stack     = new StackInfo[max_stacks];
  pc        = new Pc[max_pc];

  monitor_tick_time       = 0.0; 
  compile_time            = 0.0;
  recompile_time          = 0.0;
  scavenging_time         = 0.0;
  garbage_collection_time = 0.0;
  nmethod_flush_time      = 0.0;
  nmethod_compact_time    = 0.0;
  uncommon_branch_time    = 0.0;

  stack_index    = 0;
  first_stack_in_interval = 0;

# if TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
  user_time      = 0.0;
# endif

  number_of_conversions = 0;

  # if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
       prof_time.clear();
    collect_time.clear();
  # elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
       prof_timer.reset();
    collect_timer.reset();
  # endif

  # if GENERATE_DEBUGGING_AIDS
    if (SpendTimeForDebugging) {
      prior_node_count = 0;
      prior_total_time = 0.0;
    }
  # endif
  
  allocationMonitor.clear();
  start_of_match = 0;
  start_of_fold  = 0;
  max_self_frames_on_stack = 0;
  path = NULL;
  back_edges_visited = NULL;
  
  if (num_of_profilers == 0) {
    IntervalTimer* it = IntervalTimer::CPU_timer();
    it->enroll_async(it->ticks_per_second() * it->oversample_rate /* for profiler debugging */, 
                     profilerTick);
  }
  num_of_profilers++;

  profilers->insert(this);

  CREATE_LOG;
}

static Profiler* _prof;
static void cont_delete() { 
  delete _prof->root;
  delete _prof->new_list;
  delete _prof->cache;
}

Profiler::~Profiler() {
  num_of_profilers--;
  if (num_of_profilers == 0) {
    IntervalTimer::CPU_timer()->withdraw(profilerTick);
  }  
  // Perform the deletion on the VM stack to avoid stack overflow
  _prof = this;
  switchToVMStack(cont_delete);

  profilers->remove(this);
  delete [] last_stack;
  delete [] last_path;
  delete [] stack;
  delete [] pc;
  DELETE_LOG;
}

void Profiler::handleOverflow() {
  static bool have_warned = false;
  if (!have_warned) {
    warning("Entering handleOverflow() in nprofiler.cpp. As of 2/04, I do not understand how Lars meant this to work. -- dmu");
    have_warned= true;
  }
  StackInfo saved_stack;
  stop_tick_timer();
  status= suspended;
  saved_stack = current_stack();
  convert_nmethod_information();

  stack[++stack_index] = saved_stack;
  if (stack_index >= max_stacks) 
    fatal("must increase max_stacks");

  collect_stack();
  compute_time();
  resume();
}


void Profiler::start_tick_timer() {
# if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
  ProcessInfo::update();
  current_stack().u_time = ProcessInfo::user_time();
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
  current_stack().e_timer.reset();
  current_stack().e_timer.start();
  u_timer.start();
# endif
}

void Profiler::stop_tick_timer() {
# if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
  ProcessInfo::update();
  current_stack().u_time = ProcessInfo::user_time() - current_stack().u_time;
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==   SUNOS_VERSION
  current_stack().e_timer.stop();
  user_time += u_timer.millisecs(); 
# endif
}

// Start the timing.
void Profiler::engage(Process* proc) {
  CSect cs(profilerSemaphore);

  // Make the process aware of this profiler.
  proc->profiler = this;

  _process = proc;

# if TARGET_ARCH == SPARC_ARCH
  fint min_self_frame_size = WindowSize * oopSize;
# elif TARGET_ARCH == PPC_ARCH
  fint min_self_frame_size = linkage_area_size;
# else
  fint min_self_frame_size = 1;
# endif

  max_self_frames_on_stack = proc->stack()->size / min_self_frame_size;

  if (currentProcess == proc) {
    resume();
  } else {
    status = inactive;
  }
}

// Stop the timing and convert the vframe dependent information.
void Profiler::disengage() {
  CSect cs(profilerSemaphore);

  if (status == profiling) {
    assert(currentProcess == _process, "_process is wrong");

    LOG_REC("disengage");
    stop_tick_timer();
    collect_stack();
    compute_time();
  }

  convert_nmethod_information();
  cache->flush();
  status = inactive;
}

void Profiler::suspend() {
  CSect cs(profilerSemaphore);

  assert(status == profiling, "Profiler in wrong state");

  LOG_REC("suspend");
  stop_tick_timer();
  collect_stack();
  compute_time();

  status = suspended;
}


void Profiler::resume() {
  CSect cs(profilerSemaphore);

  assert(status != profiling, "Profiler in wrong state");
  allocationMonitor.clear();
  start_tick_timer();
  status = profiling;
  LOG_REC("resume");
}

# define FOR_EACH_STACK(ST) \
    for (StackInfo* ST = &stack[0]; ST < &current_stack(); ST++)

# define FOR_EACH_STACK_IN_INTERVAL(ST) \
    for (StackInfo* ST = &stack[first_stack_in_interval]; \
         ST < &current_stack(); ST++)

// Compute user time for all stack samples.
void Profiler::compute_time() {
# if TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
  float slice_time = 0.0;

  FOR_EACH_STACK_IN_INTERVAL(st) slice_time += st->e_timer.millisecs();

  float factor;
  if (user_time > 0 && slice_time > 0) {
    factor = user_time / slice_time;
  } else {  
    factor = 1.0;
  }

  if (PrintProfiling) {
    lprintf("Computed time: user = %2.0f, elapsed = %2.3f -> factor = %2.3f\n", 
           user_time, slice_time, factor);
  }

  FOR_EACH_STACK_IN_INTERVAL(s)
    s->computed_user_time = s->e_timer.millisecs() * factor;

  user_time = 0.0;
# endif

  collect_ticks.add(stack_index - first_stack_in_interval);
  first_stack_in_interval = stack_index;
}

typedef BoundedListTemplate<ScopeDesc*> ScopeDescBList;
static const fint ScopeDescBList_initial_size = 20;


void Profiler::addToCache(nmethod *nm, float optimized_user_time, fint bci) {
  call_graph_node* cache_result= cache->find(nm, 0);
  if (cache_result == NULL) {
    cache_result= new_node(nm->scopes->root());
    cache->insert(nm, 0, cache_result);
  }
  update_callee(bci, cache_result, optimized_user_time);
}


fint Profiler::merge_pc(Pc curr_pc, Pc prev_pc, fint prev_bci,
                        float user_time_arg) {
  ScopeDesc* sd = NULL;

  fint last_top_bci = 0;

  if (curr_pc.in_self()) {
    nmethod* nm  = curr_pc.my_nmethod();
    float optimized_user_time =  (nm->compiler() != NIC) ? user_time_arg : 0.0;
    assert( nm, "nmethod must be found from pc.");

    fint top_bci = prev_bci;

    if (nm->isAccess()) {
      addToCache(nm, optimized_user_time, top_bci);
    } 
    else {
      PcDesc* pd = curr_pc.pcDesc(nm);
      while(pd > nm->pcs() && pd->byteCode == IllegalBCI) pd--;
      last_top_bci = pd->byteCode;
      if (   pd->byteCode == PrologueBCI 
          || pd->byteCode == EpilogueBCI
          || pd->byteCode == IllegalBCI) {
        addToCache(nm, optimized_user_time, top_bci);
      } 
      else {
        call_graph_node* cache_result= cache->find(nm, pd->scope);
        if (!cache_result) {
          // Build the cache entry
          sd = pd->containingDesc(nm);
          cache_result = new_node(sd);

          fint bci = sd->isTop() ? 0 : sd->senderByteCodeIndex();
          for (sd = sd->sender(); sd; sd = sd->sender()) {
            call_graph_node* n = new_node(sd);
            n->edges = new call_graph_edge(bci);
            n->edges->callee = cache_result;
            cache_result = n;
            bci = sd->isTop() ? 0 : sd->senderByteCodeIndex();
          }
          cache->insert(nm, pd->scope, cache_result);
        }

        fint bci = top_bci;

        for (call_graph_node* n = cache_result; n; 
             n = n->edges ? n->edges->callee : NULL) {
          update_callee(bci, n, optimized_user_time);
          if (n->edges) bci = n->edges->bci;
        }
      }
    }
  }
  else if (prev_pc.value != NULL  &&  prev_bci >= 0  &&  prev_pc.in_self()) {
    // A c-frame has been encountered. Use the previous self frame 
    // to find the name of the called primitive.
    if (prev_pc.in_block_clone()) {
      stringOop sel = Memory->string_table->lookup("_BlockClone", 11);
      update_callee(prev_bci, sel);
    }
    else {
      stringOop selector = prev_pc.current_selector();
      if (selector && selector->is_prim_name()) {
        // add prim node
        update_callee(prev_bci, selector);
      } else {
        // at the beginning of a lookup
      }
    }
  }

  return last_top_bci;
}

void Profiler::update_back_edges_visited(call_graph_edge *edge) {
  assert( back_edges_visited, "list must be present");
  if (back_edges_visited->find(edge) == -1) 
    back_edges_visited->append(edge);
}


void Profiler::fix_stack_bottom(StackInfo* st) {
  // Fix the bottom of the stack by merging the information
  // collected in collect_stack and in the assembly routine
  // ContinueAfterProfilerInterrupt.
  
  fint begin = st->begin + max_activations_in_context;
  
  # if TARGET_ARCH == SPARC_ARCH
    if (st->interrupted_fp > st->frame_pointer) {
      // tick occured in just after a restore instruction which means the
      // first element in the pc sub-array is equal to the return addr,
      // in the interrupted context.
      ++begin; 
    } 
    else if (st->interrupted_fp != st->frame_pointer) {
      assert( 0 < begin  &&  begin <= max_pc,  "bounds");
      pc[--begin].set_value( st->interrupted_stored_return_addr.value, 1 );
    }
  # endif
  
  assert( 2 <= begin  &&  begin <= max_pc,  "bounds");
  if ( st->interrupted_pc.in_self() ) {
    nmethod* nm = st->interrupted_pc.my_nmethod();
    
    if (nm == NULL)
      ;
    else if (nm->isAccess()) {
      // access method
      //   caller          -- interrupted_return_addr_reg
      //   access method   -- pc
      pc[--begin].set_value( st->interrupted_return_addr_reg.value, 2 );
      pc[--begin].set_value( st->interrupted_pc.value, 3 );
    }
    else if (nm->has_frame_at(st->interrupted_pc.value)) {
      // pc could be in the prologue or the epilogue in case we should leave 
      // out the activation.
      if (nm->in_self_code_at(st->interrupted_pc.value)) {
        pc[--begin].set_value( st->interrupted_pc.value, 4 );
      }
      else {
        pc[--begin].set_value( st->interrupted_pc.value, 5 );
        // in self nmethod but in prologue or epilogue
      }
    }
    else {
      // no frame but could be execution real code
      pc[--begin].set_value( st->interrupted_return_addr_reg.value, 6 );
      
      if (nm->in_self_code_at(st->interrupted_pc.value)) {
        pc[--begin].set_value( st->interrupted_pc.value, 7 );
      }
      else {
        // in self nmethod but in prologue or epilogue
        // ignore interrupted_pc if receiver hasn't been verified
        if (st->interrupted_pc.value >= nm->verifiedEntryPoint()) {
          pc[--begin].set_value( st->interrupted_pc.value, 8 );
        }
      }
    }
  }
  else {
    if (st->interrupted_pc.in_pics()) {
      // Stack:
      //    non access method -- interrupted_return_addr_reg
      //    pic stub          --_pc
      if ( st->interrupted_return_addr_reg.in_self()
      &&  !st->interrupted_return_addr_reg.in_pics() )  { // PPC temporarily sets link reg
        pc[--begin].set_value( st->interrupted_return_addr_reg.value, 9 );
      }
    }
    else if (st->interrupted_stored_return_addr.in_self()) {
      // Stack:
      //    method             -- return_addr
      //    method or nonsense -- interrupted_return_addr_reg
      //    non-self           -- pc
      //
      // The only way to determine if o7 is valid is to test if
      // interrupted_stored_return_addr is calling self code
      //
      // Only pertains to SPARC.
      
      nmethod* nm = st->interrupted_stored_return_addr.my_nmethod();
      assert(!nm->isAccess(), "access method must be at the bottom most");
      
      if (nm->in_self_code_at(st->interrupted_stored_return_addr.value)) {
        if (st->interrupted_stored_return_addr.in_block_clone()) {
          // do nothing
        } 
        else if (st->interrupted_stored_return_addr.is_calling_prim()) {
          pc[--begin].set_value( st->interrupted_pc.value, 10 );
        } 
        else {
          // Stack:
          //    method in ordiary send  -- return_addr
          //    unknown                 -- interrupted_return_addr_reg
          //    non-self                -- pc
        }
      } 
      else {
        // forget about pc and interrupted_return_addr_reg
      }
    } 
    else {
      // We are not in self at all 
      while ( begin < st->end  &&  !pc[begin].in_self())
        begin++;
    }
  }
  st->begin = begin;
}


void Profiler::merge_stack_info(StackInfo* st) {
 ResourceMark rm;

 if (st->interrupted_pc.value) fix_stack_bottom(st);

 // trace is used for folding the call graph
 path               = new tree_edge_BList(tree_edge_BList_initial_size);
 back_edges_visited = new tree_edge_BList(tree_edge_BList_initial_size);

 start_of_match = 0;
 start_of_fold  = 0;

 path->push(root_edge);

 // Traverse the stack in reverse order, from top to bottom.
 Pc   prev_pc;
 prev_pc.value = NULL;
 fint prev_bci = 0;
 fint stack_end = st->end-1;

 int  stack_ind1 = 0;

 if (ProfilerUsePatch) {
   if (pc[stack_end].value == first_inst_addr(ProfilerTrap)) {
     // The first element in the buffer is patch 
     char* frame = pc[stack_end].fr;
     stack_end--;
     for ( ;
           stack_index <  last_stack_end  &&  last_stack[stack_ind1].pc.fr >= frame;
           ++stack_ind1)
              assert(stack_ind1 < max_frames, "checking");
   }
 }
 

 if ((ProfilerUseLastStack || ProfilerUsePatch) && last_stack_end > 0) {

   last_stack_end = stack_ind1;

   if (stack_ind1 > 0 ) {
     if (ProfilerUseLastStack) {
       for ( ;
              stack_ind1  <  last_stack_end && 
              stack_end   >  st->begin      &&
              pc[stack_end].value == last_stack[stack_ind1].pc.value;              
              stack_ind1++,  stack_end-- )
         assert( stack_ind1 < max_frames, "checking");

       last_stack_end = stack_ind1;

       assert( stack_ind1 <= max_frames, "checking");

       int path_length = last_stack[stack_ind1-1].path_index;
       prev_bci        = last_stack[stack_ind1-1].bci;
       prev_pc.value   = last_stack[stack_ind1-1].pc.value;
       for (int i = 1; i < path_length; i++) {
         // root is already on the path
         path->push(last_path[i]);
       }
     } else {
       // Merge all frames into the call graph.
       for (int index = 0; index < stack_ind1; index++) {
         assert( index < max_frames, "checking");
         prev_bci = merge_pc(last_stack[index].pc, prev_pc, prev_bci,
                             st->user_time());
         prev_pc  = last_stack[index].pc;
       }
     }
   }
 }
 
 for (fint index = stack_end;  index >= st->begin;  --index) {
    prev_bci = merge_pc(pc[index], prev_pc, prev_bci, st->user_time());
    prev_pc  = pc[index];

    assert( last_stack_end <= max_frames, "checking");

    if (ProfilerUseLastStack || ProfilerUsePatch) {
      last_stack[last_stack_end].pc = pc[index];

      if (ProfilerUseLastStack) {
        last_stack[last_stack_end].bci        = prev_bci;
        last_stack[last_stack_end].path_index = path->length();
      }

      last_stack_end++;
    }
 }

 // If the top node of the path belongs to a non access/non primitive node
 // we add an edge and a node.
 call_graph_node* top = path->top()->callee;
 if (prev_pc.in_self() && 
     !top->is_prim_node() && !top->is_access_node()) {
   call_graph_edge* e;
   if (prev_bci < 0) {
     if (! (e = top->find_leaf(prev_bci))) {
       top->add_edge(e = new call_graph_edge(prev_bci, new_leaf_node()));
     }
     path->push(e);  
   } 
   else if (prev_pc.in_block_clone()) {
     stringOop sel = Memory->string_table->lookup("_BlockClone", 11);
     update_callee(prev_bci, sel);
   }
   else {
     stringOop selector = prev_pc.current_selector();
     if (selector && selector->is_prim_name()) {
       // add prim node
       update_callee(prev_bci, selector);
     } else {    
       if (! (e = top->find_leaf(prev_bci))) {
         top->add_edge(e = new call_graph_edge(prev_bci, new_leaf_node()));
       }
       path->push(e);  
     }
   }
 }

 last_path_end = 0;
 fint x;
 for (x = 0; x < path->length(); x++) {
   call_graph_edge* e = path->nth(x);
   if (ProfilerUseLastStack) {
     assert(last_path_end < max_vframes, "bounds check");
     last_path[last_path_end++] = e;
   }
   call_graph_node* n = e->callee;

   // Update the timing and allocation information to node.
   n->add_user_time(st->user_time());
   n->add_bytes(st->bytes_allocated);
   n->add_blocks(st->block_cloned);
 }

 LOG_TIME("delta", st->u_time);

 for (x = 0; x < back_edges_visited->length(); x++) {
   ((fold_edge*) back_edges_visited->nth(x))->update();
 }
 
 back_edges_visited = NULL;
}


bool Profiler::conversion_needed() {
  if (stack_index > max_stacks - 1) return true;
  if (stack_index && stack[stack_index-1].end >= max_self_frames_on_stack) return true;
  return false;
}


void Profiler::convert_nmethod_information() {
 CSect cs(profilerSemaphore);
  ProfilerTime pt(this);

  number_of_conversions++;  
  for (int i = 0; i < stack_index; i++) {
    if (ProfilerIgnoreCallGraph) {
      root->add_user_time(stack[i].user_time());
      root->add_bytes(stack[i].bytes_allocated);
      root->add_blocks(stack[i].block_cloned);
    } else {
      merge_stack_info(&stack[i]);
      stack_size.add(stack[i].end - stack[i].begin);
    }
  }
  stack_index = 0;
  first_stack_in_interval = 0;
  
  # if GENERATE_DEBUGGING_AIDS
    if (SpendTimeForDebugging)  {
      int node_count = -1;
      float total_time = -1.0;
      graph_totaller::compute_totals( root_edge, node_count, total_time );
      if (prior_node_count  > node_count)
        lprintf("count decreased from %d to %d\n", prior_node_count, node_count);
      if (prior_total_time  > total_time)
        lprintf( "time decreased from %g to %g\n", prior_total_time, total_time);
      if ( prior_node_count  > node_count
      ||   prior_total_time  > total_time )
        fatal("non-monotonic");

      prior_node_count = node_count;
      prior_total_time = total_time;
    }
  # endif
}


# if TARGET_ARCH == SPARC_ARCH
extern "C" { void ContinueAfterProfilerInterrupt(); }
# elif TARGET_ARCH == PPC_ARCH
// not used: would need return from interrupt instruction
// void ContinueAfterProfilerInterrupt() { fatal("unimp mac"); }
# endif

char** profiler_return_addr;

StackInfo* Profiler::collect_stack(bool in_interrupt) {
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  
  assert(!in_interrupt || InterruptedContext::the_interrupted_context->is_set(), "needed for stack walking");
  
  CSect cs(profilerCollectStackSemaphore);
  CollectTime ct(this);

  StackInfo* st = &current_stack();

  st->begin = stack_index ? stack[stack_index-1].end : 0;
  st->interrupted_pc.value = NULL;


  ++stack_index;
  if (stack_index >= max_stacks) 
    fatal2("need more stacks; stack_index is %d, but max_stacks is %d", stack_index, max_stacks);

  allocationMonitor.update();
  st->block_cloned    = allocationMonitor.blocks();
  st->bytes_allocated = allocationMonitor.bytes();
  allocationMonitor.clear();

  fint index = st->begin;


  FlushRegisterWindows();

  frame* last_frame;
  if (in_interrupt) {
    // Make room for activations in the interrupted context
    for ( fint i = 0;  i < max_activations_in_context; ++i ) {
      pc[index].value = NULL;
      pc[index++].fr  = NULL;
    }

    profiler_return_addr = (char**) &st->interrupted_pc;
    last_frame = InterruptedContext::the_interrupted_context->sp();

      // Want frame above the interrupted context, but on PPC that's frame
      // above sp. That way, can look at last_frame's saved return_addr.
      // The interrupted context's sp may NOT be pointing at frame wtih saved PC. -- dmu 12/03
      //
      // I don't know why, but the return PC is sometimes not in the sender, so go up another. -- dmu 1/04
      // Need to go up TWO??? Come back to this. -- dmu 2/04
      //
      // I came back to it, made sure all the stubs save the link reg before stwu'ing the sp,
      // and assuming the C compiler is well behaved this should work with only one sender(); -- dmu 2/04
      // See frame_ppc.cpp frame::return_addr()

    st->frame_pointer = (char*) last_frame;
    bool inSelf = Memory->code->isSelfPC(InterruptedContext::the_interrupted_context->pc());
    
         if (ShowLookupInMonitor::lookup_nesting)  lookup_ticks.inc();
    else if (!inSelf)                                prim_ticks.inc();
    else                                             self_ticks.inc();
  } 
  else {
    st->interrupted_pc.value = NULL;
    last_frame = _process->inSelf() ? _process->last_self_frame(false): NULL; 
    // _process->last_self_frame() ignores the last Self frame if
    // this activation is in the prologue code. However, fix_stack_bottom
    // captures the correct information by using the information in
    // the registers.
    self_ticks.inc();
  }

  st->end = index;

  if (ProfilerIgnoreCallGraph  ||  st->user_time() == 0.0)
    return NULL;
  collect_return_addresses_above_interrupted_context(st, last_frame);
  return st;
}


void Profiler::collect_return_addresses_above_interrupted_context(StackInfo* st, frame* last_frame) {
  fint index = st->end;
  bool in_c = false;
  bool reached_patch_address = false;
  fint count = 0;
  assert( block_level == 0, "should not reenter this");
  
  for (frame* f = last_frame; 
              f; 
              f = f->sender()) {
    if (!reached_patch_address) {
      // If f is not in self insert a 0 to indicate c frames.
      count++;
      if (f->is_compiled_self_frame()) {
        // XXXX add a branch for interp
        assert( index < max_pc,  "bounds");
        pc[index].set_value( f->return_addr(), 11 );
        assert( *(int*)f->return_addr(), "code check");
        assert( pc[index].my_nmethod(), "ensure the value is good");
        if (    ProfilerUsePatch
            &&  !ConversionInProgress
            &&  count > 2) {
          f->patch_profiler_trap();
          reached_patch_address =  pc[index].value == first_inst_addr(ProfilerTrap);
        }
        pc[index++].fr = (char*) f;
        in_c = false;
      } else {
        if (!in_c) {
          pc[index  ].value = NULL;
          pc[index++].fr    = (char*)f;
          in_c = true;
        }
        count = 0;
      }
      if (index > max_pc) fatal("return addr. buffer overflow");
    }
    if (f->real_return_addr() == first_inst_addr(ProfilerTrap)) {
      assert(Memory->code->contains(f->return_addr()), "should be in self");
    }
  }          
  st->end = index;
}


static void print_exclude(const char* title, float t) {
  if (t > 0.0) {
    lprintf(" %4.1f (ms) %s\n", t, title);
  }
}

class graph_counter : public graph_iterator {
 public:
  fint count;
  graph_counter(call_graph_edge* e);
  void do_edge(call_graph_edge* e);
};

graph_counter::graph_counter(call_graph_edge* r)
 : graph_iterator(r) {
   count = 0;
   do_it();
}

void graph_counter::do_edge(call_graph_edge* e) {
  Unused(e);
  count++;
}
  
void Profiler::printProfile(float cutoff) {
  if (WizardMode) {
    lprintf("\t self: %d, lookup: %d, prims: %d \n",
           self_ticks.num, lookup_ticks.num, prim_ticks.num);
    stack_size.print();
    collect_ticks.print();

    lprintf("Number of conversions: %d\n", number_of_conversions);

    lprintf("Runtime:       %3.2f msec\n", root->user_time); 

#   if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
    lprintf("Collect time:  %3.2f msec\n", collect_time.milli_secs_as_float());
    lprintf("Profiler time: %3.2f msec\n", prof_time.milli_secs_as_float());
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==   SUNOS_VERSION
    lprintf("Collect time:  %3.2f sec\n", collect_timer.millisecs());
    lprintf("Profiler time: %3.2f sec\n", prof_timer.millisecs());
#   endif

    lprintf("Number in new list = %d\n", new_list->num());

    lprintf("<Excludes>-----------------\n");
    print_exclude("monitor", monitor_tick_time);
    print_exclude("compile", compile_time);
    print_exclude("recompile", recompile_time);
    print_exclude("scavenging", scavenging_time);
    print_exclude("garbage_collection", garbage_collection_time);
    print_exclude("nmethod_flush", nmethod_flush_time);
    print_exclude("nmethod_compact", nmethod_compact_time);
    print_exclude("uncommon_branch", uncommon_branch_time);
    lprintf("\n");

    graph_counter gc(root_edge);
    lprintf("# nodes in call graph = %d\n", gc.count);
    
  }
  if (!ProfilerDumpCallGraph) return;

  graph_dumper f(root_edge, cutoff);
}


void Profiler::tick() {
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  
  if (status != profiling)    return;
  if (_process->nesting == 0) return; // ignore if _process is inactive

  // If the preemptor() has been called in THIS tick a preemption
  // is pending and the profiler suspend has taken place.
  if (_process->preemptionPending()) return;

  if (ConversionInProgress)   { conversion_ticks.inc(); return; }
  if (profilerSemaphore)      { profiler_ticks.inc();   return; }
  if (processSemaphore)       { sem_ticks.inc();        return; }

  assert(!GCInProgress, "should be in exclude part");

  assert(!theCompiler,  "should be in exclude part");
# ifdef SIC_COMPILER
  assert(!theSIC,       "should be in exclude part");
# endif

  if (conversion_needed())  {
    profilers->raiseOverflow(this);
    LOG_REC("raise");
  } else {
    // Avoid process change during stack collection
    // ContinueAfterProfilerInterrupt will clear processSemaphore.

    processSemaphore = true;

    LOG_REC("tick");
    stop_tick_timer();
    int old_stack_index = stack_index;
    StackInfo* st = collect_stack(true);
    start_tick_timer();

    # if TARGET_ARCH == SPARC_ARCH
      if ( st != NULL ) {
        InterruptedContext::set_continuation_address(first_inst_addr(ContinueAfterProfilerInterrupt),
                                                     false, false);
      }
    # elif TARGET_ARCH == PPC_ARCH
      if ( st != NULL ) {
        frame* f = InterruptedContext::the_interrupted_context->sp();

        st->interrupted_pc.                set_value( InterruptedContext::the_interrupted_context->pc(), 12 );
        st->interrupted_return_addr_reg.   set_value( InterruptedContext::the_interrupted_context->lr(), 13 );
        st->interrupted_stored_return_addr.value = NULL; // pick this up another way on PC, simply walking stack
        st->interrupted_fp = (char*)f;
      }
      processSemaphore = false;
    # endif
  }
}


call_graph_node* Profiler::new_node(ScopeDesc* sd) {
  call_graph_node* n;
       if (sd->isMethodScope())  n = new method_node(sd);
  else if (sd->isAccessScope())  n = new access_node(sd);
  else                           n = new block_node(sd);

  if (use_new_list)  new_list->insert(n);
  return n;
}


call_graph_node* Profiler::new_leaf_node() {
  call_graph_node* n = new leaf_node();
  if (use_new_list)  new_list->insert(n);
  return n;
}

call_graph_node* Profiler::new_prim_node(stringOop sel) {
  call_graph_node* n = new prim_node(sel);
  if (use_new_list)  new_list->insert(n);
  return n;
}

call_graph_node* Profiler::new_clone(call_graph_node* n) {
  call_graph_node* c = n->clone();
  return c;
}

void Profiler::update_last_path(float user_time_arg) {
  // Add user time to the last stack
  for (int i = 0; i <  last_path_end; i++) {
    last_path[i]->callee->add_user_time(user_time_arg);
  }
}

// memory functions

static void call_graph_node_scavenge(call_graph_node* n) {
  n->scavenge_contents_quickly();
}

static oop n_from, n_to;
static void call_graph_node_switch_pointers(call_graph_node* n) {
  n->switch_pointers(n_from, n_to);
}

void Profiler::scavenge_contents() {
  if (!use_new_list) fatal("!!!!");
  SCAVENGE_TEMPLATE(&_profilerOop);
  cache->scavenge_contents();
  if (use_new_list)  new_list->nodes_do(call_graph_node_scavenge);
  else               root->scavenge_contents();
}

void Profiler::gc_mark_contents() {
  MARK_TEMPLATE(&_profilerOop);
  root->gc_mark_contents();
  cache->gc_mark_contents();
  // no need to mark via new_list as these are all reachable from
  // root and cache
}

void Profiler::gc_unmark_contents() {
  UNMARK_TEMPLATE(&_profilerOop);
  root->gc_unmark_contents();
  cache->gc_unmark_contents();
  // no need to unmark via new_list as these are all reachable from
  // root and cache
}

void Profiler::switch_pointers(oop from, oop to) {
  if (from->is_old()  &&  to->is_new()  &&  use_new_list) {
    // To continue using new_list would need switch_pointers to add a node to new_list
    // if it weren't already new and an old oop were replaced by a new one.
    use_new_list = false;
    fatal("This should not happen unless memory system has changed.");
  }
  SWITCH_POINTERS_TEMPLATE(&_profilerOop);
  cache->switch_pointers(from,to);
  if (use_new_list  &&  from->is_new()) {
    n_from = from;
    n_to   = to;
    new_list->nodes_do(call_graph_node_switch_pointers);
  } else {
    root->switch_pointers(from,to);
  }
}

bool Profiler::verify() {
  bool verify_result = true;
  VERIFY_TEMPLATE(&_profilerOop);
  verify_result &= root->verify();
  verify_result &= cache->verify();
  verify_result &= new_list->verify();
  return verify_result;
}


// ---- Profilers

Profilers::Profilers(){
  _first                  = NULL;
  _profiler_with_overflow = NULL;
}

void Profilers::insert(Profiler* p) {
  p->_next = _first;
  _first = p;
}

void Profilers::remove(Profiler* p) {
  bool found = false;
  if (_first == p) {
    _first = _first->_next;
    found = true;
  } else {
    Profiler* pred = _first;
    for (Profiler *c = pred->_next; c != NULL; c = c->_next, pred = c) {
      if (c == p) {
        pred->_next = c->_next;
        found = true;
      }
    }
  }
  if (found) p->_next = NULL;
  else       fatal("profiler not found in profiler list");
}

Profiler* Profilers::find_current() {
  for (Profiler *p = _first; p != NULL; p = p->_next) {
    if (p->status == p->profiling) return p;
  }
  return NULL;
}


void Profilers::raiseOverflow(Profiler* p) {
  assert(p == _profiler_with_overflow || !hasOverflow(),
         "several profilers with overflow not allowed");
  _profiler_with_overflow = p;
  // Patch max stack to force process to return to Profilers::handleOverflow()
  // in a well-defined state.
  InterruptedContext::setupPreemptionFromSignal();
}

void Profilers::handleOverflow() {
  assert(hasOverflow(), "must have profiler with overflow");
  _profiler_with_overflow->handleOverflow();
  _profiler_with_overflow = NULL;
}
  
#  define FOR_EACH_PROFILER(P) \
   for (Profiler *P = _first; P != NULL; P = P->_next)

# else // ! PROFILER

#  define FOR_EACH_PROFILER(P) \
   Profiler *P; if (false)
   
Profilers::Profilers() : CHeapObj() {}

 
# endif // PROFILER




void Profilers::print() {
  FOR_EACH_PROFILER(p) p->print(); }
void Profilers::scavenge_contents() {
  FOR_EACH_PROFILER(p) p->scavenge_contents(); }
void Profilers::gc_mark_contents() {
  FOR_EACH_PROFILER(p) p->gc_mark_contents(); }
void Profilers::gc_unmark_contents() {
  FOR_EACH_PROFILER(p) p->gc_unmark_contents(); }
void Profilers::switch_pointers(oop from, oop to) {
  FOR_EACH_PROFILER(p) p->switch_pointers(from,to); }
bool Profilers::verify() {
  bool r = true;
  FOR_EACH_PROFILER(p) r &= p->verify(); 
  return r;
}


Profilers* profilers;

// Called during initialization from init.c
void profilers_init() {
  profilers = new Profilers;
}

  
# ifdef PROFILER

void Profilers::convert_nmethod_information() {
  FOR_EACH_PROFILER(p) {
    p->convert_nmethod_information();
    p->cache->flush();
  }
}


// ---- BlockProfilerTicks

const char* const block_type_labels[] = {
  "include type", 
  "method flush", // include method flush.
  "monitor tick", // exclude_monitor_tick, 
  "compile",      // exclude_recompile, 
  "recompile",    // exclude_recompile, 
  "scavenging",   // exclude_scavenging, 
  "gc",           // exclude_garbage_collection,
  "nm-flush",     // exclude_nmethod_flush,
  "nm-compact",   // exclude_nmethod_compact
  "uncommon_branch"
};

static bool has_flushed_once = false;
static ProcessTime block_time;

BlockProfilerTicks::BlockProfilerTicks(block_type b_t) {
  if (profilers->isEmpty()) return;
  bt = b_t;

  if (block_level == 0) {
    if ((prof = profilers->find_current()) != NULL) {
      prof->suspend();

      if (b_t == exclude_scavenging ||
          b_t == exclude_garbage_collection) {
        prof->allocationMonitor.update();
      }

# if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
      ProcessInfo::update();
      block_time = ProcessInfo::user_time();
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==   SUNOS_VERSION
      u_timer.start();
# endif
    }
  }
  ++block_level;

  // Flush all nmethod information if necessary.
  if (!has_flushed_once) 
    if ((has_flushed_once = should_flush_nmethod_information()) != false)
      profilers->convert_nmethod_information();
}


BlockProfilerTicks::~BlockProfilerTicks() {
  if (profilers->isEmpty()) return;
  
  --block_level;
  if (block_level == 0) has_flushed_once = false;

  if (block_level == 0 && prof) {
# if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
    ProcessInfo::update();
    block_time = ProcessInfo::user_time() - block_time;
    float t =  block_time.milli_secs_as_float();
# elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
    float t = u_timer.millisecs();
# else
    float t = 0.0; // unimp mac
#  endif
    switch (bt) {
     case include_type:
     case include_nmethod_flush:
      // Update the last stack collected with t
      prof->update_last_path(t);
      break;
     case exclude_monitor:
      prof->monitor_tick_time       += t; break;
     case exclude_compile:
      prof->compile_time            += t; break;
     case exclude_recompile:
      prof->recompile_time          += t; break;
     case exclude_scavenging:
      prof->scavenging_time         += t; break;
     case exclude_garbage_collection:
      prof->garbage_collection_time += t; break;
     case exclude_nmethod_flush:
      prof->nmethod_flush_time      += t; break;
     case exclude_nmethod_compact:
      prof->nmethod_compact_time    += t; break;
     case exclude_uncommon_branch:
      prof->uncommon_branch_time    += t; break;
     default:
      fatal("unexpected kind of exclude");
    }
    if (bt == exclude_scavenging ||
        bt == exclude_garbage_collection) {
      prof->allocationMonitor.set();
    }
    if (prof->event_buffer) prof->event_buffer->time("exclude", block_time);
    prof->resume();
  }
}

bool BlockProfilerTicks::should_flush_nmethod_information() {
  return 
    bt == include_nmethod_flush   ||
    bt == exclude_nmethod_flush   || 
    bt == exclude_recompile       ||
    bt == exclude_uncommon_branch ||
    bt == exclude_nmethod_compact;
}

void AllocationMonitor::update() {
  _blocks = NumberOfBlockClones - _last_blocks;
  _bytes  = Memory->new_gen->eden_space->used() - _last_bytes;
  set();
}

void AllocationMonitor::set() {
  _last_blocks = NumberOfBlockClones;
  _last_bytes  = Memory->new_gen->eden_space->used();
};


# endif // PROFILER
/* Sun-$Revision: 30.11 $ */

/* Copyright 1998-2006 Sun Microsystems, Inc.
   See the LICENSE file for license information. */

# pragma implementation "selfMonitor.hh"

# include "_selfMonitor.cpp.incl"


// control structures (hah, hah!)

  
# define NEW_INDICATOR(name, indType) \
    if (! name) name = new indType

# define NEW_LABEL(name, text) \
    if (! name) name = new IndicatorLabel(text)


# define FOR_ALL_BARS_DO(action) \
    FOR_ALL_CPU_BARS_DO(action); \
    FOR_ALL_MEM_BARS_DO(action);

# define FOR_ALL_CPU_BARS_DO(action) \
         _cpu_bar->action; \
        _self_bar->action; \
      _lookup_bar->action; \
    _compiler_bar->action; \
          _vm_bar->action;                                                            \

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
#   define FOR_ALL_ZONE_BARS_DO(action) \
      _i_zone->action; \
      _d_zone->action; \
      _s_zone->action; \
    _pic_zone->action;
    
#   define FOR_ALL_ZONE_BAR_LABELS_DO(action) \
     _pics_label->action, \
     _code_label->action, \
      _nic_label->action, \
      _sic_label->action, \
     _deps_label->action, \
    _debug_label->action
    
# else
#   define FOR_ALL_ZONE_BARS_DO(action)
#   define FOR_ALL_ZONE_BAR_LABELS_DO(action) 0
# endif
 

# define FOR_ALL_MEM_BARS_DO(action) \
    _eden->action; \
    _from->action; \
      _to->action; \
    {for (int32 _n=0; _n < _n_old_bars; ++_n) _old[_n]->action;} \
    FOR_ALL_ZONE_BARS_DO(action)
   

# define FOR_ALL_BAR_LABELS_DO(action) \
    FOR_ALL_ZONE_BAR_LABELS_DO(action)

# define FOR_ALL_LABELS_DO(action) \
        _self_label->action, \
         _cpu_label->action, \
      _lookup_label->action, \
    _compiler_label->action, \
    _show_sends ? _access_label->action : (void)0, \
          _vm_label->action, \
    FOR_ALL_BAR_LABELS_DO(action)

# define FOR_ALL_OPT_BARS_DO(action) \
    if (_show_sends) { \
      _access_bar->action; \
    }


// ========================================================
// Creation (& destruction)


void monitor_init() {
  TheSpy = new SelfMonitor();
}


void monitor_exit() { 
  TheSpy->deactivate(); 
}


SelfMonitor::SelfMonitor() : Monitor() {
  _is_paging_showing = _show_sends = false; 
  _pixels_per_mb = _n_old_bars = 0;
  _bpp = 1000000;
  _tick_no = _total_tick_no = _measurements_per_second = 0;
  _old_num_calls = _old_access_calls = _old_rsrc_used = _old_switches = 0;
  _sys = NULL;  _page = NULL;  _eden = NULL;  _from = NULL;  _to = NULL;  _old = NULL;
# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
  _i_zone = _d_zone = _s_zone = _pic_zone = NULL;
# endif 
  _cpu_bar = NULL;  _self_bar = NULL;  _lookup_bar = NULL;   _compiler_bar = NULL;  _vm_bar = NULL;  _access_bar = NULL;  
  _activity = _resource_areas = NULL;
  _allocs = NULL;
  _blockClones = _blockInvocations = _sends = NULL;
  _blkShort = _blkLong = NULL;
     _cpu_label = _self_label = _lookup_label = _compiler_label =    _vm_label = NULL;
  _access_label = _pics_label =   _code_label =     _deps_label = _debug_label = NULL,
      _nic_label = _sic_label  = NULL;
  
  _compile = NULL;  
}


void SelfMonitor::reset()   {
  ShowLookupInMonitor::lookup_nesting = 0;
  ShowCompileInMonitor::method_being_compiled = NULL;
}


void SelfMonitor:: initialize_contents() {
  _show_sends = SpyShowSends;
  _mem_usage_line_len = 0;


  int tps = get_measurements_per_second();
  assert(tps > 1 && tps <= 100, 
         "ticks per sec must be between 2 and 100");
  _measurements_per_second = tps;

  create_bars();
  create_labels();
  
  resize_mem_zone_bars();
  resize_cpu_bars();
  create_labels_and_indicators();

  
  FOR_ALL_BARS_DO(init());
  FOR_ALL_OPT_BARS_DO(init());
  ProcessInfo::update();
  
  LoadLevelMonitor::initialize();
  
  _cpu_bar->init_CPU();
  _is_paging_showing = false;
  _old_rsrc_used = -1;
  _old_num_calls = MonitorCallsToVM::all_calls() - 1;
  _old_access_calls = MonitorCallsToVM::access_method_calls() - 1;
  _elapsed_ticks = 0;
  _total_tick_no = 1;        // 1 to avoid divide-by-zero 
  _old_switches = 0;
}


int32 SelfMonitor::get_measurements_per_second() {
    return (int32)IntervalTimer::Real_timer()->ticks_per_second();
}


void SelfMonitor::resize_contents() {
  resize_mem_zone_bars();
}


void SelfMonitor::tick_measure() {
  if (!is_active())        return; // for gdb

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    // Return if the profiler is active.
    if (profilerSemaphore) return;
# endif

  count_ticks();
  measure_current_tick_activity();
  
  if (SilentSpy) {
    if (_tick_no >= _measurements_per_second) {
      if (logf) fputc('\n', logf);
      _tick_no = 0;
    }
  }    
}


void SelfMonitor::tick_redraw() {  
  if (!is_active() || _eden == NULL) return; // in progress activating
  if (MallocInProgress) return;  // to be safe, don't do X calls
  if (_elapsed_ticks < ticks_per_update  &&  incremental)  return;
  
  if (!mw()->pw->is_open()) {
    mw()->close();
    return;
  }
  if (!mw()->pw->pre_draw(incremental))  return;
  
  if ( !incremental ) {
    mw()->pw->clear_rectangle(0, mw()->width(), 0, mw()->height());
  }

  ProcessInfo::update();
            
  redraw_paging();
  redraw_labels();
  redraw_resources();
  _allocs->update(TrackCHeapInMonitor::allocated(), incremental);   
  redraw_context_switches();
  redraw_method_being_compiled();
  show_sends();
  show_activities();
  redraw_mem_usage();
  redraw_bars(incremental);
       
  mw()->pw->post_draw(incremental);

  incremental= true;
}


// Accessors, etc.


void SelfMonitor::show_paging(long how_many) { 
  char s[10];
  sprintf(s, " page%3ld", how_many);
  _page->show(s);
  _is_paging_showing = true;
}
  

void SelfMonitor::hide_paging() { 
  if (_is_paging_showing) { _page->hide();  _is_paging_showing = false; }
}

  
// Locations & sizes of graphical components:
int SelfMonitor::co_w()   {  return  CompileIndicator::compile_len*mw()->font_width(); }
int SelfMonitor::co_x()   {  return  2 + mw()->font_width(); }
int SelfMonitor::co_y()   {  return  2; }
  
int SelfMonitor::vm_w()   {  return  ind_w(); }
int SelfMonitor::vm_x()   {  return  co_x() - mw()->font_width(); }
int SelfMonitor::vm_y()   {  return  co_y() + ind_gap(); }
int SelfMonitor::vm_wb()  {  return  ind_bar_w(); }
int SelfMonitor::vm_xb()  {  return  vm_x() + vm_w() + mw()->font_width()/2; }
int SelfMonitor::vm_yb()  {  return  vm_y() + 2; }
int SelfMonitor::vm_hb()  {  return  ind_bar_h(); }
  
int SelfMonitor::lkup_w()   {  return  ind_w(); }
int SelfMonitor::lkup_x()   {  return  vm_x(); }
int SelfMonitor::lkup_y()   {  return  vm_y() + ind_gap(); }
int SelfMonitor::lkup_wb()  {  return  ind_bar_w() ; }
int SelfMonitor::lkup_xb()  {  return  vm_xb(); }
int SelfMonitor::lkup_yb()  {  return  lkup_y() + ind_bar_yoff(); }
int SelfMonitor::lkup_hb()  {  return  ind_bar_h(); }

int SelfMonitor::comp_w()   {  return  ind_w(); }
int SelfMonitor::comp_x()   {  return  vm_x(); }
int SelfMonitor::comp_y()   {  return  lkup_y() + ind_gap(); }
int SelfMonitor::comp_wb()  {  return  ind_bar_w(); }
int SelfMonitor::comp_xb()  {  return  vm_xb(); }
int SelfMonitor::comp_yb()  {  return  comp_y() + ind_bar_yoff(); }
int SelfMonitor::comp_hb()  {  return  ind_bar_h(); }

int SelfMonitor::self_w()   {  return  ind_w() ; }
int SelfMonitor::self_x()   {  return  vm_x(); }
int SelfMonitor::self_y()   {  return  comp_y() + ind_gap(); }
int SelfMonitor::self_wb()  {  return  ind_bar_w() ; }
int SelfMonitor::self_xb()  {  return  vm_xb(); }
int SelfMonitor::self_yb()  {  return  self_y() + ind_bar_yoff(); }
int SelfMonitor::self_hb()  {  return  ind_bar_h(); }

int SelfMonitor::cpu_w()   {  return  ind_w() ; }
int SelfMonitor::cpu_x()   {  return  vm_x(); }
int SelfMonitor::cpu_y()   {  return  self_y() + ind_gap(); }
int SelfMonitor::cpu_wb()  {  return  ind_bar_w() ; }
int SelfMonitor::cpu_xb()  {  return  vm_xb(); }
int SelfMonitor::cpu_yb()  {  return  cpu_y() + ind_bar_yoff(); }
int SelfMonitor::cpu_hb()  {  return  ind_bar_h(); }

int SelfMonitor::ruler_y()     {  return  eden_y() + (3 + old_thick()) * bar_height() + 12; }
int SelfMonitor::short_tick()  {  return  5; }
int SelfMonitor::long_tick()   {  return 10; }
int SelfMonitor::contents_height() {  return  max( ruler_y(), (cpu_y() + mw()->font_height()) ) + 4;  }

// process switch dot
int SelfMonitor::ctx_x()   {  return  cpu_x() + mw()->font_width(); }
int SelfMonitor::ctx_y()   {  return  cpu_y() + 12; }
int SelfMonitor::ctx_w()   {  return  cpu_xb() + cpu_wb() - ctx_x(); }
int SelfMonitor::ctx_s()   {  return  2; }

// scavenge/GC/flushing/compacting/icflush/lrusweep
int SelfMonitor::act_w()   {  return  12 * mw()->font_width(); }
int SelfMonitor::act_x()   {  return  vm_xb() + vm_wb() + 2; }
int SelfMonitor::act_y()   {  return  vm_y(); }

// page
int SelfMonitor::pg_w()    {  return  8 * mw()->font_width(); }
int SelfMonitor::pg_x()    {  return  act_x(); }
int SelfMonitor::pg_y()    {  return  lkup_y(); }

// read/write/diskio/unix/idle/nothing
int SelfMonitor::sys_w()   {  return  10 * mw()->font_width(); }
int SelfMonitor::sys_x()   {  return  pg_x() + pg_w() + 2; }
int SelfMonitor::sys_y()   {  return  pg_y(); }

// RSRC  
int SelfMonitor::res_w()   {  return  30 * mw()->font_width(); }
int SelfMonitor::res_x()   {  return  self_xb() + self_wb() + 5; }
int SelfMonitor::res_y()   {  return  self_y(); }

// C-Heap  
int SelfMonitor::all_w()   {  return  20 * mw()->font_width(); }
int SelfMonitor::all_x()   {  return  res_x(); }
int SelfMonitor::all_y()   {  return  cpu_y(); }

// sends/block creations etc (optional)
int SelfMonitor::opt_x()   {  return  res_x() + res_w() + 5; }
int SelfMonitor::opt_w()   {  return  19 * mw()->font_width(); }

// block clones
int SelfMonitor::blkcln_x()   {  return  opt_x();  }
int SelfMonitor::blkcln_w()   {  return  opt_w();  }
int SelfMonitor::blkcln_y()   {  return  vm_y();  }

// block invocations
int SelfMonitor::blkinv_x()   {  return  opt_x();  }
int SelfMonitor::blkinv_w()   {  return  opt_w();  }
int SelfMonitor::blkinv_y()   {  return  lkup_y();  }

// % block invocations (short / long-term)
int SelfMonitor::blks_x()   {  return  opt_x();  }
int SelfMonitor::blks_w()   {  return  13 * mw()->font_width();  }
int SelfMonitor::blks_y()   {  return  comp_y();  }
int SelfMonitor::blkl_x()   {  return  blks_x() + blks_w();  }
int SelfMonitor::blkl_w()   {  return  5 * mw()->font_width();  }
int SelfMonitor::blkl_y()   {  return  blks_y();  }

// total sends
int SelfMonitor::sends_x()   {  return  opt_x();  }
int SelfMonitor::sends_w()   {  return  opt_w();  }
int SelfMonitor::sends_y()   {  return  self_y();  }

// data access sends
int SelfMonitor::acc_x()    {  return  opt_x();  }
int SelfMonitor::acc_w()    {  return  10 * mw()->font_width();  }
int SelfMonitor::acc_y()    {  return  cpu_y();  }
int SelfMonitor::acc_wb()   {  return  ind_bar_w() ;  }
int SelfMonitor::acc_xb()   {  return  acc_x() + acc_w();  }
int SelfMonitor::acc_yb()   {  return  acc_y() + ind_bar_yoff();  }
int SelfMonitor::acc_hb()   {  return  ind_bar_h();  }

// memory bars position
int SelfMonitor::mbar_x()   {  return  opt_x() + (_show_sends ? opt_w() : 0);  }
int SelfMonitor::eden_y()   {  return  vm_yb();  }

// memory bar label positions
int         SelfMonitor::pics_x()   {  return  _pic_zone->x;  }
int         SelfMonitor::pics_y()   {  return  _pic_zone->y - mw()->font_height();  }
const char* SelfMonitor::pics_t()   {  return  "PICs";  }
int         SelfMonitor::pics_w()   {  return  4*mw()->font_width();  }

int         SelfMonitor::code_x()   {  return  _i_zone->x + (_i_zone->w / 2) - (code_w() / 2);  }
int         SelfMonitor::code_y()   {  return  _i_zone->y + _i_zone->h + 2;  }
const char* SelfMonitor::code_t()   {  return  "code";  }
int         SelfMonitor::code_w()   {  return  4*mw()->font_width();  }

int         SelfMonitor::nic_x()   {  return  _i_zone->x;  }
int         SelfMonitor::nic_y()   {  return  code_y();  }
const char* SelfMonitor::nic_t()   {  return  "NIC";  }
int         SelfMonitor::nic_w()   {  return  3*mw()->font_width();  }

int         SelfMonitor::sic_x()   {  return  _i_zone->x + _i_zone->w - sic_w();  }
int         SelfMonitor::sic_y()   {  return  code_y();  }
const char* SelfMonitor::sic_t()   {  return  "SIC";  }
int         SelfMonitor::sic_w()   {  return  3*mw()->font_width();  }

int         SelfMonitor::deps_x()   {  return  _d_zone->x;  }
int         SelfMonitor::deps_y()   {  return  _d_zone->y + _d_zone->h + 2;  }
const char* SelfMonitor::deps_t()   {  return  "deps";  }
int         SelfMonitor::deps_w()   {  return  4*mw()->font_width();  }

int         SelfMonitor::dbug_x()   {  return  _s_zone->x;  }
int         SelfMonitor::dbug_y()   {  return  _s_zone->y + _s_zone->h + 2;  }
const char* SelfMonitor::dbug_t()   {  return  "debug";  }
int         SelfMonitor::dbug_w()   {  return  5*mw()->font_width();  }



oop SelfMonitor::resetLog_prim(oop rcvr) {  
  TheSpy->reset_log();
  return rcvr;
}


oop SelfMonitor::annotateLog_prim(oop rcvr) { 
  ResourceMark rm;
  if (rcvr->is_byteVector())
    TheSpy->annotate_log(byteVectorOop(rcvr)->copy_null_terminated());
  return rcvr;
}


void SelfMonitor::set_bytes_per_pixel() {
  // find which of to and old space are widest
  fint maxMem= TrackObjectHeapInMonitor::new_capacity();
  fint t=      TrackObjectHeapInMonitor::old_capacity() / old_thick();
  if (t > maxMem) maxMem= t;
  
  int zone_gap = 0;
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    zone_gap = mz_gap() + (2*z_gap());
# endif

  int32 cap = 0;
  cap = MonitorCallsToVM::code_capacity();
  
  _bpp=
    (maxMem + cap/old_thick()) / 
      (mw()->width()
        - (mbar_x())
        - ((TrackObjectHeapInMonitor::n_spaces() - 1) * m_gap())
        - zone_gap
        - right_margin());
        
  if (_bpp == 0) _bpp = 1; // hack to prevent overflow below      
  _pixels_per_mb= MB / (_bpp * old_thick());
}


void SelfMonitor::draw_mem_usage_line() {
  const int line_len= TrackObjectHeapInMonitor::old_used() / (bytes_per_pixel() * old_thick());
  if (!incremental && line_len  ==  _mem_usage_line_len) return;
  mw()->pw->set_color(mw()->pw->white());
  mw()->pw->draw_line(mbar_x(), ruler_y(), mbar_x() + _mem_usage_line_len, ruler_y());
  mw()->pw->set_color(mw()->pw->black());
  _mem_usage_line_len= line_len;
  mw()->pw->draw_line(mbar_x(), ruler_y(), mbar_x() + _mem_usage_line_len, ruler_y());
}


void SelfMonitor::draw_reserve_lines() {
  // red line
  const fint red_line_len=
    TrackObjectHeapInMonitor::old_VM_reserved_memory() / (bytes_per_pixel() * old_thick());
  mw()->pw->set_color(mw()->pw->red());
  mw()->pw->set_thickness(mw()->pw->is_mono() ? 3 : 2);
  fint end=   mbar_x() + _ruler_len;
  fint start= end - red_line_len;
  mw()->pw->draw_line(start, ruler_y(), end, ruler_y());
  // yellow line
  const fint yellow_line_len=
    TrackObjectHeapInMonitor::old_low_space_threshold() / (bytes_per_pixel() * old_thick())
      - red_line_len;
  end=   start;
  start= end - yellow_line_len;
  mw()->pw->set_color(mw()->pw->yellow());
  mw()->pw->set_thickness(2);
  mw()->pw->draw_line(start, ruler_y(), end, ruler_y());
  mw()->pw->set_color(mw()->pw->black());
  mw()->pw->set_thickness(0);
}


void SelfMonitor::draw_memory_ruler() {
  _ruler_len= TrackObjectHeapInMonitor::old_capacity() / (bytes_per_pixel() * old_thick());
  const fint nTicks= _ruler_len / _pixels_per_mb + 1;
  for (int x_off= mbar_x(), n= 0; n < nTicks; ++n, x_off += _pixels_per_mb)
    mw()->pw->draw_line(x_off, ruler_y(), x_off,
                        ruler_y() - (n % 5 == 0 ? long_tick() : short_tick()));
}



// ********************* interrupt handler *********************


void SelfMonitor::resize_mem_zone_bars() {
  if ( !is_active()  ||  _eden == NULL )   return;
  SignalBlocker sb;
  mw()->pw->clear_rectangle(mbar_x(), 0, mw()->width() - mbar_x(),  mw()->height());
  incremental = false;
  set_bytes_per_pixel();
  _eden->resize(mbar_x(), eden_y(),            bar_height(), 1);
  _from->resize(mbar_x(), eden_y() + bar_height(),   bar_height(), 1);
    _to->resize(mbar_x(), eden_y() + 2*bar_height(), bar_height(), 1);
  int lastX= mbar_x();
  for (int n= 0;  n < _n_old_bars;  lastX += _old[n++]->w + m_gap())
    _old[n]->resize(lastX, eden_y() + 3*bar_height(), bar_height(), old_thick());

  lastX -= m_gap();
  if (lastX < mbar_x() + _to->w) lastX= mbar_x() + _to->w; // to is wider than old
  lastX += mz_gap();

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
  // pic_zone can move
  _pic_zone->resize(lastX, eden_y() + 2*bar_height(), bar_height(), 1,
                  Memory->code->stubs->zone());
    _i_zone->resize(lastX, eden_y() + 3*bar_height(), bar_height(), old_thick());
  lastX += _i_zone->w + z_gap();
    _d_zone->resize(lastX, eden_y() + 3*bar_height(), bar_height(), old_thick());
  lastX += _d_zone->w + z_gap();
    _s_zone->resize(lastX, eden_y() + 3*bar_height(), bar_height(), old_thick());

   _pics_label->reposition(pics_x(), pics_y(), pics_w());
   _code_label->reposition(code_x(), code_y(), code_w());
    _nic_label->reposition( nic_x(),  nic_y(),  nic_w());
    _sic_label->reposition( sic_x(),  sic_y(),  nic_w());
   _deps_label->reposition(deps_x(), deps_y(), deps_w());
  _debug_label->reposition(dbug_x(), dbug_y(), dbug_w());
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

  TrackObjectHeapInMonitor::reserve_changed();
}


void SelfMonitor::create_bars() {
  if (!_eden) { // otherwise reuse objects
    _eden= new MonitorMemBar(Memory->new_gen->eden_space);
    _from= new MonitorMemBar(Memory->new_gen->from_space);
    _to=   new MonitorMemBar(Memory->new_gen->to_space);
    
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    _i_zone=   new MonitorCodeCacheBar(Memory->code->iZone);
    _pic_zone= new MonitorZoneBar(Memory->code->stubs->zone());
    _d_zone=   new MonitorZoneBar(Memory->code->dZone);
    _s_zone=   new MonitorZoneBar(Memory->code->sZone);
#   endif
  }

  _cpu_bar      = new MonitorCPUBar(1000000 * ticks_per_update / _measurements_per_second);
  _vm_bar       = new MonitorSampledBar();
  _self_bar     = new MonitorSampledBar();
  _lookup_bar   = new MonitorSampledBar();
  _compiler_bar = new MonitorSampledBar();
  _access_bar   = new MonitorSampledBar();

  create_old_bars();
}

void SelfMonitor::create_old_bars() {    
  // need to recreate this as the number of spaces may have changed
  if (_old) {
    for (int n= 0;  n < _n_old_bars;  ++n) {
      delete _old[n];
      _old[n] = NULL; // paranoia
    }
    delete [] _old;
    _old = NULL; // paranoia
  }
  _old=  new MonitorMemBar *[TrackObjectHeapInMonitor::n_spaces()];
  _n_old_bars= 0;
  {FOR_EACH_OLD_SPACE(s) 
    _old[_n_old_bars++]= new MonitorMemBar(s);}
}


void SelfMonitor::create_labels() {
    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  NEW_LABEL( _pics_label, pics_t());
  NEW_LABEL( _code_label, code_t());
  NEW_LABEL(  _nic_label,  nic_t());
  NEW_LABEL(  _sic_label,  sic_t());
  NEW_LABEL( _deps_label, deps_t());
  NEW_LABEL(_debug_label, dbug_t());
# endif
}


void SelfMonitor::resize_cpu_bars() {
       _cpu_bar->resize(  cpu_xb(),  cpu_yb(),  cpu_wb(),  cpu_hb());
        _vm_bar->resize(   vm_xb(),   vm_yb(),   vm_wb(),   vm_hb());
      _self_bar->resize( self_xb(), self_yb(), self_wb(), self_hb());
    _lookup_bar->resize( lkup_xb(), lkup_yb(), lkup_wb(), lkup_hb());
  _compiler_bar->resize( comp_xb(), comp_yb(), comp_wb(), comp_hb());
  if (_show_sends) {
    _access_bar->resize(  acc_xb(),  acc_yb(),  acc_wb(),  acc_hb());
  }      
}


void SelfMonitor::create_labels_and_indicators() {
  NEW_INDICATOR( _page, Indicator) ;
  _page->reposition(pg_x(), pg_y(), pg_w());

  NEW_LABEL(_cpu_label, " CPU ");
  _cpu_label->reposition(cpu_x(), cpu_y(), cpu_w());
  NEW_LABEL(_vm_label, " VM ");
  _vm_label->reposition(vm_x(), vm_y(), vm_w());
  NEW_LABEL(_self_label, " Self ");
  _self_label->reposition(self_x(), self_y(), self_w());
  NEW_LABEL(_lookup_label, " Lkup ");
  _lookup_label->reposition(lkup_x(), lkup_y(), lkup_w());
  NEW_LABEL(_compiler_label, " Comp ");
  _compiler_label->reposition(comp_x(), comp_y(), comp_w());
  NEW_LABEL(_access_label, "% access ");
  _access_label->reposition(acc_x(), acc_y(), acc_w());
  
  NEW_INDICATOR(_resource_areas, Indicator);
  _resource_areas->reposition(res_x(), res_y(), res_w());
  NEW_INDICATOR(_allocs, ValueIndicator(" C-Heap: XX,XXX,XXX", false, 8));
  _allocs->reposition(all_x(), all_y(), all_w());
  NEW_INDICATOR(_compile, CompileIndicator);
  _compile->reposition(co_x(), co_y(), co_w());


  NEW_INDICATOR(_sys, SharedIndicator);
  _sys->reposition(sys_x(), sys_y(), sys_w());
  NEW_INDICATOR(_activity, Indicator);
  _activity->reposition(act_x(), act_y(), act_w());
  if (_show_sends) {
    fint tps = _measurements_per_second / ticks_per_update;
    NEW_INDICATOR(_blockClones,
                  DifferenceIndicator("BlkClone: XXX,XXX", true, 6, tps));
    _blockClones->reposition(blkcln_x(), blkcln_y(), blkcln_w());
    NEW_INDICATOR(_blockInvocations,
                  DifferenceIndicator("BlkInvoc: XXX,XXX", true, 6, tps));
    _blockInvocations->reposition(blkinv_x(), blkinv_y(), blkinv_w());
    NEW_INDICATOR(_blkShort, ValueIndicator("% blkInv:XXX%", false, 3, 1, 1));
    _blkShort->reposition(blks_x(), blks_y(), blks_w());
    NEW_INDICATOR(_blkLong, ValueIndicator("/XXX%", false, 3, 1, 1));
    _blkLong->reposition(blkl_x(), blkl_y(), blkl_w());
    NEW_INDICATOR(_sends, DifferenceIndicator("Tot Snds: XXX,XXX", true, 6, tps));
    _sends->reposition(sends_x(), sends_y(), sends_w());
  }
}

  
void SelfMonitor::count_ticks() {
  BlockProfilerTicks bpt(exclude_monitor);
  ++_tick_no;
  ++_total_tick_no;
  ++_elapsed_ticks;
}


void SelfMonitor::measure_current_tick_activity() {
  char c = ExecutionMonitor::count_tick_and_return_log_char();
  if (logf != NULL)  fputc(c, logf);
}



void SelfMonitor::redraw_labels() {
  if (!incremental) {
    FOR_ALL_LABELS_DO(show());
  }
}


void SelfMonitor::redraw_paging() {
  long faults   = ProcessInfo::page_faults_IO();
  
  update_load_level();
  update_time_bars();
  
  if (faults != ProcessInfo::page_faults_IO()) {
    show_paging(ProcessInfo::page_faults_IO() - faults);
  } else {
    hide_paging();
  }
}


void SelfMonitor::redraw_resources() {
  char s[50];
  int32 rsrc_used;
  if (!ResourceAreaMonitor::is_consistent()) {
    // interrupt during inconsistent state -- do nothing till next time
    rsrc_used = _old_rsrc_used;
  }
  else {
    rsrc_used = ResourceAreaMonitor::used() / K;
  }
  if (_old_rsrc_used != rsrc_used || ! incremental) {
    sprintf(s, " RSRC: %5ldK (%5ldK used) ", long(resources.capacity() / K),
            long(rsrc_used));
    _resource_areas->show(s);
    _old_rsrc_used = rsrc_used;
  }
}


void SelfMonitor::redraw_context_switches() {
  int32 cs = ShowContextSwitchInMonitor::context_switches();
  if (_old_switches != cs) {
    // don't get here every cs so clear all
    mw()->pw->clear_rectangle(ctx_x(), ctx_y(), ctx_w(), ctx_s() + 1);
    int32 pos = cs % (ctx_w() - ctx_x());
    mw()->pw->draw_square(ctx_x() + pos, ctx_y(), ctx_s());
    _old_switches = cs;
  }
}


void SelfMonitor::redraw_method_being_compiled() {
  if (ShowCompileInMonitor::method_being_compiled == NULL) {
    _compile->hide();
  }
  else if (ShowCompileInMonitor::method_changed()) {
    _compile->hide();
    _compile->show(ShowCompileInMonitor::method_being_compiled, 
                  ShowCompileInMonitor::current_compiler_name, 
                  ShowCompileInMonitor::recompiling);
  }
}


void SelfMonitor::redraw_mem_usage() {
  if (!incremental  ||  _mem_usage_line_len == 0) {
#     if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    mw()->pw->clear_rectangle(mbar_x(), ruler_y() - long_tick(),
                              _i_zone->x - mbar_x(), mw()->height() - ruler_y() - long_tick());
#     endif
    draw_memory_ruler();
    draw_reserve_lines();
  }
  draw_mem_usage_line();
}


void SelfMonitor::redraw_bars(bool incremental) {
  FOR_ALL_MEM_BARS_DO(draw(incremental)); 
  FOR_ALL_OPT_BARS_DO(draw(incremental)); 
  FOR_ALL_CPU_BARS_DO(draw(incremental));
}


void SelfMonitor::update_load_level() {
  if (_tick_no >= _measurements_per_second) {
    // quick reality check: if we're running on a loaded machine, we'll miss
    // ticks, and so what we think is a second is really more
    LoadLevelMonitor::compute_load_level();
    _tick_no = 0;
    FOR_ALL_BARS_DO(calculate_VM_stats());    // only once a second (expensive)
  }    
}
  

void SelfMonitor::update_time_bars() {
    
  _cpu_bar->update();

  fint nticks = ExecutionMonitor::all_ticks()
              + ShowCompileInMonitor     ::all_ticks();
              
  if (nticks == 0) {
    // just reset the bars, don't update long-term averages etc
        _self_bar->update0();
      _lookup_bar->update0();
    _compiler_bar->update0();
          _vm_bar->update0();
  } 
  else {
    fint percent = _cpu_bar->current_usage();
    fint elapsed = _elapsed_ticks * 100;
    if (Interpret)
      _self_bar->update(percent * ExecutionMonitor::self_ticks[0], 
                       percent * ExecutionMonitor::self_ticks[1], 
                       percent * ExecutionMonitor::self_ticks[2],
                       0,
                       elapsed);
    else
      _self_bar->update(percent * ExecutionMonitor::self_ticks[1], 
                       percent * ExecutionMonitor::self_ticks[2], 
                       elapsed);
    _lookup_bar->update(percent * ExecutionMonitor::lookup_ticks, 0, elapsed);
    _compiler_bar->update(percent * ShowCompileInMonitor::compiler_ticks[0],
                         percent  * ShowCompileInMonitor::compiler_ticks[1], elapsed);
    _vm_bar->update(percent * ExecutionMonitor::vm_ticks, 0, elapsed);
    ExecutionMonitor::_used_ticks += percent * nticks;
                            
  }
  ExecutionMonitor::self_ticks[0] = ExecutionMonitor::self_ticks[1] = ExecutionMonitor::self_ticks[2] = ExecutionMonitor::lookup_ticks = 0;
  ShowCompileInMonitor::compiler_ticks[0] = ShowCompileInMonitor::compiler_ticks[1] = ExecutionMonitor::vm_ticks = 0;
  _elapsed_ticks = 0;
  if (logf) fputc('\n', logf);
}



void SelfMonitor::show_sends() {
  if (!_show_sends) 
    return;
    
  _blockClones->     update(MonitorCallsToVM::block_clones(),       incremental);
  _blockInvocations->update(MonitorCallsToVM::block_method_calls(), incremental);
  int32 shortPercent = _blockClones->value()
    ?  _blockInvocations->value() * 100 / _blockClones->value()
    :  0;
  _blkShort->update(shortPercent, incremental);
  int32 longPercent = MonitorCallsToVM::block_clones() 
    ? int32(100.0 * MonitorCallsToVM::block_method_calls() / MonitorCallsToVM::block_clones()) 
    : 0;
  _blkLong->update(longPercent, incremental);

  int32 numCalls = MonitorCallsToVM::all_calls();
  int32 calls = numCalls - _old_num_calls;
  _old_num_calls = numCalls;
  _sends->update(numCalls, incremental);
  if (_old_access_calls > MonitorCallsToVM::access_method_calls()) _old_access_calls = 0;
  int32 percentAccess;
  if (calls) {
    percentAccess =
      (MonitorCallsToVM::access_method_calls() - _old_access_calls) * 100 / calls;
    _access_bar->update(percentAccess, 0, 100);
  }
  _old_access_calls = MonitorCallsToVM::access_method_calls();
}

  
  
void SelfMonitor::show_activities() {
  long inblock  = ProcessInfo::block_input_operations();
  long outblock = ProcessInfo::block_output_operations();
  ProcessInfo::update();
 
   inblock  = ProcessInfo:: block_input_operations() -  inblock;
  outblock  = ProcessInfo::block_output_operations() - outblock;

  OSActivityMonitor::SystemState s = OSActivityMonitor::activity();
  if ( s == OSActivityMonitor::nothing )  _sys->hide();
  else                                _sys->show( OSActivityMonitor::state_string(s), s);
  
  _activity->hide();
  if (ShowVMActivityInMonitor::current_activity) {
    _activity->show(ShowVMActivityInMonitor::current_activity);
  }
}
 
  
void SelfMonitor::recreate_old_bars() {
  if (!is_active()) return;
  SignalBlocker sb; // block out signals until done

  create_old_bars();

  set_bytes_per_pixel();
  adjust_after_resize();
}






fint  ExecutionMonitor::self_ticks[3] = {0, 0, 0};
fint  ExecutionMonitor::lookup_ticks = 0;
fint  ExecutionMonitor::vm_ticks = 0;

int32 ExecutionMonitor::_used_ticks    = 1; // avoid divide by zero

void ExecutionMonitor::initialize() {
  _used_ticks = 1;
}


ExecutionMonitor::Activities ExecutionMonitor::compiled_activity(char* pc) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    nmethod* nm = nmethod::findNMethod(pc);
    fint c = nm->compiler();
    return  c == NIC  ?  fast_execution  :  opt_execution;
# else
    return idle;
# endif
}


ExecutionMonitor::Activities ExecutionMonitor::current_tick_activity() {
  // find part of system to which current tick should be attributed
  if ( ShowLookupInMonitor::lookup_nesting >  1 
  ||   (ShowLookupInMonitor::lookup_nesting == 1  &&  !ShowCompileInMonitor::method_being_compiled) )
    return lookup;
  
  if (ShowCompileInMonitor::method_being_compiled)
    return ShowCompileInMonitor::current_compiler_ticks == &ShowCompileInMonitor::compiler_ticks[0] 
             ?  fast_compiler
             :   opt_compiler;

  
  frame* f;
  char* pc;
  InterruptedFrameMonitor::get_frame_and_pc( f, pc );
    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    if (Memory->code->contains(pc))
      return compiled_activity(pc);
    
    if (Memory->code->stubs->contains(pc)) {
      // charge PIC time to Self
      NCodeBase* s = findThing(pc);
      if (s->isCacheStub()) {
        pc = (char*) ((CacheStub*)s)->sd();
      } 
      else {
        assert(s->isCountStub(), "what is it?");
        CacheStub* pic = ((CountStub*)s)->pic();
        pc = pic ? (char*)pic->sd() : (char*)((CountStub*)s)->sd();
      }
      return compiled_activity(pc);
    }
# endif  // defined(FAST_COMPILER) || defined(SIC_COMPILER)

  if ( f != NULL  &&  f->is_interpreted_self_frame())
    return int_execution;

  if (processes->isIdle())  return idle;
  if (ScavengeInProgress)   return scavenge;
  if (     GCInProgress)    return gc;
                            return virtual_machine;
}


char ExecutionMonitor::count_tick_and_return_log_char() {
  static fint dummy = 0;
  switch ( current_tick_activity() ) { 
   case          lookup: ++lookup_ticks;                          return 'L';
   case   fast_compiler: ++*ShowCompileInMonitor::current_compiler_ticks;  return 'F';
   case    opt_compiler: ++*ShowCompileInMonitor::current_compiler_ticks;  return 'O';
   case   int_execution: ++self_ticks[0];                         return 'i';
   case  fast_execution: ++self_ticks[1];                         return 's';
   case   opt_execution: ++self_ticks[2];                         return 'S';
   case virtual_machine: ++vm_ticks;                              return 'V';
   case        scavenge: ++vm_ticks;                              return 'g';
   case              gc: ++vm_ticks;                              return 'G';
   case            idle: ++dummy;                                 return ' ';
   default: fatal("should not be here"); 
  }
  return '\0';
}



bool MonitorCallsToVM::in_read_trap()   { return InterruptedContext::the_interrupted_context->in_read_trap(); }
bool MonitorCallsToVM::in_write_trap()  { return InterruptedContext::the_interrupted_context->in_write_trap(); }
bool MonitorCallsToVM::in_system_trap() { return InterruptedContext::the_interrupted_context->in_system_trap(); }


int MonitorCallsToVM::block_clones()         { return NumberOfBlockClones; }
int MonitorCallsToVM::block_method_calls()   { return NumberOfBlockMethodCalls; }
int MonitorCallsToVM::access_method_calls()  { return NumberOfAccessMethodCalls; }
int MonitorCallsToVM::method_calls()         { return NumberOfMethodCalls; }
int MonitorCallsToVM::di_method_calls()      { return NumberOfDIMethodCalls; }
int MonitorCallsToVM::all_calls() { 
  return block_method_calls() + access_method_calls() + method_calls() + di_method_calls(); 
}

fint MonitorCallsToVM::code_capacity() { return Memory->code->capacity(); }

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "runtime.hh"
# include "_runtime.cpp.incl"

// This file holds C-level things that some platforms put in
//  their runtime_foo.s 's.


// Well, all platforms need this here.
// It is used by asm glue for trap handling
char* continuePC;


// a routine that assembly glue can call if a process returns off the top

extern "C" void ReturnOffTopOfProcess() {
  fatal("Self process returned past top");
}

/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "hprofiler.hh"
# include "_hprofiler.cpp.incl"

HProfiler* hprofiler;

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

// this is an early hierarchical profiler; it is now obsolete (only useful
// for VM performance debugging, at most)

# define MAX_STACK_DEPTH 10000

const char* IndentString = "  ";
const float LeavesCutoff = 0.02;        // whom to list in the leaves section

static AsyncAllocator* pnodeAllocator;
 
typedef void (*pnodeDoFn)(pnode* p);

class pnodeBase {
 public:
  virtual int32 ownTicks() = 0;
  virtual void printSelf(float percent) = 0;

  virtual ~pnodeBase() {};
};

static pnode* newList = NULL;   // list of pnodes with new oops

class pnode : public pnodeBase {      // node in profiled calling tree
public:
  pnode* next;                  // sibling
  pnode* callee;                // methods called by me
  pnode* nextNew;               // for scavenging (pnodes w/new oops)
  oop method;                   // method oop 
  oop selector;
  bool isAccess;
  int32 _ticks[2];              // 0 = self, 1 = vm; includes callees
  int32 _ownTicks[2];           // my own ticks (calculated during sort)

  static char*   prefix;        // line prefix for printing

 
  pnode(oop meth, oop sel, bool isAcc) {
    method = meth; selector = sel; isAccess = isAcc;
    callee = next = NULL;
    _ticks[0] = _ticks[1] = _ownTicks[0] = _ownTicks[1] = 0;
    if (is_new()) {
      nextNew = newList->nextNew; newList->nextNew = this;
    } else {
      nextNew = NULL;
    }
  }

  void* operator new(size_t size) { return pnodeAllocator->async_malloc(size); }
  void  operator delete(void* p)  { pnodeAllocator->async_free(p); }
  void  dealloc();

  void pnodeDo(pnodeDoFn f) {
    f(this);
    if (callee) callee->pnodeDo(f);
    if (next)   next->pnodeDo(f);
  }
  
  void  sort();
  void  addLeaf();
  void  tick(bool self)         {  _ticks[self ? 0 : 1]++; }
  
  bool  equal(nmethod* nm);
  bool  isTop()                 { return this == hprofiler->top; }
  bool  is_new()                { return method->is_new() ||
                                         selector->is_new(); }
  int32 selfTicks()             { return _ticks[0]; }
  int32 vmTicks()               { return _ticks[1]; }
  int32 ticks()                 { return selfTicks() + vmTicks(); }
  int32 ownSelfTicks()          { return _ownTicks[0]; }
  int32 ownVMTicks()            { return _ownTicks[1]; }
  int32 ownTicks()              { return ownSelfTicks() + ownVMTicks(); }
  
  void print(int32 total, float cutoff, float skip, smi maxDepth, bool first);
  void printSelf(float percent);
};

class primPnode : public pnode {      // for primitive leaves: lookup, compile, gc
public:
  // ignore all inherited members -- they're not used
  int32       _ticks;
  const char* name;
  bool        included;

  primPnode(const char* n, int32 t, bool i) : pnode(0,0,false) {
    name = n; _ticks = t; included = i;}
  void* operator new(size_t size);
  void  operator delete(void* p);
  int32 ownTicks()                      { return _ticks; }
  void  printSelf(float percent);
};

void* primPnode::operator new(size_t size) { return selfs_malloc(size); }
void  primPnode::operator delete(void* p) { selfs_free(p); }


char*  pnode::prefix;
static pnode** leaves;          // used during profile printing
static int32   nleaves;         // # elements in leaves


# if  GENERATE_CODE_TO_AID_DEBUGGING_ALLOC
  static pnode* ref_ptr;
  static pnode* last_ref;
  static void count_ref_fn(pnode* pn) {
    if (pn->next == ref_ptr || pn->callee == ref_ptr) {
      if (last_ref) fatal("has more than one reference to it");
      last_ref = pn;
    }
  }
# endif

void pnode::dealloc() {
# if  GENERATE_CODE_TO_AID_DEBUGGING_ALLOC
    ref_ptr = this;             // wish we had GC...
    last_ref = NULL;
    hprofiler->top->pnodeDo(count_ref_fn);
    lprintf("deleting %#lx ", this);
#endif
  if (callee) callee->dealloc();
  if (next) next->dealloc();
  delete this;
}

static int compare_pn(const void* p1,  const void* p2) {
  pnode** pn1 = (pnode**)p1;
  pnode** pn2 = (pnode**)p2;
  return (*pn1)->ticks() - (*pn2)->ticks();
}

static int compare_pn_leaf(const void* p1,  const void* p2) {
  pnode** pn1 = (pnode**)p1;
  pnode** pn2 = (pnode**)p2;
  return (*pn2)->ownTicks() - (*pn1)->ownTicks();
}

void pnode::sort() {
  // sort all children of this node
  if (!callee) return;
  if (callee->next) {
    ResourceMark rm;
    pnode** pns = NEW_RESOURCE_ARRAY( pnode*, MAX_STACK_DEPTH);
    int32 n = 0;
    pnode* p;
    for ( p = callee; p; p = p->next) {
      pns[n++] = p;
      if (n == MAX_STACK_DEPTH)
        break;
    }
    qsort(pns, n, sizeof(pnode*), compare_pn);
    callee = pns[--n];
    for ( ; n > 0; n--) {
      pns[n]->next = pns[n-1];
    }
    pns[0]->next = NULL;
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        for (p = callee; p->next; p = p->next) {
          assert(p->ticks() >= p->next->ticks(), "not sorted");
        }
      }
#   endif
  }
  _ownTicks[0] = _ticks[0]; _ownTicks[1] = _ticks[1];
  for (pnode* p = callee; p; p = p->next) {
    _ownTicks[0] -= p->_ticks[0]; _ownTicks[1] -= p->_ticks[1]; 
    p->sort();
  }
  assert(ownVMTicks() >= 0 && ownSelfTicks() >= 0, "inconsistent ticks");
}

void printNum(float val) {
  lprintf("%s%*.1f%%   ", pnode::prefix, val >= 99.5 ? 5 : 4, val);
}

void pnode::printSelf(float percent) {
  if (isTop()) return;          // dummy top node
  printNum(percent);
  if (isAccess) {
    printName(NULL, selector);
    lprintf(" (slot accessor)");
  } else {
    printName((methodMap*) method->map(), selector);
  }
  lprintf("\n");
}

void primPnode::printSelf(float percent) {
  printNum(percent);
  lprintf("<%s>", name);
  if (included) {
    lprintf(" (included in tree under <primitives>)\n");
  } else {
    lprintf(" (not included in tree)\n");
  }
}

void pnode::print(int32 total, float cutoff, float skip, smi depth, bool first) {
  if (ticks() >= cutoff) {
    if (ownTicks() >= LeavesCutoff * total) addLeaf();
    printSelf(100.0 * ticks() / total);
    int32 len = strlen(prefix);         // extend prefix string
    strcat(prefix, IndentString);
    if (next && next->ticks() >= cutoff) prefix[len + 1] = '|';
    if (callee && depth > 0) {
      // skip intermediate frames if they contain less than skip ticks
      // but not if this is the top node
      pnode* c = callee;
      if (!isTop()) {
        for ( ; c && c->ticks() > ticks() - skip; c = c->callee) ;
      }
      if (c) c->print(total, cutoff, skip, depth - 1, true);
    }
    if (ownVMTicks() >= cutoff) {
      printNum(100.0 * ownVMTicks() / total);
      lprintf("<primitives>\n");
    }      
    prefix[len] = '\0';
    if (next) next->print(total, cutoff, skip, depth, false);
  } else if (! first) {
    // rest must be below threshold; print if not first callee
    int32 t = 0, vmt = 0;
    for (pnode* pn = this; pn; pn = pn->next) {
      assert(pn->ticks() < cutoff, "not ordered");
      t += pn->ticks(); vmt += pn->vmTicks();
    }
    if (t >= cutoff) {
      printNum(100.0 * t / total);
      lprintf("<all others>\n");
    }
  }
}

void pnode::addLeaf() {
  if (nleaves == MAX_STACK_DEPTH)
    return;
  // add myself to the leaves list
  for (int32 i = 0; i < nleaves; i++) {
    if (leaves[i]->method == method) {
      // already there
      leaves[i]->_ownTicks[0] += _ownTicks[0];
      leaves[i]->_ownTicks[1] += _ownTicks[1];
      return;
    }
  }
  // not there -- add a copy of myself
  leaves[nleaves] = new pnode(method, selector, isAccess);
  *leaves[nleaves] = *this;
  leaves[nleaves]->next = leaves[nleaves]->callee = NULL;
  nleaves++;
}

bool pnode::equal(nmethod* nm) {
  if (selector != nm->key.selector) return false;
  if (nm->isAccess()) {
    return isAccess;
  } else {
    return !isAccess && method == nm->method();
  }
}

void HProfiler::reset() {
  if (top) { top->dealloc(); top= NULL; }
  p = NULL;
  ticks = gcTicks = lookupTicks = compilerTicks = primTicks = 0;
  if (newList) { delete newList; newList= NULL; }
}

// Would be safer to synchronize with Self (wait til next interrupt point)
// but this would distort the profile since primitives wouldn't show up.
// Disadvantage of doing it here: tricky, charges scavenges etc to most
// recent Self activation.
static void htick() { hprofiler->tick(); }

void HProfiler::start(Process* pr) {
  if (p && p != pr) reset();
  p = pr;
  top = new pnode(0, 0, false);    // dummy root
  newList = new pnode(0, 0, false);
  IntervalTimer::CPU_timer()->enroll_async( IntervalTimer::CPU_timer()->ticks_per_second(), htick);
}

void HProfiler::stop() {
  IntervalTimer::CPU_timer()->withdraw(htick);
  p = NULL;
}

nmethod** nms;

void HProfiler::tick() {
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  
  if (!resources.in_consistent_state()) {
    warning("HProfiler::tick() skipping because resource area is not consistent");
    return;
  }
  ResourceMark rm; // needed because new debug info allocates scopeDescs
  if ( currentProcess != p              // our process isn't running
  ||   !p->inSelf() || !p->isRunnable() // newborn or dead process
  ||   MallocInProgress)                // malloc isn't reentrant, 
                                        // scopeDescs may do mallocs
    return;

  ticks++;
  if (GCInProgress) {
    gcTicks++;                          // don't charge individuals for GC
    return;
  }
  if (theCompiler                     // same for compiles
#     ifdef SIC_COMPILER
        || theSIC
#     endif
      ) {
    compilerTicks++;
    return;
  }
  if (ConversionInProgress || processSemaphore) {  
    return;                             // stack/zone may look weird
  }

  bool inSelf = Memory->code->isSelfPC((char*)InterruptedContext::the_interrupted_context->pc());
  top->tick(inSelf);
  
  if (ShowLookupInMonitor::lookup_nesting) {
    lookupTicks++;                      // for leaves list
    assert(!inSelf, "should be C code");
  } else if (!inSelf) {
    primTicks++;                        // for leaves list
  }
  
  
  // collect all nmethods on stack
  // cannot use malloc or resource area since we're executing async to the VM
  frame* f = p->stack()->last_self_frame(true);
  int32 n = 0;
  // XXXX interp?
  if (Memory->code->iZone->contains(InterruptedContext::the_interrupted_context->pc())) {
    // bottommost nmethod may have no frame (e.g. access methods)
    // so it may be omitted from the stack trace
    nmethod* last = nmethod::findNMethod(InterruptedContext::the_interrupted_context->pc());
    if (last != f->code()) nms[n++] = last;
  }
  for ( ; f; f = f->selfSender()) {
    // XXXX interp?
    if (f->is_compiled_self_frame()) {
      nms[n++] = f->code();
    }
    if (n == MAX_STACK_DEPTH)
      break;
  }

  // update ticks of nmethods that remained on stack since last tick
  n--;
  pnode *pn, *prevpn;
  for (pn = top->callee, prevpn = top;
       pn && n >= 0;
       n--, pn = prevpn->callee) {
    pnode *sibling;
    for (sibling = pn;
         sibling && !sibling->equal(nms[n]);
         sibling = sibling->next) ;
    if (sibling) {
      // found nmethod in tree
      assert(sibling->equal(nms[n]), "not found");
      sibling->tick(inSelf);
      prevpn = sibling;
    } else {
      // a new path starts here
      break;
    }
  }

  // insert new path
  pn = prevpn;
  for ( ; n >= 0; n--, pn = pn->callee) {
    // insert new node at front of callee chain
    pnode* t = pn->callee;
    oop selector  = nms[n]->key.selector;
    bool isAccess = nms[n]->isAccess();
    oop method    = isAccess ? 0 : nms[n]->method();
    pn->callee    = new pnode(method, selector, isAccess);
    pn->callee->next = t;
    pn->callee->tick(inSelf);
  }

  IntervalTimer::CPU_timer()->enable();       // reset tick interval
}

void HProfiler::sort() {
  // sort siblings in descending order
  if (inactive()) return;
  top->sort();
}

void HProfiler::print_hp(float fcutoff, float fskip, smi maxDepth) {
  if (inactive()) return;
  ResourceMark rm;
  lprintf("\n  total ticks: %ld; one tick = %3.2f%%\n\n",
         long(ticks), 100.0 / ticks);
  leaves = NEW_RESOURCE_ARRAY( pnode*, MAX_STACK_DEPTH);
  nleaves = 0;
  top->prefix = NEW_RESOURCE_ARRAY( char, MAX_STACK_DEPTH);
  strcpy(top->prefix, IndentString);
  float cutoff = max(1.0, fcutoff * ticks);
  float skip   = max(1.0, fskip * ticks);
  top->print(ticks, cutoff, skip, maxDepth, true);
  if (gcTicks >= cutoff)
    leaves[nleaves++] = new primPnode("garbage collection", gcTicks, false);
  if (compilerTicks >= cutoff)
    leaves[nleaves++] = new primPnode("compilation", compilerTicks, false);
  if (lookupTicks >= cutoff)
    leaves[nleaves++] = new primPnode("lookup", lookupTicks, true);
  if (primTicks >= cutoff)
    leaves[nleaves++] = new primPnode("primitives w/o lookup", primTicks, true);
  if (nleaves) {
    qsort(leaves, nleaves, sizeof(pnode*), compare_pn_leaf);
    lprintf("\n  leaves:\n");
    for (int32 i = 0; i < nleaves; i++) {
      int32 ownTicks = leaves[i]->ownTicks();
      leaves[i]->printSelf(100.0 * ownTicks / ticks);
      delete leaves[i];
    }
  }
  leaves = NULL;
  top->prefix = NULL;
}

static oop start_cont_p;
static void start_cont() { 
  hprofiler->start(processOop(start_cont_p)->process()); }

static void reset_cont() { hprofiler->reset(); }
static float pnode_cutoff, pnode_skip;
static smi pnode_maxDepth;

static void print_profile_cont() {
  ResourceMark mark;
  hprofiler->sort();
  hprofiler->print_hp(pnode_cutoff, pnode_skip, pnode_maxDepth);
}
  


// memory functions

static void newListDo(pnodeDoFn f) {
  pnode* prev = newList;
  pnode* pn;
  while ((pn = prev->nextNew) != NULL) {
    f(pn);
    if (pn->is_new()) {
      prev = pn;
    } else {
      prev->nextNew = pn->nextNew;      // no longer new -- remove from list
      pn->nextNew = NULL;
    }
  }
}

static oopsDoFn odfn;
static void pnode_oopsDo(pnode* pn) { odfn(&pn->method); odfn(&pn->selector); }
static void pnode_scavenge(pnode* pn) {
  SCAVENGE_TEMPLATE(&pn->method); SCAVENGE_TEMPLATE(&pn->selector); }
static void pnode_gc_mark(pnode* pn) { 
  MARK_TEMPLATE(&pn->method); MARK_TEMPLATE(&pn->selector); }
static void pnode_gc_unmark(pnode* pn) { 
  UNMARK_TEMPLATE(&pn->method); UNMARK_TEMPLATE(&pn->selector); }

static oop pnode_from, pnode_to;
static void pnode_switch_pointers(pnode* pn) {
  if (pn->method == pnode_from) pn->method = pnode_to;
  if (pn->selector == pnode_from) pn->selector = pnode_to;
}
static bool pnode_verify_result = true;
static void pnode_verify(pnode* pn) {
  bool verify_result = true;
  VERIFY_TEMPLATE(&pn->method); VERIFY_TEMPLATE(&pn->selector);
  pnode_verify_result &= verify_result;
}
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


HProfiler::HProfiler() {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  top = NULL;
  reset();
# endif
}

void initHProfiler() {
  if (hprofiler) fatal("only one profiler, please");
  hprofiler = new HProfiler();
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  nms =  NEW_C_HEAP_ARRAY(nmethod*, MAX_STACK_DEPTH);
  pnodeAllocator = new AsyncAllocator(sizeof(pnode), 500);
# endif
}

oop ResetProfile_prim(oop rcvr) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  switchToVMStack(reset_cont);
# endif
  return rcvr;
}

oop Profile_prim(oop p, bool on) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  // start/stop profiling process p 
  if (on) {
    start_cont_p = p;           // recursive - may need lotsa stack space
    switchToVMStack(start_cont);
  } else {
    hprofiler->stop();
  }
# else
  Unused(on);
# endif
  return p;
}


oop PrintProfile_prim(oop rcvr, float cutoff, float skip, smi maxDepth) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  pnode_cutoff = cutoff; pnode_skip = skip; pnode_maxDepth = maxDepth;
  switchToVMStack(print_profile_cont);
# else
  Unused(cutoff); Unused(skip); Unused(maxDepth);
# endif
  return rcvr;
}

void HProfiler::scavenge_contents() {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!inactive()) newListDo(pnode_scavenge);
# endif
}

void HProfiler::oops_do(oopsDoFn f) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  odfn = f;
  if (!inactive()) top->pnodeDo(pnode_oopsDo);
# else
  Unused(f);
# endif
}

void HProfiler::switch_pointers(oop from, oop to) {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  pnode_from = from; pnode_to = to;
  if (!inactive()) {
    if (from->is_new()) {
      newListDo(pnode_switch_pointers);
    } else {
      top->pnodeDo(pnode_switch_pointers);
    }
  }
# else
  Unused(from); Unused(to);
# endif
}

bool HProfiler::verify() {
  bool r = true;
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!inactive()) {
    pnode_verify_result = r;  
    top->pnodeDo(pnode_verify); 
    r = pnode_verify_result; 
  }
# endif
  return r;
}

void HProfiler::gc_mark_contents() {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!inactive()) top->pnodeDo(pnode_gc_mark);
# endif
}

void HProfiler::gc_unmark_contents() {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!inactive()) top->pnodeDo(pnode_gc_unmark);
# endif
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1998-2006 Sun Microsystems, Inc.
   See the LICENSE file for license information. */
   
// Implementation of general (abstract) spy.


# pragma implementation "monitor.hh"

# include "_monitor.cpp.incl"

// Variables:

Monitor* TheSpy;


// Creation (& destruction)

Monitor::Monitor() : _mw() {
  incremental = _is_active = false; 
  logf = NULL; logfn = NULL;
}

Monitor::~Monitor() { deactivate(); }


// Activating


void Monitor::activate(const char* filename)   {
  _measurements_per_second = get_measurements_per_second();
  if (filename) {
    logfn = OS::strdup(filename);
    logf = open_log_file(logfn);
  }
  if (!mw()->open_and_resize(this)) {
    _is_active = false;
    return;
  }
  _is_active = true; // this must be true so resize_mem_zone_bars, e.g. works 
  bool gotSema = _mw.pw->pre_draw(false);
  if (!gotSema) warning("could not get graphics semaphore");
  initialize_contents(); // does drawing, must do pre_draw above
  if (gotSema) _mw.pw->post_draw(false);
  incremental= false; // be sure to redraw whole thing  
  tick_redraw(); // draw the inside now
  enroll();        
}


void Monitor::deactivate() {
  if (!is_active())
    return;
  // must falsify me before calling mw()->close() to prevent recursion
  _is_active = false;
  withdraw();
  mw()->close();
  if (logf != NULL) {
    fclose(logf); 
    logf = NULL; 
  } 
}


// And the log file...

FILE* Monitor::open_log_file(const char* filename) {
  FILE* logf = fopen(filename, "w");
  if (!logf) {
    perror("spy: cannot open log file");
  } 
  else {
    // NB: must allocate buffer here because default file buffer is allocated
    // lazily --> crash if first write to file is in tick_*() when tick
    // interrupts another malloc
    char* buf = AllocateHeap(32 * K, "spy log buffer");
    OS::set_log_buf(logf, buf, 32 * K);
  }
  return logf;
}


void Monitor::reset_log() {
  if (logf) {
    // you might think that ftruncate would do the job...but it doesn't work
    // (SunOS 4.1.3)
    FILE* f = logf;
    logf = NULL;
    fclose(f);
    unlink(logfn);
    logf = open_log_file(logfn);
  }
}


void Monitor::annotate_log(const char* fn) {
  if (logf) {
    SignalBlocker sb;   // to block timer ticks 
    fputc('\n', logf);
    fputc('%',  logf);
    fputs(fn, logf);
    fputc('\n', logf);
  }
}



// Enrolling/withdrawing the periodic interrupt handler(s)


void Monitor::enroll() {
  // sync because mac needs it and because of X race condition
  IntervalTimer::Real_timer()->enroll_async_if_safe( _measurements_per_second, monitor_tick);
}

void Monitor::withdraw() {
  IntervalTimer::Real_timer()->withdraw(monitor_tick); // removes monitor_tick_measure, too
}

void Monitor::full_redraw() {
  incremental= false;
  tick_redraw();
}


// ==============================================

// Meat: starting with the interrupt handlers:



// this is called at every tick of our timer, synchronously
void Monitor::monitor_tick() {
  // need the casts to circumvent C++ funny privacy rules!
  TheSpy->tick_measure(); // cannot crawl up stack async, hit 68k frames    
  TheSpy->tick_redraw();
}
  
  
void Monitor::adjust_after_resize() {
  resize_contents();
}


fint Monitor::max_height() {
  if (!is_active()) return 0;
  return mw()->height();
}


// General indicator locations:

int Monitor::ind_w()        {  return  5*mw()->font_width(); }
int Monitor::ind_h()        {  return  mw()->font_height() + 2; }
int Monitor::ind_gap()      {  return  ind_h() + 1; }
int Monitor::ind_bar_w()    {  return  6*mw()->font_width(); }
int Monitor::ind_bar_h()    {  return  ind_h() / 2; }
int Monitor::ind_bar_yoff() {  return  2; }

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_iterator.hh"

# include "_frame_iterator.cpp.incl"


FrameIterator::FrameIterator( frame* _f, RegisterLocator* _rl, bool z, RegisterString m, bool r, OopClosure* c ) {
  f = _f; rl = _rl;  zap = z;  mask = m;  reinit = r;  oop_closure = c;
  do_all();
}


void FrameIterator::do_all() {
       if (f->is_interpreted_self_frame())  do_interpreted();
  else if (f->is_compiled_self_frame())     do_compiled();
  else                                      do_vm_frame();
}
  
  
void FrameIterator::do_interpreted() {
  interpreter* interp = f->get_interpreter();
  InterpreterIterator ii(interp, oop_closure, zap, reinit);
}


# if !(defined(FAST_COMPILER) || defined(SIC_COMPILER))
  void do_compiled() { ShouldNotReachHere(); }
# endif  

void FrameIterator::do_patched_frame_saved_outgoing_args() {
  if ( !SaveOutgoingArgumentsOfPatchedFrames )
    return;
  if ( f->is_patched() ) {
    oop* p = (oop*)f->patched_frame_saved_outgoing_args_addr();
    oop_closure->do_oop(p);
  }
}
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stack.hh"
# include "_stack.cpp.incl"


# define MAGIC  123456789

// This must be a macro - can't return current frame from a real function.
# define GET_LAST_FRAME1(fr)                                                  \
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();      \
  frame* fr;                                                                  \
  if (process != currentProcess) {                                            \
    fr = (frame*)process->lastSP();                                           \
  } else if (InterruptedContext::the_interrupted_context->is_set()) {         \
    fr = InterruptedContext::the_interrupted_context->sp();                   \
  } else {                                                                    \
    FlushRegisterWindows();                                                   \
    fr = currentFrame();                                                      \
  }
 
# ifdef ASSERT_but_hprofiler_does_this
#   define GET_LAST_FRAME(fr)                                                 \
      GET_LAST_FRAME1(fr);                                                    \
      if (!is_off_signal_stack((char*)fr))  \
        warning("traversing stack in interrupt handler - might not work");
# else
#   define GET_LAST_FRAME(fr) GET_LAST_FRAME1(fr)
# endif 

frame* Stack::callee_of(const frame* f) {
  GET_LAST_FRAME(p);
  for (frame* q;  p;  p = q) {
    q = p->sender();
    if (q == f) return p;
    assert(!p->is_first_self_frame(), "cannot find bottom of frame!");
  }
  ShouldNotReachHere(); // cannot find bottom of frame
  return NULL;
}



frame* Stack::first_VM_frame() {
  if (process->nesting == 0)
    return NULL;
  GET_LAST_FRAME(f);
  assert(f->is_aligned(), "alignment");
  frame* res = f;
  frame* senderFrame;
  for (;;) {
    senderFrame = res->sender();
    if (senderFrame == NULL)
      return NULL;
    if (senderFrame->is_self_frame())
      return res;
    res = senderFrame;
  }
}


// May not work if this frame is the first_VM_frame,
//  but I don't know why -- dmu
// Also returns corresponding RegisterLocator if rl is non null

frame* Stack::last_self_frame(bool includePrologue, RegisterLocator** rl) {
  frame* ff = first_VM_frame(); // sets senderFrame as side-effect
  if (ff == NULL) return NULL;
  
  frame* senderFrame = ff->sender();
  frame* result = !includePrologue  &&  senderFrame->is_in_prologue()
    ?  senderFrame->selfSender()    // if in prologue. sender is the first real frame
    :  senderFrame;                 // includePrologue means: return this frame even if nmethod is in prologue

  if (rl != NULL) {
    GET_LAST_FRAME(f);
    RegisterLocator* first_rl = RegisterLocator::for_sender_of(f);
    *rl = first_rl->climb_to_frame(result);
    assert(*(int32*)(*rl)->fr() || true,  "ensure frame is valid");
  }
  
  return result;
}


frame* Stack::find_frame_entering(char* ep) {
  GET_LAST_FRAME(f);
  
  for (  frame* res = f;  res;  res = res->sender())
    if (res->c_entry_point() == ep)
      return res;    
  return NULL;
}

// If the interpreter calls a primitive which starts a non-local
// return (e.g. unwind_protect),
// NLRSupport::continue_NLR_into_Self needs to find the interpreter frame to return to.
// It is the one which called CallPrimitiveFromInterpreter

frame* Stack::interpreter_frame_for_continuing_NLR_from_primitive() {
  GET_LAST_FRAME(f);
  
  for (  frame* res = f;  res;  res = res->sender()) {
    char* ep = res->c_entry_point();
    if ( ep == first_inst_addr( CallPrimitiveFromInterpreter )
    ||   ep == first_inst_addr( interruptCheck ) 
    ||   ep == first_inst_addr( InterpreterLookup_cont ))
      return res->sender();    
  }
  return NULL;
}


// If the interpreter ends up in Conversion::returnToSelf
// it must have gotten there from calling HandleReturnTrap

frame* Stack::interpreter_frame_for_continuing_from_return_trap() {
  return find_frame_entering( first_inst_addr(HandleReturnTrap) )
           ->sender();
}


void Stack::consistencyCheck(primDoFn pfn, frame* lastSelfFrame) {
  if (preemptCause == cNonLIFOBlock) return;

  if (!lastSelfFrame) {
    lastSelfFrame = last_self_frame(true);
  }
 
  assert(lastSelfFrame->is_self_frame(), "should be a self frame");
  PrimDesc* pd;
  if ( lastSelfFrame->is_compiled_self_frame() ) {
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    sendDesc* sd = lastSelfFrame->send_desc();
    if (sd == NULL) return;     // uncommon trap, or after fatal error
    // sd could be a prim. call or a lookup
    // don't need to check canScavenge et al for a lookup
    if (!sd->isPrimCall()) return;
    char* fn_start = sd->jump_addr();
    pd = getPrimDescOfFirstInstruction(fn_start, true);
    if (!pd) {
      warning1("stack: entry for primitive call to %#lx not found!!!",
               fn_start);
      return;
    }
#   endif
  } else {
    interpreter* interp = lastSelfFrame->get_interpreter();
    assert( interp != NULL,
            "if interpreted, lastSelfFrame must be tops for this activation");
    pd = interp->getPrimDesc();
    if (pd == NULL) return;
  }
  pfn(pd);
}


static int frame_count;

// Note: caller must have a resource mark; (for RegisterLocators)

void Stack::frames_do(framesDoFn fn, primDoFn pfn) {
  // use true for PPC: need to catch OOPS even if first self frame called interrupt check from its prologue
  if (!process->inSelf(true)) return; 
  
  frame* f = first_VM_frame();
  RegisterLocator* reg_locs = RegisterLocator::for_sender_of(f); // RegisterLocators only work for Self frames
  
  if (pfn) consistencyCheck(pfn);    // do consistency check for prim call
  
  frame_count = 0;
# if  GENERATE_DEBUGGING_AIDS
    frame *ff, *fff, *ffff;
# endif
  for ( ; f != NULL;  
          f = f->sender()) {
          
    if (f->is_self_frame()) {
      reg_locs = reg_locs->climb_to_frame(f);
      (*fn)(f, reg_locs);
    }
    else
      (*fn)(f, NULL);
      
    ++frame_count;
    assert(frame_count < 1000000, "figure-6 bug");
#   if  GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging) {
        ffff = fff;  fff = ff;  ff = f;
      }
#   endif
  }
}

void Stack::vframes_do(vframesDoFn fn, frame* fr) {
  if (!process->inSelf()) return;
  abstract_vframe* vf = new_vframe(fr ? fr : last_self_frame(false));
  assert(vf->real_bci() != PrologueBCI && vf->real_bci() != EpilogueBCI,
         "bottom most activation must be a valid vframe");
  do {
    (*fn)(vf);
    vf = vf->sender();
  } while (vf);
}

static int32 d;
static void frame_inc_depth(frame* f, RegisterLocator*) { if (f->is_self_frame())  ++d; }
static void vframe_inc_depth(abstract_vframe* f) { Unused(f); d ++; }

int32 Stack::depth() {
  ResourceMark m;
  d = 0;
  frames_do(frame_inc_depth);
  return d;
}

static void frame_remove_patch(frame* f, RegisterLocator*) { if (f->is_self_frame())  f->remove_patch(); }

void Stack::remove_patches() {
  // remove all return address patches
  ResourceMark m;
  frames_do(frame_remove_patch);
}


// I am called repeatedly on every process's stack to
//  build linked lists of frames that use a particular nmethod.
// When this is done, every nmethod that is on a stack will
//  have its frame chain link set to a frame whose frame chain link
//  will point to the next frame using the same nmethod, etc. -- dmu

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Stack::chainFrames() {
  // this is called a lot, so watch out for performance bugs
  // that's also why it doesn't use frames_do
  frame* f = last_self_frame(true);
  while (f) {
    if (f->is_compiled_self_frame()) {
      nmethod* nm = f->code();
      f->set_nmethod_frame_chain(nm->frame_chain, nm);
      nm->frame_chain = f;
    }
    f = f->selfSender();
  }
}


void Stack::unchainFrames() {
  frame* f = last_self_frame(true);
  while (f) {
    if (f->is_compiled_self_frame())
      f->code()->clear_frame_chain();
    f = f->selfSender();
  }
}

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Stack::chainFrames() {}
void Stack::unchainFrames() {}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


    
static enumeration *theEnumeration;

static void vframe_enumerate_references(abstract_vframe* vf) {
  vf->enumerate_references(theEnumeration);
}

static void vframe_enumerate_families(abstract_vframe* vf) {
  vf->enumerate_families(theEnumeration);
}

void Stack::enumerate_references(enumeration *e) {
  theEnumeration = e;
  vframes_do(vframe_enumerate_references);

  // Check for block references in this stack
  if (e->get_num_live_vframes() > 0 ) {
     oop *targetp = e->get_targets();
     for (int index = 0; 
              index < e->get_num_live_vframes();
            ++index) {

      abstract_vframe* vf   = new_vframe(last_self_frame(false));
      abstract_vframe* tvf  = vframeOop(targetp[index + e->get_num_live_vframes()])->as_vframe();
      abstract_vframe* prev = NULL;
      for (  ; vf  &&  !vf->EQ(tvf);  prev = vf, vf = vf->sender()) {
        abstract_vframe* parentVF = vf->parent();
        if (parentVF && parentVF->EQ(tvf)) {
          e->add_obj(new_vframeOop(process, vf));
        } 
      }
      if (prev && vf) {
        assert(prev->sender()->EQ(vf), "just checking");
        e->add_obj(new_vframeOop(process, prev));
      }
    }
  }
}

void Stack::enumerate_families(enumeration *e) {
  theEnumeration = e;
  vframes_do(vframe_enumerate_families);
} 

int32 Stack::vdepth(frame* f) {
  ResourceMark m;
  d = 0;
  vframes_do(vframe_inc_depth, f);
  return d;
}

void Stack::print() {
  ResourceMark m;
  LOG_EVENT("VM stack dump");           // mark likely point of error
  int32 maxFrame = StackPrintLimit;
  frame* last = last_self_frame(false);
  if (last == NULL) {
    lprintf("No Self stack!\n\n");
    return;
  }
  abstract_vframe* v = new_vframe(last);
  int32 depth = vdepth(last);
  bool omitFirst = currentProcess != vmProcess;
  if (omitFirst) depth--;
  
  fint curFrame = 0;
  fint printFrame = 0;
  for (; curFrame < maxFrame;  ++curFrame) {
    if (v == NULL || (v->is_first_self_vframe() && omitFirst)) return;
    if (v->print_frame(printFrame)) {
      printFrame ++;
    }
    v = v->sender();
  }
  
  if (curFrame  <  depth - maxFrame)
    lprintf("\n# %d - # %d: skipped, stack print limit is %d\n\n",
           curFrame,  depth - maxFrame - 1, maxFrame);
  
  for (;  curFrame  <  depth - maxFrame;  ++curFrame) {
    assert(!v->is_first_self_vframe(), "bad stack depth");
    v = v->sender();
  }
  
  for (;  v;  ++curFrame) {
    if (v->is_first_self_vframe() && omitFirst) break;
    v->print_frame(curFrame);
    v = v->sender();
  }
}

void print_stack() {
  currentProcess->stack()->print();
}

static void frame_scavenge_contents(frame* f, RegisterLocator* rl) { f->scavenge_contents(rl); }
static void frame_gc_mark_contents(frame* f, RegisterLocator* rl) { f->gc_mark_contents(rl); }
static void frame_gc_unmark_contents(frame* f, RegisterLocator* rl) { f->gc_unmark_contents(rl); }
static bool frame_verify_result = true;
static void frame_verify(frame* f, RegisterLocator* rl) { frame_verify_result &= f->verify(frame_count, rl); }
static oop from;
static oop to;
static void frame_switch_pointers(frame* f, RegisterLocator* rl) { f->switch_pointers(from, to, rl); }

static void check_scavenge(PrimDesc* pd) {
  if (!pd->canScavenge()) {
    fatal1("incorrect primitive entry for %s: canScavenge must be set",
           pd->name());
  }
}
void Stack::scavenge_contents()  { ResourceMark rm;  frames_do(frame_scavenge_contents, check_scavenge); }
void Stack::gc_mark_contents()   { ResourceMark rm;  frames_do(frame_gc_mark_contents); }
void Stack::gc_unmark_contents() { ResourceMark rm;  frames_do(frame_gc_unmark_contents); }
bool Stack::verify()             {
  bool r = true;
  ResourceMark rm;  
  if (markDestroyed()) {
    error1("Stack of process %#lx has overflowed", process);
    r = false;
  }
  frame_verify_result = r;
  frames_do(frame_verify);
  return frame_verify_result;
}

void Stack::switch_pointers(oop f, oop t) {
  from = f;
  to = t;
  ResourceMark rm;  
  frames_do(frame_switch_pointers, check_scavenge);
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Stack::convert() {
  ResourceMark rm;
  if (!process->inSelf()) return;
  frame* f = last_self_frame(true);
  frame* prev = NULL;
  do {
    if (f->is_compiled_self_frame()) {
      nmethod* nm = f->code();
      if (nm->isInvalid()) {
        // need to convert this frame when control returns to it
        f->patch(prev);
      }
    }
    prev = f;
    f = f->sender();
  } while (f);
}

# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Stack::convert() {}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)



void Stack::mark() {
  int32* p = mark_addr();
  *p-- = MAGIC; *p-- = MAGIC; *p-- = MAGIC; *p = MAGIC;
  p = (int32*)base;
  *p++ = MAGIC; *p++ = MAGIC; *p++ = MAGIC; *p = MAGIC;
}

bool Stack::markDestroyed() {
  if (!base) return false;      // stack hasn't been allocated yet
  int32* p = (int32*)base;
  if (!(*p++ == MAGIC && *p++ == MAGIC && *p++ == MAGIC && *p == MAGIC)) {
    fatal("unrecoverable stack overflow while executing in VM");
  }
  p = mark_addr();
  return !(*p-- == MAGIC && *p-- == MAGIC && *p-- == MAGIC && *p == MAGIC);
}

bool Stack::allocate() {
  base= AllocateHeap(size, "a process stack", false);
  return base != NULL;
}

void Stack::deallocate() {
  if (markDestroyed())
    warning("A process stack has overflowed - memory may be corrupted!!");
  selfs_free(base);
  base = (char*)1;      // cause bus error if dereferenced
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "clist.hh"
# include "_clist.cpp.incl"

CList* CList::prependList(CList* l) {
  assert(this != l || this == EMPTY, "prepending list to itself");
  if (l->isEmpty()) return this;
  l = l->copy();
  if (isEmpty()) return l;
  l->tail()->setNext(head());
  l->_tail = tail();
  return l;
}

CList* CList::appendList(CList* l) {
  assert(this != l || this == EMPTY, "appending list to itself");
  CList* c = this;
  for (CListElem* e = l->head(); e; e = e->next()) {
    c = c->append(e->data());
  }
  return c;
}

CList* CList::addList(CList* l) {
  assert(this != l || this == EMPTY, "adding list to itself");
  CList* c = this;
  for (CListElem* e = l->head(); e; e = e->next()) {
    c = c->add(e->data());
  }
  return c;
}

CList* CList::identityAddList(CList* l) {
  assert(this != l || this == EMPTY, "adding list to itself");
  CList* c = this;
  for (CListElem* e = l->head(); e; e = e->next()) {
    c = c->identityAdd(e->data());
  }
  return c;
}

CList* CList::copy() {
  CList* c = EMPTY;
  return c->appendList(this);
}

CListEntry* CList::realDeepCopy() {
  CList* c = EMPTY;
  for (CListElem* e = head(); e; e = e->next()) {
    if (e->data()) {
      c = c->append(e->data()->realDeepCopy());
    } else {
      c = c->append(e->data());
    }
  }
  return c;
}

CList* CList::reverse() {
  CList* r = EMPTY;
  for (CListElem* e = head(); e; e = e->next()) {
    CListEntry* x = e->data();
    r = r->prepend(x);
  }
  return r;
}

int32 CList::length() {
  int32 l = 0;
  for (CListElem* e = head(); e; e = e->next()) l++;
  return l;
}

CListEntry* CList::nth(fint i) {
  CListElem* e;
  for (e = head(); i--; e = e->next()) {
    assert(e, "nth past end of list");
  }
  assert(e, "nth past end of list");
  return e->data();
}

void CList::nthPut(fint i, CListEntry* d) {
  CListElem* e;
  for (e = head(); i--; e = e->next()) {
    assert(e, "nthPut past end of list");
  }
  assert(e, "nthPut past end of list");
  e->setData(d);
}

CListElem* CList::spliceOutNext(CListElem* pe) {
  CListElem* e;
  if (pe == NULL) {
    e = head()->next();
    _head = e;
  } else {
    e = pe->next()->next();
    pe->setNext(e);
  }
  if (e == NULL) {
    _tail = pe;
  }
  return e;
}

CList* CList::pop(fint count) {
  CList* l = EMPTY;
  while (count--) l = l->prepend(pop());
  return l;
}

bool CList::includes(CListEntry* d) {
  for (CListElem* e = head(); e; e = e->_next) {
    if (e->data()->EQ(d)) return true;
  }
  return false;
}

bool CList::includesList(CList* l) {
  if (this == l) return true;
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (! includes(e->data())) return false;
  }
  return true;
}

bool CList::includesAny(CList* l) {
  if (this == l) return nonEmpty();
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (includes(e->data())) return true;
  }
  return false;
}

bool CList::EQlist(CList* l) {
  if (this == l) return true;
  if (length() != l->length()) return false;
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (! includes(e->data())) return false;
  }
  return true;
}

bool CList::identityIncludes(CListEntry* d) {
  for (CListElem* e = head(); e; e = e->_next) {
    if (e->data() == d) return true;
  }
  return false;
}

bool CList::identityIncludesList(CList* l) {
  if (this == l) return true;
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (! identityIncludes(e->data())) return false;
  }
  return true;
}

bool CList::identityIncludesAny(CList* l) {
  if (this == l) return nonEmpty();
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (identityIncludes(e->data())) return true;
  }
  return false;
}

bool CList::identityEQ(CList* l) {
  if (this == l) return true;
  if (length() != l->length()) return false;
  for (CListElem* e = l->head(); e; e = e->next()) {
    if (! identityIncludes(e->data())) return false;
  }
  return true;
}

void CList::remove(CListEntry* d) {
  for (CListElem* e = head(), *pe = NULL; e; pe = e, e = e->next()) {
    if (e->data()->EQ(d)) {
      spliceOutNext(pe);
      break;
    }
  }
}

void CList::removeList(CList* l) {
  for (CListElem* e = l->head(); e; e = e->next()) {
    remove(e->data());
  }
}

void CList::identityRemove(CListEntry* d) {
  for (CListElem* e = head(), *pe = NULL; e; pe = e, e = e->next()) {
    if (e->data() == d) {
      spliceOutNext(pe);
      break;
    }
  }
}

void CList::identityRemoveList(CList* l) {
  for (CListElem* e = l->head(); e; e = e->next()) {
    identityRemove(e->data());
  }
}

#ifdef UNUSED
CList* CList::intersection(CList* l, bool makeCopy) {
  CListElem* e = head(), *pe = NULL;
  while (e) {
    CListEntry* x = e->data();
    if (! l->includes(x)) {
      if (makeCopy) {
        return copy()->intersection(l, false);
      }
      e = spliceOutNext(pe);
    } else {
      pe = e, e = e->next();
    }
  }
  return this;
}
#endif

CList* CList::identityIntersection(CList* l, bool makeCopy) {
  CListElem* e = head(), *pe = NULL;
  while (e) {
    CListEntry* x = e->data();
    if (! l->identityIncludes(x)) {
      if (makeCopy) {
        return copy()->identityIntersection(l, false);
      }
      e = spliceOutNext(pe);
    } else {
      pe = e, e = e->next();
    }
  }
  return this;
}

# define DEFINE_ITERATOR(name,template)                                       \
    void CList::name {                                                        \
      for (CListElem* e = head(); e; e = e->next()) {                         \
        e->data()->template;                                                  \
      }                                                                       \
  }

DEFINE_ITERATOR(scavenge_contents(),scavenge_contents())
DEFINE_ITERATOR(gc_mark_contents(), gc_mark_contents())
DEFINE_ITERATOR(gc_unmark_contents(), gc_unmark_contents())
DEFINE_ITERATOR(verify(), verify())
DEFINE_ITERATOR(switch_pointers(oop from, oop to), switch_pointers(from,to))
DEFINE_ITERATOR(relocate(), relocate())
DEFINE_ITERATOR(oops_do(oopsDoFn f), oops_do(f))

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "machineCache.hh"
# include "_machineCache.cpp.incl"

// define FLUSH_ALL (for debugging) to flush the complete I cache of nmethods
// and PICs very regularly

# ifndef FLUSH_ALL
    void MachineCache::flush_instruction_cache_for_debugging() {}
#  endif
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "debugPrintable.hh"

# include "_debugPrintable.cpp.incl"

# if  GENERATE_DEBUGGING_AIDS
void pp(void* p) {
  FlagSetting fl(PrintVMMessages, true);
  if (p == NULL) {
    lprintf("0x0");
  } else if (int32(p) < 10000 && int32(p) > -10000) {
    // guess that it's a location
    printLocation(*(Location*) &p); // use addr to silence warning
  } else if (oop(p)->is_mem() || oop(p)->is_mark()) {
    // guess that it's an oop
    oop(p)->print_real();
  } else if (oop(p)->is_float()) {
    // guess that it's a floatOop (C++ objects should be word-aligned)
    oop(p)->print_real();
  } else {
    // guess that it's a VMObj*
    ((VMObj*) p)->print_zero();
  }
  lprintf("\n");
}

void pp_prev(void* p) {
  Unused(p);
}

void pp_next(void* p) {
  Unused(p);
}
#endif

void pp_short(void* p) {
  FlagSetting fl(PrintVMMessages, true);
  if (p == NULL) {
    lprintf("0x0");
  } else if (int32(p) < 10000 && int32(p) > -10000) {
    // guess that it's a location
    printLocation(*(Location*) &p); // extra indirection to silence warning
  } else if (oop(p)->is_mem()) {
    // guess that it's a memOop
    oop(p)->print_real_oop();
  } else {
    // guess that it's a VMObj*
    ((VMObj*) p)->print_short_zero();
  }
}

# if  GENERATE_DEBUGGING_AIDS
void pr(oop p) {
  FlagSetting fl(PrintVMMessages, true);
  p->print_real();
}

void pm(Map* p) {
  FlagSetting fl(PrintVMMessages, true);
  p->print_map();
}
#endif
/* Sun-$Revision: 30.19 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame.hh"
# pragma implementation "frame_inline.hh"

# include "_frame.cpp.incl"


frame* frame::nmethod_frame_chain(nmethod* nm) {
  return *nmethod_frame_chain_addr(nm); }
  
void   frame::set_nmethod_frame_chain(frame* f, nmethod* nm) {
  *nmethod_frame_chain_addr(nm) = f; }
  
objVectorOop frame::patched_frame_saved_outgoing_args(nmethod* nm) {
  if (!SaveOutgoingArgumentsOfPatchedFrames)
    return NULL;
  assert(is_patched(), "saved outoing args only in patched frame");
  return *patched_frame_saved_outgoing_args_addr(nm); 
}
  
void frame::set_patched_frame_saved_outgoing_args(objVectorOop args, nmethod* nm) {
  *patched_frame_saved_outgoing_args_addr(nm) = args;
}


char* frame::platform_independent_return_addr() {
  char* r = real_return_addr();
  if (isPatchedReturnAddress(r)) {
    char** addr = currentPC_addr();
    assert(Memory->code->contains(*addr), "saved currentPC not in zone");
    return *addr;
  }
  else {
    return r;
  }
}


void frame::set_return_addr(char* addr) {
  char** r = real_return_addr_addr();
  if (isPatchedReturnAddress(*r)) {
    r = currentPC_addr();
    assert(Memory->code->contains(*r), "saved currentPC not in zone");
  }
  *r = addr;
}


char* frame::currentPC() {
  return *currentPC_addr();
}

void  frame::set_currentPC(char* p) {
  *currentPC_addr() = p;
}

char*  frame::real_return_addr() { return *real_return_addr_addr(); }
void   frame::set_real_return_addr(char* x) { *real_return_addr_addr() = x; }

void frame::adjust_return_addr(int32 delta) {set_return_addr(return_addr() + delta);}

Stack* frame::my_stack()   { 
  Stack* s = processes->stackFor(this); 
  assert(s != NULL, "");
  return s;
}

bool frame::is_compiled_self_frame() {
  return Memory->code->contains(return_addr());
}

bool frame::is_self_stub_frame() {
  return Memory->code->sZone->contains(return_addr());
}

bool frame::is_self_frame() {
  return is_compiled_self_frame() || is_interpreted_self_frame();
}
    
bool frame::is_first_self_frame() {
  return is_self_frame() && selfSender() == NULL;
}


frame* frame::sendee(frame* startFrameHint) {
  // search for full frame starting at the given frame; do full stack search
  // if not found
  // used during conversions if the full frame could be a copied frame
  frame* f = startFrameHint;
  while (f && f->sender() != this) f = f->sender();
  if (f == NULL) f = my_stack()->callee_of(this);
  return f;
}

// return next Self frame (skipping all C frames in between) or zero
// if there are no more Self frames
  
frame* frame::selfSender() {
  frame* f = this;
  for (;;) {
    f = f->sender();
    if (f == NULL  ||  f->is_self_frame())
      return f;
  }
}

// same but do not skip C frames

frame* frame::immediateSelfSender() {
  frame* f = sender();
  return f != NULL  &&  f->is_self_frame() ? f : NULL;
}


int32 frame::frame_size() {
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (ConversionInProgress && is_compiled_self_frame()) {
    // could be a copied frame (in the resource area), i.e. sl < l; take
    // size from nmethod
    return code()->frameSize();
  }
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  return frame_size_of_uncopied_frame();
}


// testers

interpreter* frame::get_interpreter_of_block_scope() {
  char* cep = c_entry_point();
  if (cep == NULL)  return NULL;
  Location loc = location_of_interpreter_of_block_scope( cep);
  return loc == IllegalLocation ? NULL : (interpreter*) *location_addr(loc);
}


interpreter* frame::get_interpreter() {
  //  on sparc uses sender, on ppc, same frame
  frame* f= block_scope_of_home_frame();
  if (f == NULL)
    return NULL;
  return f->get_interpreter_of_block_scope();
}



bool frame::is_interpreted_self_frame() {
# if TARGET_OS_VERSION == MACOSX_VERSION
  // breaks when scanning stack for spy
  if (Interpret) fatal("Interpreter does not work on OSX");
  return false;
# else
  return get_interpreter() != NULL;
# endif
}
  
  
fint frame::vdepth(bool includePrologueVframe) {
  abstract_vframe* vf = new_vframe(this);
  if (!includePrologueVframe &&
      (vf->real_bci() == PrologueBCI || vf->real_bci() == EpilogueBCI)) {
    if (vf->is_top()) return 0;
    vf = vf->immediateSender();
  }
  fint n;
  for (n = 1;  !vf->is_top();  vf = vf->immediateSender()) n++;
  return n;
}


nmethod* frame::code() {
  // this is called a lot, so watch out for performance bugs
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  assert(is_compiled_self_frame(), "not a self frame");
  // The subtraction below does not seem to be needed to me.
  // But We need to work on Klein, so I'm not messing with it. -- dmu 2/04
  // Note, this also occurs in Pc::my_nmethod.
  nmethod* r = nmethod::nmethodContaining(return_addr() - sizeof(class nmethod), NULL);
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      char* ra = return_addr();
      nmethod* nm1 = NULL;
           if   (r->contains(ra))  ;
      else if ((nm1 = nmethod::nmethodContaining(ra, NULL))->contains(ra))
              fatal("should not have subtracted");
      else    fatal3("value 0x%x is not in any nmethod, either 0x%x, or 0x%x",
                     ra, r, nmethod::nmethodContaining(ra, NULL));
    }
  # endif
  return r;
# else
  ShouldNotReachHere();
  return NULL;
# endif
}

void frame::nmethod_moved_by(int32 delta, nmethod* where_nm_is_now) {
  for (frame* f = this;
       f != NoFrameChain && f != SavedFrameChain;
       f = f->nmethod_frame_chain(where_nm_is_now)) {
    f->adjust_return_addr(delta);
  }
}


void frame::patch(frame* prev, bool forceSelfFrame ) {
  if (is_interpreted_self_frame())
    patch_interpreted_self_frame(false);
  else
    patch_compiled_self_frame( return_addr_for_patching(forceSelfFrame, prev) );
}


void frame::patch_profiler_trap() {
  if (is_interpreted_self_frame()) {
    patch_interpreted_self_frame(true);
  }
  else {
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    if (isPatchedReturnAddress(return_addr())) {
      return;
    }
    patch_compiled_self_frame((void(*)(...)) ProfilerTrap);

#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        sendDesc* sd = sendDesc::sendDesc_from_return_PC(real_return_addr());
        char* insts =  sd->jump_addr();
        assert( Memory->code->contains(insts) ||
                Memory->code->stubs->contains(insts) ||
                insts == Memory->code->trapdoors->SendMessage_stub_td(),
                "should call self code");
      }
#   endif
#   else
    ShouldNotReachHere();
#   endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  }
}


void frame::patch_interpreted_self_frame(bool forProfiling) {
  interpreter* interp = get_interpreter();
  if (forProfiling && interp->is_return_patched())
    return;
  interp->patch_return(forProfiling ? patched_for_profiling : patched);
}


// sets a patch so that execution stops when control returns to this frame

void frame::patch_compiled_self_frame(returnTrapHandlerFn new_fn) {
  char* ret = return_addr();
  // don't patch top Self frame
  if ( ret == first_inst_addr((void*)firstSelfFrame_returnPC))
    return;

  assert(is_compiled_self_frame(), "must be compiled");
  assert( new_fn == (returnTrapHandlerFn)ReturnTrap  ||  
          new_fn == (returnTrapHandlerFn)PrimCallReturnTrap  ||
          new_fn == (returnTrapHandlerFn)ProfilerTrap, "patching to wild address");
  assert(   Memory->code->contains(ret)
         || ret == first_inst_addr((void*)firstSelfFrame_returnPC),
         "ret addr not in zone, already patched?");

  trace_patch(new_fn);
  
  // must be before we set currentPC so following code can test it
  save_outgoing_arguments();
  
  set_currentPC(ret);
  set_real_return_addr(first_inst_addr((void*)new_fn));
}


// In order to be able to restart a send after a return trap
// (for the KillActivationsUpTo primitive, or uncommon branches, etc.)
// We will need the outgoing arguments of the patched frame.
// On architectures like the PPC, we need to get this from the callee frame
// BEFORE it returns. So get them now and save in a place findable from this frame.
// This counts on a trampoline when going into primitives that can walk the stack,
// which only currently works for PPC.

void frame::save_outgoing_arguments() {
  if (!SaveOutgoingArgumentsOfPatchedFrames) {
    return;
  }
  frame* s = sendee();
  if (s->return_addr() == (char*)&ReturnTrap_returnPC
  ||  s->return_addr() == (char*)&ReturnTrapNLR_returnPC 
  ||  s->return_addr() == (char*)&Recompile_stub_returnPC 
  ||  s->return_addr() == (char*)&DIRecompile_stub_returnPC
  ||  s->return_addr() == (char*)&    MakeOld_stub_returnPC ) {
    // we are currently in HandleReturnTrap and this is the frame
    // corresponding to the original patched frame, so use its
    // outgoing args  -- dmu 2/03
    assert(OutgoingArgsOfReturnTrapOrRecompileFrame->verify(), "check args");
    set_patched_frame_saved_outgoing_args(OutgoingArgsOfReturnTrapOrRecompileFrame);
  }
  else
    set_patched_frame_saved_outgoing_args(extract_outgoing_args());
}


objVectorOop frame::extract_outgoing_args(frame* sendeeOrNULL) {
  if (!SaveOutgoingArgumentsOfPatchedFrames) {
    return NULL;
  }
  assert(is_self_frame(), "only works for Self frames");
  const bool traceEOA = false;
 
  frame* s = sendeeOrNULL == NULL  ?  sendee()  :  sendeeOrNULL;
  fint nargs = outgoing_arg_count(s);
  ResourceMark rm;
  RegisterLocator* srl = s->is_self_frame() ? RegisterLocator::for_frame(s) : NULL;
  objVectorOop saved_args_and_rcvr = Memory->objVectorObj->cloneSize(nargs + 1 /* rcvr */);
  if (traceEOA) lprintf("***** frame::extract_outgoing_args: ");
  for (fint i = 0;  i < nargs + 1;  ++i) {
    oop arg = *(oop*) s->location_addr_of_incoming_argument(LocationOfSavedOutgoingArgInSendee(i-1), srl);
    assert(arg->verify_oop(), "bad outgoing argument");
    saved_args_and_rcvr->obj_at_put(i, arg);
    if (traceEOA) { lprintf(" %s", arg->debug_print()); }
  }
  if (traceEOA)  lprintf("\n\n");
  return saved_args_and_rcvr;
}


fint frame::outgoing_arg_count(frame* sendee) {
  const bool traceOAC = false;
  // w/o rcvr
  // May return -1 of there is not even a receiver.
  sendDesc* sd = send_desc();
  fint r;
  if (sd->isPrimCall()) {
    char* addr = sd->jump_addr();
    PrimDesc* pd = getPrimDescOfFirstInstruction(addr, true);
    assert(pd != NULL, "no primitive descriptor");
    r = pd->arg_count();
    if (traceOAC) lprintf("***** frame::outgoing_arg_count: pd = %s, r = %d\n", pd->name(), r);
  }
  // calling self, may be _Perform
  else
    r = sd->arg_count();
  return r;
}


void frame::trace_patch(returnTrapHandlerFn new_fn) {
  LOG_EVENT2("patching frame %#lx to %#lx", this, first_inst_addr((void*)new_fn));
  
  if (traceV) {
    const char* n =
        new_fn == (returnTrapHandlerFn)        ReturnTrap ?         "ReturnTrap"
      : new_fn == (returnTrapHandlerFn)PrimCallReturnTrap ? "PrimCallReturnTrap"
      : new_fn == (returnTrapHandlerFn)      ProfilerTrap ?       "ProfilerTrap"
      : "Unknown";
    
    lprintf("*patching frame %#lx to %s from %#lx\n",
           (long unsigned)this, n,
           (long unsigned)(real_return_addr()));
  }
}
  

returnTrapHandlerFn frame::return_addr_for_patching( bool forceSelfFrame,
                                                     frame* sendee_arg) {
  if (forceSelfFrame)
    return (void (*)(...)) ReturnTrap;
  
  if (!sendee_arg)
    sendee_arg = sendee();
  assert( sendee_arg->sender() == this, "incorrect sendee_arg");
  assert( !sendee_arg->is_interpreted_self_frame(), "XXX not implemented yet");

  return sendee_arg->is_compiled_self_frame()
    ?  (void (*)(...))         ReturnTrap
    :  (void (*)(...)) PrimCallReturnTrap;
}
  

bool frame::is_patched() {
  interpreter* interp = get_interpreter();
  if (interp != NULL)  return interp->is_return_patched();
  return isPatchedReturnAddress(real_return_addr()); 
}


void frame::remove_patch() {
  if (traceV && is_patched())
    lprintf("*removing patch from frame %#lx\n", this);
    
  if (is_patched())
    LOG_EVENT1("removing patch from frame %#lx", this);
  else
   return;
   
  if (is_interpreted_self_frame())
    get_interpreter()->patch_return(not_patched);
  else {
    set_patched_frame_saved_outgoing_args(0); // not strictly necessary
    set_real_return_addr(return_addr());
  }
}


void frame::remove_patches_up_to_C() {
  frame* f = this;
  do {
    f->remove_patch();
    f = f->sender();
  } while (f->is_self_frame());
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
   
void frame::trace(oop receiver, nmethod* c) {
  print_trace_indent();
  lprintf("calling ");
  receiver->print_oop();
  lprintf(" %s", c->key.selector_string());
  assert(receiver->verify() && c->key.selector->verify(),
         "invalid oops");
  assert(receiver != Memory->deadBlockObj, "uncreated block");
  
  bool first = true;
  
  FOR_EACH_SLOTDESC(c->method()->map(), s) {
    if (s->is_arg_slot()) {
      if (first) {
        lprintf(" | ");
        first = false;
      }
      lprintf(":");
      s->name->string_print();
      lprintf(" = ");
      oop a = get_lookup_arg(smiOop(s->data)->value());
      assert(a->verify(), "invalid oops");
      a->print_oop();
      lprintf(". ");
      assert(a != Memory->deadBlockObj, "uncreated block");
    }
  }
  if (! first) lprintf("|");
  if (WizardMode) lprintf(" (code: 0x%lx)", c);
  lprintf("\n");
}

void frame::traceAssignment(oop receiver, nmethod* c) {
  print_trace_indent();
  lprintf("assigning ");
  receiver->print_oop(); 
  lprintf(" ");
  stringOop(c->key.selector)->string_print();
  lprintf(" ");
  oop a = get_lookup_arg(0);
  assert(receiver->verify() && a->verify(), "invalid oops");
  a->print_oop();
  if (WizardMode) lprintf(" (code: 0x%lx)", c);
  lprintf("\n");
}

void frame::traceLookup(oop receiver, nmethod* c) {
  print_trace_indent();
  lprintf("fetching ");
  assert(receiver->verify() && c->key.selector->verify(), "invalid oops");
  receiver->print_oop();
  lprintf(" ");
  stringOop(c->key.selector)->string_print();
  if (WizardMode) lprintf(" (code: 0x%lx)", c);
  lprintf("\n");
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


void frame::print_trace_indent() {
  for (fint i = my_stack()->depth() % 79; i > 0; i--) lprintf(" ");
}


void frame::print() {
  lprintf("%sframe %#lx: PC = %#lx, sender = %#lx,",
         is_compiled_self_frame() ? "Self " : "C ",
         this,
         real_return_addr(),
         sender());
  if (is_interpreted_self_frame()) {
    interpreter* interp = get_interpreter();
    lprintf(" interpreter = %#lx\n", interp);
  } else {
    print_compiled();
  }
}
  

static Conversion* conversion;

static void ConvertFrame_cont() { conversion->doit(); }

static void ConvertFrame(oop result, char* sp,
                         bool nlr, frame* nlrHome, int32 nlrHomeID, 
                         bool isInterp) {
  ConversionInProgress = true;
  // rm can't be on the stack (cause it might be overwritten; also needs
  // to deallocate itself
  ResourceMark* rm = new ResourceMark;
  conversion = new Conversion(result, sp, nlr, nlrHome, nlrHomeID, rm, isInterp);
  // CAUTION: while converting the frame, the original activation record of
  // this procedure will be OVERWRITTEN. Therefore, we do the actual work
  // in ConvertFrame_cont (after switching to the VM stack).
  switchToVMStack(ConvertFrame_cont);
}


// HandleReturnTrap helpers:

static void  unpatch_the_convertFrame_and_get_returnTrap_info(
        char* sp_of_patched_frame,
        frame*  patched_self_frame, 
        frame*& convertFrame, 
        char* & selfPC);
        
static bool return_trap_was_just_for_vframeOops(char* selfPC, frame* convertFrame);

static void trivial_exit_from_return_trap(oop result, char* sp_of_patched_frame, bool nlr,
                                          frame* nlrHome, int32 nlrHomeID, char* selfPC,
                                          frame* patched_self_frame);

static bool conversion_needed_for_return_trap(bool nlr, frame* nlrHome, int32 nlrHomeID,
                                              frame* convertFrame);
static void NLR_exit_from_return_trap(
              oop result, char* sp_of_patched_frame,
              frame* nlrHome, int32  nlrHomeID,
              frame* convertFrame, char* selfPC);
static void patch_frame_at_end_of_NLR_for_returnTrap(frame* convertFrame, frame* nlrHome);



// The caller of HandleReturnTrap is the frame that must be converted;
// the assembly glue makes sure no registers are clobbered before 
// HandleReturnTrap is called and creates a new stack frame.

void HandleReturnTrap(oop result, char* sp_of_patched_frame,
                      bool nlr, frame* nlrHome, int32 nlrHomeID) {
  // called by assembly glue when returning into a marked frame
  // first, make it look as if assembly glue was called from Self

  assert(nlr == true || nlr == false, "assembly glue passed bogus values 2");

  processSemaphore = true;
  FlushRegisterWindows();
  // next line must in IN THIS routine for SPARC

  // Remember, patched_self_frame is frame that would have been RETURNED INTO
  // had not the patching happened. -- dmu 1/03
  frame* patched_self_frame = currentFrame()->get_patched_self_frame(sp_of_patched_frame);

  char* selfPC;
  frame* convertFrame;
  unpatch_the_convertFrame_and_get_returnTrap_info(sp_of_patched_frame, patched_self_frame,
                                                   convertFrame, selfPC);
    
  if (traceV) 
    lprintf("*** HandleReturnTrap: sp_of_patched_frame = 0x%x, nlr = %d, nlrHome = 0x%x, nlrHomeID = %d patched_self_frame = 0x%x\n",
            sp_of_patched_frame, nlr, nlrHome, nlrHomeID, patched_self_frame);

  LOG_EVENT3("HandleReturnTrap res=%#lx sp_of_patched_frame=%#lx pc=%#lx", result, sp_of_patched_frame, selfPC);

  currentProcess->killVFrameOopsAndSetWatermark(convertFrame);  // kill extra vframes
  
  // figure out why the frame was marked & exit appropriately

  if ( return_trap_was_just_for_vframeOops(selfPC, convertFrame)) {
    trivial_exit_from_return_trap(result, sp_of_patched_frame,
                                  nlr, nlrHome, nlrHomeID,
                                  selfPC, patched_self_frame);
    ShouldNotReachHere();
  }
  // programming conversion / single-stepping trap / stop trap / unc. trap
  if (  conversion_needed_for_return_trap(nlr, nlrHome, nlrHomeID, convertFrame)) {
    ConvertFrame(result, sp_of_patched_frame, nlr, nlrHome, nlrHomeID,  selfPC == 0);
    ShouldNotReachHere();
  }
  // just return through this frame, don't need to convert
  NLR_exit_from_return_trap(result, sp_of_patched_frame, nlrHome, nlrHomeID, convertFrame, selfPC);
}  


objVectorOop OutgoingArgsOfReturnTrapOrRecompileFrame = NULL;

void  unpatch_the_convertFrame_and_get_returnTrap_info(
        char* sp_of_patched_frame,
        frame*  patched_self_frame, 
        frame*& convertFrame, 
        char* & selfPC) {
  selfPC = patched_self_frame->currentPC();
  
  if ( Memory->code->contains(selfPC) ) {
    // aha! compiled code
    assert(!Interpret, "interpreted code should not get here");
    if (patched_self_frame->is_patched()) {
      // really is a return trap; outgoing args were saved when frame was patched
      OutgoingArgsOfReturnTrapOrRecompileFrame = patched_self_frame->patched_frame_saved_outgoing_args();
      if (SaveOutgoingArgumentsOfPatchedFrames) {
        assert(patched_self_frame->sendee()->return_addr() == (char*)&ReturnTrap_returnPC
          ||  patched_self_frame->sendee()->return_addr() == (char*)&ReturnTrapNLR_returnPC ,
              "ensure patched_frame_saved_outgoing_args() will work");
        assert(OutgoingArgsOfReturnTrapOrRecompileFrame->verify_oop(), "check outgoing args");
        assert_objVector(OutgoingArgsOfReturnTrapOrRecompileFrame, "check outgoing args");
        assert(!currentProcess->isUncommon(), "must be real return trap");
      }
    }
    else {
      // was an uncommonBranch, only works on SPARC, are no outgoing args, just saved_out_regs
      assert(currentProcess->isUncommon(), "must be uncommon branch");
    }
    patched_self_frame->set_real_return_addr(selfPC);
    // skip asm frame
    convertFrame = currentProcess->last_self_frame(true);
    assert(convertFrame == patched_self_frame, "should be the same if compiled");
  }
  else {
    selfPC = 0; // mark in interp
    assert(Interpret, "compiled code should not get here");
    // must be in interpreter, find the frame
    // skip C interp frames
    convertFrame = currentProcess->last_self_frame(true);
    // Don't know if the following line works, not supporting the interp these days...dmu 2/03
    OutgoingArgsOfReturnTrapOrRecompileFrame = convertFrame->patched_frame_saved_outgoing_args();
    convertFrame->remove_patch();
  }
}


bool return_trap_was_just_for_vframeOops(char* selfPC, frame* convertFrame) {
  return !currentProcess->isKillingOrDeoptimizing()
      && !currentProcess->isUncommon()
#     if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      && !(selfPC && nmethod::findNMethod(selfPC)->isInvalid())
#     endif
      && !currentProcess->isSingleStepping()
      && !currentProcess->isStopping()
      && currentProcess->stopFrame() != convertFrame->vfo_locals_of_home_frame();
}

void trivial_exit_from_return_trap(oop result, char* sp_of_patched_frame, bool nlr,
                                   frame* nlrHome, int32 nlrHomeID, char* selfPC,
                                   frame* patched_self_frame) {
  // trap was for vframeOops; continue
  conversion = NULL;
  // returnToSelf will clear processSemaphore
  conversion->returnToSelf(result, sp_of_patched_frame, nlr, nlrHome, nlrHomeID,
                            selfPC == 0
                              ?  NULL
                              :  patched_self_frame->send_desc(),
                            selfPC == 0);
  ShouldNotReachHere();
}

                                          
bool conversion_needed_for_return_trap(bool nlr, frame* nlrHome, int32 nlrHomeID, frame* convertFrame) {
  if (!nlr) return true; // no nlr, must be here for conversion
  if (convertFrame->block_scope_of_home_frame() != nlrHome)  
    return false;  // not at frame of home, keep NLRing for now
  // doing NLR & have arrived at home frame
  if (currentProcess->isKillingOrDeoptimizing()) return true; // better convert
  return nlrHomeID != 0; // NLR ing into SIC, do conversion
}

  
void NLR_exit_from_return_trap(
              oop result,
              char* sp_of_patched_frame,
              frame* nlrHome,
              int32  nlrHomeID,
              frame* convertFrame,
              char* selfPC) {
              
  patch_frame_at_end_of_NLR_for_returnTrap(convertFrame, nlrHome);
  
  if (selfPC == 0) { // interpreting
    convertFrame->get_interpreter()->restartSend = false;
    processSemaphore = false;
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done with this
    // return into interpreter
    return;
  } 
  
  # if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    ((Conversion*)NULL)->nlr_to_compiled_self( result, false, nlrHome, nlrHomeID, (sendDesc*)selfPC, sp_of_patched_frame);
  # endif
    ShouldNotReachHere();
}


void patch_frame_at_end_of_NLR_for_returnTrap(frame* convertFrame, frame* nlrHome) {
  if (nlrHome == NULL) {
    // the block is from another process -- home isn't on our stack
    LOG_EVENT("HandleReturnTrap, nlrHome=0");
  }
  else {
    frame *f, *prev;
    for (f= convertFrame, prev= NULL;
         f && f->block_scope_of_home_frame() != nlrHome;
         prev= f, f= f->sender())
      ;
    if (f) f->patch(prev, true);
    LOG_EVENT2("HandleReturnTrap f=%#lx prev=%#lx", f, prev);
  }
}



# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

char *HandleProfilerTrap(char* return_address) {
  // called by assembly glue when returning into a marked frame
  // first, make it look as if assembly glue was called from Self
  assert(Memory->code->contains(return_address), "not in Self");

  sendDesc* sd = sendDesc::sendDesc_from_return_PC(return_address);
  char* insts =  sd->jump_addr();
  
  assert( Memory->code->contains(insts)
     ||   Memory->code->stubs->contains(insts)
     ||   insts == Memory->code->trapdoors->SendMessage_stub_td(),
          " should call self code");

  return return_address 
       + sendDesc::sendDesc_from_return_PC(return_address)->endOffset();
}

# endif


bool frame::verify(int n, RegisterLocator* rl) {
  bool r = true;
  interpreter* interp = get_interpreter();
  if (interp != NULL)
    r &= interp->verify();
  if (!ConversionInProgress || n > 0) {
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      if (is_compiled_self_frame()  &&  code()->isInvalid() && !is_patched()) {
        error1("frame %#lx should have patched return address", this);
        r = false;
      }
#   endif
  }
  r &= verify_oops(rl);
  return r;
}


bool frame::is_in_prologue() {
  assert( is_self_frame(), "only for self frames");

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    if (!is_interpreted_self_frame())
      return !code()->in_self_code_at(return_addr());
      
# endif

  return get_interpreter()->pc  ==  PrologueBCI;
}  


void frame::printVerbose() {
  print();
  printRegs();
  printVerbose_on_this_platform();

  # if defined(FAST_COMPILER) || defined(SIC_COMPILER)

    if (is_compiled_self_frame() && !ConversionInProgress) {
      lprintf("nmethod: %#lx \"%s\"; ",
             (long unsigned)(code()),
             selector_string(code()->key.selector));
      if (send_desc()) {
        lprintf("mask: "); printMask(send_desc()->mask()); lprintf("\n");
      }
    }

  # endif

  interpreter* interp = get_interpreter();
  if (interp != NULL) interp->print();
}


// called from conversion.c:

// return a copy of this frame and its nsenders senders
// need to copy 3 sparc_sps (at least on the Sparc) because extra args
// are in 3rd sparc_sp


frame* frame::copy(fint nsenders, bool adjustBlocks) {
  assert(this != NULL, "");
  frame* last_frame_to_copy = find_last_frame_to_copy(nsenders);
  if (last_frame_to_copy == NULL) return NULL;
  
  frame* first_copied_frame = copy_frames_through(last_frame_to_copy);
  adjust_frame_links_of_copied_frames(last_frame_to_copy, first_copied_frame);
  
  if (adjustBlocks) {
    // have to do this in separate loop because need correct links
    adjust_blocks_of_copied_frames(nsenders, first_copied_frame);
  }
  return first_copied_frame;
}


// helpers for copy()

frame* frame::find_last_frame_to_copy(fint nsenders) {
  // init to sender, because even if nsenders is zero,
  //  need my sender to copy any arguments to me beyond i5 -- dmu
  //
  // historic:
  // Also, on PPC need my sender to copy any incoming arguments
  // (which are saved in sender's frame). -- dmu 6/99
  frame* last_frame_to_copy = sender();
  for ( fint i = 0;  i < nsenders;  ++i ) {
    last_frame_to_copy = last_frame_to_copy->sender();
    if (last_frame_to_copy == NULL) return NULL;
  }
  return last_frame_to_copy;
}


frame* frame::copy_frames_through(frame* last_frame_to_copy) {
  int32 size = copy_through_oop_count(last_frame_to_copy);
  int32 size_for_aligning = size + frame_word_alignment - 1 + frame_alignment_offset*BytesPerWord; // align copied frames for assertions
  oop* frame_area = NEW_RESOURCE_ARRAY( oop, size_for_aligning);
  int frame_byte_alignment = frame_word_alignment << 2;
  frame* first_copied_frame = (frame*) 
    roundTo(int(frame_area), frame_byte_alignment)  +  frame_alignment_offset * BytesPerWord;
  assert((oop*)first_copied_frame + size  <=  frame_area + size_for_aligning, 
         "make sure aligning does not cause overflow");
  copy_oops((oop*)this, (oop*)first_copied_frame, size);

  return first_copied_frame;
}


// after copying this frame and nsenders to
//  first_copied_frame, adjust the blocks

void frame::adjust_blocks_of_copied_frames( fint   nsenders,
                                            frame* first_copied_frame) {
  frame* oldf = this;
  frame* newf = first_copied_frame;
  for ( fint i = 0;  i <= nsenders;  ++i ) {
    
    frame* news = newf->block_scope_of_home_frame();
    frame* olds = oldf->block_scope_of_home_frame();
    ResourceMark rm; // for the register locators
    newf->adjust_blocks(olds, news, RegisterLocator::for_frame(newf ));
    
    oldf = oldf->sender();
    newf = newf->sender();
  }
}


RegisterString frame::mask_if_present() {
  if (!is_self_frame())  return 0;
  sendDesc* s = send_desc();
  // bottommost frame may have no sendDesc (uncommon trap etc.)
  return s == NULL  ?  0  :  s->mask();
}

bool frame::is_aligned() { 
  return ((int32(this)  +  frame_alignment_offset * BytesPerWord)    &    (frame_word_alignment * BytesPerWord  -  1)) 
          == 0;
}

 

// and finally the functions that use the iterator/closures:


void frame::scavenge_contents(                       RegisterLocator* rl)  { FrameIterator( this, rl,  CheckAssertions,  mask_if_present(),  true,   OopScavenger     (           ).a() ); }
void frame::gc_mark_contents(                        RegisterLocator* rl)  { FrameIterator( this, rl,  CheckAssertions,  mask_if_present(),  false,  OopGCMarker      (           ).a() ); }
void frame::gc_unmark_contents(                      RegisterLocator* rl)  { FrameIterator( this, rl,  false,            mask_if_present(),  true,   OopGCUnmarker    (           ).a() ); }
void frame::switch_pointers(oop from, oop to,        RegisterLocator* rl)  { FrameIterator( this, rl,  false,            mask_if_present(),  true,   OopSwitcher      ( from, to  ).a() ); }
void frame::zap(RegisterString m,                    RegisterLocator* rl)  { FrameIterator( this, rl,  true,                             m,  true,   OopZapper        (           ).a() ); }
void frame::oops_do(oopsDoFn f,                      RegisterLocator* rl)  { FrameIterator( this, rl,  false,            mask_if_present(),  false,  OopDoer          ( f         ).a() ); }
void frame::stack_locations_do(oopsDoFn f,           RegisterLocator* rl)  { FrameIterator( this, rl,  false,            mask_if_present(),  false,  OopLocationsDoer ( f         ).a() ); }
void frame::adjust_blocks(frame* olds,  frame* news, RegisterLocator* rl)  { FrameIterator( this, rl,  CheckAssertions,  mask_if_present(),  true,   OopBlockAdjuster ( olds, news).a() ); }

bool frame::verify_oops(                             RegisterLocator* rl)  { bool r = true;
                                                                             FrameIterator( this, rl,  false,            mask_if_present(),  false,  OopVerifier      ( r         ).a() );
                                                                             return r; }


/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "process.hh"
# include "_process.cpp.incl"

// valid state transitions for processes:
//
// initialized --> ready --> stopped --> aborting --> defunct
//                   |                     ^
//                   |                     |
//                    ---------------------
//
// dummyProcess starts out in state 'defunct', all others in 'initialized'.

static Process* dummyProcess;
static Process* processList = NULL;
Process* currentProcess;
Process* prevProcess = NULL;            // proc transferring to current
Process* vmProcess;                     // vm prompt process
Process* twainsProcess = NULL;
static char* vmStackEnd = NULL;         // start (highest addr) of VM stack
bool ConversionInProgress = false;
bool8 processSemaphore = false;
PreemptCause preemptCause = cNoCause;
oop yieldArg  = 0;
oop yieldRcvr = 0;

char* newSPLimit = NULL;                  // for setSPLimitAndContinue
static void (* processTermFunc)();      // termination func; set during abort

void process_init() {
  processes = new Processes;
  dummyProcess = currentProcess = vmProcess = new Process(0, 0);
}

bool traceP = false;
bool traceV = false;

# if GENERATE_DEBUGGING_AIDS
  void printP(const char* s, void* p) {
    if (traceP) {
      lprintf("**P** %s ", s);
      if (p == (void*)&dummyProcess) lprintf("dummyProcess ");
      else if (p == (void*)vmProcess) lprintf("vmProcess ");
      else if (p == (void*)twainsProcess) lprintf("twainsProcess ");
      lprintf("0x%lx\n", p); 
    }
  }
# else
# define printP(a,b)
# endif

bool isOnVMStack(void* sp) {            // just a guess
  return (char*)sp <= vmStackEnd  &&  (char*)sp > vmStackEnd - 500000;
}

void Process::transfer() {
  bool initializingDstProcStack;
  
  printP("transferring to", this);
  assert(currentProcess != this, "transferring to myself");
  assert(! isOnVMStack(currentFrame()), "must not be on VM stack");
  assert(state == initialized || !hadStackOverflow(), "had stack overflow");
  assert(!processes->needsInvalidate, "should have handled this");
  assert(sizeof(processSemaphore) == 1,
         "assembly code assumes processSemaphore is a byte");
  assert(!processSemaphore, "processSemaphore shouldn't be set");

  processSemaphore = true;
  if (state == initialized) {
    init(suspendedPC);
    state = ready;
    initializingDstProcStack = true;
  } else {
    initializingDstProcStack = false;
  }
  char** currAddr = &currentProcess->suspendedSP;
  char** nextAddr = &this          ->suspendedSP;
  if (currentProcess == dummyProcess) {
    // creation of first process
    currAddr = NULL;
    vmStackEnd = (char*)currentFrame() - 128;
    prevProcess = this;
  } else {
    assert(currentProcess->state == stopped ||
           currentProcess->state == ready ||
           currentProcess->state == aborting, "wrong process state");
    prevProcess = currentProcess;
  }
  // Check for vframeOops.  This check must *always* be done before
  // resuming Self code.
  if (!initializingDstProcStack) {
    assert(nesting > 0, "not in Self");
    killVFrameOopsAndSetWatermark(NULL);
  } else {
    clearPreemption();
  }
  patchForSingleStepping();
  ShowContextSwitchInMonitor cs;
  

  assert(currentProcess->verifyFramePatches(), "frame patching bug");
  assert(verifyFramePatches(), "frame patching bug");
  
  currentProcess = this;
  
  if (currentProcess->profiler) {
    currentProcess->profiler->resume();
  }
  assert(sizeof(processSemaphore) == 1,
         "SetSPAndCall assumes bool8* points to a byte");
  bool8 ws = pcWasSet;  pcWasSet = false;
  SetSPAndCall(currAddr, nextAddr, initializingDstProcStack,
               &processSemaphore, ws);
}


Process::Process(processOop p, int32 sSize, oop rcvr, stringOop sel, 
                 objVectorOop args) {
  printP("creating", this);
  pcWasSet = false; 
  current_hash = IntervalTimer::dont_use_any_timer ? 0 : rand();
  if (sSize) {
    LOG_EVENT1("creating process %#lx", this);
    int pageSize = OS::get_page_size();
    int32 npages = (sSize + pageSize - 1) / pageSize;
    int32 size = npages * pageSize;
    while (size < 2 * PrimStackSize) size += pageSize;
    stk.init(this, NULL, size);
    method= as_smiOop(0); // just for the vmProcess
    state = initialized;
    procObj = currentProcess == dummyProcess ? Memory->processObj : p->clone();
    procObj->set_process(this);
  } else {
    // dummy process
    stk.init(NULL, NULL, 0); procObj = NULL; state = defunct;
  }
  
  next = NULL;
  suspendedPC = NULL; nesting = 0; 
  stopActivation = NULL;
  stepping = stopping = deoptimizing = restartAfterConversion = false;
  _killUpToVF = NULL;
  _uncommonPC = NULL;
  aborter = NULL;
  clear_check_vfo_locals();
  zombie= false;
  if (rcvr) {
    initialize(rcvr, sel, args);
  } else {
    init(0);
  }
  if (processList) addAfter(processList);
  profiler = NULL;
}

void Process::init(char* pcVal) {
  if (hasStack()) {
    printP("reinitializing", this);
    suspendedSP = stackEnd() - oopSize;
    suspendedSP = adjust_initial_SP(suspendedSP);
    set_words((int32*)suspendedSP, (stackEnd() - suspendedSP) / oopSize);
    resetStackOverflow();
  }
  setPC((process_p)pcVal);
}

// Create the method starting the process; parameters have already been
// checked.
void Process::initialize(oop rcvr, stringOop sel, objVectorOop args) {
  ResourceMark rm;
  ByteCode b(true);
  slotList* slots = EMPTY;

  stringOop receiverString = new_string("receiver");
  slots = slots->add(receiverString, parent_map_slotType, rcvr);

  b.GenSendByteCode(0, 0, receiverString, true, false, NULL);
  for(int32 i = 0; i < sel->arg_count(); i++) {
    b.GenLiteralByteCode(0, 0, args->obj_at(i));
  }
  b.GenSendByteCode(0, 0, sel, false, false, NULL);
  bool ok = b.Finish("<process creation>",
                     " \"create the process and send the initial message\" ");
  assert(ok, "should not be errors");
  
  method = create_outerMethod(slots, &b);
  init(first_inst_addr(startCurrentProcess));
}

void Process::startCurrentProcess() {
  currentProcess->start();
}


void Process::start() {
  printP("starting", this);


  if (Trace) {
    lprintf ("starting process\n");
  }
  nesting = 0;  // runDoItMethod will increment

  // used to call EnterSelf, I just had it call CallSelf (now runDoItMethod) to
  //  be better factored -- dmu 9/11/95
  // oop res = EnterSelf(Memory->lobbyObj, c->insts(), badOop);
  // now it calls runDoItMethod

  oop res = runDoItMethod(Memory->lobbyObj, method); 

  // process has finished
  if (this != currentProcess) fatal("this isn't current process");
  // nesting = 0;

  preservedList.clear();
  killVFrameOops(NULL);
  NLRSupport::reset_have_NLR_through_C();
  SignalInterface::unblock_self_signals();

  // Conceptually, this process is now dead.  But we can't delete this
  // right here because it's the current process, i.e. this procedure
  // is executing on cp's stack.  Thus, we just mark it as dead; it will
  // be deallocated either by TWAINS or during the next scavenge.
  
  state = aborting;
  processOop cpo = processObj();
  cpo->kill();
  
  if (aborter) {
    aborter->transfer();        // continue with the process which aborted this
  } else if (twainsProcess) {
    if (res == badOop) {
      preemptCause   = cAborted;
    } else {
      preemptCause   = cTerminated;
      cpo->set_return_oop(res);
    }
    twainsProcess->transfer();
  } else {
    // will be cleaned up at next scavenge
    vmProcess->transfer();
  }
}


// Resulting oop is interpreted by start above
volatile void Process::abort_process()     { NLRSupport::unwind_stack_to_kill_process(badOop); }
volatile void Process::terminate_process() { NLRSupport::unwind_stack_to_kill_process(0); }



Process::~Process() {
  if (this == dummyProcess) return;
  printP("destroying", this);
  LOG_EVENT3("destroying process %#lx, obj = %#lx, base = %#lx",
             this, processObj(), stk.base);
  if (this == currentProcess) fatal("process: cannot destroy myself");
  if (state == defunct) fatal("process already destroyed");
  if (this == prevProcess) prevProcess = NULL;
  Process *p, *q = NULL;
  for (p = processList; p && p != this; q = p, p = p->next) ;
  assert(p, "process not on process list");
  if (q) {
    remove(q);
  } else {
    processList = next;
  }
  state = defunct;
  if (hasStack()) stk.deallocate();
  if (processObj()) processObj()->kill();
}


bool Process::allocate() {
  printP("allocating", this);
  if (!hasStack()  &&  stk.size) {
    return stk.allocate();
  }
  return true;
}

// The process termination functions could be written more concisely
// using macros, but it would be a pain to debug.

void terminateMe() {
  if (currentProcess == twainsProcess) twainsProcess = NULL;
  void (* p)() = processTermFunc;
  processTermFunc = NULL;       // clear it for assertions
  p();
}

static void checkAbort(PrimDesc* pd) {
  if (!pd->canAbortProcess()) {
    fatal1("incorrect primitive entry for %s: canAbort must be set",
           pd->name());
  }
}

void Process::kill() {
  LOG_EVENT1("killing process %#lx", this);
  assert(state != aborting && state != defunct, "wrong status");
  if (state == initialized) {   
    assert(currentProcess != this, "wrong context");
  } else {
    assert(nesting > 0, "should be in Self");
    stack()->consistencyCheck(checkAbort);
    // make sure we don't crash (again) because we don't have a sendDesc
    //   but only compiled frames have one
    { // It's possible that lsf is NULL (eg first frame calls interruptCheck
      // and the process is then killed).
      frame *lsf;
      
      while (    lsf= last_self_frame(false),
                 lsf
             && !lsf->is_interpreted_self_frame()
             &&  lsf->send_desc() == NULL)
        suspendedSP = (char*)lsf->sender();
    }
    // don't do any programming conversions - just pop all frames; not strictly
    // necessary but saves some work
    killVFrameOops(NULL);       // kill all activation mirrors
    stack()->remove_patches();  
    if (this == currentProcess) {
      terminateMe();    
    } else {    
      setPC(terminateMe);
      state = ready;
      aborter = currentProcess;
      transfer();       
    }                           
  }     
}

void Process::terminate() {
  processTermFunc = (void (*)()) &terminate_process;
  kill();
}

void Process::abort() {
  processTermFunc = (void (*)()) &abort_process;
  kill();
}

void Process::setStopPoint(vframeOop stop) {
  stopActivation = stop;
}

void Process::setSingleStepping() {
  assert(isRunnable(), "should be runnable");
  stepping = true;
  patchForSingleStepping();
}

void Process::resetSingleStepping() {
  stepping = false;
}

void Process::patchForSingleStepping(frame* belowFrame) {
  if (isSingleStepping() || stopping) {
    // make sure we stop at the next possible byte code
    setupPreemption();
    if (inSelf()) {
      // make sure we catch returns, too
      // patch last Self frame; if passed in, belowFrame is the frame below
      // the one to be patched
      if (!belowFrame) belowFrame = stack()->first_VM_frame();
      frame* last = belowFrame->sender();
      last->patch(belowFrame);
    }
  }
}

frame* Process::stopFrame() {
  if (stopActivation) {
    assert(stopActivation->is_live(), "should be live");
    return stopActivation->locals();
  } else {
    return NULL;
  }
} 

void Process::killFrames(abstract_vframe* vf) {
  // kill off all activations below vf
  assert(this != currentProcess, "wrong context");
  resetSingleStepping();
  resetStopping();
  killVFrameOops(vf);
  vf->fr->patch(NULL);
  _killUpToVF = new_vframeOop(this, vf);
  if (traceV)
    lprintf("*** killFrames: starting kiling: _killUpToVF = 0x%x\n, fr = 0x%x, descOffset = %d\n",
            _killUpToVF, vf->fr, _killUpToVF->descOffset()->value());
            
  // switch processes and restart the receiver at doKillFrames; the process
  // will do a fake NLR to pop off the frames, then convert
  // the target frame and do one more NLR to pop the rest.  Finally,
  // having popped all frames, the receiver will suspend itself and return
  // to the controlling process
  // (currentProcess = process executing the killUpTo primitive)
  
  NLRSupport::set_NLR_home_from_C( (int32)vf->fr->block_scope_of_home_frame() );
  NLRSupport::set_NLR_home_ID_from_C(     vf->scopeID());
  transfer();
}

void Process::deoptimize(frame* last) {
  // deoptimize last stack frame
  assert(this != currentProcess, "wrong context");
  resetSingleStepping();
  resetStopping();
  setDeoptimizing();
  last->patch(NULL);
  NLRSupport::set_NLR_home_from_C( (int32)last->block_scope_of_home_frame() );
  // Why no NLRSupport::set_NLR_home_ID_from_C??? -- dmu 1/03
  // I don't think the arguments need to be saved, since gotoByteCode caller will set them anyway
  // -- dmu 1/03
  transfer();
}

void Process::gotoByteCode(abstract_vframe* vf, smi newBCI, objVectorOop exprStack,
                           void* FH) {
  // skip the current byte code of the bottomost vframe (vf)
  // first, deoptimize  the last stack frame
  { vframeOop vfo = new_vframeOop(this, vf);
    preserved p1(vfo);
    preserved p2(exprStack);
    deoptimize(vf->fr);
    vf = vframeOop(p1.value)->as_vframe();
    exprStack = objVectorOop(p2.value);
  }
 

  // now check the arguments (easier to do here - otherwise all oops would
  // have to be preserved)
  
  methodMap* mm = (methodMap*)vf->method()->map();
  fint currentBCI = vf->bci();
  fint maxBCI = mm->length_codes() - 1;
  if (newBCI < 0 || newBCI > maxBCI) {
    failure(FH, "invalid byte code index");
    return;
  }
  
  if (vf->is_interpreted()) {
    fatal("unimp for interpreter");
    return;
  }
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  assert(vf->as_compiled()->code->isDebug(), "should be debug-compiled");
  compiled_vframe** vfs;
  NameDesc** nds;
  fint len;
  compiled_vframe* newVF = new compiled_vframe( vf->fr,
                                                vf->as_compiled()->code,
                                                vf->as_compiled()->desc,
                                                newBCI,
                                                vf->as_compiled()->reg_loc());
  newVF->get_exprStackInfo(NULL, vfs, nds, len);
  if (len != exprStack->length()) {
    char msg[80];
    sprintf(msg, "invalid expression stack length (should be %ld)", long(len));
    failure(FH, msg);
    return;
  }

  fint i;
  for (i = 0; i < len; i++) {
    oop v = exprStack->obj_at(i);
    if (nds[i]->isMemoizedBlock() && !(v->is_block() && v->is_live())) {
      char msg[180];
      const char* blk = v->is_block() && !v->is_live() ? "live " : "";
      sprintf(msg, "implementation restriction: expression stack entry %ld must be a %sblock", long(i), blk);
      failure(FH, msg);
      return;
    } 
    else {
      assert(!nds[i]->isBlockValue(), "shouldn't be a block value");
    }
  }

  // all args are ok; kill off / create blocks between the current and new bci
  IntList* currentBlocks = mm->blocks_upto(currentBCI);
  IntList* newBlocks     = mm->blocks_upto(newBCI);
    
  if (newBlocks->length() < currentBlocks->length()) {
    // need to kill some blocks
    assert(newBCI < currentBCI, "expected to go backwards");
    for (IntListElem* e = currentBlocks->head(); e; e = e->next()) {
      if (!newBlocks->includes(e->data())) {
        NameDesc* n = vf->as_compiled()->desc->blockElem(e->data());
        assert(n->isMemoizedBlock(), "should be memoized");
        oop block = vf->as_compiled()->get_contents(n);
        assert_block(block, "expected a block");
        block->kill();
      }
    }
  } else if (newBlocks->length() > currentBlocks->length()) {
    // need to create some blocks
    assert(newBCI > currentBCI, "expected to go forwards");
    for (IntListElem* e = newBlocks->head(); e; e = e->next()) {
      if (!currentBlocks->includes(e->data())) {
        NameDesc* n = newVF->desc->blockElem(e->data());
        assert(n->isMemoizedBlock(), "shouldn't be memoized");
        
        // get_contents creates the block
        oop block = newVF->get_contents(n);
        
        // also set block in expr stack (may be a different location) -- dmu
        NameDesc* n2 = newVF->desc->exprStackElem(e->data());
        newVF->set_contents(n2, block);
        
        assert_block(block, "should be a block");
        assert(block->is_live(), "should be live");
      }
    }
  }

  // now fill in new expression stack
  for (i = 0;  i < len;  i++) {
    oop v = exprStack->obj_at(i);
    newVF->set_contents(nds[i], v);
  }

  // set the PC
  sendDesc* sd = newVF->code->sendDescFor(newVF, false);
  newVF->fr->set_return_addr((char*)sd);
  // and finally, replace the frame *below* newVF->fr with the correct one;
  // deoptimize will pop it, do a no-op deoptimization and create the right one
  deoptimize(newVF->fr);
  
# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  ShouldNotReachHere();
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
}


bool Process::inSelf(bool including_prologue) {
  return nesting > 0 && last_self_frame(including_prologue) != NULL;
}
  
bool Process::hasEmptyStack() {
  ResourceMark rm;
  // is the current stack empty?  (ignoring "invisible" frames)
  if (!inSelf()) return true;
  if (this == vmProcess) {
    return false;
  } else {
    // top frame is hidden doIt (with the initial perform)
    // check if we have a second vframe
    abstract_vframe* vf = new_vframe(last_self_frame(false));
    return vf->sender() == NULL;
  }
}

bool isStackOverflow(char* addr) {  
  return currentProcess != dummyProcess &&
    currentProcess->isStackOverflow(addr);
}

bool Process::verifyFramePatches() {
  bool ok = true;
  // make sure all frames are correctly patched
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (!processTermFunc && nesting > 0) {
    for (frame *f = last_self_frame(true), *f0= f; f; f = f->selfSender()) {
      if (ConversionInProgress && f == f0) continue;
      if (f->is_interpreted_self_frame()) continue;
      if (f->code()->isInvalid() && !f->is_patched()) {
        ok = false;
        error1("frame %#lx: should be patched", f);
      }
    }
  }
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  return ok;
}

bool Process::verify() {
  bool r = true;
  if (state != initialized)  r &= stack()->verify();
  if (processObj()->process() != this) {
    error2("Process object %#lx doesn't point to process %#lx",
           processObj(), this);
    r = false;
  }
  bool verify_result = true;
  VERIFY_TEMPLATE(&procObj);
  VERIFY_TEMPLATE(&method);
  r &= verify_result;
  if (!verifyVFrameList()) {
    lprintf("\t of vframeList of process %#lx\n", this);
    r = false;
  }
  if (deoptimizing) { error1("Process %lx:  deoptimizing flag set", this);  r= false; }
  if (restartAfterConversion) {
    error1("Process %lx: restartAfterConversion flag set", this);
    r = false;
  }
  r &= verifyFramePatches();
  return r;
}


void Process::print() {
  static const char* pstate[] = {
    "initialized", "ready", "stopped", "aborting", "defunct" };
  lprintf("Process %#lx: state = %s, %s, obj = %#lx, method = %#lx\n",
         this, pstate[state],
         inSelf() ? "in Self" : "not in Self",
         procObj, method);
  lprintf("\tp ((Process*)%#lx)->stack()->print()\n", this);
  stack()->print();
}

void Process::print_short() { lprintf("process %#lx", this); }

void Process::convert() {
  stack()->convert();
}

# if GENERATE_DEBUGGING_AIDS
  void pps(Process* p) { p->stack()->print(); }
# endif

// ------------------------- memory functions ------------------------

// the preservedList contains all preservedObjs of the process; it's really
// a stack, i.e. elem 0 is the oldest, elem(n) the youngest

inline bool moreRecent(void* p, void* q) {  // is p more recent than q?
  return p < q || isOnVMStack(p) != isOnVMStack(q);
}

void PreservedList::oops_do(oopsDoFn f, Process* proc) {
  for (fint i = list.length() - 1; i >= 0; i--) {
    preservedVmObj* p = list.nth(i);
    assert(proc->contains((char*)p) || proc == currentProcess,
           "preservedObj not on process stack");
    UsedOnlyInAssert(proc);
    if (i > 0) {
      preservedVmObj* prev = list.nth(i - 1);
      // The latest MW compiler does not guarantee this property:
      // Two stack allocated objects in same activation may not be ordered.
      // -- dmu 6/1
      // assert(moreRecent(p, prev), "prev preservedObj should be more recent");
    }
    p->oops_do(f);
  }
}

void Process::preserve(preservedVmObj* p) {
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  traceP) lprintf("*preserving %#lx (process %#lx)\n",
                                           p, this);
# endif

// The latest MW compiler does not guarantee this property:
// Two stack allocated objects in same activation may not be ordered.
// -- dmu 6/1
# if 0
    while (preservedList.list.nonEmpty()) {
      preservedVmObj* prev = preservedList.list.last();
      if (moreRecent(p, prev)) {
        break;            // everything is ok
      } else {
        // there's at least one dangling preservedObj on the stack
        // (not cleaned up because of some C frame popping)
        if (traceP) lprintf("*discarding %#lx (process %#lx)\n",
                            (long unsigned)prev, (long unsigned)this);
        preservedList.list.pop();
      }
    }
# endif
  preservedList.list.push(p);
}

void Process::un_preserve(preservedVmObj* p) {
# if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions  &&  traceP) lprintf("*unpreserving %#lx (process %#lx)\n",
                                           p, this);
# endif
  while(preservedList.list.pop() != p) {
    // Get rid of all extra preservedVmObjs created after p.
    // This can happen e.g. when a process is killed within an _Eval
    // (i.e. SelfNesting > 1)
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  traceP) lprintf("*  also discarding %#lx (process %#lx)\n",
                                               (preservedList.list.top()),
                                               this);
#   endif
  }
}

static void scavenge_func(oop* p) { SCAVENGE_TEMPLATE(p); }
static void gc_mark_func(oop* p) { MARK_TEMPLATE(p); }
static void gc_unmark_func(oop* p) { UNMARK_TEMPLATE(p); }
static oop from, to;
static void switch_pointers_func(oop* p) { SWITCH_POINTERS_TEMPLATE(p) }
static bool markingFromZombies;

void Process::scavenge_contents() {
  if (state != aborting) stack()->scavenge_contents();
  SCAVENGE_TEMPLATE(&procObj);
  SCAVENGE_TEMPLATE(&method);
  SCAVENGE_TEMPLATE(&stopActivation);
  SCAVENGE_TEMPLATE(&_killUpToVF);
  preservedList.oops_do(scavenge_func, this);
}

void Process::gc_mark_contents() {
  assert(zombie, "shouldn't visit twice");
  if (!markingFromZombies) zombie= false;
  if (state != aborting) stack()->gc_mark_contents();
  MARK_TEMPLATE(&procObj);
  MARK_TEMPLATE(&method);
  MARK_TEMPLATE(&stopActivation);
  MARK_TEMPLATE(&_killUpToVF);
  preservedList.oops_do(gc_mark_func, this);
}

void Process::gc_unmark_contents() {
  if (state != aborting) stack()->gc_unmark_contents();
  UNMARK_TEMPLATE(&procObj);
  UNMARK_TEMPLATE(&method);
  UNMARK_TEMPLATE(&stopActivation);
  UNMARK_TEMPLATE(&_killUpToVF);
  preservedList.oops_do(gc_unmark_func, this);
}

void Process::switch_pointers() {
  if (state != aborting) stack()->switch_pointers(from, to);
  SWITCH_POINTERS_TEMPLATE(&procObj);
  SWITCH_POINTERS_TEMPLATE(&method);
  SWITCH_POINTERS_TEMPLATE(&stopActivation);
  SWITCH_POINTERS_TEMPLATE(&_killUpToVF);
  preservedList.oops_do(switch_pointers_func, this);
}

void Process::write_snapshot(FILE* f) { Unused(f); }

void Process::read_snapshot(FILE* f) {
  Unused(f);
  // a hack until we have real snapshots
  if (this == vmProcess) {
    procObj = Memory->processObj;
    procObj->set_process(this);
  } else {
    procObj = NULL;           // procObj was overwritten by snapshot
    state = aborting;         // discard this process ASAP
  }
}


// --------------------------- preemption --------------------------

inline void handlePreemption() {
  if (Memory->needs_scavenge()) Memory->scavenge();
  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  if (Memory->code->needsWork()) Memory->code->doWork();
  if (Memory->code->stubs->needsWork) Memory->code->stubs->cleanup();
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

  if (twainsProcess == NULL && Memory->old_gen->needToSignalLowOnSpace) {
    Memory->old_gen->needToSignalLowOnSpace= false;
    Memory->default_low_space_handler();
  }

  frame* last = currentProcess->stack()->last_self_frame(true);
  currentProcess->killVFrameOopsAndSetWatermark(last);
  if (currentProcess->isKillingOrDeoptimizing()) {
    // done killing frames / deoptimizing - return to caller of primitive
    currentProcess->resetKilling();
    currentProcess->resetDeoptimizing();
    prevProcess->transfer();
  } else if (twainsProcess && preemptCause != cNoCause) {
    if (SignalInterface::are_self_signals_blocked()  &&  preemptCause == cSignal) {
      // ignore the signal(s) until blocking is removed
    } else if (preemptCause == cSingleStepped &&
               currentProcess->hasEmptyStack()) {
      // initial single-step - skip "invisible" doIt method
      // and prologue part of a nmethod.
      if (!currentProcess->isSingleStepping()) {
        currentProcess->setSingleStepping();
      }
      currentProcess->patchForSingleStepping();
    } else {
      // suspend current process and return to twains process (which will
      // return from the TWAINS primitive)
      // Self processes that unwillingly yielded the CPU are suspended here;
      // the other suspension point is in Yield_prim

      if (Memory->old_gen->needToSignalLowOnSpace) {
        preemptCause= cLowOnSpace;
        Memory->old_gen->needToSignalLowOnSpace= false;
      }
      if (currentProcess->profiler) {
        currentProcess->profiler->suspend();
      }
      twainsProcess->transfer();
    }
  } else if ( PendingSelfSignals::count(sigquit) != 0 ) {
    abortSelf();
  } else {
    // no need to switch processes
  }
   
  // do this last because the timer might go off again and
  //  we don't want to undo preemption after that happens
  IntervalTimer::do_all_sync_tasks();
}


bool Process::preemptionPending() {
  if (currentSPLimit() == stackEnd()) return true;
  if (newSPLimit == stackEnd()) {
    // After InterruptedContext::setupPreemptionFromSignal has been called but before setSPLimitAndContinue is
    // executed preemption is pending.
    assert( processSemaphore, "processSemaphore should be set");
    return true;
  }
  return false;
}


void preemptor() {
  if (SignalInterface::are_self_signals_blocked()) 
    return;
  if (processSemaphore || currentProcess == twainsProcess) {
    // Either the twainsProcess is idle (and will wake up immediately after we
    // return), or we are in a critical section and should not be disturbed.
    return;
  } else if (currentProcess->nesting == 0) {
    // This is either the VM process (near the prompt) or a process that
    // is just about to start (i.e., in startCurrentProcess).  Wait.
    return;
  }
  // Set the SP limit so that the next call will cause a "stack overflow".
  InterruptedContext::setupPreemptionFromSignal();
}


// ----------------------- vframeOop management --------------------

// All live vframeOops are in the vframeList of their respective process.
// Within the list, vframes are ordered from most recent to least recent
// (closest to top-of-stack to farthest).

vframeOop Process::findInsertionPoint(abstract_vframe* target) {
  // Returns vframeOop after which vf should be inserted.  If an identical
  // vframe is already in the list, the position returned is before this
  // vframe.
  ResourceMark rm;
  if (traceV) {
    static fint findInseritonPoint_count = 0;
    ++findInseritonPoint_count;
    lprintf("*** Entering Process::findInsertionPoint(0x%x), %d\n", target, findInseritonPoint_count);
    verifyVFrameList();
  }
  assert(stack()->contains((char*)target->fr) ||
         (ConversionInProgress || theRecompilation)
          && isOnVMStack(target->fr), "not in my stack");
  
  vframeOop sentinel = procObj->vframeList();

  // set l to first vfo above or same as target (target),
  // set prev to l's predecessor
  
  vframeOop l, prev= sentinel;
  for (l = prev->next();
       l  &&  l->is_below(target->fr);
       prev = l,  l = prev->next())
    ;
  
  // if none above target, or if none equal to target, return prev
  // can also return prev for interpreter, since all vfo's in same real frame
  //  are for same vframe.

  if ( l == NULL  ||  l->is_above(target->fr)  ||  target->is_interpreted()) {
    return prev;
  }

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  compiled_vframe* ctarget = target->as_compiled(); // shortcut
  
  // several vframes with same frame or vframeOop has not been created yet
  
  assert(l->is_equal(ctarget->fr), "just checkin'");

  compiled_vframe* trial_vframe_in_target_frame =
    new_vframe(ctarget->fr)->as_compiled();

  // trial_vframe_in_target_frame iterates through all vframes of the frame,
  // and l/prev follow
  // through the list.
  // CAUTION: some vframeOops might be dead --> do not construct vframes
  
  for (;;) {
    if (l == NULL) {
      // at end of list, insert after prev
      assert(  prev == sentinel
           ||  prev->is_below(ctarget->fr)
           ||  ctarget->isCallerOf(
                  ctarget->code->scopes->at(prev->descOffset())),
               "incorrect order");
      if (traceV) lprintf("A");
      return prev;
    }
    if (  trial_vframe_in_target_frame == NULL
      ||  l->is_above( trial_vframe_in_target_frame->fr )) {
      // list didn't contain any vframe above target
      assert(l->is_above(ctarget->fr), "should be beyond this frame");
      if (traceV) lprintf("B");
      return prev;
    }
    if ( trial_vframe_in_target_frame->isCallerOf(
            trial_vframe_in_target_frame->code->scopes->at(
                                                 l->descOffset()))) {
      // l is below target; advance it
      if (traceV) lprintf("C");
      prev = l;
      l = l->next();
    }
    else if ( trial_vframe_in_target_frame->desc->is_equal( ctarget->desc )) {
      // found right place to insert
      if (traceV) lprintf("D");
      assert( prev->is_below(ctarget->fr)
           || ctarget->isCallerOf(
                        ctarget->code->scopes->at(prev->descOffset())),
              "incorrect order");
      assert( l->is_equal(ctarget->fr)
              &&  !ctarget->isCallerOf(ctarget->code->scopes->at(
                                                      l->descOffset())),
              "incorrect position");

      return prev;
    }
    else {
      if (traceV) lprintf("E");
      trial_vframe_in_target_frame =
        trial_vframe_in_target_frame->immediateSender()->as_compiled();
    }
  }
# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  ShouldNotReachHere();
  return NULL;
# endif
}


vframeOop Process::findVFrameOop(abstract_vframe* vf) {
  // find vframeOop for vf
  vframeOop last = findInsertionPoint(vf);
  vframeOop nxt = last->next();
  if (nxt && vf->EQ(nxt->as_vframe())) {
    return nxt;
  } else {
    return NULL;
  }
}

vframeOop Process::insertVFrameOop(vframeOop vfm) {
  ResourceMark rm;
  assert(vfm->process() == this, "wrong process");
  assert(vfm->next() == NULL, "shouldn't be linked");
  vframeOop last = findInsertionPoint(vfm->as_vframe());
  vframeOop nxt = last->next();
  if (nxt && vfm->equal(nxt)) {
    fatal("vframeOop already exists");
    return nxt;
  } else {
    if (traceV) lprintf("*** inserting vframeOop %#lx\n", vfm);
    vfm->insertAfter(last);
    return vfm;
  }
}


// kill all vframes up to (but not including) currentVF

void Process::killVFrameOops(abstract_vframe* currentVF) {
  ResourceMark rm;
  vframeOop lastToKill;
  if (currentVF) {
    lastToKill = findInsertionPoint(currentVF);
    if (traceV) 
      lprintf("*** killVFrameOops(currentVF = 0x%x, fr = 0x%x, lastToKill 0x%x)\n",
              currentVF, currentVF->fr, lastToKill);
  } else {
    // was last self frame - kill all vframes
    lastToKill = NULL;
  }
  vframeOop sentinel = procObj->vframeList();
  if (lastToKill != sentinel) {
    vframeOop l;
    for (l = sentinel->next(); l != lastToKill; l = l->next()) {
      l->kill(); 
    }
    if (l) l->kill();
    sentinel->set_next(l ? l->next() : NULL);
  }

  if (check_vfo_locals) {
    killVFrameOopsInCurrentFrame(currentVF);
    clear_check_vfo_locals();
  }
  
  // check if we returned from stopActivation
  if (stopActivation && !stopActivation->is_live()) {
    stopping = true;
    if (preemptCause == cNoCause) preemptCause = cFinishedActivation;
  }

  if (traceV) verifyVFrameList();
}


// kill vframes which were in current frame at time of last interrupt
//  but have since died

void Process::killVFrameOopsInCurrentFrame(abstract_vframe* currentVF) {
  
  frame* f = frame_for_check_vfo_locals(currentVF);
  if (f == NULL) return;

  vframeOop sentinel = procObj->vframeList();

  abstract_vframe* vf = new_vframe(f);
  vframeOop lastToKill = findInsertionPoint(vf);

  trace_killVFrameOopsInCurrentFrame(lastToKill, vf);

  if (lastToKill != sentinel) {
    vframeOop firstSurvivor = lastToKill->next();       // don't kill this one
    vframeOop prev = sentinel;  // prev guy (to delete elems from list)
    vframeOop l    = sentinel->next();
    for ( ;
          l  &&  l != firstSurvivor  &&  l->locals() <= check_vfo_locals;
          l = l->next()) {
      if (l->locals() == check_vfo_locals) {
        // kill l and remove it from the list
        l->remove(prev);
        l->kill(); 
      } else {
        prev = l;
      }
    }
  }
}
  

// check_vfo_locals records the vfo locals of the last frame there
//  was a live vfo for.
// If that frame is not still on the stack, just return NULL.

// This routine finds the corresponding frame for check_vfo_locals.
// It also checks that control came back here soon enough
//  to kill any vfos that needed it.
// 
// check_vfo_locals was the bottommost frame last time we checked; it could have
// changed its value if its nmethod branched to an uncommon case.
// If this happened, we have to correct the corresponding vfos.
// Note that we executed at most one call or return since last time,
// but we never get here on a return, since killVFrameOopsInCurrentFrame
// checks for it and does not call me if so,
// so check_vfo_locals must be one of the first two frames.

frame* Process::frame_for_check_vfo_locals(abstract_vframe* currentVF) {
  
  if (currentVF == NULL)
    return NULL;
  
  frame* first  = currentVF->fr;
  frame* second = first->selfSender();

  // check to see if we have returned since check_vfo_locals
  
  // if frame is no longer on stack, all vfos have been killed through the
  // normal procedure

  if ( check_vfo_locals_sender < first->vfo_locals_of_home_frame()->sender() )
     return NULL;

     
  if (traceV) lprintf("*check ");

  if (check_vfo_locals_sender == first->vfo_locals_of_home_frame()->sender()) {
    // still bottom frame
    if (traceV) lprintf("first ");
    return first;
  }
  if ( check_vfo_locals_sender ==
       second->vfo_locals_of_home_frame()->sender() ) {

    // 2nd frame (a call was executed since last check)
    if (traceV) lprintf("second ");
    return second;
  }
  fatal("check_vfo_locals must be either first or second - stopped too late");
  return NULL;
}


void Process::set_check_vfo_locals(abstract_vframe* currentVF) {
  // NB: the check_vfo_locals business is necessary because we can't just remember
  // frame* pointers -- these may change!  (Since they depend on the 
  // size of the sp area below the sp area we're interested in)
  
  check_vfo_locals             = currentVF->fr->vfo_locals_of_home_frame();
  check_vfo_locals_sender      = check_vfo_locals->sender();
  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  check_vfo_locals_is_uncommon = currentVF->is_compiled()
                              && currentVF->as_compiled()->code->isUncommon();
# else
  check_vfo_locals_is_uncommon = false;
# endif
}


void Process::clear_check_vfo_locals() {
  check_vfo_locals = check_vfo_locals_sender = NULL;
  check_vfo_locals_is_uncommon = false;
}
  

void Process::trace_killVFrameOopsInCurrentFrame( vframeOop lastToKill,
                                                  abstract_vframe* vf ) {
  if (traceV) {
    lprintf("*lastToKill %#lx, vf %#lx\n",
           (long unsigned)lastToKill, (long unsigned)vf);
    vf->print_frame();
  }
}


void Process::convertVFrameOops( frame* fr,
                                 frame* vfoLocals,
                                 nmethod* invalidNM,
                                 int32 vdepth,
                                 abstract_vframe** old_vf,
                                 abstract_vframe** new_vf ) {
  // convert vframes after programming
  // fr, vfoLocals:  old frame/vfo locals (which no longer exist)
  //                   (fr is interpreter_frame... if interpreted)
  // vdepth:  size of the next two params
  // old_vf:  all old vframes (which point to a copy of the invalid frame)
  // new_vf:  the corresponding new vframes (with actual frame pointers)
  //          both arrays are ordered from least to most recent vframe
  //          and the frames are in elements 1..vdepth (0 is the caller)
  ResourceMark rm;
  
  vframeOop l = procObj->vframeList()->next();
  clear_check_vfo_locals();

  LOG_EVENT3("convertVFrameOops %#lx %#lx %d", fr, vfoLocals, vdepth);
  if (traceV) lprintf("*** converting vframeOop %#lx w/ locals 0x%x, descOffset %d\n",
                      l, l->locals(), l->descOffset()->value());
  
  if ( l  &&  l->locals() == vfoLocals ) {

    // Make all vfos which used to point to vfoLocals point to their new
    // frame.  Also change the desc offset.

    for ( int32 i = vdepth;
          l  &&  l->locals() == vfoLocals;
          l = l->next() ) {

      // find i so old_vf[i] matches l

      if (old_vf[i]->is_interpreted()) {
        ; // no inlining: trivially matches
      }
      else {
#       if defined(FAST_COMPILER) || defined(SIC_COMPILER)
          ScopeDesc* ldesc = invalidNM->scopes->at( l->descOffset() );
          for ( ;
                !old_vf[i]->as_compiled()->desc->is_equal(ldesc)  &&  i > 0;
                i--)
            ;
          assert( old_vf[i]->as_compiled()->desc->is_equal(ldesc),  "not found");
#       else
          ShouldNotReachHere(); Unused(invalidNM);
#       endif
      }

      // convert l
      
      l->set_locals(     new_vf[i]->fr->vfo_locals_of_home_frame());
      l->set_descOffset( new_vf[i]->descOffset());
      l->set_method(     new_vf[i]->method());

      if (traceV) lprintf("*** converted vframeOop %#lx to locals 0x%x, descOffset %d\n",
                          l, l->locals(), l->descOffset()->value());
      LOG_EVENT1("converting vframeOop %#lx", l);
    }
  }
  if (traceV) {
    verifyVFrameList();
    printVFrameList(5);
  }
}



// remove dead vframeOops and set up things to regain control at the
// appropriate point(s)
// current is current Self frame (passed in as a speed optimization) or NULL

void Process::killVFrameOopsAndSetWatermark(frame* current) {

  // first check for the common case - no vframeOops at all

  if (procObj->vframeList()->next() == NULL) {
    clearWatermark();
    return;
  }

  ResourceMark rm;

  if (current == NULL && inSelf()) current = stk.last_self_frame(true); 
  assert(     !current
          ||  stack()->contains((char*)current )
          ||  ConversionInProgress && isOnVMStack(current),
         "not in my stack");

  abstract_vframe* currentVF = current ? new_vframe(current) : NULL;

  traceAndLog_killVFrameOopsAndSetWatermark(current, currentVF);

  killVFrameOops(currentVF);

  // may not be anymore vframes to do

  if (procObj->vframeList()->next() == NULL) {
    clearWatermark();
    return;
  }
  setWatermark( currentVF);

  traceEpilog_killVFrameOopsAndSetWatermark();
}


// do not need to regain control for vframe killing

void Process::clearWatermark() {
  if (!Memory->needs_scavenge()) clearPreemption();
  clear_check_vfo_locals();
}


// called to arrange to regain control in future to kill
//  any vfos that may be left.
// Assumes procObj vframeList is non-null

void Process::setWatermark( abstract_vframe* currentVF ) {
  
  frame* current = currentVF->fr;
  vframeOop first = procObj->vframeList()->next();

  if (first->is_above(currentVF->fr)) {

    // the only live vframeOops are above the current frame - patch return
    // address

    frame* target = first->locals();
    frame* sender = current->sender();
    for ( ;
         sender->vfo_locals_of_home_frame() != target;
         current = sender, sender = current->sender()) ;
    sender->patch(current);
    clearWatermark();
  }
  else {

    // have live vframeOops in the current frame
    // at next interrupt, we need to check if these are still live (even
    // if the frame isn't the current frame anymore)

    assert( first->is_equal(current),  "cannot be below me");
    assert( first->locals() == current->vfo_locals_of_home_frame(),
            "must be the same");

    set_check_vfo_locals( currentVF );
    
    setupPreemption();
    frame* target = current->selfSender();
    if (target) target->patch(NULL);
  }
}



void Process::traceAndLog_killVFrameOopsAndSetWatermark( frame* current,
                                                  abstract_vframe* currentVF) {
  LOG_EVENT2("killVFrameOopsAndSetWatermark(%#lx, %#lx)", current, check_vfo_locals);
  if (traceV) {
    lprintf("killVFrameOopsAndSetWatermark(%#lx, %#lx):\n",
           (long unsigned)current, (long unsigned)check_vfo_locals);
    frame* f = current;
    for (int32 i = 0; f && i < 5; i++, f = f->sender())
      lprintf("%#lx ", f);
    lprintf("\n");
    if (currentVF) {
      currentVF->print_frame(0);
      abstract_vframe* s = currentVF->sender();
      if (s) s->print_frame(1);
    }
  }
}


void Process::traceEpilog_killVFrameOopsAndSetWatermark() {
  if (traceV) {
    printVFrameList(5);
    lprintf("\t");
    if (preemptionPending()) lprintf("preemption pending; ");
    lprintf("check_vfo_locals = %#lx, check_vfo_locals_sender = %#lx%s\n",
           (long unsigned)check_vfo_locals,
           (long unsigned)check_vfo_locals_sender,
           check_vfo_locals_is_uncommon ? ", uncommon" : "");
  }
}


bool Process::verifyVFrameList() {
  ResourceMark rm;
  vframeOop last = procObj->vframeList();
  vframeOop nxt;
  for (vframeOop l = last->next(); l; last = l, l = nxt) {
    nxt = l->next();
    if (nxt) {
      frame* fr = l->fr();
      if (l->is_above(nxt->fr())) {
        error2("wrong frame ordering (%#lx > %#lx)", fr, nxt->fr());
        return false;
      }
#     if defined(FAST_COMPILER) || defined(SIC_COMPILER)
      if (fr->is_compiled_self_frame()) {
        ScopeDesc* s = fr->code()->scopes->at(l->descOffset());
        if (l->is_equal(nxt->fr()) && !nxt->as_vframe()->isCallerOf(s)) {
          error2("wrong vframe ordering (vframeOops %#lx and %#lx)", l, nxt);
          return false;
        }
      }
#     endif
      if (l->equal(nxt)) {
        error2("duplicate vframeOops (%#lx and %#lx)", l, nxt);
        return false;
      }
      frame* f = last_self_frame(true);
      while (l->is_above(f)) f = f->sender();
      if (!l->is_equal(f)) {
        error2("invalid frame pointer %#lx in vframeOop %#lx", fr, l);
        return false;
      } else if (f->vfo_locals_of_home_frame() != l->locals()) {
        error2("invalid locals pointer %#lx in vframeOop %#lx",
               l->locals(), l);
        return false;
      }
      // construct the vframe - will fail if we have a bogus vframeOop
      l->as_vframe();
    }
  }
  return true;
}


void Process::printVFrameList(fint howMany) {
  if (procObj->vframeList()->next()) {
    lprintf("***vframe list of process %#lx:\n", this);
    vframeOop l;
    for (l = procObj->vframeList()->next(); l && howMany-- > 0;
         l = l->next()) {
      lprintf("\t"); l->print();
    }
    if (l) lprintf("\t...\n");
  }
}

// --------------------------- processes --------------------------

Processes* processes;
int32 causeString[cLast];       // translates  PreemptCause --> VMString index

Processes::Processes() {
  idle = false; needsInvalidate = false;
  causeString[ cTerminated           ] = PROCESS_TERMINATED;
  causeString[ cAborted              ] = PROCESS_ABORTED;
  causeString[ cStackOverflowed      ] = STACK_OVERFLOW;
  causeString[ cNonLIFOBlock         ] = NON_LIFO;
  causeString[ cYielded              ] = YIELDED;
  causeString[ cSingleStepped        ] = SINGLESTEPPED;
  causeString[ cFinishedActivation   ] = FINISHEDACTIVATION;
  causeString[ cSignal               ] = SIGNAL;
  causeString[ cLowOnSpace           ] = LOWONSPACE;
  causeString[ cCouldntAllocateStack ] = COULDNTALLOCATESTACK;
}

void Processes::startVMProcess() {
  processList = vmProcess;
  vmProcess->init(first_inst_addr(run_the_VM));
  (void)vmProcess->allocate();
  vmProcess->transfer();
}

static enum { terminateIt, abortIt, discardIt } termMode;
static void terminateProcess(Process* p) {
  if (p == twainsProcess) twainsProcess = NULL;
  if (p != vmProcess) {
    switch (termMode) {
     case terminateIt: p->terminate(); break;
     case abortIt:     p->abort(); break;
     case discardIt:   break;
     default:          fatal1("wrong termMode %ld", termMode);
    }
  }
}

# define PROCESSES_DO_ALL(name, doName)                                       \
                                                                              \
  void Processes::name() {                                                    \
    SignalInterface::block_self_signals(); /* prevent further context switches */   \
    if (currentProcess != vmProcess) {                                        \
      vmProcess->setPC((process_p)&doName);                                   \
      vmProcess->transfer();                                                  \
    } else {                                                                  \
      doName();                                                               \
    }                                                                         \
  }     

static void proc_deleteDead(Process* p) { if (p->state == aborting) delete p; }

# define PROCESSES_DO_NON_WM(mode)                                            \
  termMode = mode;                                                            \
  assert(currentProcess == vmProcess, "executing in wrong context");          \
  /* kill all processes except vmProcess */                                   \
  processes->processesDo(terminateProcess);                                   \
  prevProcess = vmProcess;      /* otherwise prevProcess is invalid */        \


void doDiscardAll() {
  PROCESSES_DO_NON_WM(discardIt);
  processes->processesDo(proc_deleteDead, true); // deallocate dead processes
  // return to prompt
  DiscardStack();
}

void Processes::processesDo(processesDoFn f, bool doItForAll) {
  Process* q;
  for (Process* p = processList; p; p = q) {
    q = p->next;                // because p could be deleted
    assert(p->state != defunct, "shouldn't have defunct processes in list");
    if (doItForAll || p->state != aborting) {
      f(p);
    }
  }
}

void printProcess(Process* p) { p->print(); }

void Processes::print() {
  processesDo(printProcess, true);
  lprintf("\n");
}

static char* pf;
static Process* pfp;
static void proc_frame_for(Process* p) { if (p->contains(pf)) pfp = p; }
Stack* Processes::slowStackFor(void* f) {
  pf = (char*)f;
  pfp = NULL;
  processesDo(proc_frame_for);
  if (pfp == NULL) {
    if (ConversionInProgress) {
      // saved stack frame (can only be printed by VM -- for debugging)
      return currentProcess->stack();
    } else {
      // usually fatal error, but don't fatal here because verify tests for it
      return NULL;
    }
  }
  assert(pfp != currentProcess, "must not return current stack");
  return pfp->stack();
}

static void proc_chainFrames(Process* p)        { p->chainFrames(); }
static void proc_unchainFrames(Process* p)      { p->unchainFrames(); }
static void proc_scav_contents(Process* p)      { p->scavenge_contents(); }
static void proc_gc_mark_contents(Process* p)   { 
  if (p->zombie) p->processObj()->gc_mark(); }
static void proc_gc_unmark_contents(Process* p) { p->gc_unmark_contents(); }
static void proc_set_zombie(Process *p)         { p->zombie= true; }

static enumeration *theEnumeration;
static void proc_enumerate_references(Process* p) {
  if (p != currentProcess) p->stack()->enumerate_references(theEnumeration); }
static void proc_enumerate_families(Process* p) {
  if (p != currentProcess) p->stack()->enumerate_families(theEnumeration); }

static bool proc_verify_result = true;
static void proc_verify(Process* p)             { proc_verify_result &= p->verify(); }
static void proc_switch_pointers(Process* p)    { p->switch_pointers(); }
static FILE* sf;
static void proc_read_snapshot(Process* p)      { p->read_snapshot(sf); }
static void proc_write_snapshot(Process* p)     { p->write_snapshot(sf); }

void Processes::chainFrames()        { processesDo(proc_chainFrames); }
void Processes::unchainFrames()      { processesDo(proc_unchainFrames); }
void Processes::scavenge_contents()  {
  processesDo(proc_scav_contents, true); 
  processesDo(proc_deleteDead, true);   // deallocate dead processes
}

/*
 GC of processes

 This is done in two parts, in order to detect and mark all processes
 which are not reachable from the system roots (`zombies').  We cannot
 reclaim these processes directly during GC because each must be
 abort()ed to:
 1) Run any NLR code associated with the process' stack.
 2) Zap all blocks associated with the process.
 The Self-side code can get a vector of zombies (using _ZombieProcesses)
 after GC, and abort them.

 The current process and the TWAINS process are considered to be
 roots.  Their C-side structs are scanned, which causes the
 corresponding Self process objects to be marked and later scanned.
 In this first phase of scanning, all non-zombie processes
 are found, and the corresponding C structure are scanned.

 In a second phase, all zombie processes are scanned.

 Important points:
  - Each C Process struct must be scanned precisely once.
  - All structs except those of the current and TWAINS processes are reached
    via ProcessMap::gc_mark_contents.

 miw 4/17/95
*/

void Processes::gc_mark_contents()   {
  processesDo(proc_set_zombie, true);
  markingFromZombies= false;
  currentProcess->gc_mark_contents();
  if (twainsProcess) twainsProcess->gc_mark_contents();
}

void Processes::gc_mark_remaining_processes() {
  markingFromZombies= true;
  processesDo(proc_gc_mark_contents, true); // ensure all processes are marked
}

void Processes::gc_unmark_contents() {
  processesDo(proc_gc_unmark_contents, true);
}

static unsigned int nZombies;
static objVectorOop zombiesVec;
static void count_zombie(Process *p) { if (p->zombie) nZombies++; }
static void add_zombie(Process *p) {
  if (p->zombie)
    zombiesVec->obj_at_put(nZombies++, p->processObj()); }

oop zombie_prim() { return processes->zombies(); }

objVectorOop Processes::zombies() {
  // count the zombies
  nZombies= 0;
  processesDo(count_zombie, true);
  // put them in a vector
  zombiesVec= Memory->objVectorObj->cloneSize(nZombies);
  nZombies= 0;
  processesDo(add_zombie, true);
  return zombiesVec;
}

bool Processes::verify() { 
  bool r = true;
  proc_verify_result = r;
  processesDo(proc_verify, true);
  r &= proc_verify_result;
  return r;
}
void Processes::enumerate_references(enumeration *e) {
  theEnumeration = e; processesDo(proc_enumerate_references); }
void Processes::enumerate_families(enumeration *e) {
  theEnumeration = e; processesDo(proc_enumerate_families); }
void Processes::switch_pointers(oop f, oop t) {
  from = f; to = t; processesDo(proc_switch_pointers, true);
}
void Processes::read_snapshot(FILE* f) {
  sf = f;
  processesDo(proc_read_snapshot);
}
void Processes::write_snapshot(FILE* f) {
  sf = f; processesDo(proc_write_snapshot);
}


static void convertProcess(Process* p) { p->convert(); }
static void convert_cont() { processes->convert_cont(); }

void Processes::convert() { 
  if (needsInvalidate) {
    LOG_EVENT("Processes::convert");
    switchToVMStack(::convert_cont);
    needsInvalidate = false;
  }
}

void Processes::convert_cont() {
  ResourceMark rm;
  FlushRegisterWindows();
  Memory->code->chainFrames();
  processesDo(convertProcess);
  Memory->code->unchainFrames();
}

void Process::nonLifoError() {
  currentProcess->state = stopped;
  if (twainsProcess) {
    preemptCause = cNonLIFOBlock;
    twainsProcess->transfer();
    ShouldNotReachHere();
  } else {
    lprintf("\n*** attempt to invoke non-lifo block (block's enclosing scope has returned)! ***\n");
    currentProcess->stack()->print();
    abort();
  }
}


void interruptCheck() {
   
  if (isStackOverflow((char*)currentFrame())) {
    currentProcess->state = stopped;
    if (twainsProcess) {
      preemptCause = cStackOverflowed;
      twainsProcess->transfer();
      ShouldNotReachHere(); // shouldn't restart this process
    } else {
      lprintf("\n*** Stack Overflow!!\n");
      currentProcess->stack()->print();
      Process::abort_process();
    }
  } else if (profilers->hasOverflow()) {
    // A profiler call InterruptedContext::setupPreemptionFromSignal to empty its filled buffer.
    profilers->handleOverflow();
  } else {
    handlePreemption();
  }
  if (currentProcess->isKillingOrDeoptimizing()) {
    // continue pseudo non-local return (e.g. for KillActivations primitive)
    NLRSupport::continue_NLR_into_Self(false);
  }
}


// run a method, can optionally pass in nm
// caller must preserve arguments in situ!

oop Process::runDoItMethod( oop rcv,
                            oop method,
                            oop* args,
                            fint arg_count ) {
  
  assert(Interpret  ||  arg_count <= 1,
         "unimplemented: cannot pass in > 1 arg to compiled code");
  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  nmethod* nm;
  if (!Interpret) { 
    nm = constructDoItMethod( rcv, method );
    if (Trace && WizardMode) {
      lprintf("running do It method 0x%lx\n", (unsigned long) nm);
    }
  }
# endif //  defined(FAST_COMPILER) || defined(SIC_COMPILER)

  oop res = prepare_to_call_self();
  if (res->is_mark())
    return res;

  if (!Interpret) {
#   if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    EventMarker("entering self %d", (void*)nesting);
    res = EnterSelf( rcv, nm->insts(),  arg_count < 1  ?  badOop  : args[0]);
#   else
    ShouldNotReachHere();
#   endif
  }
  else {
    res = interpret( rcv, VMString[DO_IT], NULL, method, rcv, args, arg_count);
  }  
  cleanup_after_calling_self();

  return res;
}


oop Process::prepare_to_call_self() {
  if (    PendingSelfSignals::keyboard_signals() != 0
      &&  this == vmProcess 
      &&  nesting == 0) {
    // ^C pressed at the prompt - ignore
    PendingSelfSignals::reset_keyboard_signals();
  }
  if (hadStackOverflow()) {
    if (this == vmProcess && nesting == 0) {
      // clear the stack overflow indicator
      resetStackOverflow();
      state = ready;
    } else {
      // stack overflow in C, e.g. during parsing
      return ErrorCodes::vmString_prim_error(STACKOVERFLOWERROR);
    }
  }
  if (!PendingSelfSignals::are_any_pending() && !preemptionPending()) {
    clearPreemption();
  } else {
    // should immediately preempt
    setupPreemption();
  }
  killVFrameOopsAndSetWatermark(NULL);
  nesting++;
  
  // in case mac itimer is pending, but it zapped max sp of
  //  wrong process
  currentProcess->setupPreemption();

  return smiOop_zero;
}


void Process::cleanup_after_calling_self() {
  nesting--;
  killVFrameOopsAndSetWatermark(NULL);
  if ( nesting == 0  &&  this == vmProcess ) {

    // return to the VM prompt; make sure that everything is consistent
    // (could have been aborted)

    assert(twainsProcess == NULL, "shouldn't have twains process");
    NLRSupport::reset_have_NLR_through_C();
    SignalBlocker sb;
    processSemaphore = false;
    SignalInterface::unblock_self_signals();
    vmProcess->state = ready;
    assert(isClean(), "should be clean now");
    SignalInterface::flush_input_after_ctrl_c();
  }
  patchForSingleStepping();
}


void Process::cleanup_after_eval_prim(oop res) {
  if (NLRSupport::have_NLR_through_C()) {
    // continue non-local return
    NLRSupport::continue_NLR_into_Self(false);
  } 
  else if (res->is_mark()) {
    if (res == badOop) {
      // syntax error - abort process
      currentProcess->abort();
    } 
    else {
      // return error indication
      if (res == ErrorCodes::vmString_prim_error(STACKOVERFLOWERROR)) {
        currentProcess->stack()->mark();        // repair stack mark
      }
    }
  } 
  else {
    // normal termination
  }
}


// A process can temporarily execute on the Unix stack (e.g. for compiles).

# define SWITCH_TO_VM_STACK(header, call1, call2)                             \
  header {                                                                    \
    frame* f = currentFrame();                                                \
    if (isOnVMStack(f)) {                                             \
      call1;                                                                  \
    } else {                                                                  \
      call2;                                                                  \
    }                                                                         \
  }

SWITCH_TO_VM_STACK(void switchToVMStack(doFn continuation),
                   continuation(),
                   SwitchStack0(first_inst_addr(continuation), vmStackEnd))
                   
SWITCH_TO_VM_STACK(oop switchToVMStack(fntype continuation, void* arg1),
                   return continuation(arg1),
                   return (oop)SwitchStack1(first_inst_addr(continuation),
                                            vmStackEnd, arg1))
                                            
SWITCH_TO_VM_STACK(
  nmethod* switchToVMStack(
    nmethod *continuation(compilingLookup*),
    compilingLookup* L ),
  return continuation(L),
  return (nmethod*)SwitchStack1(first_inst_addr(continuation),
                                vmStackEnd, (void*)L))
                                            
SWITCH_TO_VM_STACK(
  nmethod* switchToVMStack(oop receiver, oop method,
                           constructDoItMethodContinuation continuation),
  return continuation(receiver, method),
  return (nmethod*)SwitchStack2(first_inst_addr(continuation),
                                vmStackEnd, receiver, method))

SWITCH_TO_VM_STACK(
  void switchToVMStack_intSend( simpleLookup* L, int32 arg_count,
                                intLookupContinuation continuation),
  continuation( L, arg_count ),
  SwitchStack2( first_inst_addr(continuation),
                vmStackEnd,
                L,
                (void*)arg_count ) )
     
                   
PROCESSES_DO_ALL(  discardAll, doDiscardAll)


void Process::prepare_to_return_to_self_after_conversion(
                frame* new_last_self_frame, 
                bool& restartSend,
                bool& wasUncommon,
                int32*& uncPC) {
  killVFrameOopsAndSetWatermark(new_last_self_frame);
  wasUncommon = currentProcess->isUncommon();
  // restartAfterConversion is only set by recompilation
  restartSend = wasUncommon || restartAfterConversion;
  uncPC = (int32*)uncommonPC();
  resetUncommon();
  restartAfterConversion = false;
  patchForSingleStepping(new_last_self_frame);
  
  frame* kill_target = new_last_self_frame;
    
  # if TARGET_ARCH == PPC_ARCH // only works for PPC -- dmu 1/03
    assert(!isKilling()  ||  killVF()->as_vframe()->fr >= kill_target,
           "passed kill_target");  
  # endif
  if (traceV && isKilling() && killVF()->as_vframe()->fr == kill_target)
    lprintf("*** stopping senseless killing of innocent frames, kill_target = 0x%x\n", kill_target);
}

bool Process::is_done_with_killing_or_deoptimizing(frame* dest_self_fr) {
  return  isDeoptimizing() // only used for goto bytecode primitive (only last stack frame)
  ||      (isKilling()  &&  killVF()->as_vframe()->fr == dest_self_fr); // for killActivationsUpTo prim

}

/* Sun-$Revision: 30.22 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "shell.hh"
# include "_shell.cpp.incl"

oop evalExpressions(Scanner* scanner) {
  // evaluate expressions until scanner is at EOF or an error occurs
  oop res = NULL;
  fint line, len;
  const char* source;
  while (!res->is_mark() && !scanner->is_done()) {
    // This resource mark must be inside the loop otherwise long files can 
    // cause the resource area to become huge -miw, 12/5/94
    ResourceMark rm;
    Parser parser(scanner, false);
    scanner->commentList = new TokenList(100);
    Expr* e = parser.readExpr(line, source, len);
    if (e &&
        (AddFileAnnotationsToSlots ||
         (ConvertCommentsIntoAnnotations && 
          scanner->commentList->length() > 0))
      ) {
      e->addCommentAnnotations(scanner);
    }
    
    if (e) {
      preservedObj x(e);                // for GenByteCodes
      if (ScavengeInPrimitives && Memory->needs_scavenge()) {
        Memory->scavenge();
      }
      
      ByteCode b(true);
      slotsOop evalMethod;
      if (! e->GenByteCodes(&b, 0) 
      ||  NLRSupport::have_NLR_through_C() )
        res = badOop;
      else if ( !b.Finish(scanner->fileName(), line, source, len)) {
        e->ErrorMessage(b.errorMessage);
        res = badOop;
      }
      else
        evalMethod = create_outerMethod(EMPTY, &b);
      if (!res->is_mark()) {
        res = currentProcess->runDoItMethod(Memory->lobbyObj, evalMethod);
        if (NLRSupport::have_NLR_through_C()) break;             // let NLR go through
      }
    } else if (!parser.noParseError()) {
      res = badOop;
    } else {
      // empty line
    }
    // Tokens are allocated in the resource area, which is about to be freed.
    // Reset the list so that we don't attempt to use any next time.
    scanner->resetTokenList();
  }
  
  if (currentProcess->hadStackOverflow()) {
    res = ErrorCodes::vmString_prim_error(STACKOVERFLOWERROR);
  }
  return res;
}

oop eval(const char* expression, const char* fn) {
  StringScanner scanner(expression, strlen(expression), fn);
  return evalExpressions(&scanner);
}

static char* spyLogFile= NULL;
char* startUpSelfFile= NULL;

const char **prog_argv;
int    prog_argc;



static void processArguments(int argc, const char *argv[]) {

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
# else
  Interpret = true; // no compiler!
# endif
  

  // Some implementations of getopt (I'm looking at you, GNU) rearrange argv for their own purposes.
  // So we give getopt a copy of argv so we can later expose the real original argv to 
  // Self via _CommandLine
  // rca 2 Feb 2014
  char **copy_of_argv = (char **) malloc( sizeof(char *) * argc );
  memcpy(copy_of_argv, argv, sizeof(char *) * argc);
  
  char c;
  fint opt_i;
  OS::opterr= 0; // disable printed warning about unrecognized options
  while(opt_i= OS::optind,
        (c = OS::getopt(argc, (char* const*)copy_of_argv, "Ff:hl:prtcs:woa")) != (char)-1) {
    if (strlen(argv[opt_i]) != 2) continue; // ignore multi-character options
    switch(c) {
    case 'F':
      okToUseCodeFromSnapshot= false;
      lprintf("not okToUseCodeFromSnapshot\n");
      break;
    case 'f':
      // file to read after snapshot has been loaded (for debugging)
      startUpSelfFile= OS::optarg;
      break;
    case 'h': // help
      lprintf("%s: usage: %s [-f filename] [-h] [-s snapshot] ...\n", argv[0], argv[0]);
      lprintf("Options:\n");
      lprintf("  -f filename\tReads filename (Self source) immediately after startup\n");
      lprintf("  -h\t\tPrints this message\n");
      lprintf("  -p\t\tDon't do `snapshotAction postRead' after reading snapshot\n");
      lprintf("  -s snapshot\tReads initial world from snapshot\n");
      lprintf("  -w\tSuppress warnings about optimized code being discarded\n");
      lprintf("For debugging use only:\n");
      lprintf("  -F\t\tDiscards saved code from snapshot\n");
      lprintf("  -l logfile\twrite spy log to logfile\n");
      lprintf("  -r\t\tDisable real timer\n");
      lprintf("  -t\t\tDisable all timers\n");
      lprintf("  -c\t\tUse real timer instead of CPU timer (for OS X)\n");
      lprintf("  -o\t\tOversample the timers (Run them 10x faster to flush out bugs)\n");
      lprintf("  -a\t\tTest the Assembler (added for Intel)\n");
      lprintf("Other command line switches may be interpreted by the Self world\n");
      ::exit(0);
      break;
     case 'r':
      // dont use real timer (useful for gdb)
      IntervalTimer::dont_use_real_timer = true;
      lprintf("Setting IntervalTimer::dont_use_real_timer to %s\n",
              IntervalTimer::dont_use_real_timer ? "true" : "false");
      break;
     case 't':
      // dont use any timer (needed for tracing)
      IntervalTimer::dont_use_any_timer = true;
      lprintf("Setting IntervalTimer::dont_use_any_timer to %s\n",
              IntervalTimer::dont_use_any_timer ? "true" : "false");
      break;
     case 'c':
      // dont use CPU timer (needed for X under OS X)
      // sometimes, process dies with SIGVTALRM when you start X -- dmu 8/1
      // Put in inversion for experimentation.
      IntervalTimer::use_real_instead_of_cpu_timer = !IntervalTimer::use_real_instead_of_cpu_timer;
      lprintf("Setting IntervalTimer::use_real_instead_of_cpu_timer to %s\n",
              IntervalTimer::use_real_instead_of_cpu_timer ? "true" : "false");
      break;
     case 'l':
      // write spy log to a log file 
      spyLogFile= OS::optarg;
      break;
     case 's':
      // default world
      WorldName= OS::optarg;
      break;
     case 'p':
      // turn off post-read snapshot evaluation
      postReadSnapshot= false;
      break;
     case 'w':
      noCodeWarnings= true;
      break;
     case 'o':
      IntervalTimer::oversample_rate = 10;
      break;
     case 'a':
      Assembler::do_the_tests = true;
      break;
     }
  }
  // save remaining args for Self-level access
  prog_argc= argc;
  prog_argv= argv;
  
  // clean up argv copy
  free(copy_of_argv);
}

void abortSelf() {
  // ^C was hit at the prompt or while _RunScripting
  lprintf("\n*user abort*\n");
  if (currentProcess->inSelf())
    currentProcess->stack()->print();
  VMScanner->reset();
  VMScanner->discardInput();
  Indent = 0;
  if (currentProcess->nesting > 0) currentProcess->abort();
}


void run_the_VM() {
  assert(! bootstrapping, "don't get screwed again by this");
  Memory->scavenge();
  SignalInterface::initialize();
  initHProfiler();
  
  IntervalTimer::start_all();

  ResourceMark mark;

  InteractiveScanner scanner;
  VMScanner = &scanner;

  resetTerminal();

  if (startUpSelfFile) { 
    ResourceMark m;
    FileScanner scnr(startUpSelfFile);
    if (scnr.fileError) {
      fatalNoMenu1("Couldn't read file %s!\n\n", startUpSelfFile);
    } else {
      lprintf("Reading %s...", scnr.fileName());
      (void) evalExpressions(&scnr);
      lprintf("done\n");
    }
  }
  
  // After reading a snapshot we need to evaluate the snapshot actions.
  if (postReadSnapshot) {
    postReadSnapshot = false;
    eval("snapshotAction postRead", "<postRead Snapshot>");
  }

  // The read-eval-print loop
  for (;;) {
    ResourceMark m;
    fint line, len;
    const char* source = NULL;
    Parser parser(VMScanner, false);
    resetTerminal();
    VMScanner->start_line("VM# ");
    processes->idle = true;
    if (VMScanner->is_done())
      break;
    processes->idle = false;
   
    Expr* e = parser.readExpr(line, source, len);  // dont fail
    if (e) {
      preservedObj x(e);
      oop res = e->Eval(true, false, true);
      assert(currentProcess->isClean(), "vm process should be clean now");
      if (res == badOop) {
        VMScanner->reset();
        clearerr(stdin);          // don't get hung on ^D
      }
    }
    if (Memory->needs_scavenge()) Memory->scavenge();
  }

  lprintf("\n");
# if TARGET_IS_PROFILED
    lprintf("Writing profile statistics...\n");
# endif
  clearerr(stdin);
  OS::handle_suspend_and_resume(true);            // make sure stdin is in normal mode
  OS::terminate(0);
}


# if TARGET_OS_FAMLIY == UNIX_FAMILY
extern "C" void __main() {}     // so we can link with the standard linker
# endif


# if COCOA_EXP
#   define main old_main  // the "real" main is in main.m
# endif

int main(int argc, char *argv[]) {
  // On Mac, the first printf does an InstallConsole which sets AE handlers
  // so init os first to clear SIOUXSettings.standalone to fix this
  OS::init();
  OS::set_args(argc, argv);

  /* Do not buffer stdin. If stdin is buffered, input will disappear at the
     Self level. The reason is that the VM will use fread(stdin, ...) which 
     does a read-ahead on the descriptor for stdin (0). Self, however, reads 
     directly from the descriptor, so any characters that were read ahead 
     in the fread(stdin, ...) call will not show up at the Self level. */

  OS::do_not_buffer(stdin);

  /* Set the maximum core size to be large, so that core dumps are produced
     when Self crashes.  (Under Mac OS X, the default core size is 0, so
     core dumps weren't produced. -mabdelmalek 4/28/03 */
  OS::enable_core_dumps();

  processArguments(argc, (const char **)argv);
  
  TrackCHeapInMonitor::reset();
  init_globals();
  set_flags_for_platform();
  bootstrapping = false; 

# ifdef DYNLINK_SUPPORTED
    initDynLinker(argc, (const char **)argv);
# endif

  if (spyLogFile) TheSpy->activate(spyLogFile);

# ifdef EXPERIMENT_WITH_APPLE_EVENTS
  extern void handle_dropped_snapshot();
  handle_dropped_snapshot();
# endif  
  
  vmProcess = new Process(NULL, SelfStackLimit);
  processes->startVMProcess();
  ShouldNotReachHere();
  return 0;
}
/* Sun-$Revision: 30.20 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "recompile.hh"
# include "_recompile.cpp.incl"

nmethod* recompilee = NULL;

fint nstages;
fint* compilers;        
static fint* recompileLimits;
static smi* compileCounts;

static const fint MaxStackToLookAt = 100;       // limits depth of stack search
static const fint TicksPerSec = 50;

static SlidingAverage* compileAvg;
static void compileAvgTracker() {
  compileAvg->add(activeCompiler ? 100 / TicksPerSec : 0);
}

void recompile_init() {
  # if defined(SIC_COMPILER)
    # if !defined(FAST_COMPILER)
      # error unsupported: must have nic if have sic
    # endif
    nstages = 2;
  # elif defined(FAST_COMPILER)
    nstages = 1;
  # else
    nstages = 0;
  # endif
  
  compilers=       NEW_C_HEAP_ARRAY(fint, nstages);
  compileCounts=   NEW_C_HEAP_ARRAY( smi, nstages);
  recompileLimits= NEW_C_HEAP_ARRAY(fint, max(0, nstages - 1));
  
  switch (nstages) {
   case 0:  break;
   case 1:
    compilers[0] = NIC;
    break;
   case 2:
    compilers[0] = NIC; compilers[1] = SIC;
    recompileLimits[0] = 10 * K;
    break;
 
  default: fatal("what compiler?");
  }
  for (fint i= 0; i < nstages; i++)
    compileCounts[i]= 0;
  compileAvg = new SlidingAverage(TicksPerSec);
  IntervalTimer::CPU_timer()->enroll_async(TicksPerSec, compileAvgTracker); 
}


# if ( TARGET_ARCH ==  PPC_ARCH  \
||     TARGET_ARCH == I386_ARCH )
  oop DIRecompile_stub(...) { fatal("no DI recompilation"); return NULL; }
# endif


# if defined(SIC_COMPILER)


// this code has grown without reorganization for a while and should probably
// be cleaned up

AbstractRecompilation* theRecompilation;
bool RecompilationInProgress = false;


bool allowedToRecompile() {
  return compileAvg->sum() < MaxCompilationPercentage;
}

// The following funcs are an ad-hoc hack defining the recompilation
// stages; they should be cleaned up.

bool needRecompileCode(fint level) {
  // Called by the compiler to find out if it should generate a check
  // for recompilation.
  assert(level >= 0, "invalid level");
  return level < nstages - 1;
}


fint recompileLimit(fint level) {
  return recompileLimits[min(level, nstages - 2)];
}


fint recompileLevel() {
  return recompilee ? recompilee->level() : -1;
}


fint currentCompiler() {
  if (recompilee == NULL) return compilers[0];

  fint lev = recompilee->level() + 1;
  assert(lev >= 0 && lev <= MaxRecompilationLevels, "invalid level");
  // NB: can be max  because max-level nmethod could get uncommon trap
  lev =  min(lev, nstages - 1);
  if (lev < 0) {
    warning1("incorrect recompilee level %ld", lev);
    lev = min(nstages - 1, 1);  // fix this
  }
  return compilers[lev];
}


void countCompilation() {
  if (recompilee) {
    compileCounts[min(recompilee->level() + 1, nstages - 1)]++;
  } else {
    compileCounts[0]++;
  }
}



class Recompilation;

//----------------------------------------------------------------------

// nvframes are vframes decorated with some extra state; they're canonicalized
// via the recompilation's stack inst var
class nvframe : public compiled_vframe {
 public:
  fint depth;           // distance from top of stack (0 = TOS)
  Recompilation* r;     // my recompilation
 protected:
  nvframe(compiled_vframe* vf, Recompilation* rec, fint n);
 public:
  nvframe(frame* f,   Recompilation* rec, fint n = -1);
  nvframe* sender();
  abstract_vframe* parent();
  abstract_vframe* top();
  bool is_n() { return true; }

# if  GENERATE_DEBUGGING_AIDS
  nvframe* callee();
  bool isParentOf(nvframe* other);
# endif
};

typedef BoundedListTemplate<nvframe*> NVFrameBList;


class RecompileFrame : public ResourceObj {
 public:
  frame* fr;
  nmethod* nm;                  // fr->code()
  sendDesc* sd;                 // fr->send_desc()
  bool monomorphic;             // sd is monomorphic
  fint orig_invocations;        // est. total invocation count of nm
  fint orig_sends;              // nm->nsends()
  fint invocations;             // current invocation est. (for this frame)
  fint sends;                   // sends caused by this frame
  fint cumulSends;              // sends + sends of nested blocks
  bool looping;                 // contains loop (containing current send)

  RecompileFrame(frame* fr);
  bool adjust(RecompileFrame* caller, RecompileFrame* callee);
  void computeSends();
  void print();
};


typedef BoundedListTemplate<RecompileFrame*> RecompileFrameBList;


//----------------------------------------------------------------------


class Recompilation: public AbstractRecompilation {
 public:
  AbstractRecompilation* prevRecompilation;
  bool recursive;               
  frame* lastFrame;             // last Self frame on VM stack (see fixupStack)
  frame* tripFrame;             // frame calling tripNM
  frame* copiedFrame;           // copy of tripFrame (during conversion)
  frame* callerFrame;           // frame calling recompilee
  sendDesc* sd;                 // calling sendDesc
  oop receiver;                 // recompilee's receiver
  NVFrameBList  stack;
  fint rdepth;                  // vf index of recompilee 
  NCodeBase* thing;             // whoever called us (stub or NIC nmethod)
  bool calledFromStub;          // thing->isCountStub()
  nmethod* tripNM;              // nmethod tripping its counter
  nmethod* recompilee;          // nmethod to recompile
  nmethod* newNM;               // recompiled nmethod
  nmethod* prevNM;              // optimized nmethod of previous recompilation
                                // (top-down recompiles)
  char *restartAddr;            // where to resume execution (possibly NULL)
  frame* firstReplaced;         // first (most recent) frame (partly) replaced
                                // by newNM
  frame* lastReplaced;          // last (oldest) frame replaced (at least
                                // partially) by newNM
  frame* origFirstReplaced;     // same as first/lastReplaced,
  frame* origLastReplaced;      // but original frame pointers (not copied)
  frame* block_scope_of_origFirstReplaced; // must cache these
  frame* block_scope_of_origLastReplaced;  // because need them after overwriting
                                           // frame link
  fint nreplaced;               // number of frames completely replaced by
                                // newNM (0 = at least one vframe in tripNM
                                // is not in newNM)
  fint nremaining;              // # of *vframes* not replaced (on-stack repl.)
  char* newSP;                  // SP after on-stack replacement
  char* newPC;                  // continuation PC after on-stack repl.

# if TARGET_ARCH == SPARC_ARCH
  // I'm in the process of porting the SIC to PPC.  I haven't got around
  // to porting this file yet.  For now, I just enclose Sparc-specific
  // code with "# if TARGET_ARCH == SPARCH_ARCH".
  // Once the SIC port is completed, platform-dependent versions of this file
  // will be created. -mabdelmalek 10/02
  sparc_sp* newFramePiece;      // part of frame with newNM = newNM block home
# endif

  Recompilation(frame* f, sendDesc* s, oop r, frame* lf, nmethod* rc,
                frame* tf, bool rec = false, nmethod* prev = NULL);
    
  void doit(char* pc);
  void init(char* pc);
  nmethod* recompile();
  void checkEffectiveness(nmethod* oldNM, nmethod* newNM);
  bool shouldRecompile(nmethod* nm, frame* caller);
  bool shouldNotRecompile(nmethod* nm, frame* caller);
  bool mustNotRecompile(nmethod* nm, frame* caller);
  void doShowStack(fint howMany = 10);
  void setNewTarget(nvframe* nvf);
  void find_break_frame(RecompileFrameBList frames);
  void compute_better_counts(RecompileFrameBList frames);
  void findRecompilee(nmethod* n);
  void findTopRecompilee();
  bool findRecompileeHome(nvframe* vf);
  nvframe* nvframeFromBlock(blockOop block, nvframe* thisFrame);
  void checkForBlockArgs();
  void tryRecompileCaller(nmethod* nm, fint limit, bool blocksAreOK = true);
  void getVScopes();
  const char* replaceOnStack();
  void replaceFrames(fint n, fint diff);
  bool pushFrame(frame* copy);
  void fillValues(frame* newFR);
  void killBlocks();
  bool checkForRemappedBlocks();
  bool checkForActivationMirrors();
  void handleRemappedBlocks();
  void discardNMethods();
  void fixStack();
  void printNewFrames(frame* newFr);
  void noRecompilee()     { recompilee= NULL; }
  bool recompiling_trip() { return rdepth == -1; }
  void finalize() {
    assert(theRecompilation == this, "forgot to call finalize on prev.");
    theRecompilation = prevRecompilation;
  }
};


//----------------------------------------------------------------------


nvframe::nvframe(frame* f, Recompilation* rec, fint n) 
 : compiled_vframe(f) {
  depth = n; r = rec;
  if (n == -1) {
    // find depth
    fint i;
    for (i = r->stack.length() - 1;
         i >= 0 && r->stack.nth(i)->fr > fr;
         i--) ;
    for ( ; i >= 0; i--) {
      nvframe* vf = r->stack.nth(i);
      if (vf->EQ(this)) {
        depth = i;              // found it
        break;
      }
      if (vf->fr < fr) break;   // no point looking further
    }
      
    if (depth == -1) {
      // didn't find a vframe; create new ones
      nvframe* vf;
      if (r->stack.isEmpty()) {
        vf = new nvframe(currentProcess->last_self_frame(true), r, 0);
      } else {
        vf = r->stack.top();
      }
      for (i = r->stack.length() - 1; !vf->EQ(this); i++, vf = vf->sender()) ;
      depth = i;
    }
  }
  assert(depth >= 0, "should have depth");
  if (depth == r->stack.length()) {
    r->stack.append(this);
  } else {
    assert(depth < r->stack.length(), "oops");
  }
  assert(this->EQ(r->stack.nth(depth)), "bad depth");
}


// For vf->bci() below see comment in conversion.cpp copyVFrame()

nvframe::nvframe(compiled_vframe* vf, Recompilation* rec, fint n)
  : compiled_vframe(vf->fr, vf->code, vf->desc, vf->bci()) {
  depth = n; r = rec;
  assert(depth == r->stack.length(), "should be in sequence");
  r->stack.append(this);
}


nvframe* nvframe::sender() {
  if (depth < r->stack.length() - 1) {
    return r->stack.nth(depth + 1);
  } else {
    compiled_vframe* vf = compiled_vframe::sender()->as_compiled();
    return vf ? new nvframe(vf, r, depth+1) : NULL;
  }
}


abstract_vframe* nvframe::parent() {
  compiled_vframe* vf = compiled_vframe::parent()->as_compiled();
  if (vf == NULL) return NULL;
  if (!currentProcess->stack()->contains((char*)vf->fr)) return NULL;
  nvframe* p;
  for (p = sender(); !p->EQ(vf); p = p->sender()) ;
  return p;
}


abstract_vframe* nvframe::top() {
  nvframe* topVF = this;
  while (!topVF->is_top()) topVF = topVF->sender();
  return topVF;
}


# if  GENERATE_DEBUGGING_AIDS
nvframe* nvframe::callee() {
  if (depth > 0) {
    return r->stack.nth(depth - 1);
  } else {
    return NULL;
  }
}


bool nvframe::isParentOf(nvframe* other) {
  do {
    other = other->parent()->as_n();
  } while (other && other->depth < depth);
  assert(!other || other->depth != depth || other == this,
         "nvframe not canonicalized");
  return other == this;
}
#endif


//----------------------------------------------------------------------

RecompileFrame::RecompileFrame(frame* f) {
  fr= f;
  nm= f->code();
  sd= f->send_desc();
  monomorphic= sd == NULL  ||  sd->pic() == NULL;
  orig_invocations= invocations= nm->invocationCount();
  orig_sends= sends= nm->nsends();
  cumulSends= 0;
  looping= false;

  for (abstract_vframe* vf = new_vframe(f);
       vf && vf->fr == f;
       vf = vf->sender()) {
    Map* m= vf->method()->map();
    assert(m->has_code(), "should be a method");
    if (((methodMap*)m)->containsLoop()) {
      looping= true;
      break;
    }
  }

  f= f->selfSender();
  if (f == NULL)
    return;

  // try to get better invocation count from count stub
  sendDesc* s= f->send_desc();
  CountStub* cs= s->countStub();
  if (cs) {
    // NB: s->get_method(0) may be != nm because nm was replaced by
    // recompiled nm, but invocation count is still correct
    invocations= cs->count();
    return;
  }

  CacheStub* pic= s->pic();
  if (pic == NULL)
    return;

  for (fint i= pic->arity() - 1; i >= 0; i--) {
    if (pic->get_method(i) == nm && (cs= pic->countStub(i))) {
      // assert(cs->count() <= invocations + 100, "invocations too small");
      // NB: assertion will fail because e.g. a new NIC nmethod may be 
      // linked into an existing sd)
      invocations= cs->count();
      return;
    }
  }
}


void RecompileFrame::print() {
  lprintf("*(RecompileFrame*)%#lx %-20.20s %s%s%s %5ld/%5ldi %5ld/%5ld/%6ld\n",
          this,
          selector_string(nm->key.selector),
          nm->compiler() == NIC ? " nic " : " ",
          looping ? "L" : " ",
          monomorphic ? "M" : " ",
          (void*)orig_invocations,
          (void*)invocations,
          (void*)orig_sends,
          (void*)sends,
          (void*)cumulSends);
}


static inline fint my_min(fint i, fint j) {
  // make sure that bogus "0 invocations" for optimized nmethods don't screw up
  if (i) {
    return j ? min(i,j) : i;
  } else {
    return j;
  }
}


bool RecompileFrame::adjust(RecompileFrame* caller, RecompileFrame* callee) {
  // try to make invoc. estimates better based on caller/callee info
  // return true if estimate changed
  fint old_inv = invocations;
  if (nm->compiler() == NIC && monomorphic && callee) {
    invocations = my_min(invocations, callee->invocations);
  }
  if (caller && !caller->looping) {
    invocations = my_min(invocations, caller->invocations);
  }
  return old_inv != invocations;
}


void RecompileFrame::computeSends() {
  // compute estimate for # of sends
  if (orig_sends) {
    fint orig = max(invocations, orig_invocations); // could be zero otherwise
    sends = (int)(orig_sends * (float)invocations / orig);
  } else {
    sends = invocations;                // wild guess
  }
}


//----------------------------------------------------------------------

bool showStack = false;


inline void printNM(nmethod* nm) {
  lprintf("%#lx %s%s \"%-25.25s\":%6ld snds %6ld inv %3ld clr\n",
          nm,
          nm->isYoung() ? "Y" : " ",
          nm->compiler() == NIC ? "U" : "S",
          selector_string(nm->key.selector),
          (void*)nm->nsends(),
          (void*)nm->invocationCount(),
          (void*)nm->ncallers());
}


Recompilation::Recompilation(frame* f, sendDesc* s, oop r, frame* lf,
                             nmethod* rc, frame* tf, bool rec, nmethod* prev)
  : stack(100) {
  callerFrame = f; sd = s; receiver = r; lastFrame = lf; 
  recompilee = rc; tripFrame = tf; recursive = rec; prevNM = prev;
  rdepth = -1; // -1 indicates recompilee==tripNM, and tripNM was in prologue
               // or in stub
  thing = NULL;
  tripNM = newNM = NULL;
  newSP = newPC = NULL;
  copiedFrame = NULL;
  firstReplaced = lastReplaced = NULL;
  nreplaced = nremaining = -1; 
  prevRecompilation = theRecompilation;
  theRecompilation = this;
  while (!tripFrame->is_self_frame())
    tripFrame = tripFrame->sender();
}


bool Recompilation::shouldNotRecompile(nmethod* nm, frame* caller) {
  return mustNotRecompile(nm, caller) || nm->shouldNotRecompile();
}


bool Recompilation::mustNotRecompile(nmethod* nm, frame* caller) {
  // answers true if nm must not be recompiled (for non-performance-related
  // reasons)
  if (!caller->is_self_frame() || caller->send_desc()->isPrimCall())
    return true;  // can't recompile yet if invoked by primitive
  return nm->mustNotRecompile();
}


bool Recompilation::shouldRecompile(nmethod* nm, frame* caller) {
  assert(! currentProcess->isUncommon(), "shouldn't call");
  return !shouldNotRecompile(nm, caller) && nm->shouldRecompile();
}


void Recompilation::setNewTarget(nvframe *nvf) {
  recompilee = nvf->code;
  receiver = nvf->receiver();
  callerFrame = nvf->fr->sender();
  rdepth = nvf->depth;
  assert(!callerFrame->send_desc()->isPrimCall(), "must be a send");
  assert(!mustNotRecompile(recompilee, callerFrame),
         "must not recompile this nmethod");
}


bool Recompilation::findRecompileeHome(nvframe* vf) {
  // find the first enclosing scope which needs recompilation
  nvframe* top;
  for (top = vf;
       top && !shouldRecompile(top->code, top->fr->sender());
       top = top->parent()->as_n()) 
    ;
  if (top == NULL) {
    // enclosing scopes are all optimized
    return false;
  }
  // find top vframe within this frame
  while (!top->is_top())
    top = top->sender();
  frame* caller = top->fr->sender();
  if (mustNotRecompile(top->code, caller)) {
    // do not optimize
    return false;
  }
  assert(shouldRecompile(top->code, caller), "just checkin'");
  setNewTarget(top);
  return true;
}


void Recompilation::doShowStack(fint howMany) {
  lprintf("\n");
  printNM(tripNM);
  frame* f = currentProcess->last_self_frame(true);
  for (fint i = 0; i < howMany && f; i++, f = f->selfSender()) {
    printNM(f->code());
  }
}


void Recompilation::findRecompilee(nmethod* nm) {
  // nm has tripped its counter; try to figure out whom to recompile
  // updates receiver
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      char buf[sizeof(Recompilation)];
      Recompilation* oldRecomp = (Recompilation*)buf;
      *oldRecomp = *this;
    }
# endif
  recompilee = nm;
  fint nsends = fint(9 * recompileLimits[min(nm->level(), nstages - 2)] / 10);
  fint prev_rdepth;

  // try to find a recompilee; each iteration may walk further up on the stack
  do {
    if (PrintRecompilation2) { lprintf("*trying "); printNM(recompilee); }
    prev_rdepth = rdepth;
    checkForBlockArgs();
    if (prev_rdepth == rdepth) {
      tryRecompileCaller(recompilee, nsends);
    }
  } while (prev_rdepth != rdepth && rdepth < MaxStackToLookAt);

  if (recompiling_trip() && !shouldRecompile(nm, callerFrame)) {
    noRecompilee();
  } 
  else if ( recompilee  &&  recompilee->ncallers() == 1)  {
    // try going up one more step, i.e. inline often-called nmethod into caller
    if (PrintRecompilation2) { lprintf("*trying "); printNM(recompilee); }
    tryRecompileCaller(recompilee, 1, receiver->is_block());
  }
}


nvframe* Recompilation::nvframeFromBlock(blockOop block, nvframe* thisFrame) {
  frame* bs = block->scope(true);
  // NB: block may be dead (receiver is block, but it may not be executing
  // the block method) or from a different process
  if (bs == NULL || !currentProcess->contains(bs))
    return NULL;
  nvframe* f = new nvframe(bs->home_frame_of_block_scope(), this);
  if (f  &&  f->depth - thisFrame->depth > MaxRecompilationDepth)
    return NULL;
  else
    return f;
}


void Recompilation::checkForBlockArgs() {
  // check if the recompilee has any block arguments; and go up to the
  // block's home frame if it seems worth it
  if (callerFrame == NULL || !callerFrame->is_self_frame())
    return;
  nvframe* blockFrame = NULL;
  nvframe* caller = new nvframe(callerFrame, this);
  if (receiver->is_block())
    blockFrame = nvframeFromBlock((blockOop)receiver, caller);
  if (blockFrame == NULL) {
    // look at the args.  NB: get info from caller, because callee may not
    // exist (is in prologue)
    oop* exprStack;
    smi len;
    caller->get_expr_stack(exprStack, len, true);
    methodMap* mm = caller->desc->method_map();
    stringOop selector = mm->get_selector_at(caller->bci());
    const char* sel = selector_string(selector);
    fint nargs = str_arg_count(sel);
    for (fint n = 0; n < nargs; n++) {
      // search for a "promising" block arg
      oop arg = exprStack[len - nargs + n];
      if (arg->is_block()) {
        blockFrame = nvframeFromBlock((blockOop)arg, caller);
        if (blockFrame) {
          if (PrintRecompilation2)
            lprintf("*found block arg %ld in %s\n", (void*)(n + 1), sel);
          break;
        }
      }
    }
  }

  if (blockFrame && findRecompileeHome(blockFrame)) {
    // optimize block's parent
    if (PrintRecompilation2) lprintf("   (going up lexical link)\n");
  }
}


void Recompilation::tryRecompileCaller(nmethod* nm, fint limit,
                                       bool blocksAreOK) {
  // try to find out whether n's caller should be recompiled
  frame* callerFrame2 = callerFrame->sender();
  if (!callerFrame2->is_self_frame()) {
    // no caller
    return;
  }
  nmethod* caller = callerFrame->code();
  if (caller->isDI()) {
    if (PrintRecompilation2) lprintf("   (not going up - DI)\n");
    return;
  }
  if (!blocksAreOK) {
    nvframe* top = (new nvframe(callerFrame, this))->top()->as_n();
    if (top->receiver()->is_block()) return;
  }
# ifdef SIC_COMPILER
    if (caller->compiler() == SIC) {
      sendDesc* s = callerFrame->send_desc();
      if (s->isUninlinable()) {
        // the SIC decided this send wasn't inlinable
        if (PrintRecompilation2) lprintf("  (not going up - not inlinable)\n");
        return;
      }
    }
# endif
  fint nsends = caller->nsends();
  if (   (nsends >= limit || nm->isTiny())
      && !shouldNotRecompile(caller, callerFrame2)) {
    nvframe* top = (new nvframe(callerFrame, this))->top()->as_n();
    setNewTarget(top);
  }
}


void Recompilation::compute_better_counts(RecompileFrameBList frames) {
  fint i;
  const fint maxfr= frames.length() - 1;
  for (i= maxfr; i >= 0;  ) {
    RecompileFrame* caller= i < maxfr ? frames.nth(i + 1) : NULL;
    RecompileFrame* callee= i > 0     ? frames.nth(i - 1) : NULL;
    if (frames.nth(i)->adjust(caller, callee) && i < maxfr) {
      i++;
    } 
    else {
      i--;
    }
  }

  int32 cumul= 0;
  for (i= 0; i <= maxfr; i++) {
    RecompileFrame* fr= frames.nth(i);
    fr->computeSends();
    cumul += fr->sends;
    fr->cumulSends= cumul;
  }
    
  if (showStack || PrintRecompilation2) {
    lprintf("\n");
    for (i= maxfr; i >= 0; i--) frames.nth(i)->print();
    lprintf("\n"); 
  }
}


// Find the first frame exceeding 50% of the total send count and having 
// `significantly more' sends than its callee.

void Recompilation::find_break_frame(RecompileFrameBList frames) {
  const float SignificantlyMore = 1.2;
  const fint maxfr= frames.length() - 1;
  fint limit= max(frames.nth(maxfr)->cumulSends / 2, recompileLimits[0]);
  for (fint j= maxfr-1; j >= 0 && frames.nth(j)->cumulSends >= limit; j--) {
    float ratio=
      j > 0
        ? (float)frames.nth(j)->cumulSends / frames.nth(j-1)->cumulSends
        : 0;
    if (ratio >= SignificantlyMore)
      break;
  }
  // go down further if unsuitable for recompilation
  fint i;
  for (i= maxfr;
       i >= 0 && shouldNotRecompile(frames.nth(i)->nm, frames.nth(i + 1)->fr);
       i--)
    ;
  if (i >= 0 && frames.nth(i)->cumulSends >= limit) {
    // found the "break"
    RecompileFrame* fr= frames.nth(i);
    if (PrintRecompilation2) { lprintf("*top-down:\n"); fr->print(); }
    nvframe* top= (new nvframe(fr->fr, this))->top()->as_n();
    setNewTarget(top);
    findRecompilee(recompilee);         // go bottom-up from here
  }
  if (!recompilee) findRecompilee(tripNM);
}


void Recompilation::findTopRecompilee() {
  // find recompilee in top-down fashion (if SICRecompileTopDown); also
  // set recompilee if follow-up recompilation (prevNM != NULL)
  if (!SICRecompileTopDown && prevNM == NULL) return;  // nothing to do

  RecompileFrameBList frames(MaxStackToLookAt);
  fint i = 0;
  // collect the first MaxStackToLookAt stack frames, but if prevNM exists
  // stop at that frame
  for (frame* f = currentProcess->last_self_frame(true);
       f && (i < MaxStackToLookAt || prevNM);
       f = f->selfSender(), i++) {
    RecompileFrame* rf = new RecompileFrame(f);
    frames.append(rf);
    if (rf->nm == prevNM) break; // should this check rdepth? not sure -- miw
  }
  
  if (prevNM == NULL && SICRecompileTopDown) {
    // first recompilation of this series

    compute_better_counts(frames);

    find_break_frame(frames);

  } 
  else {
    // recompile frame below prevNM
    assert(prevNM, "should have prevNM");
    fint j = frames.length() - 1;
    assert(frames.nth(j)->nm == prevNM, "oops");
    // go down further if unsuitable for recompilation
    for ( --j; 
          j >= 0   &&  shouldNotRecompile(frames.nth(j)->nm, frames.nth(j+1)->fr);
              --j) ;
    if (j >= 0) {
      nvframe* top = (new nvframe(frames.nth(j)->fr, this))->top()->as_n();
      if (PrintRecompilation2) { lprintf("*top-down2: "); printNM(top->code); }
      setNewTarget(top);
    } 
    else {
      // no recompilee
    }
  }
}


inline void verifyAfterRecompile(char* cont, char* pc) {
  # if TARGET_ARCH == SPARC_ARCH
    assert( Byte_Map_Base() == Memory->remembered_set->byte_map_base(),
           "byte map base reg corrupted");
    assert (Byte_Map_Base() == (char*)saved_globals[ByteMapBaseReg - G0],
           "saved_globals for byte map base reg corrupted");
    assert( (char*)saved_globals[SPLimitReg - G0] == currentProcess->stackEnd()
    ||      (char*)saved_globals[SPLimitReg - G0] == currentProcess->spLimit(),
            "saved_globals for splimit reg corrupted");
  # endif
  if (VerifyAfterRecompilation) {
    bool safe = cont != NULL;
    if (! safe) {
      sendDesc* sd = sendDesc::sendDesc_from_return_PC(pc);
      if (   isCall((int32*)sd->call_instruction_addr()) 
          && (   !sd->isPrimCall()
              || (   sd->isPrimCall()
                  && getPrimDescOfFirstInstruction(sd->jump_addr(),
                                                   true)->canScavenge()))) {
        safe = true;
      }
    }
    if (safe) Memory->verify();
    for (frame* f = currentProcess->last_self_frame(true);
         f;
         f = f->selfSender()){
      if (f->code()->isInvalid() && !f->is_patched()) {
        fatal1("frame %#lx: should be patched", f);
      }
    }
  }
}


class RecompBuf {
 public:
  sendDesc* sd;
  frame* callerFrame;
  oop receiver;
  char* diChild;
  char* caller;
  void fill(sendDesc* s, frame* cf, oop r, char* d, char* c) {
    sd = s; callerFrame = cf; receiver = r; diChild = d; caller = c; }
};


// change whole thing for interp! XXX - dmu

static char* continueRecompile2(RecompBuf* buf, frame* last) {
  char *pc, *sp, *cont;
  nmethod* prevNM = NULL;
  frame* lastReplaced = NULL; // last (oldest) frame affected by recompile
  FlushRegisterWindows();       // make sure last & above is flushed to stack
  assert(!currentProcess->restartAfterConversion, "shouldn't be set");

  // NB: don't use needsWork/doWork because that could move nmethods, thus
  // invalidating the pointers in buf
  if (Memory->code->needsSweep()) Memory->code->doSweep();
  
  if (buf->callerFrame->is_self_frame()) {
    OutgoingArgsOfReturnTrapOrRecompileFrame = buf->callerFrame->extract_outgoing_args();
    assert(!SaveOutgoingArgumentsOfPatchedFrames
    ||     buf->callerFrame->sendee()->return_addr() == (char*)&  Recompile_stub_returnPC
    ||     buf->callerFrame->sendee()->return_addr() == (char*)&DIRecompile_stub_returnPC
    ||     buf->callerFrame->sendee()->return_addr() == (char*)&    MakeOld_stub_returnPC,
            "ensure patched_frame_saved_outgoing_args() will work");
  }

  fint i;
  for (i = 0; true; i++) {
    ResourceMark rm;
    Recompilation recomp(buf->callerFrame, buf->sd, buf->receiver, last, NULL,
                         last, false, prevNM);
    { BlockProfilerTicks bpt(exclude_recompile);
      FlagSetting f1(RecompilationInProgress, true);
      FlagSetting f2(PrintCompilation, PrintCompilation || PrintRecompilation);
      // NB: recompilee could be called from VM; e.g. Eval or UnwindProtect
      // assert(buf->callerFrame->is_self_frame(), "should be a Self frame");
      if (buf->diChild) fatal("fix this - DI not implemented");
      recomp.recompilee = ::recompilee;
      assert(isOnVMStack(last), "should be on VM stack");
      if (PrintRecompilation) lprintf("*%d: ", (void*)i);
      recomp.doit(buf->caller);
      recomp.finalize();
    }
    cont= recomp.restartAddr;
    if (cont) {
      if (i == 0) {
        pc = cont;      // first comp., no on-stack replacement
      } else {
        cont = NULL;    // previous recomp set newPC...don't set them again
      }
      break;
    }
    // was on-stack replacement
    pc = recomp.newPC; sp = recomp.newSP;
    if (!lastReplaced) {
      // remember oldest frame affected by recompilation
      for (lastReplaced = last;
           lastReplaced->code() != recomp.newNM;
           lastReplaced = lastReplaced->sender()) ;
      while (isOnVMStack(lastReplaced)) {
        lastReplaced = lastReplaced->sender();
      }
    }
    if (   !SICMultipleRecompilation
        || recomp.nremaining == 0
        || !allowedToRecompile()) {
      // stop recompiling
      break;
    }
    // set up next recompilation
    buf->callerFrame= last->selfSender();
    buf->sd= buf->callerFrame->send_desc();
    buf->receiver= new_vframe(last)->top()->receiver();
    buf->caller= recomp.newPC;
    prevNM= recomp.newNM;
  }

  // at this point, (cont != NULL) == there was no on-stack replacement
  processes->convert();
  verifyAfterRecompile(cont, pc);
  
  nmethod* continueNM = nmethod::findNMethod(pc);
  if (ScavengeAfterRecompilation) Memory->need_scavenge();
  assert(currentProcess->verifyFramePatches(), "patching bug");
  OutgoingArgsOfReturnTrapOrRecompileFrame = NULL;
  
  if (continueNM->isInvalid()) {
    // the current nmethod belongs to a block that was invalidated because its
    // home was recompiled, or it was invalidated because the sendDesc size
    // changed (mh bogusness)
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
        frame* f = currentFrame();
        while (!f->is_compiled_self_frame()) f = f->sender();
        assert(f->currentPC() == pc, "currentPC should be set");
        assert(f->is_patched(), "frame should be patched");
    }
#   endif
    // deoptimize the frame and continue execution
    // (we'll fall into [PrimCall]ReturnTrap)
    currentProcess->restartAfterConversion = true;
    LOG_EVENT("Continue into [PrimCall]ReturnTrap");
    return NULL;        
  } 
  else if (cont) {
    assert(i == 0, "shouldn't be here");
    LOG_EVENT1("Continue normally, cont = %#lx", cont);
    return cont;        // no on-stack replacement, continue normally
  } 
  else {
    LOG_EVENT2("ContinueAfterRecompilation pc=%#lx sp=%#lx", pc, sp);
#   if TARGET_ARCH == SPARC_ARCH
      ContinueAfterReturnTrap(pc, sp);
#   elif TARGET_ARCH ==  PPC_ARCH  \
     ||  TARGET_ARCH == I386_ARCH
      fatal("xxx unimp mac, unimp intel: where is result?");
      // ContinueAfterReturnTrap(result, pc, sp);
#   else
#     error what machine?
#   endif
  }
  ShouldNotReachHere();
  return NULL;
}


static char* continueRecompile(RecompBuf* buf) {
  // make an additional call to get extra frame on VM stack for fixStack
  return continueRecompile2(buf, currentFrame());
}


char* Recompile(sendDesc* sd, frame* callerFrame, oop receiver,
                char* diChild, char* caller) {
  // this activation may be overwritten during recompilation, so continue
  // on VM stack
  FlushRegisterWindows();
  assert(Byte_Map_Base() == Memory->remembered_set->byte_map_base(),
         "byte map base reg corrupted");
  static RecompBuf buf; // must be static (or else it might be overwritten)
  buf.fill(sd, callerFrame, receiver, diChild, caller);
  char* r = (char*)switchToVMStack((fntype)continueRecompile, &buf);
  assert(r != (char*)sd, "not supposed to happen");
  return r;
}


nmethod* also_Recompile( sendDesc* send_desc,
                         compilingLookup* L,
                         nmethod* reCompilee) {
  assert(isOnVMStack(currentFrame()), "should be on VM stack");
  if (reCompilee->isAccess()) return reCompilee;
  if (!allowedToRecompile()) return reCompilee;
  frame* vmfr = currentProcess->stack()->last_self_frame(true);
  FlagSetting fs1(SICMultipleRecompilation, false);
  FlagSetting fs2(SICReplaceOnStack, false);
  FlagSetting fs3(PrintCompilation, PrintCompilation || PrintRecompilation);
  FlagSetting fs4(InlineCache, false);
        // because inline caching will be done by calling lookup routine
  Recompilation recomp(L->sendingVFrame->fr,
                       send_desc,
                       L->receiver,
                       vmfr,
                       reCompilee, vmfr);
  recomp.doit(reCompilee->insts());
  recomp.finalize();
  nmethod* nm= nmethod::findNMethod(recomp.restartAddr); // why not just save nmethod? xxx miw
  if (nm == reCompilee) { // xxx miw
    // may not have been optimized, e.g. because of DI; force optimization
    FrameChainer fc(Memory->code);
    reCompilee->flush();
    nm = NULL;
  }
  return nm;
}


void Recompilation::init(char* pc) {
  thing = findThing(pc);        // caller thing (nmethod or count stub)
  calledFromStub= thing->isCountStub();
  if (calledFromStub) {
    CountStub *cs= (CountStub*)thing;
    assert(    cs->sd() == sd
           ||  cs->pic() && cs->pic()->sd() == sd,
           "sds don't match");
    tripNM= cs->target();
    restartAddr= tripNM->entryPointFor(sd);
  } else {
    assert(thing->isNMethod(), "should be a nmethod");
    if (currentProcess->isUncommon()) {
      // uncommon trap - recompilee already decided
      assert(recompilee, "should have a recompilee");
      tripNM = recompilee;
      // make stack look as if trap called us
      lastFrame = tripFrame = currentProcess->last_self_frame(true);
    } else {
      tripNM = (nmethod*)thing;
      assert(   SICMultipleRecompilation
             || recursive
             || tripNM->compiler() == NIC,
             "should be a NIC method");
    }
    restartAddr= tripNM->verifiedEntryPoint();
  }
  assert(tripNM->isNMethod(), "should be a nmethod now");
  if (PrintRecompilation)
    lprintf("*%srecompiling: tripNM = %#lx (%s)",
           currentProcess->isUncommon() ? "uncommon-" : "", tripNM, tripNM->key.selector_string());

  if (recompilee) {
    // already decided what to recompile (e.g. uncommon branch)
    // compute rdepth
    nvframe* vf = (new nvframe(lastFrame, this))->top()->as_n();
    while (vf && vf->code != recompilee) { // xxx miw
      vf = vf->sender();
      if (vf) vf = vf->top()->as_n();
    }
    // vf==NULL => recompilee must be callee of last_self_frame
    rdepth= vf ? vf->depth : -1; 
  } else {
    if (!allowedToRecompile()) {
      if (PrintRecompilation) lprintf(" (%CPU exceeded)");
      return;
    } else if (calledFromStub) {
      CountStub* cs= (CountStub*)thing;
      if (cs->count() < recompileLimit(tripNM->level())) {
        if (PrintRecompilation) lprintf(" (counter decayed)");
        return;
      }
    }
    findTopRecompilee();
    if (recompilee == NULL) {
      if (showStack || PrintRecompilation2) doShowStack();
      findRecompilee(tripNM);
      assert(!recompilee || !mustNotRecompile(recompilee, callerFrame),
             "oops");
    }
  }

  if (recompilee && recompilee->isAccess()) {
    noRecompilee(); // don't recompile access methods
  }

  if (tripNM != recompilee && useCount[tripNM->id] > 0) { //xxx miw
    // give the tripNM another chance to get recompiled (esp. since it
    // could be uncommon)
    useCount[tripNM->id] = ClearTripNMUseCount ? 0 : useCount[tripNM->id] / 2;
  }
}
  

void Recompilation::doit(char* pc) {
  init(pc);

  EventMarker em( "recompiling %#lx: %#lx --> %#lx", tripNM, NULL, NULL);

  if (Interpret) return; // XXXX no recompile for interp for now -- dmu
  if (recompilee == NULL) {
    if (PrintRecompilation) lprintf(": no recompilee\n");
    if (calledFromStub) {
      ((CountStub*)thing)->set_count(0);
    }
    return;
  } 
  if (PrintRecompilation) lprintf(", recompilee = %#lx", recompilee);
  FrameChainer fc(Memory->code);
  newNM = recompile();
  em.event.args[1] = recompilee; em.event.args[2] = newNM; 
  ::recompilee = NULL;
  recompileeVScopes = NULL;
  if (recompilee == NULL) return; // couldn't recompile (e.g. DI)
  
  checkEffectiveness(recompilee, newNM);
  recompilee->unlink_saved_frame_chain();

  if (newNM != tripNM && UsePICRecompilation) { // xxx miw
    // Link the new method into the inline caches linked to the
    // recompilee.  Necessary to preserve type info of these
    // methods.

    if (recompilee->reusable()) {
      // If old method was reusable, cannot redirect linked sends from old,
      // generic NIC method to new, specialized SIC method: may be wrong
      // context.  Also, do not want to throw away reusable nmethods.
      recompilee->remove_me_from_inline_cache();
    } else {
      recompilee->forwardLinkedSends(newNM);
      recompilee->flush();      // no longer need its PICs
    }
  }
  
  const char* msg = replaceOnStack();
  if (msg == NULL) {
    // replaced on stack
    restartAddr= NULL;
    return; 
  }
  if (PrintRecompilation && SICReplaceOnStack) {
    lprintf("*not replacing on stack: %s\n", msg); 
  }
  LOG_EVENT1("*not replacing on stack: %s", msg);
  if (recompiling_trip()) {
    // continue with new (optimized) version
    restartAddr= calledFromStub
                 ? newNM->entryPointFor(sd)
                 : newNM->verifiedEntryPoint();
  } else {
    // didn't change the method which actually overflowed its counter
  }
}


nmethod* Recompilation::recompile() {
  nmethod* new_nm = NULL;
  if (   recompiling_trip()
      && receiver->map() != recompilee->key.receiverMap()
      && calledFromStub
      && ((CountStub*)thing)->jump_addr() != tripNM->verifiedEntryPoint()) {
    // The lookup would have missed in the inline cache if it hadn't
    // triggered recompilation.  Try again later (need correct receiver
    // oop below)
    CountStub* cs= (CountStub*)thing;
    cs->set_count(cs->count() - 1);
    if (PrintRecompilation) lprintf(" -- NOT recompiled (will miss)\n");
    noRecompilee();
    return tripNM;
  }
  if (recompilee->isUncommon()) {
    fatal("uncommon branches shouldn't be recompiled");
  } 
  if (recompilee != tripNM) { // xxx miw
    tripNM->save_unlinked_frame_chain(); // protect n from being flushed
  }
  if (recompilee->isDI()) {
    // for now, don't recompile DI methods
    // The problem is that the state of the DI parents can be different
    // from the original state, so that the recompiled method tests
    // for different parent(s) than the recompilee, screwing up the
    // whole di chain (possibly leading to endless recompiles).
    // To solve this, we should probably pass in the recompilee's
    // parent maps to the recompiler.   -Urs

    // to achieve at least something, mark sendDesc as optimized
    sendDesc* s = callerFrame->send_desc();
    s->setOptimized();
    s->unlink();
    
    noRecompilee();
    if (PrintRecompilation) lprintf(": not recompiling because of DI\n");
    tripNM->unlink_saved_frame_chain();
    return tripNM;
  } 
  else {
    recompilee->flushPartially();
    // recompilation: recompilee is still in sd, tell lookup not to put
    // new nmethod in there
    // also, with uncommon trap in recursive nmethods, sendDesc could be
    // bound twice to same nmethod if InlineCache were true
    FlagSetting fs(InlineCache, false);
    
    getVScopes();
    sendDesc* s = callerFrame->send_desc();
    oop selector = recompilee->key.selector;
    oop delegatee = recompilee->key.delegatee;
    abstract_vframe* sendingVFrame = new_vframe(callerFrame);
    oop smh = sendingVFrame->methodHolder_object();
    ::recompilee = recompilee;
    
    // don't use SendMessage (has ResourceMark which may corrupt markers
    // constructed by compiler); also, don't use stack allocation for L
    // because top scope refers to it (key)
    cacheProbingLookup* L =
      new cacheProbingLookup( receiver,
                              selector,
                              delegatee,
                              smh,
                              new_vframe(callerFrame),
                                      // windows already flushed
                              s,
                              NULL,   // DIDesc
                              false); // don't want debug version

    new_nm = s->lookup_compile_and_backpatch(L);
    
/* The map test may fail due to a programming change and has been
   disabled, Lars 1/26/94
#  if GENERATE_DEBUGGING_AIDS
   assert(    (recursive || SICMultipleRecompilation)
           && new_nm->key.receiverMapOop()->equal(recompilee->key.receiverMapOop())
        || new_nm->key == recompilee->key,
        "keys don't match");
#  endif
*/
  }

  if (recompilee != tripNM) { // xxx miw
    tripNM->unlink_saved_frame_chain();
  }
  if (PrintRecompilation) lprintf("\n");
  if (new_nm && nstages > 1 && new_nm->version() < MaxVersions)
    new_nm->flags.isYoung= 1;
  return new_nm;
}


void Recompilation::checkEffectiveness(nmethod* oldNM, nmethod* new_nm) {
  // new_nm is a recompiled version of oldNM; try to catch cases where
  // recompilation didn't get us anything and prevent further recompilation
  assert(oldNM != new_nm, "nmethods should differ");
  addrDesc* a1 = oldNM->locs(), *aend1 = oldNM->locsEnd();
  addrDesc* a2 = new_nm->locs(), *aend2 = new_nm->locsEnd();
  // check if the two nmethods have the same sendDescs
  for  ( ; a1 < aend1 && a2 < aend2; a1++, a2++) {
    for ( ; a1 < aend1 && !a1->isCall(); a1++, a2++) ;
    if (a1 >= aend1) {
      if (a2 < aend2) return;   // not the same
      break;
    }
    if (a2 >= aend2 || !a2->isCall()) return;
    if (a1->isPrimitive() != a2->isPrimitive() ||
        a1->isSendDesc()  != a2->isSendDesc()) return;
    if (!a1->isSendDesc()) continue;
    sendDesc* sd1 = a1->asSendDesc(oldNM);
    sendDesc* sd2 = a2->asSendDesc(new_nm);
    LookupType l1 = sd1->lookupType();
    if (l1 != sd2->lookupType()) return;
    if (   !isPerformLookupType(l1)
        && sd1->selector() != sd2->selector())
      return;
  }
  // ok, they have the same sendDescs; make sure all sendDescs are non-counting
  if (PrintRecompilation)
    lprintf("*recompilation ineffective (%#lx)!\n", new_nm);
  for (a2 = new_nm->locs(); a2 < aend2; a2++) {
    if (a2->isSendDesc()) {
      a2->asSendDesc(new_nm)->setCounting(NonCounting);
    }
  }
  if (oldNM->version() == MaxVersions - 1) {
    new_nm->setVersion(MaxVersions);
  } else {
    // bump version counter by an extra generation, but don't set to max --
    // could have been ineffective because we recompiled too early
    new_nm->setVersion(min(new_nm->version() + 1, MaxVersions - 1));
  }
}


void Recompilation::getVScopes() {
  assert(recompileeVScopes == NULL, "shouldn't be set");
  if (!SICReplaceOnStack) return;
  VScopeBList* vscopes = new VScopeBList(50);
  abstract_vframe* stop = new_vframe(callerFrame);
  VScope* prev = NULL;
  for (abstract_vframe* vf = new_vframe(currentProcess->last_self_frame(true));
       !vf->EQ(stop);
       vf = vf->sender()) {
    vscopes->push(prev = new VScope(vf, prev));
  }
  if (vscopes->isEmpty()) {
    // stopped in prologue - replacing on stack is trivial
    vscopes = NULL;
  }
  recompileeVScopes = vscopes;
  markers = vscopes ? new MarkerNodeBList(vscopes->length()) : NULL;
}


const char* Recompilation::replaceOnStack() {
  // try to replace unoptimized nmethods on stack with new nmethod
  // return NULL if successful, error string if not
  if (!SICReplaceOnStack) return "!SICReplaceOnStack";
  if (currentProcess->isUncommon()) {
    // frame::adjust_blocks will fail (no sendDesc, no reg. mask)
    return "not possible yet (uncommon branch)";
  }
  if (   recompiling_trip()
      && !currentProcess->isUncommon()
      && new_vframe(callerFrame)->is_prologue()) {
    return "not necessary (at beginning of nmethod)";
  }
  if (recompilee == NULL) return "no recompilee";
  if (!isReplacementSimple) return "not simple";

  if (!activeMarker || !activeMarker->active)
    return "no active marker";


  // added tripNM == recompilee qualification to assert below
  //  because sd is the tripping sd, so if the recompilee is up
  //  the stack, this assertion is meaningless -- dmu 5/96

  assert( tripNM != recompilee
      ||     sd->endOffset(sd->lookupType()) 
          == sd->endOffset(newNM->key.lookupType),
         "lookupType changed");


  if (rdepth > MaxStackToLookAt) return "rdepth too big";

  sendDesc* s = NULL;
  nremaining = rdepth - activeMarker->depth();
  if (nremaining > 0) {
    // didn't inline everything; make sure we can find sendDesc
    // XXXX interp?
    s = activeMarker->send_desc(newNM);
    if (s == NULL) return "can't find sendDesc";
  }

  // now check for C frames between tripFrame and recompilee's frame
  fint nframes = 0;
  for (frame* f = tripFrame; f != callerFrame; f = f->sender(), nframes++) {
    if (!f->is_self_frame()) {
      return "C frame inbetween";
    }
  }
  nframes--;

  if (nremaining > 0) {
    // find top frame not replaced by optimized nmethod
    abstract_vframe* vf = new_vframe(tripFrame);
    for (fint i = 1; i < nremaining; i++) vf = vf->sender();
    if (!vf->is_top()) return "stopped inlining in middle of existing frame";
    // too complicated -- e.g. pushFrame() and block remapping would have to be
    // rewritten to handle partially-replaced firstReplaced frame
  }

  if (checkForRemappedBlocks()) {
    return "potential trouble with remapped blocks";
  }

  if (checkForActivationMirrors()) {
    return "potential trouble with activation mirrors";
  }

  if (PrintSICReplacement) {
    lprintf("**old vframes (unoptimized):\n");
    // make sure stack[rdepth+1] exists
    nvframe* vf = stack.nth(rdepth)->sender();
    for (fint i = 0; i <= rdepth + 1; i++) {
      stack.nth(i)->print_frame(i);
    }
  }
  // write current state to event log (for debugging)
  for (fint i = 0; i <= rdepth; i++) {
    nvframe* vf = stack.nth(i);
    LOG_EVENT2("*old frame: fr=%#lx, sender=%#lx", vf->fr, vf->fr->sender());
    LOG_EVENT3("*old frame: sel=%#lx, nm=%#lx, pc=%#lx",
               vf->selector(), vf->code, vf->fr->return_addr());
  }
  FlagSetting fs(ConversionInProgress, true);
  replaceFrames(nframes, nremaining);
  fillValues(lastFrame);

  if (nremaining > 0) {
    // didn't inline everything; find sendDesc and copy the remaining
    // unoptimized frames back to the stack
    LOG_EVENT1("setting newPC from sd: %#lx", s);
    newPC = (char*)s;
    fixStack();         // for better debugging
    frame **frames= NEW_RESOURCE_ARRAY( frame*, nframes);
    frame* fr = copiedFrame;
    fint j;
    for (j = 0; fr != firstReplaced; j++, fr = fr->sender()) {
      frames[j] = fr;
    }
    assert(j <= nframes, "went too far");
    bool needsPatching= false;
    for (--j ; j >= 0; j--) {
      needsPatching |= pushFrame(frames[j]);
    }
    if (needsPatching)
      currentProcess->convert();
  }
  killBlocks();
  handleRemappedBlocks();
  if (PrintSICReplacement) printNewFrames(lastFrame);
  discardNMethods();
  return NULL;
}


void Recompilation::replaceFrames(fint nframes, fint diff) {
  // copy the old frames

  frame* frameToCopy = tripFrame->make_full_frame_on_user_stack();

  if (frameToCopy != tripFrame) {
    assert(recursive || SICMultipleRecompilation, "shouldn't normally happen");
  }
  
  copiedFrame = frameToCopy->copy(nframes, true);
  if (!copiedFrame) fatal("couldn't copy frame");

  // find frame range that will be replaced by new frame
  if (diff > 0) {
    // didn't inline everything
    frame* f = copiedFrame;
    // find top frame not replaced by optimized nmethod
    abstract_vframe* vf = new_vframe(copiedFrame);
    fint i;
    for (i = 1; i < diff; i++) vf = vf->sender();
    firstReplaced = vf->sender()->fr;
    for (i = 0; f != vf->fr; i++, f = f->sender()) ;
    assert(i <= nframes, "went too far");
    nreplaced = nframes - i;
    if (!vf->is_top()) {
      nreplaced--;      // didn't completely replace last frame
      ShouldNotReachHere();     // not implemented yet
    }
    origFirstReplaced = frameToCopy;
    for (origFirstReplaced = frameToCopy, f = copiedFrame;
         f != firstReplaced;
         origFirstReplaced = origFirstReplaced->sender(), f = f->sender()) ;
  } else {
    firstReplaced = copiedFrame;
    nreplaced = nframes + 1;
    origFirstReplaced = frameToCopy;
  } 

  lastReplaced = firstReplaced;
  origLastReplaced = origFirstReplaced;
  for (fint i = nreplaced; i > 1; i--) {
    lastReplaced = lastReplaced->sender();
    origLastReplaced = origLastReplaced->sender();
  }
  assert(lastReplaced->code()->method() == newNM->method(), "wrong frame");
  
  block_scope_of_origFirstReplaced = 
    origFirstReplaced->block_scope_of_home_frame();
  block_scope_of_origLastReplaced = 
    origLastReplaced->block_scope_of_home_frame();

  // pop off the frames to be replaced
# if TARGET_ARCH != SPARC_ARCH
  // sorry - right now this code is machine-dependent;
  fatal("unimp");
# else
  // create new frame and initialize FP/PC

  // XXX This creates a newFramePiece as does push_new_sp.
  //  So we should factor them later.
  //  Also this object seems to include redundant instance vars.
  
  assert( callerFrame->my_sp()->as_oops() ==
          callerFrame->callees_sp()->as_oops() + recompilee->frameSize(),
          "I simplified, and want to check that this works --dmu");
  
  char* addr = callerFrame->real_return_addr();
  sparc_fp* link = callerFrame->callees_sp()->link();
  
  newFramePiece = (sparc_sp*) (callerFrame->my_sp()->as_oops()
                               - newNM->frameSize());
  callerFrame = newFramePiece->as_callers_frame();
  
  newSP = (char*)newFramePiece;
  newPC = newNM->insts() + activeMarker->pcOffset;

  LOG_EVENT1("setting newPC from active marker: %#lx", newPC);

  // NULL out frame (needed for stack temps)
  //  Why? XXX -- dmu
  set_oops(newFramePiece->as_oops(),  newNM->frameSize(),  0);
  
  newFramePiece->set_link(link);
  newFramePiece->set_return_addr(addr);

  fixStack();

  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      // make things break quickly if locations remain uninitialized
      lastFrame->zap(0, RegisterLocator::for_frame(lastFrame));      // FYI: this zaps with 0x1, not UNINITIALIZED
      if (nremaining == 0) {
        LOG_EVENT("zapping global regs");
        const oop UNINITIALIZED = (oop)0xfffffffd;
        for (fint ii = 0; ii < NumTempRegs; ii++) {
          Location r = TempRegs[ii];
          if (r <= G7) {
            assert(r >= G0, "oops");
            saved_globals[r - G0] = UNINITIALIZED;
          } else {
            assert(O0 <= r && r < O7, "oops");
            // can't alway zap out regs since out regs of current send are not
            // described by activeMarker
            // saved_outregs[r - O0] = UNINITIALIZED;     
          }
        }
      } else {
        // must not overwrite regs - needed by remaining frame
      }
    }
  # endif // GENERATE_DEBUGGING_AIDS
# endif // TARGET_ARCH == SPARC_ARCH
}


void Recompilation::discardNMethods() {
  // flush all nmethods that were replaced on stack
  // HACK: force unchaining of frames to see if any of the nmethods can
  // be discarded (necessary because stack manipulations mess up the
  // frame chains)
  if (recursive) return;
  fint nesting = zone::frame_chain_nesting;
  zone::frame_chain_nesting = 1;
  Memory->code->unchainFrames();
  frame* f;
  for (f = copiedFrame; true; f = f->sender()) {
    f->code()->clear_frame_chain();
    if (f == lastReplaced) break;
  }
  Memory->code->chainFrames();
  zone::frame_chain_nesting = nesting;
  // first mark all nmethods to flush, then flush in a 2nd pass
  // (nmethod may be on stack twice because of recursion)
  const frame* MagicFrame = (frame*)-1;
  nmethodBList flushList(50);
  for (f = copiedFrame; true; f = f->sender()) {
    nmethod* nm = f->code();
    if (nm->frame_chain != NoFrameChain) {
      // still used (on another stack), or already in flushList
    } else if (nm != recompilee && !nm->reusable()) { // xxx miw
      nm->frame_chain = (frame*)MagicFrame;
      flushList.append(nm);
    }
    if (f == lastReplaced) break;
  }
  while (flushList.nonEmpty()) {
    nmethod* nm = flushList.pop();
    assert(nm->frame_chain == MagicFrame, "should be magic");
    nm->clear_frame_chain();
    nm->flush();
  }
}


static frame* kill_old_blocks_lo;
static frame* kill_old_blocks_hi;
static void kill_old_blocks(oop* p) {
  if ((*p)->is_block()) {
    frame* bs = blockOop(*p)->scope(true);
    if (kill_old_blocks_lo <= bs  &&  bs <= kill_old_blocks_hi) {
      (*p)->kill();
    }
  }
}


void Recompilation::killBlocks() {
  // kill off any remaining blocks in belonging to the frames replaced by
  // newFr (these are never created in the new optimized nmethod)
  ::kill_old_blocks_lo = firstReplaced->block_scope_of_home_frame();
  ::kill_old_blocks_hi = lastReplaced ->block_scope_of_home_frame();
  assert(kill_old_blocks_lo <= kill_old_blocks_hi, "bad range");
  RegisterLocator* rl = RegisterLocator::for_frame(firstReplaced);
  for (frame* f = firstReplaced; 
              f <= lastReplaced; 
              f = f->sender(),  rl = rl->sender()) {
    f->oops_do(kill_old_blocks, rl);
  }
}


bool Recompilation::checkForRemappedBlocks() {
  // Check for real blocks that may block (har, har) on-stack replacement.
  // A pathological situation occurs if recompilee has a block currently active
  // on the stack (say, oldBlockNM), and oldBlockNM has real nested blocks.
  // Then, if we replace recompilee but not oldBlockNM, the nested blocks have
  // the wrong map no matter what: the old map is wrong because newNM's
  // layout is different, but the new map is wrong too because oldBlockNM
  // is still on the stack and may have a different layout from newBlockNM.
  fint last = rdepth - activeMarker->depth(); // depth of newest vframe to be recompiled
  frame* startF = stack.nth(last)->fr; // newest frame to be recompiled
  frame* endF   = stack.nth(rdepth)->fr; // oldest
  fint i;
  for ( i = 0; i < rdepth; i++) {
    nvframe* vf = stack.nth(i);
    oop rcvr = vf->receiver();
    if (rcvr->is_block()) {
      // check if this block's home is in newNM (transitively)
      
      // used to be:

      //  nvframe* home;      
      //  for ( home = vf;
      //        home != NULL;
      //        home = home->parent()->as_n()) 
      //     if (last <= home->depth  &&  home->depth <= rdepth)
      //       break;           
      //  if (home == NULL) 
      //    continue;

      // but this only works if method is block method, and
      //  we need to find outer methods if the receiver is a block
      //  with a recompiled enclosing scope -- dmu

      bool parentWillBeRecompiled = false;
      for ( abstract_vframe*  parentVF = blockOop(rcvr)->parentVFrame(NULL, true);
                              parentVF != NULL;
                              parentVF = parentVF->parent() ) {
        if ( startF <= parentVF->fr  &&  parentVF->fr <= endF ) {
          parentWillBeRecompiled = true;
          break;
        }
      }  
      if ( !parentWillBeRecompiled )
        continue;

      // ok, this is a block that could be troublesome; find out if it has
      // been inlined into newNM
      if (rdepth - vf->depth < activeMarker->depth()) 
       continue;

      // Oops, this guy could really cause trouble: it will be in a
      // nmethod called (transitively) by newNM.  If there exist any
      // real blocks whose home is in vf (transitively), we can't do the
      // replacement.  (Of course we could do it anyway if we knew that
      // this other nmethod could be replaced on the stack, but since
      // replacement isn't always possible we're stuck.  A less conservative
      // approach would be to find out if the second replacement can be done.)

      if (((methodMap*)(vf->method()->map()))->hasSubBlocks()) {
        // check if any of the nested blocks has a chance of executing before
        // vf's activation resumes; unfortunately, this is always possible
        // even if all these blocks are BlockValues (i.e. never created),
        // single-stepping etc. could create them anyway
        return true;
      }
    }
  }
  return false;         // no blocks that may cause trouble
}


bool Recompilation::checkForActivationMirrors() {
  // Check if any of the frames that would be replaced has an activation
  // mirror on it.  Such mirrors could be remapped with some work, but
  // for now just don't replace on stack (fix this).   -Urs 8/94
  // (see Process:convertVFrameOops if you want to lift this restriction.)
  // returns true if there is a problem

  vframeOop vfo = currentProcess->findInsertionPoint(stack.nth(0))->next();
  LOG_EVENT1("checkForActivationMirrors %#lx", vfo);

  if (vfo == NULL) return false;

  frame* start = stack.nth(0)->fr;
  frame* end   = stack.nth(max(0, rdepth))->fr;

  assert( start->vfo_locals_of_home_frame() <= end->vfo_locals_of_home_frame(),
          "oops");

  LOG_EVENT3("checkForActivationMirrors %#lx %#lx %#lx",
             vfo->locals(),
             start->vfo_locals_of_home_frame(),
             end->vfo_locals_of_home_frame());

  return  vfo->is_equal(start)
    ||    (vfo->is_above(start) && (vfo->is_below(end)
    ||    vfo->is_equal(end)));
}


void Recompilation::fillValues(frame* newFr) {
  frame* firstBS1 = firstReplaced->block_scope_of_home_frame();
  frame*  lastBS1 =  lastReplaced->block_scope_of_home_frame();
  frame* firstBS2 = block_scope_of_origFirstReplaced;
  frame*  lastBS2 = block_scope_of_origLastReplaced;
  
  // fill in new frame's oops
  compiled_vframe* newVF = new_vframe(newFr)->as_compiled();
  for (fint i = 0;  i < activeMarker->locs->length();  i++) {
    assert(activeMarker->locs->nth(i)->isValueLocation(), "oops");
    ValueLocationNameDesc* nd =
      (ValueLocationNameDesc*)activeMarker->locs->nth(i);
    Location loc = nd->location();

    if (nremaining  &&  isInitializedInFillValues(loc)) {
      // don't initialize this register - not bottom frame
      LOG_EVENT2("not setting %s (%d remaining)",
                 locationName(loc), nremaining);
      continue;
    }

    oop b = nd->value();
    if (b->is_block()) {
      if (oop(nd->block) == badOop || nd->block->map() == b->map()) {
          LOG_EVENT3("not remapping block %#lx (namedesc block = %#lx%s)",
                     b, nd->block,
                     nd->block == Memory->deadBlockObj  
                       ?  " is deadBlockObj" : "");
      }
      else {
        // block may have to be remapped - old frame was replaced by newFr
        // but be careful not to remap blocks whose homes still run the old
        // version of the nmethod
        frame* bs = blockOop(b)->scope(true);
        if (   (bs >= firstBS1 && bs <= lastBS1)
            || (bs >= firstBS2 && bs <= lastBS2)) {
          // block's home is either in removed frame or in copy of it
          blockOop(b)->remap(nd->block->map(), newFr);
        }
        else {
          // doesn't need to be remapped
          LOG_EVENT1("not remapping block %#lx (not in replaced frames)", b);
        }
      }
    }
    LOG_EVENT2("setting %s to %#lx", locationName(loc), b);
    // don't use register_contents_addr - don't want location on stack
    if (TARGET_ARCH == I386_ARCH)
      fatal("This is not right for the Intel architecture.");
    if (isArgRegister(loc))   fillRegisterValue(loc, b);
    else                      *newVF->register_contents_addr(loc) = b;
  }

  // fill in any extra args (stored in our frame, not callee's)
  // XXXX fix for interp
  sendDesc* s = activeMarker->send_desc(newNM);
  if (s && !s->isPrimCall() && s->arg_count() > NumArgRegisters) {
    compiled_vframe* oldVF = new_vframe(firstReplaced)->as_compiled();
    for (fint j = s->arg_count() - 1; j >= NumArgRegisters; j--) {
      Location loc = ArgLocation(j);
      oop val = oldVF->register_contents(loc);
      oop* p = newVF->register_contents_addr(loc);
      LOG_EVENT2("setting extra arg %s to %#lx", locationName(loc), val);
      *p = val;
    }
  }
}


void Recompilation::handleRemappedBlocks() {
  // If a block method is on the stack whose receiver block was remapped
  // because its home is in newNM, that frame needs to be recompiled.
  // Why? Because that method may clone blocks and give them maps that will find
  // nmethods in the zone that assume the old lexical parent stack frames. (dmu)
  // Yet another annoying side-effect of nmethod-specific block maps...
  assert(rdepth >= 0, "rdepth shouldn't be negative");
  frame **frames= NEW_RESOURCE_ARRAY( frame*, rdepth);
  fint n = 0;
  for (frame* f = lastFrame; f->code() != newNM; f = f->sender()) {
    frames[n++] = f;
  }
  nmethod* nm;
  oop rcvr;
  fint i;
  for ( i = n - 1; i >= 0; i--) {
    // NB: for PPC this will have to be modified to avoid quadratic RegisterLocator allocation
    abstract_vframe* vf = (new_vframe(frames[i]))->top();
    rcvr = vf->receiver();
# if TARGET_ARCH == SPARC_ARCH
    if (   rcvr->is_block()
        && blockOop(rcvr)->scope() == newFramePiece->as_callers_frame()) {
      // receiver is a block with home in recompiled nmethod
      // NB: in theory, only have a problem if it is the block method (rather
      // than, e.g., whileTrue:), because only the block method has lexical
      // access.  However, the non-block method could have an inlined copy of
      // the block method which is currently not active but might be invoked
      // any time.  Thus, the code below catches non-block cases, too.
      nm = frames[i]->code();
      if ( rcvr->map() != nm->key.receiverMap()  // xxx miw
      &&   nm->isValid()) 
        break;
    }
# else
    fatal("recompilation unimplemented on this architecture");
# endif
  }
  if (i < 0)
    return;
        
  // yup, this guy must be recompiled because the block map has changed
  assert(nm->frame_chain != NoFrameChain, "should be on stack");
  bool wasReplaced = false;
  Recompilation* recomp = NULL;
  
  // Originally, the next bit was skipped if SICMultipleRecompilation
  //  was set, with the explanation:
    // don't recompile now - will (probably) recompile later
    // (but must mark it as invalid)
    
  // BUT, if this frame is about to perform a send with an
  //  argument that is a block-literal, when said block literal
  //  is invoked, it will find a block nmethod that has the wrong map
  //  for SELF (since we are trying to change this block's map
  //  (recall this block is the receiver of this frame).
  // So, I think we have to recompile anyway, right away!
  // Except that, recompiling is not guaranteed, it may
  //  not replace on stack.
  // So I have strengthed checkForRemappedBlocks, so that
  //  now it should not be correctness-critical to replace here.
  //   -- dmu 1/12/96
  

  if (!SICMultipleRecompilation) {
    nm->flushPartially();
    frame* sender = frames[i]->sender();
    recomp = new Recompilation(sender, sender->send_desc(), rcvr,
                               lastFrame, nm, lastFrame, true);
    recomp->doit(nm->insts());
    recomp->finalize();
    wasReplaced = recomp->nreplaced >= 0;
    
    // (end of next bit)
  }

  if (wasReplaced) {
    // on-stack replacement was successful
    lastFrame = recomp->lastFrame;
    newPC = recomp->newPC;
    LOG_EVENT1("setting newPC from recomp: %#lx", newPC);
    newSP = recomp->newSP;
  } else {
    // hopefully, this doesn't happen too often
    // deoptimize frame when control returns to it
    nm->invalidate();
    
  }
  // check again - there may be others
  handleRemappedBlocks();
}



bool Recompilation::pushFrame(frame* copy) {
  // copy the frame onto the run-time stack
  assert(copy < firstReplaced, "shouldn't push this frame");
  char* oldSP = newSP;
  fint size = copy->frame_size();
  newSP -= size * oopSize;
  copy->copy_to(newSP, oldSP, newPC, true);
  newPC = copy->return_addr();
  LOG_EVENT1("setting newPC in pushFrame: %#lx", newPC);
  fixStack();
  // can't patch the frame now as it may be overwritten during stack
  // replacement 
  assert(!copy->code()->isInvalid()
         || currentProcess->last_self_frame(true)->code()->isInvalid(),
         "code should be invalid");
  return copy->code()->isInvalid();
}


void Recompilation::fixStack() {
  lastFrame->fix_frame( newPC, newSP);
}


void Recompilation::printNewFrames(frame* newFr) {
  lprintf("**new vframes (optimized):\n");
  abstract_vframe* vf = new_vframe(newFr);
  fint vdiff = newFr->vdepth() - (rdepth < 0 ? 0 : rdepth);
  while (vdiff-- > 0) {
    // Printing these vframes is somewhat risky since newNM isn't at an
    // interrupt point.  So just print their names.
    lprintf("skipping vframe %#lx \"%s\"\n",
           vf, selector_string(vf->selector()));
    vf = vf->sender();
  }
  fint i;
  for (i = 0; vf->fr == newFr || i < rdepth; vf = vf->sender()) {
    if (vf->is_prologue()) {
      // bottommost vf doesn't really exist (hasn't completed prologue yet,
      // e.g. locals may be uninitialized)
      assert(i == 0, "not bottommost");
    } else {
      vf->print_frame(i++);
    }
  }
  vf->print_frame(i++);
}


char* MakeOld(sendDesc* sd, frame* callerFrame, oop receiver,
              char* diChild, char* caller) {
  nmethod* nm;
  NCodeBase* thing;

  { ResourceMark rm;
    thing= findThing(caller);   // caller thing (count stub)
    assert(thing->isCountStub(), "should be a stub");
    CountStub* stub = (CountStub*)thing;
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
        sendDesc* sd2 = stub->sd();
        if (!sd2) sd2 = stub->pic()->sd();
        assert(sd2 == sd, "sd mismatch");
    }
#   endif

    nm = stub->target();
    const bool BypassMakeOld = false; // for debugging, set to true to bypass me
    if (BypassMakeOld)   return nm->entryPointFor(sd);
    // assert(nm->isYoung(), "should be young nmethod");
    // nm can be old: megamorphic PICs just reuse count stubs on a miss, so if
    // a young nmethod is in a megamorphic PIC and gets thrown out before
    // becoming old, some random nmethod is linked to the AgingStub
    nm->makeOld();
  }
  if (nm->isToBeRecompiled()) {
    // recompile this guy now
    LOG_EVENT("calling Recompile from MakeOld");
    return Recompile(sd, callerFrame, receiver, diChild, nm->insts());
  } 
  // It is possible to have a call chain with all SIC methods
  //  where the caller needs recompilation, but makes no other calls but
  //  this one. So check the caller, too. -- dmu 5/96

  nmethod* sendingNM = nmethod::findNMethod(sd);
  if ( sendingNM->isToBeRecompiled()) {
    // Must make sender old, so it can be chosen as recompilee
    {
      ResourceMark rm; // above code does it, maybe needed for makeOld?
      sendingNM->makeOld();
    }
    // Don't know how to point compiler to caller, so just invoke as above
    LOG_EVENT("calling Recompile from MakeOld (2)");
    return Recompile(sd, callerFrame, receiver, diChild, nm->insts());
  }
  return nm->entryPointFor(sd);
}


static fint checkCompiler(oop c, bool recompiler) {
  // return compiler or -1 if invalid
  if (!c->is_byteVector()) return -1;
  if (VMString[NIC]->equals(byteVectorOop(c))) {
    // the nic is not a valid recompiler
    return recompiler ? -1 : fint(NIC);
#   ifdef SIC_COMPILER
    } else if (VMString[SIC]->equals(byteVectorOop(c))) {    
      return SIC; // change to NIC to run test suite w/ NIC even if it tries for SIC
#   endif
  }
  return -1;
}


oop set_recompilation_prim(oop r, objVectorOop comps,
                           objVectorOop limits, void* FH) {
  fint len = comps->length();
  if (len < 1) {
    failure(FH, "must have at least one compiler");
    return 0;
  }
  if (len != limits->length() + 1) {
    failure(FH, "limit vector should be one shorter than compilers vector");
    return 0;
  }
  fint* newcomps = (fint*)AllocateHeap(len * sizeof(fint),
                                       "compilers settings");
  fint* newlimits = (fint*)AllocateHeap(len * sizeof(fint),
                                        "recompilation limits");
  smi* newcounts = (smi*)AllocateHeap(len * sizeof(smi), "compilers settings");
  fint i;
  for (i = 0; i < len; i++) {
    newcomps[i] = checkCompiler(comps->obj_at(i), i);
    if (newcomps[i] < 0) {
      char msg[80];
      sprintf(msg, "arg1[%ld]: invalid compiler", long(i));
      failure(FH, msg);
      return 0;
    }
    newcounts[i] = i < nstages ? compileCounts[i] : 0;
  }
  for (i = 0; i < len - 1; i++) {
    oop lim = limits->obj_at(i);
    if (!lim->is_smi() || smiOop(lim)->value() < 1) {
      char msg[BUFSIZ];
      sprintf(msg,
              "arg2[%ld]: invalid recompilation limit (not a positive integer)",
              long(i));
      failure(FH, msg);
      return 0;
    }
    newlimits[i] = roundTo(smiOop(lim)->value(), K);
    // for simplicity (Sparc sethi), round up values to next K
  }
  // everything ok, install new values
  nstages = len;
  selfs_free(compilers); compilers = newcomps;
  selfs_free(compileCounts); compileCounts = newcounts;
  selfs_free(recompileLimits); recompileLimits = newlimits;
  return r;
}


# else

# define recompilee NULL // use define to avoid defining nmethod

nmethod* also_Recompile( sendDesc*         /* send_desc */,
                         compilingLookup*  /* L */,
                         nmethod*          /* reCompilee */) {
  return NULL;
}


# endif //  defined(SIC_COMPILER)


oop get_compilers_prim(oop) {
  objVectorOop res = Memory->objVectorObj->cloneSize(nstages);
  for (fint i = 0; i < nstages; i++) {
    res->obj_at_put(i, VMString[compilers[i]]);
  }
  return res;
}


oop get_compile_counts_prim(oop) {
  objVectorOop res = Memory->objVectorObj->cloneSize(nstages);
  for (fint i = 0; i < nstages; i++) {
    res->obj_at_put(i, as_smiOop(compileCounts[i]));
  }
  return res;
}


oop get_recompile_limits_prim(oop) {
  objVectorOop res = Memory->objVectorObj->cloneSize(nstages - 1);
  for (fint i = 0; i < nstages - 1; i++) {
    res->obj_at_put(i, as_smiOop(recompileLimits[i]), false);
  }
  return res;
}
/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "conversion.hh"
# include "_conversion.cpp.incl"


void Conversion::doit() {
  if (VerifyBeforeConversion) Memory->verify();
  FlushRegisterWindows();
  lastFrame = currentFrame()->sender();
  convert();
  if (VerifyAfterConversion) Memory->verify();
  returnToSelf(result, sp, nlr, nlrHome, nlrHomeID, sd, isInterpreting);
  ShouldNotReachHere(); 
}




void Conversion::fixConversionStack(pc_t pc, char* sp_) {
  assert(isOnVMStack(lastFrame), "will be overwritten");
  lastFrame->fix_frame( pc, sp_);
}


void Conversion::convert() {
  // Convert the convertFrame into a sequence of frames (with one scope per
  // frame, i.e. no inlining).  Frame must be bottommost frame on stack. 
  
  // do not need ResourceMark (for RegisterLocator) because ConvertFrame has it
  convertFrame = currentProcess->last_self_frame(true, &convertFrame_rl);

  if (isInterpreting) {
    // already interpreted frame
    vdepth = 0;
    sd = convertFrame->send_desc();
    convertFrame_rl = NULL; // deallocating it
    return;
  }
  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  stk = currentProcess->stack();
  char* returnPC = convertFrame->currentPC();
  convertNM = convertFrame->code();
  EventMarker em("converting frame %#lx nm %#lx", convertFrame, convertNM);
  assert(convertNM->contains(returnPC), "return PC not in nmethod");
    if (convertNM->isValid() && convertNM->isDebug()) {
    // already in debug form
    sd = convertFrame->send_desc();
    vdepth = 0;
    convertFrame_rl = NULL;
    return;
  }
  if (PrintFrameConversion) {
    lprintf("*converting frame %#lx (nmethod %#lx)...",
           (long unsigned)convertFrame, (long unsigned)convertNM);
  }
  
  Memory->code->chainFrames();
  assert(convertNM->frame_chain != NoFrameChain, "should be on stack");
  init();
  convertVFrames();
  finish();
  Memory->code->unchainFrames();

  if (PrintFrameConversion) {
    lprintf("done.\n");
  }  
  assert( ((frame*)sp)->is_aligned(), "should be aligned");
  convertFrame_rl = NULL;
# else // defined(FAST_COMPILER) || defined(SIC_COMPILER)
  ShouldNotReachHere();
# endif
}



# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Conversion::build_vfs_to_convert() {
  // build vframe array of frames to convert
  vdepth = convertFrame->vdepth(true);
  vf = NEW_RESOURCE_ARRAY(compiled_vframe*, vdepth+1);
  vf[vdepth] = new_vframe(convertFrame, convertFrame_rl)->as_compiled();
  int32 i;
  for ( i = vdepth-1; i > 0; i--) {
    // when interop w/ interp, immediateSender can return C frame (nil?)
    vf[i] = vf[i+1]->immediateSender()->as_compiled();
    assert(vf[i]->fr == convertFrame, "should be same frame");
  }
}


void Conversion::create_previously_optimized_blocks() {
  // create all blocks which were completely optimized away in convertNM
  blockValues = NULL;
  { abstract_vframe* callee= NULL;
    for (fint i= vdepth; i > 0; i--) {
      vf[i]->createBlocks(callee, blockValues);
      callee= vf[i];
    }
  }
}


void Conversion::retarget_vfs_to_convert(frame* copiedFrame, RegisterLocator* copiedFrame_rl) {
  // build array of vframes to convert (overwrite the optimized vframes)
  vf[vdepth] = new_vframe(copiedFrame, copiedFrame_rl)->as_compiled();
  for (fint i = vdepth-1;  i > 0;  i--) {
    // XXX when interop w/ interp, immediateSender can return C frame (nil?)
    vf[i] = vf[i+1]->immediateSender()->as_compiled();
    assert(vf[i]->fr == copiedFrame, "should be same frame");
  }
}


void Conversion::copy_caller() {
  frame* callerFrame = convertFrame->immediateSelfSender();
  // callerFrame can be NULL e.g. when convertNM = doIt
  // also, callerFrame is only a piece of a frame because we popped the
  // convertLocals; thus we better use a copy of it
  
  // XXXX May get an interpreter frame here, then what?
  
  if (callerFrame == NULL) {
    sd = sendDesc::first_sendDesc();
    nonvols_for_caller = convertFrame_rl->sender()->for_copied_frame(convertFrame->sender());
    vf[0] = NULL;
  }
  else {
    sd = callerFrame->send_desc();
    frame* copied_callerFrame = callerFrame->copy();
    nonvols_for_caller = convertFrame_rl->sender()->for_copied_frame(copied_callerFrame);
    vf[0] = new_vframe(copied_callerFrame, nonvols_for_caller)->as_compiled();
  }
}


void Conversion::init() {
  build_vfs_to_convert();
  create_previously_optimized_blocks();

  oldBlockHome  = convertFrame->block_scope_of_home_frame();
  convertLocals = convertFrame->vfo_locals_of_home_frame();

  // copy the frame to convert
  frame*           copiedFrame    = convertFrame   ->copy();
  RegisterLocator* copiedFrame_rl = convertFrame_rl->for_copied_frame(copiedFrame);
  
  # if TARGET_ARCH != PPC_ARCH  // vdepth fails for copiedFrame on PPC because it has no register locator -- dmu 12/02
    assert(copiedFrame->code() == convertNM &&
           copiedFrame->vdepth(true) == convertFrame->vdepth(true),
           "frame copy doesn't work");
  # endif

  // pop off the frame to be converted; use copiedFrame for the conversion
  // because the original frame will be overwritten
  sp += copiedFrame->frame_size() * oopSize;    // assume stack grows downwards
  
  retarget_vfs_to_convert(copiedFrame, copiedFrame_rl);

  copy_caller();

  nms   = NEW_RESOURCE_ARRAY(         nmethod*, vdepth+1);
  newVF = NEW_RESOURCE_ARRAY( compiled_vframe*, vdepth+1);
  for (fint i = 0; i <= vdepth; i++) { nms[i] = NULL;  newVF[i] = NULL; }

  newFr = NULL;
}


void Conversion::convertVFrames() {  // convert all vframes
  FlagSetting f1(Inline, false);
    
  for (fint i = 1; i <= vdepth; i++) {
    // generate new (unoptimized) nmethod
    nmethod *newNM = nms[i] = newCodeForVframe(i);
    
    // create new stack frame
    bool wasInInterruptCheck= createFrame(i, newNM);
    fixConversionStack_for_vframe_conversion();
    
    // copy values to new frame
    copyVFrame(i, newNM, wasInInterruptCheck);
  }
}


nmethod* Conversion::newCodeForVframe(fint i) {
  // construct lookup and generate nmethod for vf[i]
  // (cannot repeat lookup because it might find different result)
  oop receiver;
  if (vf[i]->is_interpreted()) {
    receiver = vf[i]->receiver();
  } else {
    NameDesc* rnd = vf[i]->get_receiver_name();
    receiver = vf[i]->get_contents(rnd, false);
    if (receiver->is_block()) {
      if (rnd->isBlockValue() || rnd->isMemoizedBlock()) {
        // wasn't a real block in old frame - must substitute real block now
        blockOop block = blockOop(receiver);
        oop clone = blockValues->lookup(block);
        assert(clone, "should have been cloned");
        assert_block(clone, "should be a block");
        receiver = clone;
      }
    }
  }
  compilingLookup* L= new cacheProbingLookup( receiver,
                                              vf[i]->selector(),
                                              vf[i]->delegatee(),
                                              MH_TBD, // method holder
                                              vf[i-1],
                                              sd, 
                                              NULL, // DIDesc
                                              true,  // need a debug method
                                              // gen reusable iff orig reusable
                                              convertNM->reusable()
                                            );
  // Setting the result this way indicates we don't really want a lookup.
  L->setResult(vf[i]->method(), vf[i]->methodHolder_or_map());
  nmethod *new_nm= L->lookupNMethod();
  assert(new_nm->isDebug(), "must be debug method");
  new_nm->save_frame_chain();   // protect from flushing
  return new_nm;
}


void Conversion::copyVFrame(fint i, nmethod *newNM, bool wasInInterruptCheck) {
  // create the new vframe and copy over the values of its locals etc
  // from the old vframe

  // create new vframe

  ScopeDesc* s= newNM->correspondingScopeDesc(vf[i]->desc);
  assert(s && s->s_equivalent(vf[i]->desc), "not the same scope");
  // When investigating a bug, Michael Abd-El-Malek & I (Dave)
  // thought vf[i]->bci() below should really be vf[i]->real_bci()
  // in order to make a faithful copy (e.g. prolog BCI vs. 0).
  // But, we are too afraid to make this change.
  // Maybe copied vframes are supposed to be different from discovered vframes.
  // Anyway, the rest of the system tolerates 0 for prolog just fine.
  // -- dmu 6/03
  newVF[i] = new compiled_vframe(newFr, newNM, s, vf[i]->bci(), newFrRl );
  // copy values from old to new frame

  abstract_vframe* calleeOrNull = i < vdepth ? vf[i + 1] : NULL;

  // XXXX must redo for interp

  newVF[i]->as_compiled()
    ->copyValuesFrom(vf[i]->as_compiled(), 
                     calleeOrNull->as_compiled(),
                     oldBlockHome,
                     blockValues,
                     i==vdepth && wasInInterruptCheck);
  
  // adjust NLR registers if necessary

  if (nlr && oldBlockHome == nlrHome && vf[i]->scopeID() == nlrHomeID) {
    abstract_vframe* newHome = newVF[i]->home();
    nlrHome = newHome->fr->block_scope_of_home_frame();
    nlrHomeID = newHome->scopeID();
  }
  
  if (PrintFrameConversion2  ||  traceV) {
    lprintf("***old vframe:\n");
    vf[i]->print_frame(i);
    lprintf("***new vframe:\n");
    newVF[i]->print_frame(i);
  }
}


void Conversion::ensure_death_of_conversion_nmethods() {
  fint i;
  for ( i = 1; i <= vdepth; i++) {
    if (nms[i] == NULL) continue;
    assert(nms[i]->is_frame_chain_saved(), "should be set");
    nms[i]->clear_frame_chain();
    // Conversion nmethods are flushed as soon as they are off the stack;
    // they can't be cached because their dependencies are empty since
    // they don't do a lookup.
    // Also, DI methods can't be cached because their debug versions don't
    // have the DI prologue (since the conversion doesn't redo the lookup,
    // it can't rebuild the adeps).
    // (However, debug methods created via the normal lookup process are
    // cached.)
    nms[i]->makeZombie();
  }
  if (!convertNM->isValid()) convertNM->makeZombie();
  convertNM->clear_frame_chain();
}


void Conversion::patch_caller() {
  // reinstall return patch if caller of converted nmethod also needs conversion
  // XXX when interop w/ interp, immediateSender can return C frame (nil?)
  abstract_vframe* sender = newVF[1]->immediateSender();
  if (sender) {
    assert(newVF[1]->fr != sender->fr, "should be a different real frame");
    assert(newVF[1]->fr->sender() == sender->fr, "should be the calling frame");
    if (sender->as_compiled()->code->isInvalid()) {
      sender->fr->patch(newVF[1]->fr, true);
    }
  }
  
}


void Conversion::finish() {
  ensure_death_of_conversion_nmethods();
  fix_new_vfs();
  patch_caller();
 
  // convert old frame's vframeOops
  currentProcess->convertVFrameOops(convertFrame, convertLocals, convertNM,
                                    vdepth,
                                    (abstract_vframe**)vf,
                                    (abstract_vframe**)newVF);
}


char* Conversion::copyArgsAndGetContinuationPC(sendDesc *sd_arg) {
  // work out continuationPC, copy outgoing args
  char *continuationPC;
  nmethod* nm= nmethod::findNMethod(sd_arg);
  PcDesc* pcd= nm->containingPcDescOrNULL((char*)sd_arg);
  assert(pcd, "shouldn't be in prologue");
  continuationPC= nm->insts() + pcd->pc;
  // initialize arguments of pending send / prim call
  // because the call is going to be re-executed
  if (this && vdepth) {
    newVF[vdepth]->copyOutgoingArgs(vf[vdepth], oldBlockHome,
                                    blockValues, false);
  } else {
    // no conversion was necessary, expr stack is still there
  }
  return continuationPC;
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Conversion::returnToSelf(oop res, char* self_sparc_fp_or_ppc_sp,
                              bool nlr_arg, frame* nlrHome_arg,
                              int32 nlrHomeID_arg, sendDesc* self_sd,
                              bool isInterpretingArg) {

  // All values are passed on the stack so that we can safely reset
  // the resource mark below ("this" is in the resource area!).
  // Also, this method can be called with this == NULL (in this case, no
  // conversion was performed and we just want to return to the Self
  // program).
  assert(zone::frame_chain_nesting == 0, "frames shouldn't be chained");
  frame* dest_self_fr =
                 this == NULL       ?  currentProcess->last_self_frame(true)
              :  isInterpretingArg  ?  convertFrame  // for interpreter, needs to be the official interp frame
                                    :  fixConversionStack_for_returning_to_self( self_sparc_fp_or_ppc_sp, self_sd );
  
  bool wasUncommon, restartSend;
  int32* uncommonPC;
  currentProcess->prepare_to_return_to_self_after_conversion(dest_self_fr, restartSend, wasUncommon, uncommonPC);
 
  if (  currentProcess->is_done_with_killing_or_deoptimizing(dest_self_fr) ) {
     restartSend = true; // need to restart send at top of stack
     nlr_arg = false;    // peace! stop the senseless killing of innocent frames
     assert(currentProcess->preemptionPending(),  "should stop immediately after send");
  }
  
  if (isInterpretingArg)
    return_to_interpreted_self( dest_self_fr, restartSend,
                                self_sparc_fp_or_ppc_sp,  res, nlrHome_arg,  nlrHomeID_arg);
  else if (nlr_arg)
    nlr_to_compiled_self(res, restartSend, nlrHome_arg, nlrHomeID_arg,
                          self_sd, self_sparc_fp_or_ppc_sp);
  else
    return_to_compiled_self(res, restartSend, nlr_arg,
                            wasUncommon, uncommonPC,
                            dest_self_fr, self_sd,
                            self_sparc_fp_or_ppc_sp);

  ShouldNotReachHere();
}

  
  
// this may be NULL
void Conversion::return_to_interpreted_self(frame* dest_self_fr, bool restartSend,
                                                   char* self_sparc_fp_or_ppc_sp, oop res, frame* nlrHome_arg, int32 nlrHomeID_arg) {
   // a bit slow, for sparc f is just callee of self_sparc_fp_or_ppc_sp, same frame on ppc
    frame* f= currentProcess->stack()
                ->interpreter_frame_for_continuing_from_return_trap();
    assert(f, "must have frame to return to");
    char* continuationPC = f->c_return_pc();
    dest_self_fr->get_interpreter()->set_restartSend(restartSend);
    if (restartSend)
      NLRSupport::reset_have_NLR_through_C();
    ConversionInProgress = false;
    if (this) delete rm; // free all resources
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done
    ContinueNLRAfterReturnTrap( continuationPC, self_sparc_fp_or_ppc_sp, res, nlrHome_arg, nlrHomeID_arg );
    ShouldNotReachHere();
}
 
// this may be NULL
void Conversion::nlr_to_compiled_self( 
                      oop res, bool restartSend, frame* nlrHome_arg, int32 nlrHomeID_arg,
                      sendDesc* self_sd,
                      char* self_sparc_fp_or_ppc_sp) {

  assert(!restartSend, "cannot be uncommon/killing");
  ConversionInProgress = false;
  if (this) delete rm; // free all resources

  # if defined(FAST_COMPILER) || defined(SIC_COMPILER)
    char* continuationPC = (char*)self_sd + sendDesc::non_local_return_offset;
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done with this
    ContinueNLRAfterReturnTrap( continuationPC, self_sparc_fp_or_ppc_sp, res, nlrHome_arg, nlrHomeID_arg );
  # endif
  ShouldNotReachHere();
}


// this may be NULL
void Conversion::return_to_compiled_self( 
                    oop res, bool restartSend, bool nlr_arg,
                    bool wasUncommon, int32* uncommonPC,
                    frame* dest_self_fr, sendDesc* self_sd,
                    char* self_sparc_fp_or_ppc_sp) {

  # if !defined(FAST_COMPILER) && !defined(SIC_COMPILER)
    ShouldNotReachHere(); return 0;
  # else

    char* continuationPC;
    if (restartSend)
      setup_compiled_restart( continuationPC, res, nlr_arg, self_sd, wasUncommon, uncommonPC);
 
    else if (self_sd->isPrimCall()) {
      // not a real inline cache but a primitive call
      // continue after prim call
      char* fn_start = self_sd->jump_addr();
      continuationPC = (char*)self_sd + getPrimCallEndOffset(fn_start);
    }
    else {
      assert(self_sd->verify(), "should be a sendDesc");
      continuationPC = (char*)self_sd + self_sd->endOffset();
    }
    
    ConversionInProgress = false;
    if (this) delete rm; // free all resources

    assert(Byte_Map_Base() == Memory->remembered_set->byte_map_base(),
           "byte map base reg corrupted");
    LOG_EVENT1("continuationPC=%#lx", continuationPC);
    continue_after_return_trap_with_result(res, continuationPC, self_sparc_fp_or_ppc_sp);

  # endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void Conversion::setup_compiled_restart( char*& continuationPC,
                                         oop&   res,
                                         bool nlr_arg,
                                         sendDesc* sd_arg,
                                         bool wasUncommon,
                                         int32* uncommonPC ) {
  if (sd_arg->isPrimCall() && wasUncommon) {
    setup_compiled_restart_for_uncommon_prim(
                continuationPC, res, nlr_arg, sd_arg, uncommonPC );
    return;
  }

  // I don't understand how you could get here w/o the SIC -- dmu 6/99

  stringOop sel = NULL;
  if (this && vdepth) {
    methodMap* mm = (methodMap*)vf[vdepth]->method()->map();
    sel = mm->get_selector_at(vf[vdepth]->bci());
  }
  if (  sel
    &&  sel->is_prim_name()
    && !sel->has__Perform_prefix()
    && !vf[vdepth]->is_primCall()) {

    // primitive failure: just returned from value:With:
    // saved_outregs contains correct args (vframe expr stack gives
    // wrong stack, i.e. prim call args)

    continuationPC = (char*)sd_arg;
    assert(wasUncommon || !sd_arg->isPrimCall(), "should be a send");
    assert(!nlr_arg, "shouldn't be nlr");
    res = get_result();
    LOG_EVENT1("continuing after primitive failure block eval, continuationPC=%#lx",
                continuationPC);
  } 
  else {
    // must redo the send - restart at beginning of call bytecode
    // so that arg registers etc are set correctly
    // must do computation here because of resource mark dealloc

    continuationPC= copyArgsAndGetContinuationPC(sd_arg);
    LOG_EVENT1("continue by redoing send, continuationPC=%#lx",
                continuationPC);
  }
}

void Conversion::setup_compiled_restart_for_uncommon_prim(
                    char*& continuationPC,
                    oop&   res,
                    bool nlr_arg,
                    sendDesc* sd_arg,
                    int32* uncommonPC ) {
  # if !defined(SIC_COMPILER)
    ShouldNotReachHere();
    Unused(continuationPC); Unused(res); Unused(nlr_arg); Unused(sd_arg); Unused(uncommonPC);
  # else
    if (shouldRestartSend(uncommonPC)) {
      // redo the entire primitive call (was inlined [eg _IntAdd:])
      continuationPC= copyArgsAndGetContinuationPC(sd_arg);
      LOG_EVENT1("continuing by redoing primitive, continuationPC=%#lx",
                  continuationPC);
      return;
    }
    // uncommon trap in primitive failure branch (happens only with SIC)
    // do NOT redo the primitive call, just return the failure oop
    oop s = failure_oop_for_restarting_uncommon_prim();    
    assert(s->is_mark(), "should be error mark");
    memOop m= s->memify();
    assert(m->verify_oop(), "bad error oop");
    assert(!nlr_arg, "shouldn't be NLR");
    
    // need to initialize outgoing args; only last arg (fail
    // block) is really needed: if it is a non-literal, the receiver
    // of the fail message is taken from the expr stack
    // NB: don't copy other args because they may be bad oops
    newVF[vdepth]->copyOutgoingArgs(vf[vdepth], oldBlockHome,
                                  blockValues, true);
    res = get_result();
    m= res->memify();
    LOG_EVENT1("continue after uncommon primitive failure %#lx", m);
    
    // continue after prim call
    char* fn_start = sd_arg->jump_addr();
    continuationPC = (char*)sd_arg + getPrimCallEndOffset(fn_start);
  # endif
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "asyncMalloc.hh"
# include "_asyncMalloc.cpp.incl"
  
AsyncAllocator::AsyncAllocator(int32 elemSize, int32 bufSize) {
  size = elemSize; capacity = n = bufSize;
  // the buffer has size 2*bufSize, but we try to keep it half-full
  buf = (char**)::selfs_malloc(sizeof(char*) * bufSize * 2);
  for (int32 i = 0; i < bufSize; i++) buf[i] = (char*)::selfs_malloc(size);
}

AsyncAllocator::~AsyncAllocator() {
  for (int32 i = 0; i < n; i++) ::selfs_free(buf[i]);
  ::selfs_free(buf);
}

char* AsyncAllocator::async_malloc_elem() {
  if (MallocInProgress) {
    if (WizardMode && (capacity >> 3) > n)
      warning("AsyncAllocator: buffer nearly empty");
    if (n == 0) fatal("AsyncAllocator: buffer empty");
    return buf[--n];
  } else {
    balanceBuffer(capacity + 1);
    return buf[--n];
  }     
}

void AsyncAllocator::async_free_elem(void* ptr) {
  balanceBuffer(capacity - 1);
  buf[n++] = (char*)ptr;
}

void AsyncAllocator::balanceBuffer(int32 howMany) {
  while (n < howMany) buf[n++] = (char*)::selfs_malloc(size);
  while (n > howMany) selfs_free(buf[--n]);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "print.hh"
# include "_print.cpp.incl"

void printName(methodMap* mm, oop selector) {
  lprintf(sprintName(mm, selector));
}

const char* sprintName(methodMap* mm, oop selector) {
  // used for debugging only; use static buffer for result string
  static char buf[800];
  const char* sel = selector == NULL ? "" : selector_string(selector);
  if (mm) {
    stringOop file = mm->file();
    smiOop line = mm->line();
    const char *f,   *fn = selector_string(file);
    // drop path name from file name to avoid excessive length
    for (f = fn + strlen(fn); f > fn && *f != '/'; f--) ;
    if (*f == '/') f++;
    if (file->length() > 0) {
      sprintf(buf, "%s (%s:%ld)", sel, f, long(line->value()));
      return buf;
    }
  } 
  return sel;
}

// should factor out last part of this and previous methods:

const char* sprintValueMethod( oop rcvr ) {
  // used for debugging only, if rcvr is block, print out source info for block
  if (!rcvr->is_block_with_code())
    return const_cast<char*>("");
  methodMap* mm = (methodMap*)blockOop(rcvr)->value()->map();
  // get outermost method map
  methodMap* omm = mm;
  for (;;) {
    methodMap* nmm = omm->get_lexical_link_map();
    if (!nmm)
      break;
    omm = nmm;
  }
  stringOop file = omm->file();
  smiOop line = omm->line();
  const char *f,  *fn = selector_string(file);
  // drop path name from file name to avoid excessive length
  for (f = fn + strlen(fn); f > fn && *f != '/'; f--) ;
  if (*f == '/') f++;
  static char buf[800];
  if (file->length() > 0) {
    sprintf(buf, "(%s:%ld)", f, long(line->value()));
    return buf;
  }
  return const_cast<char*>("");
}


void CList::print_short() {
  if (isEmpty()) {
    lprintf("{}");
  } else {
    lprintf("{ ");
    bool first = true;
    for (CListElem* e = head(); e; e = e->next()) {
      if (first) first = false; else lprintf(", ");
      pp_short(e->data());
    }
    lprintf(" }");
  }
}

oop printVMObj_prim(oop rcvr) {
  // rcvr is address of a C++ object
  if (rcvr->is_smi()) {
    VMObj* obj = (VMObj*)smiOop(rcvr)->value();
    obj->print();
  } else {
    return ErrorCodes::vmString_prim_error(BADTYPEERROR);
  }
  return rcvr;
}

void CList::print() {
  if (isEmpty()) {
    lprintf("{}");
  } else {
    lprintf("{ ");
    bool first = true;
    for (CListElem* e = head(); e; e = e->next()) {
      if (first) first = false; else lprintf(", ");
      lprintf("%d", e->data());
    }
    lprintf(" }");
  }
}

void TableElem::print_short() {
  pp_short(key);
  lprintf("=");
  pp_short(value);
}

void TableElem::print() {
  print_short();
}

/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "uncommonBranch.hh"
# include "_uncommonBranch.cpp.incl"


# if !defined(SIC_COMPILER)

  void uncommonBranch_init() {}

  bool handleUncommonTrap() { return false; }

# else //  defined(SIC_COMPILER)


  // uncommon branches are "unimp" instructions, and their immediate field
  // is used to count how many times the uncommon branch was taken.
  // MaxUncommonCount is an upper limit on the count to help detect
  // compiler bugs generating "real" unimplemented instructions.
  const fint MaxUncommonCount = 1000;

  // defined in uncommonBranch_<arch>.cpp
  void setTrapCount(int32* instp, unsigned count);

  extern "C" char* recompileUncommon(char* pc) {
    // called from assembly glue via trap handler; pc contains trap address
    // (can't handle trap in trap handler itself - stack is messed up)
    BlockProfilerTicks bpt(exclude_uncommon_branch);
    ResourceMark rm;
    assert(continuePC == NULL, "should have been reset by glue");
    frame* fr = currentProcess->last_self_frame(false);
    compiled_vframe* vf = new_vframe(fr)->as_compiled();
    nmethod* oldNM = vf->code;
    if (PrintUncommonBranches) {
      lprintf("*uncommon branch in nmethod %#lx \"%s\":\n",
             (long unsigned)oldNM,
             selector_string(oldNM->key.selector));
      vf->print_frame();
    }
    assert(oldNM == nmethod::findNMethod(pc), "should be the same");
    assert(!oldNM->isDI(), "can't handle DI yet");
    vf = vf->top()->as_compiled();
    compiled_vframe* vfs = vf->sender()->as_compiled();
    assert(vfs->fr == fr->sender(), "oops");
    recompilee = oldNM;
    if (Interpret) fatal("XXXX cannot recompile when interping yet");
    char* newInsts = Recompile(vfs->fr->send_desc(), vfs->fr, vf->receiver(),
                               NULL, pc);
    // should replace on stack if in loop - fix this
    // for now, just convert
    assert(currentProcess->isUncommon(), "should be in uncommon mode");
    return first_inst_addr(ReturnTrap2);
  }

  bool handleUncommonTrap() {
    // handle SIC uncommon trap; return false if the trap wasn't a SIC trap
    int32* instp = (int*)InterruptedContext::the_interrupted_context->pc();
    if (!Memory->code->contains(instp)) {
       return false;
    }
    if (!isUnimp(instp)) {
      return false;
    }

    // sanity check: trap must be in an nmethod
    nmethod* trapNM = nmethod::findNMethod(instp);
    if (trapNM->insts() > (char*)instp || trapNM->instsEnd() <= (char*)instp)
      fatal("got trap which isn't in any nmethod");

    unsigned count = trapCount(instp) + 1;
    if (count > MaxUncommonCount) {
      if (WizardMode) error("uncommon count > max");
    }

    assert(!currentProcess->isUncommon(), "shouldn't be in uncommon mode");
    currentProcess->setUncommon((char*)instp);

    // update the counter in the instruction
    if (count < MaxUncommonCount) {
      setTrapCount(instp, count);
      MachineCache::flush_instruction_cache_word(instp);
      assert(trapCount(instp) == count, "setTrapCount failed");
    }

    frame* bottom = InterruptedContext::the_interrupted_context->sp()
      ->make_full_frame_after_trap((char*)instp);

    if (count >= UncommonTrapLimit) {

      // need to recompile the method to eliminate uncommon case
      // easier to do outside of signal handler

      if (!Memory->code->contains(bottom->return_addr())) {
        // sender isn't in Self, must be a doIt nmethod
        warning("shouldn't use uncommon traps in doIt methods");
      } else {

        bottom->set_currentPC(InterruptedContext::the_interrupted_context->pc());
        if (InterruptedContext::the_interrupted_context->next_pc() == first_inst_addr(HandleUncommonTrap)) {
          error1("uncommon trap found in delay slot at pc %#lx.",
                 InterruptedContext::the_interrupted_context->sp());
          lprintf("(Will try to recover from error.)\n");
          InterruptedContext::the_interrupted_context->set_pc(first_inst_addr(HandleUncommonTrap));
          InterruptedContext::the_interrupted_context->set_next_pc(first_inst_addr(HandleUncommonTrap) + 4);
        } else {
          InterruptedContext::set_continuation_address(first_inst_addr(HandleUncommonTrap), true, true);
        }
        return true;
      }
    }
    // deoptimize the frame and continue execution

    bottom->set_currentPC((char*)instp);
    InterruptedContext::the_interrupted_context->set_pc(first_inst_addr(ReturnTrap2));
    InterruptedContext::the_interrupted_context->set_next_pc(first_inst_addr( ReturnTrap2 ) + 4);
    
    return true;
  }


# if  TARGET_OS_VERSION == SOLARIS_VERSION \
   || TARGET_OS_FAMILY == MACOS_FAMILY \
   || TARGET_OS_VERSION == MACOSX_VERSION \
   || TARGET_OS_VERSION == LINUX_VERSION
  void uncommonBranch_init() { }
# elif  TARGET_OS_VERSION == SUNOS_VERSION
  extern "C" {
    void G1_mapLoadHandler();
    void G2_mapLoadHandler();
    void G3_mapLoadHandler();
    void G4_mapLoadHandler();
    void G5_mapLoadHandler();
    
    void O0_mapLoadHandler();
    void O1_mapLoadHandler();
    void O2_mapLoadHandler();
    void O3_mapLoadHandler();
    void O4_mapLoadHandler();
    void O5_mapLoadHandler();
    
    void L0_mapLoadHandler();
    void L1_mapLoadHandler();
    void L2_mapLoadHandler();
    void L3_mapLoadHandler();
    void L4_mapLoadHandler();
    void L5_mapLoadHandler();
    void L6_mapLoadHandler();
    void L7_mapLoadHandler();
  
    void I1_mapLoadHandler();
    void I2_mapLoadHandler();
    void I3_mapLoadHandler();
    void I4_mapLoadHandler();
    void I5_mapLoadHandler();
  }

  void uncommonBranch_init() {
    mapLoadHandler[G0] = NULL;
    mapLoadHandler[G1] = G1_mapLoadHandler;
    mapLoadHandler[G2] = G2_mapLoadHandler;
    mapLoadHandler[G3] = G3_mapLoadHandler;
    mapLoadHandler[G4] = G4_mapLoadHandler;
    mapLoadHandler[G5] = G5_mapLoadHandler;
    mapLoadHandler[G6] = NULL;
    mapLoadHandler[G7] = NULL;
    
    mapLoadHandler[O0] = O0_mapLoadHandler;
    mapLoadHandler[O1] = O1_mapLoadHandler;
    mapLoadHandler[O2] = O2_mapLoadHandler;
    mapLoadHandler[O3] = O3_mapLoadHandler;
    mapLoadHandler[O4] = O4_mapLoadHandler;
    mapLoadHandler[O5] = O5_mapLoadHandler;
    mapLoadHandler[O6] = NULL;
    mapLoadHandler[O7] = NULL;
    
    mapLoadHandler[L0] = L0_mapLoadHandler;
    mapLoadHandler[L1] = L1_mapLoadHandler;
    mapLoadHandler[L2] = L2_mapLoadHandler;
    mapLoadHandler[L3] = L3_mapLoadHandler;
    mapLoadHandler[L4] = L4_mapLoadHandler;
    mapLoadHandler[L5] = L5_mapLoadHandler;
    mapLoadHandler[L6] = L6_mapLoadHandler;
    mapLoadHandler[L7] = L7_mapLoadHandler;
    
    mapLoadHandler[I0] = NULL;
    mapLoadHandler[I1] = I1_mapLoadHandler;
    mapLoadHandler[I2] = I2_mapLoadHandler;
    mapLoadHandler[I3] = I3_mapLoadHandler;
    mapLoadHandler[I4] = I4_mapLoadHandler;
    mapLoadHandler[I5] = I5_mapLoadHandler;
    mapLoadHandler[I6] = NULL;
    mapLoadHandler[I7] = NULL;
  }
# else
  # error what?
  
# endif  // SIC && (OS is Mac or Solaris)

# endif //  defined(SIC_COMPILER)
/* Sun-$Revision: 30.21 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "allocation.hh"
# include "_allocation.cpp.incl"


bool isNull(VMObj* p) {
  return (p == NULL);
}

void VMObj::print()            { print_short();}
void VMObj::print_short()      { lprintf("%#lx", this); }
void VMObj::print_zero() { 
  if (!isNull(this)) {
    VMObj* temp = this;  // Hack to avoid seg faults in optimized version 
    temp->print(); 
  } else {
    lprintf("0x0");
  }
}
void VMObj::print_short_zero() {
  if (!isNull(this)) {
    VMObj* temp = this;  // Hack to avoid seg faults in optimized version 
    temp->print_short(); 
  } else {
    lprintf("0x0");
  }
}

const bool allocatePersistent = false;
// this is simpler than using #ifdefs for code below

void* ResourceObj::operator new(size_t size){
  char* r = (bootstrapping || allocatePersistent) ? 
    (char *) selfs_malloc(size) : allocateResource(size);
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions  &&  size && r == (char*) catchThisOne) breakpoint();
# endif
  return r;
}

void ResourceObj::operator delete(void* p){
  if (p != NULL) {
#   if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  p == (void*) catchThisOne) breakpoint();
#   endif
    if (allocatePersistent) {
      selfs_free(p);
    }
  }
}

void* CHeapObj::operator new(size_t size){
  return (void *) AllocateHeap(size, "operator-new");
}

void CHeapObj::operator delete(void* p){
 assert( !resources.contains((char*)p),
        "CHeapObj should not be in resource area");
 FreeHeap(p);
}

ResourceAreaChunk::ResourceAreaChunk(fint min_capacity,
                                     ResourceAreaChunk* previous) {
  if (PrintResourceChunkAllocation)
    lprintf("allocating new resource chunk %#lx\n", this);
  fint size = max(min_capacity + min_resource_free_size,
                  min_resource_chunk_size);
  bottom = (char*) AllocateHeap(size,"resourceAreaChunk");
  top    = bottom + size;
  initialize(previous);
}

void ResourceAreaChunk::initialize(ResourceAreaChunk* previous) {
  first_free  = bottom;
  prev        = previous;

  _allocated     = capacity() + (prev ? prev->_allocated : 0);
  _previous_used = prev ? (prev->_previous_used + used()) : 0;
}

ResourceAreaChunk::~ResourceAreaChunk() {
  FreeHeap(bottom);
}

void ResourceAreaChunk::print() {
  if (prev) prev->print();
  print_short();
  lprintf(": [%#lx, %#lx), prev = %#lx\n", 
         bottom, top, prev);
}

ResourceArea::ResourceArea() {
  chunk = NULL;
# if GENERATE_DEBUGGING_AIDS
  nesting = 0;
# endif
}

ResourceArea::~ResourceArea() {
  // deallocate all chunks
  ResourceAreaChunk* prevc;
  for (ResourceAreaChunk* c = chunk; c != NULL; c = prevc) {
    prevc = c->prev;
    resources.addToFreeList(c);
  }
}

char* ResourceArea::allocate_more_bytes(int32 size) {
  chunk = resources.new_chunk(size, chunk);
  char* p = chunk->allocate_bytes(size);
  assert(p, "Nothing returned");
  return p;
}

int32 ResourceArea::used() {
  if (chunk == NULL) return 0;
  return chunk->used() + (chunk->prev ? chunk->prev->_previous_used : 0);
}

Resources resources;

int32 Resources::capacity() { return _allocated; }

static int32 rsrc_used;

static void rsrcf1(Process* p) { rsrc_used += p->resource_area.used(); }

int32 Resources::used() {
  int32 old_rsrc_used = rsrc_used;      // to make this reentrant w/ the spy
  rsrc_used = 0;
  processes->processesDo(rsrcf1);
  int32 used = rsrc_used;
  rsrc_used = old_rsrc_used;
  return used;
}

static bool  in_rsrc;
static char* p_rsrc;
static void rsrcf2(Process* p) {
  in_rsrc = in_rsrc || p->resource_area.contains(p_rsrc);
}

bool Resources::contains(char* p) {
  in_rsrc = false;
  p_rsrc = p;
  processes->processesDo(rsrcf2);

# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions)
      for (ResourceAreaChunk* c = freeChunks; c; c = c->prev) {
        if (c->contains(p)) warning("Resources::contains(): p is deallocated!");
      }
# endif
  return in_rsrc;
}

void Resources::addToFreeList(ResourceAreaChunk* c) {
  if (PrintResourceChunkAllocation)
    lprintf("adding chunk %#lx to free list\n", c);
  # if GENERATE_DEBUGGING_AIDS
    if (ZapResourceArea)
      c->clear();
  # endif
  c->prev = freeChunks;
  freeChunks = c;
}

ResourceAreaChunk* Resources::getFromFreeList(fint min_capacity) {
  CSect cs(profilerCollectStackSemaphore); // ensure nprofiler does not mess up free list
  if (!freeChunks) return NULL;

  // Handle the first element specially
  if (freeChunks->capacity() >= min_capacity) {
    ResourceAreaChunk* res = freeChunks;
    freeChunks = freeChunks->prev;
    if (PrintResourceChunkAllocation)
      lprintf("getting resource chunk %#lx from free list\n",
             (long unsigned)res);
    return res;
  }

  ResourceAreaChunk* cursor = freeChunks;
  while (cursor->prev) {
    if (cursor->prev->capacity() >= min_capacity) {
      ResourceAreaChunk* res = cursor->prev;
      cursor->prev = cursor->prev->prev;
      if (PrintResourceChunkAllocation)
        lprintf("getting resource chunk %#lx from free list\n",
               (long unsigned)res);
      return res;
    }
    cursor = cursor->prev;
  }

  // No suitable chunk found
  return NULL;
}

    
ResourceAreaChunk* Resources::new_chunk(fint min_capacity,
                                        ResourceAreaChunk* previous) {
  _in_consistent_state = false;
  ResourceAreaChunk* res = getFromFreeList(min_capacity);
  if (res) {
    res->initialize(previous);
  } else {
        res = new ResourceAreaChunk(min_capacity, previous);
        _allocated += res->capacity();
        // Subtract the size of the resource chunk from the 'C-Heap' area.
        TrackCHeapInMonitor::adjust(-res->capacity());
  }
  _in_consistent_state = true;
  return res;
}

void ResourceAreaChunk::freeTo(char *new_first_free) {
  assert(new_first_free <= first_free, "unfreeing in resource area");
  # if GENERATE_DEBUGGING_AIDS
    if (ZapResourceArea) clear(new_first_free, first_free);
  # endif
  first_free= new_first_free;
}

Resources::Resources() {
 _allocated           = 0;
 _in_consistent_state = true;
}

ResourceMark::ResourceMark() {
  area  = &currentProcess->resource_area;
  chunk = area->chunk;
  top   = chunk ? chunk->first_free : NULL;
  ++area->nesting;
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      if (   !SignalInterface::is_in_map_load()
          &&  SignalInterface::is_on_signal_stack((char*)currentFrame())) {
        // gdb sometimes (rarely) screws up SIGALRM handling during expression
        // evaluation, so this is a warning instead of a fatal.  -Urs

        // HProfiler does this, so silence warning -- Dave 12/03
        static bool haveWarned = false;
        if (!haveWarned) {
          warning("cannot use ResourceMarks in interrupt handlers!");
          if (!hprofiler->inactive()) {
            warning("silencing this warning for HProfiler");
            haveWarned = true;
          }
        }
      }
    }
# endif
  assert(area->nesting > 0, "nesting must be positive");
  if (PrintResourceAllocation) {
    lprintf("setting mark at %#lx\n", top);
  } 
}

ResourceMark::~ResourceMark() {
  assert(area->nesting > 0, "nesting must be positive");
  --area->nesting;
  if (PrintResourceAllocation) {
    lprintf("deallocating to mark %#lx\n", top);
  }
  ResourceAreaChunk *c, *prevc;
  for (c = area->chunk; c != chunk; c = prevc) {
    // deallocate all chunks behind marked chunk
    prevc = c->prev;
    resources.addToFreeList(c);
  }
  area->chunk = c;
  if (c == NULL) {
    top = NULL;
    return;
  }
  c->freeTo(top); 
  if (top == c->bottom) {
    // this chunk is completely unused - deallocate it
    area->chunk = c->prev;
    resources.addToFreeList(c);
  }
}


// Programming convention: never use "delete" on resource-allocated objects;
// if finalization is necessary, add a "finalize" method and make the
// destructor call finalize (to make stack-allocated objects work).  -Urs



int32 wastedBytes = 0;  // sum of wasted bytes on C heap (int. fragmentation
                        // and header info overhead)
                        
caddr_t mallocReserve= NULL;
static const size_t mallocReserveAmount= 1 * 1024 * 1024; // wild guess
bool MallocInProgress = false;

static void MallocFailed(void) { OS::allocate_failed("VM use"); } 


static int32 true_size_of_malloced_obj(int32* p) {
  static const int32 s_offset = 
#   if    TARGET_ARCH == SPARC_ARCH  &&  COMPILER == GCC_COMPILER
      -2;
#   elif  TARGET_ARCH == M68K_ARCH   &&  COMPILER == GCC_COMPILER
      -1;
#   elif  TARGET_ARCH == PPC_ARCH    &&  COMPILER == GCC_COMPILER
      -2;
#   elif  TARGET_ARCH == I386_ARCH   &&  COMPILER == GCC_COMPILER   &&   TARGET_OS_VERSION == MACOSX_VERSION
      -2;
#   elif  TARGET_ARCH == I386_ARCH   &&  COMPILER == GCC_COMPILER   &&    (TARGET_OS_VERSION ==  LINUX_VERSION  ||  TARGET_OS_VERSION == SOLARIS_VERSION)
      -1;
#   else
	# error What is it?
#   endif
#   if  TARGET_OS_VERSION == MACOSX_VERSION
      // this routine breaks sometimes when you kill the scheduler and do prompt start -- dmu 8/1
      if (true) return 0;
# endif

  return p[s_offset] & ~3; // some mallocs use low-order bit as flag
}


// Linux malloc allocates addresses with bit 31 set, this conflicts with Self oop marking
# if TARGET_OS_VERSION == LINUX_VERSION
    # include <malloc.h> // for mallopt
# endif

void malloc_init() {
    # if TARGET_OS_VERSION == LINUX_VERSION
      mallopt(M_MMAP_MAX, 0); // if Linux malloc mmaps, we get bit 31 on, which conflicts with memOop marking
    # endif
    mallocReserve= (caddr_t)malloc(mallocReserveAmount);
    if (mallocReserve == NULL)
        warning("Couldn't reserve enough memory: system is unlikely to run.\n"
                "You should increase the amount of swap space available");
    std::set_new_handler(MallocFailed);
  }
  
  
  void* selfs_malloc(size_t size) {
         if (!MallocInProgress)              ;
    else if (profilerCollectStackSemaphore)  warning("profiler is reentering malloc; hope it will work");
    else {
      MallocInProgress = false;       // because fatal uses printf uses malloc...
      fatal("malloc/free aren't reentrant");
    }
    MallocInProgress = true;              // for hprofiler
    size = max(1, size); // malloc does not seem to like zero, at least on PPC
    char* result = (char*)malloc(size);
  if ((int)result & 0x80000000) fatal("xxxxxxx");
    MallocInProgress = false;
  
    if (result == NULL) {
      warning1("malloc failed to allocate %d bytes", size);
      if (mallocReserve  &&  size <= mallocReserveAmount) {
        MallocInProgress= true;
        free(mallocReserve);
        mallocReserve= NULL;
        result= (char*)malloc(size);
  if ((int)result & 0x80000000) fatal("xxxxxxx");
        MallocInProgress= false;
        warning(
            "Memory for the Self VM is running low; the system may crash\n"
            "soon.  You would be advised to save then exit, and increase the amount\n"
            "of swap space available to Self.  Forcing a garbage collection is\n"
            "unlikely to help and may in fact precipitate a crash.\n"
            "You can force an immediate snapshot with the expression\n"
            "\t'snapshotFileName' _WriteSnapshot"
            );
      } else
        return NULL;
    }
    int32* p = (int32*)result;
    // assert(true_size_of_malloced_obj(p) >= size, "should be rounded size");
    int ts = true_size_of_malloced_obj(p);
  # if GENERATE_DEBUGGING_AIDS
     if (CheckAssertions) {
      // cannot use assert because printf uses malloc() ...
       if (ts != 0  &&  ts < size) breakpoint();
      if (result == (char*)catchThisOne) 
        breakpoint();
     }
  # endif
     if (ts) {
       TrackCHeapInMonitor::adjust(ts);
       wastedBytes += (ts - size);
     }
    return result;
  }
  
  
  void selfs_free(void* ptr) {
    int32* p = (int32*)ptr;
    TrackCHeapInMonitor::adjust(-true_size_of_malloced_obj(p));
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      if (resources.contains((char*)p)) 
        fatal("should not delete resource object");
      // cannot use assert because printf uses malloc() ...
      if ( true_size_of_malloced_obj(p) < 0 
      ||   TrackCHeapInMonitor::allocated() < 0 )
        breakpoint();
      if (ptr == (void*)catchThisOne) 
        breakpoint();
    }
  # endif
    if (MallocInProgress) fatal("malloc/free are't reentrant");
    MallocInProgress = true;              // for hprofiler
    free(ptr);
    MallocInProgress = false;
  }
  
  // Linux: sbrk returns ponters with the high-order bit and Self objects

  # if TARGET_OS_VERSION == LINUX_VERSION
  
    static void *self_morecore(ptrdiff_t n) {
      // try first way, if it doesn't work, try second way
      # if 1
        void* r = sbrk(n);
        return r;
      # else
        static char malloc_heap[ 0x10000000 ];
        static char* malloc_heap_next = malloc_heap;
  
        void* r = (void*)malloc_heap_next;
        malloc_heap_next += n;
        if (malloc_heap_next > &malloc_heap[sizeof(malloc_heap)])
          fatal("ran out of memory");
        return r;
    # endif
  }
  
  void *(*__morecore)(ptrdiff_t __size) = self_morecore;
# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "monitorHooks.hh"

# include "_monitorHooks.cpp.incl"

fint  ShowLookupInMonitor::lookup_nesting = 0;
const char* ShowVMActivityInMonitor::current_activity = NULL;
const char* ShowCompileInMonitor::method_being_compiled = NULL;
const char* ShowCompileInMonitor::old_method_being_compiled = NULL;
const char* ShowCompileInMonitor::current_compiler_name = NULL;
fint* ShowCompileInMonitor::current_compiler_ticks = NULL;
fint  ShowCompileInMonitor::compiler_ticks[2] = {0, 0};
bool  ShowCompileInMonitor::recompiling = false;
int32 TrackCHeapInMonitor::_allocated = 0;
int32 ShowContextSwitchInMonitor::_context_switches = 1; // prevent zero divide


void TrackCHeapInMonitor::adjust(int32 delta) {
  if (bootstrapping)  return;
  _allocated += delta;
  if (allocated() < 0) {
    static bool warned = false;
    if (!warned  &&  TARGET_ARCH == SPARC_ARCH) {
      warning("_allocated went negative");
      warned = true;
    }
    _allocated = 0;
  }
}


int32 TrackCHeapInMonitor::allocated() {
  return _allocated;
}


void TrackCHeapInMonitor::set_allocated(int32 a) { _allocated = a; }


ShowCompileInMonitor::ShowCompileInMonitor(oop selector, const char* compiler, bool optimize) {
    if (TheSpy->is_active()) do_show_compile(selector, compiler, optimize); }

ShowCompileInMonitor::~ShowCompileInMonitor() { method_being_compiled = NULL; }

bool ShowCompileInMonitor::method_changed() { 
  bool r = old_method_being_compiled != method_being_compiled;
  old_method_being_compiled = method_being_compiled;
  return r;
}

ResetMonitor::ResetMonitor() { if (TheSpy != NULL) TheSpy->reset(); } 


// ==============================================================
  
void ShowCompileInMonitor::do_show_compile(oop sel, const char* compiler, bool optimize) {
  method_being_compiled = selector_string(sel);
  current_compiler_name = compiler;
  current_compiler_ticks = &compiler_ticks[optimize];
  assert(optimize == 0 || optimize == 1, "should use true or false");
  recompiling = recompilee != NULL;
}

ShowVMActivityInMonitor::ShowVMActivityInMonitor(const char* what)  { 
  old_activity = current_activity;
  current_activity = what;
}

ShowVMActivityInMonitor::~ShowVMActivityInMonitor() {
  current_activity = old_activity;
}


bool TrackObjectHeapInMonitor::_reserve_changed = true;

void TrackObjectHeapInMonitor::reserve_changed()      {  _reserve_changed = true; }
bool TrackObjectHeapInMonitor::has_reserve_changed()  {  bool r = _reserve_changed;  _reserve_changed = false;  return r; }


void TrackObjectHeapInMonitor::recreate_old_bars() {
  ((SelfMonitor*)TheSpy)->recreate_old_bars();
}

fint TrackObjectHeapInMonitor::new_capacity() { return Memory->new_gen->to_space->capacity(); }
fint TrackObjectHeapInMonitor::old_capacity() { return Memory->old_gen->capacity(); }
fint TrackObjectHeapInMonitor::n_spaces()     { return Memory->old_gen->nSpaces; }
fint TrackObjectHeapInMonitor::old_used()     { return Memory->old_gen->used(); }
fint TrackObjectHeapInMonitor::old_VM_reserved_memory() { return Memory->old_gen->get_VM_reserved_mem(); }
fint TrackObjectHeapInMonitor::old_low_space_threshold() { return Memory->old_gen->getLowSpaceThreshold(); }
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_format_abstract.hh"
# pragma implementation "frame_format.hh"

# include "_frame_format.cpp.incl"


/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "interruptedCtx.hh"
# pragma implementation "asserts.hh"
# pragma implementation "error.hh"

# include "_interruptedCtx.cpp.incl"


static int32 abortLevel = 0;

class InterruptedContext AbortContext;


void InterruptedContext::fatal_menu() {
  // this really belongs in lprintf_fatal but the declarations are a mess
  lprintf("\nVM Version: %d.%d.%d, %s\n", 
          Memory ? (void*)Memory->major_version     : 0, 
          Memory ? (void*)Memory->minor_version     : 0,
          Memory ? (void*)Memory->snapshot_version  : 0,
          vmDate);
  // block all interrupts (e.g. timers)
  continuePC = NULL;
  SignalBlocker* sb = new SignalBlocker(SignalBlocker::block_signals_self_uses);
  
  OS::handle_suspend_and_resume(true);            // set stdin to normal mode

  static char buf[BUFSIZ];
  setbuf(stdin, buf); // Needed by Mac to get input working for menu


  PrintVMMessages = true;
  if (++abortLevel > 3) {
    lprintf("\nError while handling error - hit return to exit >");
    getchar();
    OS::terminate(1);
  }
  int32 iter = 0;
  for (;;) {
    lprintf("\nSelf process %ld on %s has crashed.\nDo you want to:\n",
           long(getpid()), OS::get_host_name());
    lprintf("   1) Quit Self (optionally attempting to write a snapshot)\n");
    lprintf("   2) Try to print the Self stack\n");
    lprintf("   3) Try to return to the Self prompt\n");
    lprintf("   4) Force a core dump\n");
    lprintf("   5) Print the interrupted context registers\n");
    lprintf("Your choice: ");
    
    char c[255];

    if (fgets(c, sizeof(c), stdin) == NULL && !(feof(stdin))) {
      lprintf("\nError while handling error - hit return to exit >");
      getchar();
      OS::terminate(1);
    }

    if (feof(stdin) || iter > 10) {
      print_stack();
      lprintf("\nAborting because of eof(stdin).\n");
      delete sb;
      OS::core_dump();
    }
    switch (c[0]) {
     case '1':
      quit_self();
     case '2':
      print_stack();
      break;
     case '3':
      return_to_prompt(sb);
      ShouldNotReachHere();
     case '4':
      delete sb;
      OS::core_dump();
      break;
     case '5':
      print_registers();
      break;
     default:
      lprintf("\nIllegal choice.\n");
      iter++;
    }
  }
}

void InterruptedContext::quit_self() {
  lprintf("Enter snapshot name (hit return to omit snapshot) > ");
  char c[255];
  c[0] = '\0';
  if (fgets(c, sizeof(c), stdin) == NULL) {
    lprintf("\nError while reading snapshot name - hit return to exit >");
    getchar();
    OS::terminate(1);
  }
  
  if (c[0] == '\n') {
    lprintf("No snapshot specified, will skip this step\n");
  }
  else {
    lprintf("Attempting to write a snapshot to `%s'...", c);
    Memory->write_snapshot(c, NULL, NULL, &Memory->current_sizes);
    lprintf("done.\n");
    lprintf("Note that it cannot be guaranteed that the snapshot is good.\n");
    lprintf("Even if it appears to start without problems, it is prudent\n");
    lprintf("to verify the state of the system after startup using the\n");
    lprintf("_Verify primitive.  If this reports problems, it is not\n");
    lprintf("wise to continue using the snapshot.\n");
    lprintf("Hit return to exit > ");
    if (fgets(c, sizeof(c), stdin) == NULL) {
      lprintf("\nError while reading newline? - hit return to exit >");
      getchar();
      OS::terminate(1);
    }
  }
  currentProcess = NULL;            // to avoid a silly assertion failure 
  OS::terminate(-1);
}


void InterruptedContext::print_stack() {
  FlagSetting wm(WizardMode, true);
  FlagSettingInt spl(StackPrintLimit, 1000);
  if (abortLevel == 1) {
    // try to print the C stack
    AbortContext.print_C_stack();
  }
  if (currentProcess->inSelf()) {
    char* pc = the_interrupted_context ? AbortContext.pc() : NULL;
    frame* vmfr = NULL;
    char* oldPC = NULL;
    if (pc && Memory->code->contains(pc)) {
      // patch stack so that last Self frame is displayed
      vmfr = currentProcess->stack()->first_VM_frame();
      oldPC = vmfr->real_return_addr();
      vmfr->set_real_return_addr(pc);
    }
    currentProcess->stack()->print();
    if (vmfr) vmfr->set_real_return_addr(oldPC);  // restore state
  } else {
    lprintf("\nNo Self method is running.\n");
  }
}


void InterruptedContext::print_C_stack() {
  // print C stack of interrupted process
  frame* f;
  FlushRegisterWindows();
  if (sp()) {
    f = (frame*)sp();
    lprintf("Interrupt at PC %#lx\n", (pc()));
  } else {
    f = currentFrame();
  }
  while (f && !f->is_compiled_self_frame()) {
    lprintf("C frame %#lx  (retPC %#lx)\n", f,
           (long unsigned)(f->real_return_addr()));
    f = f->sender();
  }
}


void InterruptedContext::return_to_prompt(SignalBlocker* sb) {
  if (SignalInterface::is_initializing() || !vmProcess->inSelf()) {
    lprintf("\nCannot return to the prompt - Self is not properly initialized\n");
    return;
  }
  delete sb;
  abortLevel = 0;
  AbortContext.invalidate();
  // Don't bother to zap blocks on stack (too complicated)
  processes->discardAll();
  ShouldNotReachHere();
}


void InterruptedContext::continue_abort_at(char *addr, bool addDummyFrame) {
  // continue abort handling code on normal stack; resume at addr
  // create a dummy frame for the last frame peice so that Self stack trace
  // is complete
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  

  if (addDummyFrame) {
    the_interrupted_context->set_sp(
      the_interrupted_context->sp()
        ->make_full_frame_after_trap(the_interrupted_context->pc()));
  }
  // make sure set_continuation_address works
  the_interrupted_context->set_next_pc(the_interrupted_context->pc() + 4);
  set_continuation_address(addr, true, true);
}



volatile void fatal_handler() {
  error_breakpoint();
  SignalInterface::simulate_fatal_signal();
  OS::terminate(-1);
}


void InterruptedContext::set(self_sig_context_t* scp_arg) {
  must_be_in_self_thread();
  if (scp_arg && is_set())
     fatal2("should not be set already, possibly signals were not blocked,\n"
            "see comment about ApplicationEnhancer in sig_unix.hh,\n"
            "currentTimerSignal = %d, currentNonTimerSignal = %d",
            SignalInterface::currentTimerSignal,
            SignalInterface::currentNonTimerSignal);
  scp = scp_arg ? scp_arg : &dummy_scp;
}


void InterruptedContext::invalidate() {
  // mark as invalid
  set();
}


char* InterruptedContext::pc() { return *pc_addr(); }
void  InterruptedContext::set_pc(void *pc) { *pc_addr() = (char*)pc; }


frame* InterruptedContext::sp() { 
  return !is_set() ? NULL : (frame*) *sp_addr(); } 
  
void InterruptedContext::set_sp(void* sp) { *sp_addr() = (int) sp; }


int InterruptedContext::code_at_pc() {
  return ((int*) pc())[0];
}


class InterruptedContext* InterruptedContext::the_interrupted_context;


void abort_init() {
  InterruptedContext::the_interrupted_context = new InterruptedContext();
  InterruptedContext::the_interrupted_context->set_the_self_thread();
  InterruptedContext::the_interrupted_context->invalidate();
}  
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "nlrSupport.hh"

# include "_nlrSupport.cpp.incl"


// helpers for assembly routines
extern "C" {
  // As we are returning up Self stack (NLR returns)
  // we might return into C++ (via firstSelfFrame_returnPC in runtime.s).
  // Before returning to C++, asm code calls this function,
  // which also sets NLRSupport::_have_NLR_through_C flag.
  oop  capture_NLR_parameters_from_registers(oop result, int32 target_frame, int32 target_ID);

// These are used for communication with the assembly routine because
// there aren't enough temporary registers to hold all this stuff.
// Don't access these this way from C++;

  oop   NLRResultFromC;    // params for ContinueNLRFromC
  int32 NLRHomeFromC;
  int32 NLRHomeIDFromC;
  
  void continue_discard_stack();
}


// Called by assembly glue when a NLR reaches a VM frame on the stack
oop capture_NLR_parameters_from_registers(oop result, int32 targetFrame, int32 targetID) {
  NLRSupport::save_NLR_results(result, targetFrame, targetID);
  // return to assembly glue which then returns from EnterSelf()
  return result;
}


// Called by assembly glue when a DiscardStack reaches a VM frame on the stack
// Should only return when stack is completely traversed
// used by NLRs, etc., that toss the whole stack
void continue_discard_stack() {
  if (currentProcess->selfNesting() > 1) {
    currentProcess->nesting--;
    DiscardStack();
  } else {
    // return to assembly glue which then returns from EnterSelf()
  }
}




bool NLRSupport::_have_NLR_through_C = false;


// accessors for NLR vars above:

oop    NLRSupport::NLR_result_from_C()  { return NLRResultFromC; }
int32  NLRSupport::NLR_home_from_C()    { return NLRHomeFromC; }
int32  NLRSupport::NLR_home_ID_from_C() { return NLRHomeIDFromC; }

void   NLRSupport::set_NLR_result_from_C(oop x)    { NLRResultFromC = x; }
void   NLRSupport::set_NLR_home_from_C(int32 x)    { NLRHomeFromC = x; }
void   NLRSupport::set_NLR_home_ID_from_C(int32 x) { NLRHomeIDFromC = x; }


volatile void NLRSupport::continue_NLR_into_Self(bool remove_patches) {
  processSemaphore = true;
  frame* vmf = currentProcess->stack()->first_VM_frame();
  assert(!vmf->is_self_frame() && vmf->sender()->is_self_frame(),
         "not the right frame");
  if (remove_patches) vmf->remove_patches_up_to_C();
  
  if (vmf->sender()->is_interpreted_self_frame()) 
    continue_NLR_into_interpreted_Self();
  else if (preemptCause == cNonLIFOBlock) 
    continue_NLR_from_compiled_nonLIFO_block(vmf);
  else
    continue_NLR_into_compiled_Self(remove_patches, vmf);
  ShouldNotReachHere(); // should not return to here
}


volatile void NLRSupport::continue_NLR_into_interpreted_Self() {
  set_have_NLR_through_C(); // interp needs this
  frame* f =
    currentProcess->stack()->interpreter_frame_for_continuing_NLR_from_primitive();
  assert(f != NULL, "could not find frame to return to");
  ContinueNLRFromC(f->real_return_addr(), true, false);
  ShouldNotReachHere(); // should not return to here
}


volatile void NLRSupport::continue_NLR_from_compiled_nonLIFO_block(frame* vmf) {
  reset_have_NLR_through_C(); // normally clear this when resuming NLR in compiled code
  // either a trap instruction or a load; skip bottommost frame because
  // it has no proper inline cache to return through
  frame* above = vmf->sender()->sender();
  if (!above->is_self_frame())
    fatal("can't handle block called by VM");
  ContinueNLRFromC( above->return_addr(), false, true);
  ShouldNotReachHere(); // should not return to here
}


volatile void NLRSupport::continue_NLR_into_compiled_Self(bool remove_patches, frame* vmf) {
  reset_have_NLR_through_C(); // normally clear this when resuming NLR in compiled code
  char* ret_addr = vmf->sender()->real_return_addr();
  char* vm_addr  = vmf->return_addr();
  assert(!remove_patches || (ret_addr == vmf->sender()->return_addr() &&
                              vm_addr == vmf->real_return_addr()),
         "patches not removed");
  UsedOnlyInAssert(remove_patches);
  // Need to use two different ways to return:
  // 1) if VM was called through a normal Self inline cache (i.e. lookup)
  // 2)  "  "  "    "       "    " stack ovfl test or other primitive
  // This is because the "prim call inline cache" has a different format.
  // On SPARC, the return pointer is increased at a prim call site so a normal return
  // will skip in the inline cache.
  // All lookup routines are between Low- and HighReturnAddress.
  
  // On PPC, is_Self_IC flag ignored, and do not have to put lookup routines
  // between Low and HighReturnAddress.
  bool is_Self_IC = is_Self_return_address(vm_addr);  
  ContinueNLRFromC(ret_addr, false, is_Self_IC);
  ShouldNotReachHere(); // should not return to here
}


void NLRSupport::save_NLR_results(oop res, int32 targetFrame, int32 targetID) { 
  assert(res == badOop  ||  res == 0  ||  targetFrame != NULL,
         "either aborting/terminating process or have a target");
  set_NLR_result_from_C(res);
  set_NLR_home_from_C(targetFrame);
  set_NLR_home_ID_from_C(targetID);
  NLRSupport::set_have_NLR_through_C();
}


bool NLRSupport::is_bad_home_reference(char* addr) {
  static const int bad_home_range = 1 << 12; /* address range for refs from dead blks */

  int32 iaddr = (int32)addr;
  return (iaddr & Tag_Mask) == Int_Tag &&
         (iaddr >= 0  ?  iaddr  :  -iaddr)  <  bad_home_range;
}


// use NLR mechanism to unwind process stack when killing

void volatile NLRSupport::unwind_stack_to_kill_process(oop res) {
  save_NLR_results(res);
  ResetMonitor sa; // tell the spy
  continue_NLR_into_Self(true); // reuse NLR for abort
  ShouldNotReachHere(); // should not return to here
}


  

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void NLRSupport::non_lifo_abort_from_continuePC() {
  char* nonLifoAbortPC = continuePC;
  continuePC = NULL;
  non_lifo_abort(nonLifoAbortPC);
}


void NLRSupport::non_lifo_abort(pc_t nonLifoAbortPC) {
  if (nonLifoAbortPC == 0)  fatal("nonLifoAbortPC is zero");
  
  // was in compiled code
  // need to flush nmethod of dead block (next invocation could be ok)
  nmethod* deadBlock = nmethod::findNMethod(nonLifoAbortPC);

  // Initialize our return address so that it looks like our caller is C
  // (the "caller" really is the non-lifo block nmethod, but we can't display
  // its stack frame anyway, so we make it look like C).  At the same time,
  // must make it look like our caller was invoked via
  // SendMessage_stub so that abort()
  // returns throudgh the inline cache.
  fix_current_return_address(Memory->code->trapdoors->SendMessage_stub_td());

  Memory->code->chainFrames();
  deadBlock->flush();
  Memory->code->unchainFrames();

  processSemaphore = false; 
  // HACK to avoid assertion in Process:: transfer(), Lars
  currentProcess->nonLifoError();
}


void NLRSupport::fix_current_return_address(char* addr) {
  // make sure this function doesn't get inlined
  FlushRegisterWindows();
  currentFrame()->fix_current_return_address(addr);
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "init.hh"
# include "_init.cpp.incl"

// Add new initialization/finalization methods to the macro below; your
// functions must be parameterless and must be named after the .c file
// they belong to (e.g. "void foo_init(); void foo_exit()" for foo.c).
// The INIT_DO macro defines the initialization sequence; you need not
// add a corresponding entry to EXIT_DO if you don't need finalization.

// processInfo_init used to be after countStub2, 
// but universe makes zone, zone now uses timer, timer needs processInfo
// template(vmStrings_init)     vmStrings is called during universe_init

// abort_init must preceed OS::init on Mac to init InterruptedContext::the_interrupted_context

// mapVtbls_init must preceed universe_init

# define INIT_DO(template)                                                    \
  template(abort_init)                                                        \
  template(kinds_init)                                                        \
  template(process_init)                                                      \
  template(profilers_init)                                                    \
  template(malloc_init)                                                       \
  template(monitor_init)                                                      \
  template(errorCodes_init)                                                   \
  template(sendDesc_init)                                                     \
  template(IntervalTimer_init)                                                \
  template(files_init)                                                        \
  template(opcode_init)                                                       \
  template(prim_init)                                                         \
  template(mapVtbls_init)                                                     \
  template(sic_init)                                                          \
  template(eventlog_init)                                                     \
  template(processInfo_init)                                                  \
  template(universe_init)                                                     \
  template(countStub1_init)  /* now uses trapdoors, so need universe */       \
  template(debug_init)                                                        \
  template(nmethod_init)                                                      \
  template(recompile_init)                                                    \
  template(monitorBar_init)                                                   \
  template(inlining_init)                                                     \
  template(countStub2_init)                                                   \
  template(uncommonBranch_init)                                               \
  template(slotIterator_init)                                                 \
  template(findSlot_init)                                                     \
  
// Mac OS X compiler will not allow ErrorCodes::init above, so use inlines:
inline void    errorCodes_init() {    ErrorCodes::init(); }
inline void      sendDesc_init() {      sendDesc::init(); }
inline void IntervalTimer_init() { IntervalTimer::init(); }
inline void IntervalTimer_exit() { IntervalTimer::exit(); }
 
# define PPC_INIT_DO(template) \
  template(regs_ppc_init)                                                     \
  
# define I386_INIT_DO(template) \
  template(regs_i386_init)                                                     \
  
# define UNIX_INIT_DO(template) \
  template(unixPrims_init)                                                    \
  
# define SPARC_INIT_DO(template) \
  template(regs_sparc_init)                                                   \

# define EXIT_DO(template)                                                    \
  template(files_exit)                                                        \
  template(lprintf_exit)                                                      \
  template(monitor_exit)                                                      \
  template(IntervalTimer_exit)                                               \
  
# define UNIX_EXIT_DO(template)                                               \
  template(unixPrims_exit)                                                    \

# define DEFINE_TEMPLATE(name)    void name();

# if GENERATE_DEBUGGING_AIDS // set to 1 to debug initialization
  # define CALL_TEMPLATE(name)  \
      { \
      if (SPEND_TIME_FOR_DEBUGGING_BY_DEFAULT) fprintf(stderr, "init.cpp: calling %s\n", #name); \
      name(); \
      }
# else
  # define CALL_TEMPLATE(name)      name();
# endif


INIT_DO(DEFINE_TEMPLATE)
# if TARGET_ARCH == SPARC_ARCH
  SPARC_INIT_DO(DEFINE_TEMPLATE)
# elif TARGET_ARCH == PPC_ARCH
  PPC_INIT_DO(DEFINE_TEMPLATE)
# elif TARGET_ARCH == I386_ARCH
  I386_INIT_DO(DEFINE_TEMPLATE)
# endif
# if TARGET_OS_FAMILY == UNIX_FAMILY
  UNIX_INIT_DO(DEFINE_TEMPLATE)
# endif


EXIT_DO(DEFINE_TEMPLATE)
# if TARGET_OS_FAMILY == UNIX_FAMILY
  UNIX_EXIT_DO(DEFINE_TEMPLATE)
# endif

void init_globals() {

# if TARGET_IS_PROFILED
    OS::profile(false);
# endif
  INIT_DO(CALL_TEMPLATE)
# if TARGET_ARCH == SPARC_ARCH
  SPARC_INIT_DO(CALL_TEMPLATE)
# elif TARGET_ARCH == PPC_ARCH
  PPC_INIT_DO(CALL_TEMPLATE)
# elif TARGET_ARCH == I386_ARCH
  I386_INIT_DO(CALL_TEMPLATE)
# endif
# if TARGET_OS_FAMILY == UNIX_FAMILY
  UNIX_INIT_DO(CALL_TEMPLATE)
# endif

if (Assembler::do_the_tests) Assembler::test(); 
}

void exit_globals() {
  static bool destructorsCalled = false;
  if (!destructorsCalled) {
    destructorsCalled = true;
    EXIT_DO(CALL_TEMPLATE);
#   if TARGET_OS_FAMILY == UNIX_FAMILY
    UNIX_EXIT_DO(CALL_TEMPLATE)
#   endif
  }
}


bool do_exit_cleanup = true;

void prepare_to_exit_self() {
  if (do_exit_cleanup) {
#   if TARGET_IS_PROFILED
    moncontrol(false);
#   endif
    exit_globals();
  }
}



/* Sun-$Revision: 30.14 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// Part of the nprofiler creating the self level call graph 

# pragma implementation "nprofiler.copygraph.hh"

# include "_nprofiler.copygraph.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


// Dictionary for mapping a map to an example object.
// Use when enumerating the heap for objects matching a set of maps.

class MapTableEntry : public CHeapObj {
 public:
  MapTableEntry(mapOop m, oop o) { map = m; obj = o; next = NULL; }
  ~MapTableEntry()               { if (next) delete next; }
  mapOop map;
  oop    obj;
  MapTableEntry* next;
};

class MapTable : public ResourceObj {
  // Table for (map,oop) pair
 private:
  static const fint size;
  fint num;

  MapTableEntry **table;

  fint hash(mapOop map) { return (fint) map % size; }

  MapTableEntry* entryFor(fint value) { return table[value]; }

 public:
  MapTable();
  ~MapTable() { delete [] table; }

  // deallocates all MapTableEntries
  void deallocate();

  bool insertIfAbsent(mapOop map, oop obj = NULL);

  void update(mapOop map, oop obj);
  
  bool member(mapOop map);

  oop find(mapOop map);

  friend class map_enumeration;
};

const fint MapTable::size = 3001;

MapTable::MapTable() {
  num = 0;
  table= new MapTableEntry *[size];
  for (fint i = 0; i < size; i++ ) table[i] = NULL;
}

void MapTable::deallocate() {
  for (fint i = 0; i < size ; i++ ) {
    if (table[i]) { delete table[i]; table[i]= NULL; }
  }
}

bool MapTable::insertIfAbsent(mapOop map, oop obj) {
  fint value = hash(map);
  MapTableEntry** entry = &table[value];
  for (entry = &table[value]; *entry; entry = &(*entry)->next) {
    if ((*entry)->map->equal(map)) return false;
  }
  num++;
  *entry = new MapTableEntry(map, obj);
  return true;
}

void MapTable::update(mapOop map, oop obj) {
  for (MapTableEntry* link = table[hash(map)]; link; link = link->next) {
    if (link->map->equal(map)) {
      assert(link->obj == NULL, "Invalid update");
      link->obj = obj;
      return;
    }
  }
  ShouldNotReachHere(); // no element found
}

bool MapTable::member(mapOop map) {
  for (MapTableEntry* link = table[hash(map)]; link; link = link->next) {
    if (link->map->equal(map)) return true;
  }
  return false;
}

oop MapTable::find(mapOop map) {
  for (MapTableEntry* link = table[hash(map)]; link; link = link->next) {
    if (link->map->equal(map)) return link->obj;
  }
  return NULL;
}

class method_node_info;
class block_node_info;
class access_node_info;
class prim_node_info;
class leaf_node_info;
class fold_edge_info;

class graph_creator : public graph_iterator {
 private:
  // Prototypes for the self call graph nodes
  oop method_node_pt;
  oop block_node_pt;
  oop access_node_pt;
  oop prim_node_pt;
  oop leaf_node_pt;
  oop fold_edge_pt;

  // Slot information for the prototypes
  method_node_info* method_pt_info;
  block_node_info*  block_pt_info;
  access_node_info* access_pt_info;
  prim_node_info*   prim_pt_info;
  leaf_node_info*   leaf_pt_info;
  fold_edge_info*   fold_pt_info;

  // Clone and fill operations
  oop clone_method_pt(method_node* n);
  oop clone_block_pt(block_node* n);
  oop clone_prim_node_pt(prim_node* n);
  oop clone_access_node_pt(access_node* n);
  oop clone_leaf_node_pt(leaf_node* n);
  oop clone_fold_edge_pt(fold_edge* e);

  oop _root;
  oop _unknown_oop;

  // Variables used during the graph traversal
  oop parent;
  oop next_parent;
  int index;
  bool out_of_memory;

  MapTable* graph_maps;

  OopBList*        oop_path;
  tree_edge_BList* node_path;

 public:
  graph_creator(call_graph_edge* e, MapTable* gm,   
                oop method_pt,  oop block_pt, oop access_pt,
                oop prim_pt,    oop leaf_pt,  oop fold_pt, oop unknown_oop,
                smi cutoff_pct);

  // returns  the root of the self call graph
  oop root() { return _root;} 

  bool ran_out_of_memory() { return out_of_memory; };

  // Creates an object from the map. All data slots will be filled with
  // _unknown_oop
  oop oop_from_map(oop m);

  void do_edge(call_graph_edge* e);
  int  do_sub_edges(call_graph_edge* e);  
};

// Structure to keep slot information for call graph nodes
class slot_info {
 private:
  smi _offset;
  static const smi invalid_offset;
 public:
  smi offset() { return _offset; }
  void init(oop node_pt, const char* selector);
  bool is_present() { return _offset >= 0; }
  void set(oop node_oop, oop value) {
    Memory->store(oopsOop(node_oop)->oops(offset()), value);
  }
};

const smi slot_info::invalid_offset = -1;

void slot_info::init(oop node_pt, const char* selector) {
  // Check if node_pt has data_slot selector and save offset in _offset.
  stringOop sel = Memory->string_table->lookup(selector, strlen(selector));
  slotDesc* sd = node_pt->find_slot(sel);
  if (sd && sd->type->is_obj_slot()) {
    _offset = smiOop(sd->data)->value();
  } else {
    _offset = invalid_offset;
  }
}

# define FILL_SLOT(N, V) if (N.is_present()) N.set(node_oop,V)

class fold_edge_info {
 public:
  fint _num;
  slot_info fold, average, min, max;  

  fold_edge_info(oop node_pt) {
    _num = 0;
    fold.init(node_pt,    "fold");
    average.init(node_pt, "average");
    min.init(node_pt,     "min");
    max.init(node_pt,     "max");
  }

  void fill(oop node_oop, fold_edge* e, oop fold_oop) {
    _num++;
    FILL_SLOT(fold,    fold_oop);
    FILL_SLOT(average, as_floatOop(e->average()));
    FILL_SLOT(min,     as_smiOop(e->min()));
    FILL_SLOT(max,     as_smiOop(e->max()));
  }
};

class node_info {
 public:
  fint _num;
  slot_info user_time, optimized, bytes, blocks;
# if TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
  slot_info accuracy;
# endif

  node_info(oop node_pt) {
    _num = 0;
    user_time.init(node_pt, "time");
    optimized.init(node_pt, "optimized");
    bytes.init(node_pt,     "bytes");
    blocks.init(node_pt,    "blocks");
# if TARGET_OS_VERSION_FOR_NPROF_TIMER == SUNOS_VERSION
    accuracy.init(node_pt,  "accuracy");
# endif
  }

  void fill(oop node_oop, call_graph_node* n);
};

void node_info::fill(oop node_oop, call_graph_node* n) {
  _num++;
  FILL_SLOT(user_time, as_floatOop(n->user_time));
  FILL_SLOT(optimized, as_floatOop(n->optimized()));
  FILL_SLOT(bytes,     as_smiOop(n->bytes_allocated));
  FILL_SLOT(blocks,    as_smiOop(n->block_clones));
# if TARGET_OS_VERSION_FOR_NPROF_TIMER == SUNOS_VERSION
  FILL_SLOT(accuracy,  as_floatOop(n->accuracy()));
# endif
}

class method_node_info : public node_info {
 public:
  slot_info selector, receiver, methodHolder, method;  
  
  method_node_info(oop node_pt) : node_info(node_pt) {
    selector.init(node_pt,      "selector");
    receiver.init(node_pt,      "receiver");
    methodHolder.init(node_pt, "methodHolder");
    method.init(node_pt,        "method");
  }

  void fill(graph_creator* c, oop node_oop, method_node* n);
};

void method_node_info::fill(graph_creator* c, oop node_oop, method_node* n) {
  node_info::fill(node_oop, n);
  FILL_SLOT(selector,      n->selector());
  FILL_SLOT(receiver,      c->oop_from_map(n->receiverMapOop()));
  FILL_SLOT(method,        (oop) n->method()->as_mirror());
  FILL_SLOT(methodHolder, c->oop_from_map(n->methodHolder_map()));
}

class block_node_info : public node_info {
 public:
  slot_info selector, method, outer;
  
  block_node_info(oop node_pt) : node_info(node_pt) {
    selector.init(node_pt, "selector");
    method.init(node_pt,   "method");
    outer.init(node_pt,    "outer");
  }

  void fill(oop node_oop, block_node* n, oop outer_oop);
};

void block_node_info::fill(oop node_oop, block_node* n, oop outer_oop) {
  node_info::fill(node_oop, n);
  FILL_SLOT(selector, n->selector());
  FILL_SLOT(method,   (oop) n->method()->as_mirror());
  FILL_SLOT(outer,    outer_oop);
};

class access_node_info : public node_info {
 public:
  slot_info selector, receiver, methodHolder;  
  
  access_node_info(oop node_pt) : node_info(node_pt) {
    selector.init(node_pt,      "selector");
    receiver.init(node_pt,      "receiver");
    methodHolder.init(node_pt, "methodHolder");
  }
  void fill(graph_creator* c, oop node_oop, access_node* n);
};

void access_node_info::fill(graph_creator* c, oop node_oop, access_node* n) {
  node_info::fill(node_oop, n);
  FILL_SLOT(selector,      n->selector());
  FILL_SLOT(receiver,      c->oop_from_map(n->receiverMapOop()));
  FILL_SLOT(methodHolder, c->oop_from_map(n->methodHolder_map()));
}

class prim_node_info : public node_info {
 public:
  slot_info selector;

  prim_node_info(oop node_pt) : node_info(node_pt) {
    selector.init(node_pt, "selector");
  }

  void fill(oop node_oop, prim_node* n);
};

void prim_node_info::fill(oop node_oop, prim_node* n) {
  node_info::fill(node_oop, n);
  FILL_SLOT(selector, n->selector());
}

class leaf_node_info : public node_info {
 public:
  leaf_node_info(oop node_pt) : node_info(node_pt) { }
};

graph_creator::graph_creator(call_graph_edge* e, MapTable* gm,     
                             oop method_pt,  oop block_pt, oop access_pt,
                             oop prim_pt,    oop leaf_pt,  oop fold_pt,
                             oop unknown_oop, smi cutoff_pct) 
  : graph_iterator(e) {

  graph_maps = gm;
  cutoff = top()->callee->user_time *  (float(cutoff_pct) / 100.0);


  // Save the prototypes
  method_node_pt = method_pt;
  block_node_pt  = block_pt;
  access_node_pt = access_pt;
  prim_node_pt   = prim_pt;
  leaf_node_pt   = leaf_pt;
  fold_edge_pt   = fold_pt;

  _unknown_oop   = unknown_oop;
  
  // Compute the slot info for all proto
  method_pt_info = new method_node_info(method_node_pt);
  block_pt_info  = new block_node_info(block_node_pt);
  access_pt_info = new access_node_info(access_node_pt);
  prim_pt_info   = new prim_node_info(prim_node_pt);
  leaf_pt_info   = new leaf_node_info(leaf_node_pt);
  fold_pt_info   = new fold_edge_info(fold_edge_pt);

  // path from the root in VM nodes and self nodes.
  node_path = new tree_edge_BList(200, true);
  oop_path  = new OopBList(200, true);

  out_of_memory = false;
  _root = NULL;
  parent = next_parent = NULL;
  index = 0;
}

void graph_creator::do_edge(call_graph_edge* e) {
  oop node_oop;
  if (e->is_fold_edge()) {
    node_oop = clone_fold_edge_pt((fold_edge*) e);
  } else {
    call_graph_node* n = e->callee;
    if (n->is_method_node()) {
      node_oop = clone_method_pt((method_node*) n);
    } else if (n->is_block_node()) {
      node_oop = clone_block_pt((block_node*) n);
    } else if (n->is_prim_node()) {
      node_oop = clone_prim_node_pt((prim_node*) n);
    } else if (n->is_access_node()) {
      node_oop = clone_access_node_pt((access_node*) n);
    } else if (n->is_leaf_node()) {
      node_oop = clone_leaf_node_pt((leaf_node*) n);
    } else {
      fatal("unexpected node type in call graph");
    }
  }
  if (node_oop == failedAllocationOop) {
    out_of_memory= true;
    return;
  }
  if (parent) {
    assert(parent->is_objVector(), "should be an objVector");
    // fill in an element in the parent vector
    objVectorOop p = objVectorOop(parent);
    assert( p->obj_at(index * 2    ) == NULL, "already there\n");
    assert( p->obj_at(index * 2 + 1) == NULL, "already there\n");
    p->obj_at_put(index * 2,     as_smiOop(e->bci));
    p->obj_at_put(index * 2 + 1, node_oop);
    index++;
  } 
  else {
    assert(_root == NULL, "_root already set");
    _root = node_oop;
  }
  
  next_parent = node_oop;
}


int graph_creator::do_sub_edges(call_graph_edge* e) {
  int edges = 0;

  if ( !ran_out_of_memory()  &&  e->callee->edges ) {
    int old_index  = index;
    oop old_parent = parent;

    parent = next_parent;
    node_path->push(e);
    oop_path->push(parent);

    index = 0;
    edges = graph_iterator::do_sub_edges(e);

    oop_path->pop();
    node_path->pop();
    index  = old_index;
    parent = old_parent;
  }
  return edges;
}

oop graph_creator::clone_method_pt(method_node* n) {
  int size = n->num_of_edges(cutoff);
  oop oop_node= method_node_pt->cloneSize(size * 2, CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;
  method_pt_info->fill(this, oop_node, n);
  return oop_node;
}

oop graph_creator::clone_block_pt(block_node* n) {
  int size = n->num_of_edges(cutoff);
  oop oop_node= block_node_pt->cloneSize(size * 2, CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;

  // find outer most lexical node
  oop o = n->outer_method();
  oop lex_oop = NULL;
  for (int i= node_path->length() - 1; i >= 0 && !lex_oop ; i--) {
    if (node_path->nth(i)->callee->is_method_node()) {
      method_node* mn = (method_node*) node_path->nth(i)->callee;
      if (mn->method() == o) lex_oop = oop_path->nth(i);
    }
  }

  if (!SpendTimeForDebugging) 
    assert(TARGET_ARCH == PPC_ARCH  ||  lex_oop,
           "lexical node should be on stack (but PPC may miss frame next to bottom)");
  else if (TARGET_ARCH != PPC_ARCH  &&  !lex_oop)  { 
    lprintf("\n\nlexical node should be on stack\n"); 
    lprintf("\n\nselector:\n");
    n->selector()->print();
    lprintf("\nnmethod\n");
    n->method()->print(); 
    lprintf("\n\nouter method\n");
    n->outer_method()->print();
    for (int i= node_path->length() - 1;  i >= 0 && !lex_oop;  --i) {
      lprintf("node %d in path:\n", i);
      if (node_path->nth(i)->callee->is_method_node()) {
        method_node* mn = (method_node*) node_path->nth(i)->callee;
        lprintf("\nselector\n");
        mn->selector()->print();
        lprintf("\nreceiverMapOop\n");
        mn->receiverMapOop()->print();
        lprintf("\nprint_map\n");
        mn->receiverMap()->print_map();
        lprintf("\nmethod\n");
        mn->method()->print();
        lprintf("\n\n");
      }
    }
    warning1("method = 0x%x\n\n", o); 
  }

  block_pt_info->fill(oop_node,  n,  lex_oop ? lex_oop : Memory->nilObj);
  return oop_node;
} 

oop graph_creator::clone_prim_node_pt(prim_node* n) {
  int size = n->num_of_edges(cutoff);
  oop oop_node= prim_node_pt->cloneSize(size * 2, CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;
  prim_pt_info->fill(oop_node, n);
  return oop_node;
}

oop graph_creator::clone_access_node_pt(access_node* n) {
  oop oop_node= access_node_pt->clone(CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;
  access_pt_info->fill(this, oop_node, n);
  return oop_node;
}

oop graph_creator::clone_leaf_node_pt(leaf_node* n) {
  oop oop_node= leaf_node_pt->clone(CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;
  leaf_pt_info->fill(oop_node, n);
  return oop_node;
}

oop graph_creator::clone_fold_edge_pt(fold_edge* e) {
  oop oop_node= fold_edge_pt->clone(CANFAIL);
  if (oop_node == failedAllocationOop) return failedAllocationOop;

  // find the fold node
  oop fold_oop = NULL;
  for (int i = node_path->length() - 1; i >= 0 && !fold_oop ; i--) {
    if(node_path->nth(i)->callee == e->callee) fold_oop = oop_path->nth(i);
  }
  assert(fold_oop,"fold node should be on stack");

  fold_pt_info->fill(oop_node, e, fold_oop);
  return oop_node;
}

oop graph_creator::oop_from_map(oop m) {
  assert( m->is_map(), "Expecting a map");
  mapOop mo = (mapOop) m;

  Map* map = mo->map_addr();
  if (map->is_smi() || map->is_float()) return map->dummy_obj(_unknown_oop);
  assert(map->is_slots() || map->is_block(), "expected to be slots");

  // A hash table would help speedup this search.
  oop obj = graph_maps->find(mo);
  if (obj) return obj;

  // Create an object based on the map.
  obj = map->dummy_obj(_unknown_oop);
  if (!graph_maps->insertIfAbsent(mo, obj)) {
    graph_maps->update(mo, obj);
  }
  return obj;
}

class map_collector : public graph_iterator {
 protected:
  MapTable* graph_maps;
  void add_map(oop mo);
 public:
  map_collector(call_graph_edge* e, MapTable* gm);
  void do_edge(call_graph_edge* e);
};

map_collector::map_collector(call_graph_edge* e, MapTable* gm)
: graph_iterator(e) {
  graph_maps = gm;
}
 
void map_collector::add_map(oop mo) {
  static bool warned = false;
  if (CheckAssertions  &&  !warned  &&  mo == NULL) {
    warning("collector::add_map: NULL mo (should never happen)\n");
    warned = true;
    return;
  }
  assert(mo->is_map(), "should be a map");
  Map* map = ((mapOop) mo)->map_addr();
  if (map->is_smi()   ||
      map->is_float() ||
      map->is_block() ) return; 

  graph_maps->insertIfAbsent((mapOop) mo);
}

void map_collector::do_edge(call_graph_edge* e) {
  if (!e->is_fold_edge()) {
    call_graph_node* n = e->callee;
    if (n->is_method_node()) {
      add_map(((method_node*) n)->receiverMapOop());
      add_map(((method_node*) n)->methodHolder_map());
    } else if (n->is_access_node()) {
      add_map(((access_node*) n)->receiverMapOop());
      add_map(((access_node*) n)->methodHolder_map());
    }
  }
}

class map_enumeration : public enumeration {
 public:
  MapTable* graph_maps;
  map_enumeration(MapTable* gm);
  void enumerate();
  void add_obj(oop obj);
  void filter_match(oopsOop obj, oop* matching_cell, smi targetNo) {
    Unused(obj); Unused(matching_cell); Unused(targetNo);
    ShouldNotCallThis(); }
  void filter_map(mapOop obj) {
    Unused(obj); ShouldNotCallThis(); }
};

map_enumeration::map_enumeration(MapTable* gm)
 : enumeration((unsigned long)AllBits) {
  graph_maps = gm;
  // add all maps in gm
  for (fint i = 0; i < gm->size; i++) {
    for (MapTableEntry* link = gm->table[i]; link; link = link->next) {
      add_map(link->map->map_addr());
    }
  }
}

void map_enumeration::enumerate() {
  if (map_count > 0) {
    pack_maps();
    Memory->enumerate_families(this);
    processes->enumerate_families(this);
  }
}

void map_enumeration::add_obj(oop obj) {
  assert(graph_maps->member(obj->map()->enclosing_mapOop()), "map should be known");
  if (!graph_maps->find(obj->map()->enclosing_mapOop())) {
    if (obj->is_block()) {
      // TO BE FIXED WHEN THE SIC IS BETTER TO KILL BLOCK OOPS WHEN
      // A SCOPE TREMINATE. 3/31/93 LB.
      // Clone and kill the copy to avoid the problem.
      obj = obj->clone(); 
      blockOop(obj)->kill_block();
    }
    graph_maps->update(obj->map()->enclosing_mapOop(), obj);
  }
}

static map_collector* _collector;
static void cont_collect() { _collector->do_it(); }

static graph_creator* _creator;
static void cont_copy() { _creator->do_it(); }

oop Profiler::copy_call_graph(oop method_pt, oop block_pt, oop access_pt,
                              oop prim_pt,   oop leaf_pt,  oop fold_pt,
                              oop unknown_oop,
                              smi cutoff_pct) {
  ResourceMark rm;

  if ( ProfilerIgnoreCallGraph  ||  !root->edges ) {
    // If the call graph is ignored the root contains the
    // accumulated timing and allocation information
    
    if (!this->root->edges)
      warning("Profiler::copy_call_graph: root edges are NULL, profiled program probably was too brief");
    
    oop node = leaf_pt->clone(CANFAIL);
    if (node == failedAllocationOop) return failedAllocationOop;
    leaf_node_info* leaf_pt_info = new leaf_node_info(leaf_pt);
    leaf_pt_info->fill(node, root);
    return node;
  }
  
  if ( PrintProfiling ) {
    // Old debugging code; checking for multiple roots -- dmu 2/04
    int c; float t;
    graph_totaller::compute_totals(root_edge, c, t);
    lprintf("in copy_call_graph: count = %d, time = %g\n", c, t);
    lprintf(" and root and rootedge are ");
    {
      for (call_graph_edge* e = root_edge;  e; e = e->next) {
        e->callee->print_node(), lprintf("\n");
          for (call_graph_edge* ee = e->callee->edges;  ee; ee = ee->next)
            lprintf("     "),  ee->callee->print_node(), lprintf("\n");
      }
    }
    lprintf("\n\n");
  }

  // Prepare for the enumeration by collecting all referred maps in the graph.
  MapTable graph_maps; 
  { 
    ResourceMark rm2;
    map_collector collector(this->root->edges, &graph_maps);
    _collector = &collector;
    switchToVMStack(cont_collect);
  }

  // Enumerate the maps
  {
    ResourceMark rm2;
    map_enumeration enumeration(&graph_maps);
    enumeration.enumerate();
  }
  
  graph_creator creator(this->root->edges, &graph_maps,
                        method_pt, block_pt, access_pt,
                        prim_pt,   leaf_pt,  fold_pt, unknown_oop, cutoff_pct);

  // Do the recursive graph traversal on the VM stack.
  _creator = &creator;
  switchToVMStack(cont_copy);

  graph_maps.deallocate();
  
  return creator.ran_out_of_memory() ? failedAllocationOop : creator.root();
}

class tick_info {
 public:
  slot_info inSemaphore, inSelf, inLookup,
            inPrim, inProfiler, inConversion;

  tick_info(oop tick_pt) {
    inSemaphore.init(tick_pt, "inSemaphore");
    inSelf.init(tick_pt,      "inSelf");
    inLookup.init(tick_pt,    "inLookup");
    inPrim.init(tick_pt,      "inPrim");
    inProfiler.init(tick_pt,  "inProfiler");
    inConversion.init(tick_pt,"inConversion");
  }

  void fill(oop node_oop, Profiler *p) {
    FILL_SLOT(inSemaphore, as_smiOop(p->sem_ticks.num));
    FILL_SLOT(inSelf,      as_smiOop(p->self_ticks.num));
    FILL_SLOT(inLookup,    as_smiOop(p->lookup_ticks.num));
    FILL_SLOT(inPrim,      as_smiOop(p->prim_ticks.num));
    FILL_SLOT(inProfiler,  as_smiOop(p->profiler_ticks.num));
    FILL_SLOT(inConversion,as_smiOop(p->conversion_ticks.num));
  }
};

oop Profiler::copy_tick_info(oop tick_pt) {
  // Make a copy of the prototype.
  oop tick_node= tick_pt->clone(CANFAIL);
  if (tick_node == failedAllocationOop) return failedAllocationOop;

  tick_info  t(tick_pt);

  t.fill(tick_node, this);
  return tick_node;
}

class time_info {
 public:
  slot_info runtime, collect, build, 
            monitor, scavenge, garbageCollect, 
            compile, recompile, uncommon, 
            methodFlush, methodCompact;

  time_info(oop time_pt) {
    runtime.init(time_pt,        "runtime");
    collect.init(time_pt,        "collect");
    build.init(time_pt,          "build");

    monitor.init(time_pt,        "monitor");
    scavenge.init(time_pt,       "scavenge");
    garbageCollect.init(time_pt, "garbageCollect");

    compile.init(time_pt,        "compile");
    recompile.init(time_pt,      "recompile");
    uncommon.init(time_pt,       "uncommon");

    methodFlush.init(time_pt,    "methodFlush");
    methodCompact.init(time_pt,  "methodCompact");
  }

  void fill(oop node_oop, Profiler *p) {
    FILL_SLOT(runtime,        as_floatOop(p->root->user_time));

#   if  TARGET_OS_VERSION_FOR_NPROF_TIMER == SOLARIS_VERSION
    FILL_SLOT(collect,
              as_floatOop(p->collect_time.milli_secs_as_float()));
    FILL_SLOT(build,
              as_floatOop(p->prof_time.milli_secs_as_float()));
#   elif  TARGET_OS_VERSION_FOR_NPROF_TIMER ==  SUNOS_VERSION
    FILL_SLOT(collect,        as_floatOop(p->collect_timer.millisecs()));
    FILL_SLOT(build,          as_floatOop(p->prof_timer.millisecs()));
#   endif
    FILL_SLOT(monitor,        as_floatOop(p->monitor_tick_time));
    FILL_SLOT(scavenge,       as_floatOop(p->scavenging_time));
    FILL_SLOT(garbageCollect, as_floatOop(p->garbage_collection_time));

    FILL_SLOT(compile,        as_floatOop(p->compile_time));
    FILL_SLOT(recompile,      as_floatOop(p->recompile_time));
    FILL_SLOT(uncommon,       as_floatOop(p->uncommon_branch_time));

    FILL_SLOT(methodFlush,    as_floatOop(p->nmethod_flush_time));
    FILL_SLOT(methodCompact,  as_floatOop(p->nmethod_compact_time));
  }
};

oop Profiler::copy_time_info(oop time_pt) {
  // Make a copy of the prototype
  oop time_node= time_pt->clone(CANFAIL);
  if (time_node == failedAllocationOop) return failedAllocationOop;

  time_info  t(time_pt);

  t.fill(time_node, this);
  return time_node;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "monitorPieces.hh"

# include "_monitorPieces.cpp.incl"



const int one_billion = 1000 * 1000 * 1000;


ProcessTime ProcessTime::operator-(const ProcessTime& t) {
  ProcessTime &tt= (ProcessTime&)t;
  ProcessTime res;
  if (tt.nano_secs() > nano_secs()) {
    res._secs      = secs()      - tt.secs() - 1;
    res._nano_secs = one_billion - tt.nano_secs() + nano_secs();
  } else {
    res._secs      = secs()      - tt.secs();
    res._nano_secs = nano_secs() - tt.nano_secs();
  }
  return res;
}


ProcessTime ProcessTime::operator+(const ProcessTime& t) {
  ProcessTime &tt= (ProcessTime&)t;
  ProcessTime res;
  res._secs      = secs()      + tt.secs();
  res._nano_secs = nano_secs() + tt.nano_secs();
  if (res.nano_secs() >= one_billion) {
    res._nano_secs -= one_billion;
    res._secs      += 1;
  }
  return res;
}




OSActivityMonitor::SystemState OSActivityMonitor::activity() {
  long inblock  = ProcessInfo::block_input_operations();
  long outblock = ProcessInfo::block_output_operations();
  ProcessInfo::update();
 
   inblock  = ProcessInfo:: block_input_operations() -  inblock;
  outblock  = ProcessInfo::block_output_operations() - outblock;

  if (inblock && outblock)    return disk_IO;
  if (inblock)                return disk_in;
  if (outblock)               return disk_out;
  if (processes->isIdle())    return idle;
  
  if ( MonitorCallsToVM::in_read_trap()   )   return reading;
  if ( MonitorCallsToVM::in_write_trap()  )   return writing;
  if ( MonitorCallsToVM::in_system_trap() )   return in_os;
                                              return nothing;
}


const char* OSActivityMonitor::state_string(OSActivityMonitor::SystemState s) {
  switch (s) {
   case disk_IO:  return " disk I/O ";
   case disk_in:  return " disk in ";
   case disk_out: return " disk out ";
   case idle:     return " idle ";
   case reading:  return " read ";
   case writing:  return " write ";
   case in_os:    return " OS ";
   default:       return "";
  }
};


fint LoadLevelMonitor::load_level = 100;

void LoadLevelMonitor::initialize() { 
  compute_load_level();
}

 

bool  ResourceAreaMonitor::is_consistent() { return resources.in_consistent_state(); }
int32 ResourceAreaMonitor::used()          { return resources.used(); }



Indicator::Indicator() {
  visible = false;
}


void Indicator::reposition(fint X, fint Y, fint W) {
  x = X; y = Y; w = W;
}


void Indicator::show(const char *text) {
  if (TheSpy->is_active()) {
    TheSpy->mw()->pw->draw_text(text, x, y+TheSpy->mw()->font_height()-3);
    visible = true;
  }
}


void Indicator::draw_under() {
  TheSpy->mw()->pw->clear_rectangle(x, y, w, TheSpy->ind_h());
}


void Indicator::hide() {
  if (TheSpy->is_active() && visible) {
    draw_under();
    visible = false;
  }
}


SharedIndicator::SharedIndicator()
                     : Indicator() {
  state = -1;
}


void SharedIndicator::show(const char *text, fint new_state) {
  if (state != new_state || !TheSpy->incremental) {
    if (visible) draw_under();
    Indicator::show(text);
    state = new_state;
  }
}


void SharedIndicator::hide() {
  Indicator::hide();
  state = -1;
}
  
ValueIndicator::ValueIndicator(const char* t, bool sum, fint d, fint n, fint off) {
  text = OS::strdup(t); showSum = sum; lastVal = -1; digits = d; offset = off;
  data = new SlidingAverage(n);
}


void ValueIndicator::printWithCommas( char* s, fint length, fint offset, int32 n) {
  // format n into s as "999,999" (length digits), right-justified at end-offset
  const char* digits = "0123456789";
  assert(n >= 0, "number must be positive");
  fint pos = strlen(s) - 1 - offset;
  assert(length <= pos, "string too short");
  fint i = 0;
  do {
    if (i == 3 || i == 6 || i == 9) s[pos--] = ',';
    s[pos--] = digits[n % 10];
    n = n / 10;
    ++i;
  } while (n && i < length);
  while (i < length) {
    if (i == 3 || i == 6 || i == 9) s[pos--] = ' ';
    s[pos--] = ' ';
    ++i;
  }
}


void ValueIndicator::update(int32 newVal, bool incremental) {
  data->add(newVal);
  int32 val = showSum ? data->sum() : data->average();
  if (val != lastVal || !incremental) {
    changed = true;
    printWithCommas(text, digits, offset, val);
    show();
    lastVal = val;
  } else {
    changed = false;
  }
}


void DifferenceIndicator::update(int32 newSum, bool incremental) {
  if (newSum < lastSum) lastSum = 0;      // sum was apparently reset
  ValueIndicator::update(newSum - lastSum, incremental);
  lastSum = newSum;
}


// ==============================================================
  
void CompileIndicator::show(const char *Name, const char* compiler, bool recompiling) {
  sprintf(name, "%s-%s%scompiling %-*.*s", compiler, 
          currentProcess->isUncommon() ? "unc-" : "", recompiling ? "re" : "",
          compile_len - 20, compile_len - 20, Name);
  Indicator::show(name);
}


void monitorBar_init() {}


MonitorBar::MonitorBar() {
  resize(0, 0, 0, 0);
  in_core_percentage_1 = in_core_percentage_2 = 100;
}


void MonitorBar::resize(fint X, fint Y, fint W, fint H) {
  x = X; y = Y; w = W; h = H; 
}    
  

void MonitorBar::init() { 
  old_len_1 = old_len_2 = -1; 
}


void MonitorBar::draw(bool incremental) {
  int32 len1 = min(w,        int32((float)used1() * w / capacity())); 
  int32 len2 = min(w - len1, int32((float)used2() * w / capacity())); 
  assert(len1 >= 0 && len2 >= 0 && len1  + len2 <= w, "wrong bar length");

  if (!incremental || len1 != old_len_1 || len2 != old_len_2) {
    if (w == 0) {
      TheSpy->mw()->pw->draw_line(x, y, x, y+h);
      return;
    }
    // draw lower part
    int32 blen = len1 * in_core_percentage_1 / 100;
  
    TheSpy->mw()->pw->set_color(TheSpy->mw()->pw->gray());
    TheSpy->mw()->pw->fill_rectangle(x, y+1, len1-blen, h-1);
    TheSpy->mw()->pw->set_color(TheSpy->mw()->pw->black());
    TheSpy->mw()->pw->fill_rectangle(x+len1-blen, y+1, blen, h-1);
    // draw empty part
    TheSpy->mw()->pw->clear_rectangle(x+len1, y+1, w-len1-len2, h-1);
    // draw upper part
    if (len2 != 0) {
      blen = len2 * in_core_percentage_2 / 100;
      fint glen = len2 - blen;
      TheSpy->mw()->pw->fill_rectangle(x+w-len2, y+1, blen, h-1);
      TheSpy->mw()->pw->set_color(TheSpy->mw()->pw->gray());
      TheSpy->mw()->pw->fill_rectangle(x+w-glen, y+1, glen, h-1);
      TheSpy->mw()->pw->set_color(TheSpy->mw()->pw->black());
    }
    // draw outline
    TheSpy->mw()->pw->draw_rectangle_black(x, y, w, h);
    old_len_1 = len1;
    old_len_2 = len2;
    draw_long_term(true);
    
  } else {
    draw_long_term(false);
  }
}      


void MonitorBar::calculateVMRegion(int32 startAddr, int32 size, 
                       fint& in_core_percentage, fint& oldlen) {
  int32 pagesize = OS::get_page_size();
  int32 start    = startAddr & ~(pagesize - 1);
  int32 end      = roundTo(startAddr + size, pagesize);
  int32 npages   = (end - start) / pagesize;
# define PSIZE 4096
  static char m[PSIZE]; // shouldn't use (m)alloc - in int handler!
  int32 incore   = 0;

  if (npages != 0) {
    int32 remaining = npages;
    while (start < end) {
      int32 len = end - start;  // len = size of area in bytes
      int32 n = remaining;              // n = size of area in pages
      if (len > PSIZE * pagesize) {
        len = PSIZE * pagesize;
        n   = PSIZE;
      }
      if (OS::min_core((caddr_t)start, len, m)) {
        perror("monitor: OS::min_core failed! ");
      } else {
        for (int32 i = 0; i < n; i++) {
          if (m[i] & 1) incore++;
        }
      }
      start  += len;
      remaining -= n;
    }
    fint percent = (100 * incore + npages / 2) / npages;
    if (in_core_percentage != percent) oldlen = -1;  // time to update
    in_core_percentage = percent;
  }
# undef PSIZE
}
  
  
void MonitorBar::calculate_VM_stats() {
  calculateVMRegion(int32(start1()), used1(), in_core_percentage_1, old_len_1);
  assert(in_core_percentage_1 >= 0 && in_core_percentage_1 <= 100,
         "wrong percentage");
  if (start2() != NULL) {
    calculateVMRegion(int32(start2()), used2(), in_core_percentage_2, old_len_2);
    assert(in_core_percentage_2 >= 0 && in_core_percentage_2 <= 100,
           "wrong percentage");
  }
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  void MonitorCodeCacheBar::calculate_VM_stats() { /* no-op */ }
# endif


void MonitorCPUBar::init_CPU() {
  ProcessTime user_time   = ProcessInfo::user_time();
  ProcessTime system_time = ProcessInfo::system_time();

  secsU  = user_time.secs();
  usecsU = user_time.micro_secs();
  secsS  = system_time.secs();
  usecsS = system_time.micro_secs(); 
}


MonitorCPUBar::MonitorCPUBar(int32 usec)
: MonitorBar() { 
  interval_usec = usec; 
  interval_usecN = usec * n_intervals; 
  init();
}


void MonitorCPUBar::init() {
  sum_usecsU = usedU = sum_usecsS = usedS = n = 0;
  secsU = usecsU = secsS = secsS = 0;
  for(fint i=0; i < n_intervals; i++) used_usecsU[i] = used_usecsS[i] = 0;
}


int32 MonitorCPUBar::used1() {
  return (usedU + usedS) * LoadLevelMonitor::load_level / 100;
}


void MonitorCPUBar::calculate_VM_stats() {
  in_core_percentage_1 =
    (usedS + usedU) ? usedU * 100 / (usedS + usedU) : 100;
  if (in_core_percentage_1 < 0 || in_core_percentage_1 > 100) {
    warning1("cpuBar: in_core_percentage_1 is %ld", in_core_percentage_1);
    if (in_core_percentage_1 <   0) in_core_percentage_1 =   0;
    if (in_core_percentage_1 > 100) in_core_percentage_1 = 100;
  }
}


void MonitorCPUBar::update() {

  ProcessTime user_time   = ProcessInfo::user_time();
  ProcessTime system_time = ProcessInfo::system_time();

  // I believe it takes the differences here because we need the
  // additional time spend since last querying ProcessInfo
  int32 usecDiffU = user_time.micro_secs()   - usecsU;
  int32 usecDiffS = system_time.micro_secs() - usecsS;
  int32 secDiffU  = user_time.secs()   - secsU;
  int32 secDiffS  = system_time.secs() - secsS;
  
  used_usecsU[n]  = secDiffU * 1000000 + usecDiffU;
  used_usecsS[n]  = secDiffS * 1000000 + usecDiffS;
  assert(used_usecsU[n] >= 0 && used_usecsS[n] >= 0, "negative CPU usage");

  usecsU = user_time.micro_secs();
  usecsS = system_time.micro_secs();
  secsU  = user_time.secs();
  secsS  = system_time.secs();
  
  fint nextn = n == n_intervals - 1 ? 0 : n + 1;
  sum_usecsU += used_usecsU[n] - used_usecsU[nextn];
  sum_usecsS += used_usecsS[n] - used_usecsS[nextn];
  n = nextn;
  assert(interval_usecN % 100 == 0, "should be divisible by 100");
  usedU = sum_usecsU / (interval_usecN / 100);
  usedS = sum_usecsS / (interval_usecN / 100);
  calculate_VM_stats();
  // The correction below is needed because of lost timer interrupts.
  if (usedU > 100) usedU = 100; 
  if (usedS + usedU > 100) usedS = 100 - usedU;
  assert(usedU >= 0 && usedS >= 0, "negative CPU usage");
}


void MonitorMemBar::resize(fint X, fint Y, fint H, fint factor) {
  MonitorBar::resize(X, Y, int(capacity()/(((SelfMonitor*)TheSpy)->bytes_per_pixel() * factor)), H*factor); 
}


MonitorSampledBar::MonitorSampledBar() { 
  init();
}


void MonitorSampledBar::init() {
  sum1 = sum2 = sum3 = sum4 = n = 0;
  sum1T = sum2T = sum3T = sum4T = elapsedT = 0;
  for(fint i=0; i < n_samples; i++) 
    samples1[i] = samples2[i] = samples3[i] = samples4[i] = ticks[i] = 0;
  elapsed_ticks = 0;
  old_lta1 = old_lta2 = old_lta3 = old_lta4 = 0;
}


void MonitorSampledBar::calculate_VM_stats() {
  in_core_percentage_1 =  (sum1 + sum2) ? sum1 * 100 / (sum1 + sum2) : 100;
  in_core_percentage_2 =  (sum3 + sum4) ? sum3 * 100 / (sum3 + sum4) : 100;
    
  assert(in_core_percentage_1 >= 0 && in_core_percentage_1 <= 100,
         "wrong percentage");
}


void MonitorSampledBar::update0() {
  for(fint i=0; i < n_samples; i++) 
    samples1[i] = samples2[i] = samples3[i] = samples4[i] = ticks[i] = 0;
  sum1 = sum2 = sum3 = sum4 = elapsed_ticks = 0;
  calculate_VM_stats();
}


void MonitorSampledBar::update(fint count1, fint count2, fint et) {
  update(count1, count2, 0, 0, et);
}


void MonitorSampledBar::update(fint count1, 
                            fint count2, 
                            fint count3, 
                            fint count4,
                            fint et) {
  fint nextn = n == n_samples - 1 ? 0 : n + 1;
  sum1 += count1 - samples1[nextn]; samples1[nextn] = count1;
  sum2 += count2 - samples2[nextn]; samples2[nextn] = count2;
  sum3 += count3 - samples3[nextn]; samples3[nextn] = count3;
  sum4 += count4 - samples4[nextn]; samples4[nextn] = count4;
    
  sum1T += count1; sum2T += count2; sum3T += count3; sum4T += count4;
  elapsedT += et;
  elapsed_ticks += et - ticks[nextn]; ticks[nextn] = et;
  n = nextn;
  calculate_VM_stats();
}


int32 MonitorSampledBar::used1() {
  int32 u = sum1 + sum2;
  int32 c = capacity();
  // this can exceed capacity slightly due to sampling errors
  if (u > 2*c)
    warning2("MonitorSampledBar: usage(1) is way too high: %ld >> %ld", u, c);
  return u <= c ? u : c;
}


int32 MonitorSampledBar::used2() {
  int32 u = sum3 + sum4;
  int32 c = capacity();
  // this can exceed capacity slightly due to sampling errors
  if (u > 2*c)
    warning2("MonitorSampledBar: usage(2) is way too high: %ld >> %ld", u, c);
  return u <= c ? u : c;
}


void MonitorSampledBar::draw_long_term(bool mustDraw) {
  if (!SpyShowLongTerm) return;

  // calculate long-term average (use floats to avoid overflow)
  if (elapsedT == 0) return;
  fint lta1 = fint((w-2) * ((float)sum1T / elapsedT) + 1);
  fint lta2 = fint((w-2) * ((float)sum2T / elapsedT) + 1);
  fint lta3 = fint((w-2) * ((float)sum3T / elapsedT) + 1);
  fint lta4 = fint((w-2) * ((float)sum4T / elapsedT) + 1);

  const fint markerH = 3;
  fint ys = y + (h - markerH) / 2;
  TheSpy->mw()->pw->set_xor();
  if (mustDraw || lta1 != old_lta1) {
    if (!mustDraw) TheSpy->mw()->pw->draw_line(x+old_lta1, ys, x+old_lta1, ys+markerH);
    TheSpy->mw()->pw->draw_line(x + lta1, ys, x + lta1, ys+markerH);
    old_lta1 = lta1;
  }
  // draw 2nd bar only if not at left end, and if not at same position as 1st
  if (mustDraw || lta2 != old_lta2) {
    if (!mustDraw && old_lta2 != 1)
      TheSpy->mw()->pw->draw_line(x+old_lta2, ys, x+old_lta2, ys+markerH);
    if (lta2 != 1 && lta2 != lta1)
      TheSpy->mw()->pw->draw_line(x + lta2, ys, x + lta2, ys+markerH);
    old_lta2 = lta2;
  }
  if (mustDraw || lta3 != old_lta3) {
    if (!mustDraw && old_lta3 != 1)
      TheSpy->mw()->pw->draw_line(x+old_lta3, ys, x+old_lta3, ys+markerH);
    if (lta3 != 1 && lta3 != lta2 && lta3 != lta1)
      TheSpy->mw()->pw->draw_line(x + lta3, ys, x + lta3, ys+markerH);
    old_lta3 = lta3;
  }
  if (mustDraw || lta4 != old_lta4) {
    if (!mustDraw && old_lta4 != 1)
      TheSpy->mw()->pw->draw_line(x+old_lta4, ys, x+old_lta4, ys+markerH);
    if (lta4 != 1 && lta4 != lta3 && lta4 != lta2 && lta4 != lta1)
      TheSpy->mw()->pw->draw_line(x + lta4, ys, x + lta4, ys+markerH);
    old_lta4 = lta4;
  }
  TheSpy->mw()->pw->set_copy();
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  void MonitorZoneBar::resize(fint X, fint Y, fint H, fint factor,
                                Heap *newZ) {
    if (newZ) z= newZ;
    MonitorBar::resize(X, Y, int(capacity()/(((SelfMonitor*)TheSpy)->bytes_per_pixel() * factor)), H*factor); 
  }
    
# endif
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "files.hh"
# include "_files.cpp.incl"

FileTable* Files;

void files_init() { Files = new FileTable; }
void files_exit() { delete Files; }

FileTable::FileTable() {
  for (fint i = 0; i < FileTableSize; i++) {
    _table[i] = NULL;
  }
}

FileTable::~FileTable() {
  closeAll();
}


static bool isFullPath(const char* path) {
  switch (path[0]) {
   case '/':  case '~':  return true;

   case '.':
    switch (path[1]) {
     case '.':  case '/':  case '\0': return true;
    }
  }
  return false;
}


FILE* FileTable::openSnapshotFile( const char*  name,
                                   const char*  modeArg,
                                   const char**       fullFileName) {
  return openFile(name, OS::mode_for_binary(modeArg), NULL, "", fullFileName);
}


// searches path, where path is colon separated list of directories,
// and a null path component means the current directory-- dmu 10/91

// returns the full path name of the file in *fullname if the file
// could be opened and if fullname != NULL.  The string is allocated
// statically.

FILE* FileTable::openFile(const char* name,
                          const char* mode,
                          const char* path,
                          const char* suffix,
                          const char** fullname) {

  if (fullname) *fullname = NULL;
  
  // null path means current directory
  if (OS::is_non_unix_path(name)  ||  isFullPath(name)  ||  !path) {
    return tryOpen(name, suffix, mode, fullname);
  }

  int name_length = strlen(name);
  const char *s, *e;

  for (s = path;  ;   s = e + 1) {
    const char* tryMe;

    // s points to  start of next path element or colon or null

    for ( e = s;  *e  &&  *e != ':';  e++) {} // find end of this component
    // s <= e. e is just past end of segment

    char dirAndName[OS::max_path_length]; // must have same scope as tryOpen call

    if (s == e) { // null path seg
      tryMe = name;

    } else {
      // s < e

      const char  delimiter[] = "/";
      const int   delimiter_length = sizeof(delimiter) - 1;


      if (e - s  >=  sizeof(dirAndName)) {
        char* buf = new char[e - s + 1];
        lsprintf_string(buf, e - s, s);
        fatal2("%s exceeds %d chars in length", buf,  sizeof(dirAndName) - 1);
      }
      lsprintf_string(dirAndName, e - s,  s);

      if (e - s  +  delimiter_length + name_length  >=  sizeof(dirAndName)) {
        fatal4("file name %s%s%s is longer than %d",
               dirAndName,  delimiter,  name,  sizeof(dirAndName) - 1);
      }
      char buf[OS::max_path_length]; 
      strcpy(buf, dirAndName);
      sprintf(dirAndName, "%s%s%s",  buf,  delimiter, name);

      tryMe = dirAndName;
    }
    
    FILE* f = tryOpen(tryMe, suffix, mode, fullname);
    if (f != NULL) {
      return f;
    }
    if (!*e)
      return NULL;
  }
}


FILE* FileTable::tryOpen(const char*   name,
                         const char*   suffix,
                         const char*   mode,
                         const char**  fullname) {

  static char expandedName[OS::max_path_length];
  if (!OS::expand_dir(name, expandedName)) return NULL;


  FILE* f= fopen(expandedName, mode);
  if (f == NULL) {
    if (!suffix) return NULL;
  
    if (strlen(expandedName) + strlen(suffix)  >=  sizeof(expandedName)) {
      fatal3("file name %s%s is longer than %d",
             expandedName,  suffix,  sizeof(expandedName) - 1);
    }
    
    sprintf(expandedName, "%s%s", expandedName, suffix);
    f= fopen(expandedName, mode);
  }
  if (f) {
    // sanity check
    struct stat st_buf;
    if (fstat(fileno(f), &st_buf) != 0
        || S_ISDIR(st_buf.st_mode)) { // no directories allowed
      fclose(f);
      f= NULL;
#     if TARGET_OS_FAMILY == UNIX_FAMILY
        errno= EISDIR;
#     endif
    }
  }
  if (f && fullname) *fullname= expandedName;
  return f;
}


FILE* FileTable::record(FILE* f) {
  for (fint i = 0;  i < FileTableSize;  i++) {
    if (_table[i] == NULL) {
      _table[i] = f;
      return f;
    }
  }
  fclose(f);
  error("Can't open file -- file table full");
  errno = 0;
  return NULL;
}


void FileTable::closeFile(FILE* f) {
  for (fint i = 0; i < FileTableSize; i++) {
    if (_table[i] == f) {
      _table[i] = NULL;
    }
  }
  (void) fclose(f);
}


void FileTable::closeAll() {
  for (fint i = 0; i < FileTableSize; i++) {
    if (_table[i] != NULL) {
      (void) fclose(_table[i]);
      _table[i] = NULL;
    }
  }
}
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "aCompiler.hh"
# include "_aCompiler.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

AbstractCompiler* activeCompiler;


// lookup and doIt entry point
AbstractCompiler::AbstractCompiler(compilingLookup* l,
                                   sendDesc* sd,
                                   nmln* d) {
  L = l;
  diLink = d;
  send_desc = sd;
  
  countCompilation(); parentVFrame = NULL; initBugHunt();
}


fint AbstractCompiler::level() {
  fint lev = min(recompileLevel() + 1, MaxRecompilationLevels - 1);
  if (!(lev >= 0 && lev < MaxRecompilationLevels)) {
    error1("invalid compiler level %d", lev);       // fix this
    lev = 0;
  }
  return lev;
}


fint AbstractCompiler::version() {
  fint v;
  if (recompilee) {
    fint l = recompilee->version();
    v = min(l + 1, MaxVersions);
  } else {
    v = 0;
  }
  if (parentVFrame && name() == SIC) {
    // since block nmethods are discarded when their home is recompiled,
    // their version number is at least as high as the parent's version
    // but don't do this for NIC nmethods
    v = max(v, parentVFrame->code->version());
  }
  return v;
} 

// for debugging
void AbstractCompiler::initBugHunt() {
  // set to true to debug every method compiled
  debugging = false;
  if (BugHuntNames->is_objVector()) {

    oop name_of_outer_method = L->selector();
    
    // or if this is a block, turn on debugging of the blocks outer method's name is in bug hunt names
    if (L->resultType() == methodResult) {
      // not an access or assignment
      oop meth = method();
      Map* mm = meth->map();
      assert(mm->has_code(), "");
      methodMap* mmm = (methodMap*)mm;
      if (mmm->kind() == BlockMethodType) {
        // is a block method, get method name
        oop r = L->receiver;
        assert(r->is_block(), "");
        blockOop b = blockOop(r);
        name_of_outer_method = b->outermostMethodSelector();
      }
    }
    
    objVectorOop names = (objVectorOop)BugHuntNames;
    for (fint i = names->length() - 1; i >= 0; i--) {
      oop n = names->obj_at(i);
      if (n == name_of_outer_method) {
        // turn on all debugging flags for this compilation
        debugging = true;
      }
    }
  }
  
  if (debugging) {
    LogVMMessages = true;
    PrintVMMessages = true;  // was false, why? -- dmu
    
    lprintf("found a method in BugHuntNames: \n");
    method()->print();
    
    #   ifdef FAST_COMPILER
          PrintCompilation = PrintCompiledCode = true;
    #   endif
    
    #   ifdef SIC_COMPILER
        SICDebug = PrintSICTempRegisterAllocation = PrintSICCode =
          PrintSICCompiledCode = PrintSICCopyPropagation = PrintSICRegAlloc =
          PrintSICRegTargeting = PrintSICMarkers = PrintSICReplacement =
          PrintSICEliminateUnneededNodes = PrintSICExposed = PrintSICTypeTestOpt =
          PrintInlining = 
          PrintSICCompilation = true;
    #   endif
  }
}


void AbstractCompiler::finalize() {
  if (debugging) {
    PrintVMMessages = true;


    #   ifdef FAST_COMPILER
          PrintCompilation = PrintCompiledCode = false;
    #   endif
    
    #   ifdef SIC_COMPILER
        SICDebug = PrintSICTempRegisterAllocation = PrintSICCode =
            PrintSICCompiledCode = PrintSICCopyPropagation = PrintSICRegAlloc =
            PrintSICRegTargeting = PrintSICMarkers = PrintSICReplacement =
            PrintSICEliminateUnneededNodes = PrintSICExposed = PrintSICTypeTestOpt =
            PrintSICCompilation = false;
    #   endif
    flush_logFile();
  }
}

void AbstractCompiler::dispatchToCode() {
  switch (L->resultType()) {
   case     methodResult:      methodCode();  break;
   case   constantResult:    constantCode();  break;
   case       dataResult:        dataCode();  break;
   case assignmentResult:  assignmentCode();  break;
   default: fatal("no more cases");
  }
}

oop AbstractCompiler::method() { return L->result()->contents(); }

oop AbstractCompiler::methodHolder_or_map() {
  return L->result()->generalized_methodHolder_or_map(L->receiver);
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.10 $ */
/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerLocator.hh"

# include "_registerLocator.cpp.incl"



RegisterLocator::RegisterLocator(bool clear) 
 : ResourceObj() {
 
  my_frame = NULL;
  # if GENERATE_DEBUGGING_AIDS
    if (SpendTimeForDebugging) {
      sendee = NULL;
      setHow = 0;
    }
  # endif
    
  if (clear)
    forget_everything();
}


RegisterLocator* RegisterLocator::clone() {
  RegisterLocator* r = new RegisterLocator(false);
  *r = *this;
  # if GENERATE_DEBUGGING_AIDS
    if (SpendTimeForDebugging)
      r->sendee = this;
  # endif
  return r;
}


void RegisterLocator::set_address(Location reg, oop* a) {
  assert( ( int(a) & 3 ) == 0, "alignment check");
  addresses()[index_for_reg(reg)] = a;
}


oop* RegisterLocator::address_of(Location reg) {
  assert( my_frame->is_aligned(), "frame not set");
  oop* r = addresses()[index_for_reg(reg)];
  assert(r != NULL  &&  (int(r) & (oopSize-1)) == 0,
         "If this is called from a primitive, be sure the primtive table entry sets canWalkStack");
  return r;
}


fint RegisterLocator::index_for_reg(Location reg) {
  assert(isRegister(reg), "");
  assert(LowestLocalNonVolReg <= reg  &&  reg <= HighestNonVolReg, "");
  return reg - LowestLocalNonVolReg;
}



RegisterLocator* RegisterLocator::sender() {
  RegisterLocator* result = clone();
  result->be_for_sender();
  return result;
}

void RegisterLocator::be_for_sender() {
  // NB: a frame saves registers and so provides their addresses for its SENDER
  assert(my_frame != NULL, "checking PPCSIC bug");
  if (my_frame->is_compiled_self_frame())  update_addresses_from_self_frame();
  else                                     update_addresses_from_VM_frame();
  my_frame = my_frame->sender();
}


RegisterLocator* RegisterLocator::climb_to_frame(frame* f) {
  if (fr() == f)
    return this;
  RegisterLocator* r;
  // Gah! Conversion and Recompilation thread the stack from the vm stack to the self stack
  // so I had to add the "ConversionInProgress  ||  RecompilationInProgress" part. -- dmu 6/99
  // Also when profiling, can trace a stack (on Vm stack) that is threaded back to self stack.
  // Maybe should just test for vm stack to self stack transition? -- dmu 1/04
  // That's what I'm trying now. -- dmu 1/04
  for ( r = clone();  
        r->fr() != f;  
        r->be_for_sender() )
    assert(isOnVMStack(r->fr())  || r->fr() < f,  "passed it");
    
  // nprofiler debug code needs vframes, but since there may be self vframes,
  // then vm frames, then the signal, there may not be all addresses defined. -- dmu 1/04
  assert(profilerCollectStackSemaphore || r->are_all_defined(), "");
  return r;
}


bool RegisterLocator::are_all_defined() {
  // return true of all addrs are defined
  for (fint i = LowestLocalNonVolReg;  i <= HighestNonVolReg;  ++i)
    if ( address_of((Location)i) == NULL )
      return false;
  return true;
}


void RegisterLocator::forget_everything() {
  for (fint i = LowestLocalNonVolReg;  i <= HighestNonVolReg;  ++i)
    set_address((Location)i, NULL);
}


RegisterLocator* RegisterLocator::for_sender_of(frame* f) {
  RegisterLocator* r = new RegisterLocator();
  r->my_frame = f;
  r->be_for_sender();
  return r;
}


RegisterLocator* RegisterLocator::for_copied_frame(frame* copied_frame) {
  // when copying a frame for conversion, need to copy the saved registers
  oop* regs = NEW_RESOURCE_ARRAY(oop, NumLocalNonVolRegisters);
  RegisterLocator* r = new RegisterLocator(false);
  r->my_frame = copied_frame;
  
  for (int i = LowestLocalNonVolReg;  i <= HighestNonVolReg;  ++i) {
    oop* p = &regs[i - LowestLocalNonVolReg];
    *p = *address_of(Location(i));
    r->set_address(Location(i), p);
  }
  return r;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "util.hh"

# include "_util.cpp.incl"

fint Indent = 0;

void printIndent() {
  for (fint i = 0; i < Indent; i++) lprintf("  ");
}

# define DO_UP(from) LOOP_UNROLL(count, *to++ = from)
# define DO_DOWN(from) LOOP_UNROLL(count, *--to = from)

void copy_oops_up(oop* from, oop* to, fint count) {
  assert(maskBits(int32(from), Tag_Size) == 0, "not word aligned");
  assert(maskBits(int32(to), Tag_Size) == 0, "not word aligned");
  assert(count >= 0, "negative count");
  DO_UP(*from++)
  }

void copy_oops_down(oop* from, oop* to, fint count) {
  assert(maskBits(int32(from), Tag_Size) == 0, "not word aligned");
  assert(maskBits(int32(to), Tag_Size) == 0, "not word aligned");
  assert(count >= 0, "negative count");
  DO_DOWN(*--from)
  }

void set_oops(oop* to, fint count, oop value) {
  assert(maskBits(int32(to), Tag_Size) == 0, "not word aligned");
  assert(count >= 0, "negative count");
  DO_UP(value)
  }

void copy_bytes_up(const char* from, char* to, fint count) {
  assert(count >= 0, "negative count");
  DO_UP(*from++)
  }

void copy_bytes_down(const char* from, char* to, fint count) {
  assert(count >= 0, "negative count");
  DO_DOWN(*--from)
  }

void set_bytes(char* to, fint count, char value) {
  assert(count >= 0, "negative count");
  DO_UP(value)
  }


char* copy_string(const char* s) {
  fint len = strlen(s) + 1;
  char* str = NEW_RESOURCE_ARRAY( char, len);
  strcpy(str, s);
  return str;
}

char* copy_c_heap_string(const char* s) {
  fint len = strlen(s) + 1;
  char* str = NEW_C_HEAP_ARRAY( char, len);
  strcpy(str, s);
  return str;
}

char* copy_string(const char* s, smi len) {
  char* str = NEW_RESOURCE_ARRAY( char, len+1);
  memcpy(str, s, len+1);
  str[len] = '\0';
  return str;
}


int compare_slot_names(const char *s1, fint l1, const char *s2, fint l2) {
  int len= min(l1, l2);
  while (len > 0 && *s1 == *s2) {
    ++s1;
    ++s2;
    --len;
  }
  if (len == 0) 
    return l1 < l2 ? -1 : l1 == l2 ? 0 : 1;
  char c1= *s1;
  char c2= *s2;
  assert(c1 != c2, "oops");
  return c1 == ':' ? -1
       : c2 == ':' ?  1
       : c1 < c2   ? -1
                   :  1;
}

// like strcmp, but w/o null-termination and fast
//  assumes int32 is 4 bytes long
int compare_bytes(const char* b1, int l1, const char* b2, int l2) {
  int len = min(l1, l2);
  if (len >= 16  &&  !((int(b1) | int(b2)) & 3)) {
    int32 *i1 = (int32*)b1;
    int32 *i2 = (int32*)b2;
    int    len32 = len >> 2;
    while( len32 >= 16 ) {
      if (i1[0] != i2[0]) {i1 += 0; i2 += 0; len32 -= 0; goto check_word; }
      if (i1[1] != i2[1]) {i1 += 1; i2 += 1; len32 -= 1; goto check_word; }
      if (i1[2] != i2[2]) {i1 += 2; i2 += 2; len32 -= 2; goto check_word; }
      if (i1[3] != i2[3]) {i1 += 3; i2 += 3; len32 -= 3; goto check_word; }
      if (i1[4] != i2[4]) {i1 += 4; i2 += 4; len32 -= 4; goto check_word; }
      if (i1[5] != i2[5]) {i1 += 5; i2 += 5; len32 -= 5; goto check_word; }
      if (i1[6] != i2[6]) {i1 += 6; i2 += 6; len32 -= 6; goto check_word; }
      if (i1[7] != i2[7]) {i1 += 7; i2 += 7; len32 -= 7; goto check_word; }
      if (i1[8] != i2[8]) {i1 += 8; i2 += 8; len32 -= 8; goto check_word; }
      if (i1[9] != i2[9]) {i1 += 9; i2 += 9; len32 -= 9; goto check_word; }
      if (i1[10] != i2[10]){i1 += 10; i2 += 10; len32 -= 10; goto check_word; }
      if (i1[11] != i2[11]){i1 += 11; i2 += 11; len32 -= 11; goto check_word; }
      if (i1[12] != i2[12]){i1 += 12; i2 += 12; len32 -= 12; goto check_word; }
      if (i1[13] != i2[13]){i1 += 13; i2 += 13; len32 -= 13; goto check_word; }
      if (i1[14] != i2[14]){i1 += 14; i2 += 14; len32 -= 14; goto check_word; }
      if (i1[15] != i2[15]){i1 += 15; i2 += 15; len32 -= 15; goto check_word; }
                            i1 += 16; i2 += 16; len32 -= 16;
    }
    {
      int32* e32 = i1 + len32;
      for ( ;  i1 < e32;  ++i1, ++i2 )
        if (*i1 != *i2) break;
      len32 = e32 - i1;
    }
   check_word: 
    // at this point i1 and i2 point to words to be compared
    // len32 contains how many words left to be compared
    int charsCompared = ((len >> 2) - len32) << 2;
    b1  += charsCompared;
    b2  += charsCompared;
    len -= charsCompared;
  }
  const char* e1 = b1 + len; 
  while ( b1 < e1 ) {
    char c1 = *b1++;
    char c2 = *b2++;
    if (c1 != c2)
      return c1 < c2 ? -1 : 1;
  }
  return l1 < l2 ? -1 : (l1 == l2 ? 0 : 1);
}


oop catchThisOne;

void breakpoint() {
  static fint junk = 0;
  junk = 1;
}

void error_breakpoint() {
  static fint junk = 0;
  (void)junk;
}

volatile void ShouldNotCall(const char *file, int line) {
  lprint_fatal(file, line, "Calling member function which shouldn't be called");
}

volatile void ShouldNotReach(const char *file, int line) {
  lprint_fatal(file, line, "Reached supposedly impossible case");
}

// just in case asserts go haywire
# if defined(assert)
#    undef assert
# endif
void assert(bool b, const char* msg) {};
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "framePieces.hh"

# include "_framePieces.cpp.incl"

# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "search_ppc.hh"
# include "_search_ppc.cpp.incl"



// sw pipelining for 1-case, xpose for many case


# define WORDS_PER_VECTOR 4


static int32* altivec_vectorfind_next_match(int32* start, int32* targets, int32 num_targets, int32* hit_num);
extern "C" {
  static oop* altivec_find_this_object(oop* middle);
  static oop* altivec_find_next_object(oop* middle);
  static oop* altivec_find_prior_reference(oop* middle, oop target);
}

# define C(n) \
  static int32* altivec_vectorfind_next_match_##n(int32* space, int32* targets, int32* target_buf, int32* hit_num);

    C( 1) C( 2) C( 3) C( 4) C( 5) C( 6) C( 7) C( 8) C( 9) C(10)
    C(11) C(12) C(13) C(14) C(15) C(16) C(17) C(18) C(19) C(20)
    C(21) C(22) C(23) C(24) C(25) C(26) C(27) C(28) C(29) C(30)
    C(31) C(32) C(33) C(34) C(35) C(36) C(37) C(38) C(39) C(40)
# undef C

// simple C implementation

int32 Vectorfind_max_targets = 40;   
int32 Vectorfind_max_chunk_size = 0x40000000; 

int32* vectorfind_next_match(int32* start,
                             int32* targets, int32 num_targets, 
                             int32* hit_num) {
  if (!haveAltiVec()) {
    for ( ;  ;  ++start) {
      for (int32 index = 0; index < num_targets; ++index)
        if (*start == targets[index]) { *hit_num = index; return start; }
    }
    fatal("should be sentinel");
  }
  return altivec_vectorfind_next_match(start, targets, num_targets, hit_num);
}

static int32* altivec_vectorfind_next_match(int32* start,
                                            int32* targets, int32 num_targets, 
                                            int32* hit_num) {
  // ensure quad alignment
  for ( ;  int32(start) & (WORDS_PER_VECTOR * sizeof(int32) - 1);  ++start) {
    for (int32 index = 0; index < num_targets; ++index)
      if (*start == targets[index]) { *hit_num = index; return start; }
  }
  
  // find aligned-by-four target buffer
  int32 buf[4];
  int32* bp = &buf[0];
  while (int(bp) & (WORDS_PER_VECTOR * sizeof(int32) - 1)) ++bp;
  

  int32* r = NULL;
  # define C(n) case n: r = altivec_vectorfind_next_match_##n(start, targets, bp, hit_num); break;
  switch (num_targets) { 
    default: fatal("must match");
    C( 1) C( 2) C( 3) C( 4) C( 5) C( 6) C( 7) C( 8) C( 9) C(10)
    C(11) C(12) C(13) C(14) C(15) C(16) C(17) C(18) C(19) C(20)
    C(21) C(22) C(23) C(24) C(25) C(26) C(27) C(28) C(29) C(30)
    C(31) C(32) C(33) C(34) C(35) C(36) C(37) C(38) C(39) C(40)
  }
  # undef C
  
  for ( start = r;  start  < r + WORDS_PER_VECTOR;  ++start) {
    for ( int32 index = 0; index < num_targets; ++index)
      if (*start == targets[index]) { *hit_num = index; return start; }
  }
  fatal("not found");
  return NULL;
}

  
extern "C" {
  // enumeration:   // simple C implementation

  oop* find_this_object(oop* middle) {
    if (!haveAltiVec()) {
      for ( ; !(*middle)->is_mark(); --middle) {} 
      return middle; 
    }
  return altivec_find_this_object(middle);
  }
  
  static oop* altivec_find_this_object(oop* middle) {
    switch ( (int32(middle) / sizeof(oop)) & (WORDS_PER_VECTOR - 1) ) {
      case 2:  if ( (*middle)->is_mark() ) return middle; --middle; // FALL THROUGH
      case 1:  if ( (*middle)->is_mark() ) return middle; --middle; // FALL THROUGH
      case 0:  if ( (*middle)->is_mark() ) return middle; --middle; // FALL THROUGH
      case 3:  break;
    }
    middle -= WORDS_PER_VECTOR - 1;

    vector signed int data;
    vector signed int masks    = {Tag_Mask, Tag_Mask, Tag_Mask, Tag_Mask};
    vector signed int markTags = {Mark_Tag, Mark_Tag, Mark_Tag, Mark_Tag};
    for ( ;
          ! vec_any_eq( markTags,  vec_and( masks, vec_ldl(0, (int*)middle)));
          middle -= WORDS_PER_VECTOR )
      ;
    if (middle[3]->is_mark())  return middle + 3;
    if (middle[2]->is_mark())  return middle + 2;
    if (middle[1]->is_mark())  return middle + 1;
    if (middle[0]->is_mark())  return middle + 0;
    fatal("not found");
    return NULL;
  }
  

  oop* find_next_object(oop* middle) {
    ++middle;
    if (!haveAltiVec()) {
      for ( ; !(*middle)->is_mark(); ++middle) {} 
      return middle;
    }
  return altivec_find_next_object(middle);
  }

  static oop* altivec_find_next_object(oop* middle) {
    switch ( (int32(middle) / sizeof(oop)) & (WORDS_PER_VECTOR - 1) ) {
      case 1:  if ( (*middle)->is_mark() ) return middle; ++middle; // FALL THROUGH
      case 2:  if ( (*middle)->is_mark() ) return middle; ++middle; // FALL THROUGH
      case 3:  if ( (*middle)->is_mark() ) return middle; ++middle; // FALL THROUGH
      case 0:  break;
    }

    vector signed int data;
    vector signed int masks    = {Tag_Mask, Tag_Mask, Tag_Mask, Tag_Mask};
    vector signed int markTags = {Mark_Tag, Mark_Tag, Mark_Tag, Mark_Tag};
    for ( ;
          ! vec_any_eq( markTags,  vec_and( masks, vec_ldl(0, (int*)middle)));
          middle += WORDS_PER_VECTOR )
      ;
    if (middle[0]->is_mark())  return middle + 0;
    if (middle[1]->is_mark())  return middle + 1;
    if (middle[2]->is_mark())  return middle + 2;
    if (middle[3]->is_mark())  return middle + 3;
    fatal("not found");
    return NULL;
  }    
          

  oop* find_prior_reference(oop* middle, oop target) {
    if (!haveAltiVec()) {
      for ( ; *middle != target; --middle) {}
      return middle; 
    }
  return altivec_find_prior_reference(middle, target);
  }

  static oop* altivec_find_prior_reference(oop* middle, oop target) {
    switch ( (int32(middle) / sizeof(oop)) & (WORDS_PER_VECTOR - 1) ) {
      case 2:  if ( *middle == target ) return middle; --middle; // FALL THROUGH
      case 1:  if ( *middle == target ) return middle; --middle; // FALL THROUGH
      case 0:  if ( *middle == target ) return middle; --middle; // FALL THROUGH
      case 3:  break;
    }
    middle -= WORDS_PER_VECTOR - 1;

    vector signed int data;
    static int32 buf[WORDS_PER_VECTOR];
    static int32* aligned_bp = NULL;
    if (aligned_bp == NULL)
      for ( aligned_bp = buf;
            int(aligned_bp) & (WORDS_PER_VECTOR * sizeof(int)  -  1);
            ++aligned_bp) 
        {}
    *aligned_bp = (int32)target;
    vector signed int targets = vec_splat( vec_lvewx(0, aligned_bp), 0 );
    for ( ;
          ! vec_any_eq( targets, vec_ldl(0, (int*)middle));
          middle -= WORDS_PER_VECTOR )
      ;
    if (middle[3] == target)  return middle + 3;
    if (middle[2] == target)  return middle + 2;
    if (middle[1] == target)  return middle + 1;
    if (middle[0] == target)  return middle + 0;
    fatal("not found");
    return NULL;
  }
}




// Don't know alignment of targets vector, and lvewx uses alignment, so
// use an aligned buffer


# define lt(n) \
  vector signed int targ##n; \
  *target_buf = targets[n]; \
  targ##n = vec_lvewx(0, target_buf); \
  targ##n = vec_splat(targ##n, 0);
  
# define tt(n) if (vec_any_eq(data, targ##n)) break;


static int32* altivec_vectorfind_next_match_1(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  
  vector signed int data;
  lt(0)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0);
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_2(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0) lt(1)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0);
    tt(1);
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_3(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_4(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_5(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_6(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4) tt(5)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_7(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4) tt(5) tt(6)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_8(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4) tt(5) tt(6) tt(7)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_9(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4) tt(5) tt(6) tt(7) tt(8)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_10(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0) tt(1) tt(2) tt(3) tt(4) tt(5) tt(6) tt(7) tt(8) tt(9)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_11(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_12(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_13(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_14(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_15(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_16(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_17(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_18(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_19(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18)
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_20(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_21(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_22(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_23(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_24(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_25(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) 
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_26(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_27(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_28(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_29(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28)
    
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) 
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_30(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_31(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_32(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) 
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) 
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_33(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_34(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_35(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_36(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34) lt(35)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34) tt(35)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_37(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34) lt(35) lt(36)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34) tt(35) tt(36)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_38(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34) lt(35) lt(36) lt(37)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34) tt(35) tt(36) tt(37)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_39(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34) lt(35) lt(36) lt(37) lt(38)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34) tt(35) tt(36) tt(37) tt(38)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}


static int32* altivec_vectorfind_next_match_40(int32* space, int32* targets, int32* target_buf, int32* hit_num) {
  vector signed int data;
  lt(0)  lt(1)  lt(2)  lt(3)  lt(4)  lt(5)  lt(6)  lt(7)  lt(8)  lt(9)
  lt(10) lt(11) lt(12) lt(13) lt(14) lt(15) lt(16) lt(17) lt(18) lt(19)
  lt(20) lt(21) lt(22) lt(23) lt(24) lt(25) lt(26) lt(27) lt(28) lt(29)
  lt(30) lt(31) lt(32) lt(33) lt(34) lt(35) lt(36) lt(37) lt(38) lt(39)
  
  for (;;) {
    data = vec_ldl(0, space);
    tt(0)  tt(1)  tt(2)  tt(3)  tt(4)  tt(5)  tt(6)  tt(7)  tt(8)  tt(9)
    tt(10) tt(11) tt(12) tt(13) tt(14) tt(15) tt(16) tt(17) tt(18) tt(19)
    tt(20) tt(21) tt(22) tt(23) tt(24) tt(25) tt(26) tt(27) tt(28) tt(29)
    tt(30) tt(31) tt(32) tt(33) tt(34) tt(35) tt(36) tt(37) tt(38) tt(39)
    space += WORDS_PER_VECTOR;
  }  
  return space;
}

# undef tt
# undef lt
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */


# pragma implementation "countPattern_ppc.hh"
# include "_countPattern_ppc.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
 

  void CountCodePattern::initCounting() {
    // general count stub; 8 instructions.
    // 0: lis     Temp1, high(count_addr)
    // 1: lwz     Temp2, [low(count_addr) + Temp1]
    // 2: addi    Temp2, Temp2, 1
    // 3: stw     Temp2, [low(count_addr) + Temp1]
    // 4: lis     Temp1, high(jump_addr)
    // 5: ori     Temp1, Temp1, low(jump_addr)
    // 6: mtctr   Temp1
    // 7: balwctr

    instsSize         = 8 * 4;
     countAddr_offset = 0;
           lwz_offset = 1;
           stw_offset = 3;
        nm_lis_offset = 4;
        nm_ori_offset = 5;
         limit_offset = recompile_lis_offset = recompile_ori_offset = BadOffset;

    Assembler* oldAssembler = theAssembler;
    Assembler* a = theAssembler = new Assembler(instsSize, instsSize, false, true);
    a->load_from_address(Temp2, 0, VMAddressOperand, Temp1);      // 2 instructions
    a->addi(Temp2, Temp2, 1, NumberOperand);
    a->stw(Temp2, 0, VMAddressOperand, Temp1);
    a->long_branch_to((pc_t)0, CodeAddressOperand, Temp1, false); // 4 instructions

    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_words((int32*)a->instsStart, (int32*)pattern, instsSize / 4);
    a->finalize();
    theAssembler = oldAssembler;
  }


  void CountCodePattern::initComparing() {
    // general count stub; 8 instructions.
    // 0: lis     Temp1, high(count_addr)
    // 1: lwz     Temp2, [low(count_addr) + Temp1]
    // 2: addi    Temp2, Temp2, 1
    // 3: stw     Temp2, [low(count_addr) + Temp1]
    // 4: cmpwi   Temp2, limit
    // 5: beq     10
    // 6: lis     Temp1, high(jump_addr)
    // 7: ori     Temp1, Temp1, low(jump_addr)
    // 8: mtctr   Temp1
    // 9: balwctr
    //10: mflr    RecompileTempReg
    //11: stw     RecompileTempReg, SP + saved_pc_offset * 4
    //12: bl      13
    //13: mflr    RecompileLinkReg
    //14: mtlr    RecompileTempReg
    //15: lis     R0, high(recompile_addr)
    //16: ori     R0, R0, low(recompile_addr)
    //17: mtctr   R0
    //18: balwctrl

    instsSize            = 19 * 4;
    
    countAddr_offset = 0;
          lwz_offset = 1;
          stw_offset = 3;
            limit_offset = 4;
           nm_lis_offset = 6;
           nm_ori_offset = 7;
    recompile_lis_offset = 15;
    recompile_ori_offset = 16;

    Assembler* oldAssembler = theAssembler;
    Assembler* a = theAssembler = new Assembler(instsSize, instsSize, false, true);
    { // make a new block so that the Label's destructor gets called before we reset theAssembler in Assembler::finalize
      a->load_from_address(Temp2, 0, VMAddressOperand, Temp1);      // 2 instructions
      a->addi(Temp2, Temp2, 1, NumberOperand); 
      a->stw(Temp2, 0, VMAddressOperand, Temp1);
      a->cmpwi( Temp2, 0, NumberOperand ); // assuming limit fits in si (assert in countStub_ppc)
      Label recompile(a->printing);
      a->beq(recompile, predict_usual);   // not likley to recompile, so expect to fall through
      a->long_branch_to((pc_t)0, CodeAddressOperand, Temp1, false); // 4 instructions
      recompile.define();

      // save link so can use link to get address of this code
      // also key to profiler -- dmu 2/04
      a->mflr(RecompileTempReg);
      a->stw(RecompileTempReg, saved_pc_offset * oopSize, NumberOperand, SP);
      Label next;
      a->bl(next, NumberOperand); // needed to get addr of this code
      next.define();
      a->mflr(RecompileLinkReg);
      a->mtlr(RecompileTempReg);

      // call recompiler
      char* fnaddr = Memory->code->trapdoors->Recompile_stub_td(R0);
      a->long_branch_to(fnaddr, CodeAddressOperand, R0, false);
    }
    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_words((int32*)a->instsStart, (int32*)pattern, instsSize / 4);
    a->finalize();
    theAssembler = oldAssembler;
  }
    

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "trapdoors_ppc.hh"
# include "_trapdoors_ppc.cpp.incl"

int32 Trapdoors::trapdoor_bytes() { return _bytes_of_code; }

Trapdoors::Element  Trapdoors::  SendMessage_stub_elem( (void*)   SendMessage_stub, R0       );
Trapdoors::Element  Trapdoors::SendDIMessage_stub_elem( (void*) SendDIMessage_stub, DITempReg);
Trapdoors::Element  Trapdoors::    Recompile_stub_elem( (void*)     Recompile_stub, R0       );
Trapdoors::Element  Trapdoors::  DIRecompile_stub_elem( (void*)   DIRecompile_stub, R0       );
Trapdoors::Element  Trapdoors::SaveSelfNonVolRegs_elem( (void*) SaveSelfNonVolRegs, R0       );

pc_t Trapdoors::  SendMessage_stub_td(Location tmp) { return   SendMessage_stub_elem.start(tmp); }
pc_t Trapdoors::SendDIMessage_stub_td(Location tmp) { return SendDIMessage_stub_elem.start(tmp); }
pc_t Trapdoors::    Recompile_stub_td(Location tmp) { return     Recompile_stub_elem.start(tmp); }
pc_t Trapdoors::  DIRecompile_stub_td(Location tmp) { return   DIRecompile_stub_elem.start(tmp); }

pc_t Trapdoors::  SaveSelfNonVolRegs_td(Location tmp) { return SaveSelfNonVolRegs_elem.start(tmp); }

 
 
Trapdoors::Trapdoors(pc_t code_start, int32 code_size) {
  ResourceMark rm; // for assembler & elements
  Assembler* oldAssembler = theAssembler;
  Assembler* a = theAssembler = new Assembler(1000, 1000, false, false);
  
    SendMessage_stub_elem.gen(a, code_start);
  SendDIMessage_stub_elem.gen(a, code_start);
      Recompile_stub_elem.gen(a, code_start);
     DIRecompile_stub_elem.gen(a, code_start);
  SaveSelfNonVolRegs_elem.gen(a, code_start);
  
  _bytes_of_code = roundTo(a->instsLen(), oopSize);
  if ( _bytes_of_code > code_size )
    fatal2("trapdoors are too big: are %d bytes, should be <= %d bytes", _bytes_of_code, code_size);
  copy_bytes_up(a->instsStart, code_start, _bytes_of_code);
  
  assert( a->locsLen() == 0, "");
  
  a->finalize(); 
  theAssembler = oldAssembler;
}


void Trapdoors::Element::gen(Assembler* a, pc_t code_start) {
  _start = code_start + a->offset();
  // force long because we do not relocate the code in the trapdoors
  a->long_branch_to(dest_fn_start, VMAddressOperand, tmp_reg, false);
}


pc_t Trapdoors::follow_trapdoors(pc_t target) { 
  pc_t result;
  
  if ( ( result=     SendMessage_stub_elem.translate(target)) != NULL )  return result;
  if ( ( result=   SendDIMessage_stub_elem.translate(target)) != NULL )  return result;
  if ( ( result=      Recompile_stub_elem.translate(target)) != NULL )  return result;
  if ( ( result=    DIRecompile_stub_elem.translate(target)) != NULL )  return result;
  if ( ( result=   SaveSelfNonVolRegs_elem.translate(target)) != NULL )  return result;
  
  return target;
}


pc_t Trapdoors::Element::translate(pc_t targ) {  return targ == _start  ?  dest_fn_start  :  NULL;  }
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation  "nmethod_ppc.hh"

# include "_nmethod_ppc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

void nmethod::get_platform_specific_data(AbstractCompiler* c) {
  _max_no_of_outgoing_args_and_rcvr      = c->max_no_of_outgoing_args_and_rcvr();
  _number_of_saved_nonvolatile_registers = c->number_of_saved_nonvolatile_registers();
  _number_of_memory_locals               = c->number_of_memory_locals();
  _are_register_arguments_saved_on_stack = c->are_register_arguments_saved_on_stack();
}

void nmethod::print_platform_specific_data() {
  ++Indent;
  lprintf( "max_no_of_outgoing_args_and_rcvr      = %d\n", max_no_of_outgoing_args_and_rcvr() );
  lprintf( "number_of_saved_nonvolatile_registers = %d\n", number_of_saved_nonvolatile_registers() );
  lprintf( "number_of_memory_locals               = %d\n", number_of_memory_locals() );
  lprintf( "are_register_arguments_saved_on_stack = %d\n", are_register_arguments_saved_on_stack() );
  --Indent;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "countStub_ppc.hh"
# include "_countStub_ppc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void AgingStub::initPattern() {
    // nothing to do
  }


  pc_t CountStub::jump_addr() {
    CountCodePattern* patt = CountStub::pattern[countType()];
    int32* p = (int32*)insts();
    assert(is_lis(*(p + patt->nm_lis_offset)), "wrong pattern");
    assert(is_ori(*(p + patt->nm_ori_offset)), "wrong pattern");
    int32 n = UI(*(p + patt->nm_lis_offset)) << 16;
    return (pc_t)(n | UI(*(p + patt->nm_ori_offset)));
  }

  void ComparingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts();
    set_recompile_addr(Memory->code->trapdoors->Recompile_stub_td());
    set_count_addr(patt, (int32)&sendCounts[id()]);
    assert(is_cmpwi(*(p + patt->limit_offset)), "wrong pattern");
    fint limit = recompileLimit(nm->level());
    assert(fits_within_si(limit), "can't use cmpwi");
    set_si(p + patt->limit_offset, limit);
  }

  void AgingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts();
    set_recompile_addr(first_inst_addr(MakeOld_stub));
    set_count_addr(patt, (int32)&sendCounts[id()]);
    fint limit = nm->agingLimit();
    assert(fits_within_si(limit), "can't use cmpwi");
    set_si(p + patt->limit_offset, limit);
    set_count(1);    
  }


# ifdef UNUSED
  pc_t ComparingStub::get_recompile_addr() {
     fatal("unimp mac");
  }
# endif
  
  void ComparingStub::set_recompile_addr(pc_t addr) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts();
    assert(is_lis(*(p + patt->recompile_lis_offset)), "wrong pattern");
    assert(is_ori(*(p + patt->recompile_ori_offset)), "wrong pattern");
    int32 lo, hi;
    Assembler::break_up_word_for_oring(int32(addr), lo, hi);
    set_ui(p + patt->recompile_lis_offset, hi);
    set_ui(p + patt->recompile_ori_offset, lo);
  }
  

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */


# pragma implementation "addrDesc_ppc.hh"

# include "_addrDesc_ppc.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


bool addrDesc::isShiftNeededAfterMovingMe(OopNCode* m) {
  return isBranch((inst_t*)addr(m));
}


pc_t addrDesc::instr_referent(OopNCode* m) {
  inst_t* instp = (int32*)addr(m);
  assert(m->contains(instp), "not in this nmethod");
  if (isShifted()) {
    // should be a lis followed by a load/store/addi/ori
    assert(is_immediate_pair(instp), "");
    return (pc_t) immediate_pair_target(instp);
  }
  else {
    // should be an immediate, uncond branch
    assert( isUnconditionalImmediateBranch( *instp), "");
    return (pc_t) unconditionalImmediateBranch_target(instp);
  }
}


void addrDesc::set_instr_referent(OopNCode* m, void* newVal) {
  inst_t* instp = (int32*)addr(m);
  assert(m->contains(instp), "not in this nmethod");
  if (isShifted()) {
    // should be a lis followed by a load/store/addi/ori
    assert(is_immediate_pair(instp), "");
    set_immediate_pair_target(instp, (int32) newVal);
  }
  else {
    // should be an immediate, uncond branch
    assert( isUnconditionalImmediateBranch( *instp), "");
    set_unconditionalImmediateBranch_target(instp, (int32)newVal);
  }
}


void addrDesc::relocateTarget(OopNCode* m, int32 delta) {
  // shift only if need be
  if (isEmbedded()  &&  !isShifted()  && !absoluteBit(*(inst_t*)addr(m))) {
    assert(isUnconditionalImmediateBranch(*(inst_t*)addr(m)), "");
    set_instr_referent(m, instr_referent(m) - delta);
  }
}
  

bool addrDesc::verify(nmethod* m) {
  bool flag = true;
  if (offset() >= m->instsLen() + m->scopes->length()) {
    error1("bad offset in addrDesc at %#lx", (long)this);
    flag = false;
  }
  if (isSendDesc()) {
    flag = asSendDesc(m)->verify() && flag;
  }
  else if (isDIDesc()) {
    flag = asDIDesc(m)->dependency()->verify_list_integrity() && flag;
  }
  else if (isPrimitive())
    ;
  else 
    flag = oop(referent(m))->verify_oop() && flag;
    
  inst_t* instp = (int32*)addr(m);
  if (!isEmbedded()) 
    ;
  else if (isShifted()) {
    if (!is_immediate_pair(instp)) {
      error("could not find immediate pair for shifted addrDesc");
      flag = false;
    }
  }
  else {
    if (!isUnconditionalImmediateBranch(*instp)) {
      error("could not find uncond. imm. branch for unshifted addrDesc");
      flag = false;
    }
  }
  return flag;
}

static int32 offset_to_call_inst(bool is_shifted) {
  return
    is_shifted ? 3  // magic! sequence is: lis + ori + mtlr + balwlrl
               : 0; // the branch itself
}

// not inlined to reduce .h dependencies
sendDesc* addrDesc::asSendDesc(OopNCode* m) {
  assert(isSendDesc(), "not a sendDesc location");
  return sendDesc::sendDesc_from_call_instruction((inst_t*)addr(m) + offset_to_call_inst(isShifted()));
}
  
sendDesc* addrDesc::asPrimitiveSendDesc(OopNCode* m) {
  // note that it's not really an inline cache, just a primitive call
  assert(isPrimitive(), "not a primitive location");
  return sendDesc::sendDesc_from_call_instruction((inst_t*)addr(m) + offset_to_call_inst(isShifted()));
}
  
DIDesc* addrDesc::asDIDesc(nmethod* m) {
  assert(isDIDesc(), "not a diDesc location");
  return DIDesc::DIDesc_from_call_instruction((inst_t*)addr(m) + offset_to_call_inst(isShifted()));
}





# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */
  
# pragma implementation "regs_ppc.hh"
  
# include "_regs_ppc.cpp.incl"


// change Location enum in regs_ppc.h if you change this!
char* RegisterNames[] = {
    "r0",  "sp",  "rtoc", "r3",   "r4",  "r5",  "r6",  "r7",
    "r8",  "r9",  "r10",  "r11",  "r12", "r13", "r14", "r15",
    "r16", "r17", "r18",  "r19",  "r20", "r21", "r22", "r23",
    "r24", "r25", "r26",  "r27",  "r28", "r29", "r30", "r31",
    
    "*UnAllocated*"
};
    
char *StackArgRegisterNames[] = {
  "A0", "A1", "A2", "A3", "A4", "A5", "A6", "A7", 
  "A8", "A9", "A10", "A11", "A12", "A13", "A14", "A15"
};

char *StackIArgRegisterNames[] = {
  "I0", "I1", "I2", "I3", "I4", "I5", "I6", "I7", 
  "I8", "I9", "I10", "I11", "I12", "I13", "I14", "I15"
};
    
char *StackLocationNames[] = {
  "S0", "S1", "S2", "S3", "S4", "S5", "S6", "S7", 
  "S8", "S9", "S10", "S11", "S12", "S13", "S14", "S15"
};
    
Location ArgRegisters[] = {
    Arg1, Arg2, Arg3, Arg4, Arg5, Arg6, Arg7,
    IllegalLocation
};

Location IArgRegisters[] = {
    R30, R29, R28, R27, R26, R25, R24, R23,
    IllegalLocation
};

  
static char* locationNameHelper(Location base, int num) {
  char c;
  char **tbl;
  switch (base) {
    case  ArgStackLocations:     tbl=   StackArgRegisterNames;  c= 'A';  break;
    case IArgStackLocations:     tbl=  StackIArgRegisterNames;  c= 'I';  break;
    case     StackLocations:     tbl=      StackLocationNames;  c= 'S';  break;
    default: ShouldNotReachHere();
  }
  if (num < sizeof(StackArgRegisterNames)/sizeof(StackArgRegisterNames[0])) {
    return tbl[num];
  }
  char* s= new char [30]; // known leak NEW_RESOURCE_ARRAY(char, 30);
  sprintf(s, "%c%ld",c, long(num));
  return s;
}

char *locationName(Location l) {
  Location base;
  int num;
  
  if          ( is_IArgStackLocation(l)) { base = IArgStackLocations;  num= l - base; }
  else if     (  is_ArgStackLocation(l)) { base =  ArgStackLocations;  num= l - base; }
  else if     (     is_StackLocation(l)) { base =     StackLocations;  num= l - base; }
  else if     ( l == PerformSelectorLoc) { return "(perform selector)" ; }
  else if     (l == PerformDelegateeLoc) { return "(perform delegatee)"; }
  else {
    assert(isRegister(l), "");
    return RegisterNames[l];
  }
  return locationNameHelper(base, num);
}
    

# if defined(SIC_COMPILER)
  Location TempRegs[] = { R3, R4, R5, R6, R7, R8, R9, R10 };
    
# define X(arg) -99999999     /* to make the following table look nicer */
    
  fint RegToTempNo[/* indexed by Location */] = {
    X("R0"), X("SP"), X("RTOC"), 0, 1, 2, 3, 4,
    5, 6, 7, X("R11"), X("R12"), X("R13"), X("R14"), X("R15"),
    X("R16"), X("R17"), X("R18"), X("R19"), X("R20"), X("R21"), X("R22"), X("R23"),
    X("R24"), X("R25"), X("R26"), X("R27"), X("R28"), X("R29"), X("R30"), X("R31")
  };

  Location CalleeSavedRegs[] = {
    R31, R30, R29, R28, R27, R26, R25, R24, R23,
    R22, R21, R20, R19, R18, R17, R16, LowestLocalNonVolReg
  };

  void regs_ppc_init() {
    // The SIC uses r11 and r12 during code generation, that's why they
    // aren't listed as general temp regs above.
    assert(Temp1 == R11 && Temp2 == R12, "change this");
    for (fint i = 0; i < NumTempRegs; i++) {
      assert(TempRegs[i] != R11 && TempRegs[i] != R12,
             "R11 and R12 are reserved for the code generator");
      assert(RegToTempNo[TempRegs[i]] == i, "wrong RegToTempNo entry");
    }
  }


# else

void regs_ppc_init() {}

# endif
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

  

# pragma implementation "fields_ppc.hh"

# include "_fields_ppc.cpp.incl"


  
bool is_immediate_pair(inst_t* instp) {
  // find out if instp points to lis followed by ori/addi/load/store
  inst_t i1 = instp[0];
  if (!is_lis(i1)) return false;
  fint reg = RT(i1);
  inst_t i2 = instp[1];
  return
       is_ori(i2)                    &&  RA(i2) == reg  &&  RS(i2) == reg 
  ||   is_addi(i2)                   &&  RT(i2) == reg  &&  RS(i2) == reg
  ||   is_load_store_immediate(i2)   &&  RA(i2) == reg;
}

  
int32 immediate_pair_target(inst_t* instp) {
  // find out if instp points to lis followed by ori/addi/load/store
  inst_t i2 = instp[1];
  return   is_ori(i2)
             ?    SI(instp[0]) << si_bits  |  UI(i2)
             :    SI(instp[0]) << si_bits  +  SI(i2);
}


pc_t  get_target_of_branch_instruction(inst_t* instp) {
  switch (OP(*instp)) {
   case opcd_Branch:             return (pc_t) unconditionalImmediateBranch_target(instp);
   case opcd_BranchConditional:  return (pc_t)   conditionalImmediateBranch_target(instp);
   default:                      return (pc_t) NULL;
  }
}


void set_target_of_branch_instruction(inst_t* instp, void* newVal) {
  inst_t nv = inst_t(newVal);
  inst_t inst = *instp;
  
  switch (OP(inst)) {
   case opcd_Branch:             set_unconditionalImmediateBranch_target(instp, nv);  break;
   case opcd_BranchConditional:    set_conditionalImmediateBranch_target(instp, nv);  break;
   default: fatal("not a branch");
  }
}


pc_t get_target_of_C_call_site(inst_t* instp) {
  return get_target_of_branch_instruction(instp);
}


pc_t get_target_of_Self_call_site(inst_t* instp) {
  // given ptr to the instruction before the return PC, return
  // the branch destination
  // WARNING: this routine knows the sequence of instructions generated by
  //   CodeGen::cPrimCall and by CodeGen::selfCall, also
  //   PrimNode::gen()  and SendNode::gen()
  inst_t* ip = instp; // ip points to next instruction to look at (moves backwards)
  
  pc_t r = get_target_of_branch_instruction(ip);
  if ( r != NULL ) {
    --ip; // we have the answer! (in one instruction)
  }  
  else if ( is_branch_to_ctr(ip[0])  &&  is_mtctr(ip[-1])
       ||   is_branch_to_lr (ip[0])  &&  is_mtlr (ip[-1]) ) {
    assert( is_immediate_pair(&ip[-3]), "");
    r = (pc_t) immediate_pair_target(&ip[-3]);
    ip -= 4; // preceding instruction is at ip - 4
  }
  else
    fatal("could not parse branch sequence");    

  // When calling C, Self MAY indirect through SaveSelfNonVolRegs
  if (r == Memory->code->trapdoors->SaveSelfNonVolRegs_td()) {
    --ip;  // back up to first one of pair
    assert( is_immediate_pair(ip), "");
    r = (pc_t) immediate_pair_target(ip);
  }
    
  return r;
}



void set_target_of_Self_call_site(inst_t* instp, void* target) {
  // WARNING: this routine knows the sequence of instructions generated by
  //   CodeGen::cPrimCall and by CodeGen::selfCall, and by call to
  //   SendDIDesc_stub in CodeGen::prologue
  inst_t* ip = instp; // ip points to next instruction to look at (moves backwards)
  pc_t    nv_stub = Memory->code->trapdoors->SaveSelfNonVolRegs_td();
  
  pc_t r = get_target_of_branch_instruction(ip);
  if ( r == nv_stub ) {
    // Calling via this stub, must look further back
    --ip;
  }
  else if ( r != NULL ) {
    // found it!
    set_target_of_branch_instruction(ip, target);
    return;
  }
  // not a simple jump; look for a complex one
  else if ( is_branch_to_ctr(ip[0])  &&  is_mtctr(ip[-1])
       ||   is_branch_to_lr (ip[0])  &&  is_mtlr (ip[-1]) ) {
    ip -= 3;
    assert( is_immediate_pair(ip), "");
    r = (pc_t) immediate_pair_target(ip);
    if ( r != nv_stub ) {
      // found it!
      set_immediate_pair_target(ip, (inst_t)target);
      return;
    }
    --ip; // will look prior to pair
  }
  else
    fatal("could not parse branch sequence");    

  --ip; // start of pair
  assert( is_immediate_pair(ip), "");
  set_immediate_pair_target(ip, (inst_t)target);
}


void set_immediate_pair_target(inst_t *instp, int32 nv) {
  int32 lo, hi;
  inst_t i0 = instp[0];
  inst_t i1 = instp[1];
  if ( is_ori(i1) )   Assembler::break_up_word_for_oring (nv, lo, hi);
  else                Assembler::break_up_word_for_adding(nv, lo, hi);
  instp[0] = i0 & ~si_mask  |  hi;
  instp[1] = i1 & ~si_mask  |  lo;  assert(si_mask == ui_mask, "");
  MachineCache::flush_instruction_cache_range(&instp[0], &instp[2]);
}


char* address_of_overwritten_NIC_save_instruction(int32* orig_save_addr) {
  if (is_stwu(*orig_save_addr))
    return NULL; // not overwritten
    
  // the instruction must have been patched with a branch;
  assert(isUnconditionalImmediateBranch(*orig_save_addr), "must be an uncond branch");
  return (char*)unconditionalImmediateBranch_target(orig_save_addr);
}


void check_branch_relocation( void* fromArg, void* toArg, int32 countArg) {
  inst_t *from = (inst_t*)fromArg, *to = (inst_t*)toArg;
  int32 count = countArg / sizeof(inst_t);
  
  if ( umax((uint32)from, (uint32)to)  <  umin((uint32)from + count, (uint32) to + count))
   return; // overlapping, too hard to check
  
  for ( int32 i = 0;  i < count;  ++i) {
    inst_t* f = (inst_t*)get_target_of_branch_instruction(&from[i]);
    inst_t* t = (inst_t*)get_target_of_branch_instruction(&  to[i]);

         if (f == NULL)                      ;
    else if (from <= f  &&  f < from+count)  f += to - from;

    if (f != t)
      fatal2("check_branch_relocation: branch at 0x%x moved to 0x%x but is wrong", 
             &from[i], &to[i]);
  }
}

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.14 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "asm_ppc.hh"
# pragma implementation "asm_inline_ppc.hh"  
  
# include "_asm_ppc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


Assembler* theAssembler;      // current assembler for instructions
  

Assembler::Assembler(int32 instsSize, int32 locsSize, bool pr, bool isInstrs)
  : BaseAssembler(instsSize, locsSize, pr, isInstrs) {
  
  if (bootstrapping)
    return;  // zone does not exist yet
    
  pc_t buf_lo  = instsStart,
       buf_hi  = instsOverflow,
       zone_lo = Memory->code->code_start(), // warning counts on order of iZone and stubs
       zone_hi = Memory->code->code_end();
       if (  buf_hi <= zone_lo )  li_branches_will_fit = fits_within_li( zone_hi -  buf_lo );
  else if ( zone_hi <= buf_lo  )  li_branches_will_fit = fits_within_li(  buf_hi - zone_lo );
  else fatal("zone and buffer overlap");
  
}
  
# ifdef SIC_COMPILER
    void Assembler::unimp(fint n, bool shouldRestart) {
      // add an addrDesc for the unimp instruction so they're easier to find
      int32 mask = offset();
      genLoc(mask | addrDesc::isUncommonTrapMask);
      assert(n < MaxUnimpImm / 2, "unimp immediate too large");
      if (shouldRestart) n |= UncommonRestartBit;
      Data(n);
    }
# endif

  
// branch and link to arbitrary address, temp may be any GPR
// Some callers (e.g. the branch to SendDIMessage_Stub)
// count on the fact that I do NOT disturb the link register.
void Assembler::branch_to(pc_t dest, OperandType t, Location temp, bool link) {
  bool use_short = false; // can we use a relative branch? (be conservative, assume it will be backpatched)

  if   (!UseShortBranches)        use_short = false;
  else {
    // have to check
    // branch will eventually be in zone
    char* lo = Memory->code->code_start();
    char* hi = Memory->code->code_end();
    if (dest > hi)
      use_short = fits_within_li(dest - lo);
    else if (dest < lo)
      use_short = fits_within_li(hi - dest);
    else
      use_short = fits_within_li(hi - lo);
      
    assert(!use_short || fits_within_li(hi - lo), "");
    // have to get to dest from inst assembly location, too
    if (use_short)
      use_short = fits_within_li(dest > addr()  ?  dest - addr()  :  addr() - dest);
  }

  if ( use_short )   short_branch_to(dest, t, temp, link);
  else                long_branch_to(dest, t, temp, link);
}

  
  
// branch and link to arbitrary address, temp may be any GPR
// Some callers (e.g. the branch to SendDIMessage_Stub)
// count on the fact that I do NOT disturb the link register.
void Assembler::long_branch_to(pc_t dest, OperandType t, Location temp, bool link) {
  load_immediate( temp, (int32)dest, t);
  mtctr(temp); // use count register so we can use this in method prologue before link is saved
  if (link)   balwctrl(); 
  else        balwctr();
}

  
  
// short branch and link to arbitrary address, temp may be any GPR
// Some callers (e.g. the branch to SendDIMessage_Stub)
// count on the fact that I do NOT disturb the link register.

// To speed the compiler, it calls this routine directly.
// But, if the spans become too large (because the resource area is too far away),
// it could always call branch_to. -- dmu 6/99

void Assembler::short_branch_to(pc_t dest, OperandType t, Location /*temp*/, bool link) {
  Label L(printing);
  L.define(dest);
  if (link)   bl(L, t); 
  else        b (L, t);
}


void Assembler::load_immediate( Location dest, int32 data, OperandType t ) {
  assert( isRegister(dest), "must be register");
  bool data_could_change = t == OopOperand  &&  oop(data)->is_mem()   ||   t == CodeAddressOperand;
  if ( !data_could_change  &&  fits_within_si(data)) {
    // NumberOperand instead of t because we know data cannot change
    // If we used t, then add_offset would fail because
    // it only knows that data is an oop, so it thinks it needs to 
    // relocate it (even though it is an immedate oop).
    // However, add_offset cannot
    // deal with an li instruction that is not after a lis. -- Ungar
    li(dest, data, NumberOperand);
  }
  else {
    int32 lo, hi;
    break_up_word_for_oring(data, lo, hi);
    lis(dest, hi, t, KF_shifted_for_ui); // set high-order bits
    // always do ori, even if zero, for patching later
    ori(  dest, dest, lo, t); // or in low order bits
  }
}


void Assembler::load_from_address( Location dest, void* p, OperandType t, Location base_reg ) {
  if (base_reg == IllegalLocation)  base_reg = dest;
  assert( isRegister(dest) && isRegister(base_reg) && base_reg != R0, "must be non-zero register");
  int32 lo, hi;
  break_up_word_for_adding((int32)p, lo, hi);
  lis(base_reg, hi, t, KF_shifted_for_si); // set high-order bits
  lwz(  dest, lo, t, base_reg); // or in low order bits
}


void Assembler::store_to_address( Location src, void* p, OperandType t, Location base_reg ) {
  assert( isRegister(src) && isRegister(base_reg) && base_reg != R0 && base_reg != src, "must be non-zero register");
  int32 lo, hi;
  break_up_word_for_adding((int32)p, lo, hi);
  lis(base_reg, hi, t, KF_shifted_for_si); // set high-order bits
  stw(  src, lo, t, base_reg); // or in low order bits
}
  
 
// patch instruction's immediate field by delta
void Assembler::increase_immediate_field( pc_t a, int32 delta ) {
  inst_t& x = *(inst_t*)a;
  int32 si = ::SI(x);
  si += delta;
  x = (x & ~SI(~0)) | SI(si);
}

// patch all instructions immediate field by delta
void Assembler::increase_all_immediate_fields( AddressList* l, int32 delta) {
  for (int i = 0;  i < l->length();  ++i)
    increase_immediate_field(l->nth(i), delta);
}


# if COMPILER == MWERKS_COMPILER

extern "C" asm void Untested_Stub() {
# define out_of_the_way 320 // more than frame could possibly be
# define i  roundTo(/*linkage_area_end*/6 + 2 + 32 + 8, 4) //quad word + 1 for ctr, xer, + 8 for outgoing arg (warning thinks it's getting lots args)
# define wSize 4

  subi sp, sp, out_of_the_way;
  subi sp, sp, i * wSize
  stmw r0, ((i - 32) * wSize)(sp)
  mfctr r0
  stw r0, ((i - 33) * wSize)(sp)
  mfxer r0
  stw r0, ((i - 34) * wSize)(sp)
  mflr r0
  stw r0, ((i + /*saved_pc_offset*/2) * wSize)(sp)
  mfcr r0
  stw r0, ((i + /*saved_cr_offset*/1) * wSize)(sp)
  mflr r3
  lwz r3, 0(r3)
  lwz r4, warning(RTOC)
  lwz r4, 0(r4)
  mtlr r4
  blrl
  lwz r0, ((i + /*saved_cr_offset*/1) * wSize)(sp)
  mtcrf 0xff, r0
  lwz r3, ((i + /*saved_pc_offset*/2) * wSize)(sp)
  addi r3, r3, wSize // cannot addi to r0
  mtlr r3
  lwz r0, ((i - 34) * wSize)(sp)
  mtxer r0
  lwz r0, ((i - 33) * wSize)(sp)
  mtctr r0
  lmw r0, ((i - 32) * wSize)(sp)
  addi sp, sp, i * wSize
  addi sp, sp, out_of_the_way
  blr
# undef out_of_the_way
# undef i
# undef wSize
}

# elif COMPILER == GCC_COMPILER
  extern "C" void Untested_Stub() {
    fatal("unimp for OS X");
  }
# else
  # error which?
# endif // MWERKS_COMPILER


void Assembler::_Untested(char* msg, Location temp1, bool save_link) {
  // assumes we may not have a frame
  
  // too much: lprintf("generated untested code for: %s\n", msg);
  
  Comment("start of Untested sequence");
  if (save_link) mflr(R0); // save link
  branch_to(first_inst_addr(Untested_Stub), PVMAddressOperand, temp1, true);
  Data((int32)msg);
  if (save_link) mtlr(R0); // restore link
  Comment("end of Untested sequence");
}


// Don't pass in OopOperand, unless the data is a memOop

void Assembler::add_offset(OperandType t, Kind_of_Instruction_Field f) {
  // eliminate common & simple cases quickly, call slow version for
  // real work
  switch (t) {
     case NumberOperand:      return; // also used for relative branches within this nmethod
     case VMAddressOperand:   if (f == KF_li_rel) break;  else return; // need info to fix relative branch
     case PVMAddressOperand:  break; // need to be able to find primitive call for e.g. conversion ZZZ (was like above)
     case BPVMAddressOperand: break; // need to be able to find inline cache
     case CodeAddressOperand: break; // ditto
     case DIVMAddressOperand: break; // need to be able to find DI desc
     case OopOperand:         break; // need to scavenge OOP
     default: ShouldNotReachHere();
  }
  int32 mask = 0;
  switch (f) {
    default: ShouldNotReachHere();
    // for now can discard shifted distinctions since we only allow for shifted/non pairs
    // someday, when we store the referrent as well as the offset, we will pass it through
    case KF_shifted_for_si:
    case KF_shifted_for_ui:  mask |= addrDesc::isShiftedMask;  break;
    
    case KF_li_abs:
    case KF_li_rel:  break; // discover this from the instruction
    
    case KF_si:
    case KF_ds:
    case KF_ui:
       // because I do not want to introduce varying-length descs,
       // I do not want to store the referrent separately.
       // So, these has better follow a shfited instruction:
       assert( locsLen() >= 1  
           &&  locsEnd[-1].isShifted()
           &&  locsEnd[-1].offset()  ==  offset() - oopSize,
               "stinky restriction");
       return; // will handle it via the shifted one
  }
  doAddOffset(t, true, mask);
}



  
extern "C" void TestAsm();


void Assembler::test_ppc_asm() {
  test_BranchI();
  test_BranchConditionals();
  sc();
  test_CondRegLogicals();
  test_LoadStores();
  test_FixedPointArith();
    
  // TestAsm(); 
}


void Assembler::check() {
  int32 pc = offset() - sizeof(inst_t);
  pc /= sizeof(inst_t);
  inst_t& mine   = ((inst_t*)instsStart)[pc];
  inst_t& apples = ((inst_t*)first_inst_addr(TestAsm))[pc];
  if (mine == apples)  return;
  warning5("asm bug: pc %d, mine 0x%x, apple's 0x%x, addresses: &mine = 0x%x, &apples = 0x%x\n", pc, mine, apples, &mine, &apples);
}

//dummy
void TestAsm() {}

void Assembler::test_BranchI() {
  Label La; La.define( addr()              );  b(   La, NumberOperand);  check();
  Label Lb; Lb.define( pc_t(16)            );  ba(  Lb, NumberOperand);  check();
  Label Lc; Lc.define( pc_t(( 1 << 25)-4 ) );  ba(  Lc, NumberOperand);  check();
  Label Ld; Ld.define( addr() - ( 1 << 25) );  b(   Ld, NumberOperand);  check();
  Label Le; Le.define( pc_t(1 << 25) - 4   );  ba(  Le, NumberOperand);  check();
  Label Lf; Lf.define( pc_t( -( 1 << 25))  );  ba(  Lf, NumberOperand);  check();
  Label Lg; Lg.define( addr() - 4          );  bl(  Lg, NumberOperand);  check();
  Label Lh; Lh.define( addr() + 4          );  bl(  Lh, NumberOperand);  check();
  Label Li; Li.define( 0                   );  bla( Li, NumberOperand);  check();
  Label Lj; Lj.define( pc_t(-4)            );  bla( Lj, NumberOperand);  check();
  
  
  // Next few should and do fail:
  
  // bl(int(addr()) + 1);
  // bla(2);
  // b( int(addr()) + (1 << 26) );
  // ba( 1 << 26 );
  // b( int(addr()) + (1 << 25) );
  // ba( 1 << 25 );
  // ba( -( 1 << 25) - 1);
  
  DefinedLabel L1(false);
  Label L2;
  L2.define((pc_t)0x1230);
  
  
  b(L1, NumberOperand);  check();
  ba(L2, NumberOperand);  check();
  
  Label L3;
  Label L4;
  
  bl(L3, NumberOperand);  check();
  bla(L4, NumberOperand);  check();
  
  L3.define();
  L4.define((pc_t)0x3450);
}


void Assembler::test_BranchConditionals() {
  // Label cases: 0, -4, +4, -8000, +7ffff, -8001, +8000, 2
  Label A0;      A0    .define((pc_t)0);
  Label Am4;     Am4   .define((pc_t)-4);
  Label A4;      A4    .define((pc_t)4);
  Label Am8000;  Am8000.define((pc_t)-0x8000);
  Label A7ffc;   A7ffc .define((pc_t)0x7ffc);
  
  Label Am8004;  Am8004.define((pc_t)-0x8004);
  Label A8000;   A8000 .define((pc_t)0x8000);
  Label A2;      A2    .define((pc_t)2);
  
  blta( A0 );  check();
  blea( Am4, predict_weird);  check();
  bgea( A4, predict_usual, cr1);  check();
  bgta( Am8000, predict_weird, cr2);  check();
  beqa( A7ffc, predict_usual, cr3);  check();
  bnea( A0, predict_weird, cr4);  check();
  bsoa( A0, predict_usual, cr5);  check();
  bnsa( A0, predict_weird, cr6);  check();
  buna( A0, predict_usual, cr7);  check();
  balwa( A0);  check();
  
  bltla( A0 );   check();
  balwla( A4);  check();
  
  bdzlta( A0 );   check();
  bdnzlta( A0);  check();
  bdzgea( A0 );   check();
  bdnzgea( A0);  check();
  
  bdzltla( A0 );   check();
  bdnzltla( A0);  check();
  bdzgela( A0 );   check();
  bdnzgela( A0);  check();
  
  // will fail:
  // blta(Am8004);
  // blta(A8000);
  // blta(A2);
  

  // relative branches:

  
  Label LR0; LR0.define(addr());
  Label LR1; 
  
  blt( LR0 );  check();
  ble( LR1, predict_weird);  check();
  bge( LR0, predict_usual, cr1);  check();
  bgt( LR1, predict_weird, cr2);  check();
  beq( LR0, predict_usual, cr3);  check();
  bne( LR1, predict_weird, cr4);  check();
  bso( LR0, predict_usual, cr5);  check();
  bns( LR1, predict_weird, cr6);  check();
  bun( LR0, predict_usual, cr7);  check();
  balw( LR1);  check();
  
  bltl( LR0 );   check();
  balwl( LR1);  check();
  
  bdzlt( LR0 );   check();
  bdnzlt(LR1);  check();
  bdzge( LR0 );   check();
  bdnzge( LR1);  check();
  
  bdzltl( LR0 );   check();
  bdnzltl(LR1);  check();
  bdzgel( LR0 );   check();
  bdnzgel(LR1);  check();
  
  bltlr( predict_weird, cr3 );  check();
  
  LR1.define(addr());  
  
  // to registers:
  
  bdzeqlrl();  check();
  bltctr();  check();
  bdzeqlrl(predict_weird);  check();
  bltctr(predict_weird);  check();
}


void Assembler::test_CondRegLogicals() {
  crand( cond_lt, cond_gt, cond_eq);  check();
  cror( cond_so, cond_un, cond_lt);  check();
  crxor( cond_lt, cond_gt, cond_eq);  check();
  crnand(cond_so, cond_un, cond_lt);  check();
  crnor( cond_lt, cond_gt, cond_eq);  check();
  creqv( cond_lt, cond_gt, cond_eq);  check();
  crandc( cond_lt, cond_gt, cond_eq);  check();
  crorc( cond_lt, cond_gt, cond_eq);  check();
  crmove( cond_lt, cond_gt);  check();
  crclr( cond_lt);  check();
  crnot( cond_lt, cond_gt);  check();
  crset( cond_lt);  check();
}

void Assembler::test_LoadStores() {
  lbz(  SP, 0, NumberOperand, R4);  check();
  lbzu( SP, 1, NumberOperand, R4 );  check();
  lhz(  SP, -1, NumberOperand, R4 );  check();
  lhzu( SP, 0x7fff, NumberOperand, R4 );  check();
  lha(  SP, -0x8000, NumberOperand, R4 );  check();
  lhau( SP, 0, NumberOperand, R4 );  check();
  lwz(  SP, 0, NumberOperand, R4 );  check();
  lwzu( SP, 0, NumberOperand, R4 );  check();
  lwax( R3, R4, R5);  check();
  lwaux( R5, R6, R7);  check();
  ldx( R3, R4, R5);  check();
  ldux(R3, R4, R5);  check();
  
  stb(  SP, 0, NumberOperand, R4 );  check();
  stbu( SP, 0, NumberOperand, R4 );  check();
  sth(  SP, 0, NumberOperand, R4 );  check();
  sthu( SP, 0, NumberOperand, R4 ) ;  check();
  stw(  SP, 0, NumberOperand, R4 );  check();
  stwu( SP, 0, NumberOperand, R4 );  check();
  
  lbzx(  SP, RTOC, R6 );  check();
  lbzux( R31, R30, R29 );  check();
  lhzx(  R31, R30, R29 );  check();
  lhzux( R31, R30, R29 );  check();
  lhax(  R31, R30, R29 );  check();
  lhaux( R31, R30, R29 );  check();
  lwzx(  R31, R30, R29 );  check();
  lwzux( R31, R30, R29 );  check();
  stbx(  R31, R30, R29 );  check();
  stbux( R31, R30, R29 );  check();
  sthx(  R31, R30, R29 );  check();
  sthux( R31, R30, R29 );  check();
  stwx(  R31, R30, R29 );  check();
  stwux( R31, R30, R29 );  check();
  stdx( R31, R30, R29);  check();
  stdux( R31, R30, R29 );  check();
  
  
  
  lwa( R3, 12, NumberOperand, R4);  check();
  ld( R3, 60, NumberOperand, R4);  check();
  ldu( R3, 0, NumberOperand, R4);  check();
  
  std( R31, -4, NumberOperand, R30);  check();
  stdu( R31, 0, NumberOperand, R30);  check();

  lhbrx( R5, R6, R7);  check(); 
  lwbrx( R5, R6, R7);  check(); 
  sthbrx( R5, R6, R7);  check(); 
  stwbrx( R5, R6, R7);  check(); 
  
  lmw( SP, 0, NumberOperand, R4);  check();
  stmw( R30, 0, NumberOperand, R31 );  check();  
  
  lswx( R3, R4, R5);  check();
  stswx( R6, R7, R8);  check();
  
  lswi( R3, R4, 32);  check();
  lswi( R5, R6, 1);  check();
  stswi( R7, R9, 3);  check();
  
  // should fail:
  // lswi( R6, R8, 0, NumberOperand);  check();
  // lswi( R4, R4, -1, NumberOperand);  check();
  // lswi( R3, R4, 33, NumberOperand);  check();
  
  lwarx( R3, R4, R5);  check();
  ldarx( R3, R4, R5);  check();
  stwcx_( R3, R4, R5);  check();
  stdcx_( R3, R4, R5);  check();
  
  sync();  check();
  
}

void Assembler::test_FixedPointArith() {
  addi( R3, R4, 0, NumberOperand);  check();
  addis( R30, R31, 1, NumberOperand, KF_shifted_for_si);  check();
  addic( R0, R4, -1, NumberOperand);  check();
  addic_( R3, R4, 0x7fff, NumberOperand);  check();
  subfic( R5, R6, -0x8000, NumberOperand);  check();
  mulli( R3, R4, 5, NumberOperand);  check();
  
  // should fail
  // addi(R3, R4, 0x8000, NumberOperand);  check();
  // addi(R3, R4, -0x8001, NumberOperand);  check();
  
  li( R3, 16, NumberOperand);  check();
  la( R3, -16, NumberOperand, R4);  check();
  subi( R4, R5, 7, NumberOperand);  check();
  lis( R3, 32, NumberOperand, KF_shifted_for_si);  check();
  subis( R3, R31, 64, NumberOperand, KF_shifted_for_si);  check();
  
  add(R0, SP, RTOC);  check();
  add_(R3, R4, R5);  check();
  addo(R30, R31, R29);  check();
  addo_(R3, R4, R5);  check();
  subf(R3, R4, R5);  check();
  addc(R0, R0, R0);  check();
  subfc(R31, R31, R31);  check();
  adde(R31, R31, R31);  check();
  subfe(R31, R31, R31);  check();
  mulld(R31, R31, R31);  check();
  mullw(R31, R31, R31);  check();
  divd(R31, R31, R31);  check();
  divw(R31, R31, R31);  check();
  divdu(R31, R31, R31);  check();
  divwu(R31, R31, R31);  check();
  
  addme(R3, R4);  check();
  subfme_(R31, R30);  check();
  addzeo(R3, R4);  check();
  subfzeo_(R0, R0);  check();
  neg(R3, R4);  check();
  
  mulhd( R0, R0, R0 );  check();
  mulhw( R0, R0, R0 );  check();
  mulhdu( R0, R0, R0);  check();
  mulhwu( R0, R0, R0);  check();
  
  
  cmpwi( R0, 1, NumberOperand );  check();
  cmpdi( R31, 0, NumberOperand, cr1 );  check();
  cmpw( R31, R30, cr7 );  check();
  cmpd( R3, R4 );  check();
  cmplwi( R7, 0xffff, NumberOperand);  check();
  cmpldi( SP, 0, NumberOperand);  check();
  cmplw( R3, R4);  check();
  cmpld( R5, R6);  check();
  
  trap();  check();
  twlt(R3, R4);  check();
  twlt(R31, R31);  check();
  tdlt(R31, R31);  check();
  twle(R0, R0);  check();
  tweq(R3, R4);  check();
  tdeq(R3, R4);  check();
  twge(R3, R4);  check();
  twgt(R0, R0);  check();
  tdgt(R0, R0);  check();
  twnl(R30, R31);  check();
  twne(R0, SP);  check();
  tdne(R0, SP);  check();
  twng(R3, R4);  check();
  twllt(R3, R4);  check();
  tdllt(R3, R4);  check();
  twlle(R3, R4);  check();
  twlge(R5, R6);  check();
  tdlge(R5, R6);  check();
  twlgt(SP, R3);  check();
  twlnl(R0, R3);  check();
  tdlnl(R0, R3);  check();
  twlng(R30, R31);  check();

  twlti(R3, 0, NumberOperand);  check();
  tdlti(R3, 0, NumberOperand);  check();
  twlei(R31, 1, NumberOperand);  check();
  tweqi(R0, -1, NumberOperand);  check();
  twgei(RTOC, 0x7fff, NumberOperand);  check();
  twgti(R4, -0x8000, NumberOperand);  check();
  twnli(R0, 2, NumberOperand);  check();
  
  // should fail
  // tdnei(R31, 0x8000, NumberOperand);  check();
  // twlnli(R0, -0x8001, NumberOperand);  check();
  
  andi_(R3, R4, 0, NumberOperand);  check();
  andis_(R0, R0, 1, NumberOperand, KF_shifted_for_si);  check();
  ori(R31, R30, 0xffff, NumberOperand);  check();
  oris(R3, R4, 17, NumberOperand, KF_shifted_for_si);  check();
  xori(R3, R4, 1, NumberOperand);  check();
  xoris(R3, R4, 0, NumberOperand, KF_shifted_for_si);  check();
  
  // should fail
  // ori(R0, R0, -1, NumberOperand);  check();
  // ori(R0, R0, 0x10000, NumberOperand);  check();
   
  And(R3, R4, R5);  check();
  And_(R5, R6, R7);  check();
  Or(R31, R30, R29);  check();
  Or_(R0, R0, R0);  check();
  Xor(R3, R4, R5);  check();
  Xor_(R3, R4, R5);  check();
  nand(R3, R4, R5);  check();
  nand_(R3, R4, R5);  check();
  nor(R3, R4, R5);  check();
  nor_(R3, R4, R5);  check();
  eqv(R3, R4, R5);  check();
  eqv_(R3, R4, R5);  check();
  andc(R3, R4, R5);  check();
  orc(R3, R4, R5);  check();
  
  mr(R3, R4);  check();
  mr_(R3, R4);  check();
  
  Not(R3, R4);  check();
  
  extsb(R3, R4);  check();
  extsh_(R3, R4);  check();
  extsw(R3, R4);  check();
  cntlzd(R3, R4);  check();
  cntlzw(R3, R4);  check();
  
  // unimp by asm: extldi(R3,  R4,   1, 63);  check();
  // unimp by asm: extrdi(R31, R30, 63, 1 );  check();
  // unimp by asm: insrdi(R3, R4, 2, 62);  check();
  // unimp by asm: rotldi(R3, R4, 63);  check();
  // unimp by asm: rotrdi_(R3, R4,  3);  check();
  // unimp by asm: rotld( R3, R4, R5);  check();
  // unimp by asm: sldi( R31, R30, 62 );  check();
  // unimp by asm: srdi( R3, R4, 1 );  check();
  // unimp by asm: clrldi_(R10, R11, 61 );  check();
  // unimp by asm: clrrdi(R3, R4, 20 );  check();
  // unimp by asm: clrsldi_(R31, R30, 3, 31);  check();
  
  extlwi(R3,  R4,   1, 31);  check();
  extrwi(R31, R30, 30, 1 );  check();
  inslwi(R3, R4, 2, 30);  check();
  insrwi(R3, R4, 2, 30);  check();
  rotlwi(R3, R4, 30);  check();
  rotrwi_(R3, R4, 3);  check();
  rotlw( R3, R4, R5);  check();
  slwi( R31, R30, 28 );  check();
  srwi( R3, R4, 1 );  check();
  clrlwi_(R10, R11, 29 );  check();
  clrrwi(R3, R4, 20 );  check();
  // unimp by asm: clrslwi_(R31, R30, 3, 31);  check();
  
  // unimp by asm: sld( R3, R4, R5 );  check();
  // unimp by asm: sld_(R31, R30, R29);  check();
  slw( R3, R4, R5);  check();
  slw_(R29, R30, R31);  check();
  
  // unimp by asm: srd( R3, R4, R5 );  check();
  // unimp by asm: srd_(R31, R30, R29);  check();
  srw( R3, R4, R5);  check();
  srw_(R29, R30, R31);  check();
  
  // unimp by asm: sradi( R3, R4, 63 );  check();
  // unimp by asm: sradi_(R31, R30, 62);  check();
  srawi( R3, R4, 0);  check();
  srawi_(R29, R30, 31);  check();
    
  // unimp by asm: srad( R3, R4, R5 );  check();
  // unimp by asm: srad_(R31, R30, R29);  check();
  sraw( R3, R4, R5);  check();
  sraw_(R29, R30, R31);  check();
  
  mtxer(R3);  check();
  mtlr(R31);  check();
  mtctr(R0);  check();
  
  mfxer(R3);  check();
  mflr(R31);  check();
  mfctr(R0);  check();
  
  mtcrf(0, R3);  check();
  mtcrf(31, R31);  check();
  mcrxr(0);  check();
  mcrxr(7);  check();
  mfcr(R0);  check();
  mfcr(R31);  check();
}


# ifdef SIC_COMPILER
    // for statistics
    // encode type tests with trigger instructions; when changing these, be
    // sure to change 1st instr of SendMessage_stub as well

    void Assembler::startTypeTest(fint ncases, bool prologueCheck,
                                  bool immedOnly) {
      fatal("unimp mac");
    }
    
    void Assembler::doOneTypeTest() { fatal("unimp mac"); }
    void Assembler::endTypeTest()   { fatal("unimp mac"); }
    void Assembler::markTagTest(fint n, bool isArith) {
      fatal("unimp mac");
    }
    
# endif


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "disasm_ppc.hh"

# include "_disasm_ppc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


void print_code(nmethod* /*nm*/, CORE_ADDR /*start*/, CORE_ADDR /*end*/) {
  warning("unimp mac");   
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "sendDesc_ppc.hh"
# include "_sendDesc_ppc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


void sendDesc::init_platform() {}


char* sendDesc::jump_addr() {
  int32* c = (int32*)jump_addr_addr();
  return get_target_of_Self_call_site(c);
}

void sendDesc::set_jump_addr(char* t) {
  char** jaa = jump_addr_addr();
  set_target_of_Self_call_site((int32*)jaa, (void*)t);
}


void printMask(RegisterString mask) {
  RegisterString regs = mask << R0;
  printAllocated(regs);
  lprintf("+{");
  // r will always be zero, only have registers in mask right now -- dmu 7/1
  // uint32 r = uint32(mask) >> NumRegistersInMask;
  // bool first = true;
  // for (fint d = 0; r; d++, r >>= 1) {
  //   if (isSet(r, 0)) {
  //     if (first) {
  //       first = false;
  //     } else {
  //       lprintf(",");
  //     }
  //     lprintf("T%ld", long(d));
  //     Location d1 = Location(d);
  //     while(isSet(r, 0)) { d ++, r >>= 1; }
  //     if (d - 1 > d1) lprintf("-T%ld", long(d - 1));
  //   }
  // }
  lprintf("}");
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "diDesc_ppc.hh"
# include "_diDesc_ppc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

pc_t DIDesc::jump_addr() {
  return (pc_t)get_target_of_Self_call_site((int32*)jump_addr_addr());
}


void DIDesc::set_jump_addr(pc_t insts) {
  pc_t* addr = jump_addr_addr();
  set_target_of_Self_call_site((int32*)addr, insts);
}
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

  
# pragma implementation "cacheStub_ppc.hh"
# pragma implementation "cacheStub_inline_ppc.hh"
# include "_cacheStub_ppc.cpp.incl"
  
# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


// -no SIC type tests for now

Label* CacheStub::br_if_not_smi()
{
  assert(Int_Tag == 0, "???");
  a->andi_(R0, ReceiverReg, Tag_Mask, NumberOperand);
  Label* not_smi = new Label(a->printing);
  a->bne(*not_smi);
  return not_smi;
}

Label* CacheStub::br_if_not_float()
{
  assert((Float_Tag & Mem_Tag) == 0  &&  (Float_Tag & Int_Tag) == 0, "???");
  a->andi_(R0, ReceiverReg, Float_Tag, NumberOperand);
  Label* not_float = new Label(a->printing);
  a->beq(*not_float);
  return not_float;
}


void CacheStub::add_case(nmethod* nm, CountStub* stArg, pc_t addr)
{
  n[newMethods] = nm;  st[newMethods] = stArg;  ++ newMethods;
  jump(addr);
}


Label* CacheStub::prologue(bool immediateOnly) {
  assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
  Label* miss = NULL;
  Label* loadMapAfterHandlingImmediates = NULL;
  pc_t floatAddr, smiAddr;
  computeJumpAddr(nsmi, theSendDesc,   stsmi,   smiAddr);
  computeJumpAddr(nfloat, theSendDesc, stfloat, floatAddr);

  // Load map if needed
  if (immediateOnly)
    ;
  else if (FastMapTest  &&  nsmi == NULL  &&   nfloat == NULL) {
    a->lwz(Temp1, map_offset(), NumberOperand, ReceiverReg);
    return NULL;
  }
  else {
    a->andi_(R0, ReceiverReg, Mem_Tag, NumberOperand);
    loadMapAfterHandlingImmediates = new Label(a->printing);
    a->bne(*loadMapAfterHandlingImmediates, predict_weird); // usually branches, most selectors don't have int cases
  }

  if (nsmi  &&  nfloat) {
    Label* not_smi = br_if_not_smi();
    add_case(nsmi, stsmi, smiAddr);
    
    not_smi->define();
    // if immediateOnly = false and we're down here, we've already tested for memOop
    // and hence there's no need to test for the float tag.
    if (immediateOnly)
      miss = br_if_not_float(); // br if mem
    else if (FastMapTest)
      fatal("would have to test for float and go to next case but cannot happen");
    else
      miss = NULL;
    add_case(nfloat, stfloat, floatAddr);
  }
  else if (nsmi)   { miss = br_if_not_smi();    add_case(nsmi,   stsmi,     smiAddr);  }
  else if (nfloat) { miss = br_if_not_float();  add_case(nfloat, stfloat, floatAddr);  }
  else {
    // tested for Oop above, so rcvr is int or float,
    // but there are no smi or float cases, so must be a miss
    miss = new Label(a->printing);
    a->b(*miss, NumberOperand);
  }
  if (loadMapAfterHandlingImmediates) {
    loadMapAfterHandlingImmediates->define();
    // CacheStub::test expects Temp1 to contain the receiver's map
    a->lwz(Temp1, map_offset(), NumberOperand, ReceiverReg);
  }
  return miss;
}


Label* CacheStub::test(oop map, pc_t addr, Label* prev) {
  if (prev)
    prev->define();
  a->load_immediate(Temp2, (int32)map, OopOperand);
  a->cmplw(Temp1, Temp2); // Temp1 contains the receiver under test (filled by prologue)
  Label* next_test = new Label(a->printing);
  a->bne(*next_test);
  jump(addr);
  return next_test;
}

void CacheStub::finish(Label* miss, Label* prev) {
  if (prev)
    prev->define();
  if (miss)
    miss->define();
  a->branch_to(theSendDesc->lookupRoutine(), PVMAddressOperand, Temp2, false);
}

  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 1.5 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "deadBlockNode_ppc.hh"

# include "_deadBlockNode_ppc.cpp.incl"

# ifdef SIC_COMPILER

  PrimDesc* DeadBlockNode::non_lifo_abort;

  void initDeadBlockNode() {
    DeadBlockNode::non_lifo_abort
      = getPrimDescOfFunction(fntype(&NLRSupport::non_lifo_abort), true);
  }

# endif // SIC_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "genHelper_ppc.hh"

# include "_genHelper_ppc.cpp.incl"

# if defined(SIC_COMPILER)


  fint SICGenHelper::spOffset(Location l) {
    return ::spOffset(l,
                      theSIC->frameSize(),
                      theSIC->max_no_of_outgoing_args_and_rcvr());
  }

  fint SICGenHelper::spOffset(Location l, nmethod* nm) {
    return ::spOffset(l,
                      nm->frameSize(),
                      nm->max_no_of_outgoing_args_and_rcvr());
  }

  // Warning: this clobbers the count register
  void SICGenHelper::jumpTo(void* target, Location reg, Location link) {
    assert(link == UnAllocated || link == LinkRegister, "can't link to arbitrary register");
    a->branch_to((pc_t)target, PVMAddressOperand, reg, link == LinkRegister);
  }

  void SICGenHelper::genCountCode(int32* counter) {
    a->Comment("count # calls");
    a->load_from_address(Temp1, counter, VMAddressOperand, Temp2);
    a->addi(Temp1, Temp1, 1, NumberOperand);
    int32 lo, hi;
    a->break_up_word_for_adding((int32)counter, lo, hi);
    a->stw(Temp1, lo, VMAddressOperand, Temp2);
  }

  Location SICGenHelper::loadImmediateOop(ConstPReg* r, Location dest, bool mustMove) {
    // load oop from ConstPR; return location containing the oop
    if (r->loc == UnAllocated) {
      loadImmediateOop(r->constant, dest);
      return dest;
    } else if (mustMove) {
      moveRegToReg(r->loc, dest);
      return dest;
    }
    else
      return r->loc;
  }
  
  void SICGenHelper::loadImmediateOop(oop p, Location dest, bool isInt) {
    Unused(isInt); // load_immediate does the right thing wrt integer oops
    assert(isRegister(dest), "must be a register");
    a->load_immediate(dest, (int32)p, OopOperand);
  }

  void SICGenHelper::load(Location src, fint srcOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->lwz(dest, srcOffset, NumberOperand, src);
  }

  void SICGenHelper::store(Location src, fint dstOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->stw(src, dstOffset, NumberOperand, dest);
  }

  void SICGenHelper::moveRegToReg(Location srcReg, Location destReg) {
    assert(isRegister(srcReg) && isRegister(destReg), "not a register");
    a->mr(destReg, srcReg);
  }

  // must be a VMAddressOperand operand
  void SICGenHelper::setToZeroA(void* addr, Location tempReg) {
    assert(isRegister(tempReg), "not a register");
    theAssembler->li(R0, 0, NumberOperand);
    theAssembler->store_to_address(R0, addr, VMAddressOperand, tempReg);
  }

  // clobbers R0
  void SICGenHelper::setToZero(Location dest) {
    if (isRegister(dest)) {
      a->li(dest, 0, NumberOperand);
    } else {
      a->li(R0, 0, NumberOperand);
      store(R0, spOffset(dest), SP);
    }
  }


# ifdef UNUSED
  void SICGenHelper::checkRecompilation(fint countID) {
    fatal("unimp mac");
  }
# endif // UNUSED
    
  void SICGenHelper::smiOop_prologue(char* missHandler) {
    if (SICCountIntTagTests) a->markTagTest(1);
    a->andi_(R0, LReceiverReg, Tag_Mask, NumberOperand);
    Label hit(a->printing);
    a->beq(hit, predict_weird);
    jumpTo(missHandler, Temp1, UnAllocated);
    if (SICCountTypeTests) {
      a->endTypeTest();
    }
    hit.define();
  }

  void SICGenHelper::floatOop_prologue(char* missHandler) {
    a->andi_(R0, LReceiverReg, Float_Tag, NumberOperand);
    Label hit(a->printing);
    a->bne(hit, predict_weird);
    jumpTo(missHandler, Temp1, UnAllocated);
    if (SICCountTypeTests) {
      a->endTypeTest();
    }
    hit.define();
  }

  void SICGenHelper::memOop_prologue(mapOop receiverMapOop,
                                     char* missHandler) {
    if (FastMapTest) {
      fatal("fast map tests unimplemented on PPC");
    } else {
      Label checkMap;
      a->andi_(R0, LReceiverReg, Mem_Tag, NumberOperand);
      a->bne(checkMap, predict_weird);
      
      DefinedLabel miss(a->printing);
      jumpTo(missHandler, Temp2, UnAllocated);

      checkMap.define();
      a->lwz(Temp1, map_offset(), NumberOperand, LReceiverReg);
      loadImmediateOop(receiverMapOop, Temp2);
      a->cmpw(Temp1, Temp2);
      a->bne(miss, predict_weird); // backwards branch, usually falls through

      if (SICCountTypeTests)
        a->endTypeTest();
    }
  }

  void SICGenHelper::checkOop(Label& general, oop what, Location loc_to_check) {
    // test for inline cache hit (selector, delegatee)
    loadImmediateOop(what, Temp1);              // load hard-wired value
    moveLocToReg(loc_to_check, Temp2);
    a->cmpw(Temp1, Temp2);                      // compare against actual value
    if (general.isDefined()) {
      a->bne(general, predict_weird);           // reuse miss handler, backwards, will fall through
    } else {
      Label hit(a->printing);
      a->beq(hit, predict_weird);              // forwards, will go
      general.define();
      jumpTo(Memory->code->trapdoors->SendMessage_stub_td(R0), R0, UnAllocated);
      hit.define();
    }
  }
  
  

  fint SICGenHelper::verifyParents(objectLookupTarget* target,
                                   Location t,
                                   fint count) {
    assert(target->links != 0,  "expecting an assignable parent link");
    bool isFirst = true;
    for ( assignableSlotLink* l = target->links;  
          l != 0;
          l = l->next,  isFirst = false) {
      
      if (!isFirst) {
        // if multiple dynamic parents, reload slot holder before looping (HACK!)
        t = loadPath(Temp1, target, LReceiverReg);
      }
      
      // load assignable parent slot value
      a->lwz(Temp1, smiOop(l->slot->data)->byte_count() - Mem_Tag, NumberOperand, t);      
      verifyOneImmediateParent(l, Temp1, Temp2, count);
      ++count;

      if (l->target->links) count = verifyParents(l->target, Temp1, count);
    }
    return count;
  }
  
  
  void SICGenHelper::verifyOneImmediateParent(assignableSlotLink* l, Location parentOopReg, Location scratchReg, fint count) {
    Label* ok = new Label(a->printing);
    
    if (l->target->value_constrained)  verifyConstrainedOopOfParent(l->target->obj,        parentOopReg, scratchReg, ok);
    else                               verifyMapOfParent(           l->target->obj->map(), parentOopReg, scratchReg, ok);

    // This will be backpatched to call an nmethod, so need to leave incoming link alone.
    // Pass a link to the branch and nmln in the DILinkReg.
    // Since 3 registers are needed DILink, DITemp, DICount need to use either R0 or 
    // the counter register. Since branch_to uses the count register, we use R0 here.
  
    // save link so can use link to pass nmln addr
    a->mflr(DITempReg);
    a->stw(DITempReg, saved_pc_offset * oopSize, NumberOperand, SP); // save link for stack-crawling
    Label next;
    a->bl(next, NumberOperand); // need to get addr of nmln
    next.define();
    fint link_contents = a->offset(); // where the link reg points to
    a->mflr(DILinkReg);
    a->mtlr(DITempReg);
  
    pc_t link_incr_addr = a->addr();
    assert(DILinkReg != R0, "addi will break");
    a->addi(DILinkReg, DILinkReg, 0, NumberOperand); // will come back and backpatch
  
    a->load_immediate(DICountReg, count, NumberOperand);
    a->branch_to( Memory->code->trapdoors->SendDIMessage_stub_td(DITempReg), 
                  DIVMAddressOperand, DITempReg, false);
    assert(DITempReg != R0, "callee needs a nonzero reg here");

    fint desired_link_contents = a->offset(); // where the link would be if we could use it
    a->saveExcursion(link_incr_addr);         // go back and get the increment right
    a->addi(DILinkReg, DILinkReg, desired_link_contents - link_contents, NumberOperand);
    a->endExcursion();
    a->Data(0);                                // first  part of DI nmln
    a->Data(0);                                // second part of DI nmln

    ok->define();
  }



  void SICGenHelper::verifyConstrainedOopOfParent(oop targetOop, 
                                                  Location parentOopReg, 
                                                  Location scratchReg, 
                                                  Label* ok) {
    assert(parentOopReg != scratchReg, "register collision");
    // constraint for a particular oop (ambiguity resolution)
    loadImmediateOop(targetOop, scratchReg); // load assumed value
    a->cmpw(parentOopReg, scratchReg);       // compare values
    a->beq(*ok, predict_weird);              // branch if value OK
  }

  
  // Given: map to look for, obj already in parentOopReg, scratch reg regForMap
  // test for map, fall through on miss, goto label OK if hit.
  
  void SICGenHelper::verifyMapOfParent(Map* targetMap, Location parentOopReg, Location regForMap, Label* ok) {
    if (targetMap == Memory->smi_map) {
      a->andi_(R0, parentOopReg, Tag_Mask, NumberOperand); // test for integer tag
      a->beq(*ok, predict_weird);            // branch if parent is integer
    }
    else if (targetMap == Memory->float_map) {
      a->andi_(R0, parentOopReg, Float_Tag, NumberOperand); // test for float tag
      a->bne(*ok, predict_weird);            // branch if parent is a float
    } 
    else {
      Label miss;
      assert(R0 != parentOopReg, "register collision");
      if (FastMapTest)         fatal("no fast map tests on PPC");
      a->andi_(R0, parentOopReg, Mem_Tag, NumberOperand); // test for mem tag
      a->beq(miss, predict_usual);         // branch if parent is not mem oop, fwd, will fall through
      assert(regForMap != parentOopReg, "register collision");
      a->lwz(regForMap, map_offset(), NumberOperand, parentOopReg); // load receiver map
      loadImmediateOop(targetMap->enclosing_mapOop(), R0); // load map constraint
      a->cmpw(regForMap, R0);                   // compare w/ parent's map
      a->beq(*ok, predict_weird);           // correct
      miss.define();
    }
  }
    

# endif // SIC_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.21 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "node_ppc.hh"

# include "_node_ppc.cpp.incl"

# if defined(SIC_COMPILER)

  void BasicNode::genBranch() {
    Label* l2 = new Label(theAssembler->printing);
    theAssembler->b(*l2, NumberOperand);
    l = l->unify(l2);
  }

  void BasicNode::resetSP() {
    theAssembler->la(SP, thisFrameSize * oopSize, NumberOperand, SP);
  }

  void BasicNode::restoreNonVolatileRegisters() {
    theAssembler->lmw( Location(32 - theSIC->number_of_saved_nonvolatile_registers()), 
                       -oopSize * theSIC->number_of_saved_nonvolatile_registers(),
                       NumberOperand, SP);
  }

  void BasicNode::doReturn(fint offset) {
    theAssembler->lwz(Temp1, saved_pc_offset * oopSize, NumberOperand, SP);
    if (offset)
      theAssembler->addi(Temp1, Temp1, offset, NumberOperand);
    theAssembler->mtlr(Temp1);
  }

  void BasicNode::restoreFrameAndReturn(bool haveStackFrame, fint offset) {
    if (haveStackFrame) {
      resetSP();
      restoreNonVolatileRegisters();
      doReturn(offset);
    }
    else if (offset != 0) {
      theAssembler->mflr(Temp1);
      theAssembler->addi(Temp1, Temp1, offset, NumberOperand);
      theAssembler->mtlr(Temp1);
    }
    else {
      // do nothing here; balwlr below is enough
    }
    theAssembler->balwlr();
  }

  void PrologueNode::saveReturnPC() {
    theAssembler->mflr(R0);
    theAssembler->stw(R0, saved_pc_offset * oopSize, NumberOperand, SP);
  }

  void PrologueNode::saveNonVolatileRegisters() {
    theAssembler->stmw( Location(32 - theSIC->number_of_saved_nonvolatile_registers()),
                           -oopSize * theSIC->number_of_saved_nonvolatile_registers(), 
                        NumberOperand,
                        SP);
  }

  void PrologueNode::copyIncomingRegisterArgsToNonVolatileRegisters() {
    theAssembler->Comment("flush incoming args to non-vol regs");
    int32 r = LReceiverReg;
    int32 num_reg_args_and_rcvr = theSIC->numberOfIncomingArgsAndRcvrInRegisters();
    for (int32 i = 0; i < num_reg_args_and_rcvr; ++i, ++r)
      theAssembler->mr(Location( IReceiverReg - (r - LReceiverReg)), Location(r));
  }

  void PrologueNode::saveIncomingRegisterArgsOnStackIfNecessary() {
    // Save args on stack in case they are uplevel-accessed by a block
    // (since they cannot be changed this is just a copy).
    // A faster approach would be to only save the args that are actually
    // uplevel-accessed.  The SIC has the infrastructure in place to already do this.
    // See the comment in FlushNode::gen below to see why this adds complications
    // to the stack frame traversal during GC.
    // So, we stick with the simple approach: flush all incoming arguments to the
    // stack if we have any subblocks. -mabdelmalek 11/02
    if (theSIC->are_register_arguments_saved_on_stack()) {
      int32 num_reg_args_and_rcvr = theSIC->numberOfIncomingArgsAndRcvrInRegisters();
      theAssembler->Comment("save incoming register args on stack if necessary");
      theAssembler->la(Temp1, rcvr_and_argument_offset(0) * oopSize, NumberOperand, SP);
      theAssembler->stswi(LReceiverReg, Temp1, num_reg_args_and_rcvr * oopSize);
    }
  }

  void PrologueNode::actuallyCreateStackFrame() {
    theAssembler->stwu(SP, thisFrameSize * -oopSize, NumberOperand, SP);
    theSIC->_frameCreationOffset = theAssembler->offset();
  }

  void PrologueNode::clearStackLocations() {
    theAssembler->Comment("clear stack locations");
    if (theSIC->number_of_memory_locals() == 0)
      return;
    theAssembler->li(R0, 0, NumberOperand);
    for (int32 i = 0; i < theSIC->number_of_memory_locals(); ++i) {
      fint offset = stackLocation_offset(i, theSIC->max_no_of_outgoing_args_and_rcvr());
      theAssembler->stw(R0, oopSize * offset, NumberOperand, SP);
    }
  }

  void PrologueNode::createStackFrame() {
    assert(haveStackFrame(), "shouldn't be creating a stack frame");

    thisFrameSize = theSIC->frameSize();

    saveReturnPC();
    saveNonVolatileRegisters();
    copyIncomingRegisterArgsToNonVolatileRegisters();
    saveIncomingRegisterArgsOnStackIfNecessary();
    actuallyCreateStackFrame();
    clearStackLocations();
  }

  void PrologueNode::prePrologue() { }

  void PrologueNode::postPrologue() { }

  void LoadIntNode::gen() {
    BasicNode::gen();
    if (isRegister(_dest->loc)) {
      theAssembler->load_immediate(_dest->loc, (int32)value, NumberOperand);
    }
    else {
      theAssembler->load_immediate(Temp2, (int32)value, NumberOperand);
      theAssembler->stw(Temp2, spOffset(_dest->loc), NumberOperand, SP);
    }
  }
  
  void StoreOffsetNode::gen() {
    BasicNode::gen();
    Location b = genHelper->moveToReg(base, Temp1);
    Location t = Temp2;
    if (_src->isConstPReg()) {
      // store constant
      ConstPReg* value = (ConstPReg*)_src;
      oop p = value->constant;
      // don't need to check-store if oop is old - old objs will never become
      // new again
      needCheckStore = needCheckStore && p->is_new(); // ints/floats aren't new
      assert(b != t, "must be different");
      t = genHelper->loadImmediateOop(value, t, false);
      theAssembler->stw(t, offset, NumberOperand, b);
    } else  {
      if (isRegister(_src->loc)) {
        theAssembler->stw(_src->loc, offset, NumberOperand, b);
      } else {
        theAssembler->lwz(t, spOffset(_src->loc), NumberOperand, SP);
        theAssembler->stw(t, offset, NumberOperand, b);
      }
    }
    if (needCheckStore) {
      theAssembler->Comment("record store");
      // do a check-store
      assert(isRegister(b), "base reg of check_store must be in a register");

      if (offset > card_size || !AllowOffsetCheckStores) {
        // use slow check-store sequence
        // (marked card may be off by one, but not more)
        theAssembler->addi(Temp1, b, offset, NumberOperand);
        b = Temp1;
      }

      theAssembler->srwi(Temp1, b, card_shift);  // shift target addr
      theAssembler->li(R0, 0, NumberOperand);

      if (!UseByteMapBaseReg) {
        theAssembler->load_from_address(Temp2, &byte_map_base, VMAddressOperand, Temp2);
        theAssembler->stbx(R0, Temp1, Temp2);
      }
      else {
        theAssembler->stbx(R0, Temp1, ByteMapBaseReg);  // set byte in map
      }
    }
  }
  
  void AssignNode::genOop() {
    ConstPReg* value = (ConstPReg*)_src;
    Location src = value->loc;
    if (src != UnAllocated) {
      // value is already in src register
      genHelper->moveRegToLoc(src, _dest->loc);
    } else if (isRegister(_dest->loc)) {
      genHelper->loadImmediateOop(value->constant, _dest->loc);
    } else {
      oop c = value->constant;
      genHelper->loadImmediateOop(c, Temp1);
      theAssembler->stw(Temp1, spOffset(_dest->loc), NumberOperand, SP);
    }
  }
  
  void CallNode::nlrCode() {
    if (nlrPoint()) {
      // branch to NLR code
      Label* l_ = new Label(theAssembler->printing);
      theAssembler->b(*l_, NumberOperand);
      nlrPoint()->l = l_->unify(nlrPoint()->l);
    } else {
      if (!theSIC->nlrLabel)
        theSIC->nlrLabel = new Label(theAssembler->printing);
      theAssembler->b(*theSIC->nlrLabel, NumberOperand);
    }
  }

  void SendNode::gen() {
    BasicNode::gen();
    assert(bci() != IllegalBCI, "should have legal bci");
    genPcDesc();
    genBreakpointBeforeCall();
    // WARNING: following code sequence is known to get_target_of_Self_call_site() & set_target_of_Self_call_site()
    theAssembler->branch_to(Memory->code->trapdoors->SendMessage_stub_td(R0),
                            BPVMAddressOperand, R0, true);
    offset = theAssembler->offset();
    Label past_send_desc(theAssembler->printing);
    theAssembler->b(past_send_desc, NumberOperand);
    theAssembler->Data(mask());
    nlrCode();
    theAssembler->Zero();       // nmlns
    theAssembler->Zero();
    if (sel != badOop) {
      if (isPerformLookupType(l)) {
        assert_smi(sel, "should be an integer argcount");
        theAssembler->Data(smiOop(sel)->value());       // really arg count
      } else {
        assert_string(sel, "should be a string constant");
        theAssembler->Data(sel);                        // constant selector
      }
    }
    if ((l & UninlinableSendMask) == 0) theSIC->noInlinableSends = false;
    theAssembler->Data(l);
    verifySendInfo();
    if (del) {
      assert(needsDelegatee(l), "shouldn't have a delegatee");
      theAssembler->Data(del);
    }
    if (theSIC->nlrLabel && !theSIC->nlrLabel->isDefined()) {
      theSIC->nlrLabel->define();
      restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
    }
    past_send_desc.define();
  }
  

  void reload_ByteMapBaseReg(PrimDesc* pd) {
    if (UseByteMapBaseReg && pd->canScavenge()) {
      // any prim that can scavenge, may expand heap,so need to reload this reg
     theAssembler->load_from_address(ByteMapBaseReg, &byte_map_base, VMAddressOperand, ByteMapBaseReg);
    }
  }


    // may need to call theSIC->allocateArgs in PrimNode::PrimNode, much like BlockCloneNode
  void PrimNode::gen() {
    BasicNode::gen();
    assert(bci() != IllegalBCI, "should have legal bci");
    if (pd->canWalkStack()) genPcDesc();

    if (!pd->canScavenge()) {
      genHelper->jumpTo(first_inst_addr(pd->fn()), Temp1, LinkRegister); // do the call
    }
    else {
      // must save nonvols on way to C to enable stack walking for OOPS
      theAssembler->li(SaveSelfNonVolRegs_arg_count_register, argc + 1, NumberOperand);
      theAssembler->load_immediate( R0,
                                    (int32) Memory->code->trapdoors->SaveSelfNonVolRegs_td(R0),
                                    VMAddressOperand );
      // WARNING: this code sequence is known by get_target_of_Self_call_site(inst_t* instp)
      theAssembler->load_immediate(SaveSelfNonVolRegs_entry_point_register, (int32) first_inst_addr(pd->fn()), PVMAddressOperand);
      theAssembler->mtctr(R0);
      theAssembler->balwctrl();
    }

    // skip over inline cache:
    Label past_nlr(theAssembler->printing); 
    theAssembler->b(past_nlr, NumberOperand);  // skip over mask and nlr code

    if (pd->canScavenge() || pd->needsNLRCode())
      theAssembler->Data(mask());              // used register mask for GC
  
    if (pd->needsNLRCode()) {
      reload_ByteMapBaseReg(pd);
      nlrCode();

      if (theSIC->nlrLabel && !theSIC->nlrLabel->isDefined()) {
        theSIC->nlrLabel->define();
        restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
      }
    }

    past_nlr.define(); 
    reload_ByteMapBaseReg(pd);
  }


  Location get_SPLimit() {
    if (UseSPLimitReg)
      return SPLimitReg;
    else {
      theAssembler->load_from_address(Temp1, &SPLimit, VMAddressOperand, Temp1);
      return Temp1;
    }
  }

  void InterruptCheckNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->cmplw(SP, get_SPLimit());  // test for stack overflow
    Label l_(theAssembler->printing);
    theAssembler->bge(l_, predict_weird);    // no overflow, fwd will branch
    PrimNode::gen();
    l_.define();
  }

  void RestartNode::gen() {
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->cmplw(SP, get_SPLimit());  // test for stack overflow
    Label* dest = new Label(theAssembler->printing);
    theAssembler->bge(*dest);
    loopStart->l = loopStart->l->unify(dest);
    PrimNode::gen();
    theAssembler->b(*loopStart->l, NumberOperand);
  }

  void BlockCloneNode::genCall() {
    theAssembler->Comment("block clone");
    Location dest = block()->loc;
    genHelper->loadImmediateOop(block()->block, CReceiverReg); // load block Oop
    theAssembler->mr(Arg1, SP);                       // load home frame
    theAssembler->branch_to( first_inst_addr(blockClone->fn()), PVMAddressOperand, Temp1, true);
    assert(!blockClone->canScavenge() && !blockClone->needsNLRCode(),
           "need to rewrite this");
    genHelper->moveRegToLoc(ResultReg, dest);
    if (CheckAssertions && block()->uplevelR && isRegister(dest)) {
      // flush to stack
      fatal1("because i'm not handling register-flushing on PPC, need this register "
             "to be stack-allocated: %d\n"
             "(SIC should not have allocated this to a register since it is up-level accessed)",
             dest);
      // theAssembler->stw(dest, spOffset(dest), NumberOperand, SP);
    }
  }
  
  void BlockCreateNode::gen() {
    BasicNode::gen();
    if (block()->primFailBlockScope) {
      // must generate block (in primitive fail branch)
      assert(!isMemoized(), "shouldn't be memoized");
      genCall();
    } else if (isMemoized()) {
      // test if already created
      theAssembler->Comment("test memoized block");
      Location t = genHelper->moveToReg(block(), Temp1);
      Location t2 = genHelper->loadImmediateOop(deadBlockPR, Temp2, false);
      theAssembler->cmplw(t, t2);
      Label done;
      theAssembler->bne(done, predict_weird); // optimize fast case, so predict-weird
      genCall();
      done.define();
    } else {
      // block has already been created (by initial BlockClone node)
    }
  }
  
  void NonLocalReturnNode::gen() {
    BasicNode::gen();
    restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
  }


  void MethodReturnNode::gen() {
    BasicNode::gen();

    // move result to resultReg
    Location res = genHelper->moveToReg(_src, Temp1);
    theAssembler->mr(ResultReg, res);

    restoreFrameAndReturn(haveStackFrame, 0);
  }
  

  Location arith_genHelper(PReg* sreg, PReg* oper, PReg* dest,
                           ArithOpCode op,
                           Location& t1, Location& t2, bool& reversed) {
    bool haveImmediate = false;
    reversed = false;
    oop immediate;
    bool immedIsSigned = 
          op == AddArithOp  ||  op == AddCCArithOp  ||  op == TAddCCArithOp
      ||  op == SubArithOp  ||  op == SubCCArithOp  ||  op == TSubCCArithOp
      ||  op == MulArithOp  ||  op == TMulCCArithOp
      ||  op == DivArithOp  ||  op == TDivCCArithOp
      ||  op == TModCCArithOp;
    if (sreg->isConstPReg()) {
      oop val = ((ConstPReg*)sreg)->constant;
      if (val->is_smi()
      && (immedIsSigned ? fits_within_si(int32(val)) : fits_within_ui(int32(val)))) {
        // try to reverse the sense of the operation
        switch (op) {
         case AddArithOp:       
         case AddCCArithOp:             
         case AndArithOp:               
         case AndCCArithOp:             
         case OrArithOp:                
         case OrCCArithOp:              
         case XOrArithOp:
         case TAndCCArithOp:
         case TOrCCArithOp:
         case TXorCCArithOp:
          // commutative operator, no problem
          sreg = oper; immediate = val; haveImmediate = reversed = true;
          break;
          
          // would need to reverse non-const operand/operation for subtract,
          // so no savings
         case SubArithOp:
         case SubCCArithOp:             
         case TSubCCArithOp:
          break;
          //  PPC lacks immed/ovfl, so cannot check for ovfl with immed
         case TAddCCArithOp:
         case TMulCCArithOp:
         case TDivCCArithOp:
         case TModCCArithOp:
          break;

           // not commutative
         case ArithmeticLeftShiftArithOp:
         case LogicalLeftShiftArithOp:
         case ArithmeticRightShiftArithOp:
         case LogicalRightShiftArithOp:
         case TALShiftCCArithOp:
         case TARShiftCCArithOp:
         case TLLShiftCCArithOp:
         case TLRShiftCCArithOp:
          break;
         default:       ShouldNotReachHere(); // unexpected arith type
        }
      }
    }
    else if (oper->isConstPReg()) {
      switch (op) {
        // no overflow-immedates
        case TAddCCArithOp:
        case TSubCCArithOp:
        case TMulCCArithOp:
        case TDivCCArithOp:
        case TModCCArithOp:
          break;
        default:
         oop val = ((ConstPReg*)oper)->constant;
         if (val->is_smi()
         && (immedIsSigned ? fits_within_si(int32(val)) : fits_within_ui(int32(val)))) {
           immediate = val; haveImmediate = true; 
         }
      }
    }
    
    Location src = genHelper->moveToReg(sreg, Temp1);
    Location dst = isRegister(dest->loc) ? dest->loc : Temp3;
  
    if (haveImmediate) {
      t1 = src; t2 = R0;
      int opn = (int)immediate; // for better switch formatting
      switch (op) {
       case AddArithOp:    theAssembler->addi  (dst, src, opn, NumberOperand); break;
       case SubArithOp:    theAssembler->subi  (dst, src, opn, NumberOperand); break;
       case AndArithOp:    fatal("no and-immediate that does NOT sets CC on PPC"); break;
       case OrArithOp:     theAssembler->ori   (dst, src, opn, NumberOperand); break;
       case XOrArithOp:    theAssembler->xori  (dst, src, opn, NumberOperand); break;
       case ArithmeticLeftShiftArithOp:
       case LogicalLeftShiftArithOp:
                           theAssembler->slwi  (dst, src, opn);                break;
       case ArithmeticRightShiftArithOp:
                           theAssembler->srawi (dst, src, opn);                break;
       case LogicalRightShiftArithOp:
                           theAssembler->srwi  (dst, src, opn);                break;
       // Don't have to worry about memOops, cause this platform only does ints as immediates -- dmu 4/07
       case SubCCArithOp:  opn = -opn; // use addic_ with negative operand
       case AddCCArithOp:  theAssembler->addic_(dst, src, opn, NumberOperand); break;
       case AndCCArithOp:  theAssembler->andi_ (dst, src, opn, NumberOperand); break;
       case OrCCArithOp:   fatal("no OR-immediate that sets CC on PPC");       break;
       
       case TAndCCArithOp: theAssembler->andi_(dst, src, opn, NumberOperand); break;
       case TOrCCArithOp:  theAssembler->ori  (dst, src, opn, NumberOperand); break;
       case TXorCCArithOp: theAssembler->xori (dst, src, opn, NumberOperand); break;

       case TALShiftCCArithOp:
       case TLLShiftCCArithOp: theAssembler->slwi (dst, src, opn >> Tag_Size);
                               break;
       
       case TARShiftCCArithOp: theAssembler->srawi(dst, src, opn >> Tag_Size); 
                               theAssembler->clrrwi_(dst, dst, Tag_Size);
                               break;
                               
       case TLRShiftCCArithOp: theAssembler->srwi (dst, src, opn >> Tag_Size); 
                               theAssembler->clrrwi_(dst, dst, Tag_Size);
                               break;

       default:            ShouldNotReachHere(); // unexpected arith type
      }
    } else {
      Location src2 = genHelper->moveToReg(oper, Temp2);
      t1 = src; t2 = src2;
      switch (op) {
       case AddArithOp:         theAssembler->add (dst, src, src2);            break;
       case SubArithOp:         theAssembler->sub (dst, src, src2);            break;
       case AndArithOp:         theAssembler->And (dst, src, src2);            break;
       case OrArithOp:          theAssembler->Or  (dst, src, src2);            break;
       case XOrArithOp:         theAssembler->Xor (dst, src, src2);            break;
       case ArithmeticLeftShiftArithOp:
       case LogicalLeftShiftArithOp:
                                theAssembler->slw (dst, src, src2);            break;
       case ArithmeticRightShiftArithOp:
                                theAssembler->sraw(dst, src, src2);            break;
       case LogicalRightShiftArithOp:
                                theAssembler->srw (dst, src, src2);            break;

       case AddCCArithOp:       theAssembler->add_(dst, src, src2);            break;
       case SubCCArithOp:       theAssembler->sub_(dst, src, src2);            break;
       case AndCCArithOp:       theAssembler->And_(dst, src, src2);            break;
       case OrCCArithOp:        theAssembler->Or_ (dst, src, src2);            break;

       case TAddCCArithOp:      theAssembler->addo_(dst, src, src2);           break;
       case TSubCCArithOp:      theAssembler->subo_(dst, src, src2);           break;

       case TAndCCArithOp:      theAssembler->And_(dst, src, src2);            break;
       case TOrCCArithOp:       theAssembler->Or_ (dst, src, src2);            break;
       case TXorCCArithOp:      theAssembler->Xor_ (dst, src, src2);           break;
       
       case TMulCCArithOp:      theAssembler->srawi(R0, src2, Tag_Size);
                                theAssembler->mullwo_(dst, src, R0); 
                                break;

       case TDivCCArithOp:      theAssembler->srawi(R0, src2, Tag_Size);
                                theAssembler->srawi(dst, src, Tag_Size);
                                theAssembler->divwo_(R0, dst, R0);
                                theAssembler->slwi(dst, R0, Tag_Size); 
                                break;
       
       case TModCCArithOp:      theAssembler->divwo_(R0, src, src2); 
                                theAssembler->mullw( R0,  R0, src2);
                                theAssembler->sub_(dst, src, R0);
                                break;

       case TARShiftCCArithOp:  theAssembler->srawi(R0, src2, Tag_Size);
                                theAssembler->sraw(dst, src, R0); 
                                theAssembler->clrrwi_(dst, dst, Tag_Size);
                                break;
       case TALShiftCCArithOp:
       case TLLShiftCCArithOp:  theAssembler->srawi(R0, src2, Tag_Size);
                                theAssembler->slw_(dst, src, R0); 
                                break;
       case TLRShiftCCArithOp:  theAssembler->srawi(R0, src2, Tag_Size);
                                theAssembler->srw(dst, src, R0); 
                                theAssembler->clrrwi_(dst, dst, Tag_Size);
                                break;
       
       default:                 ShouldNotReachHere(); // unexpected arith type
      }
    }
    return dst;
  }


  static void taggedArithCheckHelper(Location t1, Location t2, bool reversed,
                                     bool arg1IsInt, bool arg2IsInt,
                                     Location dest,
                                     Node* failNode, bool canOverflow, ArithOpCode op) {

    if (failNode == NULL)
      return;

    bool skipCheck = false;

    Location arg1 = reversed ? t2 : t1;
    Location arg2 = reversed ? t1 : t2;
    Label* l_ = new Label(theAssembler->printing);
    
    // check the rcvr and args' tags
    if (arg1IsInt  &&  arg2IsInt)
      skipCheck = true;
    else if (arg1IsInt) {
      // only need to check arg2
      assert(arg2 != dest, "need to check arg2, but dest is there");
      theAssembler->andi_(R0, arg2, Tag_Mask, NumberOperand);
    }
    else if (arg2IsInt) {
      // only need to check arg1
      assert(arg1 != dest, "need to check arg1, but dest is there");
      theAssembler->andi_(R0, arg1, Tag_Mask, NumberOperand);
    }
    else {
      // check both tags at the same time
      assert(t1 != dest  &&  t2 != dest, "need to check t1 and t2, but dest is there");
      theAssembler->Or   (Temp1, t1, t2);
      theAssembler->andi_(R0, Temp1, Tag_Mask, NumberOperand);
    }
    if (!skipCheck )  theAssembler->bne(*l_, predict_usual); // expect fall through, not br (fwd)

    if (canOverflow)  {
      if (skipCheck)
        theAssembler->crset(cond_eq); // set eq bit so caller will load OVERFLOWERROR
      theAssembler->bso(*l_, predict_usual); // expect fall through, not br (fwd)
    }
    if (op == TALShiftCCArithOp
    ||  op == TDivCCArithOp     )  {
      // overflow if sign bit changed for shift or
      // for divide: check for left shift of result overflowing, e.g. minSmallInt / -1 --- dmu 3/06
      if (op == TALShiftCCArithOp)  theAssembler->Xor_(R0, dest, arg1);
      else                          theAssembler->Xor_(R0, dest, R0);
      // move neg CC bit into EQ CC bit, so caller will load OVERFLOWERROR
      theAssembler->crmove(cond_eq, cond_lt);
      theAssembler->beq(*l_, predict_usual); // expect fall through, not br (fwd)
    }
    failNode->l = l_->unify(failNode->l);
  }
  
// Next two routines are placeholders in case
// any operations need Temp3, etc. -- dmu 1/03

  void TArithRRNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); U_CHECK(oper);
    switch (op) {
     default: break;
    }
    // Use Temp3 so that result does not overwrite args, so we can check args -- dmu 3/04
    // Actually, ops where you can check the result instead of the operands can
    // probably get by without using Temp3, because even if operation is in Temp2,
    // the result will just get checked instead (if result is in Temp2).
    // So, maybe Temp3 should only be used for Mul and Mod or Div and just use Temp2 instead
    // for the others. But, I'm not going to try it. -- dmu 3/04
    use_count[Temp3]++; def_count[Temp3]++;       // (potentially) uses Temp3
  }
      
  bool TArithRRNode::canCopyPropagateFrom(PReg* d) {
    switch (op) {
      default: break;
    }
    return d->loc != Temp3; // prevent local cp of Temps
  }

  bool TArithRRNode::isOpInlinable( ArithOpCode op ) {
    return  op != NilArithOp;
  }
  
  void TArithRRNode::gen() {
    BasicNode::gen();
    if (constResult) {
      Location dest = isRegister(_dest->loc) ? _dest->loc : Temp2;
      Location l_ = genHelper->moveToReg(constResult, dest);
      if (l_ != _dest->loc) genHelper->moveRegToLoc(dest, _dest->loc);
    } else {
      Location t1, t2;
      bool reversed;

      bool canOverflow =    
              op == TAddCCArithOp
          ||  op == TSubCCArithOp
          ||  op == TMulCCArithOp
          ||  op == TDivCCArithOp
          ||  op == TModCCArithOp /* if 2nd arg is 0 */;

      if (canOverflow  &&  next1())
        theAssembler->mcrxr(0); // clear summary overflow bit

      // we should check tags first to ensure rcvr and arg are smiOop's.
      // but they might not be in registers, and we'd have to move them
      // to registers first to do the tag check.
      // BUT arith_genHelper already moves them to registers...
      // So let's just do the addition/subtraction using arith_genHelper
      // and, after that, we'll do the tag check since we know rcvr and
      // arg are in registers now.
      Location dest = arith_genHelper(_src, oper, _dest, op, 
                                      t1, t2, reversed);

      taggedArithCheckHelper(t1, t2, reversed, arg1IsInt, arg2IsInt, dest, next1(), canOverflow, op);

      if (dest != _dest->loc) {
        // store result on stack (success case)
        theAssembler->stw(dest, spOffset(_dest->loc), NumberOperand, SP);
      }
    }
  }

  
  void ArithRCNode::gen() {
    BasicNode::gen();
    Location src  = genHelper->moveToReg(_src, Temp1);
    Location dest = isRegister(_dest->loc) ? _dest->loc : Temp2;
    switch (op) {
     case AddArithOp:   theAssembler->addi (dest, src, oper, NumberOperand);   break;
     // Use NumberOperand below because this is used with arbitrary numbers, not oops; see
     //   SPrimScope::genPrimFailure -- dmu 4/07
     case SubArithOp:   theAssembler->subi (dest, src, oper, NumberOperand);   break;

     // there's no and-immediate that does NOT set the CC, which is what AndArithOp
     // is supposed to do.  for now, in the only place where AndArithOp is used,
     // it's ok if the CC is set.  -mabdelmalek 11/23/02
     case AndArithOp:   theAssembler->andi_(dest, src, oper, NumberOperand);   break;
     case OrArithOp:    theAssembler->ori  (dest, src, oper, NumberOperand);   break;
     case XOrArithOp:   theAssembler->xori (dest, src, oper, NumberOperand);   break;
     case ArithmeticLeftShiftArithOp:
     case LogicalLeftShiftArithOp:
                        theAssembler->slwi (dest, src, oper);                  break;
     case ArithmeticRightShiftArithOp:
                        theAssembler->srawi(dest, src, oper);                  break;
     case LogicalRightShiftArithOp:
                        theAssembler->srwi (dest, src, oper);                  break;

     // Use NumberOperand below because this is used with arbitrary numbers, not oops; see
     //   SPrimScope::genPrimFailure -- dmu 4/07
     case SubCCArithOp: oper = -oper;
     case AddCCArithOp: theAssembler->addic_(dest, src, oper, NumberOperand);  break;
     case AndCCArithOp: theAssembler->andi_ (dest, src, oper, NumberOperand);  break;
     case OrCCArithOp:  fatal("no OR-immediate that sets CC on PPC");          break;

     default:           ShouldNotReachHere(); // unexpected arith type
    }
    if (dest != _dest->loc) {
      theAssembler->stw(dest, spOffset(_dest->loc), NumberOperand, SP);
    }
  }


  void BranchNode::gen() {
    BasicNode::gen();
    Label* l_ = new Label(theAssembler->printing);
    switch (op) {
     case ALBranchOp:   theAssembler->b  (*l_, NumberOperand); break;
     case EQBranchOp:   theAssembler->beq(*l_);                break;
     case NEBranchOp:   theAssembler->bne(*l_);                break;
     case LTBranchOp:   theAssembler->blt(*l_);                break;
     case LEBranchOp:   theAssembler->ble(*l_);                break;
     case LTUBranchOp:  fatal("not available on PPC");         break;
     case LEUBranchOp:  fatal("not available on PPC");         break;
     case GTBranchOp:   theAssembler->bgt(*l_);                break;
     case GEBranchOp:   theAssembler->bge(*l_);                break;
     case GTUBranchOp:  fatal("not available on PPC");         break;
     case GEUBranchOp:  fatal("not available on PPC");         break;
     case VSBranchOp:   theAssembler->bso(*l_);                break; // SUMMARY on PPC
     case VCBranchOp:   theAssembler->bns(*l_);                break;
     default:           ShouldNotReachHere(); // unexpected branch type
    }
    Node* n = next1();
    n->l = l_->unify(n->l);
  }


  void TBranchNode::genCompare(bool haveImmediate,
                               Location rcvrReg, Location argReg) {
    if (!intRcvr) {
      // check that rcvr is a smiOop
      theAssembler->andi_(R0, rcvrReg, Tag_Mask, NumberOperand);
      Label*& primFailure = ((MergeNode*)nexti(2))->l;
      Label* l = new Label(theAssembler->printing);
      theAssembler->bne(*l, predict_usual); // will not fail, will fall through fwd, so normal
      primFailure = primFailure->unify(l);
    }
    if (!intArg) {
      // check that arg is a smiOop
      theAssembler->andi_(R0, argReg, Tag_Mask, NumberOperand);
      Label*& primFailure = ((MergeNode*)nexti(2))->l;
      Label* l = new Label(theAssembler->printing);
      theAssembler->bne(*l, predict_usual); // will not fail, will fall through fwd, so normal
      primFailure = primFailure->unify(l);
    }

    // we're here iff arg and rcvr are smiOop's.  do the actual comparision
    if (haveImmediate) {
      oop val = ((ConstPReg*)arg)->constant;
      theAssembler->cmpwi(rcvrReg,(int32)val, NumberOperand);
    }
    else {
      theAssembler->cmpw(rcvrReg, argReg);
    }
  }


  void TBranchNode::testTagsIfNecessary(bool haveImmediate, Location rcvrReg, Location argReg) {
    // this function doesn't do anything on PPC.  it's needed because on the SPARC,
    // if there's an overflow, we need to check the tag bits to see if the overflow
    // is caused by non-smi arguments, or by an actual integer overflow.
    // but on PPC, we always check the object tags in the beginning, so if an overflow
    // occurs, it's a "real" integer overflow. -mabdelmalek 12/02
  }


  void TypeTestNode::br_if_smi(Assembler* a, Location rcvr, fint smiIndex) {
    a->andi_(R0, rcvr, Tag_Mask, NumberOperand);
    Label* label1 = new Label(a->printing);
    a->beq(*label1);
    define(smiIndex, label1);
  }

  void TypeTestNode::br_if_float(Assembler* a, Location rcvr, fint floatIndex) {
    a->andi_(R0, rcvr, Float_Tag, NumberOperand);
    Label* label2 = new Label(a->printing);
    a->bne(*label2);
    define(floatIndex, label2);
  }
  
  void TypeTestNode::br_to_unknown_case(Assembler* a) {
    Label* unknownCase = new Label(a->printing);
    a->b(*unknownCase, NumberOperand);
    define(0, unknownCase);
  }

  // Returns index of case to jump to, or 0 if none chosen.
  // Also returns loadMapAfterHandlingImmediates, label where caller gens code to load map
  // -- dmu 10/03

  fint TypeTestNode::prologue(Assembler* a, Location rcvr, fint smiIndex,
                              fint floatIndex, bool immediateOnly,
                              Label*& loadMapAfterHandlingImmediates) {
    // handle immediate cases of type test and load map if necessary
    // smiIndex/Float are the indices of the smi/float case in the map list
    // (+ 1, so that "not present" == 0)
    // returns the index (+ 1) of the fall-through case 
    assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
    assert(!immediateOnly || !needMapLoad,
           "immediateOnly implies !needMapLoad");
           
    if (needMapLoad  &&  FastMapTest  &&  !smiIndex  &&  !floatIndex)
      return 0; // 0 for no match, caller will load map

    if (!needMapLoad) {
      // no map load needed; no mem maps to test, rcvr could be memOop at this point
      if (  smiIndex)  br_if_smi  (a, rcvr,   smiIndex);
      if (floatIndex)  br_if_float(a, rcvr, floatIndex);
      return 0; // will be no more testing, so can fall-through to unknown
    }
     
    a->andi_(R0, rcvr, Mem_Tag, NumberOperand);
    loadMapAfterHandlingImmediates = new Label(a->printing);
    a->bne(*loadMapAfterHandlingImmediates, predict_weird); // fewer selectors have int cases, so probably goes
      
    if (smiIndex  &&  floatIndex) {
      br_if_smi(a, rcvr, smiIndex);
      return floatIndex;
    }
    if (  smiIndex)   br_if_smi  (a, rcvr,   smiIndex);
    if (floatIndex)   br_if_float(a, rcvr, floatIndex);
    br_to_unknown_case(a); // fall-through would fall into mem testing, since n == 0 does not gen branch
    return 0;
  }
  
  
  void TypeTestNode::testMap(ConstPReg* pr, fint index) {
    assert(pr->constant->is_map(), "should be map");
    assert(needMapLoad, "need to load receiver map");
    // if the receiver was moved to MapReg in TypeTestNode (this would
    // happen if it wasn't already in a register), then the following
    // instruction might clober the receiver (if "pr" wasn't already
    // in a register).  but this is ok since we don't need the receiver
    // any more. -mabdelmalek 10/02.
    Location maploc = genHelper->loadImmediateOop(pr, MapReg, false);  // load map
    theAssembler->cmpw(maploc, RcvrMapReg);                   // compare
    Label* match = new Label(theAssembler->printing);
    theAssembler->beq(*match, predict_weird);                 // branch if match
    define(index, match);
  }
  
  void TypeTestNode::testOop(ConstPReg* pr, fint index) {
    assert(!pr->constant->is_map(), "should be oop");
    Location loc = genHelper->loadImmediateOop(pr, RcvrMapReg, false); // load oop
    theAssembler->cmpw(loc, r);                              // compare
    Label* match = new Label(theAssembler->printing);
    theAssembler->beq(*match, predict_weird);                // branch if match
    define(index, match);
  }


  // Shoould be refactored someday ala PICs. -- dmu 2/03
  void TypeTestNode::gen() {
    // generates n-way type test; fall-through code is "unknown" case

    BasicNode::gen();
    r = genHelper->moveToReg(_src, MapReg);
    

    // indexes if smi or float branches if present. Branch index is one more than maps index -- dmu
    // NOTE: index 0 is the no match brach target
    fint   smiIndex = 0;
    fint floatIndex = 0;
         if (maps->nth(0) == Memory->  smi_map->enclosing_mapOop())   smiIndex = 1;
    else if (maps->nth(0) == Memory->float_map->enclosing_mapOop()) floatIndex = 1;
    if (maps->length() > 1) {
           if (maps->nth(1) == Memory->  smi_map->enclosing_mapOop())   smiIndex = 2;
      else if (maps->nth(1) == Memory->float_map->enclosing_mapOop()) floatIndex = 2;
    }


    fint nconstants = 0;
    fint ntests = maps->length();
    fint firstMem = max(smiIndex, floatIndex);
    bool immediateOnly = firstMem == maps->length();

    for (fint i = firstMem; i < ntests; ++i) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map()) ++nconstants;
    }

    // first test against all constants
    if (!hasUnknown  &&  nconstants == ntests) {
      // don't need to check for last constant
      --ntests;
    }
    for (fint i = firstMem;  i < ntests;  ++i) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map())   testOop(pr, i + 1);
    }
    if (!hasUnknown && nconstants >= ntests) {
      // last case; should omit branch
      Label* match = new Label(theAssembler->printing);
      theAssembler->b(*match, NumberOperand);
      define(ntests + 1, match);
      return;           // done -- tested all constants
    }

    Label* loadMapAfterHandlingImmediates = NULL;
    fint n = prologue(theAssembler, r, smiIndex, floatIndex, immediateOnly, loadMapAfterHandlingImmediates);

    if (n) {
      Label* match = new Label(theAssembler->printing);
      theAssembler->b(*match, NumberOperand);
      define(n, match);
    }

    if (!loadMapAfterHandlingImmediates)
      ;
    else if (immediateOnly)
      define(0, loadMapAfterHandlingImmediates);     // no memOop tests
    else
      loadMapAfterHandlingImmediates->define();

    if (!hasUnknown) --ntests;      // all maps known, can omit last test
    // test against all maps
    if (needMapLoad) {
      // load receiver map
      theAssembler->lwz(RcvrMapReg, map_offset(), NumberOperand, r);
    }
    for (fint i = firstMem; i < ntests; i++) {
      ConstPReg* pr = mapPRs->nth(i);
      if (pr->constant->is_map()) testMap(pr, i + 1);
    }
    if (!hasUnknown) {
      // last case; should omit branch
      Label* match = new Label(theAssembler->printing);
      theAssembler->b(*match, NumberOperand);
      define(ntests + 1, match);
    }
  }

  
  void IndexedBranchNode::gen() {
    // generates n-way indexed branch;
    // fall-through code is non-int or out of bounds
    BasicNode::gen();
    r = genHelper->moveToReg(_src, IndexReg);

    // check tag
    Label end;
    if (!srcMustBeSmi) {
      theAssembler->andi_(R0, r, Mem_Tag, NumberOperand);
      theAssembler->bne(end);
    }

    // check bounds
    genHelper->loadImmediateOop(as_smiOop(nCases), BoundsReg, true);
    theAssembler->cmplw(BoundsReg, r);   // use unsigned comp to catch negative number
    theAssembler->ble(end); // goto end if index is out of bounds
  
    Label L(theAssembler->printing);
    theAssembler->bl(L, NumberOperand); // get pc of next inst into link
    pc_t link_reg_value = theAssembler->addr();
    L.define();
  
    const int32 indexShift = 2 - Tag_Size;
    assert(indexShift == 0, "no shift needed");
    theAssembler->mflr(R0);
    theAssembler->add(Temp1, R0, r); // and displacement to link
    const int32 bytes_from_link_value_to_jumps = 20;
    theAssembler->addi(Temp1, Temp1, bytes_from_link_value_to_jumps, NumberOperand); // add in bytes from link setting to first jump
    theAssembler->mtlr(Temp1);
    theAssembler->balwlr();
    pc_t start_of_jumps= theAssembler->addr();
    assert( bytes_from_link_value_to_jumps == start_of_jumps - link_reg_value, "recount");

    for (fint i = 0;  i < nCases;  ++i) {
      Label* nthCase = new Label(theAssembler->printing);
      theAssembler->b(*nthCase, NumberOperand);
      Node* n = nexti(i + 1);
      n->l = nthCase->unify(n->l);
    }
    end.define();
  }


  void BlockZapNode::gen() {
    BasicNode::gen();
    Location t = genHelper->moveToReg(block(), Temp1);
    theAssembler->li(R0, 0, NumberOperand);
    theAssembler->stw(R0, scope_offset(), NumberOperand, t);
  }

  void AbstractArrayAtNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); U_CHECK(arg);
    if (error) D_CHECK(error);
    // Would not have to do this for Temp[12], but since Temp3 and 4 are really
    // r3 and r4 (not dedicated temps) these are needed. -- dmu 12/02
    use_count[Temp3]++; def_count[Temp3]++;       // (potentially) uses Temp3
    use_count[Temp4]++; def_count[Temp4]++;       // (potentially) uses Temp4
  }
  
    
  bool AbstractArrayAtNode::canCopyPropagateFrom(PReg* d) {
    return d->loc != Temp3  &&  d->loc != Temp4; // prevent local cp of Temps
  }


  // We have 4 temp registers on SPARC.  On PPC, we only have 2 temp registers (and R0
  // sometimes, but often it's not useful because of its 0-nature).
  // I tried writing the array primitives using only 2 temporary registers.  I almost
  // had everything done.  But in the end I realized it's just not possible to do this
  // with 2.5 temp registers.  So I'm using R3 and R4 as temp registers as well.
  // I don't think there's a problem with doing this.  I added Temp3 and Temp4 to
  // regs_ppc.hh.  But if you use these register, you must reserve them, through
  // the markAllocated function (see above).  There's a comment about this as well
  // in regs_ppc.hh.  -mabdelmalek 12/02
  //
  // Also need to define canCopyPropagateFrom to prevent local cp,
  // see canCopyPropagateFrom in node.hh. -- dmu 1/03
  void AbstractArrayAtNode::gen() {
    BasicNode::gen();
    Assembler* a = theAssembler;
    Label* argFail = NULL;          // if arg isn't a smi
    Label* indexFail = new Label(a->printing);  // if arg is out of bounds
    assert(_src->loc != Temp3  &&  _src->loc != Temp4, "_src should not be in temps");
    assert( arg->loc != Temp3  &&   arg->loc != Temp4,  "arg should not be in temps");
    Location arr = genHelper->moveToReg(_src, Temp2);
    Location index = genHelper->moveToReg(arg, Temp1);
    // load array size
    Location size = Temp3;
    a->lwz(size, sizeOffset, NumberOperand, arr);
    if (!intArg) {
      // CP may have propagated a constant into arg
      intArg = arg->isConstPReg() && ((ConstPReg*)arg)->constant->is_smi();
    }
    if (!intArg) {
      // test arg for smiOop
      a->andi_(R0, index, Tag_Mask, NumberOperand);
      Label* failLabel = new Label(a->printing);
      a->bne(*failLabel);
      argFail = argFail->unify(failLabel);
    }
    argFail = argFail->unify(testArg2());
    a->cmplw(index, size);
    a->bge(*indexFail, predict_usual); // likely to fall through
    Location res = isRegister(_dest->loc) ? _dest->loc : Temp1;
    bool needDestStore = genAccess(arr, index, res);
    if (needDestStore && !isRegister(_dest->loc)) {
      genHelper->moveRegToLoc(res, _dest->loc);
    }
    Label* done = new Label(a->printing);
    a->b(*done, NumberOperand);
    MergeNode* failMerge = (MergeNode*)next1();
    if (argFail) {
      argFail->define();
      if (error) {
        Location err = isRegister(error->loc) ? error->loc : Temp1;
        genHelper->loadImmediateOop(VMString[BADTYPEERROR], err);
        if (err != error->loc) genHelper->moveRegToLoc(err, error->loc);
      }
      if (failMerge) { // test added by dmu 4/27/96
        Label* L = new Label(a->printing);
        a->b(*L, NumberOperand);
        failMerge->l = failMerge->l->unify(L);
      }
    }
    indexFail->define();
    if (error) {
      Location err = isRegister(error->loc) ? error->loc : Temp1;
      genHelper->loadImmediateOop(VMString[BADINDEXERROR], err);
      if (err != error->loc) genHelper->moveRegToLoc(err, error->loc);
    }
    if (failMerge) { // test added by dmu 4/27/96
      Label* L = new Label(a->printing);
      a->b(*L, NumberOperand);
      failMerge->l = failMerge->l->unify(L);
    }
    done->define();
  }

  // gen array access; Temp3/4 are available, Temp1 may hold index, Temp2
  // may hold array
  bool ArrayAtNode::genAccess(Location arr, Location index, Location dest) {
    theAssembler->add(Temp3, arr, index);
    theAssembler->lwz(dest, dataOffset, NumberOperand, Temp3);
    return true;
  }

  bool ByteArrayAtNode::genAccess(Location arr, Location index, Location dest) {
    theAssembler->lwz(Temp3, dataOffset, NumberOperand, arr);
    theAssembler->srwi(Temp4, index, Tag_Size);
    theAssembler->add(Temp3, Temp3, Temp4);
    theAssembler->lbz(dest, 0, NumberOperand, Temp3);
    theAssembler->slwi(dest, dest, Tag_Size);
    return true;
  }

  bool ArrayAtPutNode::genAccess(Location arr, Location index, Location dest) {
    Unused(dest);
    Location el;
    bool needCheckStore;
    if (elem->isConstPReg()) {
      ConstPReg* value = (ConstPReg*)elem;
      el = genHelper->loadImmediateOop(value, Temp3, false);
      needCheckStore = value->constant->is_new();
    } else {
      assert(elem->loc != Temp3  &&  elem->loc != Temp4, "elem should not be in Temp3 or Temp4");
      el = genHelper->moveToReg(elem, Temp3);
      needCheckStore = true;
    }
    theAssembler->add(Temp4, arr, index);
    theAssembler->stw(el, dataOffset, NumberOperand, Temp4);
    if (needCheckStore) {
      theAssembler->addi(Temp4, Temp4, dataOffset, NumberOperand);
      theAssembler->srwi(Temp4, Temp4, card_shift);  // shift target addr
      theAssembler->li(R0, 0, NumberOperand);
      if (!UseByteMapBaseReg) {
        theAssembler->load_from_address(Temp3, &byte_map_base, VMAddressOperand, Temp3);
        theAssembler->stbx(R0, Temp4, Temp3);
      }
      else {
        theAssembler->stbx(R0, Temp4, ByteMapBaseReg);  // set byte in map
      }
    }
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src) genHelper->moveRegToLoc(arr, _dest->loc);
    return false;
  }

  Label* ByteArrayAtPutNode::testArg2() {
    // check if arg is 0..255
    if (elem->isConstPReg()) {
      if (((ConstPReg*)elem)->constant->is_smi()) {
        // no run-time check required
        return NULL;
      } else {
        // primitive will always fail
        Label* L = new Label(theAssembler->printing);
        theAssembler->b(*L, NumberOperand);
        return L;
      }
    } else {
      assert(elem->loc != Temp3  &&  elem->loc != Temp4, "elem should not be in Temp3 or Temp4");
      Location e = genHelper->moveToReg(elem, Temp4);
      Label* fail = NULL;
      if (!intElem) {
        // check for int
        theAssembler->andi_(R0, e, Tag_Mask, NumberOperand);
        fail = new Label(theAssembler->printing);
        theAssembler->bne(*fail);
      }
      theAssembler->cmplwi(e, 256 << Tag_Size, NumberOperand);
      Label* fail2 = new Label(theAssembler->printing);
      theAssembler->bgt(*fail2);
      return fail->unify(fail2);
    }
  }
  
  bool ByteArrayAtPutNode::genAccess(Location arr,
                                     Location index, Location dest) {
    Unused(dest);
    theAssembler->lwz(Temp4, dataOffset, NumberOperand, arr);
    theAssembler->srwi(Temp1, index, Tag_Size);
    theAssembler->add(Temp4, Temp4, Temp1);
    if (elem->isConstPReg()) {
      // storing a constant - may be non-smi, but then this code will never
      // be executed anyway because the primitive fails
      ConstPReg* value = (ConstPReg*)elem;
      if (value->constant->is_smi()) {
        theAssembler->load_immediate(Temp3, smiOop(value->constant)->value(), NumberOperand);
      } else {
        fatal("ByteArrayAtPutNode::genAccess: what to do?");
      }
    } else {
      assert(elem->loc != Temp3  &&  elem->loc != Temp4, "elem should not be in Temp3 or Temp4");
      Location el = genHelper->moveToReg(elem, Temp3);
      theAssembler->srwi(Temp3, el, Tag_Size);  // convert to char
    }
    theAssembler->stb(Temp3, 0, NumberOperand, Temp4); // store in byte array
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src) genHelper->moveRegToLoc(arr, _dest->loc);
    return false;           // result already handled here
  }

  void FlushNode::flushRegister(PReg* pr) {
    Location loc = pr->loc;
    assert(isRegister(loc), "inconsistency: FlushNode::gen should only call me with a register");
    theSIC->check_flushability(pr);


    if (theSIC->are_register_arguments_saved_on_stack() &&
        theSIC->is_incoming_register_arg_or_rcvr(loc)) {
      #if 1
        // Ordinarily, we'd flush the incoming argument.  But to make things simple
        // for GC, in the method prologue we flush _all_ incoming arguments if we
        // have any subblocks.
      #else
        // To illustrate where we'll save the argument, suppose we're asked to flush
        // the second argument.  The second argument is saved in register R29 in the
        // prologue, and so we'll be passed the R29 location.  So to find the argument
        // index, subtract the register from R31.  Then, since arguments are saved on
        // the calleR's stack frame, we add our frame size to the offset returned by the
        // helper function rcvr_and_argument_offset.
        fint argIndex = R31 - loc;
        fint spOffset = thisFrameSize + rcvr_and_argument_offset(argIndex);
        theAssembler->stw(loc, spOffset * oopSize, NumberOperand, SP);
      #endif
    }
  }

  void DeadBlockNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Comment("dead block code");
    Label next(theAssembler->printing);
    theAssembler->bl(next, NumberOperand); // prim needs some PC in this method
    next.define();
    theAssembler->mflr(CReceiverReg);
    PrimNode::gen();
  }
  
  void DeadEndNode::gen() {
    // this node is unreachable - generate a trap for debugging
#if 0
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      BasicNode::gen();
      theAssembler->trap();
    }
#   endif
#endif
  }

  // XXX: how to catch illegal instructions on Mac OS X?  Unlike SPARC, an illegal
  // instruction does not generate a SIGILL signal.  Instead, a Mach exception is
  // raised apparently.  For now, uncommon nodes are turned off on PPC.
   void UncommonNode::gen() {
     BasicNode::gen();
     genPcDesc();
     fatal("UncommonNode::gen: on PPC, illegal instructions don't generate SIGILL");
     //    theAssembler->unimp(0, restartSend);
  }

# endif  // sic
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */


# pragma implementation "sic_ppc.hh"
# include "_sic_ppc.cpp.incl"

# ifdef SIC_COMPILER

// On PPC, registers have no local place to be saved
// (PPC uses callee-save), so cannot simply flush a register.
// Therefore allocate even uplevelR pregs on the stack, too. -- dmu 12/02
bool SICAllocator::keepUplevelRPRegsInMemory = true; 

void SICompiler::initializeForPlatform() {
  _number_of_saved_nonvolatile_registers = 0;
  nlrLabel = NULL;
  _are_register_arguments_saved_on_stack = false;
}


int32 SICompiler::stackTempCount() {
  return number_of_saved_nonvolatile_registers()
       + number_of_memory_locals()
       + max_no_of_outgoing_args_and_rcvr();
}


int32 SICompiler::numberOfIncomingArgsAndRcvrInRegisters() {
  return min(incoming_arg_count(), NumArgRegisters) + 1 /* for rcvr */;
}


int32 SICompiler::max_no_of_outgoing_args_and_rcvr() {
  return argCount + 1 /* for rcvr */;
}


int32 SICompiler::number_of_saved_nonvolatile_registers() {
  // _number_of_saved_nonvolatile_registers is set by LongRegisterString::allocate, if we allocate any
  // registers for local variables.  
  // If _number_of_saved_nonvolatile_registers is not set,
  // this method didn't use any callee-saved registers, so _number_of_saved_nonvolatile_registers
  // never got set from LongRegisterString::allocate.  in this case, max 
  // register depth is just the number of incoming args and receiver in registers.
  //  -- Michael Abd-El-Malek, 12/02
  int32 r =  _number_of_saved_nonvolatile_registers
          ?  _number_of_saved_nonvolatile_registers
          :  numberOfIncomingArgsAndRcvrInRegisters();
  assert(r <= NumCalleeSavedRegs, "error in calculating register depth");
  return r;
}


int32 SICompiler::number_of_memory_locals() {
  return stackLocCount;
}


bool SICompiler::is_incoming_register_arg_or_rcvr(Location l) {
  return  isRegister(l)
      &&  (R31 - l)  <  numberOfIncomingArgsAndRcvrInRegisters();
}


void SICompiler::check_flushability(PReg* p) {
  assert( !p->uplevelR  ||  !isRegister(p->loc)  ||  is_incoming_register_arg_or_rcvr(p->loc)
       || (p->print(), false),
          "PPC can only flush incoming rcvr & args, other regs are callee-save");
}

void SICompiler::cope_with_uplevel_access_to(PReg* pr) {
  check_flushability(pr);
  if (isRegister(pr->loc)) 
    save_register_arguments_on_stack();
}

// allocate nonvols down from R31, must save/restore them, so track depth
void SICompiler::update_number_of_saved_nonvolatile_registers_for(Location l) {
  if (!isRegister(l))  return;
  _number_of_saved_nonvolatile_registers = max(_number_of_saved_nonvolatile_registers, 32 - l);
}

# endif // SIC_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "longRegString_ppc.hh"

# include "_longRegString_ppc.cpp.incl"


# ifdef SIC_COMPILER


void LongRegisterString::allocate(Location l) {
  theSIC->update_number_of_saved_nonvolatile_registers_for(l);
  doAllocate(l);
}


# endif // SIC_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "registerState_ppc.hh"
# include "_registerState_ppc.cpp.incl"

# ifdef FAST_COMPILER

# undef  MAX
# define MAX ((maxDepth >> LogBitsPerWord) + 1)


void RegisterState::initialize_for_platform(fint /*maxTemps*/) {
  _number_of_saved_nonvolatile_registers = 0;
}


void RegisterState::genMask() {
  RegisterString m = mask();
  theAssembler->Data(m);
}


RegisterString RegisterState::mask() {
  assert(sizeof(allocated) == oopSize, "");
  return allocated; // just the registers, have enough to worry about
}


Location RegisterState::pickLocal() {
  // pick a local
  Location r = ::pickRegister(allocated, LocalMask);
  if (r == UnAllocated) 
    r = pickStackTemp();
  allocate(r);
  return r;
}


void RegisterState::allocateArgs(fint nargs, bool isPrimCall) {
  Unused(isPrimCall);
  argDepth = max(argDepth, nargs);
}


void RegisterState::allocate(Location r) {
  if (isRegister(r)) {
    ::allocateRegister(allocated, r);
    // allocate nonvols down from R31, must save/restore them, so track depth
    _number_of_saved_nonvolatile_registers = max(_number_of_saved_nonvolatile_registers, 32 - r);
  } 
  else if (is_StackLocation(r)) {
    assert(!isSet(stackAllocs[whichMask(r)], whichBit(r)),
           "already allocated");
    setNth(stackAllocs[whichMask(r)], whichBit(r));
    fint tempNo = index_for_StackLocation(r) + 1;
    if (tempNo >= stackDepth) 
      stackDepth = tempNo;
    curDepth++;
    assert(curDepth <= stackDepth, "curDepth too big");
  } 
}
  

void RegisterState::deallocate(Location r) {
  if (isRegister(r)) {
    ::deallocateRegister(allocated, r);
    allocated = ::allocate(allocated, permanent); // ensure we do not deallocate a permanent reg
  } 
  else if (is_StackLocation(r)) {
    if (isSet(stackPerms[whichMask(r)], whichBit(r))) {
      // permanent -- don't deallocate
    } 
    else {
      assert(isSet(stackAllocs[whichMask(r)], whichBit(r)), "not allocated");
      clearNth(stackAllocs[whichMask(r)], whichBit(r));
      --curDepth;
      assert(curDepth >= 0, "negative depth");
    }
  } 
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "registerString_ppc.hh"

# include "_registerString_ppc.cpp.incl"

# ifdef FAST_COMPILER

void printRegister(Location r) {
  if (is_StackLocation(r)) {
    lprintf("T%ld", long(index_for_StackLocation(r)));
  } else if (r >= ArgStackLocations) {
    lprintf("A%ld", long(r - ArgStackLocations));
  } else if (r >= IArgStackLocations) {
    lprintf("I%ld", long(r - IArgStackLocations));
  } else {
    lprintf("%s", RegisterNames[r]);
  }
}


RegisterString registerMaskBit(Location l, fint stackLocs, fint nonRegisterArgs) {
  Unused(nonRegisterArgs);
  if (isRegister(l)) {
    assert(l >= LowestLocalNonVolReg && l <= HighestNonVolReg, "unexpected register");
    return nthBit(l);
  } else {
    // The Sparc version of this function returns a mask bit for some stack locations.
    // But it seems that on the PPC, we only return mask bits for registers (i.e. not
    // for stack locations).
    // A look at registerState{,_ppc}.cpp shows that the NIC's mask only contains
    // registers, not stack locations.  See RegisterState::mask for a supporting comment.
    
    // Dave: why don't we put stack loc's info in the registerMask?  I haven't
    // looked at the GC code too closely.  Even on the Sparc, I'm confused because if the
    // required mask bit doesn't fit in a word, we don't return it.  Do we manually
    // go through the stack locations at GC for locations not contained in the mask?
    
    // Mike: the compiler must initialize all stack locations, that way they are always GC'ed.
    // This was an optimization on the SPARC. -- dmu
    return 0;
  }
}

# endif // FAST_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */


# pragma implementation "fcompiler_ppc.hh"
# include "_fcompiler_ppc.cpp.incl"

# ifdef FAST_COMPILER
 
fint   FCompiler::max_no_of_outgoing_args_and_rcvr() {
  return codeGen->max_no_of_outgoing_args_and_rcvr();
}


fint   FCompiler::number_of_saved_nonvolatile_registers() {
  return codeGen->number_of_saved_nonvolatile_registers();
}


fint   FCompiler::number_of_memory_locals() {
  return codeGen->number_of_memory_locals();
}


bool FCompiler::are_register_arguments_saved_on_stack() {
  return codeGen->need_to_save_args_on_stack();
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.18 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "codeGen_ppc.hh"
# pragma implementation "codeGen_inline_ppc.hh"

# include "_codeGen_ppc.cpp.incl"

# ifdef FAST_COMPILER

CodeGen* theCodeGen;


# if COMPILER == MWERKS_COMPILER
# pragma dont_inline on // needed to avoid bug in CW 5.0, dmu: 9/99
# endif


void CodeGen::moveComplicated(Location dest, Location src, bool set_cc) {
  // dest may be R0
  assert(!(isRegister(src) && isRegister(dest)), "shouldn't call this");
  Location t;
  if (isRegister(dest)) {
    t = dest;
  } else if (isRegister(src)) {
    t = src;
  } else {
    t = Temp1;
  }
  if (isRegister(src)) {
    // already in register
  } else {
    fint offset = getOffset_in_my_frame(src);
    a.lwz(t, offset, NumberOperand, SP);
  }
  
  if (set_cc)
    a.cmpwi(t, 0, NumberOperand); // set the cond codes
    
  if (isRegister(dest)) {
    // already in register
  } else {
    fint offset = getOffset_in_my_frame(dest);
    a.stw(t, offset, NumberOperand, SP);
  }
}

void CodeGen::genCountCode(int32* counter) {
  a.Comment("count # calls");
  a.load_from_address(Temp1, counter, VMAddressOperand, Temp2);
  a.addi(Temp1, Temp1, 1, NumberOperand);
  int32 lo, hi;
  a.break_up_word_for_adding((int32)counter, lo, hi);
  a.stw(Temp1, lo, VMAddressOperand, Temp2);
}


Location CodeGen::get_SPLimit(Location temp) {
  if (!UseSPLimitReg) {
    a.load_from_address( Temp1, &SPLimit, VMAddressOperand, Temp1);
    return Temp1;
  }
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      a.load_from_address( Temp1, &SPLimit, VMAddressOperand, Temp1);
      a.cmpw(Temp1, SPLimitReg);
      Label ok(a.printing);
      a.beq(ok, predict_weird);
      a.load_immediate(CReceiverReg, (int)"CodeGen::get_SPLimit: SPLimitReg != SPLimit", VMAddressOperand);
      a.branch_to(first_inst_addr(lprint_error), VMAddressOperand, temp, true);
      ok.define();        
    }
  # endif

  return SPLimitReg;
}
  
  
void CodeGen::testStackOverflow(RegisterState* s) {
  a.Comment("stack overflow/interrupt check");
  a.cmplw(SP, get_SPLimit(Temp1));  // test for stack overflow
  Label L;
  a.bge(L, predict_weird); // branch if no overflow
  assert(s->mask() != 0, "should have non-zero mask");
  assert((s->allocated & nthBit(IReceiverReg)) != 0,
         "should have at least IReceiverReg allocated");
  (void)cPrimCall(intrCheck(), s, true, true, 0);
  L.define();
}


void CodeGen::testStackOverflowForLoop( Label*& dst,  Label*& nlr, RegisterState* s) {
  //    cmp sp, splimit
  //    bge dst
  //    nop
  //    <cPrimCall interruptCheck>
   
  // (this code lifted from what used to be restart)
  
  a.Comment("stack overflow/interrupt check for loop");
  a.cmplw(SP, get_SPLimit(Temp1));  // test for stack overflow
  dst = new Label(a.printing);
  a.bge(*dst);    
  nlr = cPrimCall(intrCheck(), s, false, true, 0);
}


void CodeGen::smiOop_prologue() {
  //  andi. R0, ReceiverReg, Tag_Mask (if tag is zero is smi)
  //  beq+  cache_hit
  //  bla  missHandler 
  //   <or>
  //    li R0, low(missHandler)
  //    oris r0, r0, hi(misHandler)
  //    mtlr r0
  //    balwlrl
  // cache_hit:
  a.andi_(R0, ReceiverReg, Tag_Mask, NumberOperand);
  Label hit;
  a.beq(hit, predict_weird); // cause it's taken
  jumpTo(Memory->code->trapdoors->SendMessage_stub_td(R0), R0, UnAllocated);
  hit.define();
}

void CodeGen::floatOop_prologue() {
  // andi. R0, ReceiverReg, Float_Tag
  // bne+ cache_hit // if bit is set it is a hit
  // <call missHandler>
  // hit:
  a.andi_(R0, ReceiverReg, Float_Tag, NumberOperand);
  Label hit;
  a.bne(hit, predict_weird); // cause it's taken
  jumpTo(Memory->code->trapdoors->SendMessage_stub_td(R0), R0, UnAllocated);
  hit.define();
}

void CodeGen::memOop_prologue() {
  //   andi. R0, ReceiverReg, Mem_Tag
  //   bne+  check_map
  //   <call missHandler>
  // check_map:
  //   lw R0, map_offset - Mem_Tag(ReceiverReg)
  //   loadOop Temp1, receiver_map
  //   cmpw R0, Temp1
  //   beq+  hit
  //   <call missHandler>
  // hit:
  a.andi_(R0, ReceiverReg, Mem_Tag, NumberOperand);
  Label check_map;
  a.bne(check_map, predict_weird); // cause it's taken
  DefinedLabel miss(a.printing);
  jumpTo(Memory->code->trapdoors->SendMessage_stub_td(R0), R0, UnAllocated);
  check_map.define();
  a.lwz(R0, map_offset(), NumberOperand, ReceiverReg);
  loadOop( Temp1, L->receiverMapOop()); // load customized map
  a.cmpw(R0, Temp1);
  a.bne(miss, predict_weird); // backwards, will fall through usually
}

void CodeGen::checkOop(Label& general_miss, oop what, Location loc_to_check) {
  // test for inline cache hit (selector, delegatee)
  loadOop(Temp1, what); // load hard-wired value
  move(Temp2, loc_to_check);
  a.cmplw( Temp1, Temp2);    // compare against actual value
  if (general_miss.isDefined())  // reuse miss handler
    a.bne(general_miss, predict_weird); // is back, but will fall through
  else {
    Label hit(a.printing);
    a.beq(hit, predict_weird); // is fwd, but will go
    general_miss.define(); // define miss
    jumpTo(Memory->code->trapdoors->SendMessage_stub_td(R0), R0, UnAllocated);
    hit.define();
  }      
}

void CodeGen::checkRecompilation() {
  // di recompilation doesn't work right now - see recompile.c
  if (diLink) return;
  a.Comment("test for recompilation");
  int32 countID = theCompiler->countID;
  void* counter = &useCount[countID];
  a.load_from_address(Temp2, counter, VMAddressOperand, Temp1);
  a.addi(Temp2, Temp2, 1, NumberOperand);
  int32 lo, hi;
  a.break_up_word_for_adding((int32)counter, lo, hi);
  a.stw(Temp2, lo, VMAddressOperand, Temp1);

  fint limit = recompileLimit(theCompiler->level());
  // compare to limit
  if (fits_within_si(limit)) {
    a.cmpwi( Temp2, limit, NumberOperand );
   }
   else {
     a.load_immediate( R0, limit, NumberOperand);
     a.cmpw( Temp2, R0);
  }
  Label ok;
  a.bne(ok, predict_weird); // likely to go

  // following code modelled after call to SendDIMessage_stub; see
  // CodeGen::verifyParents.  -mabdelmalek 12/02

  // Pass a link to caller's sendDesc in LinkReg.
  // Pass a link to callee (i.e. this code being generated) in RecompileLinkReg
  
  // save link so can use link to get address of this code
  a.mflr(RecompileTempReg);
  a.stw(RecompileTempReg, saved_pc_offset * oopSize, NumberOperand, SP);
  Label next;
  a.bl(next, NumberOperand); // needed to get addr of this code
  next.define();
  a.mflr(RecompileLinkReg);
  a.mtlr(RecompileTempReg);

  // call recompiler
  char* fnaddr = diLink 
               ? Memory->code->trapdoors->DIRecompile_stub_td(R0) 
               : Memory->code->trapdoors->  Recompile_stub_td(R0);
  a.branch_to(fnaddr, PVMAddressOperand, R0, false);

  // we don't return here from Recompile_stub.  Recompile_stub jumps to new method if
  // we do on-stack replacement.  if we don't do on-stack replacement, it returns directly
  // to the verified entry point in the nmethod.  -mabdelmalek 12/5/2002
  ok.define();
}

  
void CodeGen::prologue(bool isAccessMethod, fint nargs) {
  /*
    ; save PC link
    
    mflr    r0
    stw      r0, LinkageArea.savedPC(sp); save PC link (pc to return to)
  */
  /*      
    stmw    r28, -reg_save_len(sp) ; save 4 nonvol regs
    
    ; save non volatile int regs
    
    li      scr_a, 8 + (LinkageArea.size + reg_save_len); 2 words of args, in bytes = 8 bytes
    sub     scr_a,  sp, scr_a       ; scr_a now has sp - arg length - linkage area len - reg_save_len
    li      r0, 15
    andc    scr_a, scr_a, r0  ; round down
    stw     sp, 0(scr_a)  ; save soon-to-be-old sp
    mr      sp, scr_a ; setup sp for new frame
  */
  
  // *if not DI child
  //    <smi/float/memOop prologue>
  // _verified:                       (entry point from PICs)
  //    if necessary <check selector>
  //    if necessary <check delegatee>
  // *endif DI
  
  // _diCheck:                        (entry point after recompile)
  //    <verify assignable parents>
  
  // *if using recompilation
  //    <checkRecompilation>
  // *endif
  
  // *if haveStackFrame
  //    save sp, -frameSize*oopSize, sp
  // *endif
  
  // <clear stack temporaries and excess argument locations

  // CAUTION: use only Temp1/4 for temps in prologue; other temps
  // may contain lookup parameters.
  
  fint assignableParents = L->adeps->length();
  MethodKind kind =
    isAccessMethod ? MethodKind(-1) : theCompiler->method()->kind();
  _incoming_arg_count = nargs; // for eventual putting into nmethod
  
  if (diLink == 0) {
    if (!L->isReceiverStatic()) {
      // test receiver map
#       if GENERATE_DEBUGGING_AIDS
        if (CheckAssertions)
          switch (L->lookupType()) {
           case NormalLookupType:  break;
           case StaticNormalLookupType:
           case ImplicitSelfLookupType:
           case ResendLookupType:
           case DirectedResendLookupType: fatal("shouldn't miss"); break;
           default: break;
          }
#       endif
      Map* m = L->receiverMap();
      bool imm = m == Memory->smi_map || m == Memory->float_map;
      if (m == Memory->smi_map) {
        smiOop_prologue();
      } else if (m == Memory->float_map) {
        floatOop_prologue();
      } else {
        memOop_prologue();
      }
    }
    
    verifiedOffset = a.offset();
    Label generalMiss(a.printing, NULL);
    a.Comment("verified entry point:");
    
    if (L->isPerform()) {
      a.Comment("check selector");
      checkOop(generalMiss, L->selector(), PerformSelectorLoc);
    }
    
    if (needsDelegatee(L->lookupType()) && !L->isDelegateeStatic()) {
      a.Comment("check delegatee");
      checkOop(generalMiss, L->delegatee(), PerformDelegateeLoc);
    }
  } else {
    // don't check receiver map, selector, delegatee if a DI cache miss
    assert(assignableParents > 0, "should have some di parents to check");
  }
  
  diCheckOffset = a.offset();
  a.Comment("DI entry point:");
  
  if (assignableParents > 0) {
    a.Comment("verify state of assignable parents");
    fint count = 0;
    for (fint i = 0; i < assignableParents; i ++) {
      objectLookupTarget* target = L->adeps->start()[i];
      Location t = loadPath(Temp2, target, ReceiverReg, Temp1);
      count = verifyParents(target, t, count);
    }
  }

  bool recomp = needRecompileCode(theCompiler->level());
  if (recomp) checkRecompilation();

  if (!haveStackFrame) {
     prologueAddr = NULL;
  }
  else {
    a.mflr(R0);
    prologueAddr = a.addr(); // will be used to patch stmw, and stwu below:
    a.stmw(Location(R31), -4 * 1, NumberOperand, SP);         // save nonvol registers, WILL BE PATCHED
    a.stw(R0, saved_pc_offset * oopSize, NumberOperand, SP);
    
    _need_to_save_args_on_stack = ((methodMap*) theCompiler->method()->map())->hasSubBlocks();
    // Also copy args to nonvol regs
    a.Comment("copying args to nonvol regs");
    fint r = ReceiverReg;
    num_reg_args_and_rcvr = min(nargs, NumArgRegisters) + 1; // for rcvr
    for (fint i = 0;  i < num_reg_args_and_rcvr;  ++i, ++r) {
      a.mr(Location( IReceiverReg - (r - ReceiverReg)), Location(r));
    }
    // save args on stack in case they are uplevel-accessed by a block
    // (since they cannot be changed this is just a copy).
    // really, we only need to save the arguments that are uplevel-accessed.
    // but the NIC doesn't look at _which_ arguments need to be saved: it saves
    // them all if there are any subblocks.  the SIC does better :)  -mabdelmalek 11/02
    if ( need_to_save_args_on_stack() ) {
      a.Comment("flush incoming args to stack");
      a.la(Temp1, rcvr_and_argument_offset(0) * oopSize, NumberOperand, SP);
      a.stswi( ReceiverReg, Temp1, num_reg_args_and_rcvr * oopSize);
    }
    a.Comment("next instruction will be patched by subtracting frame size");
    frame_size_neg_patchees->append(a.addr()); // will have to decrease imm field in stwu
    prologueEndAddr = a.addr(); // may be needed to add instructions to clear extra stack locations
    a.stwu(SP, -0, NumberOperand, SP); // correct frame size is patched in later
    frameCreationOffset = a.offset(); // used by nmethod to find save instr, must have frame by this point
  }
  if (GenerateCountCode) {
    int32* counter;
    if (assignableParents != 0) {
      counter = &NumberOfDIMethodCalls;
    } else if (isAccessMethod) {
      counter = &NumberOfAccessMethodCalls;
    } else if (kind == BlockMethodType) {
      counter = &NumberOfBlockMethodCalls;
    } else {
      counter = &NumberOfMethodCalls;
    }
    genCountCode(counter);
  }

  if (!isAccessMethod) {
    if (!recomp && GenerateLRUCode) {
      // this code is rarely generated in practice (recomp is usually true)
      a.Comment("reset unused bit");
      void* unused_addr = &LRUflag[Memory->code->nextNMethodID()];
      a.li(R0, 0, NumberOperand);
      a.store_to_address(R0, unused_addr, VMAddressOperand, Temp2);
    }
  
    // don't keep uplevel-accessed names in regs
    // (for now, just flush everything)
    assert(haveStackFrame, "just checking");
  }
  else {
    assert(!haveStackFrame, "just checking");
  }    
  a.Comment("End Prologue");
}


Label* CodeGen::postPrologue(RegisterState* s, bool frequentPreemption) {
  // returns address of stack overflow test
  Label* l = new DefinedLabel(a.printing);
  if (frequentPreemption) {
    // no stack test necessary: have explicit check at bci 0
    // (actually I think it is more accurate to say have
    //  frequent checks -- dmu )
  } 
  else {
    testStackOverflow(s);
  }
  return l;
}


void CodeGen::fixupFrame(RegisterState* s) {
  // window size adjustment
  assert(haveStackFrame, "should have stack frame");
  
  _number_of_saved_nonvolatile_registers  = s->number_of_saved_nonvolatile_registers();
  _number_of_memory_locals                = s->stackDepth;  // extra stack locations needed (ran out of registers)
   _max_no_of_outgoing_args_and_rcvr       = s->argDepth + 1; /* for rcvr */
  if (_includes_call_to_untrusted_C) {
    // It is possible to call a C routine that expects more arguments
    // than we actually give it.
    // In that case, the C routine will save as many as 8 registers into
    // my frame, so allocate enough space in my frame.
    _max_no_of_outgoing_args_and_rcvr = max( _max_no_of_outgoing_args_and_rcvr, NumRcvrAndArgRegisters);
  }
  frameSize = number_of_saved_nonvolatile_registers()
            + number_of_memory_locals()
            + max_no_of_outgoing_args_and_rcvr()
            + linkage_area_size                         // space for saved sp, pc, etc.
            + num_extra_locals_for_runtime;            // nmethod frame chain offset
            
  frameSize = roundTo(frameSize, frame_word_alignment); // round up to quadword boundary, so load/store multiples go as fast as possible

  
  // must fixup save/restore instructions:
  
  a.Comment("going back to load/store multiple instructions to patch them");
  a.saveExcursion( prologueAddr );
  assert(is_stmw(a.addr()), "");
  a.stmw( Location(32 - number_of_saved_nonvolatile_registers()),
                   -oopSize * number_of_saved_nonvolatile_registers(), 
                   NumberOperand, SP);
  a.endExcursion();
  
  // fixup each lmw that restores the non-vol-regs
  fint n = lmw_patchees->length();
  for (fint i = 0;  i < n;  ++i) {
    a.saveExcursion( lmw_patchees->nth(i) );
    assert(is_lmw(a.addr()), "");
    a.lmw( Location(32 - number_of_saved_nonvolatile_registers()), 
                    -oopSize * number_of_saved_nonvolatile_registers(),
                    NumberOperand, SP);
    a.endExcursion();
  }
  
  a.Comment("patching instructions that need to know frame size");
  if (PrintCompiledCode) {
    lprintf("(frame size is %d words, # rcvr & outgoing args is %d words)\n", frameSize, 
           _max_no_of_outgoing_args_and_rcvr);
  }

  // patch immediate field of each instruction in this list
  a.increase_all_immediate_fields(frame_size_patchees,      oopSize *  frameSize );
  a.increase_all_immediate_fields(frame_size_neg_patchees,  oopSize * -frameSize );
  a.increase_all_immediate_fields(outgoing_arg_patchees,    oopSize *  _max_no_of_outgoing_args_and_rcvr);

  if (s->stackDepth > 0) {
    // must clear extra stack locations:
    a.saveExcursion( prologueEndAddr );
    inst_t sp_setting_inst = *(inst_t*)a.addr();
    assert( is_stwu(sp_setting_inst), "");
    Label stack_clearing_code(a.printing);
    a.b(stack_clearing_code, NumberOperand);
    DefinedLabel back_to_the_start(a.printing);
    a.endExcursion();
    
    stack_clearing_code.define();
    a.Data(sp_setting_inst); // do the sp setting
    
    // now clear all the extra stack locations
    // not strictly necessary, but simpler this way
    
    a.li(R0, 0, NumberOperand);
    for (int i = 0;  i < s->stackDepth; ++i)
      a.stw(R0, oopSize * stackLocation_offset(i, max_no_of_outgoing_args_and_rcvr()), NumberOperand, SP);
            
    a.b(back_to_the_start, NumberOperand);
  }
}


fint CodeGen::verifyParents(objectLookupTarget* target, Location t,
                            fint count) {
  
  assignableSlotLink* l = target->links;
  assert(l != 0, "expecting an assignable parent link");
    
  for (;;) {
    // load assignable parent slot value
    a.lwz(Temp1, smiOop(l->slot->data)->byte_count() - Mem_Tag, VMAddressOperand, t);
    Label ok;
    Label miss;
    Map* targetMap = l->target->obj->map();
    
    if (l->target->value_constrained) {
      // constraint for a particular oop (ambiguity resolution)
      loadOop(Temp2, l->target->obj);         // load assumed value
      a.cmpw(Temp1, Temp2);                   // compare values
      a.beq(ok, predict_weird);               // will branch
    } 
    // check if map of parent is correct
    else if (targetMap == Memory->smi_map) {
      a.andi_(R0, Temp1, Tag_Mask, NumberOperand);        // test for integer tag
      a.beq( ok, predict_weird );           // branch if parent is integer
    } 
    else if (targetMap == Memory->float_map) {
      a.andi_(R0, Temp1, Float_Tag, NumberOperand);        // test for float tag
      a.bne(ok, predict_weird);             // branch if parent is a float
    }
    else {                                  // must be mem tag
      a.andi_(R0, Temp1, Mem_Tag, NumberOperand);           // test for mem tag
      a.beq(miss);                          // branch if parent is not mem oop

      a.lwz(Temp2, map_offset(), NumberOperand, Temp1);    // load receiver map
      loadOop(R0, targetMap->enclosing_mapOop());    // load map constraint
      a.cmpw(Temp2, R0);                    // compare w/ parent's map
      a.beq(ok, predict_weird);               // correct
    }
    
    miss.define();
    // This will be backpatched to call an nmethod, so need to leave incoming link alone.
    // Pass a link to the branch and nmln in the DILinkReg.
    // Since 3 registers are needed DILink, DITemp, DICount need to use either R0 or 
    // the counter register. Since branch_to uses the count register, we use R0 here.
    
    // save link so can use link to pass nmln addr
    a.mflr(DITempReg);
    a.stw(DITempReg, saved_pc_offset * oopSize, NumberOperand, SP); // save link for stack-crawling
    Label next;
    a.bl(next, NumberOperand); // need to get addr of nmln
    next.define();
    fint link_contents = a.offset(); // where the link reg points to
    a.mflr(DILinkReg);
    a.mtlr(DITempReg);
    
    pc_t link_incr_addr = a.addr();
    assert(DILinkReg != R0, "addi will break");
    a.addi(DILinkReg, DILinkReg, 0, NumberOperand); // will come back and backpatch
    
    loadImmediate(DICountReg, count);  // count of parents verified
    // following must be parsable to set_target_of_Self_call_site
    a.branch_to( Memory->code->trapdoors->SendDIMessage_stub_td(DITempReg), 
                 DIVMAddressOperand, DITempReg, false);
    assert(DITempReg != R0, "callee needs a nonzero reg here");
    
    fint desired_link_contents = a.offset(); // where the link would be if we could use it
    a.saveExcursion(link_incr_addr);         // go back and get the increment right
    a.addi(DILinkReg, DILinkReg, desired_link_contents - link_contents, NumberOperand);
    a.endExcursion();
    a.Data(0);                                // first  part of DI nmln
    a.Data(0);                                // second part of DI nmln
    
          
    ok.define();
    
    ++count;
    if (l->target->links) count = verifyParents(l->target, Temp1, count);
    
    l = l->next;
    if (l == 0) break;
    // if multiple dynamic parents, reload slot holder before looping (HACK!)
    t = loadPath(Temp1, target, ReceiverReg, Temp1);
  }
  
  return count;
}
  
  
void CodeGen::epilogue(Location what) {
  if ( what == IllegalLocation )
    what = IReceiverReg;

  move(ReceiverReg, what, false);
  
  restore_frame_and_return(0);
}


// factored out of epilogue for continuing NLR, too
void CodeGen::restore_frame_and_return(fint byte_offset) {
  if (haveStackFrame) {
  
    a.Comment("next instruction patched by adding frame size");
    frame_size_patchees->append(a.addr());
    a.la(SP,      saved_sp_offset * oopSize, NumberOperand, SP);  // reset SP
    a.lwz(Temp1,  saved_pc_offset * oopSize, NumberOperand, SP);             // get return link
    lmw_patchees->append(a.addr()); // so we can patch next instruction
    a.lmw(Location(R31), -4 * 1, NumberOperand, SP);         // restore nonvol registers
    if (byte_offset != 0)
      a.addi(Temp1, Temp1, byte_offset, NumberOperand);
     a.mtlr(Temp1);
  }
  else if (byte_offset != 0) {
    a.mflr(Temp1);
    a.addi(Temp1, Temp1, byte_offset, NumberOperand);
    a.mtlr(Temp1);
  } 
  a.balwlr();
}


Location CodeGen::flushToStack(Location src, RegisterState* s) {
  // flush register to its corresponding location on the stack
  // For PPC, we must allocate one. (used only for block's self)
  if (isRegister(src)) {
    Location mem_loc = s->pickPermanentStackTemp();
    fint offset = getOffset_in_my_frame(mem_loc);
    a.stw(src, offset, NumberOperand, SP);
    return mem_loc;
  }
  return src;
}


void CodeGen::reload_ByteMapBaseReg(PrimDesc* p) {
  if (UseByteMapBaseReg && p->canScavenge()) {
    // any prim that can scavenge, may expand heap,so need to reload this reg
    a.load_from_address(ByteMapBaseReg, &byte_map_base, VMAddressOperand, ByteMapBaseReg);
  }
}

Label* CodeGen::cPrimCall(PrimDesc* p, RegisterState* s,
                          bool continueNLR, bool trust_fns_arg_count, fint arg_and_rcvr_count) {
  if (!trust_fns_arg_count)
    _includes_call_to_untrusted_C = true;
  Label* where_nlr_jumps_to = NULL;
  // WARNING: following code sequences are known to get_target_of_Self_call_site
  // and set_target_of_Self_call_site
  // Also, getPrimCallEndOffset assumes continuation is right after sequence.
  if (p->canScavenge()) {
    // must save nonvols on way to C to enable stack walking for OOPS
    // Note: AddrDesc::offset_to_call_inst needs a simple constant distance from
    // lis instruction to actual branch instruction. 
    // (this is 3 in the else case.)
    // So, we cannot simply do:
    //    a.load_immediate( SaveSelfNonVolRegs_entry_point_register, (int32) first_inst_addr(p->fn()), PVMAddressOperand );
    //    a.branch_to( Memory->code->trapdoors->SaveSelfNonVolRegs_td(R0), VMAddressOperand, R0, true);
    // -- dmu 7/99
    a.li(SaveSelfNonVolRegs_arg_count_register, arg_and_rcvr_count, NumberOperand);
    a.load_immediate(    R0, (int32) Memory->code->trapdoors->SaveSelfNonVolRegs_td(R0), VMAddressOperand );
    // WARNING: this code sequence is known to get_target_of_Self_call_site(inst_t* instp)
    a.load_immediate( SaveSelfNonVolRegs_entry_point_register, (int32) first_inst_addr(p->fn()), PVMAddressOperand );
    a.mtctr(R0);
    a.balwctrl();
  }
  else
    a.branch_to( first_inst_addr(p->fn()), PVMAddressOperand, Temp1, true); // do the call
  
  // inline cache:
  Label past_nlr(a.printing); 
  a.b(past_nlr, NumberOperand);  // skip over mask and nlr code
  s->genMask(); // used register mask for GC
  
  if ( p->needsNLRCode() ) {
    reload_ByteMapBaseReg(p);  
    if (continueNLR) { // NLR returns from this method, up NLR chain (only for calling intr check after stack overflow & nonLifo trap)
      continueNonLocalReturn();
    }
    else { // do the NLR bit
      where_nlr_jumps_to = new Label(a.printing);
      a.b(*where_nlr_jumps_to, NumberOperand);
    }
  }
  past_nlr.define(); 
  reload_ByteMapBaseReg(p);
  return where_nlr_jumps_to;
}


Label* CodeGen::primFailure(Location failReceiver, Location self,
                            oop failSelector, oop selector, 
                            Location successLoc, blockOop failBlock,
                            RegisterState* s) {
  
  s->allocateArgs(2, true); // will be passing out 2 args; must keep track of these for frame construction
 
  a.andi_(R0, CResultReg, Tag_Mask, NumberOperand);   // test result's tag
  a.cmpwi(R0, Mark_Tag, NumberOperand);               // test for mark tag
  Label success(a.printing); 
  a.bne(success, predict_weird);      // jump to success if not
  
  // clone failure block if necessary
  if (failBlock == NULL) {
    // block already exists
    a.subi( Arg1, CResultReg, Mark_Tag - Mem_Tag, NumberOperand); // mask mark bit
  }
  else {
    // NOTE: this will break if blockOop::clone scavenges!
    assert(!blockClone()->canScavenge(), "rewrite this");
    
    // need to save error string
    Location saved = s->pickLocal();
    if (isRegister(saved)) {
      a.subi( saved, CResultReg, Mark_Tag - Mem_Tag, NumberOperand); // save & mask mark bit
    }
    else {
      a.subi( CResultReg, CResultReg, Mark_Tag - Mem_Tag, NumberOperand); // mask mark bit
      move(saved, CResultReg);
    }
    
    loadBlockOop(failReceiver, failBlock, s);
    move(Arg1, saved); // move err str to right place
    s->deallocate(saved);
  }
  
  // now invoke the fail block
  loadOop(Arg2, selector);
  Label* l = selfCall(s, NormalLookupType, failReceiver, self, 
                      failSelector, NULL, 2);

  assert(ResultReg == CResultReg, "exploit move after success.define");    
  
  success.define();
  move(successLoc, CResultReg);       // move result to right place
  return l;
}

  
void CodeGen::recordStore(Location dst) {
  // NB: when fixed to use register revisit call sites
  a.Comment("recordStore");
  Location t = dst == Temp1  ?  Temp2  : Temp1;
  assert(isRegister(dst), "receiver to check_store must be in a register");
  a.srwi(dst, dst, card_shift);  // shift target addr 
  a.li(R0, 0, NumberOperand);
  
  if (!UseByteMapBaseReg) {
    a.load_from_address(t, &byte_map_base, VMAddressOperand, t);
    a.stbx(R0, dst, t);
  }
  else {    
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        a.load_from_address(t, &byte_map_base, VMAddressOperand, t);
        a.cmpw(t, ByteMapBaseReg);
        Label ok(a.printing);
        a.beq(ok, predict_weird);
        a.load_immediate(CReceiverReg, (int)"CodeGen::recordStore: ByteMapBaseReg != byte_map_base", VMAddressOperand);
        a.branch_to(first_inst_addr(lprint_error), VMAddressOperand, t, true);
        ok.define();
      }
    # endif
    a.stbx(R0, dst, ByteMapBaseReg);  // set byte in map
  }
}
  
  
Label* CodeGen::selfCall(RegisterState* s, LookupType lookupType,
                         Location receiver, Location self,
                         oop selector, oop delegatee, fint argc) {


  Unused(self);
  move( ReceiverReg, receiver);
  // WARNING: following code sequences are known to get_target_of_Self_call_site
  // and set_target_of_Self_call_site
  a.branch_to(Memory->code->trapdoors->SendMessage_stub_td(R0), BPVMAddressOperand, R0, true);
  Label* l = SendDesc(s, lookupType, selector,
                      delegatee ? oop(delegatee) : oop(badOop));
  return l;
}
  
Label* CodeGen::perform(RegisterState* s, LookupType lookupType,
                        Location receiver, Location self, fint argc,
                        oop delegatee) {
  a.Comment("_Perform Primitive");
  return selfCall(s, lookupType, receiver, self,
                  as_smiOop(argc), delegatee, argc);
}


void CodeGen::assignment(Location receiver, slotDesc* s, Location val) {
  assert( isRegister(receiver), "receiver must be a register");
  a.Comment("Begin Simple Assignment");
  Location t;
  if (isRegister(val) && val != CReceiverReg) {
     t = val;
  } 
  else {
    t = Temp1;
    move(t, val);    // load argument
  }
  int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
  a.stw(t, offset, NumberOperand, receiver); // store object data slot contents
  a.addi(Temp1, receiver, offset, NumberOperand); // compute store address
  recordStore(Temp1); // NB: recordStore will clobber Temp2 till I put in ByteMapBaseReg

  a.Comment("End Simple Assignment");
}


void CodeGen::assignment(Location receiver,
                         realSlotRef* path, Location val,
                         bool isMem) {
  // Hackyness inherited from SPARC:
  // I am only called from CodeGen::assignmentCode, and he caches receiver reg in Temp2,
  // so I CANNOT clobber Temp2.
  a.Comment("Begin Assignment");
  Location t;
  if (isRegister(val) && val != CReceiverReg) {
    t = val;
  } 
  else {
    assert(receiver != Temp1, "");
    t = Temp2;
    move(t, val);    // load argument
  }
  int32 offset = smiOop(path->desc->data)->byte_count() - Mem_Tag;
  if (path->holder->is_object_or_map()) {
    Location t1 = loadPath(CReceiverReg, path->holder, receiver, Temp1); // clobbers CReceiverReg
    a.stw(t, offset, NumberOperand, t1);    // store object data slot contents
    if (isMem) {
      a.addi(Temp1, t1, offset, NumberOperand);    // compute store address
      if (t == Temp2)
        fatal("NB: recordStore will clobber Temp2 till I put in ByteMapBaseReg");
      recordStore(Temp1); 
    }
  } else {
    fatal("don't support vframe lookups yet");
  }
  a.Comment("End Assignment");
}

void CodeGen::loadBlockParent(Location block, Location dst) {
  // load [block+scope_offset], dst
  block = moveToReg(block, dst);
  a.lwz(dst, scope_offset(), NumberOperand, block);
}

fint CodeGen::getOffset_in_my_frame(Location src) {
  if ( is_ArgStackLocation(src) ) {
    fint i = arg_index_for_ArgStackLocation(src) + 1 /* rcvr */;
    return rcvr_and_argument_offset(i) * oopSize;
  }
  if ( is_IArgStackLocation(src) ) {
    fint i = arg_index_for_IArgStackLocation(src) + 1 /* rcvr */;
    a.Comment("next instruction patched by adding frame size");
    frame_size_patchees->append(a.addr()); // caller's sp, will have to add in frame size
    return rcvr_and_argument_offset(i) * oopSize;
  }
  if (is_StackLocation(src)) {
    // src is a stack location, allocated after linkage area and outgoing args
    // (will need patching)
    a.Comment("next instruction will be increased by # of rcvr + outgoing args");
    outgoing_arg_patchees->append(a.addr());
    return rcvr_and_argument_offset(index_for_StackLocation(src)) * oopSize;
  }
  if (src == PerformSelectorLoc)   return PerformSelectorLoc_sp_offset;  
  if (src == PerformDelegateeLoc)  return PerformDelegateeLoc_sp_offset; 
  ShouldNotReachHere();
  return 0;
}


void CodeGen::loadSaved(Location dest, Location src,
                        Location sp, compiled_vframe* src_vf) {
  // Load location from a frame on the stack; sp is that frame's 
  // block home (SP on PPC) and frameSz its size.
  
  // PPC version caches rcvr and args in IRegisters
  // Only works for loading, so put this here:
  fint frameSz = src_vf->code->frameSize();
  fint sp_offset;
  if (isIArgRegister(src)) {
    // incoming args are saved in sender's frame after link area
    sp_offset = frameSz + rcvr_and_argument_offset(R31 - src);
  }
  else if (is_StackLocation(src)) {
    // src is a stack location, allocated after linkage area and outgoing args
    sp_offset =  stackLocation_offset(index_for_StackLocation(src), src_vf->code->max_no_of_outgoing_args_and_rcvr());
  }
  else if (isExtraIArgRegister(src)) {
    // stored in incoming arguments area, based on CALLER's sp
    sp_offset = frameSz + rcvr_and_argument_offset(1 /*rcvr*/ + arg_index_for_IArgStackLocation(src));
  }
  else
    ShouldNotReachHere();
  sp_offset *= oopSize;
  if (isRegister(dest)) {
    a.lwz(dest, sp_offset, NumberOperand, sp);
  } 
  else {
    a.lwz(Temp1, sp_offset, NumberOperand, sp);
    move(dest, Temp1);
  }
}

  
void CodeGen::storeSaved(Location         dst,    Location sp,
                         compiled_vframe* dst_vf, Location value) {
  fint frameSz = dst_vf->code->frameSize();
  assert(isRegister(sp), "must be a register");
  assert(is_StackLocation(dst), ""); // all up-level stored locs are stack-locations (mach stack, not expr stack)
  // dst is a stack location, allocated after linkage area and outgoing args
  fint sp_offset =  stackLocation_offset(index_for_StackLocation(dst), dst_vf->code->max_no_of_outgoing_args_and_rcvr());
  sp_offset *= oopSize;
  assert(sp != R0, "messed up register assignment");
  Location t = moveToReg(value, R0);
  a.stw(t, sp_offset, NumberOperand, sp);
}

#ifdef UNUSED
void CodeGen::loadSender(Location dest, Location sp) {
  // load frame's sender
  a.lwz(dest, saved_sp_offset * oopSize, NumberOperand, sp);
}
#endif

  
void CodeGen::lookup(Location dest, realSlotRef* path, Location receiver) {
  // <loadPath dest, path, receiver>
  // load receiver/dest/t, offset - Mem_Tag, dest/t
  // <move t, dest>
  
  // Since called from CodeGen::loadPath, cannot clobber Temp2
    
  Location t;
  if (isRegister(dest)) {
    t = dest;
  } else {
    t = Temp1;
  }
  if (path->holder->is_object_or_map()) {
    Location t1 = loadPath(t, path->holder, receiver, Temp1);
    a.lwz(t, smiOop(path->desc->data)->byte_count() - Mem_Tag, NumberOperand, t1);
    // load data slot
  } else {
    fatal("don't support vframe lookups");
  }
  move(dest, t);
}
  
      
static int32 nrOfRestarts = 0;


// load NLR registers
void CodeGen::prepareNLR(Location result, Location scope, smi homeID) {
  // <move scope, NLRHomeReg>
  // <loadImmediate homeID, NLRHomeIDReg>
  // <move result, NLRResultReg>
  assert(scope != NLRResultReg, "wrong reg alloc (scope is always Temp1)");
  move(NLRResultReg, result);
  move(NLRHomeReg, scope);
  loadImmediate(NLRHomeIDReg, homeID);
}

void CodeGen::testAndContinueNLR(smi homeID) {
  Label cont(a.printing);
  Label doNLR(a.printing);
  if (homeID) {       // note: will be 0 if no inlining
    a.Untested("CodeGen::testAndContinueNLR1", Temp1, true); // need SIC to test this
    if (fits_within_ui(homeID) ) {
      a.Untested("CodeGen::testAndContinueNLR2", Temp1, true);  // need SIC to test this
      a.cmplwi( NLRHomeIDReg, homeID, NumberOperand);
    } 
    else {
      a.Untested("CodeGen::testAndContinueNLR3", Temp1, true);  // need SIC to test this
      loadImmediate(Temp1, homeID);
      a.cmplw( NLRHomeIDReg, Temp1);
    }
    a.bne(doNLR);
  }
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      a.andi_(R0, NLRHomeReg, 3, NumberOperand);
      Label ok(a.printing);
      a.beq(ok, predict_weird);
      a.Untested("OOPS: bad NLRHOME", Temp1, true); // need SIC to test this
      ok.define();
    }
  # endif
  a.cmpw(SP, NLRHomeReg);
  a.beq(cont);
  doNLR.define();
  continueNonLocalReturn();
  cont.define();
  epilogue(NLRResultReg);
}

// continue NLR (return through caller's inline cache)
void CodeGen::continueNonLocalReturn() {
  assert(haveStackFrame, "should have stack frame");
  restore_frame_and_return(sendDesc::non_local_return_offset);
}

void CodeGen::zapBlock(Location block, bool memoized) {
  // <move block, t>
  // *if memoized
  //    subcc block/t, 0, g0
  //    bne,a _done           // do work in delay slot
  // *end
  // store block/t, scopeOffset, G0
  // _done: ...
  
  Location t;
  a.Comment("zap block");
  if (isRegister(block)) {
    t = block;
  } else {
    assert(Temp1 != block, "");
    t = Temp1;
  }
  move(t, block, memoized); // set cc if memoized: was block generated?
  Label done;
  if (memoized) {
    a.beq(done, predict_weird);     // br if no block, otherwize zap, the no block case probably likely
  }
  a.li(R0, 0, NumberOperand);
  a.stw(R0, scope_offset(), NumberOperand, t);    // zap the block
  done.define();
}
  
Label* CodeGen::SendDesc(RegisterState* s, LookupType lookupType,
                         oop selector, oop delegatee) {
  Label past_send_desc(a.printing);
  a.b(past_send_desc, NumberOperand);
  s->genMask();                               // mask of used regs
  Label* l = new Label(a.printing);
  a.b(*l, NumberOperand);                      // non-local return code
  a.Zero();   // nmlns
  a.Zero();
  if (selector != badOop) {
    if (isPerformLookupType(lookupType)) {
      assert_smi(selector, "should be an integer argcount");
      a.Data(smiOop(selector)->value());      // really arg count
    } else {
      assert_string(selector, "should be a string constant");
      a.Data(selector);                       // constant selector
    }
  }
  
#   ifdef SIC_COMPILER
  if (theCompiler->containsLoop) {
    // need counters for the sends to know how often the loop executes
    a.Data(withCountBits(lookupType, Counting));
  } 
  else {
    a.Data(lookupType);
  }
#   else
    a.Data(lookupType);
#   endif
  
#   if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions)
    switch (lookupType) {
     case DirectedResendLookupType:
      assert(lookupType & DelegateeStaticBit, "should have static delegatee");
      assert_string(delegatee, "should be a string");
      // fall through
     case ImplicitSelfLookupType:
     case ResendLookupType:
     case StaticNormalLookupType:
     case NormalLookupType:
      assert(!isPerformLookupType(lookupType),
             "should have a static selector");
      assert_string(selector, "should be a string");
      break;
     default: break;
    }
#   endif
  if (delegatee != badOop) {
    assert(needsDelegatee(lookupType), "shouldn't have a delegatee");
    a.Data(delegatee);
  }
  past_send_desc.define();
  return l;
}

Label* CodeGen::branch() {
  Label* L = new Label(a.printing);
  a.b(*L, NumberOperand);
  return L;
}


Label* CodeGen::unconditionalBranchCode( Label* dst,  
                                         bool allowPreemption, 
                                         RegisterState* s) {
  //    if allowPreemption <cPrimCall interruptCheck>
  //    bra,a dst
  
  // (this code lifted from what used to be restart)
  
  Label* nlr = NULL;
  if ( !allowPreemption) { 
    nlr = NULL;
  } 
  else {
    Label* dst1 = NULL;
    testStackOverflowForLoop(dst1, nlr, s); // allocs label and sets dst1
    dst->unify( dst1 );
  }
  a.b(*dst, NumberOperand);
  return nlr;
}


Label* CodeGen::conditionalBranchCode( Location testMe, 
                                       oop target, 
                                       Label* dst, 
                                       bool allowPreemption,  
                                       RegisterState* s) {
  //   <loadOop target_oop t1>
  //   <move    testMe     t2>
  //   cmp  t1, t2
  //   if no allowPreemption:
  //     beq  dst
  //     nop
  //   if allowPreemption:
  //     bne  end
  //     <unconditionalBranch dst>
  //     end:
   
  loadOop( Temp1, target );
  move( Temp2, testMe, false);
  a.cmplw(Temp1, Temp2);

  if ( !allowPreemption ) {
    a.beq(*dst);
    return NULL; // no nlr
  }
  Label end;
  a.bne(end);
  Label* nlr = unconditionalBranchCode( dst, true, s);
  end.define();
  return nlr;
}


Label* CodeGen::indexedBranchCode( Location        testMe, 
                                   LabelList*      labels,
                                   bool            allowPreemption,
                                   RegisterState*  s) {
  // if allowPreemption
  //   <testStackOverflowForLoop( afterTest ) >
  // afterTest:
  // 
  //   <move testMe, t1>
  //   <loadOop smiOop of label length, t2>
  //   tcmp t2, t1
  //   bvs end
  //   nop
  //   bleu end
  //   nop
  //   call .+8
  //   add t1, o7, t1
  //   jump t1, 12, g0
  //    bra L1
  //    bra L2
  //    ...
  // end:
  
  Label* nlr= NULL;
  if (allowPreemption) {
    // Ideally would only do this in the arms that actually do
    // branch back.
    // Beware: the stack overflow test could call other code that
    // could clobber Temp1 and Temp2, so it cannot be in the
    // middle somewhere. -- dmu
    Label* afterTest = NULL;
    testStackOverflowForLoop( afterTest, nlr, s ); // sets afterTest
    afterTest->define();
  }
  
  move( Temp1, testMe);
  a.andi_(R0, Temp1, Mem_Tag, NumberOperand); // extract tag
  Label end;
  a.bne( end ); // goto end if not int
  loadOop( Temp2, as_smiOop(labels->length()));
  a.cmplw(Temp2, Temp1);   // use unsigned comp to catch negative number
  a.ble(end); // goto end if index is out of bounds
  
  Label L(a.printing);
  a.bl(L, NumberOperand); // get pc of next inst into link
  pc_t link_reg_value= a.addr();
  L.define();
  
  const int32 indexShift = 2 - Tag_Size;
  assert(indexShift == 0, "no shift needed");
  a.mflr(R0);
  a.add(Temp1, R0, Temp1); // and displacement to link
  const int32 bytes_from_link_value_to_jumps = 20;
  a.addi(Temp1, Temp1, bytes_from_link_value_to_jumps, NumberOperand); // add in bytes from link setting to first jump
  a.mtlr(Temp1);
  a.balwlr();
  pc_t start_of_jumps= a.addr();
  assert( bytes_from_link_value_to_jumps == start_of_jumps - link_reg_value, "recount");

  for (fint i = 0;  i < labels->length(); ++i) {
    a.b(*labels->nth(i), NumberOperand);
  }
  end.define();
  return nlr;
}


void CodeGen::loadImmediate(Location dest, int32 value) {
  // *if p is 0...
  //   <move g0, dest>
  // *else...
  //   *if p is small...
  //      or g0, value, t/dest
  //   *else...
  //      sethi value, t/dest
  //      add t/dest, value, t/dest
  //   *end
  //   <move t, dest>
  // *end
  
  Location t = isRegister(dest) ? dest : Temp1;
  a.load_immediate(t, value, NumberOperand);
  move(dest, t);
}

// -1 = recvr, 0 = first arg
// move data from "from" to outgoing arg location for argNo
void CodeGen::loadArg(fint argNo, Location from, bool isPrimCall) {
  Unused(isPrimCall);
  move( argNo == -1  ?  ReceiverReg  :  ArgLocation(argNo),
        from);
}

void CodeGen::loadOop(Location dest, Location src_register, slotDesc* s) {
  // must be able to cope with loading into R0
  Location t = isRegister(dest) ? dest : Temp1;
  int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
  a.lwz(t, offset, NumberOperand, src_register);
  if (dest != t) {
    move(dest, t);
  }
}

void CodeGen::loadOop(Location dest, oop p) {
  // must be able to cope with loading into R0
  Location t = isRegister(dest) ? dest : Temp1;
  a.load_immediate(t, (int32)p, OopOperand);
  move(dest, t);
}


void CodeGen::loadBlockOop(Location dest, slotsOop p, RegisterState* s){
  //   <loadOop p, rr>
  //   <move sp, arg1>
  //   <cPrimCall BlockClone>
  //   <move rr, dest>
  
  a.Comment("Begin Block Cloning");
  s->allocateArgs(1, true);                   // track args for frame construction    
  loadOop(CReceiverReg, p);                   // load block to clone
  move(Arg1, SP);                             // load frame oop for block
  PrimDesc* pd = blockClone();
  assert(! pd->needsNLRCode(), "rewrite this - must backpatch NLR code");
  Label* l = cPrimCall(pd, s, false, true, 2 /* 1 arg + rcvr */);
  assert(l == NULL, "shouldn't need a label");
  move(dest, ResultReg);
}


void CodeGen::nonLifoTrap(RegisterState* s) {
  static PrimDesc* non_lifo_abort = NULL;
  if (non_lifo_abort == NULL)
    non_lifo_abort = getPrimDescOfFunction(
                       fntype(&NLRSupport::non_lifo_abort), 
                       true);
  
  Label next(a.printing);
  a.bl(next, NumberOperand); // prim needs some PC in this method
  next.define();
  a.mflr(CReceiverReg);
  Label* nlr_dest = cPrimCall(non_lifo_abort, s, true, true, 1);
  assert(nlr_dest == NULL, "should not need a label");
}


void CodeGen::initialize_for_platform() {
  // only used for instructions that flush arguments
  frame_size_patchees     = new AddressList(1000);
  frame_size_neg_patchees = new AddressList(1000);
  lmw_patchees            = new AddressList(1000);
  outgoing_arg_patchees   = new AddressList(1000);
  
  _max_no_of_outgoing_args_and_rcvr        = 0;
  _number_of_saved_nonvolatile_registers   = 0;
  _number_of_memory_locals                 = 0;
  _need_to_save_args_on_stack              = false;
  _includes_call_to_untrusted_C            = false;  

}

  

void CodeGen::assignmentCode(realSlotRef* dataRef) {
  prologue(true, 0 );
  move(Arg2, ReceiverReg); // GAK: need to return receiver, not parent object holding data slot
  assignment(ReceiverReg, dataRef, ArgLocation(0));
  epilogue(Arg2);
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "aCompiler_ppc.hh"
# include "_aCompiler_ppc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)



# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "registerLocator_ppc.hh"

# include "_registerLocator_ppc.cpp.incl"


void RegisterLocator::update_addresses_from_self_frame() {
  // I may be the frame that was running when the signal hit.
  // If so, use addresses of regs saved in Unix signal info.
  // -- dmu 1/04
  if ( update_addresses_from_InterruptedContext(my_frame, InterruptedContext::the_interrupted_context) ) {
    # if GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging)  setHow = 5;
    # endif
  }
  else if ( update_addresses_from_InterruptedContext(my_frame, &AbortContext) ) {
    # if GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging)  setHow = 6;
    # endif
  }
  if (my_frame->is_self_stub_frame()) 
    return; // CacheStubs and CountStubs do not muck with the nonvol regs -- dmu 1/04

  nmethod* nm = my_frame->code();
  update_addresses_from_callee_of(my_frame->sender(), nm->number_of_saved_nonvolatile_registers());
  # if GENERATE_DEBUGGING_AIDS
    if (SpendTimeForDebugging)  setHow += 10;
  # endif
}


void RegisterLocator::update_addresses_from_VM_frame() {
  char* r = my_frame->real_return_addr();
  if (isNonVolSavingReturnAddress(r)) {
    // special asm funcs that saves all NVs
    update_addresses_from_callee_of(my_frame->sender(),  NumLocalNonVolRegisters);
    # if GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging)  setHow = 2;
    # endif
  }
  else if ( update_addresses_from_InterruptedContext(my_frame, InterruptedContext::the_interrupted_context) ) {
    # if GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging)  setHow = 3;
    # endif
  }
  else if ( update_addresses_from_InterruptedContext(my_frame, &AbortContext) ) {
    # if GENERATE_DEBUGGING_AIDS
      if (SpendTimeForDebugging)  setHow = 4;
    # endif
  }
  else
    forget_everything();
}


RegisterLocator* RegisterLocator::for_NonVolSaving_frame(frame* f) {
  assert(isNonVolSavingReturnAddress(f->real_return_addr()), "");
  RegisterLocator* r = new RegisterLocator(false);
  r->my_frame = f;
  r->update_addresses_from_VM_frame();
  r->my_frame = f->sender(); // because this RL is for reg values while sender frame is running
  return r;
}


void RegisterLocator::update_addresses_from_callee_of(frame* f, fint callee_save_count) {
  // sender() because this frame saves regs based on INCOMING SP
  oop*  last_addr = f-> my_callees_last_saved_nonvolatile_register_addr();
  oop* first_addr = last_addr - (callee_save_count - 1);
  fint r = HighestNonVolReg;
  for ( oop* p = last_addr;  p >= first_addr;  --p, --r )
    set_address((Location)r, p);
}


bool RegisterLocator::update_addresses_from_InterruptedContext(frame* f, InterruptedContext* ic) {
  if (ic->sp() == f) {
    oop* p = &ic->R0_addr()[LowestLocalNonVolReg - R0];
    for ( fint r = LowestLocalNonVolReg;  r <= HighestNonVolReg;  ++r, ++p )
      set_address(Location(r), p);
    return true;
  }
  return false;
}


RegisterLocator* RegisterLocator::for_frame(frame* f) {
  frame* dbg_frame;
  Stack* dbg_stack;
  if (SpendTimeForDebugging) {
    dbg_stack = f->my_stack();
    dbg_frame = dbg_stack->first_VM_frame();
  }
  RegisterLocator* r;
  frame* dbg_self_frame = f->my_stack()->last_self_frame(true, &r);
  assert(dbg_self_frame, "no self frame");
  return r->climb_to_frame(f);
}
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "frame_format_ppc.hh"

# include "_frame_format_ppc.cpp.incl"


// return offset (in bytes) off of sp
int32 spOffset(Location loc,  fint frameSize, fint max_no_of_outgoing_args_and_rcvr) {
  int32 nWords;
  if (is_IArgStackLocation(loc)) {
    // incoming arguments are on my caller's stack frame, so add frameSize
    nWords = frameSize +
             rcvr_and_argument_offset(loc - IArgStackLocations + 1 /* rcvr */);
  }
  else if (is_ArgStackLocation(loc)) {
    nWords = rcvr_and_argument_offset(loc - ArgStackLocations + 1 /* rcvr */);
  }
  else if (is_StackLocation(loc)) {
    nWords = stackLocation_offset(index_for_StackLocation(loc), max_no_of_outgoing_args_and_rcvr);
  }
  else if (isIArgRegister(loc)) {
    nWords = frameSize + rcvr_and_argument_offset(IReceiverReg - loc);
  }
  else if (loc == PerformSelectorLoc)  return PerformSelectorLoc_sp_offset;
  else if (loc == PerformDelegateeLoc) return PerformDelegateeLoc_sp_offset;
  else
    fatal1("spOffset: don't know how to flush location %s\n", locationName(loc));

  return oopSize * nWords;
}
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

// # pragma implementation "vframe_ppc.hh"
# include "_vframe_ppc.cpp.incl"

    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


oop* dummy_vframe::register_contents_addr(Location r) {
  fint i = incoming_arg_index(r);
  assert(SaveOutgoingArgumentsOfPatchedFrames, "PPC needs this");
  assert(i + 1  <  OutgoingArgsOfReturnTrapOrRecompileFrame->length(), "bounds");
  oop* argp = OutgoingArgsOfReturnTrapOrRecompileFrame->objs(i + 1);
  assert((*argp)->verify(), "checking arg");
  return argp;
}


int32 compiled_vframe::register_offset(Location r) {
  return spOffset(r, code->frameSize(), code->max_no_of_outgoing_args_and_rcvr()) / oopSize; 
}


# if defined(SIC_COMPILER)
oop* compiled_vframe::special_register_contents_addr(Location r) {
  return NULL; // for SPARC
}
# endif


oop* compiled_vframe::register_contents_addr(Location r) {
  return (oop*) fr->location_addr(r, rl);
}


oop* compiled_vframe::register_contents_secondary_addr(Location r) {
  return (oop*) fr->location_secondary_addr(r, rl);
}
  
  
void compiled_vframe::print_code(fint curFrame) {
  
  lprintf("#%ld", (long) curFrame);
  if (!WizardMode) return;

  lprintf(" <%#lx%c@ %#lx%c%s # %ld",
         (long unsigned)fr,
         fr->return_addr() != fr->real_return_addr() ? '*' : ' ',
         (long unsigned)code,
         code->isInvalid() ? '!' : ' ',
         code->isDebug()
           ? "deb" : VMString[code->compiler()]->copy_null_terminated(),
         long(code->scopes->offsetTo(desc)));

  lprintf(">");
}


void compiled_vframe::fix_frame(frame* ) { } // for sparc


void compiled_vframe::get_search_locations_for_liveness_check(NameDesc*, frame*& fr_to_search, RegisterLocator*& rl_to_search) {
  fr_to_search = fr;
  rl_to_search = rl;
}


void compiled_vframe::copy_outgoing_arg(fint argNo, NameDesc* nd2, compiled_vframe* vf, dummy_vframe* dummy, 
                                        NameDesc* nd, frame* oldBlkHome, OopOopTable* blkValues) {

  // we're restarting a send (e.g. after recompilation that left most
  // recent frame invalid); outgoing args were saved by assembly glue

  // On PPC, ReturnTrap asm glue cannot save the outgoing argument,
  // because callee put it in a nonvolatile register. Then when
  // the callee returned, it restored the register, losing the value we want.
  //
  // For now, rely on the fact that PPC is NIC-only, and NIC always has
  // real locations for expression stack values.
  // In future, what? Save values in kill activations primitive?
  // Trap the return one level down??
  // -- dmu 6/99
  
  // Used to be:
  // if (!nd2->isIllegal())
  //   copyValue(nd, vf, nd2, oldBlkHome, blkValues); // get value from caller if possible
  // else
  //   copyValueTo(nd, new_string("<unknown outgoing argument>")); 

  Location loc= LocationOfSavedOutgoingArgInSendee(argNo);
  NameDesc* fromNd = new LocationNameDesc(loc, 0);
  if (!nd2->isIllegal()) {
    // verify that args were saved correctly
    oop val1 = vf->get_contents(nd2, false);
    oop val2 = dummy->get_contents(fromNd);
    if (val1 != val2)
      fatal3("inconsistent outgoing arg %d: %#lx vs. %#lx",
             argNo, val1, val2);
  }
  copyValue(nd, dummy, fromNd, oldBlkHome, blkValues);
}


compiled_vframe* compiled_vframe::sendeeOrNULL_for_get_expr_stack() {
  // Cannot find a register locator when this makes a dummy_vframe, so live
  // with less info for recompilation. -- dmu 2/03
  return NULL;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation  "runtime_ppc.hh"

# include "_runtime_ppc.cpp.incl"


bool check_saved_byte_map_base() {
  // nothing for PPC
  return true;
}


// cannot used simple DiscardStack (as in runtime_sparc.s) because of restoring
// nonvol registers
void volatile DiscardStack() {
  currentProcess->abort();
}


char* adjust_initial_SP(char* init_SP) {
  // don't need to adjust; asm glue (EnterSelf, I think) does it!
  return init_SP;
}



// define byte_map_base if not in a register

char* byte_map_base; // also use a register, too


// define SPLimit if not kept in a register:

char* SPLimit = NULL;
extern "C" {void set_SPLimitReg(char*); }
void  setSPLimit(char* m)        { 
  SPLimit = m; 
  if (UseSPLimitReg)
    set_SPLimitReg(m);
}
char* currentSPLimit()           { return SPLimit; }

// some machines do not need the following in asm:
extern "C" oop breakpoint_prim(oop rcvr) { 
  return rcvr; 
}


// This should be an assembly function, similar to sparc/runtime/runtime_asm_gcc_sparc.s.
// I haven't implemented it yet, so I just put the stub here. -mabdelmalek 10/02.
void  HandleUncommonTrap()
{
  fatal("uncommon traps unimplemented on PPC");
}


bool isNonVolSavingReturnAddress(char* r) {
  return   r == (char*)&SaveSelfNonVolRegs_returnPC
      ||   r == (char*)  &SendMessage_stub_returnPC
      ||   r == (char*)&SendDIMessage_stub_returnPC
      ||   r == (char*)        &ReturnTrap_returnPC
      ||   r == (char*)     &ReturnTrapNLR_returnPC
      ||   r == (char*)    &Recompile_stub_returnPC 
      ||   r == (char*)      &MakeOld_stub_returnPC;
}

fint volatile_register_sp_or_fp_offset(char* r) {
  return
           r == (char*)  &SendMessage_stub_returnPC  ?    SendMessage_stub_volatile_register_sp_offset
        :  r == (char*)&SendDIMessage_stub_returnPC  ?  SendDIMessage_stub_volatile_register_sp_offset
        :  r == (char*)    &Recompile_stub_returnPC  ?      Recompile_stub_volatile_register_sp_offset
        :  r == (char*)      &MakeOld_stub_returnPC  ?        MakeOld_stub_volatile_register_sp_offset
        :  r == (char*)&SaveSelfNonVolRegs_returnPC  ?  SaveSelfNonVolRegs_volatile_register_fp_offset
        :  0;
}

 
// Duplicated in runtime_asm_gcc.s:
fint ReturnTrap_frame_size = roundTo(linkage_area_size + HandleReturnTrap_arg_count + NumLocalNonVolRegisters, frame_word_alignment);

// Not implemented on PPC:
char* DIRecompile_stub_returnPC = NULL;

oop failure_oop_for_restarting_uncommon_prim() {
  assert(SaveOutgoingArgumentsOfPatchedFrames, "PPC needs this");
  warning("untested: failure_oop_for_restarting_uncommon_prim");
  return OutgoingArgsOfReturnTrapOrRecompileFrame->obj_at(0);
}


void fillRegisterValue(Location loc, oop b) {
  warning("untested");
  OutgoingArgsOfReturnTrapOrRecompileFrame->obj_at_put(loc - ReceiverReg, b);
}

void set_flags_for_platform() {
  FastMapTest                          = false;  lprintf("for PPC:  FastMapTest = false\n");
  LogVMMessages                        = true;   lprintf("for PPC:  LogVMMessages = true\n");
  PrintScriptName                      = true;   lprintf("for PPC:  PrintScriptName  = true\n");
  Inline                               = true;   lprintf("for PPC:  Inline = true\n");
  SICDeferUncommonBranches             = false;  lprintf("for PPC:  SICDeferUncommonBranches = false\n");
  SICReplaceOnStack                    = false;  lprintf("for PPC:  SICReplaceOnStack = false\n");
  SaveOutgoingArgumentsOfPatchedFrames = true;   lprintf("for PPC:  SaveOutgoingArgumentsOfPatchedFrames = true\n");
}


# include <sys/sysctl.h>

// call haveAltiVec() instead of me
bool slow_haveAltiVec() {
  int selectors[2] = { CTL_HW, HW_VECTORUNIT };
  int hasVectorUnit = 0;
  size_t length = sizeof(hasVectorUnit);
  int error = sysctl(selectors, 2, &hasVectorUnit, &length, NULL, 0);
  return  0 == error  &&  hasVectorUnit != 0;
}
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.6 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "uncommonBranch_ppc.hh"

# include "_uncommonBranch_ppc.cpp.incl" 


# if defined(SIC_COMPILER)

bool shouldRestartSend(int32* instp) {
  Unused(instp);
  fatal("unimp mac");
  return false;
}

unsigned trapCount(int32* instp) {
  Unused(instp);
  fatal("unimp mac");
  return 0;
}

void setTrapCount(int32* instp, unsigned count) {
  Unused(instp); Unused(count);
  fatal("unimp mac");
}

# endif


bool isMapLoad(int* instp) {
  return is_lwz(*(inst_t*)instp) && SI(*(inst_t*)instp) == map_offset();
}


void handleMapLoadTrap(InterruptedContext* c) {
  Unused(c);
  fatal("unimp mac");
}
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "frame_iterator_ppc.hh"

# include "_frame_iterator_ppc.cpp.incl"


void FrameIterator::do_vm_frame() {
  // Can ignore nonvols here because register locators ensure that
  // they are accessed when visiting the Self frame that uses them.
  // -- dmu 2/03
  assert(!processSemaphore, "oopClosure will not be called on resultOop in HandleReturnTrap");
  // SendMessage/SendDIMessage frame saves the args to be given to the send in ITS frame
  fint offset = volatile_register_sp_or_fp_offset(f->real_return_addr());
  if (offset != 0)
    do_saved_volatile_outgoing_argument_registers(offset);
}


void FrameIterator::do_saved_volatile_outgoing_argument_registers(fint vol_reg_sp_or_fp_offset) {
  if (!SaveOutgoingArgumentsOfPatchedFrames) {
    return;
  }
  fint nargs = f->sender()->outgoing_arg_count(f);
  if (nargs == -1)
    return; // internal prim, don't know how many args, will not be using them, so don't scavenge, don't zap
  assert(nargs >= 0, "just checking");
  fint num_reg_args_and_rcvr = 1 /* rcvr */ + min(nargs, NumArgRegisters);
  oop* p = f->location_addr_of_incoming_argument_register(ReceiverReg, vol_reg_sp_or_fp_offset);
  fint i = 0;
  for ( ;  i < num_reg_args_and_rcvr;  ++i)
    oop_closure->do_oop(&p[i]);
  if (zap)
    for ( ; i < NumRcvrAndArgRegisters;  ++i)
      p[i] = badOop; // zap dead vol regs
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
  void FrameIterator::do_compiled() {
    nm = f->code();

    do_saved_nonvolatile_registers();
    do_incoming_arguments();
    do_memory_locals();
    do_patched_frame_saved_outgoing_args();
  }
  
  
  void FrameIterator::do_saved_nonvolatile_registers() {
    assert(rl->fr() == f, "");
    fint nsaved = nm->number_of_saved_nonvolatile_registers();
    Location first_saved_reg = Location(HighestNonVolReg + 1 - nsaved);
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions) {
        for (fint i = 0;  i < first_saved_reg;  ++i)  {
          if (isSet(mask, i))
            fatal3("bit %d in mask 0x%x is set, send desc = 0x%x", i, mask, f->send_desc());
        }
      }
    # endif
    for (Location r = first_saved_reg;  r <= HighestNonVolReg;  r = Location(r + 1)) {
      oop* p = rl->address_of(r);
      assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, fint(r)),
            "overwriting patched_frame_saved_outgoing_args");
           if ( isSet(mask, r) )  oop_closure->do_oop(p);
      else if ( zap            )  *p = badOop;
    }
  }

  
  void FrameIterator::do_incoming_arguments() {
    // Cannot get arg count from method because might be in midst of scavenge/gc
    fint num_args_and_rcvr = nm->incoming_arg_count() + 1;
    oop* rcvr_and_arg_base = ((oop*)f) + nm->frameSize() + rcvr_and_argument_offset(0);

    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  f->is_patched()) {
        for ( fint i = 0;  i < num_args_and_rcvr;  ++i )
            assert(check_for_overwriting_patched_frame_saved_outgoing_args(&rcvr_and_arg_base[i], i),
                   "overwriting patched_frame_saved_outgoing_args");
      }
    # endif

    // Zap allocated but unused words in frame for incoming args
    if ( zap  &&  !nm->are_register_arguments_saved_on_stack() ) {
      fint num_to_zap = min(num_args_and_rcvr,  NumRcvrAndArgRegisters);
      for (fint i = 0;  i < num_to_zap;  ++i)
        rcvr_and_arg_base[i] = badOop;
    }
    // Visit saved args
    fint first_saved_arg =  nm->are_register_arguments_saved_on_stack() 
                              ? 0  // all saved
                              : NumRcvrAndArgRegisters;  
    for ( fint i = first_saved_arg;  i < num_args_and_rcvr;  ++i )
      oop_closure->do_oop(&rcvr_and_arg_base[i]);
    // Note: unsaved args do not need visiting, because they are moved to
    // nonvols in prologue, which were visited in above routine.
  }
  
  
  void FrameIterator::do_memory_locals() {
    fint nlocals = nm->number_of_memory_locals();
    oop* start = ((oop*)f) + stackLocation_offset(0, nm->max_no_of_outgoing_args_and_rcvr());
    oop* p = start;
    for ( fint i = 0;   i < nlocals;   ++i, ++p) {
        assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, i),
               "overwriting patched_frame_saved_outgoing_args");
      oop_closure->do_oop(p);
    }
    if (zap) {
      oop* limit = ((oop*)f) + f->frame_size() - nm->number_of_saved_nonvolatile_registers() + last_extra_offset;
      for ( ; p < limit;  ++p)  {
        assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, p - start),
               "overwriting patched_frame_saved_outgoing_args");
        *p = badOop;
      }
    }
  }
  

  bool FrameIterator::check_for_overwriting_patched_frame_saved_outgoing_args(oop* p, fint idx) {
    if (!f->is_patched())                                         return true;
    if (p != (oop*)f->patched_frame_saved_outgoing_args_addr(nm)) return true;
    WizardMode = true;
    lprintf("****** about to die at index %d, frame = 0x%x, next_frame = 0x%x, frame size = 0x%x (%d)\n",
            idx, f, f->sender(), f->frame_size(), f->frame_size());
    if (nm != f->code()) {
      lprintf("****** nm (0x%x) != f->code() (0x%x)\n", nm, f->code());
      nm = f->code();
    }
    if (!GCInProgress && !ScavengeInProgress) { // cannot do these when the heap is all weird
      lprintf("nmethod of offending frame is:\n");
      nm->verify();
      nm->print(); 
      lprintf("\n\n\ncode: \n");
      nm->printCode();
    }
    return false;
  }

# endif // either compiler
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "framePieces_ppc.hh"

# include "_framePieces_ppc.cpp.incl"


oop* ppc_sp::locals() {
  return &as_oops()[linkage_area_end];
}


// cloned from sparc size

ppc_sp* ppc_sp::push_new_sp( char* pc,
                             fint size_in_oops,
                             bool zapAlways ) {
  assert((size_in_oops & 3) == 0, "");
  ppc_sp* sp = (ppc_sp*)( as_oops() - size_in_oops );
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      zapAlways = true;
    }
# endif
  if (zapAlways)
    // If errorObj starts showing up where it shouldn't, I recommend
    // substituting 0xfffffffe for it below: (dmu)
    set_oops(sp->as_oops(), size_in_oops, (oop)Memory->errorObj);
  sp->set_link(this);
  sp->set_return_addr(pc);
  return sp;
}
# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# include "_interruptedCtx_ppc.cpp.incl"

char* InterruptedContext::next_pc(){ return NULL; }
void InterruptedContext::set_next_pc(void *) {}


void InterruptedContext::set_continuation_address(char *addr, bool mustWork, bool setSema) {
  assert(!continuePC, "continuePC already set");
  if (setSema) processSemaphore = true; 
  Unused(mustWork);
  the_interrupted_context->set_pc(addr);  
}


bool InterruptedContext::in_system_trap() {
  if (scp == &dummy_scp)
    return false;
  // system call, sc, svca, top 5 bits = 17, bit 30 is a 1
  int c = code_at_pc();
  return (c & 0xf8000002) == 0x88000002;
}


int InterruptedContext::system_trap() {
  return NULL; // unimp for now
}


// used to be named patchMaxSP, but it seems to do what setupPreemption does,
// only with extra SPARC stuff and it's called within signal handling -- dmu 1/96

void InterruptedContext::setupPreemptionFromSignal() {
  // SPLimit is a global, not a register
  currentProcess->setupPreemption();
}

# if TARGET_OS_VERSION == MACOSX_VERSION
  // OS 9 version is in interruptedCtx_mac

  # if OSX_RELEASE == TIGER_RELEASE
    # define __ss ss // see /usr/include/mach/ppc/thread_status.h
    
    # define __srr0 srr0
    # define __srr1 srr1
    
    # define __r0 r0
    # define __r1 r1
    # define __lr lr
    # define __cr cr
    # define __xer xer
    # define __ctr ctr
  # endif

  char** InterruptedContext::pc_addr() {
    return (char**) &scp->uc_mcontext->__ss.__srr0; // see /usr/include/mach/ppc/thread_status.h
  }
  int* InterruptedContext::sp_addr() {
    return   (int*) &scp->uc_mcontext->__ss.__r1;
  }
  char** InterruptedContext::lr_addr() {
    return (char**) &scp->uc_mcontext->__ss.__lr;
  }
# endif

char* InterruptedContext::lr() {
  return is_set() ? *lr_addr() : NULL;
}
  
oop* InterruptedContext::R0_addr() {
  return (oop*) &scp->uc_mcontext->__ss.__r0;
}

void InterruptedContext::print_registers() {
  InterruptedContext* ic = 
    the_interrupted_context && the_interrupted_context->is_set()  ?  the_interrupted_context  :
   (AbortContext.is_set() ? &AbortContext : NULL);
   
  if (ic == NULL) {
    lprintf("context is not set\n");
    return;
  }
  ppc_thread_state_t* ssp = &ic->scp->uc_mcontext->__ss;
  lprintf("srr0 (pc) = 0x%x\n", ssp->__srr0);
  lprintf("srr1      = 0x%x\n", ssp->__srr1);
  lprintf("cr        = 0x%x\n", ssp->__cr  );
  lprintf("xer       = 0x%x\n", ssp->__xer );
  lprintf("lr        = 0x%x\n", ssp->__lr  );
  lprintf("ctr       = 0x%x\n", ssp->__ctr );

  lprintf("\n");
  unsigned int* p = &ssp->__r0;
  for (fint i = 0;  i <= 31; i += 4)
    lprintf("r%-2d = 0x%08x  r%-2d = 0x%08x  r%-2d = 0x%08x  r%-2d = 0x%08x\n",
            i, p[i],  i+1, p[i+1],  i+2, p[i+2],  i+3,  p[i+3]);
  lprintf("\n\n");
}

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.17 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "frame_ppc.hh"

# include "_frame_ppc.cpp.incl"


frame* frame::sender() { return my_sp()->link()->as_frame(); }
char**  frame::real_return_addr_addr() { return my_sp()->return_addr_addr(); }

char* frame::return_addr() {
  if (!InterruptedContext::the_interrupted_context->is_set())
    return platform_independent_return_addr();
    
  // Profiler uses this from interrupts, but last return address may not be right,
  // so return null.
  if ((frame*)InterruptedContext::the_interrupted_context->sp() == this)
    return NULL;
    
  // C compiler moves sp, but does not save pc for leaves
  if ( InterruptedContext::the_interrupted_context->sp()->sender()  ==  this
  &&   !Memory->code->contains(InterruptedContext::the_interrupted_context->pc()))
    return NULL;
    
  return platform_independent_return_addr();
}

  
// Return the address I was entered at, assuming I am a C++ frame

char* frame::c_entry_point() {
  frame* s = sender();
  if ( s == NULL ) return NULL;
  
  char* r = s->real_return_addr(); // where sender will return into
  
  if ( Memory->code->contains(r))   
    return NULL;
  if ( r == (char*)&SaveNVRet )   {
    // If I was called by a frame that returns into the SaveNV asm stub,
    // then I cannot get the entry point in the usual way, because it was passed into SaveNV
    // via a register. But SaveNV stores this incoming argument (arg5) into its caller's frame
    // following the standard PPC convention.
    return (char*)s->sender()->my_sp()->locals()[SaveNV_fn_arg_index]; 
  }   
  int32* callp = (int32*)r - 1;
  if (callp == NULL  ||  !isImmediateCall(callp)) return NULL;
  return  get_target_of_C_call_site(callp);
}

# pragma warn_unusedarg off

frame** frame::nmethod_frame_chain_addr(nmethod* nm) { 
  return (frame**)first_saved_nonvolatile_register_addr(nm) + nmethod_frame_chain_offset;
}


objVectorOop* frame::patched_frame_saved_outgoing_args_addr(nmethod* nm) { 
  if (nm == NULL)  nm = code();
  return (objVectorOop*)first_saved_nonvolatile_register_addr(nm) + patched_frame_saved_outgoing_args_offset;
}


char**  frame::currentPC_addr() { 
  //  The assert below does not work because it relies on return_address
  //   which checks to see if the real return address is == to
  //   the asm ReturnTrap routines. But if the frame has called the trap
  //   routine which has called C, the return address is in the midst
  //   of the asm routine, and so return address does not realize
  //   it is patched. -- dmu
  //  Also, there is a recursion problem with the assert.
  //  assert(is_compiled_self_frame(), "currentPC only in compiled frame");
  return (char**)this + current_pc_offset;
}
 
 
// this may not be a self frame
oop*  frame::location_addr_of_incoming_argument(Location r, RegisterLocator* rl) { 
  if (is_IArgStackLocation(r)) {
    // stored in sender's frame no matter what
    fint i = arg_index_for_IArgStackLocation(r);
    return &sender()->my_sp()->as_oops()[rcvr_and_argument_offset(i + 1 /* rcvr */)];
  }
  if (is_self_frame()) 
    return rl->address_of(r);
 return location_addr_of_incoming_argument_register(Location(ReceiverReg + IReceiverReg - r));
} 


oop*  frame::location_addr_of_incoming_argument_register(Location r, fint offset) { 
  assert(r == ReceiverReg || isArgRegister(r), "just checking");
  fint o = offset ? offset : volatile_register_sp_or_fp_offset(return_addr());
  if (o == 0)
    fatal1("unexplainable VM frame 0x%x", this);
 return &(o > 0  ?  this  :  sender())->my_sp()->as_oops()[o + r - ReceiverReg];
} 

 
void**  frame::location_addr(Location r, RegisterLocator* rl) { 
  nmethod* nm = code();
  if ( is_ArgStackLocation(r)
  ||   is_IArgStackLocation(r)
  ||   is_StackLocation(r)     )
    return  (void**) ((char*)my_sp() +
                      spOffset(r, nm->frameSize(), nm->max_no_of_outgoing_args_and_rcvr()));
    
  if (isLocalNonVolatileRegister(r)) {  
    if ( nm->are_register_arguments_saved_on_stack()  
         &&  R31 - r  <  nm->incoming_arg_register_count() + 1 /* for rcvr */ ) {
      // This location corresponds to an incoming argument, and
      // since the nic saves these (for methods with blocks) in the caller's frame,
      // we can find it there.
      return (void**)my_sp()->link()->as_oops() + receiver_stack_offset + (R31 - r);
    }
    if (rl != NULL) {
      assert(this == rl->fr(), "");
      return (void**) rl->address_of(r);
    }
    else {
      ResourceMark rm;
      return (void**) RegisterLocator::for_frame(this)->address_of(r);
    }
  }
  fatal("unexpected kind of location");
  return NULL;
}
 
 
// Nmethods that spawn blocks cache their incoming arguments in the 
// saved-argument region of their callers' frames so they can keep these
// values in non-vol regs (instead of memory) while the block accesses them
// via the saved-argument region. (These values are immutable and so can be duplicated.)
//
// But, when performing a Conversion (i.e. deoptimizing a frame) the value needs to
// be placed into both locations. Hence this function, that returns the "other"
// address of location (if there is one).
//
// This routine parallels location_addr above.

void**  frame::location_secondary_addr(Location r, RegisterLocator* rl) { 
  nmethod* nm = code();
  if (is_ArgStackLocation(r)  ||  is_StackLocation(r))
    return  NULL;
    
  if (is_IArgStackLocation(r))
    return  NULL;
    
  if (isLocalNonVolatileRegister(r)) {  
    if ( nm->are_register_arguments_saved_on_stack()  
         &&  R31 - r  <  nm->incoming_arg_register_count() + 1 /* for rcvr */ ) {
      // This location corresponds to an incoming argument, and
      // since the nic saves these (for methods with blocks) in the caller's frame,
      // we can find it there.
      // So, return the SECONDARY address:
      assert(rl != NULL, "cannot cons up rl during a conversion");
      assert(this == rl->fr(), "");
      return (void**) rl->address_of(r);
    }
    return NULL;
  }
  fatal("unexpected kind of location");
  return NULL;
}



// called from recompile.c:
// copy the receiver to the new place and adjust all block homes
// (the receiver must contain references to every live block belonging
// to this frame for zapping purposes)

void frame::copy_to( char* sp,
                     char* caller,
                     char* pc,
                     bool adjust) {

  frame* new_f = (frame*)sp;
  warning("untested frame::copy_to:"); // need SIC to test this

  if (adjust) {
    // make sure all memoized blocks exist, then adjust their scope
    abstract_vframe* callee = NULL;
    OopOopTable* dummy = EMPTY;
    for ( abstract_vframe* vf = new_vframe(this);
          vf  &&  vf->fr == this;
          callee = vf,  vf = vf->sender()) {
      vf->createBlocks(callee, dummy);
    }
    ResourceMark rm; // for RegisterLocators
    adjust_blocks(block_scope_of_home_frame(), new_f, RegisterLocator::for_frame(new_f) );
  }
  copy_oops( (oop*)this, (oop*)new_f, frame_size());

  my_sp()->set_link( ((frame*)caller)->my_sp() );
  set_return_addr( pc );
}
  

// adjust fp of copied frames so they don't refer to the original stack

void frame::adjust_frame_links_of_copied_frames( frame* last_frame_to_copy,
                                                 frame* first_copied_frame) {
  
  ppc_sp* osp =                     my_sp(); // this frame
  ppc_sp* nsp = first_copied_frame->my_sp(); // copied frame's this
  int32 diff = (char*)nsp - (char*)osp;
  
  while ( osp  <  last_frame_to_copy->my_sp() ) {

    nsp->adjust_link(diff);

    osp = osp->link();
    nsp = nsp->link();
  }
  // note last copied link should just be what it was so don't do anything
}

// end of copy() helpers


// From Sparc: Note that the lastFrame frame will never be returned to, so we can
// bash its registers.  But a dummy frame is needed to make the stack
// look right (to serve as the frame of the last sparc_sp).
// (used to be fixStack)


void frame::fix_frame(char* pc, char* sp) { 
  // Although conversion happens in VM process, 
  // currentProcess still says orig Self process.
  // So, unchain_frames which does Stack::first_VM_frame (sp?)
  // will go from current (conversion) frame looking for a self frame.
  // Thus, zap a conversion frame (which we never return from anyway)
  // to lead traversal back to self frame.
  assert(isOnVMStack(this), "will be overwritten");
  my_sp()->set_link((ppc_sp*)sp);
  set_real_return_addr(pc);
}
 
// PPC uses actual SP, since it is stable

frame* frame::home_frame_of_block_scope(frame* hint) { return this; }
frame* frame::block_scope_of_home_frame()            { return this; }

frame* frame::home_frame_of_vfo_locals(frame* hint)  { return this; }
frame* frame::vfo_locals_of_home_frame()             { return this; }


int32 frame::frame_size_of_uncopied_frame() { return (oop*)sender() - (oop*)this; }

char* frame::c_return_pc() {  
  // link reg points to next inst
  return return_addr(); 
}

frame* frame::make_full_frame(char* pc)            {  return this; }
frame* frame::make_full_frame_after_trap(char* pc) {  return this; }
frame* frame::make_full_frame_on_user_stack()      {  return this; }



void frame::print_compiled() {
  if (is_compiled_self_frame())
    lprintf(" chain = %#lx;", (nmethod_frame_chain(code())));
  lprintf("\n\tlocals = [%#lx, %#lx], currPC = %#lx, size = %ld words\n",
         (long unsigned)(my_sp()),
         (long unsigned)(currentPC()),
         long(frame_size()));
}

// 0 = first arg
// basically used for printing
oop frame::get_lookup_arg(fint index) { 
  return *sendee()->location_addr_of_incoming_argument(LocationOfSavedOutgoingArgInSendee(index), NULL);
}


void frame::printRegs() {
  fatal("unimp mac (debugging only, anyway, and cannot call code from mac debugger)");
}


void frame::printVerbose_on_this_platform() {
  fatal("unimp mac (debugging only, anyway, and cannot call code from mac debugger)");
}


// return addr of first (lowest) saved nonvol reg

// pass in nm for correctness;
// when compacting the zone, code() cannot work, since the invariants are broken:
// called by frame::nmethod_frame_chain_addr()
// called by frame::nmethod_frame_chain()
// called by frame::nmethod_moved_by()
// called by nmethod::moveTo_inner
// called by NCodeBase::moveTo
// called by moveInsts
// called by Heap::compact
oop*  frame::first_saved_nonvolatile_register_addr(nmethod* nm) {
  // cannot check is_compiled_frame here, no assert
  return ((oop*)sender()) - nm->number_of_saved_nonvolatile_registers();
}



sendDesc* frame::send_desc() { 
  return sendDesc::sendDesc_from_return_PC( return_addr());  // it is always the same on PPC
}



// used for scavenging, must return allocated location
//  for every interpreter state class that may exist
//  return IllegalLocation for non-interp frame

Location frame::location_of_interpreter_of_block_scope(void* entry_point) {
  return  entry_point == first_inst_addr(interpret)  
            ?  Location(ArgStackLocations + 0)  
            :  IllegalLocation;
}


frame* frame::get_patched_self_frame(char* sp_of_patched_frame) {
  if (Interpret)
    warning("next line may be wrong for interpreter, was currentFrame()->sender()");
  return (frame*)sp_of_patched_frame;
}


int32  frame::copy_through_oop_count(frame* last_frame_to_copy) {
  return (oop*)last_frame_to_copy + last_frame_to_copy->frame_size() - (oop*)this;
}


void frame::fix_current_return_address(char* ) { }

oop frame::perform_selector_of_SendMessage_stub_frame() {
  return  sender()->my_sp()->as_oops()[PerformSelectorLoc_sp_offset / oopSize];
}  

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "stubs_ppc.hh"
# include "_stubs_ppc.cpp.incl"

# pragma warn_unusedarg off


// asm routines (only needed for SIC:)

extern "C" {
  oop UncommonBranch(...) { fatal("unimp mac");  return NULL; }
  // Note: do not have a stack frame yet; link is link of caller
  // Also, they must preserve Temp1 and Temp2
  }

// Only needed for interpreter/compiler interoperation:
extern "C" {  oop ReturnResult_stub(...) { fatal("unimp mac");  return NULL; } }
oop ReturnResult_stub_result;

# endif // TARGET_ARCH == PPC_ARCH
# if  TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2006 Sun Microsystems, Inc. and Stanford University.
   See the LICENSE file for license information. */

# pragma implementation "conversion_ppc.hh"
# include "_conversion_ppc.cpp.incl"


  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void Conversion::fix_new_vfs() { }
  
  
  void Conversion::fixConversionStack_for_vframe_conversion() {
    fixConversionStack( (pc_t)&ReturnTrap_returnPC, (char*)rlFr->my_sp() );
  }


  bool Conversion::createFrame(fint i, nmethod *newNM) {    
    // create new frame, store it in newFr and initialize it
    // XXXX what if fr is interp frame?

    // this routine seems to use sd and sp,
    //  and to set sd, sp, and newFr

    // figure out new value for sd and if isInInterruptCheck

    sendDesc* prev_sd= vf[i]->fr->send_desc();

    // Have to be careful when converting a frame at the bottom of the
    // stack which has just returned from interruptCheck, after a process
    // switch, or while single stepping -- otherwise the next send is
    // omitted.  MIW 6/8/94

    bool isInInterruptCheck=
              prev_sd // eliminate uncommon branches
          && prev_sd->isPrimCall()
          && prev_sd->jump_addr() == first_inst_addr(interruptCheck);

    assert(vf[i]->fr->is_aligned(), "frame alignment check");
    sendDesc* sd_of_created_frame = newNM->sendDescFor(vf[i], isInInterruptCheck);	  
  
    assert( i == 1   ||  newVF[i-1],  "Mac has no null check");
    
    RegisterLocator* caller_rl_before_creating_frame = 
      i == 1
        ?  nonvols_for_caller // vf[0] could be null, so don't do vf[0]->reg_loc()
           // going to be smashing regs saved below last created frame, so copy them
        :  newVF[i-1]->reg_loc()->for_copied_frame(NULL);

    // ppc frames contain their own return addresses, so use sd_of_created_frame
    ppc_sp*   newSP = ((ppc_sp*)  sp)->push_new_sp( sd_of_created_frame->return_pc(),
                                                    newNM->frameSize(),
                                                    true );
    if (stk->isStackOverflow(newSP)) {
      fatal("stack overflow while converting stack frame");
    }
    sp = (char*)newSP;
    
    newFr = (frame*)newSP;
    rlFr  = (frame*)newSP->push_new_sp((char*)&ReturnTrap_returnPC,
                                       ReturnTrap_frame_size,
                                       true); // PPC needs frame for all saved regs
    newFrRl = RegisterLocator::for_NonVolSaving_frame(rlFr);    
    RegisterLocator* caller_rl_after_creating_frame  = newFrRl->sender();
    
    // make sure all saved non vol regs for higher frames
    // are saved in either this frame or the spoof SaveNonVolRegs frame
    for (fint i = LowestLocalNonVolReg;  i <= HighestNonVolReg;  ++i ) {
      oop* srcp = caller_rl_before_creating_frame->address_of(Location(i));
      oop* dstp = caller_rl_after_creating_frame ->address_of(Location(i));
      *dstp = *srcp;
    }
    sd = sd_of_created_frame;    
    return isInInterruptCheck;
  }


  frame* Conversion::fixConversionStack_for_returning_to_self( 
           char* self_sparc_fp_or_ppc_sp, 
           sendDesc* /*self_sd*/ ) {
    frame* returnTrap_frame = (frame*)((oop*)self_sparc_fp_or_ppc_sp -  ReturnTrap_frame_size);
    assert(returnTrap_frame->return_addr() == (pc_t)&ReturnTrap_returnPC, "");
    fixConversionStack( (pc_t)&ReturnTrap_returnPC, (char*)returnTrap_frame);
    
    return (frame*)self_sparc_fp_or_ppc_sp; // first self frame we are returning to
  }


  void Conversion::continue_after_return_trap_with_result( 
                     oop res, 
                     char* continuationPC, 
                     char* self_sparc_fp_or_ppc_sp ) {
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done with this                                           
    ContinueAfterReturnTrap(res, continuationPC, self_sparc_fp_or_ppc_sp);
  }
  
  
  oop Conversion::get_result() {
    fatal("Unimplemented"); return NULL;
  }
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == PPC_ARCH
/* Sun-$Revision: 30.19 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "itimer_unix.hh"
# include "_itimer_unix.cpp.incl"


extern "C" { void IntervalTimerTick(int sig, self_code_info_t *info, self_sig_context_t *scp); }


class TimerEntry: public AbstractTimerEntry {
 private:
  int32 divisor;
  int32 count;
  
 public:
 
  void initialize(doFn sp, doFn ap, int32 factor) {
    AbstractTimerEntry::initialize(sp, ap);
    factor   *= IntervalTimer::oversample_rate; // for profiler debugging
    divisor   = factor;
    count     = 0;
  }
  
  void do_procs_if_needed() {
    if ( is_ready_to_do_sync_fn || is_doing_sync_fn )
      return; // avoid races
      
    if (++count >= divisor) {
      count = 0;
      if (async_fn != (doFn)-1)  async_fn();
      if ( sync_fn != (doFn)-1)  is_ready_to_do_sync_fn = true;
    }
  }
};


#if TARGET_OS_VERSION != SOLARIS_VERSION && TARGET_OS_VERSION != MACOSX_VERSION && TARGET_OS_VERSION != LINUX_VERSION
extern "C" int setitimer(int which,
                         struct itimerval *value,
                         struct itimerval *ovalue);
#endif

// initing the timers

void IntervalTimer::init() {
  assert(Real_timer() == NULL,  "must only init itimers once");
  
  _Real_timer = new IntervalTimer( SIGALRM,   ITIMER_REAL);
   _CPU_timer = new IntervalTimer( SIGVTALRM, ITIMER_VIRTUAL);

  // For Carbon Windows under OSX: 
  // sync, not async to avoid races in event queue code
  // -- dmu 9/01
  Real_timer()->enroll_sync(check_events_freq, OS::check_events );
}


int32 IntervalTimer::ticks_per_second() { return 100; }


// constructors & destructors

IntervalTimer::IntervalTimer(int s, int t) {
  common_initialization();
  _t = new TimerEntry[number_of_entries];

 sig = s;
 timer = t;
}


IntervalTimer::~IntervalTimer() {}


// enabling/disabling a timer:


// cannot be done in initializer because signal stack does not exist yet

void IntervalTimer::enable() {
  if (dont_use_any_timer) return;                     // no timers wanted
  if (dont_use_real_timer && sig == SIGALRM) return;  // don't install real timer
  if (!check_and_pre_enable()) return;
  
  static struct itimerval dt;          // value for activating timer

  dt.it_value.tv_sec  = dt.it_interval.tv_sec  = 0;
  dt.it_value.tv_usec = dt.it_interval.tv_usec = 1000000 / ticks_per_second() / oversample_rate; 
  
  struct sigaction action;
# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION
  action.sa_sigaction = (void (*)(int, siginfo_t*, void*)) IntervalTimerTick;
  
# elif COMPILER != GCC_COMPILER  &&  TARGET_OS_VERSION == SUNOS_VERSION
  action.sa_handler = (void (*)()) IntervalTimerTick;
# else
  # error which?
# endif

  action.sa_flags   = SignalInterface::install_flags();
 
  sigfillset(&action.sa_mask);
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      sigdelset(&action.sa_mask, SIGINT);
      sigdelset(&action.sa_mask, SIGTSTP);
      sigdelset(&action.sa_mask, SIGTRAP);
    }
# endif
  sigdelset(&action.sa_mask, SIGILL);
  sigdelset(&action.sa_mask, SIGFPE);
  sigdelset(&action.sa_mask, SIGSEGV);

  if (sigaction( sig, &action, NULL) == -1) {
    perror("sigaction");
    fatal1("couldn't install signal handler for signal %ld", sig);
  }
  if (!sigismember(&action.sa_mask, sig) || !sigismember(&SignalInterface::sig_mask, sig))
    fatal1("should have masked %d", sig);
    
  if (setitimer(timer, &dt, NULL)) fatal("cannot start timer!");
  
  post_enable();
}


void IntervalTimer::disable(bool) {
  pre_disable();
  static struct itimerval dt0;         // to deactivate (all zero)
  dt0.it_value.tv_sec = dt0.it_value.tv_usec = 0;
  if (setitimer(timer, &dt0, NULL))
    fatal("cannot reset timer!");
  // in OS X: can get timer killing process, try SIG_IGN instead of SIG_DFL
  (void)signal( sig, SIG_IGN);
  post_disable();
}  


// enrolling/withdrawing tasks


void IntervalTimer::enroll_async(float freq, doFn fn) {
  TimerEntry* e = alloc_entry();
  int32 factor = (int32) rint(float(ticks_per_second()) / freq);
  e->initialize((doFn)-1, fn, factor);
}


void IntervalTimer::enroll_sync(float freq, doFn fn) {
  TimerEntry* e = alloc_entry();
  int32 factor = (int32) rint(float(ticks_per_second()) / freq);
  e->initialize(fn, (doFn)-1, factor);
}


void IntervalTimer::enroll_async_if_safe(float freq, doFn fn) {
  # if TARGET_OS_VERSION == LINUX_VERSION
    enroll_sync(freq, fn); // Fix spy crash
  # else
   enroll_async(freq, fn);
  # endif
}


void IntervalTimer::withdraw_entry(TimerEntry*) {}

void IntervalTimer::move_entry(TimerEntry* from, TimerEntry* to) { *to = *from; }


#define SIGNONE -1

void IntervalTimerTick(int sig, self_code_info_t *info, self_sig_context_t *scp) {
  // A Mac OS X application, ApplicationEnhancer, causes the VM to receive nested
  // SIGALRM/SIGVTALRM signals.
  // We don't know why this is happening, since our call to sigaction (where
  // the SIGALRM/SIGVTALRM handlers are installed) specified that we don't want
  // recursive timer signals.
  // As a workaround to this funny behaviour induced by ApplicationEnhancer, we
  // check if we're in a recursive SIGALRM/SIGVTALRM handler.  If we receive a
  // recursive SIGALRM/SIGVTALRM signal, then we immediately return from the
  // signal handler.  This is OK since it's not a catastrophe if we ignore a
  // timer signal. -mabdelmalek 10/02
  
  // Now we are getting SIGIO's while in the SIGVTALRM handler, maybe
  // caused by ApplicationEnhancer--argh! -- dmu 6/03
  
  if (InterruptedContext::the_interrupted_context->forwarded_to_self_thread(sig))
    return;
  
  if (SignalInterface::currentNonTimerSignal || SignalInterface::currentTimerSignal) {
    static bool haveWarned = false;
    if (!haveWarned) {
# if TARGET_OS_VERSION == LINUX_VERSION
      warning3("IntervalTimerTick: signal_handler cannot nest (only one interrupted context).\n"
               "Received timer sig %d while in sig %d or timer sig %d.\n"
               "MacOSX ApplicationEnhancer causes apps to get signals that should be blocked.",
               sig, 
               SignalInterface::currentNonTimerSignal,
               SignalInterface::currentTimerSignal);
# elif TARGET_OS_VERSION == SOLARIS_VERSION
      warning6("IntervalTimerTick: signal_handler cannot nest (only one interrupted context).\n"
               "Received timer sig %d (%s) while in sig %d (%s) or timer sig %d (%s).\n",
               sig, strsignal(sig),
               SignalInterface::currentNonTimerSignal, strsignal(SignalInterface::currentNonTimerSignal),
               SignalInterface::currentTimerSignal, strsignal(SignalInterface::currentTimerSignal));
# else
      warning6("IntervalTimerTick: signal_handler cannot nest (only one interrupted context).\n"
               "Received timer sig %d (sig%s) while in sig %d (sig%s) or timer sig %d (sig%s).\n"
               "MacOSX ApplicationEnhancer causes apps to get signals that should be blocked.",
               sig, sys_signame[sig],
               SignalInterface::currentNonTimerSignal, sys_signame[SignalInterface::currentNonTimerSignal],
               SignalInterface::currentTimerSignal, sys_signame[SignalInterface::currentTimerSignal]);
# endif
      haveWarned = true;
    }
    return;
  }           
  SignalInterface::currentTimerSignal = sig;       

  // this is called at every tick of our timer
  if (processSemaphore || IntervalTimer::dont_use_any_timer) {
    // We're in a critical region, so return from the timer interrupt 
    // without overwriting InterruptedContext::the_interrupted_context.
    // The check for dont_use_any_timer is for better debugging with gdb.

    SignalInterface::currentTimerSignal = 0;
    return;
  }
  
# if TARGET_OS_VERSION != MACOSX_VERSION
  // The following assertion fails on Mac OS X when ApplicationEnhancer is running.
  // Since it's not critical that we execute on a signal stack, we can skip
  // the assertion.  -mabdelmalek 10/02
  assert(SignalInterface::is_on_signal_stack(), "should be on interrupt stack");
# endif
  assert(!IntervalTimer::dont_use_any_timer, "should not have timer interrupts");
  
  InterruptedContext::the_interrupted_context->set(scp);

  IntervalTimer* t =  sig == SIGALRM ? IntervalTimer::Real_timer() 
                                     : IntervalTimer:: CPU_timer();
  t->do_async_tasks();
  
  if ( PendingSelfSignals::keyboard_signals() != 0 ) 
    preemptor();
        
  InterruptedContext::the_interrupted_context->invalidate();

  SignalInterface::currentTimerSignal = 0;

  if (IntervalTimer::dont_use_any_timer  && sig == SIGVTALRM)
    IntervalTimerTick(SIGALRM, info, scp);  // simulate real timer
  }


void IntervalTimer::do_async_tasks() {
  for ( fint i = 0;  i < registered;  ++i)
    entry_at(i)->do_procs_if_needed();
}

TimerEntry* IntervalTimer::entry_at(int i) { return &entries()[i]; }

/* Sun-$Revision: 30.20 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sig_unix.hh"

# include "_sig_unix.cpp.incl"

int SignalInterface::currentNonTimerSignal = 0;
int SignalInterface::currentTimerSignal = 0;


# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION
  static void signal_handler(int sig, self_code_info_t *info = NULL, self_sig_context_t *scp = NULL);
# elif TARGET_OS_VERSION == SUNOS_VERSION
  static void signal_handler(int sig, int code = 0, self_sig_context_t *scp = NULL, char *addr = NULL);
# else
  # error what?
# endif

static int32 ctrl_z_handler(int sig);
  


// ==================================================================


void OSToSelfSignalMapper::init_platform() {
  _map[ SIGINT    ] = sigint;
  _map[ SIGQUIT   ] = sigquit;
  _map[ SIGIO     ] = sigio;
  _map[ SIGUSR1   ] = siguser1;
  _map[ SIGUSR2   ] = siguser2;
  _map[ SIGPIPE   ] = sigpipe;
  _map[ SIGTERM   ] = sigterm;
  _map[ SIGURG    ] = sigurg;
  _map[ SIGCHLD   ] = sigchild;
  _map[ SIGHUP    ] = sighup;
  _map[ SIGWINCH  ] = sigwinch;
  _map[ SIGALRM   ] = sigrealtimer;
  _map[ SIGVTALRM ] = sigcputimer;
}

// ==================================================================


bool SignalInterface::is_on_signal_stack(char* sp) {
  if (sp == NULL)  sp = (char*)currentFrame();
  return signal_stack <= sp  
      &&           sp <  signal_stack + signal_stack_size;
}


bool SignalInterface::is_off_signal_stack(char* sp) { 
  return !is_on_signal_stack(sp); 
}
  
  
void SignalInterface::wait_for_any() {
  select(0, NULL, NULL, NULL, NULL);
  IntervalTimer::do_all_sync_tasks();
}


void SignalInterface::initialize_platform(bool ctrlC) {
  init_sig_mask();
  if (! signal_stack) {
    init_signal_stack();
  }
  install_signal(SIGSEGV,        signal_handler);
  install_signal(SIGNonLifo,     signal_handler);
    
  // catch all fatal errors
  install_signal(SIGILL,         signal_handler);
  install_signal(SIGABRT,        signal_handler);
  // want to use gdb always
  // install_signal(SIGEMT,         signal_handler);
  install_signal(SIGFPE,         signal_handler);
  install_signal(SIGBUS,         signal_handler);  
  install_signal(SIGSYS,         signal_handler);
  // ^Z handling
  install_signal(SIGTSTP,        Signal_Handler_t(ctrl_z_handler));
  install_signal(SIGCONT,        Signal_Handler_t(ctrl_z_handler));
  
  // Self-level signals
  if (ctrlC) {
    install_signal(SIGQUIT,      signal_handler);
    install_signal(SIGINT,       signal_handler);
  }
  install_signal(SIGIO,          signal_handler);
  install_signal(SIGUSR1,        signal_handler);
  install_signal(SIGUSR2,        signal_handler);
  install_signal(SIGPIPE,        signal_handler);
  install_signal(SIGTERM,        signal_handler);
  install_signal(SIGURG,         signal_handler);
  install_signal(SIGCHLD,        signal_handler);
  install_signal(SIGHUP,         signal_handler);
  install_signal(SIGWINCH,       signal_handler);

# if TARGET_OS_VERSION == SUNOS_VERSION
  install_signal(SIGLOST, signal_handler);
# endif
}


// SunOS always seems to signal SIGBUS, but to be safe also include SEGV
bool SignalInterface::is_map_load_signal(int sig) { return sig == SIGSEGV  || sig == SIGBUS; }


# if TARGET_OS_VERSION ==  SOLARIS_VERSION
  bool SignalInterface::is_uplevel_trap(int code) { return code == ILL_ILLTRP; }
# elif TARGET_OS_VERSION ==  SUNOS_VERSION
  bool SignalInterface::is_uplevel_trap(int code) { return code == ILL_TRAP_FAULT(ST_UpLevel); }
# elif TARGET_OS_VERSION == MACOSX_VERSION
  bool SignalInterface::is_uplevel_trap(int code) { return code == ILL_ILLOPC; }
# else
  bool SignalInterface::is_uplevel_trap(int code) { fatal("unimp"); return false; }
# endif


// ================================================================

# if  GENERATE_DEBUGGING_AIDS

// For inspecting the signal mask
static void currentMask() {
 sigset_t current_mask;

 sigprocmask(SIG_SETMASK, NULL, &current_mask);
 for(int i = 1; i < 34; i++) {
   if (sigismember(&current_mask, i)) fprintf(stderr, "Sig-%d, ", i);
 }
 fprintf(stderr, "\n");
 fflush(stderr);
}

#endif

// ==================================================================

sigset_t SignalInterface::old_mask, SignalInterface::sig_mask;
char*    SignalInterface::signal_stack;
int      SignalInterface::signal_stack_size;


void SignalInterface::install_signal(int sig, Signal_Handler_t handler) {
  struct sigaction action;
  action.sa_handler = (void (*)(int))handler;
  action.sa_mask    = sig_mask;
  action.sa_flags   = install_flags();

  if (sigaction(sig, &action, NULL) == -1) {
    perror("sigaction");
    fatal1("couldn't install signal action for signal %ld", sig);
  }
  if (sig != SIGILL && sig != SIGABRT && sig != SIGFPE && sig != SIGBUS && sig != SIGSEGV
  &&  sig != SIGSYS && sig != SIGTSTP && sig != SIGCONT && sig != SIGUSR1 && sig != SIGUSR2
  && sig != SIGPIPE && sig != SIGTERM && sig != SIGHUP && sig != SIGQUIT
  && !sigismember(&sig_mask, sig))
    fatal1("should have masked %d", sig);
}


void SignalInterface::init_sig_mask() {
  if (CheckAssertions) {
    // only block timers and SIGIO - gdb doesn't work with all signals blocked
    sigemptyset(&sig_mask);
    sigaddset(&sig_mask, SIGALRM);
    sigaddset(&sig_mask, SIGVTALRM);
    sigaddset(&sig_mask, SIGIO);
    sigaddset(&sig_mask, SIGCHLD);
    sigaddset(&sig_mask, SIGURG);
    sigaddset(&sig_mask, SIGWINCH);
    sigaddset(&sig_mask, SIGINT );
    return;
  }
  // also block other signals, but not gdb essentials
  sigfillset(&sig_mask);
  sigdelset(&sig_mask, SIGTSTP);
  sigdelset(&sig_mask, SIGTRAP);
  sigdelset(&sig_mask, SIGSEGV);
  sigdelset(&sig_mask, SIGILL);
  sigdelset(&sig_mask, SIGBUS);
  sigdelset(&sig_mask, SIGFPE);
  # if  TARGET_OS_VERSION != LINUX_VERSION
  sigdelset(&sig_mask, SIGEMT);
  # endif
}


static int32 ctrl_z_handler(int sig) {
  if (InterruptedContext::the_interrupted_context->forwarded_to_self_thread(sig))
    return 0;
  
  FlagSettingInt fs(errno, 0);  // save errno
  if (sig == SIGTSTP) {
    OS::handle_suspend_and_resume(true);
    LOG_EVENT("Control-Z");
    kill(0, SIGSTOP);
    return 0;
  } else {
    OS::handle_suspend_and_resume(false);
    return 0;
  }
}

# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION
static void signal_handler(int sig, self_code_info_t *info, self_sig_context_t *scp) {
  if (InterruptedContext::the_interrupted_context->forwarded_to_self_thread(sig))
    return;
 
  if (SignalInterface::currentNonTimerSignal || SignalInterface::currentTimerSignal) {
      fatal3("signal_handler:  cannot nest (only one interrupted context).\n"
             "Received sig %d while in sig %d or timer sig %d.\n"
             "MacOSX ApplicationEnhancer causes apps to get signals that should be blocked.",
             sig, SignalInterface::currentNonTimerSignal, SignalInterface::currentTimerSignal);
    }

    SignalInterface::currentNonTimerSignal = sig;

    sigset_t oset;
    if (sigprocmask(0, NULL, &oset) != 0) {
      perror("sigprocmask");
      fatal("sigprocmask failed");
    }
    if (!sigismember(&oset, sig))
    fatal2("signal %d is not member of 0x%x", sig, (int*)(int*)&oset);
      
    InterruptedContext::the_interrupted_context->set(scp);
    SignalInterface::handle_signal( sig, 
                                    info == NULL  ?  NULL  :  (char*)info->si_addr, 
                                    info == NULL  ?  NULL  :         info->si_code );
    InterruptedContext::the_interrupted_context->invalidate();

    SignalInterface::currentNonTimerSignal = 0;
  }


# elif TARGET_OS_VERSION == SUNOS_VERSION
  static void signal_handler(int sig, int code, self_sig_context_t *scp, char *addr) {   
    InterruptedContext::the_interrupted_context->set(scp);
    SignalInterface::handle_signal( sig, addr, code );
    InterruptedContext::the_interrupted_context->invalidate();
  }
                     
# else
  # error what?
# endif


# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION == MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==  LINUX_VERSION

void SignalInterface::init_signal_stack() {
  // ensure generic val is enough for ppc
  signal_stack_size = max(1 << 15, SIGSTKSZ); // 32K should be enough
  self_stack_t ss;
  ss.ss_sp    = signal_stack = (char*)valloc(signal_stack_size);
  ss.ss_size  = signal_stack_size;
  ss.ss_flags = 0;
  if ( sigaltstack(&ss, NULL) == -1) {
    perror("sigaltstack");
    fatal("cannot install alternate signal stack");
  }
}

# elif TARGET_OS_VERSION == SUNOS_VERSION

void SignalInterface::init_signal_stack() {
  // ensure generic val is enough for ppc
  signal_stack_size = max(1 << 15, SIGSTKSZ); // 32K should be enough
  signal_stack = (char*)valloc(signal_stack_size);
  struct sigstack ss;
  ss.ss_sp = signal_stack + signal_stack_size;     // stack grows downwards
  ss.ss_onstack = false;
  if (sigstack(&ss, NULL) == -1) {
   perror("sigstack");
  fatal("cannot install signal stack");
  }
}


void SignalInterface::wait_for_any() {
  sigpause(0);
  IntervalTimer::do_all_sync_tasks();
}

# else

  # error which?

# endif // which OS_VERSION
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "errorCodes_unix.hh"

# include "_errorCodes_unix.cpp.incl"

  static const char* unixError[] = {
  
# if TARGET_OS_VERSION == SOLARIS_VERSION

    "NOERR",
    // 1 through 20
    "EPERM", "ENOENT", "ESRCH", "EINTR", "EIO",
    "ENXIO", "E2BIG", "ENOEXEC", "EBADF", "ECHILD",
    "EAGAIN", "ENOMEM", "EACCES", "EFAULT", "ENOTBLK",
    "EBUSY", "EEXIST", "EXDEV", "ENODEV", "ENOTDIR",
    
    // 21 through 40
    "EISDIR", "EINVAL", "ENFILE", "EMFILE", "ENOTTY",
    "ETXTBSY", "EFBIG", "ENOSPC", "ESPIPE", "EROFS",
    "EMLINK", "EPIPE", "EDOM", "ERANGE", "ENOMSG",
    "EIDRM", "ECHRNG", "EL2NSYNC", "EL3HLT", "EL3RST",
    
    // 41 through 60
    "ELNRNG", "EUNATCH", "ENOCSI", "EL2HLT", "EDEADLK",
    "ENOLCK", "ECANCELED", "ENOTSUP", "", "EBADE",
    "EBADR", "EXFULL", "ENOANO", "EBADRQC", "EBADSLT",
    "EDEADLOCK", "EBFONT", "", "", "ENOSTR",
    
    // 61 through 80
    "ENODATA", "ETIME", "ENOSR", "ENONET", "ENOPKG",
    "EREMOTE", "ENOLINK", "EADV", "ESRMNT", "ECOMM",
    "EPROTO", "", "", "EMULTIHOP", "",
    "", "EBADMSG", "ENAMETOOLONG", "EOVERFLOW", "ENOTUNIQ",
    
    // 81 through 100
    "EBADFD", "EREMCHG", "ELIBACC", "ELIBBAD", "ELIBSCN",
    "ELIBMAX", "ELIBEXEC", "EILSEQ", "ENOSYS", "ELOOP",
    "ERESTART", "ESTRPIPE", "ENOTEMPTY", "EUSERS", "ENOTSOCK",
    "EDESTADDRREQ", "EMSGSIZE", "EPROTOTYPE", "ENOPROTOOPT", "",
    
    // 101 through 120
    
    "", "", "", "", "",
    "", "", "", "", "",
    "", "", "", "", "",
    "", "", "", "", "EPROTONOSUPPORT",
    
    // 121 through 140
    "ESOCKTNOSUPPORT", "EOPNOTSUPP", "EPFNOSUPPORT", "EAFNOSUPPORT", "EADDRINUSE",
    "EADDRNOTAVAIL", "ENETDOWN", "ENETUNREACH", "ENETRESET", "ECONNABORTED",
    "ECONNRESET", "ENOBUFS", "EISCONN", "ENOTCONN", "EUCLEAN",
    "", "ENOTNAM", "ENAVAIL", "EISNAM", "EREMOTEIO",
    
    
    // 141 through 151
    "EINIT", "EREMDEV", "ESHUTDOWN", "ETOOMANYREFS", "ETIMEDOUT",
    "ECONNREFUSED", "EHOSTDOWN", "EHOSTUNREACH", "EALREADY", "EINPROGRESS",
    "ESTALE"
    
# elif  TARGET_OS_VERSION == SUNOS_VERSION

    "NOERR",
    // 1 through 20
    "EPERM", "ENOENT", "ESRCH", "EINTR", "EIO",
    "ENXIO", "E2BIG", "ENOEXEC", "EBADF", "ECHILD",
    "EAGAIN", "ENOMEM", "EACCES", "EFAULT", "ENOTBLK",
    "EBUSY", "EEXIST", "EXDEV", "ENODEV", "ENOTDIR",
    
    // 21 through 40
    "EISDIR", "EINVAL", "ENFILE", "EMFILE", "ENOTTY",
    "ETXTBSY", "EFBIG", "ENOSPC", "ESPIPE", "EROFS",
    "EMLINK", "EPIPE", "EDOM", "ERANGE", "EWOULDBLOCK",
    "EINPROGRESS", "EALREADY", "ENOTSOCK", "EDESTADDRREQ", "EMSGSIZE",
    
    // 41 through 60
    "EPROTOTYPE", "ENOPROTOOPT", "EPROTONOSUPPORT", "ESOCKTNOSUPPORT", "EOPNOTSUPP",
    "EPFNOSUPPORT", "EAFNOSUPPORT", "EADDRINUSE", "EADDRNOTAVAIL", "ENETDOWN",
    "ENETUNREACH", "ENETRESET", "ECONNABORTED", "ECONNRESET", "ENOBUFS",
    "EISCONN", "ENOTCONN", "ESHUTDOWN", "ETOOMANYREFS", "ETIMEDOUT",
    
    // 61 through 80
    "ECONNREFUSED", "ELOOP", "ENAMETOOLONG", "EHOSTDOWN", "EHOSTUNREACH",
    "ENOTEMPTY", "EPROCLIM", "EUSERS", "EDQUOT", "ESTALE",
    "EREMOTE", "ENOSTR", "ETIME", "ENOSR", "ENOMSG",
    "EBADMSG", "EIDRM", "EDEADLK", "ENOLCK", "ENONET",
    
    // 81 through 90
    "ERREMOTE", "ENOLINK", "EADV", "ESRMNT", "ECOMM",
    "EPROTO", "EMULTIHOP", "EDOTDOT", "EREMCHG", "ENOSYS"
    
# elif  TARGET_OS_VERSION == MACOSX_VERSION

    "NOERR",
    // 1 through 20
    "EPERM", "ENOENT", "ESRCH", "EINTR", "EIO",
    "ENXIO", "E2BIG", "ENOEXEC", "EBADF", "ECHILD",
    "EDEADLK", "ENOMEM", "EACCES", "EFAULT", "ENOTBLK",
    "EBUSY", "EEXIST", "EXDEV", "ENODEV", "ENOTDIR",
    
    // 21 through 40
    "EISDIR", "EINVAL", "ENFILE", "EMFILE", "ENOTTY",
    "ETXTBSY", "EFBIG", "ENOSPC", "ESPIPE", "EROFS",
    "EMLINK", "EPIPE", "EDOM", "ERANGE", "EAGAIN",
    "EINPROGRESS", "EALREADY", "ENOTSOCK", "EDESTADDRREQ", "EMSGSIZE",
    
    // 41 through 60
    "EPROTOTYPE", "ENOPROTOOPT", "EPROTONOSUPPORT", "ESOCKTNOSUPPORT", "ENOTSUP",
    "EPFNOSUPPORT", "EAFNOSUPPORT", "EADDRINUSE", "EADDRNOTAVAIL", "ENETDOWN",
    "ENETUNREACH", "ENETRESET", "ECONNABORTED", "ECONNRESET", "ENOBUFS",
    "EISCONN", "ENOTCONN", "ESHUTDOWN", "ETOMANYREFS", "ETIMEDOUT",
    
    // 61 through 80
    "ECONNREFUSED", "ELOOP", "ENAMETOOLONG", "EHOSTDOWN", "EHOSTUNREACH",
    "ENOTEMPTY", "EPROCLIM", "EUSERS", "EDQUOT", "ESTALE",
    "EREMOTE", "EBADRPC", "ERPCMISMATCH", "EPROGUNAVAIL", "EPROGMISMATCH",
    "EPROCUNAVAIL", "ENOLCK", "ENOSYS", "EFTYPE", "EAUTH",
    
    // 81 through 100
    "ENEEDAUTH", "EPWROFF", "EDEVERR", "EOVERFLOW", "EBADEXEC",
    "EBADARCH", "ESHLIBVERS", "EBADMACHO", "ELAST"


# elif  TARGET_OS_VERSION == LINUX_VERSION

    "NOERR",
    // 1 through 20
    "EPERM", "ENOENT", "ESRCH", "EINTR", "EIO",
    "ENXIO", "E2BIG", "ENOEXEC", "EBADF", "ECHILD",
    "EAGAIN", "ENOMEM", "EACCES", "EFAULT", "ENOTBLK",
    "EBUSY", "EEXIST", "EXDEV", "ENODEV", "ENOTDIR",
    
    // 21 through 40
    "EISDIR", "EINVAL", "ENFILE", "EMFILE", "ENOTTY",
    "ETXTBSY", "EFBIG", "ENOSPC", "ESPIPE", "EROFS",
    "EMLINK", "EPIPE", "EDOM", "ERANGE", "EDEADLK",
    "ENAMETOOLONG", "ENOLCK", "ENOSYS", "ENOTEMPTY", "ELOOP",
    
    // 41 through 60
    "E41-UNUSED", "ENOMSG", "EIDRM", "ECHRNG", "EL2NSYNC",
    "EL3HLT", "EL3RST", "ELNRNG", "EUNATCH", "ENOCSI",
    "EL2HLT", "EBADE", "EBADR", "EXFULL", "ENOANO", 
    "EBADRQC", "EBADSLT", "E58-Unused", "EBFONT", "ENOSTR",
    
    // 61 through 80
    "ENODATA", "ETIME", "ENOSR", "ENONET", "ENOPKG",
    "EREMOTE", "ENOLINK", "EADV", "ESRMNT", "ECOMM",
    "EPROTO", "EMULTIHOP", "EDOTDOT", "EBADMSG", "EOVERFLOW",
    "ENOTUNIQ", "EBADFD", "EREMCHG", "ELIBACC", "ELIBBADD",
    
    // 81 through 100
    "ELIBSCN", "ELIBMAX", "ELIBEXEC", "EILSEQ", "ERESTART",
    "ESTRPIPE", "EUSERS", "ENOTSOCK", "EDESTADDRREQ", "EMSGSIZE",
    "EPROTOTYPE", "ENOPROTOOPT", "EPROTONOSUPPORT", "ESOCKTNOSUPPORT", "EOPNOTSUPP",
    "EPFNOSUPPORT", "EAFNOSUPPORT", "EADDRINUSE", "EADDRNOTAVAIL", "ENETDOWN",

    // 101 through 120
    "ENETUNREACH", "ENETRESET", "ECONNABORTED", "ECONNRESET", "ENOBUFS",
    "EISCONN", "ENOTCONN", "ESHUTDOWN", "ETOOMANYREFS", "ETIMEOUT",
    "ECONNREFUSED", "EHOSTDOWN", "EHOSTUNREACH", "EALREADY", "EINPROGRESS",
    "ESTALE", "EUCLEAN", "ENOTNAM", "ENAVAIL", "EISNAM", 

    // 121 through 140
    "EREMOTEIO", "EDQUOT", "ENOMEDIUM", "EMEDIUMTYPE", "ECANCELED",
    "ENOKEY", "EKEYEXPIRED", "EKEYREVOKED", "EKEYREJECTED", "EOWNERDEAD",
    "ENOTRECOVERABLE"

#   else
	# error what?

#   endif // OS_VERSION

};


static inline int numUnixErrors() { return sizeof(unixError) / sizeof(unixError[0]); }


#if TARGET_OS_VERSION != MACOSX_VERSION  &&  TARGET_OS_VERSION != LINUX_VERSION
extern char* sys_errlist[];             // error messages corr. to errno.
extern int   sys_nerr;                  // length of above table
#endif // !MACOSX_VERSION


const char* ErrorCodes::os_error_name(int error) {
  static char buf[128];
  return 0 <= error && error < numUnixErrors()  &&  unixError[error][0]
    ? unixError[error]
    : (sprintf(buf, "UNKNOWN %d", error), buf);

}

# if TARGET_OS_VERSION ==  LINUX_VERSION \
  || TARGET_OS_VERSION == MACOSX_VERSION
  # define ERROR_STRING(i) strerror(i)
# else
  # define ERROR_STRING(i) (i < sys_nerr ? (char*)sys_errlist[i] : (char*)-1)
# endif

char* ErrorCodes::os_error_message(char* s) {  
  // search unix error table
  int i;
  for( i = 0; i < numUnixErrors(); i++) {
    if (!strncmp(s, unixError[i], strlen(unixError[i]))) {
      return ERROR_STRING(i);
    }
  }
  return NULL;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "signalBlocker_unix.hh"

# include "_signalBlocker_unix.cpp.incl"


SignalBlocker::SignalBlocker(SignalBlockerType sbt) {
 
  sigset_t new_mask;
  
  switch ( sbt ) {
   case block_signals_self_uses:
    if (sigprocmask(SIG_BLOCK, &SignalInterface::sig_mask, &saved_mask))
      perror("sigprocmask");
    break;
  
   default:
    if ( sigfillset(&new_mask) ) perror("sigemptyset");
    if ( sbt == allow_user_int ) {
      if (sigdelset(&new_mask, SIGINT)) perror("sigdelset");
    }
    if (sigprocmask(SIG_SETMASK, &new_mask, &saved_mask))
      perror("sigprocmask");
    break;
  }
}


SignalBlocker::~SignalBlocker() {
  if (sigprocmask(SIG_SETMASK, &saved_mask, NULL)) perror("sigprocmask");
}
/* Sun-$Revision: 30.32 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

/* There used to be a TARGET_OS_VERSION == MACOSX_VERSION check instead of __APPLE__.
 * However, the TARGET_OS_VERSION test only works after the header files are included,
 * but we need to have the Carbon header included before the other headers.
 * So we have to make an exception and not use the TARGET_OS_VERSION macro. -ma and dmu 4/02 */
# ifdef __APPLE__
  #  undef ASSEMBLER
  #  undef Alloc
  #  include <Carbon/Carbon.h>

  // remove Carbon macros to avoid name collisions
  #  undef assert
  #  undef assert_type
  #  undef assert_smi
  #  undef assert_byteVector
  #  undef assert_objVector

  #  include "asserts.hh"
  #  undef verify
  #  undef check
  #  undef XLIB
# endif

# include "_os_unix.cpp.incl"


# if TARGET_OS_VERSION != MACOSX_VERSION 

  bool OS::is_directed_allocation_supported() { return true; } // should return desiredAddress

  char* OS::allocate_heap_aligned(caddr_t desiredAddress,
                                  int32 size, int32 align, const char* name,
                                  bool mustAllocate) {
      if ( desiredAddress != NULL
      &&   desiredAddress ==
                 mmap(desiredAddress, 
                      size + align,
                      PROT_READ|PROT_WRITE|PROT_EXEC,
                      MAP_PRIVATE|MAP_FIXED,
                      zero_fd, 0)) 
        return desiredAddress;
        
      char* b = (char*)memalign(align, size);
      if (b == NULL && mustAllocate)  allocate_failed(name);
      return b;     
  }


# else
  // No point in horsing around trying to make mmap work on OS X.
  // Works OK without it & and I have better things to do.  dmu 9/1
  
  bool OS::is_directed_allocation_supported() { return false; } 

  char* OS::allocate_heap_aligned(caddr_t ,
                                  int32 size, int32 align, const char* name,
                                  bool mustAllocate) {
    // fake it
    char* b = (char*)selfs_malloc(size + align - 1);
    if (b == NULL) {
      if (mustAllocate)
         allocate_failed(name);
      return b;
    }
    if ((int32)b & (align-1)) {
      char* newB = (char*) ((int32)b & ~(align-1)) + align;
      assert(b + size + align - 1 >=  newB + size, "rounding error");
      return newB;
    }
    else
      return b;
  }



# endif

  
# ifdef EXPERIMENT_WITH_APPLE_EVENTS
  
  static OSErr handle_open_app_event(const AppleEvent *theAppleEvent, AppleEvent *reply, long /*handlerRefcon*/) {
    lprintf("handle_open_app_event\n");
    long int n = -1;
    if (AECountItems(theAppleEvent, &n) != noErr) {warning("AECountItems failed"); return noErr;}
    lprintf("n = %d\n", n);
    return noErr;
  }
  static OSErr handle_open_document_event(const AppleEvent *theAppleEvent, AppleEvent *reply, long /*handlerRefcon*/) {
    lprintf("handle_open_document_event\n");
    long int n = -1;
    if (AECountItems(theAppleEvent, &n) != noErr) {warning("AECountItems failed"); return noErr;}
    lprintf("n = %d\n", n);
    AEDesc theAEDesc;
    OSErr e = AEGetParamDesc ( theAppleEvent, keyDirectObject, typeFSRef, &theAEDesc);
    if (e != noErr) {warning("AEGetParemDesc"); return noErr;}
    
    AppleEvent newAppleEvent;
    // e = AECreateAppleEvent(kCoreEventClass, kAEOpenApplication, 
    e =  AESendMessage (&newAppleEvent, NULL, kAENoReply, 0);
    if (e != noErr) {warning1("AESend %d", e); return noErr;}

    return noErr;
  }

  void handle_dropped_snapshot() {
    lprintf("handle_dropped_snapshot\n");
    OSErr e;
    e = AEInstallEventHandler ( kCoreEventClass,  kAEOpenApplication, handle_open_app_event, 0, false );
    if (e != noErr) {
      warning1("could not install event handler: %d", (int)e);
      return;
    }
    e = AEInstallEventHandler ( kCoreEventClass,  kAEOpenDocuments, handle_open_document_event, 0, false );
    if (e != noErr) {
      warning1("could not install event handler: %d", (int)e);
      return;
    }
    EventRecord evt;
    do {
      WaitNextEvent( highLevelEventMask, &evt, 0, NULL ); // was 1 instead of zero but buttons sluggish
        if ( evt.what == kHighLevelEvent ) {
          OSErr r = AEProcessAppleEvent(&evt);
          if (r != noErr)
            warning1("ignoring failed Apple event:, %d", (int)r);
        }
    } while ( evt.what != nullEvent ); 
  }  
  
  # endif // EXPERIMENT_WITH_APPLE_EVENTS


char* OS::get_user_directory(const char* user) {
  struct passwd* p = getpwnam(user);
  return  p == NULL  ?  NULL  :  p->pw_dir;
}


bool OS::is_fifo(mode_t m) {  return S_ISFIFO(m); }

time_t OS::combine_time(smi day, smi msec) {
  return  day * seconds_per_day + msec / 1000;
}


void OS::init() {

  SignalInterface::initialize(false);           // everything except ^C

  zero_fd= open("/dev/zero", O_RDONLY);
  if (zero_fd < 0) fatal("couldn't open /dev/zero");


# if  TARGET_OS_VERSION == SOLARIS_VERSION 

  real_mem_size=
    get_page_size() * (sysconf(_SC_PHYS_PAGES) - sysconf(_SC_AVPHYS_PAGES));

# elif  TARGET_OS_VERSION == SUNOS_VERSION
  // can't get this from SunOS without being super user, so do the
  // next best thing (which is pretty disgusting)
  {
    void (*old_h)(int, void*)= signal(SIGCHLD, SIG_IGN);
    bool ok= false;
    // assume 32Mb machine - 12Mb for Unix and X
    real_mem_size= 20 * 1024 * 1024; 
    FILE *p= popen("/etc/dmesg | /usr/bin/grep '^mem = '", "r");
    if (p) {
      if (fscanf(p, "mem = %dK", &real_mem_size) == 1) {
        real_mem_size *= 1024;  ok= true;
      }
      pclose(p);
    }
    if (!ok) {
      p= popen("/usr/bin/sed -n '/.*mem = \\([0-9]*\\)K.*/s//\\1/p' /var/adm/messages*", "r");
      if (p) {
        if (fscanf(p, "%d", &real_mem_size) == 1) {
          real_mem_size *= 1024;  ok= true;
        }
        pclose(p);
      }
    }
    signal(SIGCHLD, old_h);
  }
# elif  TARGET_OS_VERSION ==  MACOSX_VERSION

  int mib[2], mem_size;
  size_t len;

  mib[0] = CTL_HW;
  mib[1] = HW_USERMEM;
  len = sizeof(mem_size);
  sysctl(mib, 2, &mem_size, &len, NULL, 0);
  real_mem_size = mem_size;
# elif TARGET_OS_VERSION == LINUX_VERSION
  real_mem_size = 0x40000000; // punt for now
# else
  # error which?
# endif

# if TARGET_OS_VERSION == SUNOS_VERSION
  { // don't run SunOS 4 VM under Solaris!
    struct utsname name;
    uname(&name);
    if (name.release[0] == '5') {
      lprintf("This version of the Self virtual machine is for SunOS 4\n"
             "and cannot be used under SunOS 5 (aka Solaris 2).  Sorry!\n\n");
      exit(1);
    }
  }
#endif
}


void OS::terminate(int code) {
   prepare_to_exit_self();
   ::exit(code);
}


void OS::handle_suspend_and_resume(bool stopping) {
  static int  stdinFlags = 0;
  if (stopping) {
    stdinFlags = fcntl(0, F_GETFL, 0);
    if (stdinFlags == -1)                        perror("fcntl");
    if (fcntl(0, F_SETFL, 0) == -1)          perror("fcntl");
  } else {
    if (fcntl(0, F_SETFL, stdinFlags) == -1) perror("fcntl");
  }
}


void OS::Mmap(void *start, void *fin, FILE *file)
{
  long int len= (char*)fin - (char*)start;
  if (len == 0) return;
  if (mmap((char*)start,
           len,
           PROT_READ|PROT_WRITE,
           MAP_PRIVATE|MAP_FIXED,
           fileno(file),
           ftell(file)) != start) {
    perror("cannot read from file");
    fatal("read error");
  }
  if (fseek(file, len, SEEK_CUR) < 0) {
    perror("cannot read from file");
    fatal("seek error");
  }
}

void OS::do_not_buffer(FILE* stream) { ::setbuf(stream, NULL); }


# if TARGET_OS_VERSION == SUNOS_VERSION
extern "C" {
  int munmap(caddr_t addr, int len);
}
# endif

char* OS::map_or_read_source_file(FILE* source_file, int32 length) {
  return (char*) mmap(0, length, PROT_READ, MAP_PRIVATE, fileno(source_file), 0);
}


void OS::unmap_source_file(caddr_t first, int length) {
  munmap(first, length);
}


bool OS::is_non_unix_path(const char* s) {
  Unused((char*)s);
  return false;
}


# if TARGET_OS_VERSION != MACOSX_VERSION

bool OS::setup_snapshot_to_run(const char* fileName) {
  // make executable
  struct stat stb;
  if (stat(fileName, &stb)) return false;
  mode_t old_perms= stb.st_mode;
  mode_t mask= umask(0);
  (void)umask(mask);
  if (chmod(fileName,
       old_perms | (~mask & (S_IXUSR | S_IXGRP | S_IXOTH))))
    return false;
  return true;
}

# else




/* The following two functions were written by Kristen McIntyre, 
 changed by Tobias Pape because of deprecation warnings. */
static bool path_to_fsref(const char *path, FSRef *fref)
{
    if (FSPathMakeRef((UInt8*) path, fref, NULL) == noErr)
        return true;
    return false;
}

static bool set_type_creator(const char *path, const char *type, const char *creator)
{
  FileInfo* finfo;
  FSRef fsr;
  FSCatalogInfo cinfo;
    
  if (path_to_fsref(path, &fsr) == false) {
    return false;
  }
  if (FSGetCatalogInfo(&fsr, kFSCatInfoFinderInfo | kFSCatInfoFinderInfo, &cinfo, NULL, NULL, NULL) != noErr) {
    return false;
  }
  
  finfo = (FileInfo*) &cinfo.finderInfo;

  finfo->fileType = 0;
  if (type != NULL) {
    for (int i = 0; i < 4; i++) {
        finfo->fileType |= ((unsigned char)type[i]) << ((3 - i) * 8); 
    }
  }

  finfo->fileCreator = 0;
  if (creator != NULL) {
    for (int i = 0; i < 4; i++) {
       finfo->fileCreator |= (unsigned char)creator[i] << ((3 - i) * 8);
    }
  }

  return FSSetCatalogInfo(&fsr, kFSCatInfoFinderInfo, &cinfo) == noErr;
}

bool OS::setup_snapshot_to_run(const char* fileName) {
  return set_type_creator(fileName, "Snap", "Self");
}

# endif


void OS::set_log_buf(FILE* f, char* buf, int bs) {    
# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION
  setvbuf(f, buf, _IOFBF, bs);
  
# elif  TARGET_OS_VERSION == SUNOS_VERSION
  extern "C" void setbuffer(FILE *stream, char *buf, int size);
  setbuffer(f, buf, bs);

# else
  error NO OS VERSION
# endif
}


const char* OS::log_file_name() {
  static char fname[100];
  sprintf(fname, "/tmp/Self.vmlog.%ld", long(getpid()));
  assert(strlen(fname) < sizeof(fname), "");
  return fname;
}

void OS::set_args(int& , char**& ) {}


# if  TARGET_OS_VERSION == SOLARIS_VERSION

extern "C" int swapctl(int cmd, void *arg);


bool OS::get_swap_space_info(int &totalK, int &freeK) {
  struct anoninfo ai;

  // documentation for this syscall is a little, ermm, sparse..
  if (swapctl(SC_AINFO, &ai) == -1) return false;

  // allocated = anon memory not free
  // reserved = anon memory reserved but not allocated
  // available = anon memory not reserved
  int allocated= ai.ani_max - ai.ani_free;
  int reserved= ai.ani_resv - allocated;
  int available= ai.ani_max - ai.ani_resv;

  int pageSize= get_page_size();
  if (pageSize >= K) {
    totalK= (allocated + reserved) * (pageSize / K);
    freeK= available * (pageSize / K);
  } else {
    totalK= (allocated + reserved) / (K / pageSize);
    freeK= available / (K / pageSize);
  }
  return true;
}


# elif  TARGET_OS_VERSION ==  SUNOS_VERSION \
    ||  TARGET_OS_VERSION == MACOSX_VERSION \
    ||  TARGET_OS_VERSION ==  LINUX_VERSION

bool OS::get_swap_space_info(int &, int &) {
  return false;
}

# endif


void OS::core_dump() {
  SignalBlocker sb; // ensure nothing is lost on OS X -- dmu 2/03
  IntervalTimer::disable_all(true); // a shotgun attempt to remedy MacOSX coredump/crash bug -- dmu 5/03
  lprintf("\nForcing core dump (and disabling IntervalTimer)...\n");
  SignalInterface::install_signal(SIGABRT, Signal_Handler_t(SIG_DFL));
  abort(); // will signal SIGABRT
}


void OS::enable_core_dumps() {
  // lprintf("\nSetting core limit to maximum value...\n");
  struct rlimit corelim;
  if (getrlimit(RLIMIT_CORE, &corelim) == -1)
    perror("getting core limit");
  corelim.rlim_cur = corelim.rlim_max;
  if (setrlimit(RLIMIT_CORE, &corelim) == -1)
    perror("setting core limit");
}


void OS::discard_pages(char *start, char *end) {
  # if TARGET_OS_VERSION == MACOSX_VERSION \
    || TARGET_OS_VERSION == MACOSX_VERSION
    return; // mmap of /dev/zero just fails on OSX -- dmu 6/04
        // and just punt for now in Linux - dmu 12/07
  # endif

  if (end - start < 2 * dont_bother) return;

  char *ps= real_page_end  (start);
  char *pe= real_page_start(end  );
  if (mmap(ps, pe - ps,
           PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED,
           zero_fd, 0) != ps)
    warning("mmap of /dev/zero failed");
}


int OS::get_page_size() {
# if  TARGET_OS_VERSION == SOLARIS_VERSION
    static int p= sysconf(_SC_PAGESIZE);
    return p;
# else
    return getpagesize();
# endif 
}


int OS::min_core(caddr_t addr, size_t len, char *vec) {
  # if TARGET_OS_VERSION == LINUX_VERSION
    return  mincore(addr, len, (unsigned char*)vec);
  # else
    return  mincore(addr, len, vec);
  # endif
}


# if TARGET_OS_VERSION == SOLARIS_VERSION // decl wont come it!
  extern "C" {int madvise(caddr_t, size_t, int);}
# endif

#if  TARGET_OS_VERSION == SOLARIS_VERSION \
 ||  TARGET_OS_VERSION ==  MACOSX_VERSION
  void OS::setPageAdvisory(char *start, char *end, int code) {
    if (end - start < OS::dont_bother) return;
    char *ps= real_page_start(start);
    char *pe= real_page_end  (end  );
    if (madvise(ps, pe - ps, code) != 0) {
      // Solaris 2.4 returns -1 for no apparent reason sometimes
      // warning("madvise() returned unexpected value");
    }
  }


  void OS::random_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_RANDOM); 
  }

  void OS::sequential_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_SEQUENTIAL); 
  }
   
  void OS::normal_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_NORMAL); 
  }

  void OS::will_need_pages(char *start, char *end) {
    setPageAdvisory(start, end, MADV_WILLNEED); 
  }

  void OS::dont_need_pages(char *start, char *end) {
    setPageAdvisory(start + get_page_size(),
                    end - get_page_size(),
                    MADV_DONTNEED); }

  void OS::set_bytes(char* start, char* end, int filler) {
    OS::  will_need_pages(start, end);
    OS::sequential_access(start, end);
    memset(start, filler, end-start);
    OS::random_access(start, end);
  }


  void  OS::set_access_before_writing_space(void* /* objs_bottom*/, void* /* objs_top*/, 
                                            void* /*bytes_bottom*/, void* /*bytes_top*/) {
    // disabled until I figure out why it pages everything out
    // OS::sequential_access((char*)objs_bottom, (char*)objs_top);
    // OS::sequential_access((char*)bytes_bottom, (char*)bytes_top);
  }

  void  OS::reset_access_after_writing_space(void* /* objs_bottom*/, void* /* objs_top*/, 
                                             void* /*bytes_bottom*/, void* /*bytes_top*/) {
    //  OS::normal_access((char*) objs_bottom,  (char*) objs_top);
    //  OS::normal_access((char*)bytes_bottom,  (char*)bytes_top);
  }

  void OS::set_sequential_access_before_writing_snapshot() {}
  void OS::set_normal_access_after_writing_snapshot() {}


# elif  TARGET_OS_VERSION == SUNOS_VERSION

  void OS::    random_access(char*, char*) { vadvise(VA_ANOM); }
  void OS::sequential_access(char*, char*) { vadvise(VA_SEQL); }
  void OS::    normal_access(char*, char*) { vadvise(VA_NORM); }
  void OS::  will_need_pages(char*, char*) {}
  void OS::  dont_need_pages(char*, char*) {}
    

  void OS::set_bytes(char* start, char* end, int filler) {
    OS::sequential_access(start, end);
    memset(start, filler, end-start);
    OS::normal_access(start, end);
  }
    
  void  OS::set_access_before_writing_space( void*, void*, void*, void*) {}
  void  OS::reset_access_after_writing_space(void*, void*, void*, void*) {}


  void OS::set_sequential_access_before_writing_snapshot() {
    sequential_access((char*)0, (char*)~0);
  }
  
  void OS::set_normal_access_after_writing_snapshot() {
    normal_access((char*)0, (char*)~0);
  }

# elif  TARGET_OS_VERSION == LINUX_VERSION
  void OS::setPageAdvisory(char *start, char *end, int code) {
    if (end - start < OS::dont_bother) return;
    char *ps= real_page_start(start);
    char *pe= real_page_end  (end  );
    if (madvise(ps, pe - ps, code) != 0) {
      // Solaris 2.4 returns -1 for no apparent reason sometimes
      // warning("madvise() returned unexpected value");
    }
  }

  void OS::random_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_RANDOM);
  }

  void OS::sequential_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_SEQUENTIAL);
  }

  void OS::normal_access(char *start, char *end) {
    setPageAdvisory(start, end, MADV_NORMAL);
  }

  void OS::will_need_pages(char *start, char *end) {
    setPageAdvisory(start, end, MADV_WILLNEED);
  }

  void OS::dont_need_pages(char *start, char *end) {
    setPageAdvisory(start + get_page_size(),
                    end - get_page_size(),
                    MADV_DONTNEED); }

  void OS::set_bytes(char* start, char* end, int filler) {
    OS::  will_need_pages(start, end);
    OS::sequential_access(start, end);
    memset(start, filler, end-start);
    OS::random_access(start, end);
  }


  void  OS::set_access_before_writing_space(void* /* objs_bottom*/, void* /* objs_top*/,
                                            void* /*bytes_bottom*/, void* /*bytes_top*/) {
    // disabled until I figure out why it pages everything out
    // OS::sequential_access((char*)objs_bottom, (char*)objs_top);
    // OS::sequential_access((char*)bytes_bottom, (char*)bytes_top);
  }

  void  OS::reset_access_after_writing_space(void* /* objs_bottom*/, void* /* objs_top*/,
                                             void* /*bytes_bottom*/, void* /*bytes_top*/) {
    //  OS::normal_access((char*) objs_bottom,  (char*) objs_top);
    //  OS::normal_access((char*)bytes_bottom,  (char*)bytes_top);
  }

  void OS::set_sequential_access_before_writing_snapshot() {}
  void OS::set_normal_access_after_writing_snapshot() {}
# else
  # error which?
#endif


void OS::real_time(smi buf[]) {
  struct timeval t;
  gettimeofday(&t, 0); // changed for OS X, maybe should use unix headers after all?
  buf[0] = t.tv_sec / seconds_per_day;                               //days
  buf[1] = (t.tv_sec % seconds_per_day) * 1000 + (t.tv_usec / 1000); //msecs    
}


// Returns a string containing the current time in same format
// as the Self level time printString.
char* OS::current_time_string() {
  char*  buffer  = NEW_RESOURCE_ARRAY(char, 100);
  
  struct timeval t;
  gettimeofday(&t, 0);
# if TARGET_OS_VERSION == MACOSX_VERSION
  time_t sec = t.tv_sec;
  strftime(buffer, 100, "%A, %d %B %Y, %T", localtime(&sec));
# else
  strftime(buffer, 100, "%A, %d %B %Y, %T", localtime(&t.tv_sec));
# endif

  return buffer;
}


char* OS::strdup(const char* s) { return ::strdup(s); }


// Convert the (local) time specified in the objVector to days and ms
// Return boolean: true => success, false => failure.
bool OS::time_to_day_and_ms(objVectorOop timeVector, smi* msAndDays) {
  struct tm tod;
  tod.tm_year  = ((smiOop) timeVector->obj_at(0))->value() - 1900;
  tod.tm_mon   = ((smiOop) timeVector->obj_at(1))->value() - 1;
  tod.tm_mday  = ((smiOop) timeVector->obj_at(2))->value();
  tod.tm_wday  = ((smiOop) timeVector->obj_at(3))->value();
  tod.tm_hour  = ((smiOop) timeVector->obj_at(4))->value();
  tod.tm_min   = ((smiOop) timeVector->obj_at(5))->value();
  tod.tm_sec   = ((smiOop) timeVector->obj_at(6))->value();
  tod.tm_isdst = ((smiOop) timeVector->obj_at(7))->value();  // often -1.
  
  time_t t = mktime(&tod);
  if (t == -1) return false;
  msAndDays[0] = t / seconds_per_day;          //days
  msAndDays[1] = (t % seconds_per_day) * 1000; //msecs
  return true;
}


# if  TARGET_OS_VERSION == SUNOS_VERSION
  extern "C" {
    int gethostname(char *n, int len);
  }
# endif


char* OS::get_host_name() {
  char* b = NEW_RESOURCE_ARRAY( char, 255); // mac needs 255
  gethostname(b, 255);
  return b;
}


const char* OS::get_user_name() {
  static struct passwd* p = getpwuid (getuid());
  const char *username= p ? p->pw_name : "the user with no name";
  return username;
};


void OS::convert_unix_filename(const char* src, char* dst) {
  if (strlen(src) >= MAXPATHLEN)
    fatal("path too long");
  strcpy( dst, src);
}


int OS::zero_fd;


FILE*  OS::start_compressing_snapshot(const char* compression_f, const char* fullFileName, SignalBlocker*& sb) {
  char *cmd= new char[strlen(compression_f) + 4 +
                      strlen(fullFileName)  + 1];
  strcpy(cmd, compression_f);
  strcat(cmd, " >> ");
  strcat(cmd, fullFileName);
  sb= new SignalBlocker;
  FILE* snapFile= popen(cmd, "w");
  delete [] cmd;
  return snapFile;
}


int OS::end_compressing_snapshot(FILE* f) { return pclose(f); }


static pid_t child;

# if TARGET_OS_VERSION == SUNOS_VERSION
extern "C" {
  int seteuid(uid_t euid);
  int setegid(gid_t egid);
}
# endif

// Spawn off a decompression program reading from snap at the current offset
// and return a stream reading from the decompression program.
FILE* OS::start_decompressing_snapshot(FILE *snap, const char* decompression_filter)
{
  // The trick here is get the decompression program started in the
  // right place in the file...were it not for this, we could just
  // use popen(). 

  int p[2];
  if (pipe(p))
    fatal("Couldn't open pipe to do snapshot decompression");
  
  pid_t pid= FORK();
  if (pid == (pid_t)-1)
    fatal("Couldn't fork process to do snapshot decompression");

  if (pid == 0) { // this is the child
    long offset= ftell(snap);
    int fd= fileno(snap);
    if (lseek(fd, (off_t)offset, SEEK_SET) != (off_t)offset)
      fatal("lseek failed!");
    if (   dup2(fd, 0) < 0
        || dup2(p[1], 1) < 0)
      fatal("dup2 failed!");
    seteuid(getuid()); // just in case someone makes Self setuid...
    setegid(getgid());
    execlp(decompression_filter, decompression_filter, (char*)0);
    fatal("exec failed!");
  }
  child= pid;
  // this is the parent
  return fdopen(p[0], "r");
}


void OS::end_decompressing_snapshot() {
  int status;
  if (    wait(&status) != child
      ||  !WIFEXITED(status)
      ||  WEXITSTATUS(status))
    fatal("Decompression of snapshot failed");
}


void OS::snapshot_failed(FILE* snapFile, bool compressed_snapshot, SignalBlocker* sb) {
  if (errno == EINTR)
    lprintf("fwrite has failed. If you are writing to a Sun OS 4.1.1 machine\n"
            "the problem is most likely a known kernel bug. You can get\n"
            "a patch to fix the write problem from Sun (bug ID 1052649).\n"
            "Or, try using adb to clear ufs_WRITES in the kernel.\n");

  if (compressed_snapshot) {
    pclose(snapFile); delete sb;
  } else
    fclose(snapFile);
}

const char* OS::get_manufacturer_name() { return "sun"; }

void OS::print_memory() {
  Indent++;
  # if TARGET_OS_VERSION == MACOSX_VERSION
    warning("OS::print_memory unimp for OSX");
  # else
    printIndent();
    lprintf("Text: 0x%lx (%ld) bytes\n", (long)&etext, (long)&etext);
    printIndent();
    lprintf("Data: 0x%lx (%ld) bytes\n\n",
           (long)&end - (long)&etext,  (long)&end - (long)&etext);
  # endif
  Indent--;
}

const char* OS::mode_for_binary(const char* m)  { return m; }


# if TARGET_OS_VERSION == SUNOS_VERSION
  extern "C" {
    int getopt(int argc, char *const *argv, const char *optstring);
    extern char *optarg;
    extern int opterr, optind;
  }
# endif


# if    COMPILER != GCC_COMPILER  &&  TARGET_OS_VERSION == SUNOS_VERSION
  typedef const char* const*  argv_t;
# elif  COMPILER == GCC_COMPILER  ||  TARGET_OS_VERSION == SUNOS_VERSION
  typedef       char* const*  argv_t;
# endif

  
int     OS::getopt(int argc,  char* const* argv,  const char* optstring) {
  ::optarg = optarg;
  ::optopt = optopt;
  ::opterr = opterr;
  ::optind = optind;
  
  int r = ::getopt(argc, (argv_t)argv, optstring);
  
  optarg = ::optarg;
  optopt = ::optopt;
  opterr = ::opterr;
  optind = ::optind;
  
  return r;
}


void OS::check_events() { 
  # if defined(QUARTZ_LIB)
    QuartzWindow::check_carbon_events();
  # endif
} 


# if  TARGET_OS_VERSION == SOLARIS_VERSION \
  ||  TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||  TARGET_OS_VERSION ==  LINUX_VERSION
extern "C" {
  int malloc_verify() { return 1; }
}
# endif


int OS::make_memory_executable(void* addr, size_t len) {
  return mprotect(addr, len, PROT_READ|PROT_WRITE|PROT_EXEC);
}
/* Sun-$Revision: 30.8 $ $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "timer_unix.hh"

# include "_timer_unix.cpp.incl"

// gdb 4.7 has this nasty bug causing lots of sig 8s after evaluating stuff
// from the command line.  Set workAroundSignal8Bug to true to get rid of
// must of these signals (but return incorrect times).   -Urs 12/92
bool workAroundSignal8Bug = false;


void ElapsedTimer::reset_platform() {
  t.tv_sec = t.tv_usec = 0;
}
  

void ElapsedTimer::start() { 
  if (!is_running) {
    gettimeofday(&start_time, 0);
    is_running = true;
  }
}


void ElapsedTimer::stop() {
  if (is_running) {
    timeval stop_time;
    gettimeofday(&stop_time, 0);
    is_running = false;

    if (stop_time.tv_usec < start_time.tv_usec) {
      assert( stop_time.tv_sec > start_time.tv_sec, "secs must have inc.");
      t.tv_sec  += stop_time.tv_sec  - start_time.tv_sec - 1;
      t.tv_usec += one_million - (start_time.tv_usec - stop_time.tv_usec);
    } else {
      t.tv_sec  += stop_time.tv_sec  - start_time.tv_sec;
      t.tv_usec += stop_time.tv_usec - start_time.tv_usec;
    }

    // Check for usec overflow 
    if (t.tv_usec > one_million) {
      t.tv_usec -= one_million;
      t.tv_sec++;
    }
  }
}

float ElapsedTimer::millisecs() { 
  if (workAroundSignal8Bug) return 1.0;
  return (float) t.tv_sec * one_thousand + (float) t.tv_usec / one_thousand;
}


fint ElapsedTimer::secs() { 
  if (workAroundSignal8Bug) return 0;
  return  t.tv_usec < one_million/2 ? t.tv_sec : t.tv_sec + 1; 
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "unixPrims.glue.hh"

#include "_unixPrims.glue.cpp.incl"

# if  TARGET_OS_VERSION == SOLARIS_VERSION
extern "C" {
  int open(const char *path, int  oflag,  /*  mode_t  mode  */ ...);
  int fcntl(int fildes, int cmd, /* arg */ ...);
}
# endif

  DO_NOT_CROSS_COMPILE

# define WHAT_GLUE FUNCTIONS
    unix_glue
    unix_syscall_glue
    macosx_uname_glue
# undef  WHAT_GLUE

/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# ifdef XLIB
 
# pragma implementation "xlibPrims.hh"
# include "_xlibPrims.cpp.incl"

  const char* Display_seal = "Display";

  Display* XOpenDisplay_wrap(char *name, void *FH) {
    // XOpenDisplay fails if a signal is received during the call.
    // All signals except user interrupts are therefore blocked. 
    SignalBlocker sb(SignalBlocker::allow_user_int);
    
    Display* result = XOpenDisplay(name);
    if (result == 0)  {
      prim_failure(FH, PRIMITIVEFAILEDERROR);
      return result;
    }

      XSetErrorHandler(XErrorHandlers::handle_X_error);
    XSetIOErrorHandler(XErrorHandlers::handle_X_IO_error);

    // add this file descriptor to list of all open files
    FD_SET(ConnectionNumber(result), &activeFDs);
    return result;
  }

  void XCloseDisplay_wrap(Display* display) {
    int fd = ConnectionNumber(display);
    XCloseDisplay(display);
    FD_CLR(fd, &activeFDs);
  }

# define WHAT_GLUE FUNCTIONS
    xlib_static_glue
# undef  WHAT_GLUE

  int XErrorHandlers::handle_X_error(Display* disp, XErrorEvent* error) {
    const int msg_buf_len = 100;
    char buf[msg_buf_len];
    XGetErrorText(disp, error->error_code, buf, msg_buf_len);
    if (TheSpy->is_active()  &&  disp == (Display*)TheSpy->mw()->pw->xdisplay()) {
      if (PrintSpyXErrors) {
        warning2("X Error in the Spy: %s (request code %d).\n",
                 buf, (int)error->request_code);
      }
    } 
    else {
      lprintf("X Error: %s.\n", buf);
      print_stack_and_abort();
    }
    return(0);
  }

  // the X I/O error handler must not return (or X will abort Self)
  int XErrorHandlers::handle_X_IO_error(Display* display) {
    Unused(display);
    const char *msg= "X I/O Error .. aborting process.\n";
    ssize_t ret;   // to mollify compiler
    ret = write(1, msg, strlen(msg)); // don't use lprintf (calls malloc)
    print_stack_and_abort();
    return ret;    // to mollify compiler
  }
 

  void XErrorHandlers::print_stack_and_abort() {
    //  currentProcess->stack()->print();  // this corrupts the system
    currentProcess->abort();
  }
  
# endif
/* Sun-$Revision: 30.18 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# if !( TARGET_OS_VERSION ==  MACOSX_VERSION \
  ||    TARGET_OS_VERSION ==   LINUX_VERSION \
  || (  TARGET_OS_VERSION == SOLARIS_VERSION  && TARGET_ARCH == I386_ARCH))
  # define FD_SETSIZE     256             /* max. number of open files */
# endif
  
  // not defined in usr/include/CC/fcntl.h
  
# define STDIN     0
  
# pragma implementation "unixPrims.hh"
# include "_unixPrims.cpp.incl"

  


# if  TARGET_OS_VERSION == SOLARIS_VERSION

extern "C" {
  int open(const char *path, int  oflag,  /*  mode_t  mode  */ ...);
  int fcntl(int fildes, int cmd, /* arg */ ...);
}

# elif TARGET_OS_VERSION == MACOSX_VERSION

  // removed for Tiger: (5/05 dmu)
  // typedef int socklen_t;

# elif  TARGET_OS_VERSION == SUNOS_VERSION

extern "C" {
  int ioctl(int fd, int request, caddr_t arg);
  int select(int width, fd_set *readfds, fd_set *writefds, fd_set *exceptfds,
             struct timeval *timeout);
  int socket(int domain, int type, int protocol);
  int bind(int s, struct sockaddr *name, int namelen);
  void bcopy(char *b1, char *b2, int length);
  void bzero(char *b1, int length);
  int connect(int s, struct sockaddr *name, int namelen);
  struct hostent* gethostbyname(const char*);
  int syscall(int number, ...);
  int accept(...);
}

# else

# endif


# if  COMPILER != GCC_COMPILER  ||  TARGET_OS_VERSION == SOLARIS_VERSION 
extern "C" {
  int syscall(int number, /* arg */ ...);
}
# endif

const char *UnixFile_seal = "UnixFile";

fd_set activeFDs;                      // active file descriptors


static struct termios normalSettings;

class IOCleanup {
 public:
  IOCleanup()  {
# if TARGET_OS_VERSION ==  SUNOS_VERSION \
  || TARGET_OS_VERSION == SOLARIS_VERSION  \
  || TARGET_OS_VERSION ==   LINUX_VERSION
    if (isatty(STDIN))
      ioctl(STDIN, TCGETS, (caddr_t)&normalSettings);
# elif TARGET_OS_VERSION == MACOSX_VERSION
    // not sure what to do
    if (isatty(STDIN))
      ioctl(STDIN, TIOCGETA, (caddr_t)&normalSettings);
# else
  # error what?
# endif
    FD_SET(0, &activeFDs); FD_SET(1, &activeFDs); FD_SET(2, &activeFDs); 
  }
  ~IOCleanup() { resetTerminal(); }
};
static IOCleanup* ioC;        // make sure we exit in blocking mode etc

void resetTerminal() {
# if  TARGET_OS_VERSION ==  SUNOS_VERSION \
  ||  TARGET_OS_VERSION == SOLARIS_VERSION  \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION
    if (isatty(STDIN))
      ioctl(STDIN, TCSETS, (caddr_t)&normalSettings);
# elif TARGET_OS_VERSION == MACOSX_VERSION
    if (isatty(STDIN))
      ioctl(STDIN, TIOCSETA, (caddr_t)&normalSettings);
# else
    # error what?
# endif
  fcntl(STDIN, F_SETFL, 0);
}

// I tried a typedef here without success, must use the macro! Argh!!! - dmu 4/99
# if  TARGET_OS_VERSION == SOLARIS_VERSION

  // Emulate blocking write on a possibly non-blocking filedescriptor; this
  // is necessary because e.g. lprintf doesn't expect non-blocking files.

  typedef unsigned int nbytes_t;
  # if TARGET_ARCH != I386_ARCH
  extern "C"  int write(int, const void*, nbytes_t);
  extern "C"  int  _write(int fd, const void* b, nbytes_t nbytes) { return write(fd, b, nbytes); }
  extern "C" int _libc_write(int fd, const void* b, unsigned int nbytes) {
    return _write(fd, b, nbytes); }
  # endif

  static int c_lib_write(int fd, const char* buf, int nbytes) {
  # if TARGET_ARCH == I386_ARCH
    return write(fd, buf, nbytes);
  # else
    return _write(fd, buf, nbytes);
  # endif
  }

# elif  TARGET_OS_VERSION == SUNOS_VERSION

  // Emulate blocking write on a possibly non-blocking filedescriptor; this
  // is necessary because e.g. lprintf doesn't expect non-blocking files.
  extern "C" {
    int WRITE(int fd, const char* buf, int nbytes);
    // C library write()
  }
  static int c_lib_write(int fd, const char* buf, int nbytes) {
    return WRITE(fd, buf, nbytes);
  }

  typedef long unsigned int nbytes_t;
  
  
# elif  TARGET_OS_VERSION == MACOSX_VERSION

  // Emulate blocking write on a possibly non-blocking filedescriptor; this
  // is necessary because e.g. lprintf doesn't expect non-blocking files.
  
  // next statement is why we lose output from console, I bet -- dmu 12/02
  # if 0 // don't use WRITE for now, seems to work, WRITE is commented in runtime_asm_i386.S
    extern "C" {
      int WRITE(int fd, const char* buf, int nbytes);
      // C library write()
    }
    static int c_lib_write(int fd, const char* buf, int nbytes) {
      return WRITE(fd, buf, nbytes);
    }
    extern "C"  int  _write(int fd, const void* b, nbytes_t nbytes) { return write(fd, b, nbytes); }
    extern "C" int _libc_write(int fd, const void* b, unsigned int nbytes) {
      return _write(fd, b, nbytes); }
      
  # else
    static int c_lib_write(int fd, const char* buf, int nbytes) {
      return write(fd, buf, nbytes);
    }
  # endif  

# elif  TARGET_OS_VERSION == LINUX_VERSION
    static int c_lib_write(int fd, const char* buf, int nbytes) {
      return write(fd, buf, nbytes);
    }    
# else
  # error what?
# endif

 # if TARGET_OS_VERSION != MACOSX_VERSION  &&  TARGET_OS_VERSION != LINUX_VERSION

extern "C" int write(int fd, const void* b, nbytes_t nbytes) {
    int32 res;
    int32 bytesTransferred = 0;
    char* buf = (char*)b;
    do {
      res = c_lib_write(fd, buf + bytesTransferred, nbytes);
      if (res == -1) {
        # if  TARGET_OS_VERSION == SOLARIS_VERSION
          const int theError = EAGAIN;
        # elif  TARGET_OS_VERSION == SUNOS_VERSION
          const int theError = EWOULDBLOCK;
        # endif
        if (errno == theError) {
          // can't write right now, buffers are full; wait for 1ms
          // NOTE: don't use usleep!  It manipulates the timers and
          // sometimes forgets to restart them... arghhh!
          timeval millisecond;
          millisecond.tv_sec = 0; millisecond.tv_usec = 1000;
          select(0, NULL, NULL, NULL, &millisecond);
          errno = 0;
        } else {
          return -1;    // write error
        }
      } else {
        bytesTransferred += res;
        nbytes -= res;
      }
    } while (nbytes > 0);
    return bytesTransferred;
  }
 # endif


// to do a connect:
// s = socket_wrap(PF_INET, SOCK_STREAM, 0)
// bind_wrap(s, AF_INET, 0 /* any port in a storm*/, INADDR_ANY)
// connect(s, AF_INET, 0, INADDR_ANY)
  
  
// example: socket_wrap(PF_INET, SOCK_STREAM, 0);

int socket_wrap(int domain, int type, int protocol) {
  int s = socket(domain, type, protocol);
  register_file_descriptor(s);
  return s;
}


// set up socket address; only works for internet right now

static void set_sockaddr_in(struct sockaddr_in &a,
                       short family,
                       unsigned short port,
                       char* address,
                       int address_length,
                       void* FH) {
  if (family != AF_INET) {
    char buf[128];
    sprintf(buf, "bad family %d: only AF_INET (%d) supported",
            family,
            AF_INET);
    if (strlen(buf) >= sizeof(buf))
      fatal("buf too small");
    failure(FH, buf);
    return;
  }
  if (address_length < sizeof(long)) {
    char buf[128];
    sprintf(buf, "address is too short; (%d), must be >= sizeof(long)",
            address_length);
    if (strlen(buf) >= sizeof(buf))
      fatal("buf too small");
    failure(FH, buf);
    return;
  }

  long aLong;
#   if  TARGET_OS_VERSION == SOLARIS_VERSION  \
    ||  TARGET_OS_VERSION == MACOSX_VERSION   \
    ||  TARGET_OS_VERSION == LINUX_VERSION
    memcpy((char*) &aLong, address, sizeof(long));
    memset(a.sin_zero, 0, sizeof(a.sin_zero));
# elif  TARGET_OS_VERSION == SUNOS_VERSION
    bcopy(address, (char*) &aLong, sizeof(long));
    bzero(a.sin_zero, sizeof(a.sin_zero));
#   endif

     a.sin_family        = family;
     a.sin_port          = htons(port);
     a.sin_addr.s_addr   = aLong;
}

  
// bind, see /usr/include/netinet/in.h
// example (socket, port_no (1275), AF_INET, INADDR_ANY)

int bind_wrap(int socket,
              short family,
              unsigned short port,
              char* address,
              int   address_length,
              void *FH) {
  struct sockaddr_in a;
  set_sockaddr_in(a, family, port, address, address_length, FH);
  int res = bind(socket, (sockaddr*)&a, sizeof(a));

  if (res == -1) return -1;     /* Error in bind call. */

  if (port != 0) return port;

  /* We requested port 0, so the system chose a port number for us.
     As a favor to the caller, return this port number. 
     Yes, this is overloading of a single primitive. Agesen, June 1996.*/
  socklen_t len = sizeof(a);
  if (getsockname(socket, (sockaddr*)&a, &len) == -1) return -1;
  return ntohs(a.sin_port);
  
}


const int addrSize = sizeof(struct in_addr);

byteVectorOop addrAsByteVector(struct in_addr *addr) {
  byteVectorOop bv = Memory->byteVectorObj->cloneSize(addrSize);
  copy_bytes((char*)addr, bv->bytes(0), addrSize);
  return bv;
}


oop gethostbyname_wrap(char* name, void* FH) {
  int addrCount = 0;
  hostent* h = gethostbyname(name);

  if (h == NULL) { unix_failure(FH); return NULL; }
  for (char **p = h->h_addr_list; *p; p++) 
    addrCount++;
  objVectorOop res = Memory->objVectorObj->cloneSize(addrCount);

  char **p = h->h_addr_list;
  for (int i = 0; i < addrCount; i++) {
    /* Seems imposible to avoid this cast. */
    res->obj_at_put(i, addrAsByteVector((struct in_addr *)*p));
    p++;
  }
  return res;
}


char *gethostbyaddr_wrap(char *addr, int addrlen, int addrtype, void *FH) {
  struct hostent *h = gethostbyaddr(addr, addrlen, addrtype);
  if (!h) { unix_failure(FH, h_errno); return NULL; }
  return h->h_name; 
}


int connect_wrap(int socket,
                 short family,
                 unsigned short port,
                 char* address,
                 int   address_length,
                 void* FH) {

  struct sockaddr_in a;
  set_sockaddr_in(a, family, port, address, address_length, FH);

  /* for debugging:
    lprintf("before: socket %d, family %hd, port %hu, address %x, "
           "address_length %d\n",
           socket, family, port, *(int*)&address, address_length);
    lprintf("after: socket %d, sin_family %hd, sin_port %hu, sin_addr %x, "
           "sin_zero1 %d, sin_zero2 %d, len %d\n",
           socket, a.sin_family, a.sin_port, *(int*)&a.sin_addr,
           *(int*)&a.sin_zero[0], *(int*)&a.sin_zero[4], sizeof(a));
  */
  int r =  connect(socket, (sockaddr*)&a, sizeof(a));
  return r;
}
      


// to get a socket for accepting connections:

// s = socket_wrap(PR_INET, SOCK_STREAM, 0)
// bind_wrap(s, AF_INET, port_no (1275), INADDR_ANY)
// listen(s, queueSize? (5))


int accept_wrap(int sock, objVectorOop info) {

  struct sockaddr_in from;
  socklen_t len = sizeof(from);

  int acceptResult = accept(sock, (sockaddr*)&from, &len);

  if (acceptResult == -1) return -1;    /* Error return. */

  register_file_descriptor(acceptResult);

  /* Now fill in the 'info' structure with information about the
     accepted connection. */
  if (info->length_obj_array() >= 3) {
    info->obj_at_put(0, as_smiOop(ntohs(from.sin_port)), false);
    info->obj_at_put(1, as_smiOop(from.sin_family), false);
    info->obj_at_put(2, addrAsByteVector(&from.sin_addr));
  }

  return acceptResult;
}


// need to do this whenever we open a file so select will check it
    
extern "C"
void register_file_descriptor(int fd) {
    // check if this file will mess up select
    // (/dev/rsr0 does under SVR4) -- dmu

  if (fd < 0) return;

  timeval nowait;
  nowait.tv_sec = 0;
  nowait.tv_usec = 0;
  
  static fd_set zeroes; // static to initialize to zero
  fd_set r = zeroes, w = zeroes;
  FD_SET(fd, &r);
  FD_SET(fd, &w);
  
  if ( select(FD_SETSIZE, &r, &w, NULL, &nowait) < 0 )
    return;
  
  // end of check
  
  FD_SET(fd, &activeFDs);
  
}

int open_wrap(char *path, int flags, int mode) {
    int result = open(path, flags, mode);
    register_file_descriptor(result);
    return result;
  }




int close_wrap(int fd) {
  int result = close(fd);
  if (result != -1)
    FD_CLR(fd, &activeFDs);
  return result;
}


int select_wrap(objVectorOop vec, int howMany, void *FH) {
  if (howMany > vec->length_obj_array()) {
    prim_failure(FH, BADSIZEERROR);
    return 0;
  }
  if (howMany > FD_SETSIZE) 
    howMany = FD_SETSIZE;
  fd_set r = activeFDs, w = activeFDs;
  timeval nowait;
  nowait.tv_sec  = 0; 
  nowait.tv_usec = 0;
  if (select(howMany, &r, &w, NULL, &nowait) < 0) {
    unix_failure(FH);
    return 0;
  }
  // now extract all ready file descriptors
  int index= 0;
  for (int fd = 0; fd < FD_SETSIZE && index < howMany; fd++) {
    if (FD_ISSET(fd, &r) || FD_ISSET(fd, &w))
      vec->obj_at_put(index++, as_smiOop(fd), false);
  }
  return index;
}

int select_read_wrap(objVectorOop vec, int howMany, void *FH) {
  if (howMany > vec->length_obj_array()) {
    prim_failure(FH, BADSIZEERROR);
    return 0;
  }
  if (howMany > FD_SETSIZE) 
    howMany = FD_SETSIZE;
  fd_set r = activeFDs;
  timeval nowait;
  nowait.tv_sec  = 0; 
  nowait.tv_usec = 0;
  if (select(howMany, &r, NULL, NULL, &nowait) < 0) {
    unix_failure(FH);
    return 0;
  }
  // now extract all ready file descriptors
  int index= 0;
  for (int fd = 0; fd < FD_SETSIZE && index < howMany; fd++) {
    if (FD_ISSET(fd, &r))
      vec->obj_at_put(index++, as_smiOop(fd), false);
  }
  return index;
}


int system_wrap(char *cmd) {
  if (!IntervalTimer::use_real_instead_of_cpu_timer)  IntervalTimer::CPU_timer()->disable(false);
  IntervalTimer::Real_timer()->disable(false);
  int result = system(cmd);
  if (!IntervalTimer::use_real_instead_of_cpu_timer)  IntervalTimer::CPU_timer()->enable();
  IntervalTimer::Real_timer()->enable();
  return result;
}




int putenv_wrap(char *name, char *value) {
  char *buf= new char[strlen(name)+strlen(value)+2];
  strcpy(buf, name);
  strcat(buf, "=");
  strcat(buf, value);
  return putenv(buf); // buf become part of the env; don't deallocate
}


char *getcwd_wrap(void *FH) {
  static char path[MAXPATHLEN];
  char *r= getcwd(path, sizeof(path));
  if (strlen(path) >= sizeof(path))
    fatal("just checkin'");
  if (r == NULL) {
    unix_failure(FH);
    return NULL;
  }
  return path;
}

int read_wrap(int fd, char *buf, int buf_len, 
              int offset, int nbytes, void *FH) {
  if (offset < 0 ||  nbytes < 0) {
    prim_failure(FH, BADSIGNERROR);
    return 0;
  }
  if (offset + nbytes > buf_len) {
    prim_failure(FH, BADINDEXERROR);
    return 0;
  }
  return read(fd, buf + offset, nbytes);
}


int write_wrap(int fd, char *buf, int buf_len, 
               int offset, int nbytes, void *FH) {
  if (offset < 0 ||  nbytes < 0)
    prim_failure(FH, BADSIGNERROR);
  else if (offset + nbytes > buf_len)
    prim_failure(FH, BADINDEXERROR);
  else 
    return c_lib_write(fd, buf + offset, nbytes);
  return 0;
}



// Since the result of syscall uses all 32 bits of a long the result is 
// converted into a 4 element byteVector instead of a smiOop.
// Since most system calls return -1 for error, we assume -1 means failure.
// Self code can tell because errno will be zero if there were no error.

inline byteVectorOop convertLongToByteVector(long value,  void* FH) {
  if (value == -1)
    unix_failure(FH);
  
  byteVectorOop b = Memory->byteVectorObj->cloneSize(sizeof(long));
  b->byte_at_put(0, char((value & 0xff000000) >> 24));
  b->byte_at_put(1, char((value & 0x00ff0000) >> 16));
  b->byte_at_put(2, char((value & 0x0000ff00) >>  8));
  b->byte_at_put(3, char((value & 0x000000ff) >>  0));
  return b;
}

byteVectorOop syscall0(int n, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n), FH);}

byteVectorOop syscall1(int n, void* a0, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0), FH);}

byteVectorOop syscall2(int n,  void* a0, void* a1, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0, a1), FH);}

byteVectorOop syscall3(int n,  void* a0, void* a1, void* a2, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0, a1, a2), FH);}

byteVectorOop syscall4(int n,  void* a0, void* a1, void* a2, void* a3, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0, a1, a2, a3), FH);}

byteVectorOop syscall5(int n,  void* a0, void* a1, void* a2,
                       void* a3, void* a4, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0, a1, a2, a3, a4), FH);}

byteVectorOop syscall6(int n,  void* a0, void* a1, void* a2, void* a3,
                       void* a4, void* a5, void* FH) {
  errno = 0;
  return convertLongToByteVector(syscall(n, a0, a1, a2, a3, a4, a5), FH);}


void unixPrims_init() { ioC = new IOCleanup; }

void unixPrims_exit() { delete ioC; }

# if TARGET_OS_VERSION == MACOSX_VERSION \
  || TARGET_OS_VERSION ==  LINUX_VERSION
  static struct utsname my_utsname;
  char*  sysname_wrap(void* FH) { return uname(&my_utsname) ? (unix_failure(FH), (char*)NULL) : my_utsname. sysname; }
  char* nodename_wrap(void* FH) { return uname(&my_utsname) ? (unix_failure(FH), (char*)NULL) : my_utsname.nodename; }
  char*  release_wrap(void* FH) { return uname(&my_utsname) ? (unix_failure(FH), (char*)NULL) : my_utsname. release; }
  char*  version_wrap(void* FH) { return uname(&my_utsname) ? (unix_failure(FH), (char*)NULL) : my_utsname. version; }
  char*  machine_wrap(void* FH) { return uname(&my_utsname) ? (unix_failure(FH), (char*)NULL) : my_utsname. machine; }
# endif
    
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# include "_monitorHooks_unix.cpp.incl"


void TrackCHeapInMonitor::reset() { 
  // Do platform-dependent reset
  set_allocated( 0 );
}

/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "machineCache_unix.hh"
# include "_machineCache_unix.cpp.incl"


// define FLUSH_ALL (for debugging) to flush the complete I cache of nmethods
// and PICs very regularly

# ifdef FLUSH_ALL
  void MachineCache::flush_instruction_cache_for_debugging() {
    flush_instruction_cache_range(Memory->code->iZone->startAddr(),
                                  Memory->code->iZone->endAddr());
    flush_instruction_cache_range(Memory->code->stubs->zone()->startAddr(),
                                  Memory->code->stubs->zone()->endAddr());
}
# endif  

# define OVERDO_IT 0
# if TARGET_OS_VERSION == MACOSX_VERSION  \
  && (     TARGET_ARCH == PPC_ARCH \
       ||  TARGET_ARCH == I386_ARCH  &&  !OVERDO_IT )

  extern "C" { void MakeDataExecutable(void*, unsigned long); }

  void MachineCache::flush_instruction_cache_word(void* addr) {
    MakeDataExecutable(addr, sizeof(int32)); }

  void MachineCache::flush_instruction_cache_range(void* s, void* e) {
    MakeDataExecutable(s, (char*)e - (char*)s);
  }

# elif TARGET_OS_VERSION == MACOSX_VERSION \
    && TARGET_ARCH == I386_ARCH  &&  OVERDO_IT
  extern "C" { void MakeDataExecutable(void*, unsigned long); }
  void MachineCache::flush_instruction_cache_word(void* addr) { 
    const fint maxInstLen = 8; // a guess
    flush_instruction_cache_range(((char*)addr) - maxInstLen,  ((char*)addr) + maxInstLen);
  }

  void MachineCache::flush_instruction_cache_range(void* s, void* e) {
    MakeDataExecutable(s, (char*)e - (char*)s);

    // Could make this more efficient by depending on cache characteristics,
    // e.g. I-cache line size.  Also, should be a nop on machines without
    // split I/D caches (everything up to SS-2).

    const fint cacheLineSize = 8; // a guess

    char* start = (char*) ((int)s & ~(cacheLineSize - 1));
    char* end   = (char*) roundTo((int)e, cacheLineSize);
    for ( ; start < end; start += cacheLineSize) {
      FlushInstruction(start);  // FLUSH inst flushes (at least) a doubleword
    }
  }

# elif TARGET_ARCH == SPARC_ARCH

  void MachineCache::flush_instruction_cache_word(void* addr) { 
    FlushInstruction(addr); 
  }

  void MachineCache::flush_instruction_cache_range(void* s, void* e) {
    // Could make this more efficient by depending on cache characteristics,
    // e.g. I-cache line size.  Also, should be a nop on machines without
    // split I/D caches (everything up to SS-2).

    // Round s down to a double-word boundary
    char* start = (char*)(int(s) & ~(oopSize + oopSize - 1));
    char* end   = (char*)e;
    for ( ; start < end; start += 2 * sizeof(char*)) {
      FlushInstruction(start);  // FLUSH inst flushes (at least) a doubleword
    }
  }

# elif TARGET_OS_VERSION == LINUX_VERSION \
    && TARGET_ARCH       == I386_ARCH
  // Don't know if Linux does this; rely on I386 for now
  void MachineCache::flush_instruction_cache_word(void* addr) { }

  void MachineCache::flush_instruction_cache_range(void* s, void* e) {}

# elif TARGET_OS_VERSION == SOLARIS_VERSION \
    && TARGET_ARCH       == I386_ARCH
void MachineCache::flush_instruction_cache_word(void* addr) { }

void MachineCache::flush_instruction_cache_range(void* s, void* e) {}

# else
  # error for which machine?

# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include "_interruptedCtx_unix.cpp.incl"

# pragma implementation "interruptedCtx_unix.hh"


self_sig_context_t InterruptedContext::dummy_scp;


void InterruptedContext::set_the_self_thread() { 
  the_self_thread = pthread_self(); }

bool InterruptedContext::is_in_self_thread() {
  return pthread_self() == the_self_thread;
}

void InterruptedContext::must_be_in_self_thread() {
  if (CheckAssertions && !is_in_self_thread())
    fatal("must_be_in_self_thread");
}



bool InterruptedContext::forwarded_to_self_thread(int sig) {
  if (is_in_self_thread()) return false;
  if (pthread_kill(the_self_thread, sig)) {
    perror("pthread_kill");
    fatal("forwarded_to_self_thread failed");
  }
  return true;
}


bool InterruptedContext::in_read_trap()  { return  in_system_trap()  &&  system_trap() == SYS_read;  }
bool InterruptedContext::in_write_trap() { return  in_system_trap()  &&  system_trap() == SYS_write; }
/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# include "_monitorPieces_unix.cpp.incl"

 
const int one_billion = 1000 * 1000 * 1000;

ProcessTime ProcessTime::get_real_time() {
  ProcessTime res;
  struct timeval t;
  if (gettimeofday(&t, 0))       perror("unable to set real_time");
  res._secs      = t.tv_sec;
  res._nano_secs = t.tv_usec * 1000;
  return res;
}


# if  TARGET_OS_VERSION == SOLARIS_VERSION

static int       process_info_fd = -1;
static prusage_t process_info_pr;


void processInfo_init() {
  char buffer[12]; // len("/proc/NNNNN") + 1
  sprintf(buffer, "/proc/%05ld", getpid());
  process_info_fd = open(buffer, O_RDONLY);
}

void ProcessInfo::update() {
  if (ioctl(process_info_fd, PIOCUSAGE, &process_info_pr) == -1) 
    perror("ioctl");
}

int ProcessInfo::page_faults_IO()             { return process_info_pr.pr_majf;  }
int ProcessInfo::page_faults_NonIO()          { return process_info_pr.pr_minf;  }
int ProcessInfo::block_input_operations()     { return process_info_pr.pr_inblk; }
int ProcessInfo::block_output_operations()    { return process_info_pr.pr_oublk; }
int ProcessInfo::forced_context_switches()    { return process_info_pr.pr_ictx;  }
int ProcessInfo::voluntary_context_switches() { return process_info_pr.pr_vctx;  }

ProcessTime ProcessInfo::user_time() {
  ProcessTime t;
  t._secs      = process_info_pr.pr_utime.tv_sec;
  t._nano_secs = process_info_pr.pr_utime.tv_nsec;
  return t;
}
ProcessTime ProcessInfo::system_time(){
  ProcessTime t;
  t._secs      = process_info_pr.pr_stime.tv_sec;
  t._nano_secs = process_info_pr.pr_stime.tv_nsec;
  return t;
} 

# elif  TARGET_OS_VERSION == SUNOS_VERSION  \
  ||  TARGET_OS_VERSION ==   LINUX_VERSION

static struct rusage process_info_ru;

extern "C" int getrusage(int who, struct rusage *rusage);

void processInfo_init() { }

void ProcessInfo::update() { 
  if (getrusage(RUSAGE_SELF, &process_info_ru)) perror("getrusage");
}

int ProcessInfo::page_faults_IO()             { return process_info_ru.ru_majflt;  }
int ProcessInfo::page_faults_NonIO()          { return process_info_ru.ru_minflt;  }
int ProcessInfo::block_input_operations()     { return process_info_ru.ru_inblock; }
int ProcessInfo::block_output_operations()    { return process_info_ru.ru_oublock; }
int ProcessInfo::forced_context_switches()    { return process_info_ru.ru_nivcsw;  }
int ProcessInfo::voluntary_context_switches() { return process_info_ru.ru_nvcsw;   }

ProcessTime ProcessInfo::user_time() {
  ProcessTime t;
  t._secs      = process_info_ru.ru_utime.tv_sec;
  t._nano_secs = process_info_ru.ru_utime.tv_usec * 1000;
  return t;
}

ProcessTime ProcessInfo::system_time(){
  ProcessTime t;
  t._secs      = process_info_ru.ru_stime.tv_sec;
  t._nano_secs = process_info_ru.ru_stime.tv_usec * 1000;
  return t;
} 

# elif  TARGET_OS_VERSION == MACOSX_VERSION

// Use Mach facilities; could try SUNOS version, too.
// -- dmu 2/04

# include <mach/mach.h>
# include <mach/mach_error.h>

static thread_basic_info my_thread_info;
static  task_events_info   my_task_info;

void processInfo_init() {
}

void ProcessInfo::update() {
  SignalBlocker sb; // attempt to head off weird mach errors
  
  unsigned int thread_info_count = THREAD_BASIC_INFO_COUNT;
  unsigned int   task_info_count =  TASK_EVENTS_INFO_COUNT;
  
  kern_return_t thread_err, task_err;
  thread_err = thread_info( mach_thread_self(),  THREAD_BASIC_INFO,  (thread_info_t)&my_thread_info,  &thread_info_count);
    task_err =   task_info( mach_task_self(),     TASK_EVENTS_INFO,    (task_info_t)&  my_task_info,  &  task_info_count);
    
  static bool aw = true,  hw = true;
  if (   task_err == MIG_ARRAY_TOO_LARGE  ||   task_err == MACH_RCV_TOO_LARGE) { if (aw) {aw = false; warning("  task_info has grown"); }    task_err = KERN_SUCCESS; }
  if ( thread_err == MIG_ARRAY_TOO_LARGE  || thread_err == MACH_RCV_TOO_LARGE) { if (hw) {hw = false; warning("thread_info has grown"); }  thread_err = KERN_SUCCESS; }
  
  // I don't know why this sometimes fails, but it's not fatal. -- dmu 2/04
  if (  task_err != KERN_SUCCESS  &&  !aw)  {aw = true; warning1("Error calling   task_info() %s\n", mach_error_string(  task_err)); }
  if (thread_err != KERN_SUCCESS  &&  !hw)  {hw = true; warning1("Error calling thread_info() %s\n", mach_error_string(thread_err)); }
}


int ProcessInfo::page_faults_IO()             { return my_task_info.faults;  }
int ProcessInfo::page_faults_NonIO()          { return 0;  }
int ProcessInfo::block_input_operations()     { return 0; }
int ProcessInfo::block_output_operations()    { return 0; }
int ProcessInfo::forced_context_switches()    { return my_task_info.csw / 2;  } // all switches
int ProcessInfo::voluntary_context_switches() { return my_task_info.csw / 2;  }

// could add (Mach-specific):
//  struct task_events_info {
//          integer_t       faults;         /* number of page faults */
//          integer_t       pageins;        /* number of actual pageins */
//          integer_t       cow_faults;     /* number of copy-on-write faults */
//          integer_t       messages_sent;  /* number of messages sent */
//          integer_t       messages_received; /* number of messages received */
//          integer_t       syscalls_mach;  /* number of mach system calls */
//          integer_t       syscalls_unix;  /* number of unix system calls */
//          integer_t       csw;            /* number of context switches */
//  };

ProcessTime ProcessInfo::user_time() {
  ProcessTime t;
  t._secs      = my_thread_info.user_time.seconds;
  t._nano_secs = my_thread_info.user_time.microseconds * 1000;
  return t;
}

ProcessTime ProcessInfo::system_time() {
  ProcessTime t;
  t._secs      = my_thread_info.system_time.seconds;
  t._nano_secs = my_thread_info.system_time.microseconds * 1000;
  return t;
} 

# else
  # error which?

#endif



void LoadLevelMonitor::compute_load_level() {
  static timeval oldT;
  
  timeval t;
  if (gettimeofday(&t, 0) != 0)
    return;
   
  int32 usecs = 1000000 * (t.tv_sec  - oldT.tv_sec) +
                          (t.tv_usec - oldT.tv_usec);
  oldT = t;
  load_level = 100 * 1000000 / usecs;
}


void InterruptedFrameMonitor::get_frame_and_pc( frame*& f, char*& pc ) {
  if (!InterruptedContext::the_interrupted_context->is_set()) {
    f = NULL;  pc = NULL;
    return;
  }
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  

  f  = InterruptedContext::the_interrupted_context->sp();
  pc = InterruptedContext::the_interrupted_context->pc();
}

/* Sun-$Revision: 30.3 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "search_sparc.hh"
# include "_search_sparc.cpp.incl"
/* Sun-$Revision: 30.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "countPattern_sparc.hh"
# include "_countPattern_sparc.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
 

  void CountCodePattern::initCounting() {
    // general count stub; 7 instrs = 12 cycles on SS-2.  Could be reduced to
    // 6 insts / 10 cy if one additional register were available (e.g. could
    // use %o5 for sends with < 5 args).
    // 0: sethi count_addr, Temp1
    // 1: ld    [Temp1+lo(count_addr)], Temp2
    // 2: inc   Temp2
    // 3: st    Temp2, [Temp1+lo(count_addr)]
    // 4: sethi jump_addr, Temp1
    // 5: jmpl   Temp1 + low(jump_addr), g0
    // 6: nop
    instsSize = 7 * 4;
    countAddr_offset = 0;
    ld_offset = 1;
    st_offset = 3;
    nm_sethi_offset = 4;
    nm_jmp_offset = 5;
    limit_sethi_offset = countAddr_offset2 = recompile_offset = BadOffset;

    Assembler* a = new Assembler(instsSize, instsSize, false, true);
    a->SetHiA(0, Temp1);
    a->LoadI(Temp1, 0, Temp2);
    a->AddI(Temp2, 1, Temp2);
    a->StoreI(Temp1, 0, Temp2);
    a->SetHiX(0, CodeAddressOperand, Temp1);
    a->JmpLC(Temp1, 0, G0);
    a->Nop();

    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_words((int32*)a->instsStart, (int32*)pattern, instsSize / 4);
    a->finalize();
  }


  void CountCodePattern::initComparing() {
    instsSize = 13 * 4;
    // 0: sethi count_addr, Temp1
    // 1: ld    [Temp1+lo(count_addr)], Temp2
    // 2: setHi limit, Temp1
    // 3: inc   Temp2
    // 4: cmp   Temp2, Temp1
    // 5: sethi count_addr, Temp1
    // 6: st    Temp2, [Temp1+lo(count_addr)]
    // 7: beq   10
    // 8: sethi jump_addr, Temp1
    // 9: jmpl   Temp1 + low(jump_addr), g0
    //10: sethi recompile_addr, Temp2
    //11: jmpl   Temp2 + low(recompile_addr), linkReg
    //12: nop
    countAddr_offset = 0;
    countAddr_offset2 = 5;
    ld_offset = 1;
    st_offset = 6;
    nm_sethi_offset = 8;
    nm_jmp_offset = 9;
    limit_sethi_offset = 2;
    recompile_offset = 10;

    Assembler* a = new Assembler(instsSize, instsSize, false, true);
    a->SetHiA(0, Temp1);
    a->LoadI(Temp1, 0, Temp2);
    a->SetHiA(0, Temp1);
    a->AddI(Temp2, 1, Temp2);
    a->SubCCR(Temp2, Temp1, G0);
    a->SetHiA(0, Temp1);
    a->StoreI(Temp1, 0, Temp2);
    Label* l = a->BgeForward(false);
    a->SetHiX(0, CodeAddressOperand, Temp1);
    a->JmpLC(Temp1, 0, G0);
    l->define();
    a->SetHiP(Memory->code->trapdoors->Recompile_stub_td(), Temp2);
    a->JmpLP(Temp2, Memory->code->trapdoors->Recompile_stub_td(), RecompileLinkReg);
    a->Nop();
    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_words((int32*)a->instsStart, (int32*)pattern, instsSize / 4);
    a->finalize();
  }
    

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "countStub_sparc.hh"
# include "_countStub_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void AgingStub::initPattern() {
    Assembler* a = new Assembler(oopSize, oopSize, false, true);
    a->AddI(G0, 0, Temp1);
    add_inst = *(int32*)a->instsStart;
    a->finalize();
  }


  pc_t CountStub::jump_addr() {
    CountCodePattern* patt = CountStub::pattern[countType()];
    int32* p = (int32*)insts();
    assert(isSetHi(p + patt->nm_sethi_offset), "wrong pattern");
    assert(isJump (p + patt->nm_jmp_offset), "wrong pattern");
    pc_t n = (pc_t)getSetHiImm(p + patt->nm_sethi_offset);
    n += getArithImm(p + patt->nm_jmp_offset);
    return n;
  }




  void ComparingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts();
    set_recompile_addr(Memory->code->trapdoors->Recompile_stub_td());
    set_count_addr(patt, (int32)&sendCounts[id()]);
    assert(isSetHi(p + patt->limit_sethi_offset), "wrong pattern");
    fint limit = recompileLimit(nm->level());
    setSetHiImm(p + patt->limit_sethi_offset, limit);
  }

  void AgingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts();
    set_recompile_addr(first_inst_addr(MakeOld_stub));
    set_count_addr(patt, (int32)&sendCounts[id()]);
    fint limit = nm->agingLimit();
    if (limit > 1023) {
      setSetHiImm(p + patt->limit_sethi_offset, limit);
    } else {
      // setHi would set to 0, so use an add instead
      p[patt->limit_sethi_offset] = add_inst;
      setArithImm(p + patt->limit_sethi_offset, limit);
    }
    set_count(1);
  }


# ifdef UNUSED
  pc_t ComparingStub::get_recompile_addr() {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts() + patt->recompile_offset;
    assert(isSetHi(p), "wrong pattern");
    assert(isJump(p + 1), "strange instruction after setHi");
    int32 val = getSetHiImm(p) + getArithImm(p + 1);
    return (pc_t)val;
  }
# endif
  
  void ComparingStub::set_recompile_addr(pc_t addr) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)insts() + patt->recompile_offset;
    assert(isSetHi(p), "wrong pattern");
    assert(isJump(p + 1), "strange instruction after setHi");
    setSetHiImm(p    , int32(addr));
    setArithImm(p + 1, int32(addr));
  }
  

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "trapdoors_sparc.hh"
# include "_trapdoors_sparc.cpp.incl"

Trapdoors::Trapdoors(pc_t, int32) {}

int32 Trapdoors::trapdoor_bytes() { return 0; }

pc_t Trapdoors::  SendMessage_stub_td(Location) { return first_inst_addr(   ::SendMessage_stub); }
pc_t Trapdoors::SendDIMessage_stub_td(Location) { return first_inst_addr( ::SendDIMessage_stub); }
pc_t Trapdoors::    Recompile_stub_td(Location) { return first_inst_addr(     ::Recompile_stub); }
pc_t Trapdoors::  DIRecompile_stub_td(Location) { return first_inst_addr(   ::DIRecompile_stub); }

pc_t Trapdoors::follow_trapdoors(pc_t target) { return target; } // no trapdoors
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "addrDesc_sparc.hh"

# include "_addrDesc_sparc.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  bool addrDesc::isShiftNeededAfterMovingMe(OopNCode* m) {
    return isBranch((inst_t*)addr(m));
  }
  
  
  pc_t addrDesc::instr_referent(OopNCode* m) {
    int32* instp = (int32*)addr(m);
    assert(m->contains(instp), "not in this nmethod");
    int32 val;
    
    if (::isCall(instp)) {
      val = getCallImm(instp);

    } else {
      assert(isSetHi(instp), "strange instruction for addr desc");

      int32* inst2p;
      if (inDelaySlot()) {
        // set-hi in branch delay slot
        int32* inst1p = &instp[-1];
        assert(isBranch(inst1p) &&
               (isConditionalBranch(inst1p) ?
                annulled(inst1p) : !annulled(inst1p)),
               "unexpected previous instruction");
        inst2p = getBranchTarget(inst1p);
      } else {
        inst2p = &instp[1];
      }
      while (isBranch(inst2p)) {
        if (isConditionalBranch(inst2p) || !annulled(inst2p)) {
          // skip into delay slot of call instruction after set-hi
          inst2p ++;
          assert(!isBranch(inst2p), "shouldn't have a branch in the delay slot");
          break;
        } else {
          // go to target of unconditional branch
          inst2p = getBranchTarget(inst2p);
        }
      }
      if (::isCall(inst2p) ||
          isJump(inst2p) && getSetHiD(instp) != getArithS1(inst2p)) {
        // skip into delay slot of call/unrelated jump instruction after set-hi
        inst2p ++;
      }
      assert(isAdd(inst2p) || isOr(inst2p) ||
             isLoadStore(inst2p) || isJump(inst2p),
             "strange instruction after setHi");
      assert(isImmed(inst2p), "setHi must immediately preceed rest of address");
      assert(getSetHiD(instp) == getArithS1(inst2p),
             "setHi and arith/jump/load/store should use the same register");
      
      val = getSetHiImm(instp);
      
      int32 val2 = getArithImm(inst2p);
      if (isOr(inst2p)) {
        val |= val2;
      } else {
        val += val2;
      }
    }
    
    return (pc_t)val;
  }
  
  
  void addrDesc::set_instr_referent(OopNCode* m, void* newVal) {
    int32* instp = (int32*)addr(m);
    assert(m->contains(instp), "not in this nmethod");
    
    if (::isCall(instp)) {
      setCallImm(instp, int32(newVal));

    } else {
      assert(isSetHi(instp), "strange instruction for addr desc");

      setSetHiImm(instp, int32(newVal));
      int32* inst2p;
      if (inDelaySlot()) {
        // set-hi in branch delay slot
        int32* inst1p = &instp[-1];
        assert(isBranch(inst1p) &&
               (isConditionalBranch(inst1p) ?
                annulled(inst1p) : !annulled(inst1p)),
               "unexpected previous instruction");
        inst2p = getBranchTarget(inst1p);
      } else {
        inst2p = &instp[1];
      }
      while (isBranch(inst2p)) {
        if (isConditionalBranch(inst2p) || !annulled(inst2p)) {
          // skip into delay slot of call instruction after set-hi
          inst2p ++;
          assert(!isBranch(inst2p), "shouldn't have a branch in the delay slot");
          break;
        } else {
          // go to target of unconditional branch
          inst2p = getBranchTarget(inst2p);
        }
      }
      if (::isCall(inst2p) ||
          isJump(inst2p) && getSetHiD(instp) != getArithS1(inst2p)) {
        // skip into delay slot of call/unrelated jump instruction after set-hi
        inst2p ++;
      }
      assert(isAdd(inst2p) || isOr(inst2p) ||
             isLoadStore(inst2p) || isJump(inst2p),
             "strange instruction after setHi");
      assert(isImmed(inst2p), "setHi must immediately preceed rest of address");
      assert(getSetHiD(instp) == getArithS1(inst2p),
             "setHi and arith/jump/load/store should use the same register");
      setArithImm(inst2p, int32(newVal));
    }
  }
  
  
  void addrDesc::relocateTarget(OopNCode* m, int32 delta) {
    int32* instp = (int32*)addr(m);
//    if (! m->contains(instp)) return;
    assert(m->contains(instp), "if not here, where?");
    if (::isCall(instp)) {
      int32 val= getCallImm(instp);
      val -= delta;
      setCallImm(instp, val);
    }
  }
  

bool addrDesc::verify(nmethod* m) {
  bool flag = true;
  if (offset() >= m->instsLen() + m->scopes->length()) {
    error1("bad offset in addrDesc at %#lx", (long)this);
    flag = false;
  }
  if (isUncommonTrap()) {
      if (!isUnimp((int32*)addr(m))) {
        error1("bad unimp addrDesc at %#lx", (long)this);
        flag = false;
      }
  } 
  else if (!oop(referent(m))->verify_oop()) {
    lprintf("\tin addrDesc at %#lx\n", (long)this);
    flag = false;
  }
  if (isDIDesc() &&
      !asDIDesc(m)->dependency()->verify_list_integrity()) {
    lprintf("\tin bad di cache addrDesc at %#lx\n", (long)this);
    flag = false;
  }
  return flag;
}


// not inlined to reduce .h dependencies
sendDesc* addrDesc::asSendDesc(OopNCode* m) {
  assert(isSendDesc(), "not a sendDesc location");
  return sendDesc::sendDesc_from_addrDesc_addr(addr(m));
}
  
sendDesc* addrDesc::asPrimitiveSendDesc(OopNCode* m) {
  // note that it's not really an inline cache, just a primitive call
  assert(isPrimitive(), "not a primitive location");
  return sendDesc::sendDesc_from_addrDesc_addr(addr(m));
}
  
DIDesc* addrDesc::asDIDesc(nmethod* m) {
  assert(isDIDesc(), "not a diDesc location");
  return DIDesc::DIDesc_from_addrDesc_addr(addr(m));
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "nmethod_sparc.hh"

# include "_nmethod_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

void nmethod::get_platform_specific_data(AbstractCompiler* ) {}

void nmethod::print_platform_specific_data() {}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "fields_sparc.hh"  

# include "_fields_sparc.cpp.incl"



int32 getJumpImm(int32* instp) {
  assert(isJump(instp), "expecting a jmpl");
  assert(isImmed(instp), "expecting an offset");
  assert(isSetHi(instp - 1), "expecting a sethi before the jmpl");
  int32 val = getSetHiImm(instp - 1);
  val += getArithImm(instp);
  return val;
}

void setJumpImm(int32* instp, int32 val) {
  assert(isJump(instp), "expecting a jmpl");
  assert(isImmed(instp), "expecting an offset");
  assert(isSetHi(instp - 1), "expecting a sethi before the jmpl");
  setSetHiImm(instp - 1, val);
  setArithImm(instp, val);
  MachineCache::flush_instruction_cache_word(instp - 1);
  MachineCache::flush_instruction_cache_word(instp);
}


char* address_of_overwritten_NIC_save_instruction(int32* orig_save_addr) {
 if ( op(orig_save_addr)  ==  02  &&  op3(orig_save_addr) == 074) 
   return NULL; // not overwritten
   
  // the instruction must have been patched with a branch;
  assert(op(orig_save_addr)   ==  00 &&
         cond(orig_save_addr) == 010 &&
         op2(orig_save_addr)  ==  02,
         "must be a unconditional branch instruction");
         
  return ((char*) orig_save_addr) + disp(orig_save_addr);
}


void check_branch_relocation( void*, void*, int32) {
  return; // not a problem on SPARC (as far as I know)
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "asm_sparc.hh"
# pragma implementation "asm_inline_sparc.hh"  
  
# include "_asm_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  Assembler* theAssembler;      // current assembler for instructions

  char* CondNames[] = {
    "n", "eq", "le", "lt", "leu", "cs", "neg", "vs",
    "a", "ne", "gt", "ge", "gtu", "cc", "pos", "vc"
    };


# ifdef SIC_COMPILER
    void Assembler::Unimp(fint n, bool shouldRestart) {
      // add an addrDesc for the unimp instruction so they're easier to find
      int32 mask = offset();
      genLoc(mask | addrDesc::isUncommonTrapMask);
      assert(n < MaxUnimpImm / 2, "unimp immediate too large");
      if (shouldRestart) n |= UncommonRestartBit;
      Data(n);
    }
# endif

# ifdef SIC_COMPILER
    // for statistics
    // encode type tests with trigger instructions; when changing these, be
    // sure to change 1st instr of SendMessage_stub as well

    void Assembler::startTypeTest(fint ncases, bool prologueCheck,
                                  bool immedOnly) {
      fint immed = 0;
      if (prologueCheck) immed |= tt_prologueMask;
      if (immedOnly)     immed |= tt_immediateOnlyMask;
      immed |= (ncases & tt_arityMask);
      OrNI(G0, immed, G0);
    }
    
    void Assembler::doOneTypeTest() { NxorR(G0, G0, G0); }
    void Assembler::endTypeTest()   { XorR(G0, G0, G0); }
    void Assembler::markTagTest(fint n, bool isArith) {
      n = (n << 1) | (isArith ? 1 : 0);
      XorI(G0, n, G0);
    }
    
# endif


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.3 $ */

// Adapted from GNU gdb 4.7    - Urs 12/92
// When editing this file, try not to reindent anything so diffs with
// future versions of gdb are simpler.


/* Print SPARC instructions for GDB, the GNU Debugger.
   Copyright 1989, 1991, 1992 Free Software Foundation, Inc.

This file is part of GDB, the GNU debugger.

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.  */

# pragma implementation "pinsn.hh"

#include "_pinsn_sparc.cpp.incl"



# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


union sparc_insn
  {
    unsigned long int code;
    struct
      {
        unsigned int anop:2;
#define op      ldst.anop
        unsigned int anrd:5;
#define rd      ldst.anrd
        unsigned int op3:6;
        unsigned int anrs1:5;
#define rs1     ldst.anrs1
        unsigned int i:1;
        unsigned int anasi:8;
#define asi     ldst.anasi
        unsigned int anrs2:5;
#define rs2     ldst.anrs2
#define shcnt   rs2
      } ldst;
    struct
      {
        unsigned int anop:2, anrd:5, op3:6, anrs1:5, i:1;
        unsigned int IMM13:13;
#define imm13   IMM13.IMM13
      } IMM13;
    struct
      {
        unsigned int anop:2;
        unsigned int a:1;
        unsigned int cond:4;
        unsigned int op2:3;
        unsigned int DISP22:22;
#define disp22  branch.DISP22
      } branch;
#define imm22   disp22
    struct
      {
        unsigned int anop:2;
        unsigned int adisp30:30;
#define disp30  call.adisp30
      } call;
  };

/* Nonzero if INSN is the opcode for a delayed branch.  */
static int
is_delayed_branch (
     union sparc_insn insn)
{
  unsigned int i;

  for (i = 0; i < NUMOPCODES; ++i)
    {
      const struct sparc_opcode *opcode = &sparc_opcodes[i];
      if ((opcode->match & insn.code) == opcode->match
          && (opcode->lose & insn.code) == 0)
        return (opcode->flags & F_DELAYED);
    }
  return 0;
}

static int opcodes_sorted = 0;

static int compare_opcodes (char*, char*);

/* Print one instruction from MEMADDR on STREAM.

   We suffix the instruction with a comment that gives the absolute
   address involved, as well as its symbolic form, if the instruction
   is preceded by a findable `sethi' and it either adds an immediate
   displacement to that register, or it is an `add' or `or' instruction
   on that register.  */
int
print_insn (CORE_ADDR memaddr, FILE *stream)
{
  union sparc_insn insn;

  register unsigned int i;

  if (!opcodes_sorted)
    {
      typedef int (*cmpFn)(const void *, const void *);
      qsort ((char *) sparc_opcodes, NUMOPCODES,
             sizeof (sparc_opcodes[0]), (cmpFn)compare_opcodes);
      opcodes_sorted = 1;
    }

  read_memory (memaddr, (char *) &insn, sizeof (insn));

  for (i = 0; i < NUMOPCODES; ++i)
    {
      const struct sparc_opcode *opcode = &sparc_opcodes[i];
      if ((opcode->match & insn.code) == opcode->match
          && (opcode->lose & insn.code) == 0)
        {
          /* Nonzero means that we have found an instruction which has
             the effect of adding or or'ing the imm13 field to rs1.  */
          int imm_added_to_rs1 = 0;

          /* Nonzero means that we have found a plus sign in the args
             field of the opcode table.  */
          int found_plus = 0;
          
          /* Do we have an `add' or `or' instruction where rs1 is the same
             as rsd, and which has the i bit set?  */
          if ((opcode->match == 0x80102000 || opcode->match == 0x80002000)
          /*                      (or)                           (add)  */
              && insn.rs1 == insn.rd)
            imm_added_to_rs1 = 1;

          if (insn.rs1 != insn.rd
              && strchr (opcode->args, 'r') != 0)
              /* Can't do simple format if source and dest are different.  */
              continue;

          fputs_filtered (opcode->name, stream);

          {
            register const char *s;

            if (opcode->args[0] != ',')
              fputs_filtered (" ", stream);
            for (s = opcode->args; *s != '\0'; ++s)
              {
                if (*s == ',')
                  {
                    fputs_filtered (",", stream);
                    ++s;
                    if (*s == 'a')
                      {
                        fputs_filtered ("a", stream);
                        ++s;
                      }
                    fputs_filtered (" ", stream);
                  }

                switch (*s)
                  {
                  case '+':
                    found_plus = 1;

                    /* note fall-through */
                  default:
                    fprintf_filtered (stream, "%c", *s);
                    break;

                  case '#':
                    fputs_filtered ("0", stream);
                    break;

#define reg(n)  fprintf_filtered (stream, "%%%s", reg_names(n))
                  case '1':
                  case 'r':
                    reg (insn.rs1);
                    break;

                  case '2':
                    reg (insn.rs2);
                    break;

                  case 'd':
                    reg (insn.rd);
                    break;
#undef  reg

#define freg(n) fprintf_filtered (stream, "%%%s", freg_names(n))
                  case 'e':
                  case 'v':     /* double/even */
                  case 'V':     /* quad/multiple of 4 */
                    freg (insn.rs1);
                    break;

                  case 'f':
                  case 'B':     /* double/even */
                  case 'R':     /* quad/multiple of 4 */
                    freg (insn.rs2);
                    break;

                  case 'g':
                  case 'H':     /* double/even */
                  case 'J':     /* quad/multiple of 4 */
                    freg (insn.rd);
                    break;
#undef  freg

#define creg(n) fprintf_filtered (stream, "%%c%u", (unsigned int) (n))
                  case 'b':
                    creg (insn.rs1);
                    break;

                  case 'c':
                    creg (insn.rs2);
                    break;

                  case 'D':
                    creg (insn.rd);
                    break;
#undef  creg

                  case 'h':
                    fprintf_filtered (stream, "%%hi(%#x)",
                                      (int) insn.imm22 << 10);
                    break;

                  case 'i':
                    {
                      /* We cannot trust the compiler to sign-extend
                         when extracting the bitfield, hence the shifts.  */
                      int imm = ((int) insn.imm13 << 19) >> 19;

                      /* Check to see whether we have a 1+i, and take
                         note of that fact.

                         FIXME: No longer true/relavant ??
                         Note: because of the way we sort the table,
                         we will be matching 1+i rather than i+1,
                         so it is OK to assume that i is after +,
                         not before it.  */
                      if (found_plus)
                        imm_added_to_rs1 = 1;
                      
                      if (imm <= 9)
                        fprintf_filtered (stream, "%d", imm);
                      else
                        fprintf_filtered (stream, "%#x", imm);

                      if (found_plus && insn.rs1 == SP)
                        print_stack_temp_name(imm);
                    }
                    break;

                  case 'L':
                    print_address ((CORE_ADDR) memaddr + insn.disp30 * 4);
                    break;

                  case 'l':
                  case 'n':         // to make unimp work
                    if ((insn.code >> 22) == 0)
                      /* Special case for `unimp'.  Don't try to turn
                         it's operand into a function offset.  */
                      fprintf_filtered (stream, "%#x",
                                        (int) (((unsigned) insn.disp22 << 10) >> 10));
                    else
                      /* We cannot trust the compiler to sign-extend
                         when extracting the bitfield, hence the shifts.  */
                      print_address ((CORE_ADDR)
                                     (memaddr
                                      + (((int) insn.disp22 << 10) >> 10) * 4));
                    break;

                  case 'A':
                    fprintf_filtered (stream, "(%d)", (int) insn.asi);
                    break;

                  case 'C':
                    fputs_filtered ("%csr", stream);
                    break;

                  case 'F':
                    fputs_filtered ("%fsr", stream);
                    break;

                  case 'p':
                    fputs_filtered ("%psr", stream);
                    break;

                  case 'q':
                    fputs_filtered ("%fq", stream);
                    break;

                  case 'Q':
                    fputs_filtered ("%cq", stream);
                    break;

                  case 't':
                    fputs_filtered ("%tbr", stream);
                    break;

                  case 'w':
                    fputs_filtered ("%wim", stream);
                    break;

                  case 'y':
                    fputs_filtered ("%y", stream);
                    break;
                  }
              }
          }

          /* If we are adding or or'ing something to rs1, then
             check to see whether the previous instruction was
             a sethi to the same register as in the sethi.
             If so, attempt to print the result of the add or
             or (in this context add and or do the same thing)
             and its symbolic value.  */
          if (imm_added_to_rs1)
            {
              union sparc_insn prev_insn;
              int errcode;

              errcode = target_read_memory (memaddr - 4,
                                     (char *)&prev_insn, sizeof (prev_insn));

              if (errcode == 0)
                {
                  /* If it is a delayed branch, we need to look at the
                     instruction before the delayed branch.  This handles
                     sequences such as

                     sethi %o1, %hi(_foo), %o1
                     call _printf
                     or %o1, %lo(_foo), %o1
                     */

                  if (is_delayed_branch (prev_insn))
                    errcode = target_read_memory
                      (memaddr - 8, (char *)&prev_insn, sizeof (prev_insn));
                }

              /* If there was a problem reading memory, then assume
                 the previous instruction was not sethi.  */
              if (errcode == 0)
                {
                  /* Is it sethi to the same register?  */
                  if ((prev_insn.code & 0xc1c00000) == 0x01000000
                      && prev_insn.rd == insn.rs1)
                    {
                      fprintf_filtered (stream, "\t! ");
                      /* We cannot trust the compiler to sign-extend
                         when extracting the bitfield, hence the shifts.  */
                      print_address ((CORE_ADDR)
                                     (((int) prev_insn.imm22 << 10)
                                     | (insn.imm13 << 19) >> 19));
                    }
                }
            }

          return sizeof (insn);
        }
    }

  printf_filtered ("%#8lx", insn.code);
  return sizeof (insn);
}

/* Compare opcodes A and B.  */

static int
compare_opcodes (char* a, char* b)
{
  struct sparc_opcode *op0 = (struct sparc_opcode *) a;
  struct sparc_opcode *op1 = (struct sparc_opcode *) b;
  unsigned long int match0 = op0->match, match1 = op1->match;
  unsigned long int lose0 = op0->lose, lose1 = op1->lose;
  register unsigned int i;

  /* If a bit is set in both match and lose, there is something
     wrong with the opcode table.  */
  if (match0 & lose0)
    {
      fprintf (stderr, "Internal error:  bad sparc-opcode.h: \"%s\", %#.8lx, %#.8lx\n",
               op0->name, match0, lose0);
      op0->lose &= ~op0->match;
      lose0 = op0->lose;
    }

  if (match1 & lose1)
    {
      fprintf (stderr, "Internal error: bad sparc-opcode.h: \"%s\", %#.8lx, %#.8lx\n",
               op1->name, match1, lose1);
      op1->lose &= ~op1->match;
      lose1 = op1->lose;
    }

  /* Because the bits that are variable in one opcode are constant in
     another, it is important to order the opcodes in the right order.  */
  for (i = 0; i < 32; ++i)
    {
      unsigned long int x = 1 << i;
      int x0 = (match0 & x) != 0;
      int x1 = (match1 & x) != 0;

      if (x0 != x1)
        return x1 - x0;
    }

  for (i = 0; i < 32; ++i)
    {
      unsigned long int x = 1 << i;
      int x0 = (lose0 & x) != 0;
      int x1 = (lose1 & x) != 0;

      if (x0 != x1)
        return x1 - x0;
    }

  /* They are functionally equal.  So as long as the opcode table is
     valid, we can put whichever one first we want, on aesthetic grounds.  */

  /* Our first aesthetic ground is that aliases defer to real insns.  */
  {
    int alias_diff = (op0->flags & F_ALIAS) - (op1->flags & F_ALIAS);
    if (alias_diff != 0)
      /* Put the one that isn't an alias first.  */
      return alias_diff;
  }

  /* Except for aliases, two "identical" instructions had
     better have the same opcode.  This is a sanity check on the table.  */
  i = strcmp (op0->name, op1->name);
  if (i)
      if (op0->flags & F_ALIAS) /* If they're both aliases, be arbitrary. */
          return i;
      else
          fprintf (stderr,
                   "Internal error: bad sparc-opcode.h: \"%s\" == \"%s\"\n",
                   op0->name, op1->name);

  /* Fewer arguments are preferred.  */
  {
    int length_diff = strlen (op0->args) - strlen (op1->args);
    if (length_diff != 0)
      /* Put the one with fewer arguments first.  */
      return length_diff;
  }

  /* Put 1+i before i+1.  */
  {
    char *p0 = (char *) strchr(op0->args, '+');
    char *p1 = (char *) strchr(op1->args, '+');

    if (p0 && p1)
      {
        /* There is a plus in both operands.  Note that a plus
           sign cannot be the first character in args,
           so the following [-1]'s are valid.  */
        if (p0[-1] == 'i' && p1[1] == 'i')
          /* op0 is i+1 and op1 is 1+i, so op1 goes first.  */
          return 1;
        if (p0[1] == 'i' && p1[-1] == 'i')
          /* op0 is 1+i and op1 is i+1, so op0 goes first.  */
          return -1;
      }
  }

  /* They are, as far as we can tell, identical.
     Since qsort may have rearranged the table partially, there is
     no way to tell which one was first in the opcode table as
     written, so just say there are equal.  */
  return 0;
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */
  
# pragma implementation "regs_sparc.hh"

# include "_regs_sparc.cpp.incl"


Location IArgRegisters[] = {
  IArg1, IArg2, IArg3, IArg4, IArg5,
  IllegalLocation
  };

Location ArgRegisters[] = {
  Arg1, Arg2, Arg3, Arg4, Arg5,
  IllegalLocation
  };

Location CArgRegisters[] = {
  O1, O2, O3, O4, O5,
  IllegalLocation
  };

Location AnyRegisters[] = {
  G1, G2, G3, G4, G5,
  O0, O1, O2, O3, O4, O5,
  I0, I1, I2, I3, I4, I5,
  L0, L1, L2, L3, L4, L5, L6, L7,
  IllegalLocation
  };

Location NoFrameAnyRegisters[] = {
  G1, G2, G3, G4, G5,
  I0, I1, I2, I3, I4, I5,
  IllegalLocation
  };

Location CallKilledRegisters[] = {
  G1, G2, G3, G4, G5,
  O0, O1, O2, O3, O4, O5,
  IllegalLocation  
  };

Location NoFrameKilledRegisters[] = {
  O0, O1, O2, O3, O4, O5,
  L0, L1, L2, L3, L4, L5, L6, L7,
  IllegalLocation
  };

Location MoveKilledRegisters[] = {
  ReservedAnyRegister1,
  IllegalLocation
  };

Location RegisterFromStack[] = {
  I7, I6, I5, I4, I3, I2, I1, I0,
  L7, L6, L5, L4, L3, L2, L1, L0
  };

fint StackFromRegister[] = {
  // G0 - G7
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  // O0 - O7
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  15, 14, 13, 12, 11, 10, 9, 8,       // L0 - L7
  7, 6, 5, 4, 3, 2, 1, 0              // I0 - I7
  };

Location IRegisterFromORegister[] = {
  // G0 - G7
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  // O0 - O7
  I0, I1, I2, I3, I4, I5, I6, I7,
  // L0 - L7
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  // I0 - I7
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation
  };

const char* RegisterNames[] = {
  "g0", "g1", "g2", "g3", "g4", "g5", "g6", "g7",
  "o0", "o1", "o2", "o3", "o4", "o5", "sp", "o7",
  "l0", "l1", "l2", "l3", "l4", "l5", "l6", "l7",
  "i0", "i1", "i2", "i3", "i4", "i5", "fp", "i7",
  "*UnAllocated*", "*Temp*", "*StackTemp*",
  "*AnyLocation*", "*AnyRegister*"
};

static const int staticNames = 16;
  
const char* StackRegisterNames[staticNames] = {
  "T0", "T1", "T2", "T3", "T4", "T5", "T6", "T7", 
  "T8", "T9", "T10", "T11", "T12", "T13", "T14", "T15"
};

const char* ExtraArgRegisterNames[staticNames] = {
  "E0", "E1", "E2", "E3", "E4", "E5", "E6", "E7", 
  "E8", "E9", "E10", "E11", "E12", "E13", "E14", "E15"
};
const char *ExtraIArgRegisterNames[staticNames] = {
  "I0", "I1", "I2", "I3", "I4"* "I5", "I6", "I7",
  "I8", "I9", "I10", "I11", "I12", "I13", "I14", "I15"
};

fint NoFrameRegisterNumbers[] = {
  0, 1, 2, 3, 4, 5, 6, 7,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  IllegalLocation, IllegalLocation, IllegalLocation, IllegalLocation,
  8, 9, 10, 11, 12, 13, 14, 15
};
  
static char* locationNameHelper(Location base, int num) {
  if (num  <  staticNames) {
    char **tbl;
    switch (base) {
      case StackLocations:     tbl=     StackRegisterNames; break;
      case ExtraArgLocations:  tbl=  ExtraArgRegisterNames; break;
      # if TARGET_ARCH != PPC_ARCH // same for ppc
      case ExtraIArgLocations: tbl= ExtraIArgRegisterNames; break;
      # endif
      default: ShouldNotReachHere();
    }
    return tbl[num];
  } else {
    char* s= new char [30]; // known leak NEW_RESOURCE_ARRAY(char, 30);
    sprintf(s, "%c%ld",
              base==StackLocations     ? 'T'
            : base==ExtraArgLocations  ? 'E'
            : base==ExtraIArgLocations ? 'I'
            : 0, // shouldn't get to this
            long(num));
    return s;
  }
}

const char *locationName(Location l) {
  Location base;
  int num;
  if          (isStackRegister(l)) { base= StackLocations;     num= l - base; }
  else if  (isExtraArgRegister(l)) { base= ExtraArgLocations;  num= base - l; }
  else if (isExtraIArgRegister(l)) { base= ExtraIArgLocations; num= base - l; }
  else {
    assert(isRegister(l) || l == AnyLocation || l == AnyRegister ||
           l == UnAllocated || l == Temp || l == StackTemp ||
           l == DataRegister || l == AddressRegister, "unexpected location");
    return RegisterNames[l];
  }
  return locationNameHelper(base, num);
}


# if defined(SIC_COMPILER)
    Location TempRegs[] = { G1, G2, O0, O1, O2, O3, O4, O5 };
    
#   define X(arg) -99999999     /* to make the following table look nicer */
    
    fint RegToTempNo[/* indexed by Location */] = {
      X("g0"),       0,       1, X("g3"), X("g4"), X("g5"), X("g6"), X("g7"),
            2,       3,       4,       5,       6,       7, X("sp"), X("o7"),
      X("l0"), X("l1"), X("l2"), X("l3"), X("l4"), X("l5"), X("l6"), X("l7"),
      X("i0"), X("i1"), X("i2"), X("i3"), X("i4"), X("i5"), X("fp"), X("i7")
    };

    Location CalleeSavedRegs[] = {
      I0, I1, I2, I3, I4, I5,
      L0, L1, L2, L3, L4, L5, L6, L7
    };

    void regs_sparc_init() {
      // The SIC uses g3-g5 during code generation, that's why they
      // aren't listed as general temp regs above.
      assert(Temp1 == G5 && Temp2 == G4 && Temp3 == G3, "change this");
      for (fint i = 0; i < NumTempRegs; i++) {
        assert(TempRegs[i] != G5 && TempRegs[i] != G4 && TempRegs[i] != G3,
               "G3-G5 are reserved for the code generator");
        assert(RegToTempNo[TempRegs[i]] == i, "wrong RegToTempNo entry");
      }
    } 
    

# else // defined(SIC_COMPILER)
    void regs_sparc_init() {}
# endif
  

/* Sun-$Revision: 30.3 $ */

// Adapted from GNU gdb 4.7    - Urs 12/92
// When editing this file, try not to reindent anything so diffs with
// future versions of gdb are simpler.

/* Table of opcodes for the sparc.
        Copyright 1989, 1991, 1992 Free Software Foundation, Inc.

This file is part of the BFD library.

BFD is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 2, or (at your option) any later
version.

BFD is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License
for more details.

You should have received a copy of the GNU General Public License
along with this software; see the file COPYING.  If not, write to
the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.   */

/* FIXME-someday: perhaps the ,a's and such should be embedded in the
   instruction's name rather than the args.  This would make gas faster, pinsn
   slower, but would mess up some macros a bit.  xoxorich. */


# pragma implementation "sparc.hh"
#include "_opc_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


const char *architecture_pname[] = {
        "v6",
        "v7",
        "v8",
        "sparclite",
        NULL,
};


#define COND(x)         (((x)&0xf)<<25)

#define CONDA   (COND(0x8))
#define CONDCC  (COND(0xd))
#define CONDCS  (COND(0x5))
#define CONDE   (COND(0x1))
#define CONDG   (COND(0xa))
#define CONDGE  (COND(0xb))
#define CONDGU  (COND(0xc))
#define CONDL   (COND(0x3))
#define CONDLE  (COND(0x2))
#define CONDLEU (COND(0x4))
#define CONDN   (COND(0x0))
#define CONDNE  (COND(0x9))
#define CONDNEG (COND(0x6))
#define CONDPOS (COND(0xe))
#define CONDVC  (COND(0xf))
#define CONDVS  (COND(0x7))

#define CONDNZ  CONDNE
#define CONDZ   CONDE
#define CONDGEU CONDCC
#define CONDLU  CONDCS

#define FCONDA          (COND(0x8))
#define FCONDE          (COND(0x9))
#define FCONDG          (COND(0x6))
#define FCONDGE         (COND(0xb))
#define FCONDL          (COND(0x4))
#define FCONDLE         (COND(0xd))
#define FCONDLG         (COND(0x2))
#define FCONDN          (COND(0x0))
#define FCONDNE         (COND(0x1))
#define FCONDO          (COND(0xf))
#define FCONDU          (COND(0x7))
#define FCONDUE         (COND(0xa))
#define FCONDUG         (COND(0x5))
#define FCONDUGE        (COND(0xc))
#define FCONDUL         (COND(0x3))
#define FCONDULE        (COND(0xe))

#define FCONDNZ FCONDNE
#define FCONDZ  FCONDE


/* The order of the opcodes in the table is significant:
        
        * The assembler requires that all instances of the same mnemonic must
        be consecutive. If they aren't, the assembler will bomb at runtime.

        * The disassembler should not care about the order of the opcodes.

*/

struct sparc_opcode sparc_opcodes[] = {

{ "ld", F3(3, 0x00, 0), F3(~3, ~0x00, ~0),              "[1+2],d", 0, v6 },
{ "ld", F3(3, 0x00, 0), F3(~3, ~0x00, ~0)|RS2_G0,       "[1],d", 0, v6 }, /* ld [rs1+%g0],d */
{ "ld", F3(3, 0x00, 1), F3(~3, ~0x00, ~1),              "[1+i],d", 0, v6 },
{ "ld", F3(3, 0x00, 1), F3(~3, ~0x00, ~1),              "[i+1],d", 0, v6 },
{ "ld", F3(3, 0x00, 1), F3(~3, ~0x00, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ld", F3(3, 0x00, 1), F3(~3, ~0x00, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ld [rs1+0],d */
{ "ld", F3(3, 0x20, 0), F3(~3, ~0x20, ~0),              "[1+2],g", 0, v6 },
{ "ld", F3(3, 0x20, 0), F3(~3, ~0x20, ~0)|RS2_G0,       "[1],g", 0, v6 }, /* ld [rs1+%g0],d */
{ "ld", F3(3, 0x20, 1), F3(~3, ~0x20, ~1),              "[1+i],g", 0, v6 },
{ "ld", F3(3, 0x20, 1), F3(~3, ~0x20, ~1),              "[i+1],g", 0, v6 },
{ "ld", F3(3, 0x20, 1), F3(~3, ~0x20, ~1)|RS1_G0,       "[i],g", 0, v6 },
{ "ld", F3(3, 0x20, 1), F3(~3, ~0x20, ~1)|SIMM13(~0),   "[1],g", 0, v6 }, /* ld [rs1+0],d */

{ "ld", F3(3, 0x21, 0), F3(~3, ~0x21, ~0)|RD(~0),       "[1+2],F", 0, v6 },
{ "ld", F3(3, 0x21, 0), F3(~3, ~0x21, ~0)|RS2_G0|RD(~0),"[1],F", 0, v6 }, /* ld [rs1+%g0],d */
{ "ld", F3(3, 0x21, 1), F3(~3, ~0x21, ~1)|RD(~0),       "[1+i],F", 0, v6 },
{ "ld", F3(3, 0x21, 1), F3(~3, ~0x21, ~1)|RD(~0),       "[i+1],F", 0, v6 },
{ "ld", F3(3, 0x21, 1), F3(~3, ~0x21, ~1)|RS1_G0|RD(~0),"[i],F", 0, v6 },
{ "ld", F3(3, 0x21, 1), F3(~3, ~0x21, ~1)|SIMM13(~0)|RD(~0),"[1],F", 0, v6 }, /* ld [rs1+0],d */

{ "ld", F3(3, 0x30, 0), F3(~3, ~0x30, ~0),              "[1+2],D", F_ALIAS, v6 },
{ "ld", F3(3, 0x30, 0), F3(~3, ~0x30, ~0)|RS2_G0,       "[1],D", F_ALIAS, v6 }, /* ld [rs1+%g0],d */
{ "ld", F3(3, 0x30, 1), F3(~3, ~0x30, ~1),              "[1+i],D", F_ALIAS, v6 },
{ "ld", F3(3, 0x30, 1), F3(~3, ~0x30, ~1),              "[i+1],D", F_ALIAS, v6 },
{ "ld", F3(3, 0x30, 1), F3(~3, ~0x30, ~1)|RS1_G0,       "[i],D", F_ALIAS, v6 },
{ "ld", F3(3, 0x30, 1), F3(~3, ~0x30, ~1)|SIMM13(~0),   "[1],D", F_ALIAS, v6 }, /* ld [rs1+0],d */
{ "ld", F3(3, 0x31, 0), F3(~3, ~0x31, ~0),              "[1+2],C", 0, v6 },
{ "ld", F3(3, 0x31, 0), F3(~3, ~0x31, ~0)|RS2_G0,       "[1],C", 0, v6 }, /* ld [rs1+%g0],d */
{ "ld", F3(3, 0x31, 1), F3(~3, ~0x31, ~1),              "[1+i],C", 0, v6 },
{ "ld", F3(3, 0x31, 1), F3(~3, ~0x31, ~1),              "[i+1],C", 0, v6 },
{ "ld", F3(3, 0x31, 1), F3(~3, ~0x31, ~1)|RS1_G0,       "[i],C", 0, v6 },
{ "ld", F3(3, 0x31, 1), F3(~3, ~0x31, ~1)|SIMM13(~0),   "[1],C", 0, v6 }, /* ld [rs1+0],d */


{ "ldd",        F3(3, 0x03, 0), F3(~3, ~0x03, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "ldd",        F3(3, 0x03, 0), F3(~3, ~0x03, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* ldd [rs1+%g0],d */
{ "ldd",        F3(3, 0x03, 1), F3(~3, ~0x03, ~1),              "[1+i],d", 0, v6 },
{ "ldd",        F3(3, 0x03, 1), F3(~3, ~0x03, ~1),              "[i+1],d", 0, v6 },
{ "ldd",        F3(3, 0x03, 1), F3(~3, ~0x03, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ldd",        F3(3, 0x03, 1), F3(~3, ~0x03, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ldd [rs1+0],d */
{ "ldd",        F3(3, 0x23, 0), F3(~3, ~0x23, ~0)|ASI(~0),      "[1+2],H", 0, v6 },
{ "ldd",        F3(3, 0x23, 0), F3(~3, ~0x23, ~0)|ASI_RS2(~0),  "[1],H", 0, v6 }, /* ldd [rs1+%g0],d */
{ "ldd",        F3(3, 0x23, 1), F3(~3, ~0x23, ~1),              "[1+i],H", 0, v6 },
{ "ldd",        F3(3, 0x23, 1), F3(~3, ~0x23, ~1),              "[i+1],H", 0, v6 },
{ "ldd",        F3(3, 0x23, 1), F3(~3, ~0x23, ~1)|RS1_G0,       "[i],H", 0, v6 },
{ "ldd",        F3(3, 0x23, 1), F3(~3, ~0x23, ~1)|SIMM13(~0),   "[1],H", 0, v6 }, /* ldd [rs1+0],d */
{ "ldd",        F3(3, 0x33, 0), F3(~3, ~0x33, ~0)|ASI(~0),      "[1+2],D", F_ALIAS, v6 },
{ "ldd",        F3(3, 0x33, 0), F3(~3, ~0x33, ~0)|ASI_RS2(~0),  "[1],D", F_ALIAS, v6 }, /* ldd [rs1+%g0],d */
{ "ldd",        F3(3, 0x33, 1), F3(~3, ~0x33, ~1),              "[1+i],D", F_ALIAS, v6 },
{ "ldd",        F3(3, 0x33, 1), F3(~3, ~0x33, ~1),              "[i+1],D", F_ALIAS, v6 },
{ "ldd",        F3(3, 0x33, 1), F3(~3, ~0x33, ~1)|RS1_G0,       "[i],D", F_ALIAS, v6 },
{ "ldd",        F3(3, 0x33, 1), F3(~3, ~0x33, ~1)|SIMM13(~0),   "[1],D", F_ALIAS, v6 }, /* ldd [rs1+0],d */


{ "ldsb",       F3(3, 0x09, 0), F3(~3, ~0x09, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "ldsb",       F3(3, 0x09, 0), F3(~3, ~0x09, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* ldsb [rs1+%g0],d */
{ "ldsb",       F3(3, 0x09, 1), F3(~3, ~0x09, ~1),              "[1+i],d", 0, v6 },
{ "ldsb",       F3(3, 0x09, 1), F3(~3, ~0x09, ~1),              "[i+1],d", 0, v6 },
{ "ldsb",       F3(3, 0x09, 1), F3(~3, ~0x09, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ldsb",       F3(3, 0x09, 1), F3(~3, ~0x09, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ldsb [rs1+0],d */

{ "ldsh",       F3(3, 0x0a, 0), F3(~3, ~0x0a, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* ldsh [rs1+%g0],d */
{ "ldsh",       F3(3, 0x0a, 0), F3(~3, ~0x0a, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "ldsh",       F3(3, 0x0a, 1), F3(~3, ~0x0a, ~1),              "[1+i],d", 0, v6 },
{ "ldsh",       F3(3, 0x0a, 1), F3(~3, ~0x0a, ~1),              "[i+1],d", 0, v6 },
{ "ldsh",       F3(3, 0x0a, 1), F3(~3, ~0x0a, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ldsh",       F3(3, 0x0a, 1), F3(~3, ~0x0a, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ldsh [rs1+0],d */

{ "ldstub",     F3(3, 0x0d, 0), F3(~3, ~0x0d, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "ldstub",     F3(3, 0x0d, 0), F3(~3, ~0x0d, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* ldstub [rs1+%g0],d */
{ "ldstub",     F3(3, 0x0d, 1), F3(~3, ~0x0d, ~1),              "[1+i],d", 0, v6 },
{ "ldstub",     F3(3, 0x0d, 1), F3(~3, ~0x0d, ~1),              "[i+1],d", 0, v6 },
{ "ldstub",     F3(3, 0x0d, 1), F3(~3, ~0x0d, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ldstub",     F3(3, 0x0d, 1), F3(~3, ~0x0d, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ldstub [rs1+0],d */


{ "ldub",       F3(3, 0x01, 0), F3(~3, ~0x01, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "ldub",       F3(3, 0x01, 0), F3(~3, ~0x01, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* ldub [rs1+%g0],d */
{ "ldub",       F3(3, 0x01, 1), F3(~3, ~0x01, ~1),              "[1+i],d", 0, v6 },
{ "ldub",       F3(3, 0x01, 1), F3(~3, ~0x01, ~1),              "[i+1],d", 0, v6 },
{ "ldub",       F3(3, 0x01, 1), F3(~3, ~0x01, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "ldub",       F3(3, 0x01, 1), F3(~3, ~0x01, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* ldub [rs1+0],d */

{ "lduh",       F3(3, 0x02, 0), F3(~3, ~0x02, ~0)|ASI(~0),      "[1+2],d", 0, v6 },
{ "lduh",       F3(3, 0x02, 0), F3(~3, ~0x02, ~0)|ASI_RS2(~0),  "[1],d", 0, v6 }, /* lduh [rs1+%g0],d */
{ "lduh",       F3(3, 0x02, 1), F3(~3, ~0x02, ~1),              "[1+i],d", 0, v6 },
{ "lduh",       F3(3, 0x02, 1), F3(~3, ~0x02, ~1),              "[i+1],d", 0, v6 },
{ "lduh",       F3(3, 0x02, 1), F3(~3, ~0x02, ~1)|RS1_G0,       "[i],d", 0, v6 },
{ "lduh",       F3(3, 0x02, 1), F3(~3, ~0x02, ~1)|SIMM13(~0),   "[1],d", 0, v6 }, /* lduh [rs1+0],d */



{ "lda",        F3(3, 0x10, 0), F3(~3, ~0x10, ~0),              "[1+2]A,d", 0, v6 },
{ "lda",        F3(3, 0x10, 0), F3(~3, ~0x10, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* lda [rs1+%g0],d */

{ "ldda",       F3(3, 0x13, 0), F3(~3, ~0x13, ~0),              "[1+2]A,d", 0, v6 },
{ "ldda",       F3(3, 0x13, 0), F3(~3, ~0x13, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* ldda [rs1+%g0],d */



{ "ldsba",      F3(3, 0x19, 0), F3(~3, ~0x19, ~0),              "[1+2]A,d", 0, v6 },
{ "ldsba",      F3(3, 0x19, 0), F3(~3, ~0x19, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* ldsba [rs1+%g0],d */

{ "ldsha",      F3(3, 0x1a, 0), F3(~3, ~0x1a, ~0),              "[1+2]A,d", 0, v6 },
{ "ldsha",      F3(3, 0x1a, 0), F3(~3, ~0x1a, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* ldsha [rs1+%g0],d */

{ "ldstuba",    F3(3, 0x1d, 0), F3(~3, ~0x1d, ~0),              "[1+2]A,d", 0, v6 },
{ "ldstuba",    F3(3, 0x1d, 0), F3(~3, ~0x1d, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* ldstuba [rs1+%g0],d */


{ "lduba",      F3(3, 0x11, 0), F3(~3, ~0x11, ~0),              "[1+2]A,d", 0, v6 },
{ "lduba",      F3(3, 0x11, 0), F3(~3, ~0x11, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* lduba [rs1+%g0],d */

{ "lduha",      F3(3, 0x12, 0), F3(~3, ~0x12, ~0),              "[1+2]A,d", 0, v6 },
{ "lduha",      F3(3, 0x12, 0), F3(~3, ~0x12, ~0)|RS2_G0,       "[1]A,d", 0, v6 }, /* lduha [rs1+%g0],d */



{ "st", F3(3, 0x04, 0), F3(~3, ~0x04, ~0)|ASI(~0),              "d,[1+2]", 0, v6 },
{ "st", F3(3, 0x04, 0), F3(~3, ~0x04, ~0)|ASI_RS2(~0),          "d,[1]", 0, v6 }, /* st d,[rs1+%g0] */
{ "st", F3(3, 0x04, 1), F3(~3, ~0x04, ~1),                      "d,[1+i]", 0, v6 },
{ "st", F3(3, 0x04, 1), F3(~3, ~0x04, ~1),                      "d,[i+1]", 0, v6 },
{ "st", F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|RS1_G0,               "d,[i]", 0, v6 },
{ "st", F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|SIMM13(~0),           "d,[1]", 0, v6 }, /* st d,[rs1+0] */
{ "st", F3(3, 0x24, 0), F3(~3, ~0x24, ~0)|ASI(~0),              "g,[1+2]", 0, v6 },
{ "st", F3(3, 0x24, 0), F3(~3, ~0x24, ~0)|ASI_RS2(~0),          "g,[1]", 0, v6 }, /* st d[rs1+%g0] */
{ "st", F3(3, 0x24, 1), F3(~3, ~0x24, ~1),                      "g,[1+i]", 0, v6 },
{ "st", F3(3, 0x24, 1), F3(~3, ~0x24, ~1),                      "g,[i+1]", 0, v6 },
{ "st", F3(3, 0x24, 1), F3(~3, ~0x24, ~1)|RS1_G0,               "g,[i]", 0, v6 },
{ "st", F3(3, 0x24, 1), F3(~3, ~0x24, ~1)|SIMM13(~0),           "g,[1]", 0, v6 }, /* st d,[rs1+0] */
{ "st", F3(3, 0x34, 0), F3(~3, ~0x34, ~0)|ASI(~0),              "D,[1+2]", F_ALIAS, v6 },
{ "st", F3(3, 0x34, 0), F3(~3, ~0x34, ~0)|ASI_RS2(~0),          "D,[1]", F_ALIAS, v6 }, /* st d,[rs1+%g0] */
{ "st", F3(3, 0x34, 1), F3(~3, ~0x34, ~1),                      "D,[1+i]", F_ALIAS, v6 },
{ "st", F3(3, 0x34, 1), F3(~3, ~0x34, ~1),                      "D,[i+1]", F_ALIAS, v6 },
{ "st", F3(3, 0x34, 1), F3(~3, ~0x34, ~1)|RS1_G0,               "D,[i]", F_ALIAS, v6 },
{ "st", F3(3, 0x34, 1), F3(~3, ~0x34, ~1)|SIMM13(~0),           "D,[1]", F_ALIAS, v6 }, /* st d,[rs1+0] */
{ "st", F3(3, 0x35, 0), F3(~3, ~0x35, ~0)|ASI(~0),              "C,[1+2]", 0, v6 },
{ "st", F3(3, 0x35, 0), F3(~3, ~0x35, ~0)|ASI_RS2(~0),          "C,[1]", 0, v6 }, /* st d,[rs1+%g0] */
{ "st", F3(3, 0x35, 1), F3(~3, ~0x35, ~1),                      "C,[1+i]", 0, v6 },
{ "st", F3(3, 0x35, 1), F3(~3, ~0x35, ~1),                      "C,[i+1]", 0, v6 },
{ "st", F3(3, 0x35, 1), F3(~3, ~0x35, ~1)|RS1_G0,               "C,[i]", 0, v6 },
{ "st", F3(3, 0x35, 1), F3(~3, ~0x35, ~1)|SIMM13(~0),           "C,[1]", 0, v6 }, /* st d,[rs1+0] */

{ "st", F3(3, 0x25, 0), F3(~3, ~0x25, ~0)|RD_G0|ASI(~0),        "F,[1+2]", 0, v6 },
{ "st", F3(3, 0x25, 0), F3(~3, ~0x25, ~0)|RD_G0|ASI_RS2(~0),    "F,[1]", 0, v6 }, /* st d,[rs1+%g0] */
{ "st", F3(3, 0x25, 1), F3(~3, ~0x25, ~1)|RD_G0,                "F,[1+i]", 0, v6 },
{ "st", F3(3, 0x25, 1), F3(~3, ~0x25, ~1)|RD_G0,                "F,[i+1]", 0, v6 },
{ "st", F3(3, 0x25, 1), F3(~3, ~0x25, ~1)|RD_G0|RS1_G0,         "F,[i]", 0, v6 },
{ "st", F3(3, 0x25, 1), F3(~3, ~0x25, ~1)|RD_G0|SIMM13(~0),     "F,[1]", 0, v6 }, /* st d,[rs1+0] */




{ "sta",        F3(3, 0x14, 0), F3(~3, ~0x14, ~0),              "d,[1+2]A", 0, v6 },
{ "sta",        F3(3, 0x14, 0), F3(~3, ~0x14, ~0)|RS2(~0),      "d,[1]A", 0, v6 }, /* sta d,[rs1+%g0] */





{ "stb",        F3(3, 0x05, 0), F3(~3, ~0x05, ~0)|ASI(~0),      "d,[1+2]", 0, v6 },
{ "stb",        F3(3, 0x05, 0), F3(~3, ~0x05, ~0)|ASI_RS2(~0),  "d,[1]", 0, v6 }, /* stb d,[rs1+%g0] */
{ "stb",        F3(3, 0x05, 1), F3(~3, ~0x05, ~1),              "d,[1+i]", 0, v6 },
{ "stb",        F3(3, 0x05, 1), F3(~3, ~0x05, ~1),              "d,[i+1]", 0, v6 },
{ "stb",        F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|RS1_G0,       "d,[i]", 0, v6 },
{ "stb",        F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|SIMM13(~0),   "d,[1]", 0, v6 }, /* stb d,[rs1+0] */



{ "stba",       F3(3, 0x15, 0), F3(~3, ~0x15, ~0),              "d,[1+2]A", 0, v6 },
{ "stba",       F3(3, 0x15, 0), F3(~3, ~0x15, ~0)|RS2(~0),      "d,[1]A", 0, v6 }, /* stba d,[rs1+%g0] */



{ "std",        F3(3, 0x07, 0), F3(~3, ~0x07, ~0)|ASI(~0),      "d,[1+2]", 0, v6 },
{ "std",        F3(3, 0x07, 0), F3(~3, ~0x07, ~0)|ASI_RS2(~0),  "d,[1]", 0, v6 }, /* std d,[rs1+%g0] */
{ "std",        F3(3, 0x07, 1), F3(~3, ~0x07, ~1),              "d,[1+i]", 0, v6 },
{ "std",        F3(3, 0x07, 1), F3(~3, ~0x07, ~1),              "d,[i+1]", 0, v6 },
{ "std",        F3(3, 0x07, 1), F3(~3, ~0x07, ~1)|RS1_G0,       "d,[i]", 0, v6 },
{ "std",        F3(3, 0x07, 1), F3(~3, ~0x07, ~1)|SIMM13(~0),   "d,[1]", 0, v6 }, /* std d,[rs1+0] */
{ "std",        F3(3, 0x26, 0), F3(~3, ~0x26, ~0)|ASI(~0),      "q,[1+2]", F_ALIAS, v6 },
{ "std",        F3(3, 0x26, 0), F3(~3, ~0x26, ~0)|ASI_RS2(~0),  "q,[1]", F_ALIAS, v6 }, /* std d,[rs1+%g0] */
{ "std",        F3(3, 0x26, 1), F3(~3, ~0x26, ~1),              "q,[1+i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x26, 1), F3(~3, ~0x26, ~1),              "q,[i+1]", F_ALIAS, v6 },
{ "std",        F3(3, 0x26, 1), F3(~3, ~0x26, ~1)|RS1_G0,       "q,[i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x26, 1), F3(~3, ~0x26, ~1)|SIMM13(~0),   "q,[1]", F_ALIAS, v6 }, /* std d,[rs1+0] */
{ "std",        F3(3, 0x27, 0), F3(~3, ~0x27, ~0)|ASI(~0),      "H,[1+2]", 0, v6 },
{ "std",        F3(3, 0x27, 0), F3(~3, ~0x27, ~0)|ASI_RS2(~0),  "H,[1]", 0, v6 }, /* std d,[rs1+%g0] */
{ "std",        F3(3, 0x27, 1), F3(~3, ~0x27, ~1),              "H,[1+i]", 0, v6 },
{ "std",        F3(3, 0x27, 1), F3(~3, ~0x27, ~1),              "H,[i+1]", 0, v6 },
{ "std",        F3(3, 0x27, 1), F3(~3, ~0x27, ~1)|RS1_G0,       "H,[i]", 0, v6 },
{ "std",        F3(3, 0x27, 1), F3(~3, ~0x27, ~1)|SIMM13(~0),   "H,[1]", 0, v6 }, /* std d,[rs1+0] */
{ "std",        F3(3, 0x36, 0), F3(~3, ~0x36, ~0)|ASI(~0),      "Q,[1+2]", F_ALIAS, v6 },
{ "std",        F3(3, 0x36, 0), F3(~3, ~0x36, ~0)|ASI_RS2(~0),  "Q,[1]", F_ALIAS, v6 }, /* std d,[rs1+%g0] */
{ "std",        F3(3, 0x36, 1), F3(~3, ~0x36, ~1),              "Q,[1+i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x36, 1), F3(~3, ~0x36, ~1),              "Q,[i+1]", F_ALIAS, v6 },
{ "std",        F3(3, 0x36, 1), F3(~3, ~0x36, ~1)|RS1_G0,       "Q,[i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x36, 1), F3(~3, ~0x36, ~1)|SIMM13(~0),   "Q,[1]", F_ALIAS, v6 }, /* std d,[rs1+0] */
{ "std",        F3(3, 0x37, 0), F3(~3, ~0x37, ~0)|ASI(~0),      "D,[1+2]", F_ALIAS, v6 },
{ "std",        F3(3, 0x37, 0), F3(~3, ~0x37, ~0)|ASI_RS2(~0),  "D,[1]", F_ALIAS, v6 }, /* std d,[rs1+%g0] */
{ "std",        F3(3, 0x37, 1), F3(~3, ~0x37, ~1),              "D,[1+i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x37, 1), F3(~3, ~0x37, ~1),              "D,[i+1]", F_ALIAS, v6 },
{ "std",        F3(3, 0x37, 1), F3(~3, ~0x37, ~1)|RS1_G0,       "D,[i]", F_ALIAS, v6 },
{ "std",        F3(3, 0x37, 1), F3(~3, ~0x37, ~1)|SIMM13(~0),   "D,[1]", F_ALIAS, v6 }, /* std d,[rs1+0] */

{ "stda",       F3(3, 0x17, 0), F3(~3, ~0x17, ~0),              "d,[1+2]A", 0, v6 },
{ "stda",       F3(3, 0x17, 0), F3(~3, ~0x17, ~0)|RS2(~0),      "d,[1]A", 0, v6 }, /* stda d,[rs1+%g0] */

{ "sth",        F3(3, 0x06, 0), F3(~3, ~0x06, ~0)|ASI(~0),      "d,[1+2]", 0, v6 },
{ "sth",        F3(3, 0x06, 0), F3(~3, ~0x06, ~0)|ASI_RS2(~0),  "d,[1]", 0, v6 }, /* sth d,[rs1+%g0] */
{ "sth",        F3(3, 0x06, 1), F3(~3, ~0x06, ~1),              "d,[1+i]", 0, v6 },
{ "sth",        F3(3, 0x06, 1), F3(~3, ~0x06, ~1),              "d,[i+1]", 0, v6 },
{ "sth",        F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|RS1_G0,       "d,[i]", 0, v6 },
{ "sth",        F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|SIMM13(~0),   "d,[1]", 0, v6 }, /* sth d,[+] */



{ "stha",       F3(3, 0x16, 0), F3(~3, ~0x16, ~0),              "d,[1+2]A", 0, v6 },
{ "stha",       F3(3, 0x16, 0), F3(~3, ~0x16, ~0)|RS2(~0),      "d,[1]A", 0, v6 }, /* stha ,[+%] */








{ "swap",       F3(3, 0x0f, 0), F3(~3, ~0x0f, ~0)|ASI(~0),      "[1+2],d", 0, v7 },
{ "swap",       F3(3, 0x0f, 0), F3(~3, ~0x0f, ~0)|ASI_RS2(~0),  "[1],d", 0, v7 }, /* swap [rs1+%g0],d */
{ "swap",       F3(3, 0x0f, 1), F3(~3, ~0x0f, ~1),              "[1+i],d", 0, v7 },
{ "swap",       F3(3, 0x0f, 1), F3(~3, ~0x0f, ~1),              "[i+1],d", 0, v7 },
{ "swap",       F3(3, 0x0f, 1), F3(~3, ~0x0f, ~1)|RS1_G0,       "[i],d", 0, v7 },
{ "swap",       F3(3, 0x0f, 1), F3(~3, ~0x0f, ~1)|SIMM13(~0),   "[1],d", 0, v7 }, /* swap [rs1+0],d */

{ "swapa",      F3(3, 0x1f, 0), F3(~3, ~0x1f, ~0),              "[1+2]A,d", 0, v7 },
{ "swapa",      F3(3, 0x1f, 0), F3(~3, ~0x1f, ~0)|RS2(~0),      "[1]A,d", 0, v7 }, /* swapa [rs1+%g0],d */

{ "restore",    F3(2, 0x3d, 0), F3(~2, ~0x3d, ~0)|ASI(~0),                      "1,2,d", 0, v6 },
{ "restore",    F3(2, 0x3d, 0), F3(~2, ~0x3d, ~0)|RD_G0|RS1_G0|ASI_RS2(~0),     "", 0, v6 }, /* restore %g0,%g0,%g0 */
{ "restore",    F3(2, 0x3d, 1), F3(~2, ~0x3d, ~1),                              "1,i,d", 0, v6 },
{ "restore",    F3(2, 0x3d, 1), F3(~2, ~0x3d, ~1)|RD_G0|RS1_G0|SIMM13(~0),      "", 0, v6 }, /* restore %g0,0,%g0 */

{ "rett",       F3(2, 0x39, 0), F3(~2, ~0x39, ~0)|RD_G0|ASI(~0),        "1+2", F_DELAYED, v6 }, /* rett rs1+rs2 */
{ "rett",       F3(2, 0x39, 0), F3(~2, ~0x39, ~0)|RD_G0|ASI_RS2(~0),    "1", F_DELAYED, v6 },   /* rett rs1,%g0 */
{ "rett",       F3(2, 0x39, 1), F3(~2, ~0x39, ~1)|RD_G0,        "1+i", F_DELAYED, v6 }, /* rett rs1+X */
{ "rett",       F3(2, 0x39, 1), F3(~2, ~0x39, ~1)|RD_G0,        "i+1", F_DELAYED, v6 }, /* rett X+rs1 */
{ "rett",       F3(2, 0x39, 1), F3(~2, ~0x39, ~1)|RD_G0|RS1_G0,"i", F_DELAYED, v6 }, /* rett X+rs1 */
{ "rett",       F3(2, 0x39, 1), F3(~2, ~0x39, ~1)|RD_G0|RS1_G0, "i", F_DELAYED, v6 },   /* rett X */
{ "rett",       F3(2, 0x39, 1), F3(~2, ~0x39, ~1)|RD_G0|SIMM13(~0),     "1", F_DELAYED, v6 },   /* rett rs1+0 */

{ "save",       F3(2, 0x3c, 0), F3(~2, ~0x3c, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "save",       F3(2, 0x3c, 1), F3(~2, ~0x3c, ~1),              "1,i,d", 0, v6 },
{ "save",       0x81e00000,     ~0x81e00000,    "", F_ALIAS, v6 },

{ "ret",  F3(2, 0x38, 1)|RS1(0x1f)|SIMM13(8), F3(~2, ~0x38, ~1)|SIMM13(~8),            "", F_DELAYED, v6 }, /* jmpl %i7+8,%g0 */
{ "retl", F3(2, 0x38, 1)|RS1(0x0f)|SIMM13(8), F3(~2, ~0x38, ~1)|RS1(~0x0f)|SIMM13(~8), "", F_DELAYED, v6 }, /* jmpl %o7+8,%g0 */

{ "jmpl",       F3(2, 0x38, 0), F3(~2, ~0x38, ~0)|ASI(~0),      "1+2,d", F_DELAYED, v6 },
{ "jmpl",       F3(2, 0x38, 0), F3(~2, ~0x38, ~0)|ASI_RS2(~0),  "1,d", F_DELAYED, v6 }, /* jmpl rs1+%g0,d */
{ "jmpl",       F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|SIMM13(~0),   "1,d", F_DELAYED, v6 }, /* jmpl rs1+0,d */
{ "jmpl",       F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|RS1_G0,       "i,d", F_DELAYED, v6 }, /* jmpl %g0+i,d */
{ "jmpl",       F3(2, 0x38, 1), F3(~2, ~0x38, ~1),              "1+i,d", F_DELAYED, v6 },
{ "jmpl",       F3(2, 0x38, 1), F3(~2, ~0x38, ~1),              "i+1,d", F_DELAYED, v6 },


{ "flush",      F3(2, 0x3b, 0), F3(~2, ~0x3b, ~0)|ASI(~0),      "1+2", 0, v8 },
{ "flush",      F3(2, 0x3b, 0), F3(~2, ~0x3b, ~0)|ASI_RS2(~0),  "1", 0, v8 }, /* flush rs1+%g0 */
{ "flush",      F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1)|SIMM13(~0),   "1", 0, v8 }, /* flush rs1+0 */
{ "flush",      F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1)|RS1_G0,       "i", 0, v8 }, /* flush %g0+i */
{ "flush",      F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1),              "1+i", 0, v8 },
{ "flush",      F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1),              "i+1", 0, v8 },

/* IFLUSH was renamed to FLUSH in v8.  */
{ "iflush",     F3(2, 0x3b, 0), F3(~2, ~0x3b, ~0)|ASI(~0),      "1+2", F_ALIAS, v6 },
{ "iflush",     F3(2, 0x3b, 0), F3(~2, ~0x3b, ~0)|ASI_RS2(~0),  "1", F_ALIAS, v6 }, /* flush rs1+%g0 */
{ "iflush",     F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1)|SIMM13(~0),   "1", F_ALIAS, v6 }, /* flush rs1+0 */
{ "iflush",     F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1)|RS1_G0,       "i", F_ALIAS, v6 },
{ "iflush",     F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1),              "1+i", F_ALIAS, v6 },
{ "iflush",     F3(2, 0x3b, 1), F3(~2, ~0x3b, ~1),              "i+1", F_ALIAS, v6 },



{ "stbar",      F3(2, 0x28, 0)|RS1(0xf), F3(~2, ~0x28, ~0)|RD_G0|RS1(~0xf)|SIMM13(~0),  "", 0, v8 },


 /* The 1<<12 is a long story.  It is necessary.  For more info, please contact rich@cygnus.com */
{ "sll",        F3(2, 0x25, 0), F3(~2, ~0x25, ~0)|(1<<12)|ASI(~0),      "1,2,d", 0, v6 },
{ "sll",        F3(2, 0x25, 1), F3(~2, ~0x25, ~1)|(1<<12),              "1,i,d", 0, v6 },
{ "sra",        F3(2, 0x27, 0), F3(~2, ~0x27, ~0)|(1<<12)|ASI(~0),      "1,2,d", 0, v6 },
{ "sra",        F3(2, 0x27, 1), F3(~2, ~0x27, ~1)|(1<<12),              "1,i,d", 0, v6 },
{ "srl",        F3(2, 0x26, 0), F3(~2, ~0x26, ~0)|(1<<12)|ASI(~0),      "1,2,d", 0, v6 },
{ "srl",        F3(2, 0x26, 1), F3(~2, ~0x26, ~1)|(1<<12),              "1,i,d", 0, v6 },


{ "mulscc",     F3(2, 0x24, 0), F3(~2, ~0x24, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "mulscc",     F3(2, 0x24, 1), F3(~2, ~0x24, ~1),              "1,i,d", 0, v6 },

{ "divscc",     F3(2, 0x1d, 0), F3(~2, ~0x1d, ~0)|ASI(~0),      "1,2,d", 0, sparclite },
{ "divscc",     F3(2, 0x1d, 1), F3(~2, ~0x1d, ~1),              "1,i,d", 0, sparclite },

{ "scan",       F3(2, 0x2c, 0), F3(~2, ~0x2c, ~0)|ASI(~0),      "1,2,d", 0, sparclite },
{ "scan",       F3(2, 0x2c, 1), F3(~2, ~0x2c, ~1),              "1,i,d", 0, sparclite },


{ "clr",        F3(2, 0x02, 0), F3(~2, ~0x02, ~0)|RD_G0|RS1_G0|ASI_RS2(~0),     "d", F_ALIAS, v6 }, /* or %g0,%g0,d */
{ "clr",        F3(2, 0x02, 1), F3(~2, ~0x02, ~1)|RS1_G0|SIMM13(~0),            "d", F_ALIAS, v6 }, /* or %g0,0,d       */
{ "clr",        F3(3, 0x04, 0), F3(~3, ~0x04, ~0)|RD_G0|ASI(~0),                "[1+2]", F_ALIAS, v6 },
{ "clr",        F3(3, 0x04, 0), F3(~3, ~0x04, ~0)|RD_G0|ASI_RS2(~0),            "[1]", F_ALIAS, v6 }, /* st %g0,[rs1+%g0] */
{ "clr",        F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|RD_G0,                        "[1+i]", F_ALIAS, v6 },
{ "clr",        F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|RD_G0,                        "[i+1]", F_ALIAS, v6 },
{ "clr",        F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|RD_G0|RS1_G0,         "[i]", F_ALIAS, v6 },
{ "clr",        F3(3, 0x04, 1), F3(~3, ~0x04, ~1)|RD_G0|SIMM13(~0),             "[1]", F_ALIAS, v6 }, /* st %g0,[rs1+0] */

{ "clrb",       F3(3, 0x05, 0), F3(~3, ~0x05, ~0)|RD_G0|ASI(~0),        "[1+2]", F_ALIAS, v6 },
{ "clrb",       F3(3, 0x05, 0), F3(~3, ~0x05, ~0)|RD_G0|ASI_RS2(~0),    "[1]", F_ALIAS, v6 }, /* stb %g0,[rs1+%g0] */
{ "clrb",       F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|RD_G0,                "[1+i]", F_ALIAS, v6 },
{ "clrb",       F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|RD_G0,                "[i+1]", F_ALIAS, v6 },
{ "clrb",       F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|RD_G0|RS1_G0, "[i]", F_ALIAS, v6 },
{ "clrb",       F3(3, 0x05, 1), F3(~3, ~0x05, ~1)|RD_G0|SIMM13(~0),     "[1]", F_ALIAS, v6 }, /* clrb [rs1+0],d */

{ "clrh",       F3(3, 0x06, 0), F3(~3, ~0x06, ~0)|RD_G0|ASI(~0),        "[1+2]", F_ALIAS, v6 },
{ "clrh",       F3(3, 0x06, 0), F3(~3, ~0x06, ~0)|RD_G0|ASI_RS2(~0),    "[1]", F_ALIAS, v6 }, /* sth %g0,[rs1+%g0] */
{ "clrh",       F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|RD_G0,                "[1+i]", F_ALIAS, v6 },
{ "clrh",       F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|RD_G0,                "[i+1]", F_ALIAS, v6 },
{ "clrh",       F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|RD_G0|RS1_G0, "[i]", F_ALIAS, v6 },
{ "clrh",       F3(3, 0x06, 1), F3(~3, ~0x06, ~1)|RD_G0|SIMM13(~0),     "[1]", F_ALIAS, v6 }, /* clrb [rs1+0],d */

{ "orcc",       F3(2, 0x12, 0), F3(~2, ~0x12, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "orcc",       F3(2, 0x12, 1), F3(~2, ~0x12, ~1),              "1,i,d", 0, v6 },
{ "orcc",       F3(2, 0x12, 1), F3(~2, ~0x12, ~1),              "i,1,d", 0, v6 },

/* This is not a commutative instruction.  */
{ "orncc",      F3(2, 0x16, 0), F3(~2, ~0x16, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "orncc",      F3(2, 0x16, 1), F3(~2, ~0x16, ~1),              "1,i,d", 0, v6 },

/* This is not a commutative instruction.  */
{ "orn",        F3(2, 0x06, 0), F3(~2, ~0x06, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "orn",        F3(2, 0x06, 1), F3(~2, ~0x06, ~1),              "1,i,d", 0, v6 },

{ "tst",        F3(2, 0x12, 0), F3(~2, ~0x12, ~0)|RD_G0|ASI_RS2(~0),    "1", 0, v6 }, /* orcc rs1, %g0, %g0 */
{ "tst",        F3(2, 0x12, 0), F3(~2, ~0x12, ~0)|RD_G0|RS1_G0|ASI(~0), "2", 0, v6 }, /* orcc %g0, rs2, %g0 */
{ "tst",        F3(2, 0x12, 1), F3(~2, ~0x12, ~1)|RD_G0|SIMM13(~0),     "1", 0, v6 }, /* orcc rs1, 0, %g0 */

{ "wr", F3(2, 0x30, 0),         F3(~2, ~0x30, ~0)|ASI(~0),              "1,2,m", 0, v8 }, /* wr r,r,%asrX */
{ "wr", F3(2, 0x30, 0),         F3(~2, ~0x30, ~0)|RD_G0|ASI(~0),        "1,2,y", 0, v6 }, /* wr r,r,%y */
{ "wr", F3(2, 0x30, 1),         F3(~2, ~0x30, ~1),                      "1,i,m", 0, v8 }, /* wr r,i,%asrX */
{ "wr", F3(2, 0x30, 1),         F3(~2, ~0x30, ~1)|RD_G0,                "1,i,y", 0, v6 }, /* wr r,i,%y */
{ "wr", F3(2, 0x31, 0),         F3(~2, ~0x31, ~0)|RD_G0|ASI(~0),        "1,2,p", 0, v6 }, /* wr r,r,%psr */
{ "wr", F3(2, 0x31, 1),         F3(~2, ~0x31, ~1)|RD_G0,                "1,i,p", 0, v6 }, /* wr r,i,%psr */
{ "wr", F3(2, 0x32, 0),         F3(~2, ~0x32, ~0)|RD_G0|ASI(~0),        "1,2,w", 0, v6 }, /* wr r,r,%wim */
{ "wr", F3(2, 0x32, 1),         F3(~2, ~0x32, ~1)|RD_G0,                "1,i,w", 0, v6 }, /* wr r,i,%wim */
{ "wr", F3(2, 0x33, 0),         F3(~2, ~0x33, ~0)|RD_G0|ASI(~0),        "1,2,t", 0, v6 }, /* wr r,r,%tbr */
{ "wr", F3(2, 0x33, 1),         F3(~2, ~0x33, ~1)|RD_G0,                "1,i,t", 0, v6 }, /* wr r,i,%tbr */


{ "rd", F3(2, 0x28, 0),                 F3(~2, ~0x28, ~0)|SIMM13(~0),           "M,d", 0, v8 }, /* rd %asr1,r */
{ "rd", F3(2, 0x28, 0),                 F3(~2, ~0x28, ~0)|RS1_G0|SIMM13(~0),    "y,d", 0, v6 }, /* rd %y,r */
{ "rd", F3(2, 0x29, 0),                 F3(~2, ~0x29, ~0)|RS1_G0|SIMM13(~0),    "p,d", 0, v6 }, /* rd %psr,r */
{ "rd", F3(2, 0x2a, 0),                 F3(~2, ~0x2a, ~0)|RS1_G0|SIMM13(~0),    "w,d", 0, v6 }, /* rd %wim,r */
{ "rd", F3(2, 0x2b, 0),                 F3(~2, ~0x2b, ~0)|RS1_G0|SIMM13(~0),    "t,d", 0, v6 }, /* rd %tbr,r */



{ "mov",        F3(2, 0x30, 0),         F3(~2, ~0x30, ~0)|ASI(~0),              "1,2,m", F_ALIAS, v8 }, /* wr r,r,%asrX */
{ "mov",        F3(2, 0x30, 0),         F3(~2, ~0x30, ~0)|RD_G0|ASI(~0),        "1,2,y", F_ALIAS, v6 }, /* wr r,r,%y */
{ "mov",        F3(2, 0x30, 1),         F3(~2, ~0x30, ~1),                      "1,i,m", F_ALIAS, v8 }, /* wr r,i,%asrX */
{ "mov",        F3(2, 0x30, 1),         F3(~2, ~0x30, ~1)|RD_G0,                "1,i,y", F_ALIAS, v6 }, /* wr r,i,%y */
{ "mov",        F3(2, 0x31, 0),         F3(~2, ~0x31, ~0)|RD_G0|ASI(~0),        "1,2,p", F_ALIAS, v6 }, /* wr r,r,%psr */
{ "mov",        F3(2, 0x31, 1),         F3(~2, ~0x31, ~1)|RD_G0,                "1,i,p", F_ALIAS, v6 }, /* wr r,i,%psr */
{ "mov",        F3(2, 0x32, 0),         F3(~2, ~0x32, ~0)|RD_G0|ASI(~0),        "1,2,w", F_ALIAS, v6 }, /* wr r,r,%wim */
{ "mov",        F3(2, 0x32, 1),         F3(~2, ~0x32, ~1)|RD_G0,                "1,i,w", F_ALIAS, v6 }, /* wr r,i,%wim */
{ "mov",        F3(2, 0x33, 0),         F3(~2, ~0x33, ~0)|RD_G0|ASI(~0),        "1,2,t", F_ALIAS, v6 }, /* wr r,r,%tbr */
{ "mov",        F3(2, 0x33, 1),         F3(~2, ~0x33, ~1)|RD_G0,                "1,i,t", F_ALIAS, v6 }, /* wr r,i,%tbr */

{ "mov",        F3(2, 0x28, 0),          F3(~2, ~0x28, ~0)|SIMM13(~0),                  "M,d", F_ALIAS, v8 }, /* rd %asr1,r */
{ "mov",        F3(2, 0x28, 0),          F3(~2, ~0x28, ~0)|RS1_G0|SIMM13(~0),           "y,d", F_ALIAS, v6 }, /* rd %y,r */
{ "mov",        F3(2, 0x29, 0),          F3(~2, ~0x29, ~0)|RS1_G0|SIMM13(~0),           "p,d", F_ALIAS, v6 }, /* rd %psr,r */
{ "mov",        F3(2, 0x2a, 0),          F3(~2, ~0x2a, ~0)|RS1_G0|SIMM13(~0),           "w,d", F_ALIAS, v6 }, /* rd %wim,r */
{ "mov",        F3(2, 0x2b, 0),          F3(~2, ~0x2b, ~0)|RS1_G0|SIMM13(~0),           "t,d", F_ALIAS, v6 }, /* rd %tbr,r */

{ "mov",        F3(2, 0x30, 0), F3(~2, ~0x30, ~0)|ASI_RS2(~0),  "1,y", F_ALIAS, v6 }, /* wr rs1,%g0,%y */
{ "mov",        F3(2, 0x30, 1), F3(~2, ~0x30, ~1),              "i,y", F_ALIAS, v6 },
{ "mov",        F3(2, 0x30, 1), F3(~2, ~0x30, ~1)|SIMM13(~0),   "1,y", F_ALIAS, v6 }, /* wr rs1,0,%y */
{ "mov",        F3(2, 0x31, 0), F3(~2, ~0x31, ~0)|ASI_RS2(~0),  "1,p", F_ALIAS, v6 }, /* wr rs1,%g0,%psr */
{ "mov",        F3(2, 0x31, 1), F3(~2, ~0x31, ~1),              "i,p", F_ALIAS, v6 },
{ "mov",        F3(2, 0x31, 1), F3(~2, ~0x31, ~1)|SIMM13(~0),   "1,p", F_ALIAS, v6 }, /* wr rs1,0,%psr */
{ "mov",        F3(2, 0x32, 0), F3(~2, ~0x32, ~0)|ASI_RS2(~0),  "1,w", F_ALIAS, v6 }, /* wr rs1,%g0,%wim */
{ "mov",        F3(2, 0x32, 1), F3(~2, ~0x32, ~1),              "i,w", F_ALIAS, v6 },
{ "mov",        F3(2, 0x32, 1), F3(~2, ~0x32, ~1)|SIMM13(~0),   "1,w", F_ALIAS, v6 }, /* wr rs1,0,%wim */
{ "mov",        F3(2, 0x33, 0), F3(~2, ~0x33, ~0)|ASI_RS2(~0),  "1,t", F_ALIAS, v6 }, /* wr rs1,%g0,%tbr */
{ "mov",        F3(2, 0x33, 1), F3(~2, ~0x33, ~1),              "i,t", F_ALIAS, v6 },
{ "mov",        F3(2, 0x33, 1), F3(~2, ~0x33, ~1)|SIMM13(~0),   "1,t", F_ALIAS, v6 }, /* wr rs1,0,%tbr */

{ "mov",        F3(2, 0x02, 0), F3(~2, ~0x02, ~0)|RS1_G0|ASI(~0),       "2,d", 0, v6 }, /* or %g0,rs2,d */
{ "mov",        F3(2, 0x02, 1), F3(~2, ~0x02, ~1)|RS1_G0,               "i,d", 0, v6 }, /* or %g0,i,d   */
{ "mov",        F3(2, 0x02, 0), F3(~2, ~0x02, ~0)|ASI_RS2(~0),          "1,d", 0, v6 }, /* or rs1,%g0,d   */
{ "mov",        F3(2, 0x02, 1), F3(~2, ~0x02, ~1)|SIMM13(~0),           "1,d", 0, v6 }, /* or rs1,0,d */

{ "or", F3(2, 0x02, 0), F3(~2, ~0x02, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "or", F3(2, 0x02, 1), F3(~2, ~0x02, ~1),              "1,i,d", 0, v6 },
{ "or", F3(2, 0x02, 1), F3(~2, ~0x02, ~1),              "i,1,d", 0, v6 },

{ "bset",       F3(2, 0x02, 0), F3(~2, ~0x02, ~0)|ASI(~0),      "2,r", F_ALIAS, v6 },   /* or rd,rs2,rd */
{ "bset",       F3(2, 0x02, 1), F3(~2, ~0x02, ~1),              "i,r", F_ALIAS, v6 },   /* or rd,i,rd */

/* This is not a commutative instruction.  */
{ "andn",       F3(2, 0x05, 0), F3(~2, ~0x05, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "andn",       F3(2, 0x05, 1), F3(~2, ~0x05, ~1),              "1,i,d", 0, v6 },

/* This is not a commutative instruction.  */
{ "andncc",     F3(2, 0x15, 0), F3(~2, ~0x15, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "andncc",     F3(2, 0x15, 1), F3(~2, ~0x15, ~1),              "1,i,d", 0, v6 },

{ "bclr",       F3(2, 0x05, 0), F3(~2, ~0x05, ~0)|ASI(~0),      "2,r", F_ALIAS, v6 },   /* andn rd,rs2,rd */
{ "bclr",       F3(2, 0x05, 1), F3(~2, ~0x05, ~1),              "i,r", F_ALIAS, v6 },   /* andn rd,i,rd */

{ "cmp",        F3(2, 0x14, 0), F3(~2, ~0x14, ~0)|RD_G0|ASI(~0),        "1,2", 0, v6 }, /* subcc rs1,rs2,%g0 */
{ "cmp",        F3(2, 0x14, 1), F3(~2, ~0x14, ~1)|RD_G0,                "1,i", 0, v6 }, /* subcc rs1,i,%g0 */

{ "sub",        F3(2, 0x04, 0), F3(~2, ~0x04, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "sub",        F3(2, 0x04, 1), F3(~2, ~0x04, ~1),              "1,i,d", 0, v6 },

{ "subcc",      F3(2, 0x14, 0), F3(~2, ~0x14, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "subcc",      F3(2, 0x14, 1), F3(~2, ~0x14, ~1),              "1,i,d", 0, v6 },

{ "subx",       F3(2, 0x0c, 0), F3(~2, ~0x0c, ~0)|ASI(~0),      "1,2,d", F_ALIAS, v6 },
{ "subx",       F3(2, 0x0c, 1), F3(~2, ~0x0c, ~1),              "1,i,d", F_ALIAS, v6 },

{ "subxcc",     F3(2, 0x1c, 0), F3(~2, ~0x1c, ~0)|ASI(~0),      "1,2,d", F_ALIAS, v6 },
{ "subxcc",     F3(2, 0x1c, 1), F3(~2, ~0x1c, ~1),              "1,i,d", F_ALIAS, v6 },

{ "and",        F3(2, 0x01, 0), F3(~2, ~0x01, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "and",        F3(2, 0x01, 1), F3(~2, ~0x01, ~1),              "1,i,d", 0, v6 },
{ "and",        F3(2, 0x01, 1), F3(~2, ~0x01, ~1),              "i,1,d", 0, v6 },

{ "andcc",      F3(2, 0x11, 0), F3(~2, ~0x11, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "andcc",      F3(2, 0x11, 1), F3(~2, ~0x11, ~1),              "1,i,d", 0, v6 },
{ "andcc",      F3(2, 0x11, 1), F3(~2, ~0x11, ~1),              "i,1,d", 0, v6 },

{ "dec",        F3(2, 0x04, 1)|SIMM13(0x1), F3(~2, ~0x04, ~1)|SIMM13(~0x0001), "r", F_ALIAS, v6 },      /* sub rd,1,rd */
{ "dec",        F3(2, 0x04, 1),             F3(~2, ~0x04, ~1),                 "i,r", F_ALIAS, v8 },    /* sub rd,imm,rd */
{ "deccc",      F3(2, 0x14, 1)|SIMM13(0x1), F3(~2, ~0x14, ~1)|SIMM13(~0x0001), "r", F_ALIAS, v6 },      /* subcc rd,1,rd */
{ "deccc",      F3(2, 0x14, 1),             F3(~2, ~0x14, ~1),                 "i,r", F_ALIAS, v8 },    /* subcc rd,imm,rd */
{ "inc",        F3(2, 0x00, 1)|SIMM13(0x1), F3(~2, ~0x00, ~1)|SIMM13(~0x0001), "r", F_ALIAS, v6 },      /* add rd,1,rd */
{ "inc",        F3(2, 0x00, 1),             F3(~2, ~0x00, ~1),                 "i,r", F_ALIAS, v8 },    /* add rd,imm,rd */
{ "inccc",      F3(2, 0x10, 1)|SIMM13(0x1), F3(~2, ~0x10, ~1)|SIMM13(~0x0001), "r", F_ALIAS, v6 },      /* addcc rd,1,rd */
{ "inccc",      F3(2, 0x10, 1),             F3(~2, ~0x10, ~1),                 "i,r", F_ALIAS, v8 },    /* addcc rd,imm,rd */

{ "btst",       F3(2, 0x11, 0), F3(~2, ~0x11, ~0)|RD_G0|ASI(~0), "1,2", F_ALIAS, v6 },  /* andcc rs1,rs2,%g0 */
{ "btst",       F3(2, 0x11, 1), F3(~2, ~0x11, ~1)|RD_G0, "i,1", F_ALIAS, v6 },  /* andcc rs1,i,%g0 */

{ "neg",        F3(2, 0x04, 0), F3(~2, ~0x04, ~0)|RS1_G0|ASI(~0), "2,d", F_ALIAS, v6 }, /* sub %g0,rs2,rd */
{ "neg",        F3(2, 0x04, 0), F3(~2, ~0x04, ~0)|RS1_G0|ASI(~0), "r", F_ALIAS, v6 }, /* sub %g0,rd,rd */

{ "add",        F3(2, 0x00, 0), F3(~2, ~0x00, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "add",        F3(2, 0x00, 1), F3(~2, ~0x00, ~1),              "1,i,d", 0, v6 },
{ "add",        F3(2, 0x00, 1), F3(~2, ~0x00, ~1),              "i,1,d", 0, v6 },
{ "addcc",      F3(2, 0x10, 0), F3(~2, ~0x10, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "addcc",      F3(2, 0x10, 1), F3(~2, ~0x10, ~1),              "1,i,d", 0, v6 },
{ "addcc",      F3(2, 0x10, 1), F3(~2, ~0x10, ~1),              "i,1,d", 0, v6 },
{ "addx",       F3(2, 0x08, 0), F3(~2, ~0x08, ~0)|ASI(~0),      "1,2,d", F_ALIAS, v6 },
{ "addx",       F3(2, 0x08, 1), F3(~2, ~0x08, ~1),              "1,i,d", F_ALIAS, v6 },
{ "addx",       F3(2, 0x08, 1), F3(~2, ~0x08, ~1),              "i,1,d", F_ALIAS, v6 },
{ "addxcc",     F3(2, 0x18, 0), F3(~2, ~0x18, ~0)|ASI(~0),      "1,2,d", F_ALIAS, v6 },
{ "addxcc",     F3(2, 0x18, 1), F3(~2, ~0x18, ~1),              "1,i,d", F_ALIAS, v6 },
{ "addxcc",     F3(2, 0x18, 1), F3(~2, ~0x18, ~1),              "i,1,d", F_ALIAS, v6 },

{ "smul",       F3(2, 0x0b, 0), F3(~2, ~0x0b, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "smul",       F3(2, 0x0b, 1), F3(~2, ~0x0b, ~1),              "1,i,d", 0, v8 },
{ "smul",       F3(2, 0x0b, 1), F3(~2, ~0x0b, ~1),              "i,1,d", 0, v8 },
{ "smulcc",     F3(2, 0x1b, 0), F3(~2, ~0x1b, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "smulcc",     F3(2, 0x1b, 1), F3(~2, ~0x1b, ~1),              "1,i,d", 0, v8 },
{ "smulcc",     F3(2, 0x1b, 1), F3(~2, ~0x1b, ~1),              "i,1,d", 0, v8 },
{ "umul",       F3(2, 0x0a, 0), F3(~2, ~0x0a, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "umul",       F3(2, 0x0a, 1), F3(~2, ~0x0a, ~1),              "1,i,d", 0, v8 },
{ "umul",       F3(2, 0x0a, 1), F3(~2, ~0x0a, ~1),              "i,1,d", 0, v8 },
{ "umulcc",     F3(2, 0x1a, 0), F3(~2, ~0x1a, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "umulcc",     F3(2, 0x1a, 1), F3(~2, ~0x1a, ~1),              "1,i,d", 0, v8 },
{ "umulcc",     F3(2, 0x1a, 1), F3(~2, ~0x1a, ~1),              "i,1,d", 0, v8 },
{ "sdiv",       F3(2, 0x0f, 0), F3(~2, ~0x0f, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "sdiv",       F3(2, 0x0f, 1), F3(~2, ~0x0f, ~1),              "1,i,d", 0, v8 },
{ "sdiv",       F3(2, 0x0f, 1), F3(~2, ~0x0f, ~1),              "i,1,d", 0, v8 },
{ "sdivcc",     F3(2, 0x1f, 0), F3(~2, ~0x1f, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "sdivcc",     F3(2, 0x1f, 1), F3(~2, ~0x1f, ~1),              "1,i,d", 0, v8 },
{ "sdivcc",     F3(2, 0x1f, 1), F3(~2, ~0x1f, ~1),              "i,1,d", 0, v8 },
{ "udiv",       F3(2, 0x0e, 0), F3(~2, ~0x0e, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "udiv",       F3(2, 0x0e, 1), F3(~2, ~0x0e, ~1),              "1,i,d", 0, v8 },
{ "udiv",       F3(2, 0x0e, 1), F3(~2, ~0x0e, ~1),              "i,1,d", 0, v8 },
{ "udivcc",     F3(2, 0x1e, 0), F3(~2, ~0x1e, ~0)|ASI(~0),      "1,2,d", 0, v8 },
{ "udivcc",     F3(2, 0x1e, 1), F3(~2, ~0x1e, ~1),              "1,i,d", 0, v8 },
{ "udivcc",     F3(2, 0x1e, 1), F3(~2, ~0x1e, ~1),              "i,1,d", 0, v8 },


{ "call",       F1(0x1), F1(~0x1), "L", F_DELAYED, v6 },
{ "call",       F1(0x1), F1(~0x1), "L,#", F_DELAYED, v6 },
{ "call",       F3(2, 0x38, 0)|RD(0xf), F3(~2, ~0x38, ~0)|RD(~0xf)|ASI_RS2(~0), "1", F_DELAYED, v6 }, /* jmpl rs1+%g0, %o7 */
{ "call",       F3(2, 0x38, 0)|RD(0xf), F3(~2, ~0x38, ~0)|RD(~0xf)|ASI_RS2(~0), "1,#", F_DELAYED, v6 },

/* Conditional instructions.

   Because this part of the table was such a mess earlier, I have
   macrofied it so that all the branches and traps are generated from
   a single-line description of each condition value.  John Gilmore. */

/* Define branches -- one annulled, one without, etc. */
#define br(opcode, mask, lose, flags) \
 { opcode, (mask)|ANNUL, (lose),       ",a l",   (flags), v6 }, \
 { opcode, (mask)      , (lose)|ANNUL, "l",     (flags), v6 }


/* Define four traps: reg+reg, reg + immediate, immediate alone, reg alone. */
#define tr(opcode, mask, lose, flags) \
 { opcode, (mask)|IMMED, (lose)|RS1_G0,         "i",     (flags), v6 }, /* %g0 + imm */ \
 { opcode, (mask)|IMMED, (lose),                "1+i",   (flags), v6 }, /* rs1 + imm */ \
 { opcode, (mask), IMMED|(lose),                "1+2",   (flags), v6 }, /* rs1 + rs2 */ \
 { opcode, (mask), IMMED|(lose)|RS2_G0,         "1",     (flags), v6 } /* rs1 + %g0 */


/* Define both branches and traps based on condition mask */
#define cond(bop, top, mask, flags) \
  br(bop,  F2(0, 2)|(mask), F2(~0, ~2)|((~mask)&COND(~0)), F_DELAYED|(flags)), \
  tr(top,  F3(2, 0x3a, 0)|(mask), F3(~2, ~0x3a, 0)|((~mask)&COND(~0)), (flags))

/* Define all the conditions, all the branches, all the traps.  */

cond ("b",      "t",    CONDA, 0),
cond ("ba",     "ta",   CONDA, F_ALIAS), /* for nothing */
cond ("bcc",    "tcc",  CONDCC, 0),
cond ("bcs",    "tcs",  CONDCS, 0),
cond ("be",     "te",   CONDE, 0),
cond ("bg",     "tg",   CONDG, 0),
cond ("bgt",    "tgt",   CONDG, F_ALIAS),
cond ("bge",    "tge",  CONDGE, 0),
cond ("bgeu",   "tgeu", CONDGEU, F_ALIAS), /* for cc */
cond ("bgu",    "tgu",  CONDGU, 0),
cond ("bl",     "tl",   CONDL, 0),
cond ("blt",    "tlt",   CONDL, F_ALIAS),
cond ("ble",    "tle",  CONDLE, 0),
cond ("bleu",   "tleu", CONDLEU, 0),
cond ("blu",    "tlu",  CONDLU, F_ALIAS), /* for cs */
cond ("bn",     "tn",   CONDN, 0),
cond ("bne",    "tne",  CONDNE, 0),
cond ("bneg",   "tneg", CONDNEG, 0),
cond ("bnz",    "tnz",  CONDNZ, F_ALIAS), /* for ne */
cond ("bpos",   "tpos", CONDPOS, 0),
cond ("bvc",    "tvc",  CONDVC, 0),
cond ("bvs",    "tvs",  CONDVS, 0),
cond ("bz",     "tz",   CONDZ, F_ALIAS), /* for e */

#undef cond
#undef br
#undef tr















#define brfc(opcode, mask, lose, flags) \
 { opcode, (mask), ANNUL|(lose), "l",    flags|F_DELAYED, v6 }, \
 { opcode, (mask)|ANNUL, (lose), ",a l", flags|F_DELAYED, v6 }



#define condfc(fop, cop, mask, flags) \
  brfc(fop, F2(0, 6)|COND(mask), F2(~0, ~6)|COND(~(mask)), flags), \
  brfc(cop, F2(0, 7)|COND(mask), F2(~0, ~7)|COND(~(mask)), flags)

#define condf(fop, mask, flags) \
  brfc(fop, F2(0, 6)|COND(mask), F2(~0, ~6)|COND(~(mask)), flags)

condfc("fb",    "cb",    0x8, 0),
condfc("fba",   "cba",   0x8, F_ALIAS),
condfc("fbe",   "cb0",   0x9, 0),
condf("fbz",             0x9, F_ALIAS),
condfc("fbg",   "cb2",   0x6, 0),
condfc("fbge",  "cb02",  0xb, 0),
condfc("fbl",   "cb1",   0x4, 0),
condfc("fble",  "cb01",  0xd, 0),
condfc("fblg",  "cb12",  0x2, 0),
condfc("fbn",   "cbn",   0x0, 0),
condfc("fbne",  "cb123", 0x1, 0),
condf("fbnz",            0x1, F_ALIAS),
condfc("fbo",   "cb012", 0xf, 0),
condfc("fbu",   "cb3",   0x7, 0),
condfc("fbue",  "cb03",  0xa, 0),
condfc("fbug",  "cb23",  0x5, 0),
condfc("fbuge", "cb023", 0xc, 0),
condfc("fbul",  "cb13",  0x3, 0),
condfc("fbule", "cb013", 0xe, 0),

#undef condfc
#undef brfc

{ "jmp",        F3(2, 0x38, 0), F3(~2, ~0x38, ~0)|RD_G0|ASI(~0),        "1+2", F_DELAYED, v6 }, /* jmpl rs1+rs2,%g0 */
{ "jmp",        F3(2, 0x38, 0), F3(~2, ~0x38, ~0)|RD_G0|ASI_RS2(~0),    "1", F_DELAYED, v6 }, /* jmpl rs1+%g0,%g0 */
{ "jmp",        F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|RD_G0,                "1+i", F_DELAYED, v6 }, /* jmpl rs1+i,%g0 */
{ "jmp",        F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|RD_G0,                "i+1", F_DELAYED, v6 }, /* jmpl i+rs1,%g0 */
{ "jmp",        F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|RD_G0|RS1_G0,         "i", F_DELAYED, v6 }, /* jmpl %g0+i,%g0 */
{ "jmp",        F3(2, 0x38, 1), F3(~2, ~0x38, ~1)|RD_G0|SIMM13(~0),     "1", F_DELAYED, v6 }, /* jmpl rs1+0,%g0 */

{ "nop",        F2(0, 4), 0xfeffffff, "", 0, v6 }, /* sethi 0, %g0 */

{ "set",        F2(0x0, 0x4), F2(~0x0, ~0x4), "Sh,d", F_ALIAS, v6 },

{ "sethi",      F2(0x0, 0x4), F2(~0x0, ~0x4), "h,d", 0, v6 },

{ "taddcc",     F3(2, 0x20, 0), F3(~2, ~0x20, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "taddcc",     F3(2, 0x20, 1), F3(~2, ~0x20, ~1),              "1,i,d", 0, v6 },
{ "taddcc",     F3(2, 0x20, 1), F3(~2, ~0x20, ~1),              "i,1,d", 0, v6 },
{ "taddcctv",   F3(2, 0x22, 0), F3(~2, ~0x22, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "taddcctv",   F3(2, 0x22, 1), F3(~2, ~0x22, ~1),              "1,i,d", 0, v6 },
{ "taddcctv",   F3(2, 0x22, 1), F3(~2, ~0x22, ~1),              "i,1,d", 0, v6 },

{ "tsubcc",     F3(2, 0x21, 0), F3(~2, ~0x21, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "tsubcc",     F3(2, 0x21, 1), F3(~2, ~0x21, ~1),              "1,i,d", 0, v6 },
{ "tsubcctv",   F3(2, 0x23, 0), F3(~2, ~0x23, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "tsubcctv",   F3(2, 0x23, 1), F3(~2, ~0x23, ~1),              "1,i,d", 0, v6 },

{ "unimp",      F2(0x0, 0x0), 0xffc00000, "n", F_ALIAS, v6 },

/* This *is* a commutative instruction.  */
{ "xnor",       F3(2, 0x07, 0), F3(~2, ~0x07, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "xnor",       F3(2, 0x07, 1), F3(~2, ~0x07, ~1),              "1,i,d", 0, v6 },
{ "xnor",       F3(2, 0x07, 1), F3(~2, ~0x07, ~1),              "i,1,d", 0, v6 },
/* This *is* a commutative instruction.  */
{ "xnorcc",     F3(2, 0x17, 0), F3(~2, ~0x17, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "xnorcc",     F3(2, 0x17, 1), F3(~2, ~0x17, ~1),              "1,i,d", 0, v6 },
{ "xnorcc",     F3(2, 0x17, 1), F3(~2, ~0x17, ~1),              "i,1,d", 0, v6 },
{ "xor",        F3(2, 0x03, 0), F3(~2, ~0x03, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "xor",        F3(2, 0x03, 1), F3(~2, ~0x03, ~1),              "1,i,d", 0, v6 },
{ "xor",        F3(2, 0x03, 1), F3(~2, ~0x03, ~1),              "i,1,d", 0, v6 },
{ "xorcc",      F3(2, 0x13, 0), F3(~2, ~0x13, ~0)|ASI(~0),      "1,2,d", 0, v6 },
{ "xorcc",      F3(2, 0x13, 1), F3(~2, ~0x13, ~1),              "1,i,d", 0, v6 },
{ "xorcc",      F3(2, 0x13, 1), F3(~2, ~0x13, ~1),              "i,1,d", 0, v6 },

{ "not",        F3(2, 0x07, 0), F3(~2, ~0x07, ~0)|ASI(~0), "1,d", F_ALIAS, v6 }, /* xnor rs1,%0,rd */
{ "not",        F3(2, 0x07, 0), F3(~2, ~0x07, ~0)|ASI(~0), "r", F_ALIAS, v6 }, /* xnor rd,%0,rd */

{ "btog",       F3(2, 0x03, 0), F3(~2, ~0x03, ~0)|ASI(~0),      "2,r", F_ALIAS, v6 }, /* xor rd,rs2,rd */
{ "btog",       F3(2, 0x03, 1), F3(~2, ~0x03, ~1),              "i,r", F_ALIAS, v6 }, /* xor rd,i,rd */

/* FPop1 and FPop2 are not instructions.  Don't accept them.  */

{ "fdtoi",      F3F(2, 0x34, 0x0d2), F3F(~2, ~0x34, ~0x0d2)|RS1_G0, "B,g", 0, v6 },
{ "fstoi",      F3F(2, 0x34, 0x0d1), F3F(~2, ~0x34, ~0x0d1)|RS1_G0, "f,g", 0, v6 },
{ "fqtoi",      F3F(2, 0x34, 0x0d3), F3F(~2, ~0x34, ~0x0d3)|RS1_G0, "R,g", 0, v8 },


{ "fitod",      F3F(2, 0x34, 0x0c8), F3F(~2, ~0x34, ~0x0c8)|RS1_G0, "f,H", 0, v6 },
{ "fitos",      F3F(2, 0x34, 0x0c4), F3F(~2, ~0x34, ~0x0c4)|RS1_G0, "f,g", 0, v6 },
{ "fitoq",      F3F(2, 0x34, 0x0cc), F3F(~2, ~0x34, ~0x0cc)|RS1_G0, "f,J", 0, v8 },


{ "fdtoq",      F3F(2, 0x34, 0x0ce), F3F(~2, ~0x34, ~0x0ce)|RS1_G0, "B,J", 0, v8 },
{ "fdtos",      F3F(2, 0x34, 0x0c6), F3F(~2, ~0x34, ~0x0c6)|RS1_G0, "B,g", 0, v6 },
{ "fqtod",      F3F(2, 0x34, 0x0cb), F3F(~2, ~0x34, ~0x0cb)|RS1_G0, "R,H", 0, v8 },
{ "fqtos",      F3F(2, 0x34, 0x0c7), F3F(~2, ~0x34, ~0x0c7)|RS1_G0, "R,g", 0, v8 },
{ "fstod",      F3F(2, 0x34, 0x0c9), F3F(~2, ~0x34, ~0x0c9)|RS1_G0, "f,H", 0, v6 },
{ "fstoq",      F3F(2, 0x34, 0x0cd), F3F(~2, ~0x34, ~0x0cd)|RS1_G0, "f,J", 0, v8 },

{ "fdivd",      F3F(2, 0x34, 0x04e), F3F(~2, ~0x34, ~0x04e), "v,B,H", 0, v6 },
{ "fdivq",      F3F(2, 0x34, 0x04f), F3F(~2, ~0x34, ~0x04f), "V,R,J", 0, v8 },
{ "fdivs",      F3F(2, 0x34, 0x04d), F3F(~2, ~0x34, ~0x04d), "e,f,g", 0, v6 },
{ "fmuld",      F3F(2, 0x34, 0x04a), F3F(~2, ~0x34, ~0x04a), "v,B,H", 0, v6 },
{ "fmulq",      F3F(2, 0x34, 0x04b), F3F(~2, ~0x34, ~0x04b), "V,R,J", 0, v8 },
{ "fmuls",      F3F(2, 0x34, 0x049), F3F(~2, ~0x34, ~0x049), "e,f,g", 0, v6 },

{ "fdmulq",     F3F(2, 0x34, 0x06e), F3F(~2, ~0x34, ~0x06e), "v,B,J", 0, v8 },
{ "fsmuld",     F3F(2, 0x34, 0x069), F3F(~2, ~0x34, ~0x069), "e,f,H", 0, v8 },

{ "fsqrtd",     F3F(2, 0x34, 0x02a), F3F(~2, ~0x34, ~0x02a)|RS1_G0, "B,H", 0, v7 },
{ "fsqrtq",     F3F(2, 0x34, 0x02b), F3F(~2, ~0x34, ~0x02b)|RS1_G0, "R,J", 0, v8 },
{ "fsqrts",     F3F(2, 0x34, 0x029), F3F(~2, ~0x34, ~0x029)|RS1_G0, "f,g", 0, v7 },

{ "fabss",      F3F(2, 0x34, 0x009), F3F(~2, ~0x34, ~0x009)|RS1_G0, "f,g", 0, v6 },
{ "fmovs",      F3F(2, 0x34, 0x001), F3F(~2, ~0x34, ~0x001)|RS1_G0, "f,g", 0, v6 },
{ "fnegs",      F3F(2, 0x34, 0x005), F3F(~2, ~0x34, ~0x005)|RS1_G0, "f,g", 0, v6 },

{ "faddd",      F3F(2, 0x34, 0x042), F3F(~2, ~0x34, ~0x042), "v,B,H", 0, v6 },
{ "faddq",      F3F(2, 0x34, 0x043), F3F(~2, ~0x34, ~0x043), "V,R,J", 0, v8 },
{ "fadds",      F3F(2, 0x34, 0x041), F3F(~2, ~0x34, ~0x041), "e,f,g", 0, v6 },
{ "fsubd",      F3F(2, 0x34, 0x046), F3F(~2, ~0x34, ~0x046), "v,B,H", 0, v6 },
{ "fsubq",      F3F(2, 0x34, 0x047), F3F(~2, ~0x34, ~0x047), "V,R,J", 0, v8 },
{ "fsubs",      F3F(2, 0x34, 0x045), F3F(~2, ~0x34, ~0x045), "e,f,g", 0, v6 },

#define CMPFCC(x)       (((x)&0x3)<<25)

{ "fcmpd",                F3F(2, 0x35, 0x052),            F3F(~2, ~0x35, ~0x052)|RD_G0,  "v,B",   0, v6 },
{ "fcmped",               F3F(2, 0x35, 0x056),            F3F(~2, ~0x35, ~0x056)|RD_G0,  "v,B",   0, v6 },
{ "fcmpq",                F3F(2, 0x34, 0x053),            F3F(~2, ~0x34, ~0x053)|RD_G0,  "V,R", 0, v8 },
{ "fcmpeq",               F3F(2, 0x34, 0x057),            F3F(~2, ~0x34, ~0x057)|RD_G0,  "V,R", 0, v8 },
{ "fcmps",                F3F(2, 0x35, 0x051),            F3F(~2, ~0x35, ~0x051)|RD_G0, "e,f",   0, v6 },
{ "fcmpes",               F3F(2, 0x35, 0x055),            F3F(~2, ~0x35, ~0x055)|RD_G0, "e,f",   0, v6 },

{ "cpop1",      F3(2, 0x36, 0), F3(~2, ~0x36, ~1), "[1+2],d", F_ALIAS, v6 },
{ "cpop2",      F3(2, 0x37, 0), F3(~2, ~0x37, ~1), "[1+2],d", F_ALIAS, v6 },
    

};

const int bfd_sparc_num_opcodes = ((sizeof sparc_opcodes)/(sizeof sparc_opcodes[0]));

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "disasm_sparc.hh"

# include "_disasm_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  static nmethod* printNM;      // nmethod being printed
  static PcDesc* printPC;       // current PcDesc
  static ScopeDesc* printScope; // current scope

  static Location regToMatch;   // param. for printMatchingSlot
  static char* regName;         // result of reg_names
  static bool  firstMatch;      // for printMatchingSlot/reg_names
  static PrimDesc* printPD;     // last primitive address printed
  
  static void printMatchingSlot(NameDesc* nd, char* name) {
    if (nd->hasLocation() && nd->location() == regToMatch) {
      if (firstMatch) {
        firstMatch = false;
        strcat(regName, "(");
      } else {
        strcat(regName, "/");
      }
      strcat(regName, name);
    }
  }

  char* reg_names(fint reg) {
    // try to find source-level name(s) for reg
    // inefficient but only used during debugging, so what
    regName = NEW_RESOURCE_ARRAY(char, 80);
    firstMatch = true;
    regToMatch = Location(reg);
    sprintf(regName, "%s", RegisterNames[reg]);
    if (printPC == NULL) return regName;
    fint bci = printPC->byteCode;
    for (ScopeDesc* scope = printScope; scope; scope = scope->sender()) {
      scope->doForNames(bci, printMatchingSlot);
      if (!scope->isTop()) bci = scope->senderByteCodeIndex();
    }
    if (!firstMatch) strcat(regName, ")");
    return regName;
  }

  void print_stack_temp_name(fint offset) {
    // The disassembler just printed something like "ld [%sp+offset";
    // augment it with the symbolic name for stack temps if possible.
    // The code below isn't perfect -- doesn't distinguish extra args
    // from stack temps.
    fint size = printNM->frameSize();
    fint off, t;
    for (t = StackLocations;
         (off = spOffset(Location(t), size)) != offset && off > 64;
         t++) ;
    if (off > 64) {
      putchar('/');
      lprintf("%s", locationName(Location(t)));
    }
  }

  char* freg_names(fint reg) {
    static char* fnames[] = {
       "f0",  "f1",  "f2",  "f3",  "f4",  "f5",  "f6",  "f7",  "f8",  "f9",
      "f10", "f11", "f12", "f13", "f14", "f15", "f16", "f17", "f18", "f19",
      "f20", "f21", "f22", "f23", "f24", "f25", "f26", "f27", "f28", "f29",
      "f30", "f31" };
    return fnames[reg];
  }

  void print_address(CORE_ADDR memaddr) {
    // print memaddr decorated with Self-specific annotations
    nmethod* nm;
    CacheStub* pic;
    if (oop(memaddr)->is_mem()) {
      if (Memory->really_contains(memaddr) &&
          Memory->spaceFor(memaddr)->objs_contains(memaddr) &&
          memOop(memaddr)->mark()->is_mark()) {
        // looks like a real oop
        lprintf("%s", oop(memaddr)->debug_print());
      } else {
        lprintf("%#lx", memaddr);         // not an oop
      }
    } else if (oop(memaddr)->is_float()) {
      lprintf("%#lx (%f)", memaddr,
             floatOop(memaddr)->value());
    } else if (Memory->code->iZone->contains(memaddr) &&
               (nm = nmethod::findNMethod_maybe(memaddr)) != NULL &&
               nm != printNM) {
      lprintf("%#lx (nmethod %#lx \"%s\")",
             (long unsigned)memaddr, (long unsigned)nm,
             selector_string(nm->key.selector));
    } else if (Memory->code->stubs->contains(memaddr) &&
               (pic = findCacheStub_maybe(memaddr)) != NULL) {
      lprintf("%#lx (PIC %#lx \"%s\")",
             (long unsigned)memaddr, (long unsigned)pic,
             selector_string(pic->sd()->selector()));
    } else if ((printPD = getPrimDescOfFirstInstruction(memaddr, true)) != NULL) {
      lprintf("%s", printPD->name());
    } else {
      lprintf("%#lx", memaddr);
    }
  }

  inline void startLine(CORE_ADDR pc) {
    lprintf("%#8lx: ", pc);
  }
  
  static void printOneInstruction(CORE_ADDR pc) {
    startLine(pc);
    print_insn(pc);
    putchar('\n');
  }

  static void printRegisterMaskLine(CORE_ADDR pc) {
    startLine(pc);
    int32 mask = *(int32*)pc;
    lprintf(".data %#lx ", mask);
    printMask(mask);
    putchar('\n');
  }
    

  static CORE_ADDR printCall(nmethod* nm, CORE_ADDR pc, addrDesc* loc) {
    // pc is the beginning of an inline cache or primitive call
    // return last pc printed
    printPD = NULL;
    printOneInstruction(pc);        // print call & set primDesc
    PrimDesc* pd = printPD;
    printOneInstruction(pc + 4);    // delay slot
    if (loc->isPrimitive()) {
      // NB: pd can be NULL, e.g. for jump to SendMessage_stub in prologue
      if (pd && pd->canScavenge()) {
        printRegisterMaskLine(pc + 8);
        return pc + 8;
      } else {
        return pc + 4;
      }
    } else {
      printRegisterMaskLine(pc + 8);
      sendDesc* sd = loc->asSendDesc(nm);
      printOneInstruction(pc + 12); // NLR code
      printOneInstruction(pc + 16); 
      int32* end = (int32*) (((char*)sd) + sd->endOffset());
      for (int32* instp = (int32*)(pc + 20); instp < end; instp++) {
        startLine((CORE_ADDR)instp);
        lprintf(".data ");
        print_address((CORE_ADDR)*instp);
        if (instp == (int32*)((char*)sd + sendDesc::lookupType_offset)) {
          putchar(' ');
          printLookupType(sd->raw_lookupType());
        }
        putchar('\n');
      }
      return (CORE_ADDR)end - 4;
    }
  }


  void print_code(nmethod* nm, CORE_ADDR start, CORE_ADDR end) {
    ResourceMark rm;
    printNM = nm; printPC = NULL; printScope = NULL;
    putchar('\n');
    for (CORE_ADDR p = start; p < end; p += 4) {
      ResourceMark rm2;
      if (!nm->isAccess()) {
        PcDesc* pc = nm->containingPcDesc(p);
        printScope = nm->containingScopeDesc(p);
        if (printPC == NULL || !pc->source_equals(printPC)) {
          lprintf("%8s  %s (%d)@%d:", "",
                 selector_string(printScope->key.selector),
                 printScope->offset(), pc->byteCode);
          if (pc->byteCode >= 0 && printScope->isCodeScope()) {
            methodMap* mm = printScope->method_map();
            mm->print_byteCode_at(pc->byteCode);
          }
          putchar('\n');
        }
        printPC = pc;
      }
      addrDesc* loc;
      if ((loc = nm->addrDesc_at(p)) != NULL && loc->isCall()) {
        p = printCall(nm, p, loc);
      } else {
        lprintf("%#8lx: ", p);
        print_insn(p);
        putchar('\n');
      }
    }
    printNM = 0;
  }

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sendDesc_sparc.hh"
# include "_sendDesc_sparc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

void sendDesc::init_platform() {}


char* sendDesc::jump_addr() {
 int32* c = (int32*)jump_addr_addr();
 return (char*) getCallImm(c);
}

void sendDesc::set_jump_addr(char* t) {
  char** jaa = jump_addr_addr();
  setCallImm((int32*)jaa, int32(t));
}


void printMask(RegisterString mask) {
  RegisterString regs = mask << L0;
  printAllocated(regs);
  lprintf("+{");
  bool first = true;
  unsigned r = unsigned(mask) >> (NumInRegisters + NumLocalRegisters);
  for (fint d = 0; r; d++, r >>= 1) {
    if (isSet(r, 0)) {
      if (first) {
        first = false;
      } else {
        lprintf(",");
      }
      lprintf("T%ld", long(d));
      Location d1 = Location(d);
      while(isSet(r, 0)) { d ++, r >>= 1; }
      if (d - 1 > d1) lprintf("-T%ld", long(d - 1));
    }
  }
  lprintf("}");
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

  
# pragma implementation "cacheStub_sparc.hh"
# pragma implementation "cacheStub_inline_sparc.hh"
# include "_cacheStub_sparc.cpp.incl"
  
# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


static pc_t prevAddr;

Label* CacheStub::prologue(bool immediateOnly) {
  // prologue:  andcc  rcvr, 1
  //  bnz,a  _mapTest
  //          load   [rcvr+map_offset], t
  // *if nsmi
  //    andcc  rcvr, Tag_Mask
  //    bne    _next
  //    <jmpl smi_addr>
  //  *if nfloat
  // next:
  //    <jmpl float_addr>
  //  *endif float
  // *else if nfloat
  //    andcc  rcvr, Float_Mask
  //    beq    _next
  //    <jmpl float_addr>
  // *endif
  // * if !(nsmi && nfloat)
  // next:
  //   nop
  //   bra,a   _miss
  // *endif
  // mapTest:  
  //  setHi  ..., t1                       (may be delay slot)
  assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
  prevAddr = NULL;
  Label* result = NULL;
  Label* next = NULL;
  Label* mapTest = NULL;
  char *floatAddr, *smiAddr;
  computeJumpAddr(nsmi, theSendDesc, stsmi, smiAddr);
  computeJumpAddr(nfloat, theSendDesc, stfloat, floatAddr);
  
  if (immediateOnly)
    ;
  else if (FastMapTest  &&  nsmi == NULL  &&  nfloat == NULL)
    a->LoadI(ReceiverReg, map_offset(), Temp1);

  else {
    a->AndCCI(ReceiverReg, Mem_Tag, G0);
    mapTest = a->BneForward(true);
    a->LoadI(ReceiverReg, map_offset(), Temp1); // in delay slot, only do me if branch goes
  }
 
  if (nsmi) {
    n[newMethods] = nsmi;
    st[newMethods++] = stsmi;
    if (SICCountIntTagTests) a->markTagTest(1);
    a->AndCCI(ReceiverReg, Tag_Mask, G0);
    next = a->BneForward(true);
    a->AndCCI(ReceiverReg, Float_Tag, G0);    // test for float (delay slot)
    jump(smiAddr);
    a->Nop();
    if (nfloat) {
      n[newMethods] = nfloat;
      st[newMethods++] = stfloat;
      next->define();
      if (immediateOnly) {
        // need to test float tag (test done in delay slot of int case)
        result = a->BeqForward(false);
      } else {
        // already tested for memTag, so it must be a float
      }
      jump(floatAddr);
      a->Nop();
    } else {
      result = next;
    }
  } else if (nfloat) {
    n[newMethods] = nfloat;
    st[newMethods++] = stfloat;
    a->AndCCI(ReceiverReg, Float_Tag, G0);
    result = a->BeqForward(false);
    jump(floatAddr);
    a->Nop();
  } else {
    assert(!nsmi && !nfloat, "just checkin'");
    if (next) next->define();
    if (FastMapTest) {
      assert(!next && !mapTest, "should not be set");
    } else {
      result = a->BraForward(true);
    }
  }
  if (mapTest) mapTest->define();
  return result;
}

Label* CacheStub::test(oop map, pc_t addr, Label* prev) {
  // first map test:
  //  cmp    t, map
  //  bne,a  _next
  
  // all others:
  //  <jump  prevAddr>        generate jump for prev case (in delay slot)
  //  cmp    t, map
  //  bne,a  _next

  if (prevAddr != NULL) {
    jump(prevAddr);
    prev->define();
  }
  a->SetHiO(map, Temp2);                      // load map
  a->AddO(Temp2, map, Temp2);
  a->SubCCR(Temp1, Temp2, G0);                // test
  prevAddr = addr;
  return a->BneForward(false);
}

void CacheStub::finish(Label* miss, Label* prev) {
  if (prevAddr) {
    jump(prevAddr);
    prev->define();
  }
  if (miss) miss->define();
  void* addr = theSendDesc->lookupRoutine();
  a->SetHiP(addr, Temp2);
  a->JmpLP(Temp2, addr, G0);
  if (SICCountTypeTests) {
    a->endTypeTest();
  } else {
    a->Nop();
  }
}
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "diDesc_sparc.hh"
# include "_diDesc_sparc.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

pc_t DIDesc::jump_addr() {
  return (pc_t) getJumpImm((int32*)jump_addr_addr());
}


void DIDesc::set_jump_addr(pc_t insts) {
  pc_t* addr = jump_addr_addr();
  setJumpImm((int32*) addr, int32(insts));
}
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "deadBlockNode_sparc.hh"

# include "_deadBlockNode_sparc.cpp.incl"

# ifdef SIC_COMPILER


  char* DeadBlockNode::print_string(char* buf, bool printAddr) {
    char* b = buf;
    mysprintf(buf, "DeadBlockTrap");
    if (printAddr) mysprintf(buf, "      p *(DeadBlockNode*)%#lx", this);
    return b;
  }

  void initDeadBlockNode() {} // nothing to do

# endif // SIC_COMPILER
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "sic_sparc.hh"
# include "_sic_sparc.cpp.incl"


# ifdef SIC_COMPILER
 
// On SPARC, can save registers in their usual window-flush place.
// So, don't have to keep uplevel-read pregs in memory,
// can put them in registers and flush them to memory when
// needed. -- dmu 12/02
bool SICAllocator::keepUplevelRPRegsInMemory = false; 

void SICompiler::initializeForPlatform() { }


int32 SICompiler::stackTempCount() {
  return stackLocCount + nonRegisterArgCount();
}


void SICompiler::check_flushability(PReg*) {}
void SICompiler::cope_with_uplevel_access_to(PReg*) {}


# endif // SIC_COMPILER
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "genHelper_sparc.hh"

# include "_genHelper_sparc.cpp.incl"

# if defined(SIC_COMPILER)


  fint SICGenHelper::spOffset(Location l) {
    return ::spOffset(l, theSIC->frameSize());
  }

  fint SICGenHelper::spOffset(Location l, nmethod* nm) {
    return ::spOffset(l, nm->frameSize());
  }

  void SICGenHelper::jumpTo(void* target, Location reg, Location link) {
    a->SetHiP(target, reg);
    a->JmpLP(reg, target, link);
  }

  void SICGenHelper::genCountCode(int32* counter) {
    a->Comment("count # calls");
    a->SetHiA((void*)counter, Temp2);
    a->LoadA(Temp2, (void*)counter, Temp1);
    a->AddI(Temp1, 1, Temp1);
    a->StoreA(Temp2, (void*)counter, Temp1);
  }

  Location SICGenHelper::loadImmediateOop(ConstPReg* r, Location dest, bool mustMove) {
    // load oop from ConstPR; return location containing the oop
    if (r->loc == UnAllocated) {
      if (r->constant == 0 && !mustMove) {
        return G0;
      }
      else {
        loadImmediateOop(r->constant, dest, false);
        return dest;
      }
    } else if (mustMove) {
      a->OrR(r->loc, G0, dest);
      return dest;
    }
    else
      return r->loc;
  }
  
  void SICGenHelper::loadImmediateOop(oop p, Location dest, bool isInt) {
    assert(isRegister(dest), "must be a register");
    if (p == 0) {
      a->OrR(G0, G0, dest);
    } else if ((isInt || !p->is_mem()) && isImmediate(smiOop(p))) {
      a->OrI(G0, (int)p, dest);
    } else {
      a->SetHiO(p, dest);                       // load high part of value
      a->AddO(dest, p, dest);                   // add low part 
    }
  }

  void SICGenHelper::load(Location src, fint srcOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->LoadI(src, srcOffset, dest);
  }

  void SICGenHelper::store(Location src, fint dstOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->StoreI(dest, dstOffset, src);
  }

  void SICGenHelper::moveRegToReg(Location srcReg, Location destReg) {
    assert(isRegister(srcReg) && isRegister(destReg), "not a register");
    a->OrR(srcReg, G0, destReg);
  }

  // must be a VMAddressOperand operand
  void SICGenHelper::setToZeroA(void* addr, Location tempReg) {
    assert(isRegister(tempReg), "not a register");
    a->SetHiA(addr, tempReg);
    a->StoreA(tempReg, addr, G0);
  }

  void SICGenHelper::setToZero(Location dest) {
    genHelper->moveRegToLoc(G0, dest);
  }


# ifdef UNUSED
  void SICGenHelper::checkRecompilation(fint countID) {
    // test for recompilation
    //   sethi &counter, t3
    //   load  [t3 + lo%(&counter)], t4
    //   add t4, 1, t4
    //   cmp t4, recompileLimit
    //   bne ok
    //   store t4, [t3 + lo%(&counter)]
    //   <jumpTo recompiler>
    //   nop
    // ok:

    // di recompilation doesn't work right now - see recompile.c
    if (theSIC->diLink) return;
    Assembler* ass = theAssembler;

    ass->Comment("test for recompilation");
    void* counter = &useCount[countID];
    ass->SetHiA(counter, Temp3);
    ass->LoadA(Temp3, counter, Temp2);
    ass->AddI(Temp2, 1, Temp2);
    fint limit = recompileLimit(0);
    if (limit < maxImmediate) {
      ass->SubCCI(Temp2, limit, G0);
    } else {
      ass->SetHiI2(limit, Temp1);           // limit is multiple of 1024
      ass->SubCCR(Temp2, Temp1, G0);
    }
    Label* ok = ass->BneForward(false);
    // call recompiler
    void* fnaddr = first_inst_addr(
                     theSIC->diLink ? Memory->zone->DIRecompile_stub_td()
                                    : Memory->zone->  Recompile_stub_td() );
    Location linkReg = theSIC->diLink ? DIRecompileLinkReg : RecompileLinkReg;
    jumpTo(fnaddr, linkReg, linkReg);
    // The store below is always executed so that we will call the recompiler 
    // exactly once (even if it cannot recompile for some reason).
    ok->define();
    assert(Temp3 != linkReg, "counter addr reg will be trashed by jump");
    ass->StoreA(Temp3, counter, Temp2);
  }
# endif // UNUSED
    
  void SICGenHelper::smiOop_prologue(char* missHandler) {
    //   andcc rr, Tag_Mask, g0
    //   beq   _cache_hit
    //   sethi missHandler, t
    //   jmpl t, missHandler, g0
    //   nop
    // _cache_hit:

    if (SICCountIntTagTests) a->markTagTest(1);
    a->AndCCI(LReceiverReg, Tag_Mask, G0);    // test for integer tag
    Label* hit = a->BeqForward(false);       // branch if receiver is an int
    jumpTo(missHandler, Temp1, G0);
    if (SICCountTypeTests) {
      a->endTypeTest();
    } else {
      a->Nop();
    }
    hit->define();
  }

  void SICGenHelper::floatOop_prologue(char* missHandler) {
    //   andcc rr, Float_Tag, g0
    //   bne  _cache_hit
    //   sethi missHandler, t
    //   jmpl t, missHandler, g0
    //   nop
    // _cache_hit:

    a->AndCCI(LReceiverReg, Float_Tag, G0);    // test for float tag
    Label* hit = a->BneForward(false);        // branch if receiver is a float
    jumpTo(missHandler, Temp1, G0);
    if (SICCountTypeTests) {
      a->endTypeTest();
    } else {
      a->Nop();
    }
    hit->define();
  }

  void SICGenHelper::memOop_prologue(mapOop receiverMapOop,
                                     char* missHandler) {
    //   andcc rr, Mem_Tag, g0
    //   bne,a _check_receiver_map
    //   load rr, map_offset - Mem_Tag, t3
    // _miss:
    //   sethi missHandler, t3
    //   jmpl t3, missHandler, g0
    // _check_receiver_map:
    //   <loadImmediateOop receiver_map, t4>
    //   subcc t3, t4, g0
    //   bne  _miss
    //   nop
    // _cache_hit:

    if (FastMapTest) {
      a->LoadI(LReceiverReg, map_offset(), Temp1);   // load map
      loadImmediateOop(receiverMapOop, Temp2);   // load customization map
      a->SubCCR(Temp1, Temp2, G0);      // compare against actual map
      Label* ok = a->BeqForward(false); // jump to body of nmethod
      jumpTo(missHandler, Temp2, G0);
      if (SICCountTypeTests) {
        a->endTypeTest();
      } else {
        a->Nop();       // must be here in case next instr is a save
      }
      ok->define();
    } else {
      Label* checkMap = NULL;
      a->AndCCI(LReceiverReg, Mem_Tag, G0);  // test for mem tag
      checkMap = a->BneForward(true);       // branch if receiver is a mem oop
      a->LoadI(LReceiverReg, map_offset(), Temp1);   // load map in delay slot
      DefinedLabel miss(a->printing);
      jumpTo(missHandler, Temp2, G0);
      if (checkMap) checkMap->define();
      loadImmediateOop(receiverMapOop, Temp2);   // load customization map
      a->SubCCR(Temp1, Temp2, G0);      // compare against actual map
      a->Bne(&miss, false);             // jump to miss if no match
      if (SICCountTypeTests) {
        a->endTypeTest();
      } else {
        a->Nop();       // must be here in case next instr is a save
      }
    }
  }

  void SICGenHelper::checkOop(Label& general, oop what, Location reg) {
    // test for inline cache hit (selector, delegatee)
    assert(Temp1 != PerformSelectorLoc && Temp1 != PerformDelegateeLoc,
           "wrong register setup");
    loadImmediateOop(what, Temp1);              // load hard-wired value
    a->SubCCR(reg, Temp1, G0);                  // compare against actual value
    if (general.isDefined()) {
      a->Bne(&general, false);                  // reuse miss handler
      a->Nop();
    } else {
      Label* hit = a->BeqForward(false);
      general.define();
      jumpTo(Memory->code->trapdoors->SendMessage_stub_td(), Temp1, G0);
      a->Nop();
      hit->define();
    }
  }

  fint SICGenHelper::verifyParents(objectLookupTarget* target,
                                   Location t,
                                   fint count) {
    assignableSlotLink* l = target->links;
    assert(l != 0, "expecting an assignable parent link");
      
    for (;;) {
      a->LoadI(t, smiOop(l->slot->data)->byte_count() - Mem_Tag, Temp1);
      // load assignable parent slot value
      Label* ok;
      Map* targetMap = l->target->obj->map();
      if (l->target->value_constrained) {
        // constraint for a particular oop (ambiguity resolution)
        loadImmediateOop(l->target->obj, Temp2);         // load assumed value
        a->SubCCR(Temp1, Temp2, G0);            // compare values
        ok = a->BeqForward(false);              // branch if value OK
        if (l->target->links) a->Nop();
      } else {
        // check if map of parent is correct
        if (targetMap == Memory->smi_map) {
          a->AndCCI(Temp1, Tag_Mask, G0);       // test for integer tag
          ok = a->BeqForward(false);            // branch if parent is integer
          if (l->target->links) a->Nop();
        } else if (targetMap == Memory->float_map) {
          a->AndCCI(Temp1, Float_Tag, G0);      // test for float tag
          ok = a->BneForward(false);            // branch if parent is a float
          if (l->target->links) a->Nop();
        } else {
          Label* miss = NULL;
          if (!FastMapTest) {
            a->AndCCI(Temp1, Mem_Tag, G0);      // test for mem tag
            Label* mem = a->BneForward(true);   // branch if parent is mem oop
            a->LoadI(Temp1, map_offset(), Temp2); // load receiver map
            miss = a->BraForward(true);         // branch to diLookup section
            mem->define();
          } else {
            a->LoadI(Temp1, map_offset(), Temp2); // load receiver map
          }
          loadImmediateOop(targetMap->enclosing_mapOop(), Temp3); // load map constraint
          a->SubCCR(Temp2, Temp3, G0);          // compare w/ parent's map
          ok = a->BeqForward(false);            // correct
          if (l->target->links) a->Nop();
          if (miss) miss->define();
        }
      }
      void* addr = Memory->code->trapdoors->SendDIMessage_stub_td();
      a->SetHiD(addr, Temp1);
      a->JmpLD(Temp1, addr, DILinkReg);
      a->OrI(G0, count, DICountReg);            // count of parents verified
      a->Data(0);                               // first part of DI nmln
      a->Data(0);                               // second part of DI nmln
      ok->define();
      
      count ++;
      if (l->target->links) count = verifyParents(l->target, Temp1, count);
      
      l = l->next;
      if (l == 0) break;
      // if multiple dynamic parents, reload slot holder before looping (HACK!)
      t = loadPath(Temp1, target, LReceiverReg);
    }
    
    return count;
  }
    

# endif // SIC_COMPILER
/* Sun-$Revision: 30.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "longRegString_sparc.hh"

# include "_longRegString_sparc.cpp.incl"

# ifdef SIC_COMPILER


void LongRegisterString::allocate(Location l) { doAllocate(l); }


# endif // SIC_COMPILER
/* Sun-$Revision: 30.21 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "node_sparc.hh"

# include "_node_sparc.cpp.incl"

# if defined(SIC_COMPILER)

  void BasicNode::genBranch() {
    l = l->unify(theAssembler->BraForward(true));
  }

  void PrologueNode::flushRegisterWindows() {
    // ta g0, ST_FLUSH_WINDOWS
    theAssembler->TrapI(G0, ST_FLUSH_WINDOWS);    // flush register windows
  }

  void PrologueNode::clearStackLocations() {
    // The mask in the inline cache marks the regs + the first ntemps stack 
    // locs.  Clear stack locations beyond the first ntemps.
    
    fint ntemps = BitsPerWord - (NumInRegisters + NumLocalRegisters);
    fint extraFrameSize = frameSizeToStackTempCount(thisFrameSize);
    // initialize extra stack locations 
    // potential inefficiency: if local is allocated to the stack, it need
    // not be initialized here.
    // NB: all FP offsets are negative 
    if (extraFrameSize > ntemps) {
      // initialize extra stack locations [start, end)
      // try to use std instructions for smaller code
      // After much head-scratching, I have concluded that Urs
      // adds 1 because local_slots_offset points to where the
      // "first" (topmost) local slot is, as opposed to beyond it.
      // For example if ntemps is 0, and extraFrameSize is 2,
      // we would want to zap local_slots_offset-1, and local_slots_offset.
      // Note that the loop goes [start, end). -- dmu
      fint start = ( local_slots_offset - extraFrameSize ) + 1;
      fint end   = ( local_slots_offset - ntemps         ) + 1;
      assert(start < end, "oops");
      if (start & 1) {
        // first word is unaligned
        theAssembler->StoreI(FP, start * oopSize, G0);
        start++;
      }
      if (end & 1) {
        // last doubleword store would overwrite one word too much
        theAssembler->StoreI(FP, (end - 1) * oopSize, G0);
        end--;
      }
      if (start < end) {
        theAssembler->OrR(G0, G0, G1);                // clear g1
        for (fint i = start; i < end; i += 2)
          theAssembler->StoreDI(FP, i * oopSize, G0);
      }
    }
  }

  void PrologueNode::createStackFrame() {
    assert(haveStackFrame(), "shouldn't be creating a stack frame");

    thisFrameSize = theSIC->frameSize(); 
    assert((thisFrameSize & (frame_word_alignment-1)) == 0, "frame size must be even");

    theAssembler->SaveI(SP, thisFrameSize * -oopSize, SP); // create stack frame
    theSIC->_frameCreationOffset = theAssembler->offset();
    
    clearStackLocations();
  }

  void PrologueNode::prePrologue() {
    // CAUTION: use only Temp1/4 for temps in prologue; other temps
    // may contain lookup parameters.
    assert(Temp1 != PerformSelectorLoc && Temp1 != PerformDelegateeLoc,
           "will trash lookup parameters");
    assert(Temp2 != PerformSelectorLoc && Temp2 != PerformDelegateeLoc,
           "will trash lookup parameters");
  }

  void PrologueNode::postPrologue() {
    MethodKind kind = isAccessMethod ? MethodKind(-1) : theSIC->method()->kind();
    switch (kind) {
    case BlockMethodType:
      if (theSIC->needRegWindowFlushes) flushRegisterWindows();
      break;
    case OuterMethodType:
      if (needToFlushRegWindow) {     // we inlined the receiver block
        if (theSIC->needRegWindowFlushes) flushRegisterWindows();
      } else {
        // receiver is parent, do nothing
      }
      break;
    default:
      fatal1("unknown kind: %ld", kind);
      break;
    }
  }
  
  void LoadIntNode::gen() {
    BasicNode::gen();
    if (isRegister(_dest->loc) && isImmediate(smiOop(value))) {
      theAssembler->OrI(G0, value, _dest->loc); // common case
    }
    else if (isRegister(_dest->loc)) {
      theAssembler->SetHiI(value, _dest->loc);
      theAssembler->AddI(_dest->loc, value, _dest->loc);
    }
    else {
      theAssembler->SetHiI(value, Temp1);
      theAssembler->AddI(Temp1, value, Temp1);
      theAssembler->StoreI(SP, spOffset(_dest->loc), Temp1);
    }
  }
  
  void StoreOffsetNode::gen() {
    BasicNode::gen();
    Location b = genHelper->moveToReg(base, Temp1);
    Location t = Temp2;
    if (_src->isConstPReg()) {
      // store constant
      ConstPReg* value = (ConstPReg*)_src;
      oop p = value->constant;
      // don't need to check-store if oop is old - old objs will never become
      // new again
      needCheckStore = needCheckStore && p->is_new(); // ints/floats aren't new
      if (p == 0) {
        theAssembler->StoreI(b, offset, G0);
      } else {
        assert(b != t, "must be different");
        t = genHelper->loadImmediateOop(value, t, false);
        theAssembler->StoreI(b, offset, t);
      }
    } else  {
      if (isRegister(_src->loc)) {
        theAssembler->StoreI(b, offset, _src->loc);
      } else {
        theAssembler->LoadI(SP, spOffset(_src->loc), t);
        theAssembler->StoreI(b, offset, t);
      }
    }
    if (needCheckStore) {
      // do a check-store
      assert(isRegister(b), "base reg of check_store must be in a register");
      if (offset > card_size || !AllowOffsetCheckStores) {
        // use slow check-store sequence
        // (marked card may be off by one, but not more)
        theAssembler->AddI(b, offset, Temp1);
        b = Temp1;
      }
      theAssembler->SrlI(b, card_shift, Temp1);         // shift target addr 
      theAssembler->StoreBR(Temp1, ByteMapBaseReg, G0); // set byte in map
    }
  }
  
  void AssignNode::genOop() {
    ConstPReg* value = (ConstPReg*)_src;
    Location src = value->loc;
    if (src != UnAllocated) {
      // value is already in src register
      genHelper->moveRegToLoc(src, _dest->loc);
    } else if (isRegister(_dest->loc)) {
      genHelper->loadImmediateOop(value->constant, _dest->loc);
    } else {
      Location t = Temp1;
      oop c = value->constant;
      if (c) {
        genHelper->loadImmediateOop(c, t);
      } else {
        t = G0;
      }
      theAssembler->StoreI(SP, spOffset(_dest->loc), t);
    }
  }
  
  void CallNode::nlrCode() {
    if (nlrPoint()) {
      // branch to NLR code
      Label* l_ = theAssembler->BraForward(true);
      nlrPoint()->l = l_->unify(nlrPoint()->l);
      theAssembler->Nop();
    } else {
      // continue NLR (return through caller's inline cache)
      theAssembler->JmpLI(ReturnAddr, sendDesc::non_local_return_offset, G0);
      theAssembler->RestoreR(G0, G0, G0); 
    }
  }
  
  void SendNode::gen() {
    BasicNode::gen();
    offset = theAssembler->offset();
    assert(bci() != IllegalBCI, "should have legal bci");
    genPcDesc();
    genBreakpointBeforeCall();
    theAssembler->CallB(Memory->code->trapdoors->SendMessage_stub_td());
    theAssembler->Nop();
    theAssembler->Data(mask());
    nlrCode();
    theAssembler->Zero();       // nmlns
    theAssembler->Zero();
    if (sel != badOop) {
      if (isPerformLookupType(l)) {
        assert_smi(sel, "should be an integer argcount");
        theAssembler->Data(smiOop(sel)->value());       // really arg count
      } else {
        assert_string(sel, "should be a string constant");
        theAssembler->Data(sel);                        // constant selector
      }
    }
    if ((l & UninlinableSendMask) == 0) theSIC->noInlinableSends = false;
    theAssembler->Data(l);
    verifySendInfo();
    if (del) {
      assert(needsDelegatee(l), "shouldn't have a delegatee");
      theAssembler->Data(del);
    }
  }
  
  void PrimNode::gen() {
    BasicNode::gen();
    assert(bci() != IllegalBCI, "should have legal bci");
    if (pd->canWalkStack()) genPcDesc();
    theAssembler->CallP(first_inst_addr(pd->fn()));
    fint skip = pd->canScavenge() ? oopSize : 0;      // reg. mask
    if (pd->needsNLRCode()) skip += sendDesc::abortable_prim_end_offset - sendDesc::nonabortable_prim_end_offset;
    if (skip) {
      // skip register mask / NLR code upon return
      theAssembler->AddI(CalleeReturnAddr, skip, CalleeReturnAddr);
      theAssembler->Data(mask());
      if (pd->needsNLRCode()) nlrCode();
    } else {
      theAssembler->Nop();
    }
  }
  
  void InterruptCheckNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->SubCCR(SP, SPLimitReg, G0);     // test for stack overflow
    Label* l_ = theAssembler->BgeForward(false);        // no overflow
    theAssembler->Nop();
    PrimNode::gen();
    l_->define();
  }

  void RestartNode::gen() {
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->SubCCR(SP, SPLimitReg, G0);     // test for stack overflow
    // with the advent of branch bcs, the loop start's label may be zero,
    // so replace:
    //    theAssembler->Bge(loopStart->l, false);     // no overflow
    // with:
    loopStart->l = loopStart->l->unify( theAssembler->BgeForward(false) );
    theAssembler->Nop();
    PrimNode::gen();
    theAssembler->Bra(loopStart->l, true);
  }

  void BlockCloneNode::genCall() {
    Location dest = block()->loc;
    genHelper->loadImmediateOop(block()->block, CReceiverReg);   // load block Oop
    theAssembler->CallP(first_inst_addr(blockClone->fn()));
    theAssembler->OrR(SP, G0, Arg1);                    // load home frame
    assert(!blockClone->canScavenge() && !blockClone->needsNLRCode(),
           "need to rewrite this");
    genHelper->moveRegToLoc(ResultReg, dest);
    if (block()->uplevelR && isRegister(dest)) {
      // flush to stack
      theAssembler->StoreI(SP, spOffset(dest), dest);
    }
  }
  
  void BlockCreateNode::gen() {
    BasicNode::gen();
    if (block()->primFailBlockScope) {
      // must generate block (in primitive fail branch)
      assert(!isMemoized(), "shouldn't be memoized");
      genCall();
    } else if (isMemoized()) {
      // test if already created
      theAssembler->Comment("test memoized block");
      Location t = genHelper->moveToReg(block(), Temp1);
      Location t2 = genHelper->loadImmediateOop(deadBlockPR, Temp2, false);
      theAssembler->SubCCR(t, t2, G0);
      Label* done = theAssembler->BneForward(false);
      genCall();
      done->define();
    } else {
      // block has already been created (by initial BlockClone node)
    }
  }
  
  void NonLocalReturnNode::gen() {
    BasicNode::gen();
    theAssembler->JmpLI(ReturnAddr, sendDesc::non_local_return_offset, G0);
    theAssembler->RestoreR(G0, G0, G0);
  }
  
  void MethodReturnNode::gen() {
    BasicNode::gen();
    Location res = genHelper->moveToReg(_src, Temp1);
    if (haveStackFrame) {
      theAssembler->JmpLI(ReturnAddr, offset, G0);
      theAssembler->RestoreR(res, G0, ResultReg);
    } else {
      theAssembler->JmpLI(CalleeReturnAddr, offset, G0);
      theAssembler->OrR(res, G0, ResultReg);
    }
  }
  
  Location arith_genHelper(PReg* sreg, PReg* oper, PReg* dest,
                           ArithOpCode op,
                           Location& t1, Location& t2, bool& reversed) {
    bool haveImmediate = false;
    reversed = false;
    oop immediate;
    if (sreg->isConstPReg()) {
      oop val = ((ConstPReg*)sreg)->constant;
      if (val->is_smi() && isImmediate(smiOop(val))) {
        // try to reverse the sense of the operation
        switch (op) {
         case AddArithOp:       
         case AddCCArithOp:             
         case TAddCCArithOp:    
         case AndArithOp:               
         case AndCCArithOp:             
         case OrArithOp:                
         case OrCCArithOp:              
         case XOrArithOp:
          // commutative operator, no problem
          sreg = oper; immediate = val; haveImmediate = reversed = true;
          break;
          
          // would need to reverse non-const operand/operation for subtract,
          // so no savings
         case SubArithOp:
         case SubCCArithOp:             
         case TSubCCArithOp:
          break;

         case ArithmeticLeftShiftArithOp:
         case LogicalLeftShiftArithOp:
         case ArithmeticRightShiftArithOp:
         case LogicalRightShiftArithOp:
          // don't bother
          break;
         default:       ShouldNotReachHere(); // unexpected arith type
        }
      }
    } else if (oper->isConstPReg()) {
      oop val = ((ConstPReg*)oper)->constant;
      if (val->is_smi() && isImmediate(smiOop(val))) {
        immediate = val; haveImmediate = true; 
      }
    }   
        
    Location src  = genHelper->moveToReg(sreg, Temp1);
    Location dst = isRegister(dest->loc) ? dest->loc : Temp2;
    if (haveImmediate) {
      t1 = src; t2 = G0;
      int opn = (int)immediate; // for better switch formatting
      switch (op) {
       case AddArithOp:    theAssembler->AddI(src, opn, dst);   break;
       case SubArithOp:    theAssembler->SubI(src, opn, dst);   break;
       case AndArithOp:    theAssembler->AndI(src, opn, dst);   break;
       case OrArithOp:     theAssembler->OrI (src, opn, dst); break;
       case XOrArithOp:    theAssembler->XorI(src, opn, dst); break;
       case ArithmeticLeftShiftArithOp:
       case LogicalLeftShiftArithOp:
                           theAssembler->SllI(src, opn, dst);   break;
       case ArithmeticRightShiftArithOp:
                           theAssembler->SraI(src, opn, dst);   break;
       case LogicalRightShiftArithOp:
                           theAssembler->SrlI(src, opn, dst);           break;
       case AddCCArithOp:  theAssembler->AddCCI(src, opn, dst); break;
       // Do not have to worry about immed being oop (for _Eq:) because SPARC does not
       // do immediate oops. -- dmu 4/07
       case SubCCArithOp:  theAssembler->SubCCI(src, opn, dst); break;
       case AndCCArithOp:  theAssembler->AndCCI(src, opn, dst); break;
       case OrCCArithOp:   theAssembler->OrCCI (src, opn, dst); break;
       case TAddCCArithOp: theAssembler->TAddCCI(src, opn, dst);        break;
       case TSubCCArithOp: theAssembler->TSubCCI(src, opn, dst);        break;
       default:         ShouldNotReachHere(); // unexpected arith type
      }
    } else {
      Location src2 = genHelper->moveToReg(oper, Temp2);
      t1 = src; t2 = src2;
      switch (op) {
       case AddArithOp:         theAssembler->AddR(src, src2, dst);     break;
       case SubArithOp:         theAssembler->SubR(src, src2, dst);     break;
       case AndArithOp:         theAssembler->AndR(src, src2, dst);     break;
       case OrArithOp:          theAssembler->OrR (src, src2, dst);     break;
       case XOrArithOp:         theAssembler->XorR(src, src2, dst);     break;
       case ArithmeticLeftShiftArithOp:
       case LogicalLeftShiftArithOp:
                                theAssembler->SllR(src, src2, dst);     break;
       case ArithmeticRightShiftArithOp:
                                theAssembler->SraR(src, src2, dst);     break;
       case LogicalRightShiftArithOp:
                                theAssembler->SrlR(src, src2, dst);     break;

       case AddCCArithOp:       theAssembler->AddCCR(src, src2, dst);   break;
       case SubCCArithOp:       theAssembler->SubCCR(src, src2, dst);   break;
       case AndCCArithOp:       theAssembler->AndCCR(src, src2, dst);   break;
       case OrCCArithOp:        theAssembler->OrCCR (src, src2, dst);   break;
       case TAddCCArithOp:      theAssembler->TAddCCR(src, src2, dst);  break;
       case TSubCCArithOp:      theAssembler->TSubCCR(src, src2, dst);  break;

       default:                 ShouldNotReachHere(); // unexpected arith type
      }
    }
    return dst;
  }


  void TArithRRNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); 
    U_CHECK(oper); 
  }

  bool TArithRRNode::canCopyPropagateFrom(PReg* ) {
    return true;
  }


  bool TArithRRNode::isOpInlinable( ArithOpCode op ) {
    return   op == TAddCCArithOp
        ||   op == TSubCCArithOp;
  }


  void TArithRRNode::gen() {
    BasicNode::gen();
    if (constResult) {
      Location dest = isRegister(_dest->loc) ? _dest->loc : Temp2;
      Location l_ = genHelper->moveToReg(constResult, dest);
      if (l_ != _dest->loc) genHelper->moveRegToLoc(dest, _dest->loc);
    } else {
      Location t1, t2;
      bool reversed;
      if (SICCountIntTagTests)
        theAssembler->markTagTest(arg1IsInt ? 1 : 2);
      Location dest = arith_genHelper(_src, oper, _dest, op,
                                      t1, t2, reversed);

      Node* n= next1();
      if (n) {
        Label* l_= theAssembler->BvsForward(false);
        n->l= l_->unify(n->l);
      }
      
      // fill delay slot with tag test
      // caution: code below depends on temp reg assignments in arith_genHelper
      // also, the code in sicPrimline.c depends on Temp1 being set correctly
      if (arg1IsInt) {
        // only need to check arg2
        Location t = reversed ? t1 : t2;
        if (t == G0) {
          theAssembler->Nop();
        } else {
          theAssembler->AndCCI(t, Tag_Mask, Temp1);
        }
      } else {
        theAssembler->OrR(t1, t2, Temp1);
      }
      
      if (dest != _dest->loc) {
        // store result on stack (success case)
        theAssembler->StoreI(SP, spOffset(_dest->loc), dest);
      }
    }
  }
  
  void ArithRCNode::gen() {
    BasicNode::gen();
    Location src  = genHelper->moveToReg(_src, Temp1);
    Location dest = isRegister(_dest->loc) ? _dest->loc : Temp2;
    switch (op) {
     case AddArithOp:   theAssembler->AddI(src, oper, dest);    break;
     case SubArithOp:   theAssembler->SubI(src, oper, dest);    break;
     case AndArithOp:   theAssembler->AndI(src, oper, dest);    break;
     case OrArithOp:    theAssembler->OrI (src, oper, dest);    break;
     case XOrArithOp:   theAssembler->XorI(src, oper, dest);    break;
     case ArithmeticLeftShiftArithOp:
     case LogicalLeftShiftArithOp:
                        theAssembler->SllI(src, oper, dest);    break;
     case ArithmeticRightShiftArithOp:
                        theAssembler->SraI(src, oper, dest);    break;
     case LogicalRightShiftArithOp:
                        theAssembler->SrlI(src, oper, dest);    break;

     case AddCCArithOp: theAssembler->AddCCI(src, oper, dest);  break;
     case SubCCArithOp: theAssembler->SubCCI(src, oper, dest);  break;
     case AndCCArithOp: theAssembler->AndCCI(src, oper, dest);  break;
     case OrCCArithOp:  theAssembler->OrCCI (src, oper, dest);  break;

     default:           ShouldNotReachHere(); // unexpected arith type
    }
    if (dest != _dest->loc) {
      theAssembler->StoreI(SP, spOffset(_dest->loc), dest);
    }
  }
  
  void BranchNode::gen() {
    BasicNode::gen();
    Label* l_;
    switch (op) {
     case ALBranchOp:   l_ = theAssembler->BraForward(false);   break;
     case EQBranchOp:   l_ = theAssembler->BeqForward(false);   break;
     case NEBranchOp:   l_ = theAssembler->BneForward(false);   break;
     case LTBranchOp:   l_ = theAssembler->BltForward(false);   break;
     case LEBranchOp:   l_ = theAssembler->BleForward(false);   break;
     case LTUBranchOp:  l_ = theAssembler->BltuForward(false);  break;
     case LEUBranchOp:  l_ = theAssembler->BleuForward(false);  break;
     case GTBranchOp:   l_ = theAssembler->BgtForward(false);   break;
     case GEBranchOp:   l_ = theAssembler->BgeForward(false);   break;
     case GTUBranchOp:  l_ = theAssembler->BgtuForward(false);  break;
     case GEUBranchOp:  l_ = theAssembler->BgeuForward(false);  break;
     case VSBranchOp:   l_ = theAssembler->BvsForward(false);   break;
     case VCBranchOp:   l_ = theAssembler->BvcForward(false);   break;
     default:           ShouldNotReachHere(); // unexpected branch type
    }
    theAssembler->Nop();
    Node* n = next1();
    n->l = l_->unify(n->l);
  }
  
  
  void TBranchNode::genCompare(bool haveImmediate,
                                Location rcvrReg, Location argReg) {
    if (haveImmediate) {
      oop val = ((ConstPReg*)arg)->constant;
      theAssembler->TSubCCI(rcvrReg, (int)val, G0);
    }
    else {
      theAssembler->TSubCCR(rcvrReg, argReg, G0);
    }
  }


  void TBranchNode::testTagsIfNecessary(bool haveImmediate, Location rcvrReg, Location argReg) {
    // test both tags (overflow might be false alarm)
    if (SICCountTypeTests) {
      // not sure if type test counting stuff is right anymore -- dmu
      theAssembler->startTypeTest(2, false, true);
      theAssembler->doOneTypeTest();
    }
    if (SICCountIntTagTests) theAssembler->markTagTest(2);
    Label* noPrimFailure = theAssembler->BvcForward(false);
    theAssembler->Nop();
    Label*& ovflLabel = ((MergeNode*)nexti(2))->l;
    if (!intRcvr) {
      theAssembler->TAddCCI(rcvrReg, 0, G0);
      ovflLabel = ovflLabel->unify(theAssembler->BvsForward(false));
    }
    if (!intArg) { 
      theAssembler->TAddCCI(argReg, 0, G0);
      ovflLabel = ovflLabel->unify(theAssembler->BvsForward(false));
    }
    if (SICCountTypeTests) theAssembler->endTypeTest();
    genCompare(haveImmediate, rcvrReg, argReg);
    noPrimFailure->define(); // define no non-int destination    
  }
  

  // helper functions for n-way type test; similar to PIC code (cacheStub.c)

  // Next three are for PPC, someday could refactor SPARC to use them -- dmu 10/03
  
  void TypeTestNode::br_if_smi(Assembler* , Location , fint ) {
    fatal("not used on SPARC");
  }

  void TypeTestNode::br_if_float(Assembler* , Location , fint ) {
    fatal("not used on SPARC");
  }
  
  void TypeTestNode::br_to_unknown_case(Assembler* a) {
    fatal("not used on SPARC");
  }


  fint TypeTestNode::prologue(Assembler* a, Location rcvr, fint smiIndex,
                              fint floatIndex, bool immediateOnly,
                              Label*& firstMemOopTest) {
    // handle immediate cases of type test and load map if necessary
    // smiIndex/Float are the indices of the smi/float case in the map list
    // (+ 1, so that "not present" == 0)
    // returns the index (+ 1) of the fall-through case 
    assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
    assert(!immediateOnly || !needMapLoad,
           "immediateOnly implies !needMapLoad");
    fint fallThrough = 0;
    
    if (needMapLoad) {
      if (!FastMapTest || smiIndex || floatIndex) {
        a->AndCCI(rcvr, Mem_Tag, G0);
        firstMemOopTest = a->BneForward(true);
      }
      a->LoadI(rcvr, map_offset(), RcvrMapReg);
    }
    if (smiIndex) {
      a->AndCCI(rcvr, Tag_Mask, G0);
      if (SICCountTypeTests) theAssembler->doOneTypeTest();
      if (SICCountIntTagTests && immediateOnly) {
        // check if this tag test is on behalf of integer arithmetic
        methodMap* mm = (methodMap*)scope()->method()->map();
        stringOop sel = mm->get_selector_at(bci());
        bool isArith = (sel == VMString[PLUS] || sel == VMString[MINUS]);
        theAssembler->markTagTest(1, isArith);
      }
      define(smiIndex, a->BeqForward(SICCountTypeTests));
      if (SICCountTypeTests) a->endTypeTest();
      if (floatIndex) {
        a->AndCCI(rcvr, Float_Tag, G0); // test for float (delay slot)
        if (needMapLoad && !FastMapTest) {
          // already tested for memTag, so it must be a float
          fallThrough = floatIndex;
        } else {
          // need to test float tag
          // andcc rcvr, Float_Tag  (done in delay slot of prev. test)
          if (SICCountTypeTests) theAssembler->doOneTypeTest();
          define(floatIndex, a->BneForward(true));
          if (SICCountTypeTests) {
            a->endTypeTest();
          } else {
            a->Nop();       // must be here because next instr could be unimp
          }
        }
      } else {
        if (SICCountTypeTests) {
          // already filled delay slot
        } else {
          a->Nop();         // must be here because next instr could be unimp
        }
      }
    } else if (floatIndex) {
      a->AndCCI(rcvr, Float_Tag, G0); 
      if (SICCountTypeTests) theAssembler->doOneTypeTest();
      define(floatIndex, a->BneForward(true));
      if (SICCountTypeTests) {
        a->endTypeTest();   // only executed in success case
      } else {
        a->Nop();           // must be here because next instr could be unimp
      }
      // if we get here, it must be an int (if needMapLoad) or a memOop (if
      // !needMapLoad), but we didn't expect this so jump to the unknown case
      define(0, a->BraForward(!SICCountTypeTests));
      if (SICCountTypeTests) a->endTypeTest();
    } else if (needMapLoad) {
      // neither int nor float case
      assert(maps->length(), "should have at least one memOop case");
      if (FastMapTest) {
        // no test for memOop-ness, i.e. just fall through to first memOop case
      } else {
        // it's not a memOop and we have no immediate case, so jump to unknown
        assert(fallThrough == 0, "shouldn't have a fall-through");
        define(0, a->BraForward(true));
      }
    } else {
      // neither int not float nor mapLoad - empty prologue
    }
    return fallThrough;
  }
  
  void TypeTestNode::testMap(ConstPReg* pr, fint index) {
    assert(pr->constant->is_map(), "should be map");
    assert(needMapLoad, "need to load receiver map");
    Location maploc = genHelper->loadImmediateOop(pr, MapReg, false);    // load map
    theAssembler->SubCCR(maploc, RcvrMapReg, G0);               // test
    if (SICCountTypeTests) theAssembler->doOneTypeTest();
    define(index, theAssembler->BeqForward(SICCountTypeTests));
    if (SICCountTypeTests) theAssembler->endTypeTest();         // delay slot
  }
  
  void TypeTestNode::testOop(ConstPReg* pr, fint index) {
    assert(!pr->constant->is_map(), "should be oop");
    Location loc = genHelper->loadImmediateOop(pr, MapReg, false);       // load oop
    theAssembler->SubCCR(loc, r, G0);                           // test
    if (SICCountTypeTests) theAssembler->doOneTypeTest();
    define(index, theAssembler->BeqForward(SICCountTypeTests));
    if (SICCountTypeTests) theAssembler->endTypeTest();         // delay slot
  }
  
  void TypeTestNode::gen() {
    // generates n-way type test; fall-through code is "unknown" case
    BasicNode::gen();
    r = genHelper->moveToReg(_src, Temp3);
    fint smiIndex = 0;
    fint floatIndex = 0;
         if (maps->nth(0) == Memory->smi_map  ->enclosing_mapOop()) smiIndex   = 1;
    else if (maps->nth(0) == Memory->float_map->enclosing_mapOop()) floatIndex = 1;
    if (maps->length() > 1) {
           if (maps->nth(1) == Memory->smi_map  ->enclosing_mapOop()) smiIndex   = 2;
      else if (maps->nth(1) == Memory->float_map->enclosing_mapOop()) floatIndex = 2;
    }
    fint nconstants = 0;
    fint ntests = maps->length();
    fint firstMem = max(smiIndex, floatIndex);
    bool immediateOnly = firstMem == maps->length();
    fint i;
    for (i = firstMem; i < ntests; i++) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map()) nconstants++;
    }

    if (SICCountTypeTests) {
      theAssembler->startTypeTest(ntests, false, immediateOnly);
    }

    // first test against all constants
    if (!hasUnknown && nconstants == ntests) {
      // don't need to check for last constant
      ntests--;
    }
    for (i = firstMem; i < ntests; i++) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map()) testOop(pr, i + 1);
    }

    if (!hasUnknown && nconstants >= ntests) {
      // last case; should omit branch
      if (SICCountTypeTests) {
        theAssembler->endTypeTest();
      } else {
        theAssembler->Nop();    // fill delay slot of last test
      }
      define(ntests + 1, theAssembler->BraForward(true));
      return;           // done -- tested all constants
    }
    

    Label* firstMemOopTest = NULL;
    fint n = prologue(theAssembler, r, smiIndex, floatIndex, immediateOnly,
                      firstMemOopTest);

    if (n) define(n, theAssembler->BraForward(true));
    if (firstMemOopTest) {
      if (immediateOnly) {
        define(0, firstMemOopTest);     // no memOop tests
      } else {
        firstMemOopTest->define();
      }
    }
    if (!hasUnknown) ntests--;      // all maps known, can omit last test
    // test against all maps
    for (i = firstMem; i < ntests; i++) {
      ConstPReg* pr = mapPRs->nth(i);
      if (pr->constant->is_map()) testMap(pr, i + 1);
    }
    if (!immediateOnly) theAssembler->Nop();// fill delay slot of last map test
    if (SICCountTypeTests) theAssembler->endTypeTest();
    if (!hasUnknown) {
      // last case; should omit branch
      define(ntests + 1, theAssembler->BraForward(true));
    }
  }

  void IndexedBranchNode::gen() {
    // generates n-way indexed branch;
    // fall-through code is non-int or out of bounds
    BasicNode::gen();
    r = genHelper->moveToReg(_src, IndexReg);
    
    // check bounds
    genHelper->loadImmediateOop(as_smiOop(nCases), BoundsReg, true);
    theAssembler->TSubCCR( BoundsReg, r, G0);
    Label* end = theAssembler->BleuForward(false);
    theAssembler->Nop();
    
    // check tag (reuse bounds cmp)
    if (!srcMustBeSmi) {
      end->unify( theAssembler->BvsForward(false));
      theAssembler->Nop();
    }
    char* cra= theAssembler->addr();
    const int32 bytesFromCallToJump = 4 * sizeof(int32);
    const int32 bytesFromCallToInstAfterDelay  = 2 * sizeof(int32);
    // only on V9: theAssembler.ReadPC(Temp2); 
    theAssembler->CallN( theAssembler->addr() + bytesFromCallToInstAfterDelay);
    const int32 indexShift = 2 - Tag_Size;
    assert(indexShift == 0, "no shift needed");
    theAssembler->AddR( r, CalleeReturnAddr, BoundsReg); // delay slot
    assert( theAssembler->addr() - cra  == bytesFromCallToInstAfterDelay,
            "recount instructions");
    theAssembler->JmpLI( BoundsReg, bytesFromCallToJump - (Int_Tag << indexShift), G0);
    theAssembler->Nop(); // delay slot for jmp, cannot have branch here
    assert( theAssembler->addr() - cra  == bytesFromCallToJump,  
             "recount instructions");
    for (fint i = 0;  i < nCases;  ++i) {
      Label* l_ = theAssembler->BraForward(true);
      Node* n = nexti(i + 1);
      n->l = l_->unify(n->l);      
    }
    end->define();
  }
      
    
  void BlockZapNode::gen() {
    BasicNode::gen();
    Location t = genHelper->moveToReg(block(), Temp1);
    theAssembler->StoreI(t, scope_offset(), G0); 
  }
  

  void AbstractArrayAtNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); U_CHECK(arg);
    if (error) D_CHECK(error);
    use_count[Temp4]++; def_count[Temp4]++;       // (potentially) uses Temp4
  }
  
  bool AbstractArrayAtNode::canCopyPropagateFrom(PReg* d) {
    return d->loc != Temp4; // prevent local cp of Temp4
  }

  void AbstractArrayAtNode::gen() {
    BasicNode::gen();
    Label* argFail = NULL;          // if arg isn't a smi
    Label* indexFail = NULL;        // if arg is out of bounds
    Assembler* a = theAssembler;
    Location arr = genHelper->moveToReg(_src, Temp2);
    Location index = genHelper->moveToReg(arg, Temp1);
    Location size = Temp3;
    // load array size now (avoids load interlock for range check)
    a->LoadI(arr, sizeOffset, size);  
    if (!intArg) {
      // CP may have propagated a constant into arg
      intArg = arg->isConstPReg() && ((ConstPReg*)arg)->constant->is_smi();
    }
    if (!intArg) {
      // test arg for smiOop
      if (SICCountTypeTests) {
        a->startTypeTest(1, false, true);
        a->doOneTypeTest();
      }
      if (SICCountIntTagTests) a->markTagTest(1);
      a->AndCCI(index, Tag_Mask, G0);
      argFail = argFail->unify(a->BneForward(false));
      if (SICCountTypeTests) a->endTypeTest();
    }
    argFail = argFail->unify(testArg2());
    a->SubCCR(index, size, G0);
    indexFail = a->BgeuForward(false);
    Location res = isRegister(_dest->loc) ? _dest->loc : Temp1;
    bool needDestStore = genAccess(arr, index, res);
    if (needDestStore && !isRegister(_dest->loc)) {
      genHelper->moveRegToLoc(res, _dest->loc);
    }
    Label* done = a->BraForward(true);
    MergeNode* failMerge = (MergeNode*)next1();
    if (argFail) {
      argFail->define();
      if (error) {
        Location err = isRegister(error->loc) ? error->loc : Temp1;
        genHelper->loadImmediateOop(VMString[BADTYPEERROR], err);
        if (err != error->loc) genHelper->moveRegToLoc(err, error->loc);
      }
      if (failMerge) // test added by dmu 4/27/96
        failMerge->l = failMerge->l->unify(a->BraForward(true));
    }
    indexFail->define();
    if (error) {
      Location err = isRegister(error->loc) ? error->loc : Temp1;
      genHelper->loadImmediateOop(VMString[BADINDEXERROR], err);
      if (err != error->loc) genHelper->moveRegToLoc(err, error->loc);
    }
    if (failMerge) // test added by dmu 4/27/96
      failMerge->l = failMerge->l->unify(a->BraForward(true));
    done->define();
  }

  // gen array access; Temp3/4 are available, Temp1 may hold index, Temp2
  // may hold array
  bool ArrayAtNode::genAccess(Location arr, Location index, Location dest) {
    theAssembler->AddI(index, dataOffset, Temp1);
    theAssembler->LoadR(arr, Temp1, dest);
    return true;
  }

  bool ByteArrayAtNode::genAccess(Location arr, Location index, Location dest){
    theAssembler->LoadI(arr, dataOffset, Temp3);
    theAssembler->SrlI(index, Tag_Size, Temp1); // convert index to int
    theAssembler->LoadUBR(Temp3, Temp1, dest);  // load byte
    theAssembler->SllI(dest, Tag_Size, dest);   // convert to smi
    return true;
  }

  bool ArrayAtPutNode::genAccess(Location arr, Location index, Location dest) {
    Unused(dest);
    Location el;
    bool needCheckStore;
    if (elem->isConstPReg()) {
      ConstPReg* value = (ConstPReg*)elem;
      el = genHelper->loadImmediateOop(value, Temp3, false);
      needCheckStore = value->constant->is_new();
    } else {
      el = genHelper->moveToReg(elem, Temp3);
      needCheckStore = true;
    }
    theAssembler->AddI(index, dataOffset, Temp1);
    theAssembler->StoreR(arr, Temp1, el);
    if (needCheckStore) {
      theAssembler->AddR(arr, Temp1, Temp1);
      theAssembler->SrlI(Temp1, card_shift, Temp1);     // shift target addr 
      theAssembler->StoreBR(Temp1, ByteMapBaseReg, G0); // set byte in map
    }
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src) genHelper->moveRegToLoc(arr, _dest->loc);
    return false;       
  }

  // check value; Temp3 holds array size, Temp1 may hold index, Temp2
  // may hold array; Temp4 is available
  Label* ByteArrayAtPutNode::testArg2() {
    // check if arg is 0..255
    if (elem->isConstPReg()) {
      if (((ConstPReg*)elem)->constant->is_smi()) {
        // no run-time check required
        return NULL;
      } else {
        // primitive will always fail
        return theAssembler->BraForward(false);
      }
    } else {
      Location e = genHelper->moveToReg(elem, Temp4);
      Label* fail = NULL;
      if (!intElem) {
        // check for int
        if (SICCountTypeTests) {
          theAssembler->startTypeTest(1, false, true);
          theAssembler->doOneTypeTest();
        }
        if (SICCountIntTagTests) theAssembler->markTagTest(1);
        theAssembler->AndCCI(e, Tag_Mask, G0);
        fail = theAssembler->BneForward(false);
        if (SICCountTypeTests) theAssembler->endTypeTest();
      }
      theAssembler->SubCCI(e, 256 << Tag_Size, G0);
      return fail->unify(theAssembler->BgtuForward(false));
    }
  }
  
  bool ByteArrayAtPutNode::genAccess(Location arr,
                                     Location index, Location dest) {
    Unused(dest);
    theAssembler->LoadI(arr, dataOffset, Temp4);
    theAssembler->SrlI(index, Tag_Size, Temp1); // convert index to int
    if (elem->isConstPReg()) {
      // storing a constant - may be non-smi, but then this code will never
      // be executed anyway because the primitive fails
      ConstPReg* value = (ConstPReg*)elem;
      if (value->constant->is_smi()) {
        theAssembler->OrI(G0, smiOop(value->constant)->value(), Temp3);
      } else {
        theAssembler->Unimp(1111, false);               // cause trap
      }
    } else {
      Location el = genHelper->moveToReg(elem, Temp3);
      theAssembler->SrlI(el, Tag_Size, Temp3);  // convert to char
    }
    theAssembler->StoreBR(Temp4, Temp1, Temp3); // store in byte array
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src) genHelper->moveRegToLoc(arr, _dest->loc);
    return false;           // result already handled here 
  }

  void FlushNode::flushRegister(PReg* pr) {
    Location l = pr->loc;
    assert(l >= L0 && l <= I7, "not a local register");
    theAssembler->StoreI(SP, spOffset(l), l);
  }
  
  void DeadBlockNode::gen() {
    BasicNode::gen();
    theAssembler->TrapI(G0, SignalInterface::ST_UpLevel);
  }
  
  void DeadEndNode::gen() {
    // this node is unreachable - generate a trap for debugging
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      BasicNode::gen();
      theAssembler->TrapI(G0, SignalInterface::ST_ShouldNeverHappen);
    }
#   endif
  }
  
  void UncommonNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Unimp(0, restartSend);
  }
  

# endif  // sic
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerState_sparc.hh"
# include "_registerState_sparc.cpp.incl"

# ifdef FAST_COMPILER

# undef  MAX
# define MAX ((maxDepth >> LogBitsPerWord) + 1)

void RegisterState::initialize_for_platform(fint maxTemps) {
  // maxTemps > # byte codes; in the worst case we'll have 2*maxTemps
  // inline caches (if all bcs are prim calls) plus 1 in prologue.
  // To be safe, allocate twice that in anticipation of debugging hooks.
  maskList = new AddressList(4 * (maxTemps + 10));
}


void RegisterState::genMask() {
  RegisterString m = mask();
  maskList->append(theAssembler->addr());
  theAssembler->Data(m);
}



RegisterString RegisterState::mask() {
    assert(NumInRegisters + NumLocalRegisters == 16 && I0 > L0,
           "changed SPARC definition");
    RegisterString regs =
      (unsigned(allocated) >> L0) & nthMask(NumInRegisters + NumLocalRegisters);
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
        RegisterString test = 0;
        setNth(test, 0);
        assert(test == 1, "different bit ordering - change this code");
        assert(MAX > 0, "no word for stack locals");
    }
#   endif
    RegisterString stackLocals =
      stackAllocs[0] << (NumInRegisters + NumLocalRegisters);
    assert((regs & (-1 << (NumInRegisters + NumLocalRegisters))) == 0,
           "regs overlap stackLocals");
    return regs | stackLocals;
}


void RegisterState::fixupMasks(fint stackTempSize) {
  // The mask in the inline cache marks the regs + the first 16 stack locs.
  // If we have fewer than 16 temps and have extra args, the bits for the
  // extra args must be set.
  fint ntemps = BitsPerWord - (NumInRegisters + NumLocalRegisters);
  assert(stackDepth < ntemps && argDepth > 0, "shouldn't patch");
  int32 extraMask = 0;
  for (fint i = 1; i <= argDepth; i++) {
    fint bitNo = stackTempSize - i;
    if (bitNo < ntemps) {
      setNth(extraMask, NumInRegisters + NumLocalRegisters + bitNo);
    }
  }
  assert((extraMask & nthMask(NumInRegisters + NumLocalRegisters)) == 0,
         "invalid mask overlap");
  for (fint j = 0, len = maskList->length(); j < len; j++) {
    int32* mask = (int32*)maskList->nth(j);
    assert((*mask & extraMask) == 0, "none of these bits should be set");
    *mask |= extraMask;
  }
}


Location RegisterState::pickLocal() {
  // pick a local
  Location r = ::pickRegister(allocated, LocalMask);
  if (r == UnAllocated) {
    r = pickStackTemp();
    allocate(r);
  }
  return r;
}
  

void RegisterState::allocateArgs(fint nargs, bool isPrimCall) {
  Unused(isPrimCall);
  argDepth = max(argDepth, nargs - NumArgRegisters);
}


void RegisterState::allocate(Location r) {
  if (isRegister(r)) {
    ::allocateRegister(allocated, r);
  } 
  else if (r >= StackLocations) {
    assert(!isSet(stackAllocs[whichMask(r)], whichBit(r)),
           "already allocated");
    setNth(stackAllocs[whichMask(r)], whichBit(r));
    fint tempNo = r - StackLocations + 1;
    if (tempNo >= stackDepth) 
      stackDepth = tempNo;
    curDepth++;
    assert(curDepth <= stackDepth, "curDepth too big");
  } 
  else if (r <= ExtraArgLocations) {
    fatal("don't need to allocate extra args");
  } 
  else if (r <= ExtraIArgLocations) {
    // already allocated
  } 
  else {
    fatal("cannot allocate a pseudo-register");
  }
}


void RegisterState::deallocate(Location r) {
  if (isRegister(r)) {
    ::deallocateRegister(allocated, r);
    allocated = ::allocate(allocated, permanent); // ensure we do not deallocate a permanent reg
  } 
  else if (r >= StackLocations) {
    if (isSet(stackPerms[whichMask(r)], whichBit(r))) {
      // permanent -- don't deallocate
    } 
    else {
      assert(isSet(stackAllocs[whichMask(r)], whichBit(r)), "not allocated");
      clearNth(stackAllocs[whichMask(r)], whichBit(r));
      --curDepth;
      assert(curDepth >= 0, "negative depth");
    }
  } 
  else if (r <= ExtraArgLocations) {
    fatal("shouldn't deallocate extra args");
  } 
  else {
    // ignore extra incoming args - they are permanent
  }
}

# endif // FAST_COMPILER
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerString_sparc.hh"

# include "_registerString.cpp.incl"

# ifdef FAST_COMPILER

void printRegister(Location r) {
  if (r >= StackLocations) {
    lprintf("T%ld", long(r - StackLocations));
  } else if (r <= ExtraArgLocations) {
    lprintf("E%ld", long(ExtraArgLocations - r));
  } else if (r <= ExtraIArgLocations) {
    lprintf("I%ld", long(ExtraIArgLocations - r));
  } else {
    lprintf("%s", RegisterNames[r]);
  }
}


RegisterString registerMaskBit(Location l, fint stackLocs, fint nonRegisterArgs) {
  if (isRegister(l)) {
    if (l >= I0 && l < I0 + NumInRegisters) {
      // an in register
      return nthBit(l - I0 + NumLocalRegisters);
    } else {
      assert(l >= L0 && l < L0 + NumLocalRegisters, "unexpected register");
      return nthBit(l - L0);
    }
  } else if (isStackRegister(l)) {
    l = Location(l - StackLocations);
    if (l < BitsPerWord - (NumInRegisters + NumLocalRegisters)) {
      return nthBit(l + NumInRegisters + NumLocalRegisters);
    }
  } else if (isExtraArgRegister(l)) {
    fint which = ExtraArgLocations - l;
    assert(which <= nonRegisterArgs, "nonRegisterArgs too small");
    which = stackLocs + (nonRegisterArgs - which);  // translate into stack temp
    assert(which >= 0, "should be non-negative");
    if (which < BitsPerWord - (NumInRegisters + NumLocalRegisters)) {
      return nthBit(which + NumInRegisters + NumLocalRegisters);
    }
  }
  return 0;
}


# endif // FAST_COMPILER
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "fcompiler_sparc.hh"
# include "_fcompiler_sparc.cpp.incl"

# ifdef FAST_COMPILER


# endif // FAST_COMPILER
/* Sun-$Revision: 30.15 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "codeGen_sparc.hh"
# pragma implementation "codeGen_inline_sparc.hh"

# include "_codeGen_sparc.cpp.incl"

# ifdef FAST_COMPILER

  CodeGen* theCodeGen;
  
  void CodeGen::moveComplicated(Location dest, Location src, bool delay) {
    Unused(delay);
    assert(!(isRegister(src) && isRegister(dest)), "shouldn't call this");
    Location t;
    if (isRegister(dest)) {
      t = dest;
    } else if (isRegister(src)) {
      t = src;
    } else {
      assert(! delay, "cannot fit move in delay slot");
      t = Temp1;
    }
    if (isRegister(src)) {
      // already in register
    } else {
      Location base;
      fint offset = getOffset(src, base);
      a.LoadI(base, offset, t);
    }
    if (isRegister(dest)) {
      // already in register
    } else {
      Location base;
      fint offset = getOffset(dest, base);
      a.StoreI(base, offset, t);
    }
  }
  
  void CodeGen::genCountCode(int32* counter) {
    a.Comment("count # calls");
    a.SetHiA((void*)counter, Temp2);
    a.LoadA(Temp2, (void*)counter, Temp1);
    a.AddI(Temp1, 1, Temp1);
    a.StoreA(Temp2, (void*)counter, Temp1);
  }
    
  void CodeGen::testStackOverflow(RegisterState* s) {
    a.Comment("stack overflow/interrupt check");
    a.SubCCR(SP, SPLimitReg, G0);    // test for stack overflow
    Label* l = a.BgeForward(false);            // no overflow
    a.Nop();
    assert(s->mask() != 0, "should have non-zero mask");
    assert((s->allocated & nthBit(IReceiverReg)) != 0,
           "should have at least ReceiverReg allocated");
    (void)cPrimCall(intrCheck(), s, true, true, 0);
    l->define();
  }


  void CodeGen::testStackOverflowForLoop( Label*& dst,  Label*& nlr, RegisterState* s) {
    //    cmp sp, splimit
    //    bge dst
    //    nop
    //    <cPrimCall interruptCheck>
     
    // (this code lifted from what used to be restart)
    
    a.Comment("stack overflow/interrupt check for loop");
    a.SubCCR(SP, SPLimitReg, G0);            // test for interrupt
    dst = a.BgeForward( false);                // no overflow
    a.Nop();  // can't use annulled branch! back-to-back CTIs
    nlr = cPrimCall(intrCheck(), s, false, true, 0);
  }


  void CodeGen::smiOop_prologue(pc_t missHandler) {
    //   andcc rr, Tag_Mask, g0
    //   beq   _cache_hit
    //   sethi missHandler, t
    //   jmpl t, missHandler, g0
    //   nop
    // _cache_hit:

    if (SICCountIntTagTests) a.markTagTest(1);
    a.AndCCI(ReceiverReg, Tag_Mask, G0);    // test for integer tag
    Label* hit = a.BeqForward(false);       // branch if receiver is an integer
    jumpTo(missHandler, Temp1, G0);
    if (SICCountTypeTests) {
      a.endTypeTest();
    } else {
      a.Nop();
    }
    hit->define();
  }

  void CodeGen::floatOop_prologue(pc_t missHandler) {
    //   andcc rr, Float_Tag, g0
    //   bne  _cache_hit
    //   sethi missHandler, t
    //   jmpl t, missHandler, g0
    //   nop
    // _cache_hit:

    a.AndCCI(ReceiverReg, Float_Tag, G0);    // test for float tag
    Label* hit = a.BneForward(false);        // branch if receiver is a float
    jumpTo(missHandler, Temp1, G0);
    if (SICCountTypeTests) {
      a.endTypeTest();
    } else {
      a.Nop();
    }
    hit->define();
  }

  void CodeGen::memOop_prologue(pc_t missHandler) {
    //   andcc rr, Mem_Tag, g0
    //   bne,a _check_receiver_map
    //   load rr, map_offset - Mem_Tag, t3
    // _miss:
    //   sethi missHandler, t3
    //   jmpl t3, missHandler, g0
    // _check_receiver_map:
    //   <loadOop receiver_map, t4>
    //   subcc t3, t4, g0
    //   bne  _miss
    //   nop
    // _cache_hit:

    if (FastMapTest) {
      a.LoadI(ReceiverReg, map_offset(), Temp1);  // load map
      loadOop(Temp2, L->receiverMapOop());        // load customization map
      a.SubCCR(Temp1, Temp2, G0);                 // compare against actual map
      Label* ok = a.BeqForward(false);            // jump to body of nmethod
      jumpTo(missHandler, Temp2, G0);
      if (SICCountTypeTests) {
        a.endTypeTest();
      } else {
        a.Nop();
      }
      ok->define();
    } else {
      a.AndCCI(ReceiverReg, Mem_Tag, G0);         // test for mem tag
      Label* checkMap = a.BneForward(true);       // branch if rcvr is mem oop
      a.LoadI(ReceiverReg, map_offset(), Temp1);  // load map in delay slot
      DefinedLabel miss(a.printing);
      jumpTo(missHandler, Temp2, G0);
      checkMap->define();
      loadOop(Temp2, L->receiverMapOop());      // load customized map
      a.SubCCR(Temp1, Temp2, G0);               // compare against actual map
      a.Bne(&miss, false);                      // jump to miss if no match
      if (SICCountTypeTests) {
        a.endTypeTest();
      } else {
        a.Nop();
      }
    }
  }

  void CodeGen::checkOop(Label& general, oop what, Location reg) {
    // test for inline cache hit (selector, delegatee)
    assert(Temp1 !=   PerformSelectorLoc   && Temp1 !=    PerformDelegateeLoc
        && isRegister(PerformSelectorLoc)  &&  isRegister(PerformDelegateeLoc),
           "wrong register setup");
    loadOop(Temp1, what);                       // load hard-wired value
    a.SubCCR(reg, Temp1, G0);                   // compare against actual value
    if (general.isDefined()) {
      a.Bne(&general, false);                   // reuse miss handler
      a.Nop();
    } else {
      Label* hit = a.BeqForward(false);
      general.define();
      jumpTo(Memory->code->trapdoors->SendMessage_stub_td(), Temp1, G0);
      a.Nop();
      hit->define();
    }
  }

  void CodeGen::checkRecompilation() {
    // test for recompilation
    //   sethi &counter, t3
    //   load  [t3 + lo%(&counter)], t4
    //   add t4, 1, t4
    //   cmp t4, recompileLimit
    //   bne ok
    //   store t4, [t3 + lo%(&counter)]
    //   <jumpTo recompiler>
    //   nop
    // ok:

    // di recompilation doesn't work right now - see recompile.c
    if (diLink) return;

    a.Comment("test for recompilation");
    int32 countID = theCompiler->countID;
    void* counter = &useCount[countID];
    a.SetHiA(counter, Temp3);
    a.LoadA(Temp3, counter, Temp2);
    a.AddI(Temp2, 1, Temp2);
    fint limit = recompileLimit(theCompiler->level());
    if (limit < maxImmediate) {
      a.SubCCI(Temp2, limit, G0);
    } else {
      a.SetHiI2(limit, Temp1);      // limit is multiple of 1024
      a.SubCCR(Temp2, Temp1, G0);
    }
    Label* ok = a.BneForward(false);
    // call recompiler
    void* fnaddr = diLink 
                 ? Memory->code->trapdoors->DIRecompile_stub_td() 
                 : Memory->code->trapdoors->  Recompile_stub_td();
    Location linkReg = diLink ? DIRecompileLinkReg : RecompileLinkReg;
    jumpTo(fnaddr, linkReg, linkReg);
    // The store below is always executed so that we will call the recompiler 
    // exactly once (even if it cannot recompile for some reason).
    ok->define();
    assert(Temp3 != linkReg, "counter addr reg will be trashed by jump");
    a.StoreA(Temp3, counter, Temp2);
  }
    
  void CodeGen::prologue(bool isAccessMethod, fint nargs ) {
    // *if not DI child
    //    <smi/float/memOop prologue>
    // _verified:                       (entry point from PICs)
    //    if necessary <check selector>
    //    if necessary <check delegatee>
    // *endif DI
    
    // _diCheck:                        (entry point after recompile)
    //    <verify assignable parents>
    
    // *if using recompilation
    //    <checkRecompilation>
    // *endif
    
    // *if haveStackFrame
    //    save sp, -frameSize*oopSize, sp
    // *endif
    
    // <flush register windows if neceessary>
    // <clear stack temporaries and excess argument locations

    // CAUTION: use only Temp1/4 for temps in prologue; other temps
    // may contain lookup parameters.
    assert(Temp1 != PerformSelectorLoc && Temp1 != PerformDelegateeLoc,
           "will trash lookup parameters");
    assert(Temp2 != PerformSelectorLoc && Temp2 != PerformDelegateeLoc,
           "will trash lookup parameters");

    fint assignableParents = L->adeps->length();
    MethodKind kind =
      isAccessMethod ? MethodKind(-1) : theCompiler->method()->kind();
    _incoming_arg_count = nargs; // for eventual putting into nmethod

    
    if (diLink == 0) {
      if (!L->isReceiverStatic()) {
        // test receiver map
#       if GENERATE_DEBUGGING_AIDS
          if (CheckAssertions)
            switch (L->lookupType()) {
             case NormalLookupType:  break;
             case StaticNormalLookupType:
             case ImplicitSelfLookupType:
             case ResendLookupType:
             case DirectedResendLookupType: fatal("shouldn't miss"); break;
             default: break;
            }
#       endif
          Map* m = L->receiverMap();
        bool imm = m == Memory->smi_map || m == Memory->float_map;
        if (SICCountTypeTests) {
          a.startTypeTest(1, true, imm);
          a.doOneTypeTest();
        }
        if (m == Memory->smi_map) {
          smiOop_prologue(Memory->code->trapdoors->SendMessage_stub_td());
        } else if (m == Memory->float_map) {
          floatOop_prologue(Memory->code->trapdoors->SendMessage_stub_td());
        } else {
          memOop_prologue(Memory->code->trapdoors->SendMessage_stub_td());
        }
      }
      
      verifiedOffset = a.offset();
      if (SICCountTypeTests) a.endTypeTest();
      Label generalMiss(a.printing, NULL);
      a.Comment("verified entry point:");
      
      if (L->isPerform()) {
        a.Comment("check selector");
        checkOop(generalMiss, L->selector(), PerformSelectorLoc);
      }
      
      if (needsDelegatee(L->lookupType()) && !L->isDelegateeStatic()) {
        a.Comment("check delegatee");
        checkOop(generalMiss, L->delegatee(), PerformDelegateeLoc);
      }
    } else {
      // don't check receiver map, selector, delegatee if a DI cache miss
      assert(assignableParents > 0, "should have some di parents to check");
    }
    
    diCheckOffset = a.offset();
    a.Comment("DI entry point:");
    
    if (assignableParents > 0) {
      a.Comment("verify state of assignable parents");
      fint count = 0;
      for (fint i = 0; i < assignableParents; i ++) {
        objectLookupTarget* target = L->adeps->start()[i];
        Location t = loadPath(Temp2, target, ReceiverReg, Temp1);
        count = verifyParents(target, t, count);
      }
    }

    bool recomp = needRecompileCode(theCompiler->level());
    if (recomp) checkRecompilation();

    if (haveStackFrame) {
      prologueAddr = a.addr();
      a.SaveI(SP, -1, SP);     // correct frame size is patched in later
      frameCreationOffset = a.offset();
    } else {
      prologueAddr = NULL;
    }

    if (GenerateCountCode) {
      int32* counter;
      if (assignableParents != 0) {
        counter = &NumberOfDIMethodCalls;
      } else if (isAccessMethod) {
        counter = &NumberOfAccessMethodCalls;
      } else if (kind == BlockMethodType) {
        counter = &NumberOfBlockMethodCalls;
      } else {
        counter = &NumberOfMethodCalls;
      }
      genCountCode(counter);
    }

    if (!isAccessMethod) {
      if (!recomp && GenerateLRUCode) {
        // this code is rarely generated in practice (recomp is usually true)
        a.Comment("reset unused bit");
        void* unused_addr = &LRUflag[Memory->code->nextNMethodID()];
        a.SetHiA(unused_addr, Temp2);
        a.StoreA(Temp2, unused_addr, G0);
      }
    
      // don't keep uplevel-accessed names in regs
      // (for now, just flush everything)
      if (nargs > NumIArgRegisters) nargs = NumIArgRegisters;
      a.Comment("flush incoming args to stack");
      for (fint i = 0; i < nargs; i++) {
        flushToStack(IArgLocation(i), NULL);
      }
      flushToStack(IReceiverReg, NULL);       // flush receiver to stack

      switch (kind) {
       case BlockMethodType:
        if (theCompiler->needRegWindowFlushes) flushRegisterWindows();
        break;
       case OuterMethodType:
        if (needToFlushRegWindow) {     // we inlined the receiver block
          if (theCompiler->needRegWindowFlushes) flushRegisterWindows();
        } else {
          // receiver is parent, do nothing
        }
        break;
       default:
        fatal1("unknown kind: %ld", kind);
        break;
      }
    }
      
    a.Comment("End Prologue");
  }

  Label* CodeGen::postPrologue(RegisterState* s, bool frequentPreemption) {
    // returns address of stack overflow test
    Label* l = new DefinedLabel(a.printing);
    if (frequentPreemption) {
      // no stack test necessary: have explicit check at bci 0
      // (actually I think it is more accurate to say have
      //  frequent checks -- dmu )
    } else {
      testStackOverflow(s);
    }
    return l;
  }

  void CodeGen::fixupFrame(RegisterState* s) {
    // window size adjustment
    assert(haveStackFrame, "should have stack frame");
    fint stackTempCount = s->stackDepth + s->argDepth;
    if (stackTempCount & 1) ++stackTempCount;
    frameSize = stackTempCountToFrameSize(stackTempCount);
    assert((frameSize & (frame_word_alignment-1))  ==  0, "frame size must be even");

    a.Comment("patching stack frame creation code");
    // The mask in the inline cache marks the regs + the first ntemps stack locs.
    // If we have fewer than ntemps temps and have extra args, the bits for the
    // extra args must be set.
    // All stack locations beyond the first ntemps have to be cleared.
    
    fint ntemps = BitsPerWord - (NumInRegisters + NumLocalRegisters);
    if (stackTempCount - s->argDepth  <  ntemps   &&  s->argDepth > 0) {
      s->fixupMasks(stackTempCount);
    }
    a.saveExcursion(prologueAddr);
    if (stackTempCount <= ntemps && !s->argDepth) {
      // no need to init anything on the stack - just patch frameSize
      a.SaveI(SP, frameSize * -oopSize, SP);    // make new register window
      a.endExcursion();
    } else {
      // need to initialize extra stack locations and/or extra args
      Label* l = a.BraForward(true);
      DefinedLabel cont(a.printing);
      a.endExcursion();
      l->define();
      a.SaveI(SP, frameSize * -oopSize, SP);    // make new register window
      // clear locations beyond first ntemps 
      fint i;
      for ( i = max(ntemps, s->initStackTemps); 
            i < stackTempCount; 
            i++) {
        a.StoreI(FP, (local_slots_offset - i) * oopSize, G0);
      }
      // clear extra args marked "live" by fixupMasks
      // (necessary because the simple fixup scheme marks them as live
      // for the entire method, not only after they're used)
      for ( i = 1; 
            i <= s->argDepth; 
            i++) {
        fint bitNo = stackTempCount - i;
        if (bitNo < ntemps) {
          a.StoreI(FP, (local_slots_offset - bitNo) * oopSize, G0);
        }
      }
      a.Bra(&cont, true);
    }
  }
  
  fint CodeGen::verifyParents(objectLookupTarget* target, Location t,
                              fint count) {
    assignableSlotLink* l = target->links;
    assert(l != 0, "expecting an assignable parent link");
      
    for (;;) {
      a.LoadI(t, smiOop(l->slot->data)->byte_count() - Mem_Tag, Temp1);
      // load assignable parent slot value
      Label* ok;
      Map* targetMap = l->target->obj->map();
      if (l->target->value_constrained) {
        // constraint for a particular oop (ambiguity resolution)
        loadOop(Temp2, l->target->obj);         // load assumed value
        a.SubCCR(Temp1, Temp2, G0);             // compare values
        ok = a.BeqForward(false);               // branch if value OK
        if (l->target->links) a.Nop();
      } else {
        // check if map of parent is correct
        if (targetMap == Memory->smi_map) {
          a.AndCCI(Temp1, Tag_Mask, G0);        // test for integer tag
          ok = a.BeqForward(false);             // branch if parent is integer
          if (l->target->links) a.Nop();
        } else if (targetMap == Memory->float_map) {
          a.AndCCI(Temp1, Float_Tag, G0);       // test for float tag
          ok = a.BneForward(false);             // branch if parent is a float
          if (l->target->links) a.Nop();
        } else {
          Label* miss = NULL;
          if (!FastMapTest) {
            a.AndCCI(Temp1, Mem_Tag, G0);       // test for mem tag
            Label* mem = a.BneForward(true);    // branch if parent is mem oop
            a.LoadI(Temp1, map_offset(), Temp2); // load receiver map
            miss = a.BraForward(true);          // branch to diLookup section
            mem->define();
          } else {
            a.LoadI(Temp1, map_offset(), Temp2); // load receiver map
          }
          loadOop(Temp3, targetMap->enclosing_mapOop()); // load map constraint
          a.SubCCR(Temp2, Temp3, G0);   // compare w/ parent's map
          ok = a.BeqForward(false);     // correct
          if (l->target->links) a.Nop();
          if (miss) miss->define();
        }
      }
      void* addr = Memory->code->trapdoors->SendDIMessage_stub_td();
      a.SetHiD(addr, Temp1);
      a.JmpLD(Temp1, addr, DILinkReg);
      loadImmediate(DICountReg, count);         // count of parents verified
      a.Data(0);                                // first part of DI nmln
      a.Data(0);                                // second part of DI nmln
      ok->define();
      
      count ++;
      if (l->target->links) count = verifyParents(l->target, Temp1, count);
      
      l = l->next;
      if (l == 0) break;
      // if multiple dynamic parents, reload slot holder before looping (HACK!)
      t = loadPath(Temp1, target, ReceiverReg, Temp1);
    }
    
    return count;
  }
    
  void CodeGen::epilogue(Location what) {
    // <moveToReg what, t>
    // jmpl rt, INLINE_cache_end_offset, g0
    // restore what/t, g0, rr
    
    if ( what == IllegalLocation )
      what = IReceiverReg;

    Location t = moveToReg(what, Temp1);
    
    fint offset = send_desc->endOffset(theCompiler->L->lookupType());
    if (haveStackFrame) {
      a.JmpLI(ReturnAddr, offset, G0);    // return
      a.RestoreR(t, G0, ResultReg);    // restore register window
    } else {
      a.JmpLI(CalleeReturnAddr, offset, G0);    // return
      move(ResultReg, t, true);
    }
  }

  Location CodeGen::flushToStack(Location reg, RegisterState*) {
    // flush register to its corresponding location on the stack
    if (isRegister(reg)) {
      Location breg;
      fint offset = getOffset(reg, breg);
      assert(breg == SP, "should be SP-relative");
      a.StoreI(SP, offset, reg);                // flush reg to stack
    }
    return reg; // just the same on SPARC, different when we have a machine where there is no fixed mapping
  }

  Label* CodeGen::cPrimCall(PrimDesc* p, RegisterState* s,
                            bool continueNLR, bool /*trust_fn_arg_count */, fint /* arg_and_rcvr_count */) {
    //   call <primitive>
    //   add o7, oopSize, o7            // so C skips the mask
    //   .data mask                     // need mask for scavenging
    // *if needsNLRCode
    //   <continueNonLocalReturn>       // for prims like AbortProcess
    // *endif
    
    a.CallP( first_inst_addr( p->fn() ) );
    a.AddI(CalleeReturnAddr, 
           p->needsNLRCode()
              ? sendDesc::abortable_prim_end_offset - sendDesc::nonabortable_prim_end_offset + oopSize
              : oopSize, 
           CalleeReturnAddr);
    // skip register mask upon return
    s->genMask();    // used register mask
    
    if ( !p->needsNLRCode()) {
      return NULL;
    }
    else if (continueNLR) {
      continueNonLocalReturn();
      return NULL;
    } 
    else {
      Label* l = a.BraForward(true);
      a.Nop();
      return l;
    }
  }

  Label* CodeGen::primFailure(Location failReceiver, Location self,
                              oop failSelector, oop selector, 
                              Location successLoc, blockOop failBlock,
                              RegisterState* s) {
                              
    s->allocateArgs(2, true); // will be passing out 2 args

    // *if CResultReg contains a markOop
    //    sub  ResultReg, Mark_Tag - Mem_Tag, arg2
    //    <loadOop selector, arg1>
    //    <clone fail block if necessary>
    //    <selfCall>
    // *endif

    a.AndI(CResultReg, Tag_Mask, Temp1);        // extract result's tag field
    a.SubCCI(Temp1, Mark_Tag, G0);              // test for mark tag 
    Label* success = a.BneForward(true);
    move(successLoc, CResultReg, true);         // move to right place
    // clone failure block if necessary
    if (failBlock == NULL) {
      // block already exists
      a.SubI(CResultReg, Mark_Tag - Mem_Tag, Arg1);// mask mark bit
    } else {
      // NOTE: this will break if blockOop::clone scavenges!
#     if GENERATE_DEBUGGING_AIDS
        if (CheckAssertions) {
          PrimDesc* pd = blockClone();
          assert(! pd->canScavenge(), "rewrite this");
        }
#     endif
      // need to save error string
      Location saved = s->pickLocal();
      if (isRegister(saved)) {
        a.SubI(CResultReg, Mark_Tag - Mem_Tag, saved);// save and mask mark bit
      } else {
        a.SubI(CResultReg, Mark_Tag - Mem_Tag, CResultReg);
        move(saved, CResultReg, false);
      }
      loadBlockOop(failReceiver, failBlock, s);
      move(Arg1, saved, false);                 // move it to right place
      s->deallocate(saved);
    }
    loadOop(Arg2, selector);
    Label* l = selfCall(s, NormalLookupType, failReceiver, self, 
                        failSelector, NULL, 2);
    move(successLoc, ResultReg);
    success->define();
    return l;
  }
    
  void CodeGen::recordStore(Location dst) {
    // srl dst, card_shift, dst
    // stb g0, [dst + byte_map_base]
      
    a.Comment("recordStore");
    assert(isRegister(dst), "receiver to check_store must be in a register");
    a.SrlI(dst, card_shift, dst);    // shift target addr 
    a.StoreBR(dst, ByteMapBaseReg, G0);    // set byte in map
  }
    
    
  Label* CodeGen::selfCall(RegisterState* s, LookupType lookupType,
                           Location receiver, Location self,
                           oop selector, oop delegatee, fint argc) {
    // call lookup
    // mov  receiver, ReceiverReg
    // <rest of inline cache>

    Unused(self);
    a.CallB(Memory->code->trapdoors->SendMessage_stub_td());
    move(ReceiverReg, receiver, true);
    Label* l = SendDesc(s, lookupType, selector,
                        delegatee ? oop(delegatee) : oop(badOop));
    return l;
  }
    
  Label* CodeGen::perform(RegisterState* s, LookupType lookupType,
                          Location receiver, Location self, fint argc,
                          oop delegatee) {
    a.Comment("_Perform Primitive");
    return selfCall(s, lookupType, receiver, self,
                    as_smiOop(argc), delegatee, argc);
  }


  void CodeGen::assignment(Location receiver, slotDesc* s, Location val) {
    // <move arg, t>
    // store [receiver/rr, offset - Mem_Tag], t
    // <check_store>
    assert( isRegister(receiver), "receiver must be a register");
    a.Comment("Begin Simple Assignment");
    Location t;
    if (isRegister(val) && val != CReceiverReg) {
      t = val;
    } else {
      t = Temp1;
      move(t, val);    // load argument
    }
    int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
    a.StoreI(receiver, offset, t); // store object data slot contents
    a.AddI(receiver, offset, Temp1); // compute store address
    recordStore(Temp1);

    a.Comment("End Simple Assignment");
  }

  void CodeGen::assignment(Location receiver,
                           realSlotRef* path, Location val,
                           bool isMem) {
    // <move arg, t>
    // <loadPath rr, path, receiver>
    // store receiver/rr, offset - Mem_Tag, arg/t
    // <check_store>
      
    a.Comment("Begin Assignment");
    Location t;
    if (isRegister(val) && val != CReceiverReg) {
      t = val;
    } else {
      t = Temp1;
      move(t, val);    // load argument
    }
    assert(t != Temp2, "register conflict");
    int32 offset = smiOop(path->desc->data)->byte_count() - Mem_Tag;
    if (path->holder->is_object_or_map()) {
      Location t1 = loadPath(CReceiverReg, path->holder, receiver, Temp1);
      a.StoreI(t1, offset, t);    // store object data slot contents
      if (isMem) {
        a.AddI(t1, offset, Temp1);    // compute store address
        recordStore(Temp1);
      }
    } else {
      fatal("don't support vframe lookups yet");
    }
    a.Comment("End Assignment");
  }

  void CodeGen::loadBlockParent(Location block, Location dst) {
    // given a location containing a block, get sp of frame creating the block
    // will load an invalid address for non-LIFO blocks

    // load [block+scope_offset], dst
    block = moveToReg(block, dst);
    a.LoadI(block, scope_offset(), dst);
  }
  
  fint CodeGen::getOffset(Location src, Location& baseReg) {
    // compute location (base register + offset) of saved reg src
    fint offset;
    if (isRegister(src)) {
      baseReg = SP;
      offset =
        (first_register_offset - StackFromRegister[src]) * oopSize;
    } else if (src >= StackLocations) {
      baseReg = FP;
      offset = (local_slots_offset - (src - StackLocations)) * oopSize;
    } else if (src <= ExtraArgLocations) {
      baseReg = SP;
      offset = (extra_arg_offset + (ExtraArgLocations - src)) * oopSize;
      assert(offset >= extra_arg_offset * oopSize, "should be above");
    } else if (src <= ExtraIArgLocations) {
      baseReg = FP;
      offset = (arg_bottom_offset - (src - ExtraIArgLocations)) * oopSize;
    } else {
      fatal("source cannot be a pseudo-register");
    }
    assert((offset & 3) == 0, "should be word-aligned");
    return offset;
  }

  void CodeGen::loadSaved(Location dest, Location src,
                          Location sp, compiled_vframe* dst_vf) {
    fint frameSz = dst_vf->code->frameSize();
    // Load location from a frame on the stack; sp is that frame's 
    // block home (SP on Sparc) and frameSz its size.
    Location baseReg;
    fint sp_offset = getOffset(src, baseReg);
    if (baseReg != SP) {
      // The source is addressed off FP, not SP; convert fp-relative
      // offset into sp-relative.
      assert(frameSz > WindowSize, "frame size too small");
      sp_offset += frameSz * oopSize;
    }
    if (isRegister(dest)) {
      a.LoadI(sp, sp_offset, dest);
    } else {
      a.LoadI(sp, sp_offset, Temp1);
      move(dest, Temp1);
    }
  }
    
  void CodeGen::storeSaved(Location src, Location sp,
                           compiled_vframe* src_vf, Location value) {
    fint frameSz = src_vf->code->frameSize();
    Location baseReg;
    assert(isRegister(sp), "must be a register");
    assert(src > ExtraArgLocations, "shouldn't assign to args");
    fint sp_offset = getOffset(src, baseReg);
    if (baseReg != SP) {
      // The source is addressed off FP, not SP; convert fp-relative
      // offset into sp-relative.
      assert(frameSz > WindowSize, "frame size too small");
      sp_offset += frameSz * oopSize;
    }
    assert(sp != Temp3, "messed up register assignment");
    Location t = moveToReg(value, Temp3);
    a.StoreI(sp, sp_offset, t);
  }

#ifdef UNUSED
  void CodeGen::loadSender(Location dest, Location sp) {
    // load frame's sender
    a.LoadI(sp, frame_offset * oopSize, dest);
  }
#endif

    
  void CodeGen::lookup(Location dest, realSlotRef* path, Location receiver) {
    // <loadPath dest, path, receiver>
    // load receiver/dest/t, offset - Mem_Tag, dest/t
    // <move t, dest>
      
    Location t;
    if (isRegister(dest)) {
      t = dest;
    } else {
      t = Temp1;
    }
    if (path->holder->is_object_or_map()) {
      Location t1 = loadPath(t, path->holder, receiver, Temp1);
      a.LoadI(t1, smiOop(path->desc->data)->byte_count() - Mem_Tag, t);
      // load data slot
    } else {
      fatal("don't support vframe lookups");
    }
    move(dest, t);
  }
    
        
  static int32 nrOfRestarts = 0;
  

  // load NLR registers
  void CodeGen::prepareNLR(Location result, Location scope, smi homeID) {
    // <move scope, NLRHomeReg>
    // <loadImmediate homeID, NLRHomeIDReg>
    // <move result, NLRResultReg>

    assert(result != NLRHomeIDReg && result != NLRHomeReg, "wrong reg alloc");
    result = moveToReg(result, NLRTempReg);
    move(NLRHomeReg, scope);
    loadImmediate(NLRHomeIDReg, homeID);
    move(NLRResultReg, result);
  }

  void CodeGen::testAndContinueNLR(smi homeID) {
    //   <loadImmediate homeID, NLRTempReg>   
    //   cmp NLRTempReg, NLRHomeIDReg           
    //   bne doNLR
    //   cmp NLRHomeReg, sp
    //   beq continue
    //   nop
    // doNLR:
    //   <continueNLR>
    // continue:
    //   <epilogue>

    Label* cont, *doNLR = NULL;
    if (homeID) {       // note: will be 0 if no inlining
      if (homeID < maxImmediate) {
        a.SubCCI(NLRHomeIDReg, homeID, G0);
      } else {
        loadImmediate(NLRTempReg, homeID);
        a.SubCCR(NLRHomeIDReg, NLRTempReg, G0);
      }
      doNLR = a.BneForward(false);
    }
    a.SubCCR(FrameReg, NLRHomeReg, G0);
    cont = a.BeqForward(false);
    a.Nop();
    if (doNLR) doNLR->define();
    continueNonLocalReturn();
    cont->define();
    epilogue(NLRResultReg);
  }
  
  // continue NLR (return through caller's inline cache)
  void CodeGen::continueNonLocalReturn() {
    // jmpl rt, non_local_return_offset, g0
    // restore dest/t, g0, rr

    assert(haveStackFrame, "should have stack frame");
    a.JmpLI(ReturnAddr, sendDesc::non_local_return_offset, G0);    // return
    a.RestoreR(G0, G0, G0);             // restore register window
  }

  void CodeGen::zapBlock(Location block, bool memoized) {
    // <move block, t>
    // *if memoized
    //    subcc block/t, 0, g0
    //    bne,a _done           // do work in delay slot
    // *end
    // store block/t, scopeOffset, G0
    // _done: ...
    
    Location t;
    a.Comment("zap block");
    if (isRegister(block)) {
      t = block;
    } else {
      t = NLRTempReg;
      move(t, block);
    }
    Label* done;
    if (memoized) {
      a.SubCCI(t, 0, G0);               // was block generated?
      done = a.BneForward(true);        // branch and zap if yes
    }
    a.StoreI(t, scope_offset(), G0);    // zap the block
    if (memoized) done->define();
  }
    
  Label* CodeGen::SendDesc(RegisterState* s, LookupType lookupType,
                           oop selector, oop delegatee) {
    s->genMask();                               // mask of used regs
    Label* l = a.BraForward(true);              // non-local return code
    a.Nop();
    a.Zero();   // nmlns
    a.Zero();
    if (selector != badOop) {
      if (isPerformLookupType(lookupType)) {
        assert_smi(selector, "should be an integer argcount");
        a.Data(smiOop(selector)->value());      // really arg count
      } else {
        assert_string(selector, "should be a string constant");
        a.Data(selector);                       // constant selector
      }
    }
    
    if (theCompiler->containsLoop) {
      // need counters for the sends to know how often the loop executes
      a.Data(withCountBits(lookupType, Counting));
    } else {
      a.Data(lookupType);
    }
    
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions)
      switch (lookupType) {
       case DirectedResendLookupType:
        assert(lookupType & DelegateeStaticBit, "should have static delegatee");
        assert_string(delegatee, "should be a string");
        // fall through
       case ImplicitSelfLookupType:
       case ResendLookupType:
       case StaticNormalLookupType:
       case NormalLookupType:
        assert(!isPerformLookupType(lookupType),
               "should have a static selector");
        assert_string(selector, "should be a string");
        break;
       default: break;
      }
#   endif
    if (delegatee != badOop) {
      assert(needsDelegatee(lookupType), "shouldn't have a delegatee");
      a.Data(delegatee);
    }
    return l;
  }
  
  Label* CodeGen::branch() {
    // bra,a ??
    return a.BraForward(true);
  }


  Label* CodeGen::unconditionalBranchCode( Label* dst,  
                                           bool allowPreemption, 
                                           RegisterState* s) {
    //    if allowPreemption <cPrimCall interruptCheck>
    //    bra,a dst
    
    // (this code lifted from what used to be restart)
    
    Label* nlr;
    if ( !allowPreemption) { 
      nlr = NULL;
    } 
    else {
      Label* dst1;
      testStackOverflowForLoop(dst1, nlr, s);
      dst->unify( dst1 );
    }
    a.Bra(dst, true);
    return nlr;
  }
  
  
  Label* CodeGen::conditionalBranchCode( Location testMe, 
                                         oop target, 
                                         Label* dst, 
                                         bool allowPreemption,  
                                         RegisterState* s) {
    //   <loadOop target_oop t1>
    //   <move    testMe     t2>
    //   cmp  t1, t2
    //   if no allowPreemption:
    //     beq  dst
    //     nop
    //   if allowPreemption:
    //     bne  end
    //     <unconditionalBranch dst>
    //     end:
     
    loadOop( Temp1, target );
    move( Temp2, testMe, false);
    a.SubCCR( Temp1, Temp2, G0);

    if ( !allowPreemption ) {
      a.Beq(dst, false);
      a.Nop(); // delay slot
      return NULL; // no nlr
    }
    Label* end = a.BneForward(false);
    a.Nop();
    Label* nlr = unconditionalBranchCode( dst, true, s);
    end->define();
    return nlr;
  }
  
  
  Label* CodeGen::indexedBranchCode( Location        testMe, 
                                     LabelList*      labels,
                                     bool            allowPreemption,
                                     RegisterState*  s) {
    // if allowPreemption
    //   <testStackOverflowForLoop( afterTest ) >
    // afterTest:
    // 
    //   <move testMe, t1>
    //   <loadOop smiOop of label length, t2>
    //   tcmp t2, t1
    //   bvs end
    //   nop
    //   bleu end
    //   nop
    //   call .+8
    //   add t1, o7, t1
    //   jump t1, 12, g0
    //    bra L1
    //    bra L2
    //    ...
    // end:
    
    Label* nlr= NULL;
    if (allowPreemption) {
      // Ideally would only do this in the arms that actually do
      // branch back.
      // Beware: the stack overflow test could call other code that
      // could clobber Temp1 and Temp2, so it cannot be in the
      // middle somewhere. -- dmu
      Label* afterTest;
      testStackOverflowForLoop( afterTest, nlr, s );
      afterTest->define();
    }
    
    move( Temp1, testMe, false);
    loadOop( Temp2, as_smiOop(labels->length()));
    a.TSubCCR( Temp2, Temp1, G0);
    Label* end = a.BvsForward(false);
    a.Nop();
    end->unify( a.BleuForward(false) );
    a.Nop();
    
    pc_t cra= a.addr();
    const int32 bytesFromCallToJump = 4 * sizeof(int32);
    const int32 bytesFromCallToInstAfterDelay  = 2 * sizeof(int32);
    // only on V9: a.ReadPC(Temp2); 
    a.CallN( a.addr() + bytesFromCallToInstAfterDelay);
    const int32 indexShift = 2 - Tag_Size;
    assert(indexShift == 0, "no shift needed");
    a.AddR( Temp1, CalleeReturnAddr, Temp1); // in the delay slot
    assert( a.addr() - cra  == bytesFromCallToInstAfterDelay,
      "recount instructions");
    a.JmpLI( Temp1, bytesFromCallToJump - (Int_Tag << indexShift), G0);
    a.Nop(); // delay slot for jmp, cannot have branch here
    assert( a.addr() - cra  == bytesFromCallToJump,  "recount instructions");
    for (fint i = 0;  i < labels->length(); ++i) {
      a.Bra(labels->nth(i), true);
    }
    end->define();
    return nlr;
  }


  void CodeGen::loadImmediate(Location dest, int32 value) {
    // *if p is 0...
    //   <move g0, dest>
    // *else...
    //   *if p is small...
    //      or g0, value, t/dest
    //   *else...
    //      sethi value, t/dest
    //      add t/dest, value, t/dest
    //   *end
    //   <move t, dest>
    // *end
    
    Location t;
    if (isRegister(dest)) {
      t = dest;
    } else {
      t = Temp1;
    }
    if (value == 0) {
      t = G0;
    } else if (value < maxImmediate &&
               value > -maxImmediate) {
      a.OrI(G0, value, t);
    } else {
      if (value & LOWMASK) {
        a.SetHiI(value, t);     // load high part of value
        a.AddI(t, value, t);    // add low part if nonzero
      } else {
        a.SetHiI2(value, t);    // low 10 bits are zero
      }
    }
    move(dest, t);
  }
  
  void CodeGen::loadArg(fint argNo, Location from, bool isPrimCall) {
    Unused(isPrimCall);
    if (argNo == -1) {
      // weird arg numbering - 0 is 1st arg
      move(ReceiverReg, from);
    } else {
      move(ArgLocation(argNo), from);
    }
  }

  void CodeGen::loadOop(Location dest, Location src_register, slotDesc* s) {
    Location t = isRegister(dest) ? dest : Temp1;
    int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
    a.LoadI(src_register, offset, t);
    if (dest != t)
      move(dest, t);
  }

  void CodeGen::loadOop(Location dest, oop p) {
    // *if p is 0...
    //    <move g0, dest>
    // *else...
    //    *if p is a small smiOop...
    //       or g0, p, t/dest
    //    *else...
    //       sethi p, t/dest
    //       add dest, p, t/dest
    //    *end
    //    <move t, dest>
    // *end
    
    Location t;
    if (isRegister(dest)) {
      t = dest;
    } else {
      t = Temp1;
    }
    if (p == 0) {
      t = G0;
#     ifdef TRUE_FALSE_REGISTERS
      } else if (p == Memory->trueObj) {
        t = TrueReg;
      } else if (p == Memory->falseObj) {
        t = FalseReg;
#     endif
    } else if (! p->is_mem() &&
               int32(p) < maxImmediate &&
               int32(p) > -maxImmediate) {
      a.OrO(G0, p, t);
    } else {
      a.SetHiO(p, t);                           // load high part of value
      a.AddO(t, p, t);                          // add low part 
    }
    move(dest, t);
  }
  
  
  void CodeGen::loadBlockOop(Location dest, slotsOop p, RegisterState* s){
    //   <loadOop p, rr>
    //   <move sp, arg1>
    //   <cPrimCall BlockClone>
    //   <move rr, dest>
    
    a.Comment("Begin Block Cloning");
    s->allocateArgs(1, true);                   // track args for frame construction    
    loadOop(CReceiverReg, p);                   // load block to clone
    move(Arg1, SP);                     // load frame oop for block
    PrimDesc* pd = blockClone();
    assert(! pd->needsNLRCode(), "rewrite this - must backpatch NLR code");
    Label* l = cPrimCall(pd, s, false, true, 2 /* 1 arg + rcvr */);
    assert(l == NULL, "shouldn't need a label");
    move(dest, ResultReg);
  }
  
  void CodeGen::nonLifoTrap(RegisterState*) {
    // trap g0, ST_UpLevel
    a.TrapI(G0, SignalInterface::ST_UpLevel);
  }
  
  void CodeGen::initialize_for_platform() {}
  
  
  void CodeGen::assignmentCode(realSlotRef* dataRef) {
    prologue(true, 0);
    move(Temp2, ReceiverReg);
    assignment(ReceiverReg, dataRef, ArgLocation(0));
    epilogue(Temp2);
  }


# endif // FAST_COMPILER
/* Sun-$Revision: 30.12 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include "_interruptedCtx_sparc.cpp.incl"

char* InterruptedContext::next_pc() { return *next_pc_addr(); }
void InterruptedContext::set_next_pc(void *npc){
  *next_pc_addr() = (char*)npc;
}


void InterruptedContext::set_continuation_address(char *addr, bool mustWork, bool setSema) {
  InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  

  assert(!continuePC, "continuePC already set");
  if (setSema) processSemaphore = true; 
  if (the_interrupted_context->next_pc() == the_interrupted_context->pc() + 4) {
    // normal case
    continuePC = the_interrupted_context->pc();
    the_interrupted_context->set_pc(addr);
    the_interrupted_context->set_next_pc(addr + 4);
  } else {
    // Instruction at pc is in delay slot; next instr. is non-sequential
    // Execute delay slot instruction before jumping to continuation
    int32* instp = (int32*)the_interrupted_context->pc();
    if (isCall(instp) || isJump(instp)) {
      // back-to-back CTI - shouldn't happen for Self code
      warning("setContinuationAddress: can't handle back-to-back CTI;\n");
      warning4("pc=%#lx (%lx), npc=%#lx (%lx)",
               the_interrupted_context->pc(),  *(int*)the_interrupted_context->pc(),
               the_interrupted_context->next_pc(),
               *(int*)the_interrupted_context->next_pc());
      if (mustWork) fatal("couldn't set continuation address");
    } else {
      continuePC = the_interrupted_context->next_pc();
      the_interrupted_context->set_next_pc(addr);
    }
  }
}


bool InterruptedContext::in_system_trap() {
  if (scp == &dummy_scp)
    return false;
  const int trap_code = 0x91d02000; // t g0,0x0,o0
  return code_at_pc() == trap_code;
}


int InterruptedContext::g1() {
  # if  TARGET_OS_VERSION == SOLARIS_VERSION
     return ((ucontext_t*) scp)->uc_mcontext.gregs[REG_G1];
  # elif  TARGET_OS_VERSION == SUNOS_VERSION
     return ((sigcontext *) scp)->sc_g1;
  # endif
}


int InterruptedContext::system_trap() {
  return g1();  // g1 contains function code
}


// used to be named patchMaxSP, but it seems to do what setupPreemption does,
// only with extra SPARC stuff and it's called within signal handling -- dmu 1/96

void InterruptedContext::setupPreemptionFromSignal() {
    InterruptedContext::the_interrupted_context->must_be_in_self_thread();
  
    if (continuePC) fatal("recursive setSPLimit");
    if ( the_interrupted_context->pc()      >= first_inst_addr( setSPLimitAndContinue )
     &&  the_interrupted_context->pc()      <  first_inst_addr( setSPLimitAndContinueEnd )
    ||   the_interrupted_context->next_pc() >= first_inst_addr( setSPLimitAndContinue )
     &&  the_interrupted_context->next_pc() <  first_inst_addr( setSPLimitAndContinueEnd )) {
      return;                   // already patched or just about to do it
    }
    newSPLimit = currentProcess->stackEnd();
    set_continuation_address(first_inst_addr(setSPLimitAndContinue), false, true);
}

char** InterruptedContext::pc_addr() {
  # if  TARGET_OS_VERSION == SOLARIS_VERSION
     return (char**) &scp->uc_mcontext.gregs[REG_PC];
  # elif  TARGET_OS_VERSION == SUNOS_VERSION
     return (char**) &scp->sc_pc;
  # endif
}


# if  TARGET_OS_VERSION == SOLARIS_VERSION

  // maps locations to their offset in the gregs array
  static fint loc_map[] = {     // indexed by location
    -1,     REG_G1, REG_G2, REG_G3, REG_G4, REG_G5, REG_G6, REG_G7,
    REG_O0, REG_O1, REG_O2, REG_O3, REG_O4, REG_O5, REG_O6, REG_O7,
    -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1,     
    -1,     -1,     -1,     -1,     -1,     -1,     -1,     -1};

  inline oop* reg_addr(ucontext_t* scp, Location reg) {
    fint off = loc_map[reg];
    if (off >= 0) {
      return (oop*)&scp->uc_mcontext.gregs[off];
    } else {
      // register is in bottommost window
      if (scp->uc_mcontext.gwins) {
        return (oop*)&scp->uc_mcontext.gwins->wbuf[0] + (reg - L0);
      } else {
        // was saved on stack (normal case)
        oop* sp = reg_addr(scp, SP);
        return sp + (reg - L0);
      }
    }
  }

  void  InterruptedContext::set_reg(Location reg, void* newVal) {
    *reg_addr((ucontext_t*) scp, reg) = (oop) newVal;
  }
# ifdef UNUSED
  void* InterruptedContext::get_reg(Location reg) {
    return *reg_addr((ucontext_t*) scp, reg);
  }
# endif
#endif



char** InterruptedContext::next_pc_addr() {
  # if  TARGET_OS_VERSION == SOLARIS_VERSION
     return (char**) &((ucontext_t*) scp)->uc_mcontext.gregs[REG_nPC];
  # elif  TARGET_OS_VERSION == SUNOS_VERSION
     return (char**) &((sigcontext *)scp)->sc_npc;
  # endif
}


int* InterruptedContext::sp_addr() {
  # if  TARGET_OS_VERSION == SOLARIS_VERSION
     return &((ucontext_t*) scp)->uc_mcontext.gregs[REG_SP];
  # elif  TARGET_OS_VERSION == SUNOS_VERSION
     return &((sigcontext *) scp)->sc_sp;
  # endif
}

void InterruptedContext::print_registers() {
  lprintf("Sorry, InterruptedContext::print_registers() is unimplemented for SPARC\n");
}
/* Sun-$Revision: 30.6 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "uncommonBranch_sparc.hh"

# include "_uncommonBranch_sparc.cpp.incl" 


# if defined(SIC_COMPILER)

  bool shouldRestartSend(int32* instp) {
    assert(isUnimp(instp), "not an uncommon trap instruction");
    return (disp(instp) & UncommonRestartBit) != 0;
  }

  unsigned trapCount(int32* instp) {
    assert(isUnimp(instp), "not an uncommon trap instruction");
    return disp(instp) & (UncommonRestartBit - 1);
  }

  void setTrapCount(int32* instp, unsigned count) {
    assert(isUnimp(instp), "not an uncommon trap instruction");
    assert(count < UncommonRestartBit, "count too large");
    setDisp(instp, (disp(instp) & UncommonRestartBit) | count);
  }

# endif


bool isMapLoad(int* instp) {
  return isLoadWord(instp) && getArithImm(instp) == map_offset();
}


oop mapToLoad;
const fint NumRegs = 32;
doFn  mapLoadHandler[NumRegs];


void handleMapLoadTrap(InterruptedContext* c) {
    int32* pc = (int32*)c->pc();
    assert(isMapLoad(pc), "not a map load");
    assert(c->next_pc() == c->pc() + 4, "flow should be sequential");
    Location dest = Location(rd(pc));
    mapOop resultMap;
    
    // get the result map to load
    # if  TARGET_OS_VERSION == SOLARIS_VERSION_broken
        // disabled for now -- there's some bug in get_reg  --Urs 8/94
        Location src = Location(rs1(pc));
        fint rcvrTag = int(c->get_reg(src)) & Tag_Mask;
        if (rcvrTag == Int_Tag) {
          resultMap = Memory->smi_map->enclosing_mapOop();
        } else if (rcvrTag == Float_Tag) {
          resultMap = Memory->float_map->enclosing_mapOop();
        } else {
          fatal("bad receiver tag in map load trap");
        }
    # else
        // Can't read registers, and signal handler doesn't get faulting address,
        // so don't know what the correct map is.  But it's not really needed
        // (only important thing is that it's different from any mem map) since
        // the map testing code always checks the tag if an immediate is expected.
        resultMap = NULL;
    # endif

    NCodeBase* thing = findThing(pc);
    if (!thing->isNMethod()) {
      // a PIC -- no problem, will fix itself to eliminate trap
    } else {
      nmethod* nm = nmethod::findNMethod(pc);
      if ((char*)pc >= nm->verifiedEntryPoint()) {
        // the trap happened in the body, not in the prologue
        if (nm->flags.trapCount > MapLoadTrapLimit) {
          // recompile the nmethod on next invocation to eliminate the traps
          nm->makeToBeRecompiled();
          if (nm->isYoung())
            // manipulate counters to provoke recompilation
            nm->makeVeryYoung();
          else
            nm->makeYoung();
        } else {
          nm->flags.trapCount++;
          if (nm->flags.trapCount <= 0) {
            // counter overflowed
            nm->flags.trapCount--;
          }
        }
      }
    }
    # if  TARGET_OS_VERSION == SOLARIS_VERSION
        // simply set the destination register and continue
        c->set_reg(dest, resultMap);
        c->set_pc(c->next_pc());
        c->set_next_pc(c->pc() + 4);
    # elif  TARGET_OS_VERSION == SUNOS_VERSION
        // can't set register in interrupt handler - argh!!
        if (mapLoadHandler[dest]) {
          char* cont = c->next_pc();
          InterruptedContext::set_continuation_address(first_inst_addr(mapLoadHandler[dest]), true, true);
          continuePC = cont;
          mapToLoad = resultMap;
        } else {
          fatal1("map load trap: bad destination register %d", dest);
        }
    # endif
}
/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// # pragma implementation "vframe.hh"
# include "_vframe_sparc.cpp.incl"

    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)




oop* dummy_vframe::register_contents_addr(Location r) {
  const bool traceRCA = false;
  oop* old_result = NULL;
  if (isIArgRegister(r)) {
    // the nameDesc specifies an i register, but the actual contents are
    // in saved_outregs
    old_result = &saved_outregs[r - I0];
  } else if (isExtraIArgRegister(r)) {
    // extra incoming args are ok, too
    old_result = compiled_vframe::register_contents_addr(r);
  }
  else
    ShouldNotReachHere(); // shouldn't ask dummy vframe for this register
  
  if (!SaveOutgoingArgumentsOfPatchedFrames)
    return old_result;
    
  fint i = incoming_arg_index(r);
  assert(i + 1  <  OutgoingArgsOfReturnTrapOrRecompileFrame->length(), "bounds");
  oop* argp = OutgoingArgsOfReturnTrapOrRecompileFrame->objs(i + 1);
  assert((*argp)->verify(), "checking arg");
  if (traceRCA)
    lprintf("***** vframe::register_contents_addr i = %d,  old_result = 0x%x,  argp = 0x%x\n",
            i, *old_result, *argp);
  assert(*argp == *old_result, "checking old vs. new");
  return argp;
}


oop* compiled_vframe::register_contents_secondary_addr(Location r) {
  return NULL; // never on SPARC
}


int32 compiled_vframe::register_offset(Location r) {
  return spOffset(r, fr->frame_size()) / oopSize; 
}


# if defined(SIC_COMPILER)
oop* compiled_vframe::special_register_contents_addr(Location r) {
  // get the contents of an out or global register -- works only during
  // recompilation/uncommon trap
  assert(RecompilationInProgress || ConversionInProgress ||
         currentProcess->isUncommon(), "can't get this register");
  if (O0 <= r && r <= O7) {
    // get out reg's content from frame below
    return (oop*)fr->location_addr(r, rl);
  } else {
    assert(G0 <= r && r <= G7, "expected a global register");
    return &saved_globals[r - G0];
  }
}


oop* compiled_vframe::register_contents_addr(Location r) {
    if (!isInFrame(r) 
        && (RecompilationInProgress || ConversionInProgress ||
            currentProcess->isUncommon())) {
        return special_register_contents_addr(r);
      }
  return (oop*) fr->location_addr(r, rl);
}
# endif // SIC_COMPILER
  
  
void compiled_vframe::print_code(fint curFrame) {
  
  lprintf("#%ld", (long) curFrame);
  if (!WizardMode) return;

  lprintf(" <%#lx%c@ %#lx%c%s # %ld",
         (long unsigned)fr,
         fr->return_addr() != fr->real_return_addr() ? '*' : ' ',
         (long unsigned)code,
         code->isInvalid() ? '!' : ' ',
         code->isDebug()
           ? "deb" : VMString[code->compiler()]->copy_null_terminated(),
         long(code->scopes->offsetTo(desc)));

  if (!ConversionInProgress && this->EQ(new_vframe(fr))) {
    // bottom vframe of this frame - show sendDesc
    int32* instp = (int32*)fr->real_return_addr();
    if (!isUnimp(instp)) {
      lprintf(", pc=%#lx", (fr->send_desc()));
    }
  }

  lprintf(">");
}

void compiled_vframe::fix_frame(frame* f) { fr = f;  reg_loc()->fix_frame(f); }

void compiled_vframe::get_search_locations_for_liveness_check(NameDesc* n, frame*& fr_to_search, RegisterLocator*& rl_to_search) {
  // these guys are in caller frame in SPARC
  fr_to_search = isExtraIArgRegister(n->location()) ? fr->sender() : fr;
  rl_to_search = isExtraIArgRegister(n->location()) ? rl->sender() : rl;
}


void compiled_vframe::copy_outgoing_arg(fint argNo, NameDesc* nd2, compiled_vframe* vf, dummy_vframe* dummy, 
                                        NameDesc* nd, frame* oldBlkHome, OopOopTable* blkValues) {

  // we're restarting a send (e.g. after recompilation that left most
  // recent frame invalid); outgoing args were saved by assembly glue

  // Use the value saved by the ReturnTrap asm glue (saved_outregs on SPARC)
  // But verify it against the expression stack entry of the caller.
  // For a SIC method, the latter may not be available for verification.
  // -- dmu 6/99
  Location loc= LocationOfSavedOutgoingArgInSendee(argNo);
  NameDesc* fromNd = new LocationNameDesc(loc, 0);
  if (!nd2->isIllegal()) {
    // verify that args were saved correctly
    oop val1 = vf->get_contents(nd2, false);
    oop val2 = dummy->get_contents(fromNd);
    if (val1 != val2)
      fatal3("inconsistent outgoing arg %d: %#lx vs. %#lx",
             argNo, val1, val2);
  }
  copyValue(nd, dummy, fromNd, oldBlkHome, blkValues);
}


compiled_vframe* compiled_vframe::sendeeOrNULL_for_get_expr_stack() {
  // Cannot find a register locator when this makes a dummy_vframe, so live
  // with less info for recompilation. -- dmu 2/03
  return NULL;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.8 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_format_sparc.hh"

# include "_frame_format_sparc.cpp.incl"



# if  GENERATE_DEBUGGING_AIDS
int32 fpOffset(Location reg, fint frameSize) {
  // return offset (in bytes) off of fp
    if (isRegister(reg)) {
      assert(reg >= I0 && reg < I0 + NumInRegisters ||
             reg >= L0 && reg < L0 + NumLocalRegisters,
             "can't access this register");
      return ((first_register_offset - StackFromRegister[reg]) - frameSize)
             * oopSize;
    }
    return fpOffset_abstract(reg, frameSize);
}
#endif


bool isInFrame(Location l) {
  return !isRegister(l)
       || l >= I0 && l < I0 + NumInRegisters
       || l >= L0 && l < L0 + NumLocalRegisters;
}


int32 spOffset(Location reg, fint totalFrameSize) {
  // return offset (in bytes) off of sp
  if (isRegister(reg)) {
    assert(isInFrame(reg), "can't access this register");
    return (first_register_offset - StackFromRegister[reg]) * oopSize;
  }
  else if (isStackRegister(reg)) {
    return (totalFrameSize + local_slots_offset + StackLocations - reg)
           * oopSize;
  } else if (isExtraArgRegister(reg)) {
   return (extra_arg_offset + ExtraArgLocations - reg) * oopSize;
  } else {
    assert(isExtraIArgRegister(reg), "not a stack register");
    // reg is <= ExtraIArgLocations, arg_bottom_offset is from sp
    return (totalFrameSize + arg_bottom_offset + ExtraIArgLocations - reg) * oopSize;
  }
}


int32 fpOffset_abstract(Location reg, fint frameSize) {
  // return offset (in bytes) off of fp
  if (isStackRegister(reg)) {
    return (local_slots_offset + StackLocations - reg) * oopSize;
  } else if (isExtraArgRegister(reg)) {
    return (local_slots_offset - frameSize + ExtraArgLocations - reg + 1)
           * oopSize;
  } else {
    assert(isExtraIArgRegister(reg), "not a stack register");
    return (arg_bottom_offset + ExtraIArgLocations - reg) * oopSize;
  }
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerLocator_sparc.hh"

# include "_registerLocator_sparc.cpp.incl"


void RegisterLocator::update_addresses_from_self_frame() {
}


void RegisterLocator::update_addresses_from_VM_frame() {
}


RegisterLocator* RegisterLocator::for_frame(frame* f) {
  RegisterLocator* r = new RegisterLocator;
  r->my_frame = f;
  return r;
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "aCompiler_sparc.hh"
# include "_aCompiler_sparc.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)



# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "conversion_sparc.hh"
# include "_conversion_sparc.cpp.incl"


  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void Conversion::fix_new_vfs() {
    // because sparc frames are one lower than actual frame, 
    //  and because these were built one-by-one going down the stack
    //  all new vfs except last one contain bogus frame ptr.
    // fix them up

    // (can't compare frames directly (VM stack vs real stack)
    assert(newVF[vdepth]->fr->sender() ==
           currentProcess->last_self_frame(true)->sender(),
           "should be bottom frame");

    frame* f = newVF[vdepth]->fr;
    for (fint i = vdepth-1; i >= 1; i--) {
      if (newVF[i]) {
        f = f->sender();
        newVF[i]->fix_frame(f);
      }
    }
  }


  void Conversion::fixConversionStack_for_vframe_conversion() {
    fixConversionStack(sd->return_pc(), sp);
  }
  


  bool Conversion::createFrame(fint i, nmethod *newNM) {    
    // create new frame, store it in newFr and initialize it
    // (for sparc actually have to create TWO regions,
    //  the sp area of the new frame and a dummy area below it)
    // XXXX what if fr is interp frame?

    // this routine seems to use sd and sp,
    //  and to set sd, sp, and newFr


    // figure out new value for sd and if isInInterruptCheck

    sendDesc* prev_sd= vf[i]->fr->send_desc();

    // Have to be careful when converting a frame at the bottom of the
    // stack which has just returned from interruptCheck, after a process
    // switch, or while single stepping -- otherwise the next send is
    // omitted.  MIW 6/8/94

    bool isInInterruptCheck=
             prev_sd // eliminate uncommon branches
          && prev_sd->isPrimCall()
          && prev_sd->jump_addr() == first_inst_addr(interruptCheck);

    sendDesc* sd_of_created_frame = newNM->sendDescFor(vf[i], isInInterruptCheck);
    
    
    // this piece of sparc frame holds ret addr for caller, so pass in sd->return_pc()
    sparc_sp* newSP = ((sparc_sp*)sp)->push_new_sp( sd->return_pc(),
                                                    newNM->frameSize(),
                                                    true );

    if (stk->isStackOverflow(newSP)) {
      fatal("stack overflow while converting stack frame");
    }

    sp = (char*)newSP;
    
    newFr = newSP->push_new_sp(NULL)->as_callers_frame(); // Sparc needs extra frame
    rlFr  = NULL;
    newFrRl = RegisterLocator::for_frame(newFr);
    
    sd = sd_of_created_frame;
    
    return isInInterruptCheck;
  }
	

  frame* Conversion::fixConversionStack_for_returning_to_self( 
           char* self_sparc_fp_or_ppc_sp, 
           sendDesc* self_sd ) {
    // need frame below real last frame:
    fixConversionStack((char*)self_sd, self_sparc_fp_or_ppc_sp);
    return lastFrame;
  }

 
  void Conversion::continue_after_return_trap_with_result( 
                     oop res, 
                     char* continuationPC, 
                     char* self_sparc_fp_or_ppc_sp ) {
                                               
    saved_outregs[0] = res;
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done with this                                           
    ContinueAfterReturnTrap(continuationPC, self_sparc_fp_or_ppc_sp);
  }
  
  
  oop Conversion::get_result() {
    return saved_outregs[0];
  }
  
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

/* Sun-$Revision: 30.10 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "runtime_sparc.hh"

# include "_runtime_sparc.cpp.incl"


bool check_saved_byte_map_base() {
  if (Byte_Map_Base() != (char*)saved_globals[ByteMapBaseReg - G0]) {
    error2("rSet: saved_globals for ByteMapBaseReg corrupted (%#lx vs. %#lx)",
           saved_globals[ByteMapBaseReg - G0], Byte_Map_Base());
    return false;
  }
  return true;
}


char* adjust_initial_SP(char* init_SP) {
	init_SP -= WindowSize * oopSize * 2;
	init_SP  = (char*)(int32(init_SP) & ~((frame_word_alignment << 2) - 1)); // doubleword-align it
	return init_SP;
}


oop failure_oop_for_restarting_uncommon_prim() {
  return saved_outregs[0];
  // Cannot check this; OutgoingArgs... not set for uncommon since not really args
  // assert(s == OutgoingArgsOfReturnTrapOrRecompileFrame->obj_at(0), "old vs. new");
}

void fillRegisterValue(Location loc, oop b) {
  saved_outregs[loc - ReceiverReg] = b;
}

void set_flags_for_platform() {
}
/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_sparc.hh"

# include "_frame_sparc.cpp.incl" 


// things in the various pieces:

frame** frame::nmethod_frame_chain_addr(nmethod* /*unused for sparc*/) {
  assert(is_compiled_self_frame(), "frame link only in compiled frame");
  return my_fp()->nmethod_frame_chain_addr(); 
}

objVectorOop* frame::patched_frame_saved_outgoing_args_addr(nmethod* /* unused for sparc */) {
  return my_fp()->patched_frame_saved_outgoing_args_addr(); 
}

char**  frame::currentPC_addr() { 
  //  The assert below does not work because it relies on return_address
  //   which checks to see if the real return address is == to
  //   the asm ReturnTrap routines. But if the frame has called the trap
  //   routine which has called C, the return address is in the midst
  //   of the asm routine, and so return address does not realize
  //   it is patched. -- dmu
  //  assert(is_compiled_self_frame(), "currentPC only in compiled frame");
  //  Also, there is a recursion problem with the assert.
  return my_fp()->currentPC_addr(); 
}

  
// this may not be a self frame
oop*  frame::location_addr_of_incoming_argument(Location r, RegisterLocator* /*rl*/) { 
 return (oop*)location_addr(r);
} 


// real return address, where the pc for returning TO ME is

char**  frame::real_return_addr_addr() {
  return callees_sp()->return_addr_addr(); 
}

char* frame::return_addr() {
  return platform_independent_return_addr();
}


// registers

void** frame::location_addr(Location r, RegisterLocator*) {
  return
    isInFrame(r)
    ?      my_sp()->location_addr( r )
    : callees_sp()->location_addr( IRegisterFromORegister[r] ); }



// called from recompile.c:
// copy the receiver to the new place and adjust all block homes
// (the receiver must contain references to every live block belonging
// to this frame for zapping purposes)

void frame::copy_to( char* sp,
                     char* caller,
                     char* pc,
                     bool adjust) {

  sparc_sp *new_sp = (sparc_sp*)sp;

  if (adjust) {
    // make sure all memoized blocks exist, then adjust their scope
    abstract_vframe* callee = NULL;
    OopOopTable* dummy = EMPTY;
    for ( abstract_vframe* vf = new_vframe(this);
          vf  &&  vf->fr == this;
          callee = vf,  vf = vf->sender()) {
      vf->createBlocks(callee, dummy);
    }
    frame* new_f = new_sp->as_callers_frame();
    ResourceMark rm; // for RegisterLocator
    adjust_blocks(block_scope_of_home_frame(), new_f, RegisterLocator::for_frame(new_f));
  }
  copy_oops(my_sp()->as_oops(), new_sp->as_oops(), frame_size());

  new_sp->set_link( (sparc_fp*) caller );
  new_sp->set_return_addr( pc );
}
  

// adjust fp of copied frames so they don't refer to the original stack

void frame::adjust_frame_links_of_copied_frames( frame* last_frame_to_copy,
                                                 frame* first_copied_frame) {
  
  sparc_sp* osp =                     callees_sp();
  sparc_sp* nsp = first_copied_frame->callees_sp();
  int32 diff = (char*)first_copied_frame - (char*)this;
  
  while ( osp  <=  last_frame_to_copy->my_sp() ) {

    nsp->adjust_link(diff);

    osp = osp->link()->as_callers_sp();
    nsp = nsp->link()->as_callers_sp();
  }
  // note last copied link should just be what it was so don't do anything
}

// end of copy() helpers


// stack operations

frame* frame::home_frame_of_block_scope(frame* currentFrameHint) {
  return sendee(currentFrameHint); }

frame* frame::block_scope_of_home_frame() { return sender(); }

frame* frame::home_frame_of_vfo_locals(frame* currentFrameHint) {
  return sendee(currentFrameHint); }

frame* frame::vfo_locals_of_home_frame() { return sender(); }

frame*  frame::sender() { return my_sp()->as_callers_frame(); }


// Receiver is last frame in the stack.
// Set lastFrame's return address / FP register.
// This is typically used when manipulating the stack during conversions
// to make stack traversals work out (i.e. connect VM stack to Self stack).
// (The last Self activation only has a sparc_sp; need to add a fake
// vm sparc_sp below it so we can refer to the full frame.)
// Note that the lastFrame frame will never be returned to, so we can
// bash its registers.  But a dummy frame is needed to make the stack
// look right (to serve as the frame of the last sparc_sp).
// (used to be fixStack)

void frame::fix_frame( char* pc, char* sp) {
  FlushRegisterWindows();
  assert(isOnVMStack(this), "will be overwritten");
  callees_sp()->set_link((sparc_fp*)sp);
  set_real_return_addr(pc);
}


int32 frame::frame_size_of_uncopied_frame() {
  oop* sl = my_fp()->as_oops();
  oop* l =  my_sp()->as_oops();
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      assert(sl > l || isOnVMStack(l) && !isOnVMStack(sl),
               "frame size cannot be zero or negative!");
          // NB: C frames can have neg. size: top frame on VM stack
      if (is_compiled_self_frame()) {
        int32 rf = sl - l;
        int32 nf = code()->frameSize();
        if (rf != nf && !ConversionInProgress) {
          fatal3("frame %#lx: wrong size: nmethod says %ld, really is %ld",
                 this, nf, rf);
        }
      }
    }
# endif
  return sl - l; 
}



// Return the address I was entered at, assuming I am a C++ frame
//  Since sparc frames are one below the sp (to access saved outs)
//  will have to go to up 1 to access saved ins (%i7).

char* frame::c_entry_point() {
  frame* s = sender();
  if ( s == NULL ) return NULL;
  char* r = s->real_return_addr(); // where sender will return into
  if (Memory->code->contains(r)) return NULL;
  int32* callp = (int32*) r;
  if (callp == NULL  ||  !isCall(callp)) return NULL;
  return (char*)getCallImm(callp);
}

// for a C frame, here is the address in me to return to

char* frame::c_return_pc() {
  // 2 words: the call, the delay slot
  return return_addr() + 8;
}


// called after a trap, makes a full frame

frame* frame::make_full_frame_after_trap(char* pc) {
  FlushRegisterWindows();
  return make_full_frame(pc);
}


frame* frame::make_full_frame(char* pc) {
  return callees_sp()->push_new_sp(pc)->as_callers_frame();
}


// if frame crosses stacks (VM stack --> user stack);
//  create a dummy frame on user stack

frame* frame::make_full_frame_on_user_stack() {
  if (!isOnVMStack(this))
    return this;
  
  assert(!isOnVMStack(my_sp()), "must cross stacks");

  return  my_sp()->push_new_sp(return_addr(), 0, false)
            -> as_callers_frame();
}


void frame::print_compiled() {
  if (is_compiled_self_frame())
    lprintf(" chain = %#lx;", nmethod_frame_chain(code()));
  lprintf("\n\tlocals = [%#lx, %#lx], currPC = %#lx, size = %ld words\n",
         (long unsigned)(my_sp()),
         (long unsigned)(my_fp()),
         (long unsigned)(currentPC()),
         long(frame_size()));
}


oop frame::get_lookup_arg(fint index) {
  frame* f = sendee();
  fint offset = spOffset(IArgLocation(index), f->frame_size()) / oopSize;
  return callees_sp()->as_oops()[offset];
}


void frame::printRegs() {
  sparc_sp *sp= my_sp();
  
  const fint step = 4;
  fint i= 0;
  for (Location l= L0;  l <= I7;  i++, l= Location(l+1)) {
    int32* p= (int32*)location_addr(l);
    if (i % step == 0) {
      lprintf("%#0lx ", p);
      printLocation(l);
      lprintf("-");
      printLocation(Location(l+step-1));
      lprintf(": ");
    }
    lprintf("%#10x", (void*)*p);
    if ((i + 1) % step == 0) lprintf("\n");
  }
}


void frame::printVerbose_on_this_platform() {
  // print stackTemps
  oop* first=      &my_sp()->as_oops()[top_oop_offset];
  oop* last=  &callees_sp()->as_oops()[bottom_oop_offset];
  Location t = Location(StackLocations + (last - first));
  oop* temp = first;
  while (temp <= last) {
    lprintf("%#9lx: ", temp);
    fint excess_of_4 = (t - StackLocations) & 3;
    fint n = excess_of_4 ? excess_of_4  : 3;
    if (temp == last) n = 0;
    printLocation(t); lprintf("-"); printLocation(Location(t - n));lprintf(": ");
    for (fint i = 0; i <= n; i++, temp++) {
      lprintf("%s%#10lx", i ? "\t" : "", *temp);
    }
    lprintf("\n");
    t = Location(t - (n + 1));
  }
}



sendDesc* frame::send_desc() { 
  assert(RecompilationInProgress  ||  is_self_frame(), "Only Self frames have sendDescs");
  
  if (is_interpreted_self_frame())
    return sendDesc::sendDesc_from_return_PC( return_addr()); // for Conversion::convert (& new_dummy_vframe)
    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  int32* callp = (int32*)sendDesc::sendDesc_from_return_PC( return_addr())->call_instruction_addr();

  if (!isCall(callp)) {
    callp--;                // prim frames get ret pc bumped so C can return
  }
  if (!isCall(callp)) {   // nlr code
    callp -= 2;             // 2 instructions
    if (!isCall(callp)) {
      // usually an error, but some callers tolerate it so don't break here
      return NULL;
    }
  } else if (isCall(callp - 2)) {
    // the "call" might be the register mask of a send/prim call
    return slow_send_desc(callp);
  }
  // assert(((sendDesc*)callp)->verify(), "doesn't verify");
  // wouldn't always work (e.g. during GCs)
  return sendDesc::sendDesc_from_call_instruction(callp); 

# else

  ShouldNotReachHere(); // compiled frame but no compiler?
  return NULL;

# endif
}


  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


sendDesc* frame::slow_send_desc(int32* callp) {
  // The instruction at callp looks like a call, but it could be the register
  // mask of a sendDesc at callp - 2.  Figure out which one is the true call.
  nmethod* nm = code();
  int32 offset2 = (char*)callp - nm->insts();
  int32 offset1 = offset2 - 2*oopSize;
  addrDesc *l, *end;
  for (l = nm->locs(), end = nm->locsEnd(); l->offset() < offset1; l++)
    ;
  assert(l < end, "didn't find sendDesc");
  if (l->offset() == offset1) {
    // offset1 must be a call, but maybe offset2 is, too (then offset1 must
    // be a two-instruction prim call)
    assert(l->isCall(), "not a call");
    if (l < end - 1 && l[1].offset() == offset2 && l[1].isCall()) {
      return sendDesc::sendDesc_from_call_instruction(callp);
    }
    return sendDesc::sendDesc_from_call_instruction(callp - 2);
  } else if (l->offset() == offset2) {
    assert(l->isCall(), "not a call");
    return sendDesc::sendDesc_from_call_instruction(callp);
  } else {
    fatal("call must be either at offset1 or at offset2");
    return NULL;
  }
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)


// used for scavenging, must return allocated location
//  for every interpreter state struct that may exist
//  return IllegalLocation for non-interp frame

Location frame::location_of_interpreter_of_block_scope(void* entry_point) {
  return  entry_point == first_inst_addr(interpret)  
            ?  CReceiverReg
            :  IllegalLocation;
}


frame* frame::get_patched_self_frame(char* /*sp_of_patched_frame*/ ) {
  // sparc: currentFrame = HRT's sp, sender = ReturnTrap's SP, which is frame of patched_self_frame
  // (recall that frames are sp of callee in SPARC)
  return sender();
}


int32  frame::copy_through_oop_count(frame* last_frame_to_copy) {
  return  (oop*)last_frame_to_copy->my_fp() - (oop*)this;
}


void frame::fix_current_return_address(char* addr) {
  sender()->set_real_return_addr(addr);
}


oop frame::perform_selector_of_SendMessage_stub_frame() {
  return *(oop*)callees_sp()->location_addr(nonvolatile_selector_for_performs);
}  


/* Sun-$Revision: 30.11 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_iterator_sparc.hh"

# include "_frame_iterator_sparc.cpp.incl" 

void FrameIterator::do_vm_frame() { 
  if (!SaveOutgoingArgumentsOfPatchedFrames)
    return;
  frame* s = f->sender();
  if (s == NULL  ||  !s->is_self_frame())
    return;
  // hit the outgoing args of the self frame in case it is later patched
  // (when a frame is patched we grab its outgoing args)
  // How can old sparc verison every have worked without this? -- dmu 2/03
  // As of 2/16/03, this fails on testVMSuite in debug mode for SPARC.
  fint n = s->outgoing_arg_count(f);
  for (fint i = 0;  i < n + 1 /*rcvr*/;  ++i)
    oop_closure->do_oop(
      f->location_addr_of_incoming_argument(LocationOfSavedOutgoingArgInSendee(i-1), NULL));
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

  void FrameIterator::do_compiled() {
             do_local_registers();
    fint r = do_stack_locations();
             do_above_saved_registers(r);
             do_patched_frame_saved_outgoing_args();
  }
  
  void FrameIterator::do_local_registers() {
    /* scavenge the local registers */
    oop* p = (oop*)f->location_addr(L0);

    for ( fint r = L0; r <= I7;  r++, p++) {
      if (isSet(mask,  r - L0)) {
        oop_closure->do_oop(p);
      }
      else if (zap && !isGloballyAllocatedRegister(Location(r))) {
        *p = badOop;
      }
    }
  }
  
  
  fint FrameIterator::do_stack_locations() {
    sparc_sp* sp = f->my_sp();
    sparc_fp* fp = sp->link();

    /* scavenge the ntraced memory locations */
    /* The first ntraced stack locations are marked by the sendDesc mask;
       scavenge only the ones marked as used */

    oop* end_oop = &sp->as_oops()[top_oop_offset - 1];
    oop* p       = &fp->as_oops()[bottom_oop_offset];

    fint r = NumInRegisters + NumLocalRegisters;
    int ntraced = BitsPerWord - r;
    int traced_oop_count = min(p - end_oop,  ntraced);
    int endBit = r + traced_oop_count;

    assert(p >= end_oop, "stack frame too small??");
    for (  ;  r < endBit;  ++r, --p) {
      if (isSet(mask, r)) {
        oop_closure->do_oop(p);
      } else if (zap) {
        *p = badOop;
      }
    }
    /* scavenge the untraced stack locations */

    for (  ;   p > end_oop;   --p )
      oop_closure->do_oop(p);
      
    return r;
  }
  
  
  void FrameIterator::do_above_saved_registers(fint r) {

    /* Since Self never uses the 7 words above the saved regs (C stuff),
       let's check that the register mask does not claim they are live,
       and let's zap 'em.
    */
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions)
        for ( ;  r < BitsPerWord;  ++r )
          assert(!isSet(mask, r),
                "mask says a word in callee reg args or aggr ret is live");
    # endif
    if (zap) {
      sparc_sp* sp = f->my_sp();
      oop* p = &sp->as_oops()[NumInRegisters + NumLocalRegisters];
      oop* end_oop = &sp->as_oops()[WindowSize];
      for ( ;  p < end_oop;  ++p)  *p = badOop;
    }
  }                             
# endif
/* Sun-$Revision: 30.9 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "framePieces_sparc.hh"

# include "_framePieces_sparc.cpp.incl"



// used to be frameFromStackLocals, dummyFrameFor
// don't use this if locals isn't the very last sparc_sp on the stack!
// I am the last sparc_sp on the stack, and there is no frame for it;
// construct a dummy sp frame

sparc_sp* sparc_sp::push_new_sp( char* pc,
                                 fint size_in_oops,
                                 bool zapAlways ) {
  // ( this was 1024 in one previous version and 64 in another!)
  if (size_in_oops == 0) {
    size_in_oops = stackTempCountToFrameSize(0);
    size_in_oops += size_in_oops & (frame_word_alignment-1); // make even
  }
  assert((size_in_oops & 1) == 0, "sparc frames must be even length");
  sparc_sp* sp = (sparc_sp*)( as_oops() - size_in_oops );
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      zapAlways = true;
    }
# endif
  if (zapAlways)
    // If errorObj starts showing up where it shouldn't, I recommend
    // substituting 0xfffffffe for it below: (dmu)
    set_oops(sp->as_oops(), size_in_oops, (oop)Memory->errorObj);
  sp->set_link(as_callees_fp());
  sp->set_return_addr(pc);
  assert(sp->link() == (sparc_fp*)this, "just checkin'");
  return sp;
}



void** sparc_sp::location_addr(Location r) {
  assert( isInFrame(r),  "bounds check");
  fint offset = spOffset(r,  link()->as_oops() - this->as_oops()) / oopSize;
  return &((void**) this)[offset];
}
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "search_i386.hh"
# include "_search_i386.cpp.incl"


# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "countStub_i386.hh"
# include "_countStub_i386.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void AgingStub::initPattern() {
  /*
    // nothing to do
  */
  }


  pc_t CountStub::jump_addr() {
    CountCodePattern* patt = CountStub::pattern[countType()];
    int32* p = (int32*)(int32(insts()) + patt->nmAddr_offset);
    return (pc_t) *p + int32(p) + sizeof(int32);
  }

  void ComparingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    set_recompile_addr((pc_t)Recompile_stub);
    set_count_addr(patt, (int32)&sendCounts[id()]);

    int32* p = (int32*)(int32(insts()) + patt->limit_offset);
    assert(*p == patt->initial_limit, "???");
    fint limit = recompileLimit(nm->level());
    *p = limit;
  }

  void AgingStub::init(nmethod* nm) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    set_recompile_addr(first_inst_addr(MakeOld_stub));
    set_count_addr(patt, (int32)&sendCounts[id()]);
    int32* p = (int32*)(int32(insts()) + patt->limit_offset);
    assert(*p == patt->initial_limit, "???"); 
    fint limit = nm->agingLimit();
    *p = limit;
    set_count(1);    
  } 


# ifdef UNUSED
  pc_t ComparingStub::get_recompile_addr() { 
     fatal("Unused Intel");
  }
# endif
  
  void ComparingStub::set_recompile_addr(pc_t addr) {
    CountCodePattern* patt = CountStub::pattern[Comparing];
    int32* p = (int32*)(int32(insts()) + patt->recompileStub_offset);
    *p = int32(addr) - int32(p) - sizeof(int32);
  }
  

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "addrDesc_i386.hh"

# include "_addrDesc_i386.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


bool addrDesc::isShiftNeededAfterMovingMe(OopNCode* m) {
  return isRelative();
}


pc_t addrDesc::instr_referent(OopNCode* m) {
  pc_t* dispp = addr(m);
  assert(m->contains(dispp), "not in this nmethod");
  return *dispp + (isRelative() ? int32(dispp + 1) : 0);
}


void addrDesc::set_instr_referent(OopNCode* m, void* newVal) {
  pc_t* dispp = addr(m);
  assert(m->contains(dispp), "not in this nmethod");
  *dispp = pc_t( int32(newVal) - (isRelative() ?  int32(dispp) + oopSize  :  0) );
}


void addrDesc::relocateTarget(OopNCode* m, int32 delta) {
  pc_t* dispp = addr(m);
  assert(m->contains(dispp), "not in this nmethod");
  if (!isRelative())
    return;
  *dispp -= delta;  
}
  

bool addrDesc::verify(nmethod* m) {
  bool flag = true;
  if (offset() >= m->instsLen() + m->scopes->length()) {
    error1("bad offset in addrDesc at %#lx", (long)this);
    flag = false;
  }
  if (isSendDesc()) {
    flag = asSendDesc(m)->verify() && flag;
  }
  else if (isDIDesc()) {
    flag = asDIDesc(m)->dependency()->verify_list_integrity() && flag;
  }
  else if (isPrimitive())
    ;
  else 
    flag = oop(referent(m))->verify_oop() && flag;
    
  inst_t* instp = (inst_t*)addr(m);
  return flag;
}


// not inlined to reduce .h dependencies
sendDesc* addrDesc::asSendDesc(OopNCode* m) {
  assert(isSendDesc(), "not a sendDesc location");
  return sendDesc::sendDesc_from_addrDesc_addr(addr(m));
}
  
sendDesc* addrDesc::asPrimitiveSendDesc(OopNCode* m) {
  // note that it's not really an inline cache, just a primitive call
  assert(isPrimitive(), "not a primitive location");
  return sendDesc::sendDesc_from_addrDesc_addr(addr(m));
}
  
DIDesc* addrDesc::asDIDesc(nmethod* m) {
  assert(isDIDesc(), "not a diDesc location");
  return DIDesc::DIDesc_from_addrDesc_addr(addr(m));
}




# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "countPattern_i386.hh"
# include "_countPattern_i386.cpp.incl"


# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)
 

  void CountCodePattern::initCounting() {
    // general count stub; 
    //  0: lea   count_addr, %eax
    //  6: movl  (%eax), %edx
    //  8: leal  1(%edx), %edx
    // 11: movl  %edx, (%eax)
    // 13: jmp    jump_addr
    // 18: <end>
    
    Assembler* oldAssembler = theAssembler;
    Assembler* a = theAssembler = new Assembler(100, 100, false, true);
    a->incl( no_reg, initial_count_addr, VMAddressOperand);
    countAddr_offset = a->offset() - sizeof(int);
    a->jmp(initial_nmAddr, CodeAddressOperand);
    nmAddr_offset = a->offset() - sizeof(int);
    instsSize = a->offset();

    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_bytes(a->instsStart, pattern, instsSize);
    a->finalize();
    theAssembler = oldAssembler;
  }


  void CountCodePattern::initComparing() {
    // general count stub;
    //  0: lea   count_addr, %eax
    //  6: movl  (%eax), %edx
    //  8: leal  1(%edx), %edx
    // 11: movl  %edx, (%eax)
    // 13: cmpl  %edx, limit
    // ??: jne jump_addr
    //     call recompile_addr
 
    
    Assembler* oldAssembler = theAssembler;
    Assembler* a = theAssembler = new Assembler(100, 100, false, true);

    a->leal(no_reg, initial_count_addr, VMAddressOperand, edx);
    countAddr_offset = a->offset() - sizeof(int32);
    a->movl(edx, 0, NumberOperand,  eax);
    a->incl(eax);
    a->movl(eax,  edx, 0, NumberOperand);
    a->cmpl(0xabcdef01, NumberOperand,   eax); 
    limit_offset = a->offset() - sizeof(int32);
    a->jne(0x87654321, CodeAddressOperand);
    nmAddr_offset = a->offset() - sizeof(int32);
    a->call((int32)Recompile_stub, VMAddressOperand);
    recompileStub_offset = a->offset() - sizeof(int32);

    instsSize = a->offset();
    pattern = (pc_t)AllocateHeap(instsSize, "countStub pattern");
    copy_bytes(a->instsStart, pattern, instsSize);
    *(int32*)&pattern[recompileStub_offset] -= pattern - a->instsStart; // fixup the call
    a->finalize();
    theAssembler = oldAssembler;    
  }
    

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "nmethod_i386.hh"

# include "_nmethod_i386.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)

void nmethod::get_platform_specific_data(AbstractCompiler* c) {
  _number_of_memory_locals               = c->number_of_memory_locals();
}

void nmethod::print_platform_specific_data() {
  ++Indent;
  lprintf( "number_of_memory_locals               = %d\n", number_of_memory_locals() );
  --Indent;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "trapdoors_i386.hh"
# include "_trapdoors_i386.cpp.incl"

Trapdoors::Trapdoors(pc_t, int32) {}

int32 Trapdoors::trapdoor_bytes() { return 0; }

pc_t Trapdoors::  SendMessage_stub_td(Location) { return first_inst_addr(   ::SendMessage_stub); }
pc_t Trapdoors::SendDIMessage_stub_td(Location) { return first_inst_addr( ::SendDIMessage_stub); }
pc_t Trapdoors::    Recompile_stub_td(Location) { return first_inst_addr(     ::Recompile_stub); }
pc_t Trapdoors::  DIRecompile_stub_td(Location) { return first_inst_addr(   ::DIRecompile_stub); }

pc_t Trapdoors::follow_trapdoors(pc_t target) { return target; } // no trapdoors


# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */
  
# pragma implementation "regs_i386.hh"
  
# include "_regs_i386.cpp.incl"

const char** RegisterNamesBySize[] = {ByteRegisterNames, ShortRegisterNames, RegisterNames};

// change Location enum in regs_i386.h if you change this!
const char* RegisterNames[] = {
    "%eax", "%ecx", "%edx", "%ebx", "%esp", "%ebp", "%esi", "%edi",
    "",
    
    "*UnAllocated*"
};
const char* ByteRegisterNames[] = {
    "%al", "%cl", "%dl", "%bl", "%ah", "%ch", "%dh", "%bh",
    "",
    
    "*UnAllocated*"
};
const char* ShortRegisterNames[] = {
    "%ax", "%cx", "%dx", "%bx", "%sp", "%bp", "%si", "%di",
    "",
    
    "*UnAllocated*"
};
    
    



const char *locationName(Location l) {
  const char* c;
  int num;
  
       if          ( is_IArgLocation(l)) {   c = "I";  num = index_for_IArgLocation(l); }
  else if          ( is_LArgLocation(l)) {   c = "L";  num = index_for_LArgLocation(l); }
  else if          ( is_ArgLocation(l)) {    c = "";   num = index_for_ArgLocation(l); }
  else if          ( is_StackLocation(l)) { c = "S";  num = index_for_StackLocation(l); }
  else if          ( l == IReceiverReg ) { c = "I"; num = -1; }
  else if          ( l == LReceiverReg ) { c = "L"; num = -1; }
  else if          ( l == ReceiverReg ) { c = "R"; num = -1; }
  else {
    assert(isRegister(l), "");
    return RegisterNames[l];
  }
  char* s= new char [30]; // known leak NEW_RESOURCE_ARRAY(char, 30);
  sprintf(s, "%s%ld", c, long(num));
  return s;
}
    

# if defined(SIC_COMPILER)
  Location TempRegs[] = { eax, edx, esi, edi };
    
# define X(arg) -99999999     /* to make the following table look nicer */
  fint RegToTempNo[/* indexed by Location */] = {
    0, X("ecx"), 1, X("ebx"), X("esp"), X("ebp"), 2, 3 
  };

  Location CalleeSavedRegs[] = {
  };

  void regs_i386_init() {
    // The SIC uses Temp1 and Temp2 during code generation, that's why they
    // aren't listed as general temp regs above.
    assert(Temp1 == ebx && Temp2 == ecx, "change this");
    for (fint i = 0; i < NumTempRegs; i++) {
      assert(TempRegs[i] != Temp1 && TempRegs[i] != Temp2,
             "Temp1 and Temp2 are reserved for the code generator");
      assert(RegToTempNo[TempRegs[i]] == i, "wrong RegToTempNo entry");
    }
  }

# else

void regs_i386_init() {}

# endif
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "disasm_i386.hh"

# include "_disasm_i386.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


void print_code(nmethod* /*nm*/, CORE_ADDR start, CORE_ADDR end) {
  warning2("unimp intel (no disasm) start = 0x%x, end = 0x%x", start, end);   
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "asm_i386.hh"
# pragma implementation "asm_inline_i386.hh"  
  
# include "_asm_i386.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


Assembler* theAssembler;      // current assembler for instructions
  

Assembler::Assembler(int32 instsSize, int32 locsSize, bool pr, bool isInstrs)
  : BaseAssembler(instsSize, locsSize, pr, isInstrs) {
  
  if (bootstrapping)
    return;  // zone does not exist yet
    
  pc_t buf_lo  = instsStart,
       buf_hi  = instsOverflow,
       zone_lo = Memory->code->code_start(), // warning counts on order of iZone and stubs
       zone_hi = Memory->code->code_end();
       
}



  
# ifdef SIC_COMPILER
    void Assembler::unimp(fint n, bool shouldRestart) {
      ud2();
      // add an addrDesc for the unimp instruction so they're easier to find
      int32 mask = offset();
      genLoc(mask | addrDesc::isUncommonTrapMask);
      if (shouldRestart) n |= UncommonRestartBit;
      Data(n);
    }
# endif

// Testing helpers:


static void untested_helper(char* m1, char* m2) {
  lprintf("untested generated code: %s %s\n", m1, m2);  breakpoint();
}


void Assembler::_Untested(const char* m1, const char* m2) {
  // don't use eax, is receiver
  // preserve eax  
  if (!SpendTimeForDebugging)
    return;
    
  bool p = printing;
  if (printing) {
    printing = false;
    asm_lprintf("assembling: %s %s\n", m1, m2);
  }
  pushl(eax);
  pushl(ebx);
  pushl(ecx);
  pushl(edx);
  enter(0);
  andl(~15, NumberOperand, esp);
  leal(esp, -8, NumberOperand, esp);
  pushl((int32)m2, VMAddressOperand);
  pushl((int32)m1, VMAddressOperand);
  // use indirect call to be safe, avoid need to reloc relative dest
  leal(no_reg, (int32)first_inst_addr(untested_helper), VMAddressOperand, eax);
  call(eax);
  leave();
  popl(edx);
  popl(ecx);
  popl(ebx);
  popl(eax);
  printing = p;
}


// General helpers:

void Assembler::printR(RegSize rs, Location r, const char* ps) {
  if (printing) asm_lprintf("%s%s", RegisterNamesBySize[rs][r], ps);
}


# if TARGET_OS_VERSION == MACOSX_VERSION  &&  OSX_ASM_RELEASE == PRE_2007_OSX_ASM_RELEASE
  static const bool earlierGCC = true;
# else
  static const bool earlierGCC = false;
# endif


void Assembler::printRM( RegSize rs, Location r, int32 d, OperandType t, Location index, Scale s, const char* ps, bool is_call) {
  if (!printing) return;
  if ( t == RegisterOperand ) {
    printR(rs, r, ps);
    return;
  }
  if ( r == no_reg  &&  index == no_reg )   {
    if ( is_call )  asm_lprintf("*");
    printRM_disp(d, t);
    asm_lprintf("%s", ps);
    return;
  }

  if  ( is_call && !earlierGCC )
    asm_lprintf("*");
  if ( d != 0  ||  t != NumberOperand )
    printRM_disp(d, t);
  asm_lprintf("(");
  printR(long_reg, r, "");
  if (index != no_reg)
    asm_lprintf(", %s, %d", RegisterNames[index], 1 << (int)s);
  asm_lprintf(")%s", ps);
}


void Assembler::printRM_disp(int32 d, OperandType t) {
  if (is_testing()) asm_lprintf("0x%x", d);
  else              print_disp(d, t);
}


void Assembler::printDest( int32 dest, OperandType t, Label* L ) {
  if (!printing)  return;
  if (is_testing()) {
    assert(L == NULL, "cannot test labels (yet)");
    int32 relDot = dest - int32(instsEnd);
    asm_lprintf(". %c %d", relDot < 0 ? '-' : '+', abs(relDot));
 }
 else if (L != NULL)
   asm_lprintf("L%d%c", L->id(),  L->isDefined() ? 'b' : 'f');
 else
   print_disp(dest, t);
 asm_lprintf("\n");
} 


void Assembler::printImm( int32 imm, OperandType t, const char* ps) {
  if (!printing)    return;
  if (is_testing()) asm_lprintf("$0x%x", imm);
  else            { asm_lprintf("$");  print_disp(imm, t);  }
  asm_lprintf("%s", ps);             
}


void Assembler::addOffset(OperandType t, bool isEmbedded, bool isRelative) {
  if (t == NumberOperand)
    ;
  else if (t == VMAddressOperand  &&  !isRelative)
    ; // don't need a location unless relative
  else
   doAddOffset(t, isEmbedded, (isRelative ? addrDesc::isRelativeMask : 0));
}




void Assembler::base_disp_reg( 
    RegSize rs,
    Location reg_operand, 
    Location base_reg, int32 disp, OperandType t, Location index_reg, Scale scale,
    const char* suffix,
    bool is_call ) {
    
  assert(index_reg != esp, "esp cannot be index in i386");
  printRM( rs,  base_reg, disp, t, index_reg, scale, suffix, is_call);
    
  if (disp == 0)
    switch(t) {
     default: break;
     case RegisterOperand:
      reg_reg(reg_operand, base_reg, index_reg, scale);
      return;
     case   NumberOperand:  
      if ((base_reg == ebp || base_reg == no_reg) &&  index_reg != no_reg)
        break; // hole in the map to allow for scaled-index + disp32, not noDisp
      base_noDisp_reg(reg_operand, base_reg, index_reg, scale);
      return;
    }

  if (is_disp8(disp, t)  &&   base_reg != no_reg)
    return numberBase_disp8_reg(reg_operand, base_reg, disp, index_reg, scale);

  if (base_reg == no_reg  &&  index_reg == no_reg) {
    mod_rm_reg(0, 5, reg_operand);
  }
  else if ( index_reg == no_reg ) {
    mod_rm_reg(2, base_reg, reg_operand);
    if (base_reg == esp) {
      sib(0, 4, esp);
    }
  }
  else if ( base_reg == no_reg ) {
    mod_rm_reg(0, (Location)4, reg_operand);
    sib(scale, index_reg, (Location)5);
  }
  else {
    mod_rm_reg(2, (Location)4, reg_operand);
    sib(scale, index_reg, base_reg);
  }
  addOffset(t, true, false);
  Data(disp, false);
}




void Assembler::numberBase_disp8_reg(Location reg_operand, Location base_reg, int32 disp, Location index_reg, Scale scale) {

  if (base_reg == no_reg) {
    assert(index_reg != no_reg, "???");
    mod_rm_reg(1, (Location)4, reg_operand);
    sib(scale, index_reg, (Location)5);
  }
  else if (index_reg != no_reg) {
    mod_rm_reg(1, 4, reg_operand);
    sib(int32(scale), index_reg, base_reg);
  }
  else if (base_reg == esp) {
    mod_rm_reg(1, 4, reg_operand);
    sib(0, 4, esp);
  }
  else {
    mod_rm_reg(1, base_reg, reg_operand);
  }
  Byte(disp);
}




// See Vol 2A 2-7 pp 37 of IA-32 Intel Architecture Software Developer's Manual

void Assembler::base_noDisp_reg( Location reg_operand, Location base_reg, Location index_reg, Scale scale ) {
  if (index_reg != no_reg) {
    mod_rm_reg(0, (Location)4, reg_operand);
    sib(int32(scale), index_reg, base_reg);
    return;
  }
  switch (base_reg) {
   default: fatal("???");
   
   case no_reg: fatal("no displacement or base");
   case esp:
     mod_rm_reg(0, 4, reg_operand);
     sib(0, 4, esp);
     break;
     
   case ebp:
     mod_rm_reg(1, ebp, reg_operand);
     Byte(0); // disp 8
     break;
     
   case eax: case ecx: case ebx: case edx: case esi: case edi:
     mod_rm_reg(0, base_reg, reg_operand);
     break;
  }
}


void Assembler::reg_reg(Location reg_operand, Location rm_reg,  Location index_reg, Scale scale) {
  assert(index_reg == no_reg, "???");
  mod_rm_reg(3, rm_reg, reg_operand);
}





// Jumps



void Assembler::word_branch_target(int32 dest, OperandType t, Label* L) {
  assert(L == NULL  ||  dest == 0  &&  t == NumberOperand, "???");
  if (L == NULL)
    ;
  else if (L->isDefined()) {
    dest = (int32)L->target();
    t = NumberOperand;
  }
  else {
    dest = 0;
    L->unify(new Label(printing, addr()));
    t = NumberOperand;
  }
  addOffset(t, true, true);
  int32 rel = dest - (int32(instsEnd) + sizeof(int32));
  Data(rel, false);
}


void Assembler::std_jmp_or_loop(const char* j_or_loop, const char* ccs, int32 op8, int32 op32, int32 dest, OperandType disp_type, Label* L, bool add_0f_prefix) {
  if (printing)  {
    asm_lprintf("%s%s ", j_or_loop, ccs);
  }
  int32 rel8 = (L == NULL  ?  dest  :  (int32)L->target() )  -  ((int32)instsEnd + 2);
  
  if ( L == NULL  &&  is_disp8( rel8, disp_type)  &&  op8 != -1) {
    if (printing) {
      asm_lprintf(
       	   !is_testing() ? "<byte> "  
        :  TARGET_OS_VERSION == LINUX_VERSION  ?  ""  :  "/* byte */");
      printDest(dest, disp_type, L);
    }
    Byte(op8);
    Byte(rel8);
  }
  else if (op32 != -1) {
    printDest(dest, disp_type, L);
    if (add_0f_prefix)
      Byte(0x0f);
    Byte(op32);
    word_branch_target(dest, disp_type, L);
  }  
  else fatal("not in I386");
}


void Assembler::jcc( const char* ccs, jump_conds::conds cc, jump_ops::ops op, int32 d, OperandType t, Label* L) {
  std_jmp_or_loop("j", ccs, 0x70 | op, 0x80 | op, d, t, L);
}



void Assembler::call( int32 dest, OperandType t, Label* L ) {
  if (printing) {
    asm_lprintf("call ");
    printDest(dest, t, L);
  }
  Byte(opcd_ImmCall);
  word_branch_target(dest, t, L);
}


void Assembler::jmp(   int32 d, OperandType t, Label* L ) { std_jmp_or_loop(   "j",   "mp", 0xeb, 0xe9, d, t, L, false); }
void Assembler::jecxz( int32 d, OperandType t, Label* L ) { std_jmp_or_loop(   "j", "ecxz", 0xe3,   -1, d, t, L       ); }
void Assembler::loop(  int32 d, OperandType t, Label* L ) { std_jmp_or_loop("loop",     "", 0xe2,   -1, d, t, L       ); }
void Assembler::loope( int32 d, OperandType t, Label* L ) { std_jmp_or_loop("loop",    "e", 0xe1,   -1, d, t, L       ); }
void Assembler::loopne(int32 d, OperandType t, Label* L ) { std_jmp_or_loop("loop",   "ne", 0xe0,   -1, d, t, L       ); }


# define J(cond) \
 void Assembler::CONC(j,cond)( int32 d, OperandType t, Label* L ) { \
   jcc(STR(cond), jump_conds::cond, jump_ops::cond, d, t, L); \
 } 
 
 
# define T(name) \
 void Assembler::name( Label* L ) { \
   name( 0, NumberOperand, L ); \
 }
 
J(a) J(ae) J(b) J(be) J(c) J(e) J(g) J(ge) J(l) J(le) J(na) J(nae) J(nb) J(nbe) J(nc)
J(ne) J(ng) J(nge) J(nl) J(nle) J(no) J(np) J(ns) J(nz) J(o) J(p) J(pe) J(po) J(s) J(z)

T(ja) T(jae) T(jb) T(jbe) T(jc) T(je) T(jg) T(jge) T(jl) T(jle) T(jna) T(jnae) T(jnb) T(jnbe) T(jnc)
T(jne) T(jng) T(jnge) T(jnl) T(jnle) T(jno) T(jnp) T(jns) T(jnz) T(jo) T(jp) T(jpe) T(jpo) T(js) T(jz)

T(jmp) T(jecxz) T(call)
T(loop) T(loope) T(loopne)

# undef J
# undef T



void Assembler::call( Location base, int32 disp, OperandType t, Location index_reg, Scale s) {
  if  (printing)  asm_lprintf("call ");
  Byte(0xff);
  base_disp_reg(long_reg, Location(2), base, disp, t, index_reg, s, "\n", true);
}

void Assembler::jmp(Location base, int32 disp, OperandType t, Location index_reg, Scale s) {
  if  (printing)  asm_lprintf("jmp ");
  Byte(0xff);
  base_disp_reg(long_reg, Location(4), base, disp, t, index_reg, s, "\n", true);  
}



// ADD-like instructions

// helpers:

bool Assembler::do_special_shifts(int32 op_imm_rm, int32 imm) {
  if ((op_imm_rm & ~1) != 0xc0)                                   return false;
  if (imm == 1                 ) { Byte(0xd0 | (op_imm_rm & 1));  printImm(1, NumberOperand, ", "); return true; }
  if (imm == shift_by_count_reg) { Byte(0xd2 | (op_imm_rm & 1));  printR(     byte_reg, ecx, ", "); return true; }
                                                                  return false;
} 


void Assembler::like_add8_imm(const char* name, int32 imm, Location dst_reg, int32 dst_disp, OperandType dt,
               Location index, Scale s, int32 op_imm8_al, int32 op_imm8_rm8, int32 opExt) {
  if (printing)
    asm_lprintf("%sb ", name);
    
  if (do_special_shifts(op_imm8_rm8, imm)) {
      base_disp_reg( byte_reg, Location(opExt), dst_reg, dst_disp, dt, index, s, "\n");
      return;
  }

  printImm(imm, NumberOperand, ", ");

  if (dst_reg == eax  &&  dt == RegisterOperand  &&  op_imm8_al != -1) {
    Byte(op_imm8_al);
    printR(byte_reg, dst_reg, "\n");
  }
  else if (dt == RegisterOperand  && op_imm8_rm8 == 0xc6) { /* mov */
    Byte(0xb0 + dst_reg);
    printR(byte_reg, dst_reg, "\n");
  }
  else {
    assert(op_imm8_rm8 != -1, "should not be here");
    Byte(op_imm8_rm8);
    base_disp_reg(byte_reg,  Location(opExt), dst_reg, dst_disp, dt, index, s, "\n");
  }
  assert(0 <= imm  &&  imm < 256, "imm range");
  Byte(imm);
}


void Assembler::like_add32_imm(const char* name, int32 imm, OperandType st,
                               Location dst_reg, int32 dst_disp, OperandType dt,
                               Location index, Scale s, int32 op_imm32_eax, int32 op_imm8_rm32, int32 op_imm32_rm32, int32 opExt ) {
  if (printing)
    asm_lprintf("%sl ", name);

  if (do_special_shifts(op_imm8_rm32, imm)) {
      base_disp_reg( long_reg, Location(opExt), dst_reg, dst_disp, dt, index, s, "\n");
      return;
  }
  printImm(imm, st, ", ");
  if ( is_disp8(imm, st) &&  op_imm8_rm32 != -1) {
    Byte(op_imm8_rm32);
    base_disp_reg( long_reg, Location(opExt), dst_reg, dst_disp, dt, index, s, "\n");
    Byte(imm);
    return;
  }
  
  if (dt == RegisterOperand  &&  dst_reg == eax  &&  op_imm32_eax != -1) {
    printR(long_reg, dst_reg, "\n");
    Byte(op_imm32_eax);
  }
  else if (dt == RegisterOperand  &&  op_imm32_rm32 == 0xc7) { /* mov */
    printR(long_reg, dst_reg, "\n");
    Byte(0xb8 + dst_reg);
  }
  else {
    assert(op_imm32_rm32 != -1, "should not be here");
    Byte(op_imm32_rm32);
    base_disp_reg( long_reg, Location(opExt), dst_reg, dst_disp, dt, index, s, "\n");
  }
  addOffset(st, true, false);
  Data(imm, false);
}  


static inline bool gcc_optimizes_mov(int32 op) {
  return !earlierGCC
      || (op & 1); // earlier gcc only optimizes movl, not movb
}

void Assembler::like_add_r_rm(const char* name, char bOrL, Location src, Location dst_base, int32 dst_disp, OperandType dt, Location index, Scale s, int32 op) {
  if (printing)   asm_lprintf("%s%c ", name, bOrL);
  printR(bOrL == 'l' ? long_reg : byte_reg,  src, ", ");
  // mov has special codes for just disp
  if ( (op & ~1) == 0x88  &&  dst_base == no_reg  &&  index == no_reg  &&  src == eax  &&  gcc_optimizes_mov(op) ) {
    // movb moffs8, al
    // movl moffs32, eax
    Byte(0xa2 | (op & 1));
    addOffset(dt, true, false);
    Data(dst_disp, false);
    printRM(bOrL == 'l' ? long_reg : byte_reg, dst_base, dst_disp, dt, index, s, "\n");
  }
  else {
    assert(op != -1, "should not be here");
    Byte(op);
    base_disp_reg(bOrL == 'l' ? long_reg : byte_reg, src, dst_base, dst_disp, dt, index, s, "\n");
  }
}


void Assembler::like_add_rm_r(const char* name, char bOrL, Location src_base, int32 src_disp, OperandType st, Location index, Scale s, Location dst, int32 op) {
  if (printing)  asm_lprintf("%s%c ", name, bOrL);
  // mov has special codes for just disp
  if ( (op & ~1) == 0x8a  &&  src_base == no_reg  &&  index == no_reg  &&  dst == eax  &&  gcc_optimizes_mov(op) ) {
    // movb moffs8, al
    // movl moffs32, eax
    Byte(0xa0 | (op & 1));
    addOffset(st, true, false);
    Data(src_disp, false);
    printRM(bOrL == 'l' ? long_reg : short_reg, src_base, src_disp, st, index, s, ", ");
  }
  else {
    assert(op != -1, "should not be here");
    Byte(op);
    base_disp_reg(bOrL == 'l' ? long_reg : byte_reg, dst, src_base, src_disp, st, index, s, ", ");
  }
  printR(bOrL == 'l' ? long_reg : byte_reg, dst, "\n");
}


// Other instructions


void  Assembler::pushl(int32 src_imm, OperandType disp_type) {
  if (printing) {
    asm_lprintf("pushl ");
    printImm(src_imm, disp_type, "\n");
  }
  if (is_disp8(src_imm, disp_type)) {
    Byte(0x6a);
    Byte(src_imm);
  }
  else {
    Byte(0x68);
    addOffset(disp_type, true, false);
    Data(src_imm, false);
  }
}




void Assembler::ret( int32 stack_incr) {
  if (printing) {
    asm_lprintf("ret");
    asm_lprintf(stack_incr ? " $%d\n" : "\n", stack_incr);
  }
  if (stack_incr == 0) {
    Byte(0xc3);
  }
  else {
    Byte(0xc2);
    Short(stack_incr);
  }
}
  


// Junk
  


                               

 
  

# ifdef SIC_COMPILER
    // for statistics
    // encode type tests with trigger instructions; when changing these, be
    // sure to change 1st instr of SendMessage_stub as well

    void Assembler::startTypeTest(fint ncases, bool prologueCheck,
                                  bool immedOnly) {
      fatal("unimp intel");
    }
    
    void Assembler::doOneTypeTest() { fatal("unimp intel"); }
    void Assembler::endTypeTest()   { fatal("unimp intel"); }
    void Assembler::markTagTest(fint n, bool isArith) {
      fatal("unimp intel");
    }
    
# endif




# define FOR_ALL_IMM8s(imm8) \
  for (int32 imm8 = 0; \
             imm8 != 256; \
             imm8 = imm8 ==    0 ?  1 : \
                    imm8 ==    1 ?  255 : 256)

# define FOR_ALL_REGS(x) \
  for (Location x = eax;  x <= edi;  ++*(int*)&x)
  
# define FOR_ALL_BASES(x) \
  for (Location x = eax;  x <= no_reg;  x = Location((int)x + 1))


OperandType test_ots[] = {NumberOperand, RegisterOperand, VMAddressOperand}; 
# define FOR_ALL_INTERESTING_OPERAND_TYPES(ot) \
  for (OperandType ot = RegisterOperand;  \
                   ot != OperandType(-1); \
                   ot = (ot == RegisterOperand \
                         ? NumberOperand \
                         :   ot == NumberOperand ? VMAddressOperand : OperandType(-1))) 
  
int32 test_disps[] = {0, 1, -1, 127, -128, 0x10000000, -0x10000000};
# define FOR_ALL_DISPS(x, xx, ot) \
  for (int xx = 0, x = 0;  \
           xx < sizeof(test_disps) / sizeof(test_disps[0]) \
             ? (x = test_disps[xx], true) : false; \
         ++xx) \
    if (ot == RegisterOperand && x != 0) {} /* dont bother, x will be ignored */\
    else if (ot != NumberOperand  &&  is_disp8(x)) {} /* gcc will give us a short disp */\
    else
         
# define FOR_ALL_INDICES(reg, scale, operand_type) \
  if (operand_type == RegisterOperand) {} \
  else \
    FOR_ALL_BASES(reg) \
      for (Scale scale = by_one;  \
                 scale <= (reg == no_reg ? by_one : by_eight); \
       ++*(int*)&scale) \
         if (reg == esp) {} else
     
# define FOR_ALL_RM32(base, disp, type, index, scale) \
  FOR_ALL_INTERESTING_OPERAND_TYPES(type) \
    FOR_ALL_INDICES(index, scale, type) \
      FOR_ALL_DISPS(disp, dispi, type) \
        FOR_ALL_BASES(base) \
          if (base == no_reg  &&  disp == 0  &&  index == no_reg) {} \
          else
    
# define FOR_ALL_IMM32s(imm32, type) \
  FOR_ALL_INTERESTING_OPERAND_TYPES(type) \
    FOR_ALL_DISPS(imm32, imm32i, type)
  
# define FOR_ALL_BIT_INDICES(x) \
  for (int x = 0;  x < 32;  x += 31)
  
void Assembler::generate_test_instructions() {
  tally();

  leal(esp, -8, NumberOperand, esp);  tally();
  
  enter(0);  tally();
  leave();   tally();
  ret();     tally();
  ret(16);   tally();
  ret(252);  tally();
  cwde();    tally();
  cld();     tally();
  clc();     tally();
  cmc();     tally();
  hlt();     tally();
  nop();     tally();
  stc();     tally();
  std();     tally();
  ud2();     tally();

  gen_movccs_setccs_movsxs_movzxss();
  gen_jmps();
  gen_imm32_only();
  gen_imm_reg();
  gen_rm();
  gen_imuls();
  gen_shifts();
}    


void Assembler::gen_movccs_setccs_movsxs_movzxss() {
  FOR_ALL_RM32(b, d, t, i, s) {
    seta(b, d, t, i, s);          tally();
    FOR_ALL_REGS(dst) {
      cmova(b, d, t, i, s, dst);  tally();

      movsbl(b, d, t, i, s, dst);  tally();
      movzbl(b, d, t, i, s, dst);  tally();
      movswl(b, d, t, i, s, dst);  tally();
      movzwl(b, d, t, i, s, dst);  tally();
    }
  }
  cmova( ebx, eax);  tally();
  cmovae(ecx, 17, NumberOperand, edx);  tally();
  cmovb( ebx, eax);  tally();
  cmovbe(ebx, eax);  tally();
  cmovc( ebx, eax);  tally();
  cmove( ebx, eax);  tally();
  cmovg( ebx, eax);  tally();
  cmovge(ebx, eax);  tally();
  cmovl(ebx, eax);  tally();
  cmovle(ebx, eax);  tally();
  cmovna(ebx, eax);  tally();
  cmovnae(ebx, eax);  tally();
  cmovnb(ebx, eax);  tally();
  cmovnbe(ebx, eax);  tally();
  cmovnc(ebx, eax);  tally();
  cmovne(ebx, eax);  tally();
  cmovng(ebx, eax);  tally();
  cmovnge(ebx, eax);  tally();
  cmovnl(ebx, eax);  tally();
  cmovnle(ebx, eax);  tally();
  cmovno(ebx, eax);  tally();
  cmovnp(ebx, eax);  tally();
  cmovns(ebx, eax);  tally();
  cmovnz(ebx, eax);  tally();
  cmovo(ebx, eax);  tally();
  cmovp(ebx, eax);  tally();
  // not in gcc cmovpe(ebx, eax);  tally();
  // not in gcc cmovpo(ebx, eax);  tally();
  cmovs(ebx, eax);  tally();
  // not in gcc cmovz(ebx, eax);  tally();

  seta( ebx);  tally();
  setae(edx);  tally();
  setb( ebx);  tally();
  setbe(ebx);  tally();
  setc( ebx);  tally();
  sete( ebx);  tally();
  setg( ebx);  tally();
  setge(ebx);  tally();
  setl( ebx);  tally();
  setle(ebx);  tally();
  setna(ebx);  tally();
  setnae(ebx);  tally();
  setnb(ebx);  tally();
  setnbe(ebx);  tally();
  setnc(ebx);  tally();
  setne(ebx);  tally();
  setng(ebx);  tally();
  setnge(ebx);  tally();
  setnl(ebx);  tally();
  setnle(ebx);  tally();
  setno(ebx);  tally();
  setnp(ebx);  tally();
  setns(ebx);  tally();
  setnz(ebx);  tally();
  seto(ebx);  tally();
  setp(ebx);  tally();
  // not in gcc setpe(ebx);  tally();
  // not in gcc setpo(ebx);  tally();
  sets(ebx);  tally();
  // not in gcc setz(ebx);  tally();
}


void Assembler::gen_jmps() {
    jecxz(int32(instsEnd),        NumberOperand);  tally(); 
    jecxz(int32(instsEnd) + 100,  NumberOperand);  tally(); 
    jecxz(int32(instsEnd) - 100,  NumberOperand);  tally();

    loop(  int32(instsEnd),        NumberOperand);  tally(); 
    loope( int32(instsEnd) + 100,  NumberOperand);  tally(); 
    loopne(int32(instsEnd) - 100,  NumberOperand);  tally();

# define T(name) \
    name(                    0,  NumberOperand);  tally(); \
    name(int32(instsEnd),        NumberOperand);  tally(); \
    name(int32(instsEnd) + 100,  NumberOperand);  tally(); \
    name(int32(instsEnd) - 100,  NumberOperand);  tally();
    
  T(ja) T(jae) T(jb) T(jbe) T(jc) T(je) T(jg) T(jge) T(jl) T(jle) T(jna) T(jnae) T(jnb) T(jnbe) T(jnc)
  T(jne) T(jng) T(jnge) T(jnl) T(jnle) T(jno) T(jnp) T(jns) T(jnz) T(jo) T(jp) T(jpe) T(jpo) T(js) T(jz)

  T(jmp)
  T(call)
  # undef T
}  


void Assembler::gen_imm32_only() {
  FOR_ALL_INTERESTING_OPERAND_TYPES(type) \
    FOR_ALL_DISPS(disp, dispi, type) { \
      pushl( disp, type);  tally();
    }        
}


void Assembler::gen_imm_reg() {  
  FOR_ALL_REGS(reg) {
    bswap(reg);  tally();
    
    FOR_ALL_IMM8s(imm8) {
      addb(  imm8,  reg);  tally(); 
      adcb(  imm8,  reg);  tally(); 
      subb(  imm8,  reg);  tally();
      sbbb(  imm8,  reg);  tally();
      cmpb(  imm8,  reg);  tally();
      andb(  imm8,  reg);  tally();
       orb(  imm8,  reg);  tally();
      xorb(  imm8,  reg);  tally();
      testb( imm8,  reg);  tally();
      movb(  imm8,  reg);  tally();
    }
    FOR_ALL_IMM32s(imm32, imm_type) {
      addl(  imm32,  imm_type, reg);  tally(); 
      adcl(  imm32,  imm_type, reg);  tally(); 
      subl(  imm32,  imm_type, reg);  tally();
      sbbl(  imm32,  imm_type, reg);  tally();
      cmpl(  imm32,  imm_type, reg);  tally();
      andl(  imm32,  imm_type, reg);  tally();
       orl(  imm32,  imm_type, reg);  tally();
      xorl(  imm32,  imm_type, reg);  tally();
      testl( imm32,  imm_type, reg);  tally();
      movl(  imm32,  imm_type, reg);  tally();
    }
  }
}

void Assembler::gen_rm() {    
  FOR_ALL_RM32(     base, disp, type, index, scale) {
    decb(           base, disp, type, index, scale);  tally();
    decl(           base, disp, type, index, scale);  tally();
    incb(           base, disp, type, index, scale);  tally();
    incl(           base, disp, type, index, scale);  tally();
    divb(           base, disp, type, index, scale);  tally();
    divl(           base, disp, type, index, scale);  tally();
    idivb(          base, disp, type, index, scale);  tally();
    idivl(          base, disp, type, index, scale);  tally();
    mulb(           base, disp, type, index, scale);  tally();
    mull(           base, disp, type, index, scale);  tally();
    negb(           base, disp, type, index, scale);  tally();
    negl(           base, disp, type, index, scale);  tally();
    notb(           base, disp, type, index, scale);  tally();
    notl(           base, disp, type, index, scale);  tally();
    pushl(          base, disp, type, index, scale);  tally();
    popl(           base, disp, type, index, scale);  tally();
    call(           base, disp, type, index, scale);  tally();
    jmp(            base, disp, type, index, scale);  tally();
    
    gen_imm8_rm8(   base, disp, type, index, scale);
    gen_imm32_rm32( base, disp, type, index, scale);
    gen_reg_rm(     base, disp, type, index, scale);
    gen_bit_rm(     base, disp, type, index, scale);
  }
}
  
  
void Assembler::gen_imm8_rm8( Location base, int32 disp, OperandType type, Location index, Scale scale) {
  FOR_ALL_IMM8s(imm8) {
    addb(  imm8,  base, disp, type, index, scale);  tally(); 
    adcb(  imm8,  base, disp, type, index, scale);  tally(); 
    subb(  imm8,  base, disp, type, index, scale);  tally();
    sbbb(  imm8,  base, disp, type, index, scale);  tally();
    cmpb(  imm8,  base, disp, type, index, scale);  tally();
    andb(  imm8,  base, disp, type, index, scale);  tally();
     orb(  imm8,  base, disp, type, index, scale);  tally();
    xorb(  imm8,  base, disp, type, index, scale);  tally();
    testb( imm8,  base, disp, type, index, scale);  tally();
    movb(  imm8,  base, disp, type, index, scale);  tally();
  }
}


void Assembler::gen_imm32_rm32( Location base, int32 disp, OperandType type, Location index, Scale scale) {
  FOR_ALL_IMM32s(imm32, imm_type) {
    addl(  imm32,  imm_type, base, disp, type, index, scale);  tally(); 
    adcl(  imm32,  imm_type, base, disp, type, index, scale);  tally(); 
    subl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
    sbbl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
    cmpl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
    andl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
     orl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
    xorl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
    testl( imm32,  imm_type, base, disp, type, index, scale);  tally();
    movl(  imm32,  imm_type, base, disp, type, index, scale);  tally();
  }
}


void Assembler::gen_reg_rm(Location base, int32 disp, OperandType type, Location index, Scale scale) {    
  FOR_ALL_REGS(reg) {
    addb(  reg,  base, disp, type, index, scale);  tally(); 
    adcb(  reg,  base, disp, type, index, scale);  tally(); 
    subb(  reg,  base, disp, type, index, scale);  tally();
    sbbb(  reg,  base, disp, type, index, scale);  tally();
    cmpb(  reg,  base, disp, type, index, scale);  tally();
    andb(  reg,  base, disp, type, index, scale);  tally();
     orb(  reg,  base, disp, type, index, scale);  tally();
    xorb(  reg,  base, disp, type, index, scale);  tally();
    testb( reg,  base, disp, type, index, scale);  tally();
    movb(  reg,  base, disp, type, index, scale);  tally();
    
    addl(  reg,  base, disp, type, index, scale);  tally(); 
    adcl(  reg,  base, disp, type, index, scale);  tally(); 
    subl(  reg,  base, disp, type, index, scale);  tally();
    sbbl(  reg,  base, disp, type, index, scale);  tally();
    cmpl(  reg,  base, disp, type, index, scale);  tally();
    andl(  reg,  base, disp, type, index, scale);  tally();
     orl(  reg,  base, disp, type, index, scale);  tally();
    xorl(  reg,  base, disp, type, index, scale);  tally();
    testl( reg,  base, disp, type, index, scale);  tally();
    movl(  reg,  base, disp, type, index, scale);  tally();
    
    addb(   base, disp, type, index, scale, reg);  tally();
    adcb(   base, disp, type, index, scale, reg);  tally();
    subb(   base, disp, type, index, scale, reg);  tally();
    sbbb(   base, disp, type, index, scale, reg);  tally();
    cmpb(   base, disp, type, index, scale, reg);  tally();
    andb(   base, disp, type, index, scale, reg);  tally();
     orb(   base, disp, type, index, scale, reg);  tally();
    xorb(   base, disp, type, index, scale, reg);  tally();
    movb(   base, disp, type, index, scale, reg);  tally();
    
    addl(   base, disp, type, index, scale, reg);  tally();
    adcl(   base, disp, type, index, scale, reg);  tally();
    subl(   base, disp, type, index, scale, reg);  tally();
    sbbl(   base, disp, type, index, scale, reg);  tally();
    cmpl(   base, disp, type, index, scale, reg);  tally();
    andl(   base, disp, type, index, scale, reg);  tally();
     orl(   base, disp, type, index, scale, reg);  tally();
    xorl(   base, disp, type, index, scale, reg);  tally();
    movl(   base, disp, type, index, scale, reg);  tally();
    
    leal(   base, disp, type, index, scale, reg);  tally();

    btl(    reg, base, disp, type, index, scale);  tally();
    btrl(   reg, base, disp, type, index, scale);  tally();
    btsl(   reg, base, disp, type, index, scale);  tally();
    btcl(   reg, base, disp, type, index, scale);  tally();
  }
}    


void Assembler::gen_bit_rm(Location base, int32 disp, OperandType type, Location index, Scale scale) {    
  FOR_ALL_BIT_INDICES(b) {
    btl(   b, base, disp, type, index, scale);  tally();
    btrl(  b, base, disp, type, index, scale);  tally();
    btsl(  b, base, disp, type, index, scale);  tally();
    btcl(  b, base, disp, type, index, scale);  tally();
  }
}


void Assembler::gen_imuls() {
  FOR_ALL_RM32(base, disp, type, index, scale) {
    imulb(base, disp, type, index, scale);  tally();
    imull(base, disp, type, index, scale);  tally();
    FOR_ALL_REGS(dst) {
      imull(base, disp, type, index, scale, dst);  tally();
    }
  }
  FOR_ALL_IMM32s(imm32, it)
    FOR_ALL_RM32(base, disp, type, index, scale)
      FOR_ALL_REGS(r) {
        imull(imm32,  it, base, disp, type, index, scale, r);  tally();
      }
}      


void Assembler::gen_shifts() {
  FOR_ALL_RM32(base, disp, type, index, scale) {
    salb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    sarb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    shlb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    shrb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    
    rclb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    rcrb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    rolb(shift_by_count_reg, base, disp, type, index, scale);  tally();
    rorb(shift_by_count_reg, base, disp, type, index, scale);  tally();
        
    sall(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    sarl(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    shll(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    shrl(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    
    rcll(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    rcrl(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    roll(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    rorl(shift_by_count_reg, NumberOperand, base, disp, type, index, scale);  tally();
    
    FOR_ALL_BIT_INDICES(imm8) {
      salb(imm8, base, disp, type, index, scale);  tally();
      sarb(imm8, base, disp, type, index, scale);  tally();
      shlb(imm8, base, disp, type, index, scale);  tally();
      shrb(imm8, base, disp, type, index, scale);  tally();
      
      rclb(imm8, base, disp, type, index, scale);  tally();
      rcrb(imm8, base, disp, type, index, scale);  tally();
      rolb(imm8, base, disp, type, index, scale);  tally();
      rorb(imm8, base, disp, type, index, scale);  tally();
          
      sall(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      sarl(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      shll(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      shrl(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      
      rcll(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      rcrl(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      roll(imm8, NumberOperand, base, disp, type, index, scale);  tally();
      rorl(imm8, NumberOperand, base, disp, type, index, scale);  tally();
    }
  }
}
      

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

  

# pragma implementation "fields_i386.hh"

# include "_fields_i386.cpp.incl"


pc_t  get_target_of_branch_instruction(inst_t* instp) {
  // must be long disp!
  // instp points to start of displacements
  return (char*) (*((int32*)instp) + instp + sizeof(int32));
}


pc_t get_target_of_C_call_site(inst_t* instp) {
  return get_target_of_branch_instruction(instp);
}


pc_t get_target_of_Self_call_site(inst_t* instp) {
  // On i386, instp is pointer to word displacement in call instruction
  return (char*)instp + oopSize + *(int32*)instp;
}  


void set_target_of_Self_call_site(inst_t* instp, void* target) {
  // On i386, instp is pointer to word displacement in call instruction
  *(int32*)instp = (inst_t*)target - ((inst_t*)instp + oopSize);
}


char* address_of_overwritten_NIC_save_instruction(int32* orig_save_addr) {
  fatal("unused for Intel");
  return 0;
}


void check_branch_relocation( void* fromArg, void* toArg, int32 countArg) {
  fatal("unused for Intel");
}


void set_space_reserved_by_enter_instruction( pc_t past_enter, int32 word_count ) {
  u_short s = word_count * BytesPerWord;
  assert((int32)s == word_count * BytesPerWord, "too big");
  *(u_short*)(past_enter - 3) = s;
}  

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "diDesc_i386.hh"
# include "_diDesc_i386.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

pc_t DIDesc::jump_addr() {
  return (pc_t)get_target_of_Self_call_site((inst_t*)jump_addr_addr());
}


void DIDesc::set_jump_addr(pc_t insts) {
  pc_t* addr = jump_addr_addr();
  set_target_of_Self_call_site((inst_t*)addr, insts);
}
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

  
# pragma implementation "cacheStub_i386.hh"
# pragma implementation "cacheStub_inline_i386.hh"
# include "_cacheStub_i386.cpp.incl"
  
# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)


// -no SIC type tests for now


Label* CacheStub::br_if_not_smi() {
  Label* not_smi = new Label(a->printing);
  assert(Int_Tag == 0, "???");
  a->testl( Tag_Mask, NumberOperand, Temp1);
  a->jnz(not_smi);
  return not_smi;
}

Label* CacheStub::br_if_not_float() {
  assert((Float_Tag & Mem_Tag) == 0  &&  (Float_Tag & Int_Tag) == 0, "???");
  Label* not_float = new Label(a->printing);
  a->btl( Float_Tag_bit_i386, Temp1);
  // if bit set must be hit, otherwise is mark
  a->jnc(not_float);
  return not_float;
}


void CacheStub::add_case(nmethod* nm, CountStub* stArg, pc_t addr) {
  n[newMethods] = nm;  st[newMethods] = stArg;  ++ newMethods;
  jump(addr);
}


Label* CacheStub::prologue(bool immediateOnly) {
  assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
  Label* miss = NULL;
  Label* loadMapAfterHandlingImmediates = NULL;
  pc_t floatAddr, smiAddr;
  computeJumpAddr(nsmi, theSendDesc,   stsmi,   smiAddr);
  computeJumpAddr(nfloat, theSendDesc, stfloat, floatAddr);

  // put rcvr in Temp1
  a->movl(esp, leaf_rcvr_offset * oopSize, NumberOperand, Temp1);
  // Load map if needed
  if (immediateOnly)
    ;
  else {
    a->btl(Mem_Tag_bit_i386, Temp1);
    loadMapAfterHandlingImmediates = new Label(a->printing);
    a->jc(loadMapAfterHandlingImmediates); // jump if memOop    
  }

  if (nsmi  &&  nfloat) {
    Label* not_smi = br_if_not_smi();
    add_case(nsmi, stsmi, smiAddr);
    
    not_smi->define();
    // if immediateOnly = false and we're down here, we've already tested for memOop
    // and hence there's no need to test for the float tag.
    if (immediateOnly)
      miss = br_if_not_float(); // br if mem
    else
      miss = NULL;
    add_case(nfloat, stfloat, floatAddr);
  }
  else if (nsmi)   { miss = br_if_not_smi();    add_case(nsmi,   stsmi,     smiAddr);  }
  else if (nfloat) { miss = br_if_not_float();  add_case(nfloat, stfloat, floatAddr);  }
  else {
    // tested for Oop above, so rcvr is int or float,
    // but there are no smi or float cases, so must be a miss
    miss = new Label(a->printing);
    a->jmp(miss);
  }
  if (loadMapAfterHandlingImmediates) {
    loadMapAfterHandlingImmediates->define();
    // CacheStub::test expects Temp1 to contain the receiver's map
    a->movl(Temp1, map_offset(), NumberOperand, Temp1);
  }
  return miss;
}


Label* CacheStub::test(oop map, pc_t addr, Label* prev) {
  if (prev)
    prev->define();
  a->cmpl((int32)map, OopOperand, Temp1);
  Label* next_test = new Label(a->printing);
  a->jne(next_test);
  jump(addr);
  return next_test;
}

void CacheStub::finish(Label* miss, Label* prev) {
  if (prev)
    prev->define();
  if (miss)
    miss->define();
  a->jmp((int32)theSendDesc->lookupRoutine(), PVMAddressOperand);
}

  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "sendDesc_i386.hh"
# include "_sendDesc_i386.cpp.incl"

# if defined(FAST_COMPILER) || defined(SIC_COMPILER)

extern "C" { char *firstSelfFrame_nmlns; }; // for assertion checking

void sendDesc::init_platform() {
  assert( (int(firstSelfFrame_nmlns) & 0x3) == 0, "sendDesc alignment incorrect");
}

 
char* sendDesc::jump_addr() {
  inst_t* c = (inst_t*)jump_addr_addr();
  return get_target_of_Self_call_site(c);
}

void sendDesc::set_jump_addr(char* t) {
  inst_t* jaa = (inst_t*)jump_addr_addr();
  set_target_of_Self_call_site(jaa, (void*)t);
}


void printMask(RegisterString mask) {
  assert(mask == 0, "unused for Intel");
}


# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "genHelper_i386.hh"

# include "_genHelper_i386.cpp.incl"

# if defined(SIC_COMPILER)


  fint SICGenHelper::spOffset(Location l) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, l);
    // had better be used for active frame, not a saved one,
    // because sp of saved frame is two words lower! -- dmu 5/06
    return b == esp  ?  d  :  d  +  (theSIC->frameSize() - linkage_area_size) * oopSize;
  }

  fint SICGenHelper::spOffset(Location l, nmethod* nm) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, l);
    // and this one is for a saved frame!
    // So, the "SP" is really going to be the ebp, cause that's what blocks store -- dmu 5/06

    // Don't think this should ever happen cause we doin't up-level access outgoing args. -- dmu 5/06
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions  &&  b != ebp)  warning("untested");
    # endif
    return b == ebp  ?  d  :  d - (nm->frameSize() - linkage_area_size) * oopSize;
  }

  // Warning: this clobbers the count register
  void SICGenHelper::jumpTo(void* target, Location reg, Location link) {
    fatal("not used for Intel");
  }

  void SICGenHelper::genCountCode(int32* counter) {
    a->Comment("count # calls");
    a->incl(no_reg, (int32)counter, VMAddressOperand);
  }


  Location SICGenHelper::loadImmediateOop(ConstPReg* r, Location dest, bool mustMove) {
    // load oop from ConstPR; return location containing the oop
    if (r->loc == UnAllocated) {
      loadImmediateOop(r->constant, dest);
      return dest;
    } else if (mustMove) {
      theAssembler->Untested("loadImmediateOop");
      moveRegToReg(r->loc, dest);
      return dest;
    }
    else
      return r->loc;
  }
  
  void SICGenHelper::loadImmediateOop(oop p, Location dest, bool isInt) {
    Unused(isInt); // load_immediate does the right thing wrt integer oops
    assert(isRegister(dest), "must be a register");
    a->movl((int32)p, OopOperand, dest);
  }

  void SICGenHelper::load(Location src, fint srcOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->movl(src, srcOffset, NumberOperand, dest);
  }

  void SICGenHelper::store(Location src, fint dstOffset, Location dest) {
    assert(isRegister(src) && isRegister(dest), "not a register");
    a->movl(src, dest, dstOffset, NumberOperand);
  }

  void SICGenHelper::moveRegToReg(Location srcReg, Location destReg) {
    assert(isRegister(srcReg) && isRegister(destReg), "not a register");
    a->movl(srcReg, destReg);
  }

  // must be a VMAddressOperand operand
  void SICGenHelper::setToZeroA(void* addr, Location tempReg) {
    theAssembler->movl(0, NumberOperand, no_reg, (int32)addr, VMAddressOperand);
  }

  void SICGenHelper::setToZero(Location dest) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, dest);
    a->movl(0, NumberOperand, b, d, t);
  }


# ifdef UNUSED
  void SICGenHelper::checkRecompilation(fint countID) {
    fatal("Unused Intel");
  }
# endif // UNUSED

    

  void SICGenHelper::smiOop_prologue(char* missHandler) {
    if (SICCountIntTagTests) a->markTagTest(1);

    a->testl( Tag_Mask, NumberOperand, esp, leaf_rcvr_offset*oopSize, NumberOperand);
    a->jne((int32)missHandler, PVMAddressOperand);

    assert(!SICCountTypeTests, "should have called a->endTypeTest() on failure");
  }


  void SICGenHelper::floatOop_prologue(char* missHandler) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, LReceiverReg);

    if (SICCountIntTagTests) a->markTagTest(2);
    
    a->btl( Float_Tag_bit_i386, esp, leaf_rcvr_offset*oopSize, NumberOperand);
    // if bit set must be hit, otherwise is mark
    a->jnc( (int32)missHandler, PVMAddressOperand);

    assert(!SICCountTypeTests, "should have called a->endTypeTest() on failure");
  }


  void SICGenHelper::memOop_prologue(mapOop receiverMapOop,
                                     char* missHandler) {

    a->movl(esp, leaf_rcvr_offset*oopSize, NumberOperand, Temp1);
    a->btl(Mem_Tag_bit_i386, Temp1);
    // if bit set must be hit, otherwise is mark
    a->jnc( (int32)missHandler, PVMAddressOperand);

    // check_map:
    a->movl( Temp1, map_offset(), NumberOperand, Temp2);
    a->cmpl( (int32)receiverMapOop, OopOperand, Temp2);
    a->jne((int32)missHandler, PVMAddressOperand);

    assert(!SICCountTypeTests, "should have called a->endTypeTest() on failure");
  }



  void SICGenHelper::checkOop(Label& general, oop what, Location loc_to_check) {
    // test for inline cache hit (selector, delegatee)
    moveLocToReg(loc_to_check, Temp2);
    a->cmpl((int32)what, OopOperand, Temp2);
    Unused(general);
    a->jne(int32(SendMessage_stub), PVMAddressOperand);
  }
  
  


  fint SICGenHelper::verifyParents(objectLookupTarget* target,
                                   Location t,
                                   fint count) {
    assert(target->links != 0,  "expecting an assignable parent link");
    bool isFirst = true;
    for ( assignableSlotLink* l = target->links;  
          l != 0;
          l = l->next,  isFirst = false) {
      
      if (!isFirst) {
        // if multiple dynamic parents, reload slot holder before looping (HACK!)
        t = loadPath(Temp1, target, LReceiverReg);
      }
      
      // load assignable parent slot value
      a->movl(t, smiOop(l->slot->data)->byte_count() - Mem_Tag, NumberOperand, Temp1);
      verifyOneImmediateParent(l, Temp1, Temp2, count);
      ++count;

      if (l->target->links) count = verifyParents(l->target, Temp1, count);
    }
    return count;
  }
  
  

  void SICGenHelper::verifyOneImmediateParent(assignableSlotLink* l, Location parentOopReg, Location scratchReg, fint count) {
    Label* ok = new Label(a->printing);
    
    if (l->target->value_constrained)  verifyConstrainedOopOfParent(l->target->obj,        parentOopReg, ok);
    else                               verifyMapOfParent(           l->target->obj->map(), parentOopReg, scratchReg, ok);

    // This will be backpatched to call an nmethod, so need to leave incoming link alone.
    // Pass a link to the branch and nmln in the DILinkReg.
    // See codeGen_i386.cpp
    
    a->movl(count, NumberOperand, DICountReg);  // count of parents verified
    
    // must align nmln to follow
    // There is a call coming, each 5 bytes, so want pc + 5 to be 0 mod 4.
    // See align * in asmDefs_gcc_i386.[sS]
    // get ic value to pass in, cannot use call cause of backpatching, etc.
    Label next;
    a->call(&next);
    next.define(); 

    // two tasks: compute amount to add to savedPC so it simulates a call: must point to after the jmp below
    // Also, just add enough nops so that nmln after call is word-alligned
    
    fint bytes_from_here_to_after_jmp_before_alignment = 1 /* popl */ + 3 /* addl */ + 5 /* jmp */;
    int32 here = a->offset();
    fint word_fraction_from_here_to_after_jmp_before_alignment = (here + bytes_from_here_to_after_jmp_before_alignment ) & 3;    
    fint num_nops = (BytesPerWord - word_fraction_from_here_to_after_jmp_before_alignment) & (BytesPerWord-1);
    fint bytes_from_here_to_after_jmp = bytes_from_here_to_after_jmp_before_alignment + num_nops;
    
    for (fint i = 0;  i < num_nops; ++i) a->nop();
    

    a->popl(DIInlineCacheReg); // 1 byte, prepare to calculate IC addr below
    a->addl(bytes_from_here_to_after_jmp, NumberOperand, DIInlineCacheReg);  // 3 bytes, finish calc IC addr below
    // following must be parsable to set_target_of_Self_call_site
    a->jmp( (int32)SendDIMessage_stub, DIVMAddressOperand);
    assert( a->offset() == here + bytes_from_here_to_after_jmp, "checking");
    assert((a->offset() & (BytesPerWord-1)) == 0, "must be aligned");
    a->Data(0);                                // first  part of DI nmln
    a->Data(0);                                // second part of DI nmln
    
    a->hlt();
          
    ok->define();
  }



  void SICGenHelper::verifyConstrainedOopOfParent(oop targetOop, 
                                                  Location parentOopReg, 
                                                  Label* ok) {
    // constraint for a particular oop (ambiguity resolution)
    a->cmpl((int32)targetOop, OopOperand, parentOopReg);       // compare values
    a->je(ok);              // branch if value OK
  }

  
  // Given: map to look for, obj already in parentOopReg, scratch reg regForMap
  // test for map, fall through on miss, goto label OK if hit.
  
  void SICGenHelper::verifyMapOfParent(Map* targetMap, Location parentOopReg, Location regForMap, Label* ok) {
    if (targetMap == Memory->smi_map) {
    a->Untested("verifyMapOfParent");
      a->testl(Tag_Mask, NumberOperand, parentOopReg);        // test for integer tag
      a->je( ok );                                     // branch if parent is integer
    }
    else if (targetMap == Memory->float_map) {
    a->Untested("verifyMapOfParent");
      a->btl( Float_Tag_bit_i386, parentOopReg);
      // if bit set must be hit, otherwise is mark
      a->jc(ok);             // branch if parent is a float
    } 
    else {
      Label miss;

      a->btl(Mem_Tag_bit_i386, parentOopReg);
      // if bit set must be hit, otherwise is mark
      a->jnc(&miss);                          // branch if parent is not mem oop

      a->movl(parentOopReg, map_offset(), NumberOperand, regForMap);    // load receiver map
      a->cmpl((int32)targetMap->enclosing_mapOop(), OopOperand, regForMap); // cmp to map constraint
      a->je(ok);               // correct

      miss.define();
    }
  }
  
  
  void SICGenHelper::moveToExactlyThisReg(PReg* pr, Location reg) {
    Location r = moveToReg(pr, reg);
    if (r != reg) a->movl(r, reg);
  }
    

# endif // SIC_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "node_i386.hh"

# include "_node_i386.cpp.incl"

# if defined(SIC_COMPILER)

  void BasicNode::genBranch() {
    Label* l2 = new Label(theAssembler->printing);
    theAssembler->jmp(l2);
    l = l->unify(l2);
  }


  void BasicNode::restoreFrameAndReturn(bool haveStackFrame, fint offset) {
    theAssembler->Comment("restoreFrameAndReturn");

    if (haveStackFrame)
      theAssembler->leave();

    if (offset != 0)
      theAssembler->addl(offset, NumberOperand, esp, leaf_pc_offset*oopSize, NumberOperand);

    theAssembler->ret();
  }


  void PrologueNode::actuallyCreateStackFrame() {
    // The next three instructions are supposed to be faster than this:
    //   theAssembler->enter( (thisFrameSize - linkage_area_size) * oopSize);
    //    -- dmu 5/06
    theAssembler->pushl(ebp);
    theAssembler->movl(esp, ebp);
    assert((thisFrameSize & (frame_word_alignment - 1)) == 0, "frame size check");
    theAssembler->subl((thisFrameSize - linkage_area_size) * oopSize, NumberOperand, esp);

    theSIC->_frameCreationOffset = theAssembler->offset();
  }


  void PrologueNode::clearStackLocations() {
    theAssembler->Comment("clear stack locations");
    // do not have to clear outgoing args
    for ( fint i = sizeof(RegisterString) * BitsPerByte;  i < theSIC->number_of_memory_locals();  ++i) {
      Location r;  int32 d;  OperandType t;
      reg_disp_type_of_loc(&r, &d, &t, StackLocation_for_index(i));
      theAssembler->movl(0, NumberOperand, r, d, t);
    }
  }


  void PrologueNode::createStackFrame() {
    assert(haveStackFrame(), "shouldn't be creating a stack frame");

    thisFrameSize = theSIC->frameSize();

    actuallyCreateStackFrame();
    clearStackLocations();
  }

  void PrologueNode::prePrologue() { }

  void PrologueNode::postPrologue() { }

  void LoadIntNode::gen() {
    BasicNode::gen();
    if (isRegister(_dest->loc)) {
      theAssembler->movl((int32)value, NumberOperand, _dest->loc);
    }
    else {
      theAssembler->Untested("loadIntNode");
      theAssembler->movl((int32)value, NumberOperand, Temp2);
      Location b;  int32 d;  OperandType t;
      reg_disp_type_of_loc(&b, &d, &t, _dest->loc);
      theAssembler->movl(Temp2, b, d, t);
    }
  }
  

  void StoreOffsetNode::gen() {
    BasicNode::gen();
    Location dstBase = genHelper->moveToReg(base, Temp1);
    Location t = Temp2;
    if (_src->isConstPReg()) {
      // store constant
      ConstPReg* value = (ConstPReg*)_src;
      oop p = value->constant;
      // don't need to check-store if oop is old - old objs will never become
      // new again
      needCheckStore = needCheckStore && p->is_new(); // ints/floats aren't new
      assert(dstBase != t, "must be different");
      theAssembler->movl((int32)p, OopOperand, dstBase, offset, NumberOperand);
    }
    else if (isRegister(_src->loc)) {
      theAssembler->movl(_src->loc, dstBase, offset, NumberOperand);
    }
    else {
      Location b;  int32 d;  OperandType tt;
      reg_disp_type_of_loc(&b, &d, &tt, _src->loc);
      theAssembler->movl(b, d, tt, t);
      theAssembler->movl(t, dstBase, offset, NumberOperand);
    }
    if (needCheckStore) {
      theAssembler->Comment("record store");
      // do a check-store
      assert(isRegister(dstBase), "base reg of check_store must be in a register");

      if (offset > card_size || !AllowOffsetCheckStores) {
        // use slow check-store sequence
        // (marked card may be off by one, but not more)
        theAssembler->leal(dstBase, offset, NumberOperand, Temp1);
        dstBase = Temp1;
      }

      if (dstBase != Temp1) theAssembler->movl(dstBase, Temp1);
      theAssembler->shrl(card_shift, NumberOperand, Temp1);  // shift target addr

      if (!UseByteMapBaseReg) {
        theAssembler->addl(no_reg, (int32)&byte_map_base, VMAddressOperand, Temp1);
        theAssembler->movb(0, Temp1, 0, VMAddressOperand);
      }
      else 
        theAssembler->movb(0, ByteMapBaseReg, 0, NumberOperand, Temp1, Assembler::by_one);
    }
  }
  

  void AssignNode::genOop() {
    ConstPReg* value = (ConstPReg*)_src;
    Location src = value->loc;
    if (src != UnAllocated) {
      // value is already in src register
      theAssembler->Untested("genOop");
      genHelper->moveRegToLoc(src, _dest->loc);
    }
    else if (isRegister(_dest->loc)) {
      genHelper->loadImmediateOop(value->constant, _dest->loc);
    }
    else {
      oop c = value->constant;
      Location b;  int32 d;  OperandType t;
      reg_disp_type_of_loc(&b, &d, &t, _dest->loc);
      if (b == ebp && d >= 0)
        theAssembler->Untested("genOop");
      theAssembler->movl((int32)c, OopOperand, b, d, t);
    }
  }

  
  void CallNode::nlrCode() {
    theAssembler->Comment("nlrCode");
    if (nlrPoint()) {
      // branch to NLR code
      Label* l_ = new Label(theAssembler->printing);
      theAssembler->jmp(l_);
      nlrPoint()->l = l_->unify(nlrPoint()->l);
    } 
    else {
      if (!theSIC->nlrLabel)
        theSIC->nlrLabel = new Label(theAssembler->printing);
      theAssembler->jmp(theSIC->nlrLabel);
    }
  }
  

  void SendNode::gen() {
    BasicNode::gen();
    assert(bci() != IllegalBCI, "should have legal bci");
    genPcDesc();
    genBreakpointBeforeCall();
    // WARNING: following code sequence is known to get_target_of_Self_call_site() & set_target_of_Self_call_site()
    
    // must align nmln to follow
    // There are a call and two jumps coming, each 5 bytes, so want pc to be 1 mod 4.
    // See align * in asmDefs_gcc_i386.[sS]
    fint x = ((fint)theAssembler->offset() - 1) & 3;
    fint num_nops = (4 - x) & 3;
    for (fint i = 0;  i < num_nops; ++i) theAssembler->nop();

    theAssembler->call((int32)SendMessage_stub, BPVMAddressOperand);
    offset = theAssembler->offset();
    Label past_send_desc(theAssembler->printing);
    theAssembler->jmp(&past_send_desc);
    theAssembler->Data(mask());
    nlrCode();
    assert((theAssembler->offset() & Tag_Mask) == 0, "must be aligned");
    theAssembler->Zero();       // nmlns
    theAssembler->Zero();
    if (sel != badOop) {
      if (isPerformLookupType(l)) {
        assert_smi(sel, "should be an integer argcount");
        theAssembler->Data(smiOop(sel)->value());       // really arg count
      } else {
        assert_string(sel, "should be a string constant");
        theAssembler->Data(sel);                        // constant selector
      }
    }
    if ((l & UninlinableSendMask) == 0) theSIC->noInlinableSends = false;
    theAssembler->Data(l);
    verifySendInfo();
    if (del) {
      assert(needsDelegatee(l), "shouldn't have a delegatee");
      theAssembler->Data(del);
    }
    if (theSIC->nlrLabel && !theSIC->nlrLabel->isDefined()) {
      theAssembler->Untested("SendNode::gen");
      theSIC->nlrLabel->define();
      restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
    }
    past_send_desc.define();
  }
  

  void reload_ByteMapBaseReg(PrimDesc* pd) {
    if (UseByteMapBaseReg && pd->canScavenge()) {
      // any prim that can scavenge, may expand heap,so need to reload this reg
      theAssembler->movl(no_reg, (int32)&byte_map_base, VMAddressOperand, ByteMapBaseReg);
    }
  }


    // may need to call theSIC->allocateArgs in PrimNode::PrimNode, much like BlockCloneNode
  void PrimNode::gen() {
    BasicNode::gen();
    assert(bci() != IllegalBCI, "should have legal bci");
    if (pd->canWalkStack()) genPcDesc();

    theAssembler->call((int32)first_inst_addr(pd->fn()), PVMAddressOperand); // do the call

    // skip over inline cache:
    Label past_nlr(theAssembler->printing); 
    theAssembler->jmp(&past_nlr);  // skip over mask and nlr code

    if (pd->canScavenge() || pd->needsNLRCode())
      theAssembler->Data(mask());              // used register mask for GC
  
    if (pd->needsNLRCode()) {
      reload_ByteMapBaseReg(pd);
      nlrCode();
      if (theSIC->nlrLabel && !theSIC->nlrLabel->isDefined()) {
        theSIC->nlrLabel->define();
        restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
      }
    }
    past_nlr.define(); 
    reload_ByteMapBaseReg(pd);
  }


  void InterruptCheckNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->cmpl(no_reg, int32(&SPLimit), VMAddressOperand, esp); // test for stack overflow
    Label l_(theAssembler->printing);
    theAssembler->ja(&l_); // ok
    PrimNode::gen();
    l_.define();
  }

  void RestartNode::gen() {
    genPcDesc();
    theAssembler->Comment("stack overflow/interrupt check");
    theAssembler->cmpl(no_reg, int32(&SPLimit), VMAddressOperand, esp);
    Label* dest = new Label(theAssembler->printing);
    theAssembler->ja(dest);
    loopStart->l = loopStart->l->unify(dest);
    PrimNode::gen();
    theAssembler->jmp(loopStart->l);
  }

  void BlockCloneNode::genCall() {
    theAssembler->Comment("block clone");
    Location dest = block()->loc;

    genHelper->loadImmediateOop(block()->block, Temp1); // load block Oop
    theAssembler->movl(Temp1, esp, rcvr_offset * oopSize, NumberOperand);
    theAssembler->movl(ebp, esp, first_arg_offset * oopSize, NumberOperand);

    theAssembler->call( (int32)first_inst_addr(blockClone->fn()), PVMAddressOperand);
    assert(!blockClone->needsNLRCode(),  "need to rewrite this");
    genHelper->moveRegToLoc(ResultReg, dest);
  }
  
  void BlockCreateNode::gen() {
    BasicNode::gen();
    if (block()->primFailBlockScope) {
      // must generate block (in primitive fail branch)
      assert(!isMemoized(), "shouldn't be memoized");
      genCall();
    } else if (isMemoized()) {
      // test if already created
      theAssembler->Comment("test memoized block");
      Location t = genHelper->moveToReg(block(), Temp1);
      theAssembler->cmpl((int32)deadBlockPR->constant, OopOperand, Temp1);
      Label done;
      theAssembler->jne(&done); // optimize fast case, so predict-weird
      genCall();
      done.define();
    } else
      ; // block has already been created (by initial BlockClone node)
  }
  
  void NonLocalReturnNode::gen() {
    BasicNode::gen();
    restoreFrameAndReturn(true, sendDesc::non_local_return_offset);
  }


  void MethodReturnNode::gen() {
    BasicNode::gen();
    if (_src->isNoPReg()) {
      // control should never reach here; only happens after a non-lifo abort
      // i.e. a zapped block method. -- dmu 5/06
      theAssembler->hlt();
      return;
    }
    // move result to resultReg
    genHelper->moveToExactlyThisReg(_src, ResultReg);
    restoreFrameAndReturn(haveStackFrame, 0);
  }
  
  
  static void force_overflow(Node* failNode) {
    if (failNode == NULL)
      return;
    // The SIC uses the eq condition code bit to decide if this is overflow error
    // or operand type error, sigh. See sicPrimline.cpp.
    // So, force cond code to say equals here. -- dmu 5/06
    // If the failNode checked for overflow instead of eq, we could do this MUCH more directly
    // sicPrimline.cpp:295
    theAssembler->cmpl(eax, eax); 
    // And if the label does not exist yet, do this hooha so that it works anyway.
    Label* L = new Label(theAssembler->printing);
    theAssembler->jmp(L); // have to do it ones before the left shift
    failNode->l = L->unify(failNode->l);
  }
  
  
  static void check_overflow(Node* failNode) {
    if (failNode == NULL)
      return;
    Label ok(theAssembler->printing);
    theAssembler->jno(&ok);
    force_overflow(failNode);
    ok.define();
  }


  static void reverse_if_helpful(PReg*& sreg, PReg*& oper, ArithOpCode op, bool& srcIsInt, bool& operIsInt) {
   if (!sreg->isConstPReg())
     return;

   PReg* r; bool b;
    // try to reverse the sense of the operation
    switch (op) {
     case AddArithOp:       
     case AddCCArithOp:             
     case AndArithOp:               
     case AndCCArithOp:             
     case OrArithOp:                
     case OrCCArithOp:              
     case XOrArithOp:
     case TAndCCArithOp:
     case TOrCCArithOp:
     case TXorCCArithOp:
     case TAddCCArithOp:
     case TMulCCArithOp:
      // commutative operator, no problem
      r = sreg;  sreg = oper;  oper = r; 
      b = srcIsInt; srcIsInt = operIsInt;  operIsInt = b;
      break;
      
     // not commutative
     case SubArithOp:
     case SubCCArithOp:             
     case TSubCCArithOp:
     case TDivCCArithOp:
     case TModCCArithOp:
     case ArithmeticLeftShiftArithOp:
     case LogicalLeftShiftArithOp:
     case ArithmeticRightShiftArithOp:
     case LogicalRightShiftArithOp:
     case TALShiftCCArithOp:
     case TARShiftCCArithOp:
     case TLLShiftCCArithOp:
     case TLRShiftCCArithOp:
      break;
     default:       ShouldNotReachHere(); // unexpected arith type
    }
  }
  


  // ABI constraints:
  // add: none
  // sub: none
  // imul: none (use two addr)
  // idiv: dst must be eax (Temp1)
  // s[al][lr]: opn must be in Temp2
  // and, or, xor: none
  
  static Location arith_genHelper_move_src_to_reg(PReg* sreg, bool srcIsInt, PReg* oper, ArithOpCode op, Node* failNode) {
    assert(Temp3 == eax, "for idiv");
    assert(Temp2 == ecx, "for shifts");
    switch (op) {
     default: 
      assert(oper->loc != Temp1, "going to clobber it");
      return genHelper->moveToReg(sreg, Temp1);
      
     case DivArithOp:  case TDivCCArithOp:  case TModCCArithOp:
      assert(oper->loc != eax, "going to clobber it");
      genHelper->moveToExactlyThisReg(sreg, eax);
      // Set up edx:
      //  idiv instruction needs the high order 32-bits of 64-bit
      //  signed dividend in edx. -- dmu 5/06
      assert(oper->loc != edx && Temp4 == edx, "divide uses edx");
      theAssembler->movl(0, NumberOperand, edx);
      Label pos;
      theAssembler->testl(eax, eax);
      theAssembler->jge(&pos);
      theAssembler->notl(edx);
      pos.define();
      return eax;
    }
  }
  
  static Location arith_genHelper_move_oper_to_reg(Location& dst, PReg* oper, ArithOpCode op) {
    switch (op) {
     case DivArithOp:  case TDivCCArithOp:  case TModCCArithOp:
      assert(oper->loc != edx, "remainder, etc.");
      assert(Temp1 != edx  &&  Temp2 != edx, "");
      // Fall Through
     default: 
      return genHelper->moveToReg(oper, dst == Temp1 ? Temp2 : Temp1);
    
     case ArithmeticLeftShiftArithOp:   case LogicalLeftShiftArithOp:
     case ArithmeticRightShiftArithOp:  case LogicalRightShiftArithOp:
     case TALShiftCCArithOp:  case TLLShiftCCArithOp:  case TARShiftCCArithOp:  case TLRShiftCCArithOp:
      assert(ecx == Temp2, "???");
      if (dst == ecx) {
        theAssembler->Untested("moving dst->Temp1 for shift");
        theAssembler->movl(dst, Temp1);
        dst = Temp1;
      }
      genHelper->moveToExactlyThisReg(oper, ecx);
      return ecx;
    }
  }
  
  static Location arith_genHelper_do_immediate(Location dst, int32 opn, ArithOpCode op, bool ccOnly, Node* failNode) {
    Location t = dst == Temp1 ? Temp2 : Temp1;
    # define XX(op) case op: theAssembler->Untested("immed:" STR(op)); break;
    switch (op) {
      default: break;
      XX(NilArithOp) XX(AddArithOp) XX(SubArithOp) XX(MulArithOp) XX(DivArithOp) XX(AndArithOp) XX(OrArithOp)
      XX(XOrArithOp) XX(ArithmeticLeftShiftArithOp) XX(LogicalLeftShiftArithOp) XX(ArithmeticRightShiftArithOp)
      XX(LogicalRightShiftArithOp) XX(AddCCArithOp) XX(AndCCArithOp) XX(OrCCArithOp)
      XX(TALShiftCCArithOp) XX(TARShiftCCArithOp)
    }
    # undef XX
    switch (op) {
       case TAddCCArithOp:
       case AddCCArithOp:
       case AddArithOp: 
        if (ccOnly  &&  opn != 0x80000000)  {theAssembler->cmpl(-opn, NumberOperand, dst);  return no_reg;}
        else                                {theAssembler->addl( opn, NumberOperand, dst);  return dst;}

       case TSubCCArithOp:
       case SubCCArithOp:
       case SubArithOp: 
        // Following was wrong because _Eq: inlining uses SubCCArithOp, and needs to be OopOperand.
        // That's OK because SubCCArithOp is never used with funny numbers like TagMask -- dmu 4/07
        // Really, these nodes should carry the info along
        
        // if (ccOnly)       {theAssembler->cmpl(opn, op == TSubCCArithOp ? OopOperand : NumberOperand, dst);  return no_reg;}
        // else              {theAssembler->subl(opn, op == TSubCCArithOp ? OopOperand : NumberOperand, dst);  return dst;}

        if (ccOnly)       {theAssembler->cmpl(opn, op == SubArithOp ? NumberOperand : OopOperand, dst);  return no_reg;}
        else              {theAssembler->subl(opn, op == SubArithOp ? NumberOperand : OopOperand, dst);  return dst;}
        
       case TMulCCArithOp:  opn >>= Tag_Size; // FALL THROUGH
       case MulArithOp:     theAssembler->imull(opn, NumberOperand, dst, dst); return dst;
       
       case DivArithOp:     assert(dst == eax, "idiv");
                            theAssembler->movl(opn, NumberOperand, t);
                            theAssembler->idivl(t);
                            return dst;
       
       case TDivCCArithOp:  theAssembler->sarl(Tag_Size, NumberOperand, dst);
                            assert(dst == eax, "idiv");
                            theAssembler->movl(opn >> Tag_Size, NumberOperand, t);
                            theAssembler->idivl(t);
                            check_overflow(failNode);
                            theAssembler->sall(Tag_Size - 1, NumberOperand, dst);
                            // use one-bit shifts so overflow will be set
                            theAssembler->sall(1, NumberOperand, dst);
                            return dst;
                            
       case TModCCArithOp:  assert(dst == eax, "idiv");
                            theAssembler->movl(opn, NumberOperand, t);
                            theAssembler->idivl(t);
                            return edx;
                            /*  alternatively:
                                theAssembler->pushl(dst);
                                theAssembler->imull(t, dst);
                                theAssembler->popl(t);
                                theAssembler->subl(dst, t);
                                return t;
                            */

       case TAndCCArithOp:
       case AndCCArithOp:
       case AndArithOp:
        if (ccOnly)       {theAssembler->testl(opn, NumberOperand, dst);  return no_reg;}
        else              {theAssembler->andl( opn, NumberOperand, dst);  return dst;}
        
       case TOrCCArithOp:
       case OrCCArithOp:
       case OrArithOp:     theAssembler->orl( opn, NumberOperand, dst);  return dst;

       case TXorCCArithOp:
       case XOrArithOp:    theAssembler->xorl(opn, NumberOperand, dst);  return dst;

       case TALShiftCCArithOp:
       case TLLShiftCCArithOp:
                           opn >>= Tag_Size; // FALL THROUGH
       case ArithmeticLeftShiftArithOp:
       case LogicalLeftShiftArithOp:
                           theAssembler->sall(opn, NumberOperand, dst);  return dst; // must use arith shift for overflow checking
                           
       case ArithmeticRightShiftArithOp:
                           theAssembler->sarl(opn, NumberOperand, dst);  return dst;
                           
       case LogicalRightShiftArithOp:
                           theAssembler->shrl(opn, NumberOperand, dst);  return dst;
                                        
       case TARShiftCCArithOp: theAssembler->sarl(opn >> Tag_Size, NumberOperand, dst);
                               theAssembler->andl(~Tag_Mask, NumberOperand, dst);
                               return dst;
                               
       case TLRShiftCCArithOp: theAssembler->shrl(opn >> Tag_Size, NumberOperand, dst);
                               theAssembler->andl(~Tag_Mask, NumberOperand, dst);
                               return dst;
                               
       default:            ShouldNotReachHere(); // unexpected arith type
      }
      ShouldNotReachHere();
      return no_reg;
  }
  
  
  static Location arith_genHelper_do_reg_reg(Location dst, Location opn, ArithOpCode op, bool ccOnly, Node* failNode) {
    # define XX(op) case op: theAssembler->Untested("reg:" STR(op)); break;
    switch (op) {
      default: break;
      XX(NilArithOp) XX(AddArithOp) XX(SubArithOp) XX(MulArithOp) XX(DivArithOp) XX(AndArithOp) XX(OrArithOp)
      XX(XOrArithOp) XX(ArithmeticLeftShiftArithOp) XX(LogicalLeftShiftArithOp) XX(ArithmeticRightShiftArithOp)
      XX(LogicalRightShiftArithOp) XX(AddCCArithOp) XX(AndCCArithOp) XX(OrCCArithOp)
    }
    int foo = 0;
    # undef XX
    switch (op) {
     case AddArithOp:
     case AddCCArithOp:
     case TAddCCArithOp:      theAssembler->addl(opn, dst);            return dst;
     
     case SubArithOp:
     case SubCCArithOp:
     case TSubCCArithOp:
      if (ccOnly)          {  theAssembler->cmpl(opn, dst);            return no_reg; }
      else                 {  theAssembler->subl(opn, dst);            return dst; }
      
     case TMulCCArithOp:      theAssembler->sarl(Tag_Size, NumberOperand, opn);  // FALL THROUGH
     case MulArithOp:         theAssembler->imull(opn, dst);           return dst;
     
     case DivArithOp:      assert(dst == eax, "idiv");
                           theAssembler->idivl(opn);                   return dst;
     
     case TDivCCArithOp:      theAssembler->sarl(Tag_Size, NumberOperand, opn);
                              theAssembler->sarl(Tag_Size, NumberOperand, dst);
                              assert(dst == eax, "idiv");
                              theAssembler->idivl(opn);
                              check_overflow(failNode);
                              theAssembler->sall(Tag_Size - 1, NumberOperand, dst);
                              // use one-bit shifts so overflow will be set
                              theAssembler->sall(1, NumberOperand, dst);
                              return dst;
                              
     case TModCCArithOp:      assert(dst == eax, "idiv");
                              theAssembler->idivl(opn);
                              return edx;
                              /* alternatively could:
                                  theAssembler->push(dst);
                                  theAssembler->imull(opn, dst);
                                  theAssembler->popl(opn);
                                  theAssembler->subl(dst, opn);
                                  return opn;
                              */
                              
     case AndArithOp:
     case AndCCArithOp:
     case TAndCCArithOp:
       if (ccOnly)          { theAssembler->testl(opn, dst);         return no_reg; }
       else                 { theAssembler->andl(opn, dst);          return dst; }

     case OrArithOp:
     case OrCCArithOp:
     case TOrCCArithOp:       theAssembler->orl(opn, dst);           return dst;

     case TXorCCArithOp:
     case XOrArithOp:         theAssembler->xorl(opn, dst);          return dst;
     
     case TALShiftCCArithOp:
     case TLLShiftCCArithOp:  theAssembler->sarl(Tag_Size, NumberOperand, opn); // fall through
     case ArithmeticLeftShiftArithOp:
     case LogicalLeftShiftArithOp:
                              assert(opn == ecx, "checking");
                              theAssembler->sall(Assembler::shift_by_count_reg, NumberOperand, dst); 
                              return dst;
                              
                              

     case    LogicalRightShiftArithOp: 
                              assert(opn == ecx, "checking");
                              theAssembler->shrl(Assembler::shift_by_count_reg, NumberOperand, dst); 
                              return dst;
     case ArithmeticRightShiftArithOp: 
                              assert(opn == ecx, "checking");
                              theAssembler->sarl(Assembler::shift_by_count_reg, NumberOperand, dst);     
                              return dst;


     case TARShiftCCArithOp:  theAssembler->sarl(Tag_Size, NumberOperand, opn);
                              assert(opn == ecx, "checking");
                              theAssembler->sarl(Assembler::shift_by_count_reg, NumberOperand, dst);
                              theAssembler->andl(~Tag_Mask, NumberOperand, dst);
                              return dst;
     case TLRShiftCCArithOp:  theAssembler->sarl(Tag_Size, NumberOperand, opn);
                              assert(opn == ecx, "checking");
                              theAssembler->shrl(Assembler::shift_by_count_reg, NumberOperand, dst);
                              theAssembler->andl(~Tag_Mask, NumberOperand, dst);
                              return dst;
     
     default:                 ShouldNotReachHere(); // unexpected arith type
    }
    ShouldNotReachHere();
    return no_reg;
  }

  
  static Location arith_genHelper(PReg* sreg, bool srcIsInt, PReg* oper, bool operIsInt, PReg* dest,
                           ArithOpCode op, Node* failNode) {

    reverse_if_helpful(sreg, oper, op, srcIsInt, operIsInt);
    Location dst = arith_genHelper_move_src_to_reg(sreg, srcIsInt, oper, op, failNode);
    if (!srcIsInt && failNode) {
      theAssembler->testl(Tag_Mask, NumberOperand, dst);
      Label* L = new Label(theAssembler->printing);
      theAssembler->jnz(L);
      failNode->l = L->unify(failNode->l);
    }

    if (oper->isConstPReg()) { 
       oop val = ((ConstPReg*)oper)->constant;
       if (!val->is_smi()  && failNode  &&  !operIsInt) {
         theAssembler->Untested("arith_genHelper");
         theAssembler->testl(0, NumberOperand, esp); // set ne cc for fail node
         Label* L = new Label(theAssembler->printing);
         theAssembler->jmp(L);
         failNode->l = L->unify(failNode->l);
         return NoReg;
       }
       return arith_genHelper_do_immediate(dst, int32(val), op, dest->isNoPReg(), failNode);
    }
       
    Location opr = arith_genHelper_move_oper_to_reg(dst, oper, op);
    if (!operIsInt && failNode) {
        theAssembler->testl(Tag_Mask, NumberOperand, opr);
        Label* L = new Label(theAssembler->printing);
        theAssembler->jnz(L);
        failNode->l = L->unify(failNode->l);
    }
    return arith_genHelper_do_reg_reg(dst, opr, op, dest->isNoPReg(), failNode);
  }        
  

  Location arith_genHelper(PReg* sreg, PReg* oper, PReg* dest,
                           ArithOpCode op,
                           Location& t1, Location& t2, bool& reversed) {
    // used elsewhere in the compiler for ArithRRNode
    assert(op != DivArithOp,
           "would have to do markAllocated and canCopyPropagate for ArithRRNode, will use eax");
    return arith_genHelper( sreg, true /* no check */, oper, true, dest, op, NULL);
  }    
  
  
// Next two routines are placeholders in case
// any operations need Temp3, etc. -- dmu 1/03

  void TArithRRNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); U_CHECK(oper);
    // Only Div-oidal operations use Temp3 (eax)
    switch (op) {
     default: break;
     case DivArithOp:  case TDivCCArithOp:  case TModCCArithOp:
      assert(Temp3 == eax  &&  Temp4 == edx, "used by divide");
      use_count[Temp3]++; def_count[Temp3]++;       // uses Temp3
      use_count[Temp4]++; def_count[Temp4]++;       // remainder -> Temp4
      break;
    }
  }
  
      
  bool TArithRRNode::canCopyPropagateFrom(PReg* d) {
    assert(Temp3 == eax, "???");
    switch (op) {
     default:                                                    return true;
     case DivArithOp:  case TDivCCArithOp:  case TModCCArithOp:  return d->loc != Temp3  &&  d->loc != Temp4;
    }
  }

  bool TArithRRNode::isOpInlinable( ArithOpCode op ) {
    return  op != NilArithOp;
  }
  
  
  void TArithRRNode::gen() {
    // See SPrimScope::inlineIntArithmetic
    BasicNode::gen();
    if (constResult) {
      theAssembler->Untested("TArithRRNode const");
      Location b;  int32 d;  OperandType t;
      reg_disp_type_of_loc(&b, &d, &t, _dest->loc);
      theAssembler->movl((int32)constResult->constant, OopOperand, b, d, t); 
      return;
    }
    // assembler uses large number as shift by ecx, so check for static overflow here
    if (oper->isConstPReg()  &&  (uint32)(((ConstPReg*)oper)->constant) > 32)
      switch (op) {
       default:  break;
      
       case LogicalLeftShiftArithOp:    case TLLShiftCCArithOp:
       case LogicalRightShiftArithOp:   case TLRShiftCCArithOp:
        Location b;  int32 d;  OperandType t;
        reg_disp_type_of_loc(&b, &d, &t, _dest->loc);
        theAssembler->movl(0, NumberOperand, b, d, t); 
        return;

       case ArithmeticLeftShiftArithOp:   case ArithmeticRightShiftArithOp: 
       case TALShiftCCArithOp:            case TARShiftCCArithOp: 
        force_overflow(next1());
        return;
       }
    Location dest = arith_genHelper(_src, arg1IsInt, oper, arg2IsInt, _dest, op, next1());
    bool canOverflow =    
            op == TAddCCArithOp
        ||  op == TSubCCArithOp
        ||  op == TMulCCArithOp
        ||  op == TALShiftCCArithOp
        ||  op == TDivCCArithOp
        ||  op == TModCCArithOp /* if 2nd arg is 0 */;
        
    if (canOverflow)
      check_overflow(next1());

    if (dest != _dest->loc  &&  !_dest->isNoPReg()) {
      // store result on stack (success case)
      assert(dest != no_reg, "???");
      theAssembler->movl(dest, esp, spOffset(_dest->loc), NumberOperand);
    }
  }

  
  void ArithRCNode::gen() {
    BasicNode::gen();
    
    switch (op) {
    # define XX(op) case op: theAssembler->Untested("RC:" STR(op)); break;
      default: break;
      XX(NilArithOp) XX(AddArithOp) XX(MulArithOp) XX(DivArithOp) XX(OrArithOp)
      XX(XOrArithOp) XX(ArithmeticLeftShiftArithOp) XX(LogicalLeftShiftArithOp) XX(ArithmeticRightShiftArithOp)
      XX(LogicalRightShiftArithOp) XX(AddCCArithOp) XX(OrCCArithOp)
      XX(TAddCCArithOp) XX(TSubCCArithOp) XX(TMulCCArithOp) XX(TDivCCArithOp) XX(TALShiftCCArithOp) XX(TARShiftCCArithOp)
      XX(TLLShiftCCArithOp) XX(TLRShiftCCArithOp) XX(TAndCCArithOp) XX(TOrCCArithOp) XX(TXorCCArithOp)
    }
    # undef XX
    
    Location dest;
    if (_dest->isNoPReg()) 
      switch (op) {
        // If only for CC and we do not need a dumping ground, any reg will do
       case SubCCArithOp:  
        dest = genHelper->moveToReg(_src, Temp1);
        // Use NumberOperand below because this is used with arbitrary numbers, not oops; see
        //   SPrimScope::genPrimFailure -- dmu 4/07
        theAssembler->cmpl(oper, NumberOperand, dest);  return;
       case AndCCArithOp:  
        dest = genHelper->moveToReg(_src, Temp1);
        theAssembler->testl(oper, NumberOperand, dest); return;
       default: break;
      }
      
    dest = isRegister(_dest->loc) ? _dest->loc : _src->loc == Temp1 ? Temp2 : Temp1;
         if (!isRegister(_src->loc))   genHelper->moveToReg(_src,      dest);
    else if (_src->loc != dest)         theAssembler->movl( _src->loc, dest);


    switch (op) {
     case AddCCArithOp:
     case AddArithOp:   theAssembler->addl(oper, NumberOperand, dest);         break;
     // Use NumberOperand below because this is used with arbitrary numbers, not oops; see
     //   SPrimScope::genPrimFailure -- dmu 4/07
     case SubCCArithOp:
     case SubArithOp:   theAssembler->subl(oper, NumberOperand, dest);         break;
       
     case AndCCArithOp:
     case AndArithOp:   theAssembler->andl(oper, NumberOperand, dest);         break;
     case OrCCArithOp:
     case OrArithOp:    theAssembler->orl( oper, NumberOperand, dest);         break;
     case XOrArithOp:   theAssembler->xorl(oper, NumberOperand, dest);         break;
     case ArithmeticLeftShiftArithOp:
     case LogicalLeftShiftArithOp:
                        theAssembler->shll(oper, NumberOperand, dest);         break;
     case ArithmeticRightShiftArithOp:
                        theAssembler->sarl(oper, NumberOperand, dest);         break;
     case LogicalRightShiftArithOp:
                        theAssembler->shrl(oper, NumberOperand, dest);         break;

     default:           ShouldNotReachHere(); // unexpected arith type
    }
    if (dest != _dest->loc) {
      theAssembler->movl(dest, esp, spOffset(_dest->loc), NumberOperand);
    }
  }


  void BranchNode::gen() {
    BasicNode::gen();

    Label* l_ = new Label(theAssembler->printing);
    switch (op) {
     case ALBranchOp:   theAssembler->jmp(l_);   break;
     case EQBranchOp:   theAssembler->je( l_);   break;
     case NEBranchOp:   theAssembler->jne(l_);   break;
     case LTBranchOp:   theAssembler->jl( l_);   break;
     case LEBranchOp:   theAssembler->jle(l_);   break;
     case LTUBranchOp:  theAssembler->jb( l_);   break;
     case LEUBranchOp:  theAssembler->jbe(l_);   break;
     case GTBranchOp:   theAssembler->jg( l_);   break;
     case GEBranchOp:   theAssembler->jge(l_);   break;
     case GTUBranchOp:  theAssembler->ja( l_);   break;
     case GEUBranchOp:  theAssembler->jae(l_);   break;
     case VSBranchOp:   theAssembler->jo( l_);   break;
     case VCBranchOp:   theAssembler->jno(l_);   break;
     default:           ShouldNotReachHere(); // unexpected branch type
    }
    Node* n = next1();
    n->l = l_->unify(n->l);
  }


  void TBranchNode::genCompare(bool haveImmediate,
                               Location rcvrReg, Location argReg) {
    Location rb;  int32 rd;  OperandType rt;
    Location ab;  int32 ad;  OperandType at;
    reg_disp_type_of_loc(&rb, &rd, &rt, rcvrReg);
    if (!haveImmediate)
      reg_disp_type_of_loc(&ab, &ad, &at, argReg);
    if (!intRcvr) {
      // check that rcvr is a smiOop
      theAssembler->testl(Tag_Mask, NumberOperand, rb, rd, rt);
      Label*& primFailure = ((MergeNode*)nexti(2))->l;
      Label* l = new Label(theAssembler->printing);
      theAssembler->jnz(l); // will not fail, will fall through fwd, so normal
      primFailure = primFailure->unify(l);
    }
    if (!intArg) {
      assert(!haveImmediate, "???");
      // check that arg is a smiOop
      theAssembler->testl(Tag_Mask, NumberOperand, ab, ad, at);
      Label*& primFailure = ((MergeNode*)nexti(2))->l;
      Label* l = new Label(theAssembler->printing);
      theAssembler->jnz(l); // will not fail, will fall through fwd, so normal
      primFailure = primFailure->unify(l);
    }

    // we're here iff arg and rcvr are smiOop's.  do the actual comparision
    if (haveImmediate) {
      oop val = ((ConstPReg*)arg)->constant;
      theAssembler->cmpl((int32)val, NumberOperand, rb, rd, rt);
    }
    else if (isRegister(rcvrReg)) {
      theAssembler->cmpl(ab, ad, at, rcvrReg);
    }
    else if (isRegister(argReg)) {
      theAssembler->Untested("genCompare");
      theAssembler->cmpl(argReg, rb, rd, rt);
    }
    else {
      theAssembler->Untested("genCompare");
      theAssembler->movl(rb, rd, rt, Temp1);
      theAssembler->cmpl(ab, ad, at, Temp1);
    }
  }


  void TBranchNode::testTagsIfNecessary(bool haveImmediate, Location rcvrReg, Location argReg) {
    // this function doesn't do anything on i386.  it's needed because on the SPARC,
    // if there's an overflow, we need to check the tag bits to see if the overflow
    // is caused by non-smi arguments, or by an actual integer overflow.
    // but on i386, we always check the object tags in the beginning, so if an overflow
    // occurs, it's a "real" integer overflow. -mabdelmalek 12/02
  }


  void TypeTestNode::br_if_smi(Assembler* a, Location rcvr, fint smiIndex) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, rcvr);
    theAssembler->testl(Tag_Mask, NumberOperand, b, d, t);
    Label* label1 = new Label(theAssembler->printing);
    theAssembler->jz(label1);
    define(smiIndex, label1);
  }

  void TypeTestNode::br_if_float(Assembler* a, Location rcvr, fint floatIndex) {
    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, rcvr);
    theAssembler->btl( Float_Tag_bit_i386, b, d, t);
    Label* label2 = new Label(theAssembler->printing);
    theAssembler->jc(label2);
    define(floatIndex, label2);
  }
  
  void TypeTestNode::br_to_unknown_case(Assembler* a) {
    Label* unknownCase = new Label(theAssembler->printing);
    theAssembler->jmp(unknownCase);
    define(0, unknownCase);
  }

  // Returns index of case to jump to, or 0 if none chosen.
  // Also returns loadMapAfterHandlingImmediates, label where caller gens code to load map
  // -- dmu 10/03

  fint TypeTestNode::prologue(Assembler* a, Location rcvr, fint smiIndex,
                              fint floatIndex, bool immediateOnly,
                              Label*& loadMapAfterHandlingImmediates) {
    // handle immediate cases of type test and load map if necessary
    // smiIndex/Float are the indices of the smi/float case in the map list
    // (+ 1, so that "not present" == 0)
    // returns the index (+ 1) of the fall-through case 
    assert(((Float_Tag | Int_Tag) & Mem_Tag) == 0, "tagging scheme changed");
    assert(!immediateOnly || !needMapLoad,
           "immediateOnly implies !needMapLoad");
           
    if (!needMapLoad) {
      // no map load needed; no mem maps to test, rcvr could be memOop at this point
      if (  smiIndex)  br_if_smi  (a, rcvr,   smiIndex);
      if (floatIndex)  br_if_float(a, rcvr, floatIndex);
      return 0; // will be no more testing, so can fall-through to unknown
    }
     
    Location b; int32 d; OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, rcvr);
    theAssembler->btl(Mem_Tag_bit_i386, b, d, t);
    loadMapAfterHandlingImmediates = new Label(theAssembler->printing);
    theAssembler->jc(loadMapAfterHandlingImmediates); // fewer selectors have int cases, so probably goes
      
    if (smiIndex  &&  floatIndex) {
      br_if_smi(a, rcvr, smiIndex);
      return floatIndex;
    }
    if (  smiIndex)   br_if_smi  (a, rcvr,   smiIndex);
    if (floatIndex)   br_if_float(a, rcvr, floatIndex);
    br_to_unknown_case(a); // fall-through would fall into mem testing, since n == 0 does not gen branch
    return 0;
  }
  
  
  void TypeTestNode::testMap(ConstPReg* pr, fint index) {
    assert(pr->constant->is_map(), "should be map");
    assert(needMapLoad, "need to load receiver map");
    // if the receiver was moved to MapReg in TypeTestNode (this would
    // happen if it wasn't already in a register), then the following
    // instruction might clober the receiver (if "pr" wasn't already
    // in a register).  but this is ok since we don't need the receiver
    // any more. -mabdelmalek 10/02.
    theAssembler->cmpl((int32)pr->constant, OopOperand, RcvrMapReg);  // compare
    Label* match = new Label(theAssembler->printing);
    theAssembler->je(match);                 // branch if match
    define(index, match);
  }
  
  void TypeTestNode::testOop(ConstPReg* pr, fint index) {
    assert(!pr->constant->is_map(), "should be oop");
    Location b; int32 d; OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, r);
    theAssembler->cmpl((int32)pr->constant, OopOperand, b, d, t);
    Label* match = new Label(theAssembler->printing);
    theAssembler->je(match);                // branch if match
    define(index, match);
  }


  // Shoould be refactored someday ala PICs. -- dmu 2/03
  void TypeTestNode::gen() {
    // generates n-way type test; fall-through code is "unknown" case

    BasicNode::gen();
    r = genHelper->moveToReg(_src, MapReg);
    

    // indexes if smi or float branches if present. Branch index is one more than maps index -- dmu
    // NOTE: index 0 is the no match brach target
    fint   smiIndex = 0;
    fint floatIndex = 0;
         if (maps->nth(0) == Memory->  smi_map->enclosing_mapOop())   smiIndex = 1;
    else if (maps->nth(0) == Memory->float_map->enclosing_mapOop()) floatIndex = 1;
    if (maps->length() > 1) {
           if (maps->nth(1) == Memory->  smi_map->enclosing_mapOop())   smiIndex = 2;
      else if (maps->nth(1) == Memory->float_map->enclosing_mapOop()) floatIndex = 2;
    }


    fint nconstants = 0;
    fint ntests = maps->length();
    fint firstMem = max(smiIndex, floatIndex);
    bool immediateOnly = firstMem == maps->length();

    for (fint i = firstMem; i < ntests; ++i) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map()) ++nconstants;
    }

    // first test against all constants
    if (!hasUnknown  &&  nconstants == ntests) {
      // don't need to check for last constant
      --ntests;
    }
    for (fint i = firstMem;  i < ntests;  ++i) {
      ConstPReg* pr = mapPRs->nth(i);
      if (!pr->constant->is_map())   testOop(pr, i + 1);
    }
    if (!hasUnknown && nconstants >= ntests) {
      // last case; should omit branch
      Label* match = new Label(theAssembler->printing);
      theAssembler->jmp(match);
      define(ntests + 1, match);
      return;           // done -- tested all constants
    }

    Label* loadMapAfterHandlingImmediates = NULL;
    fint n = prologue(theAssembler, r, smiIndex, floatIndex, immediateOnly, loadMapAfterHandlingImmediates);

    if (n) {
      Label* match = new Label(theAssembler->printing);
      theAssembler->jmp(match);
      define(n, match);
    }

    if (!loadMapAfterHandlingImmediates)
      ;
    else if (immediateOnly)
      define(0, loadMapAfterHandlingImmediates);     // no memOop tests
    else
      loadMapAfterHandlingImmediates->define();

    if (!hasUnknown) --ntests;      // all maps known, can omit last test
    // test against all maps
    if (needMapLoad) {
      // load receiver map
      theAssembler->movl(r, map_offset(), NumberOperand, RcvrMapReg);
    }
    for (fint i = firstMem; i < ntests; i++) {
      ConstPReg* pr = mapPRs->nth(i);
      if (pr->constant->is_map()) testMap(pr, i + 1);
    }
    if (!hasUnknown) {
      // last case; should omit branch
      theAssembler->Untested("TypeTestNode::gen");
      Label* match = new Label(theAssembler->printing);
      theAssembler->jmp(match);
      define(ntests + 1, match);
    }
  }

  
  void IndexedBranchNode::gen() {
    // generates n-way indexed branch;
    // fall-through code is non-int or out of bounds
    BasicNode::gen();
    r = genHelper->moveToReg(_src, IndexReg);

    // check tag
    Label end;
    if (!srcMustBeSmi) {
      theAssembler->testl(Tag_Mask, NumberOperand, r);
      theAssembler->jnz(&end);
    }

    // check bounds
    theAssembler->cmpl((int32)as_smiOop(nCases), OopOperand, r);
    theAssembler->jnb(&end); // goto end if index is out of bounds
  
    Label L(theAssembler->printing);
    theAssembler->call(&L); // get pc of next inst into link
    
    pc_t link_reg_value= theAssembler->addr();
    L.define();

    const int32 indexShift = 2 - Tag_Size;
    assert(indexShift == 0, "no shift needed");
    
    Location t = r == Temp1 ? Temp2 : Temp1;

    theAssembler->popl(t);
    const int32 bytes_from_link_value_to_jumps = 7;
    // indexing is by_two because index is ALREADY 4x value because of tagging
    theAssembler->leal(t,  bytes_from_link_value_to_jumps, NumberOperand, r, Assembler::by_two, t);

    theAssembler->jmp(t);

    pc_t start_of_jumps= theAssembler->addr();
    assert( bytes_from_link_value_to_jumps == start_of_jumps - link_reg_value, "recount");


    for (fint i = 0;  i < nCases;  ++i) {
      Label* nthCase = new Label(theAssembler->printing);
      theAssembler->jmp(nthCase); theAssembler->hlt(); theAssembler->hlt(); theAssembler->hlt();
      Node* n = nexti(i + 1);
      n->l = nthCase->unify(n->l);
    }
    end.define();
  }


  void BlockZapNode::gen() {
    BasicNode::gen();
    Location t = genHelper->moveToReg(block(), Temp1);
    theAssembler->movl(0, NumberOperand, t, scope_offset(), NumberOperand);
  }


  void AbstractArrayAtNode::markAllocated(fint* use_count, fint* def_count) {
    U_CHECK(_src); D_CHECK(_dest); U_CHECK(arg);
    if (error) D_CHECK(error);
  }
  
    
  bool AbstractArrayAtNode::canCopyPropagateFrom(PReg* d) {
    // covers AbstractArrayAtPut and both ats
    return true;
  }


  void AbstractArrayAtNode::gen() {
    BasicNode::gen();
    Assembler* a = theAssembler;
    Label* argFail = NULL;          // if arg isn't a smi
    Label* indexFail = new Label(a->printing);  // if arg is out of bounds
    Location arr = genHelper->moveToReg(_src, Temp2);
    Location index = genHelper->moveToReg(arg, Temp1);
    if (!intArg) {
      // CP may have propagated a constant into arg
      intArg = arg->isConstPReg() && ((ConstPReg*)arg)->constant->is_smi();
    }
    if (!intArg) {
      // test arg for smiOop
      a->testl(Tag_Mask, NumberOperand, index);
      Label* failLabel = new Label(a->printing);
      a->jnz(failLabel);
      argFail = argFail->unify(failLabel);
    }
    argFail = argFail->unify(testArg2());
    a->cmpl(arr, sizeOffset, NumberOperand, index);
    a->jae(indexFail); // likely to fall through
    Location res = isRegister(_dest->loc) ? _dest->loc : Temp1;   
    bool needDestStore = genAccess(arr, index, res);
    if (needDestStore && !isRegister(_dest->loc) && !_dest->isNoPReg())
      genHelper->moveRegToLoc(res, _dest->loc);

    Label* done = new Label(a->printing);
    a->jmp(done);
    MergeNode* failMerge = (MergeNode*)next1();

    Location eb; int32 ed; OperandType et;
    if (error)  reg_disp_type_of_loc(&eb, &ed, &et, error->loc);
    if (argFail) {
      argFail->define();
      if (error) {
        a->movl((int32)VMString[BADTYPEERROR], OopOperand, eb, ed, et);
      }
      if (failMerge) { // test added by dmu 4/27/96
        Label* L = new Label(a->printing);
        a->jmp(L);
        failMerge->l = failMerge->l->unify(L);
      }
    }
    indexFail->define();
    if (error) {
      a->movl((int32)VMString[BADINDEXERROR], OopOperand, eb, ed, et);
    }
    if (failMerge) { // test added by dmu 4/27/96
      Label* L = new Label(a->printing);
      a->jmp(L);
      failMerge->l = failMerge->l->unify(L);
    }
    done->define();
  }

  // gen array access; Temp1 may hold index, Temp2
  // may hold array
  bool ArrayAtNode::genAccess(Location arr, Location index, Location dest) {
    theAssembler->movl(arr, dataOffset, NumberOperand, index, Assembler::by_one, dest);
    return true;
  }

  bool ByteArrayAtNode::genAccess(Location arr, Location index, Location dest) {
    assert(index != Temp2, "???");
    theAssembler->movl(arr, dataOffset, NumberOperand, Temp2);
    if (index != Temp1) {
      theAssembler->Untested("ByteArrayAtNode::genAccess");
      theAssembler->movl(index, Temp1);
    }
    theAssembler->sarl(Tag_Size, NumberOperand, Temp1);
    theAssembler->movzbl(Temp1, 0, NumberOperand, Temp2, Assembler::by_one,  dest);
    theAssembler->shll(Tag_Size, NumberOperand, dest);
    return true;
  }

  bool ArrayAtPutNode::genAccess(Location arr, Location index, Location /*dest*/) {
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src  && !_dest->isNoPReg()) 
      genHelper->moveRegToLoc(arr, _dest->loc);
    
    if (elem->isConstPReg()  &&  ((ConstPReg*)elem)->constant->is_old()) {
      theAssembler->movl((int32)((ConstPReg*)elem)->constant, OopOperand, arr, dataOffset, NumberOperand, index, Assembler::by_one);
    }
    else {
      assert(elem->loc != Temp1, "???");
      theAssembler->leal(arr, dataOffset, NumberOperand, index, Assembler::by_one, Temp1);
      if (elem->isConstPReg())
        theAssembler->movl((int32)((ConstPReg*)elem)->constant, OopOperand, Temp1, 0, NumberOperand);
      else
        theAssembler->movl( genHelper->moveToReg(elem, Temp2), Temp1, 0, NumberOperand);

      theAssembler->shrl(card_shift, NumberOperand, Temp1);
      if (!UseByteMapBaseReg) {
        theAssembler->addl(no_reg, (int32)&byte_map_base, VMAddressOperand, Temp1);
        theAssembler->movb(0, Temp1, 0, NumberOperand);
      }
      else {
        theAssembler->movb(0, ByteMapBaseReg, 0, NumberOperand, Temp1, Assembler::by_one);
      }
    }
    return false;
  }


  Label* ByteArrayAtPutNode::testArg2() {
    // check if arg is 0..255
    if (elem->isConstPReg()) {
      if (((ConstPReg*)elem)->constant->is_smi()) {
        theAssembler->Untested("ByteArrayAtPutNode::testArg2");
        // no run-time check required
        return NULL;
      }
      theAssembler->Untested("ByteArrayAtPutNode::testArg2");
      // primitive will always fail
      Label* L = new Label(theAssembler->printing);
      theAssembler->jmp(L);
      return L;
    } 

    Location b;  int32 d;  OperandType t;
    reg_disp_type_of_loc(&b, &d, &t, elem->loc);
    Label* fail = new Label(theAssembler->printing);
    theAssembler->testl(~(0xff << Tag_Size), NumberOperand, b, d, t);
    theAssembler->jnz(fail);
    return fail;
  }
  
  
  bool ByteArrayAtPutNode::genAccess(Location arr,
                                     Location index, Location /*dest*/) {
    // ignore dest and handle result assignment here (saves one instruction)
    if (_dest != _src && !_dest->isNoPReg())
      genHelper->moveRegToLoc(arr, _dest->loc);

    assert(arr != Temp1, "???");
    if (index != Temp1)  theAssembler->movl(index, Temp1);
    theAssembler->sarl(Tag_Size, NumberOperand, Temp1);
    theAssembler->addl(arr, dataOffset, NumberOperand, Temp1);
    
    if (elem->isConstPReg()) {
      // storing a constant - may be non-smi, but then this code will never
      // be executed anyway because the primitive fails
      ConstPReg* value = (ConstPReg*)elem;
      assert(value->constant->is_smi(), "what to do?");
      theAssembler->Untested("ByteArrayAtPutNode::genAccess");
      theAssembler->movb(smiOop(value->constant)->value(), Temp1, 0, NumberOperand); // store in byte array
    }
    else {
      genHelper->moveToExactlyThisReg(elem, Temp2);
      theAssembler->shrl(Tag_Size, NumberOperand, Temp2);  // convert to char
      theAssembler->movb(Temp2, Temp1, 0, NumberOperand); // store in byte array
    }
    return false;           // result already handled here
  }


  void FlushNode::flushRegister(PReg* pr) {
    // a nop on I386, since args always passed in memory
  }


  void DeadBlockNode::gen() {
    BasicNode::gen();
    genPcDesc();
    theAssembler->Comment("dead block code");
    Label next(theAssembler->printing);
    theAssembler->call(&next); // prim needs some PC in this method
    next.define();
    theAssembler->popl(Temp1);
    theAssembler->movl(Temp1, esp, rcvr_offset*oopSize, NumberOperand);
    PrimNode::gen();
  }
  
  void DeadEndNode::gen() {
    // this node is unreachable - generate a trap for debugging
#   if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      BasicNode::gen();
      theAssembler->hlt();
    }
#   endif
  }


   void UncommonNode::gen() {
     BasicNode::gen();
     genPcDesc();
     theAssembler->unimp(0, restartSend);
  }

# endif  // sic
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "longRegString_i386.hh"

# include "_longRegString_i386.cpp.incl"


# ifdef SIC_COMPILER


void LongRegisterString::allocate(Location l) {
  doAllocate(l);
}


# endif // SIC_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.3 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "deadBlockNode_i386.hh"

# include "_deadBlockNode_i386.cpp.incl"

# ifdef SIC_COMPILER

  PrimDesc* DeadBlockNode::non_lifo_abort;

  void initDeadBlockNode() {
    DeadBlockNode::non_lifo_abort
      = getPrimDescOfFunction(fntype(&NLRSupport::non_lifo_abort), true);
  }

# endif // SIC_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "sic_i386.hh"
# include "_sic_i386.cpp.incl"

# ifdef SIC_COMPILER


bool SICAllocator::keepUplevelRPRegsInMemory = true; 

void SICompiler::initializeForPlatform() {
  nlrLabel = NULL;
}


int32 SICompiler::stackTempCount() {
  return number_of_memory_locals()
       + max_no_of_outgoing_args_and_rcvr();
}



int32 SICompiler::max_no_of_outgoing_args_and_rcvr() {
  return argCount + 1 /* for rcvr */;
}


int32 SICompiler::number_of_memory_locals() {
  return stackLocCount;
}

void SICompiler::check_flushability(PReg* p) {
}

void SICompiler::cope_with_uplevel_access_to(PReg* pr) {
}



# endif // SIC_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "codeGen_i386.hh"
# pragma implementation "codeGen_inline_i386.hh"

# include "_codeGen_i386.cpp.incl"

# ifdef FAST_COMPILER

CodeGen* theCodeGen;


void CodeGen::genCountCode(int32* counter) {
  a.Comment("count # calls");
  a.Untested("genCountCode");
  a.incl( no_reg, (int32)counter, VMAddressOperand);
}

  
void CodeGen::testStackOverflow(RegisterState* s) {
  a.Comment("stack overflow/interrupt check");
  a.cmpl( no_reg, int32(&SPLimit), VMAddressOperand, esp);
  Label ok;
  // jump if no preempt, if esp > SPLimit, remember asm is backwards from intel, sigh
  a.ja(&ok); 

  (void)cPrimCall(intrCheck(), s, true, true, 0);

  ok.define();
}


void CodeGen::testStackOverflowForLoop( Label*& dst,  Label*& nlr, RegisterState* s) {
  //    cmp sp, splimit
  //    bge dst
  //    nop
  //    <cPrimCall interruptCheck>
   
  // (this code lifted from what used to be restart)
  
  a.Comment("stack overflow/interrupt check for loop");
  a.cmpl( no_reg, int32(&SPLimit), VMAddressOperand, esp);
  dst = new Label(a.printing);
  a.ja(dst); // ok
  nlr = cPrimCall(intrCheck(), s, false, true, 0);
}

# define LRCVR_BDT  leaf_rcvr_base, leaf_rcvr_disp, leaf_rcvr_type

void CodeGen::smiOop_prologue() {
  a.testl( Tag_Mask, NumberOperand, LRCVR_BDT);
  a.jne((int32)SendMessage_stub, PVMAddressOperand);
}


void CodeGen::floatOop_prologue() {
  a.btl( Float_Tag_bit_i386, LRCVR_BDT);
  // if bit set must be hit, otherwise is mark
  a.jnc( (int32)SendMessage_stub, PVMAddressOperand);
}


void CodeGen::memOop_prologue() {
  a.movl(LRCVR_BDT, Temp1);
  a.btl(Mem_Tag_bit_i386, Temp1);
  // if bit set must be hit, otherwise is mark
  a.jnc( (int32)SendMessage_stub, PVMAddressOperand);

  // check_map:
  a.movl( Temp1, map_offset(), NumberOperand, ebx);
  a.cmpl( (int32)L->receiverMapOop(), OopOperand, ebx);
  a.jne((int32)SendMessage_stub, PVMAddressOperand);
}


void CodeGen::checkOop(Label& general_miss, oop what, Location loc_to_check) {
  a.Comment("checkOop");
  assert(isRegister(loc_to_check), "must be reg");
  a.cmpl( (int32)what, OopOperand, loc_to_check);
  a.jne( (int32)SendMessage_stub, PVMAddressOperand );
}


void CodeGen::checkRecompilation() {

  a.Comment("checkRecompilation");
  // di recompilation doesn't work right now - see recompile.c
  if (diLink) return;

  a.Comment("test for recompilation");
  int32 countID = theCompiler->countID;
  void* counter = &useCount[countID];
  a.leal(no_reg, int32(counter), VMAddressOperand, Temp2);
  a.movl(Temp2, 0, NumberOperand, Temp1);
  a.incl(Temp1);
  a.movl(Temp1, Temp2, 0, NumberOperand);

  fint limit = recompileLimit(theCompiler->level());
  // compare to limit
  a.cmpl(limit, NumberOperand, Temp1);
  Label ok;
  a.jne(&ok);


  // (no frame yet)
  
  // Pass a link to caller's sendDesc in LinkReg.
  // Pass a link to callee (i.e. this code being generated) in RecompileLinkReg
  
  // pass original return pc in normal place, push extra return pc on stack

  // call recompiler
  int32 fnaddr = diLink 
               ?   (int32)DIRecompile_stub 
               :   (int32)Recompile_stub;
  a.call(fnaddr, PVMAddressOperand);
  a.hlt();

  // we don't return here from Recompile_stub.  Recompile_stub jumps to new method if
  // we do on-stack replacement.  if we don't do on-stack replacement, it returns directly
  // to the verified entry point in the nmethod.  -mabdelmalek 12/5/2002

  ok.define();
}

  
void CodeGen::prologue(bool isAccessMethod, fint nargs) {
  /*
    ; make stack
    enter 1-last_extra_offset*4, $0 // make space for current_pc nmethod_frame_chain
  */
  
  // *if not DI child
  //    <smi/float/memOop prologue>
  // _verified:                       (entry point from PICs)
  //    if necessary <check selector>
  //    if necessary <check delegatee>
  // *endif DI
  
  // _diCheck:                        (entry point after recompile)
  //    <verify assignable parents>
  
  // *if using recompilation
  //    <checkRecompilation>
  // *endif
  
  // *if haveStackFrame
  //    save sp, -frameSize*oopSize, sp
  // *endif
  
  // <clear stack temporaries and excess argument locations

  // CAUTION: use only Temp1/4 for temps in prologue; other temps
  // may contain lookup parameters.
  

  a.Comment("prologue");
  fint assignableParents = L->adeps->length();
  MethodKind kind =
    isAccessMethod ? MethodKind(-1) : theCompiler->method()->kind();
  _incoming_arg_count = nargs; // for eventual putting into nmethod
  
  if (diLink == 0) {
    if (!L->isReceiverStatic()) {
      // test receiver map
#       if GENERATE_DEBUGGING_AIDS
        if (CheckAssertions)
          switch (L->lookupType()) {
           case NormalLookupType:  break;
           case StaticNormalLookupType:
           case ImplicitSelfLookupType:
           case ResendLookupType:
           case DirectedResendLookupType: fatal("shouldn't miss"); break;
           default: break;
          }
#       endif
      Map* m = L->receiverMap();
      bool imm = m == Memory->smi_map || m == Memory->float_map;
      if (m == Memory->smi_map) {
        smiOop_prologue();
      } else if (m == Memory->float_map) {
        floatOop_prologue();
      } else {
        memOop_prologue();
      }
    }
    
    verifiedOffset = a.offset();
    Label generalMiss(a.printing, NULL);
    a.Comment("verified entry point:");
    
    if (L->isPerform()) {
      a.Comment("check selector");
      checkOop(generalMiss, L->selector(), PerformSelectorLoc);
    }
    
    if (needsDelegatee(L->lookupType()) && !L->isDelegateeStatic()) {
      a.Comment("check delegatee");
      checkOop(generalMiss, L->delegatee(), PerformDelegateeLoc);
    }
  } else {
    // don't check receiver map, selector, delegatee if a DI cache miss
    assert(assignableParents > 0, "should have some di parents to check");
  }
  diCheckOffset = a.offset();
  a.Comment("DI entry point:");
  
  if (assignableParents > 0) {
    a.Comment("verify state of assignable parents");
    fint count = 0;
    for (fint i = 0; i < assignableParents; i ++) {
      objectLookupTarget* target = L->adeps->start()[i];
      Location t = loadPath(Temp2, target, LReceiverReg, Temp1);
      count = verifyParents(target, t, count);
    }
  }

  bool recomp = needRecompileCode(theCompiler->level());
  if (recomp) checkRecompilation();

  if (!haveStackFrame) {
     prologueAddr = NULL;
  }
  else {
    a.Comment("make stack frame (next instruction will be backpatched)");
    a.enter(-0 * oopSize); // will be backpatched for locals
    prologueAddr = a.addr(); // will be used to patch enter with right frame size
    frameCreationOffset = a.offset(); // used by nmethod to find save instr, must have frame by this point  XXXintel really?
    callPatchAddr = a.addr();
    a.nop(); a.nop(); a.nop(); a.nop(); a.nop(); 
    endCallPatchAddr = a.addr();
  }
  if (GenerateCountCode) {
    int32* counter;
    if (assignableParents != 0) {
      counter = &NumberOfDIMethodCalls;
    } else if (isAccessMethod) {
      counter = &NumberOfAccessMethodCalls;
    } else if (kind == BlockMethodType) {
      counter = &NumberOfBlockMethodCalls;
    } else {
      counter = &NumberOfMethodCalls;
    }
    genCountCode(counter);
  }

  if (!isAccessMethod) {
    if (!recomp && GenerateLRUCode) {
      // this code is rarely generated in practice (recomp is usually true)
      a.Comment("reset unused bit");
      void* unused_addr = &LRUflag[Memory->code->nextNMethodID()];
      a.movl(0, NumberOperand, no_reg, int32(unused_addr), VMAddressOperand);
    }
  
    // don't keep uplevel-accessed names in regs
    // (for now, just flush everything)
    assert(haveStackFrame, "just checking");
  }
  else {
    assert(!haveStackFrame, "just checking");
  }    
  a.Comment("End Prologue");
}


Label* CodeGen::postPrologue(RegisterState* s, bool frequentPreemption) {
  // returns address of stack overflow test
  Label* l = new DefinedLabel(a.printing);
  if (frequentPreemption) {
    // no stack test necessary: have explicit check at bci 0
    // (actually I think it is more accurate to say have
    //  frequent checks -- dmu )
  } 
  else {
    testStackOverflow(s);
  }
  return l;
}


void CodeGen::fixupFrame(RegisterState* s) {
  // window size adjustment
  assert(haveStackFrame, "should have stack frame");
  assert(is_end_of_enter(prologueAddr), "enter?");  

  // must include permanents and stack: local slots, blocks and stack
  _number_of_memory_locals                = s->stackDepth;  

  fint    min_space_to_reserve = s->stackDepth + s->argDepth + s->rcvrDepth + num_extra_locals_for_runtime;
  fint    min_space_from_incoming_rcvr_to_outgoing_rcvr = min_space_to_reserve + ircvr_offset /*pc, ebp*/;
  fint actual_space_from_incoming_rcvr_to_outgoing_rcvr = roundTo(min_space_from_incoming_rcvr_to_outgoing_rcvr, frame_word_alignment);
  fint actual_space_to_reserve = min_space_to_reserve 
                               + actual_space_from_incoming_rcvr_to_outgoing_rcvr
                                  - min_space_from_incoming_rcvr_to_outgoing_rcvr;
  frameSize = actual_space_from_incoming_rcvr_to_outgoing_rcvr;
  assert((frameSize & (frame_word_alignment - 1))  == 0, "alignment");
  set_space_reserved_by_enter_instruction(prologueAddr, actual_space_to_reserve);
  
  if (number_of_memory_locals()  >  sizeof(RegisterString) * BitsPerByte) {
    // must clear extra stack locations:
    Label patches(a.printing);
    a.saveExcursion( callPatchAddr );
    a.jmp(&patches);
    DefinedLabel return_from_patches(a.printing);
    assert(a.addr() == endCallPatchAddr, "???");
    a.endExcursion();
    
    a.Comment("zero locations not in mask()");
    patches.define();
    for ( fint i = sizeof(RegisterString) * BitsPerByte;  i < number_of_memory_locals();  ++i) {
      Location r;  int32 d;  OperandType t;
      reg_disp_type_of_loc(&r, &d, &t, StackLocation_for_index(i));
      a.movl(0, NumberOperand, r, d, t);
    }
    a.jmp(&return_from_patches);
  }
}


fint CodeGen::verifyParents(objectLookupTarget* target, Location t, fint count) {
  
  
  a.Comment("verify");
  assert(target->links != NULL, "expecting an assignable parent link");
    
  for ( assignableSlotLink* l = target->links; ; ) {
    // load assignable parent slot value
    a.movl(t, smiOop(l->slot->data)->byte_count() - Mem_Tag, VMAddressOperand,  Temp1);
    Label ok;
    Label miss;
    Map* targetMap = l->target->obj->map();
    
    if (l->target->value_constrained) {
      // constraint for a particular oop (ambiguity resolution)
      loadOop(Temp2, l->target->obj);         // load assumed value
      a.cmpl(Temp1, Temp2);                   // compare values
      a.je(&ok);               // will branch
    } 
    // check if map of parent is correct
    else if (targetMap == Memory->smi_map) {
      a.testl(Tag_Mask, NumberOperand, Temp1);        // test for integer tag
      a.je( &ok );                                     // branch if parent is integer
    } 
    else if (targetMap == Memory->float_map) {
      a.btl( Float_Tag_bit_i386, Temp1);
      // if bit set must be hit, otherwise is mark
      a.jc(&ok);             // branch if parent is a float
    }
    else {                                  // must be mem tag
      a.btl(Mem_Tag_bit_i386, Temp1);
      // if bit set must be hit, otherwise is mark
      a.jnc(&miss);                          // branch if parent is not mem oop

      a.movl(Temp1, map_offset(), NumberOperand, Temp2);    // load receiver map
      a.cmpl((int32)targetMap->enclosing_mapOop(), OopOperand, Temp2); // cmp to map constraint
      a.je(&ok);               // correct
    }
    
    miss.define();
    // This will be backpatched to call an nmethod, so need to leave incoming link alone.
    // Must look like the original call to me plus extra info in regs
    // Pass a link to the branch and nmln in the DILinkReg.
    a.movl(count, NumberOperand, DICountReg);  // count of parents verified
    
    // must align nmln to follow
    // There is a call coming, each 5 bytes, so want pc + 5 to be 0 mod 4.
    // See align * in asmDefs_gcc_i386.[sS]
    // get ic value to pass in, cannot use call cause of backpatching, etc.
    Label next;
    a.call(&next);
    next.define(); 

    // two tasks: compute amount to add to savedPC so it simulates a call: must point to after the jmp below
    // Also, just add enough nops so that nmln after call is word-alligned
    
    fint bytes_from_here_to_after_jmp_before_alignment = 1 /* popl */ + 3 /* addl */ + 5 /* jmp */;
    int32 here = a.offset();
    fint word_fraction_from_here_to_after_jmp_before_alignment = (here + bytes_from_here_to_after_jmp_before_alignment ) & 3;    
    fint num_nops = (BytesPerWord - word_fraction_from_here_to_after_jmp_before_alignment) & (BytesPerWord-1);
    fint bytes_from_here_to_after_jmp = bytes_from_here_to_after_jmp_before_alignment + num_nops;
    
    for (fint i = 0;  i < num_nops; ++i) a.nop();
    

    a.popl(DIInlineCacheReg); // 1 byte, prepare to calculate IC addr below
    a.addl(bytes_from_here_to_after_jmp, NumberOperand, DIInlineCacheReg);  // 3 bytes, finish calc IC addr below
    // following must be parsable to set_target_of_Self_call_site
    a.jmp( (int32)SendDIMessage_stub, DIVMAddressOperand);
    assert( a.offset() == here + bytes_from_here_to_after_jmp, "checking");
    assert((a.offset() & (BytesPerWord-1)) == 0, "must be aligned");
    a.Data(0);                                // first  part of DI nmln
    a.Data(0);                                // second part of DI nmln
    
    a.hlt();
          
    ok.define();
    
    ++count;
    if (l->target->links) count = verifyParents(l->target, Temp1, count);
    
    l = l->next;
    if (l == 0)
      break;
    // if multiple dynamic parents, reload slot holder before looping (HACK!)
    t = loadPath(Temp1, target, LReceiverReg, Temp1);
  }
  
  return count;
}
  
  
void CodeGen::epilogue(Location what) {
  a.Comment("epilogue");
  move(ResultReg, what);
  restore_frame_and_return(0);
}


// factored out of epilogue for continuing NLR, too
void CodeGen::restore_frame_and_return(fint byte_offset) {
  a.Comment("restore_frame_and_return");
  if (haveStackFrame)
    a.leave();

  if (byte_offset != 0)
    a.addl(byte_offset, NumberOperand,  esp, leaf_pc_offset, NumberOperand);

  a.ret();
}



Location CodeGen::flushToStack(Location src, RegisterState* s) {
  return src;
}

   
void CodeGen::reload_ByteMapBaseReg(PrimDesc* p) {
  if (UseByteMapBaseReg && p->canScavenge()) {
    a.leal(no_reg, (int32)&byte_map_base, VMAddressOperand, ByteMapBaseReg);
  }
}



Label* CodeGen::cPrimCall(PrimDesc* p, RegisterState* s,
                          bool continueNLR, bool trust_fns_arg_count, fint arg_and_rcvr_count) {

  a.Comment("cPrimCall");
  Label* where_nlr_jumps_to = NULL;
  // WARNING: following code sequences are known to get_target_of_Self_call_site
  // and set_target_of_Self_call_site
  // Also, getPrimCallEndOffset assumes continuation is right after sequence.

  a.call( (int32) first_inst_addr(p->fn()), PVMAddressOperand );
  
  // inline cache:
  Label past_nlr(a.printing); 
  a.jmp(&past_nlr);  // skip over mask and nlr code
  s->genMask(); // used register mask for GC
   
  if ( p->needsNLRCode() ) {
    if (continueNLR) { // NLR returns from this method, up NLR chain (only for calling intr check after stack overflow & nonLifo trap)
      continueNonLocalReturn();
    } 
    else { // do the NLR bit
      where_nlr_jumps_to = new Label(a.printing);
      a.jmp(where_nlr_jumps_to);
    }
  }
  past_nlr.define(); 

  return where_nlr_jumps_to;
}


Label* CodeGen::primFailure(Location failReceiver, Location self,
                            oop failSelector, oop selector, 
                            Location successLoc, blockOop failBlock,
                            RegisterState* s) {
  a.Comment("primFailure");
  s->allocateArgs(2, true); // will be passing out 2 args; must keep track of these for frame construction
  
  assert(CResultReg == ResultReg, "for next instruction; may be SelfABI or C ABI");
  Location result = CResultReg;
  assert(result != Temp2, "???");
  
  a.movl(result, Temp2);
  a.andl(Tag_Mask, NumberOperand, Temp2);   // test result's tag
  a.cmpl(Mark_Tag, NumberOperand, Temp2);
  Label success(a.printing);
  a.jne(&success);      // jump to success if not

  a.subl( Mark_Tag - Mem_Tag, NumberOperand, result );

  // clone failure block if necessary
  if (failBlock == NULL) {
    // block already exists
    move(ArgLocation(0), result);
  }
  else {
    Location saved = s->pickLocal();
    move(saved, result); // save error string
    loadBlockOop(failReceiver, failBlock, s);
    move(ArgLocation(0), saved);
    s->deallocate(saved);
  }
  
  loadOop(ArgLocation(1), selector); 
  
  // now invoke the fail block
  Label* l = selfCall(s, NormalLookupType, failReceiver, self, 
                      failSelector, NULL, 2);
  
  assert(ResultReg == CResultReg, "exploit move after success.define");    
  
  success.define();
  move(successLoc, CResultReg);       // move result to right place

  return l;
}

  
void CodeGen::recordStore(Location dst) {
  // NB: when fixed to use register revisit call sites
  a.Comment("recordStore");
  assert(isRegister(dst), "receiver to check_store must be in a register");
  a.shrl(card_shift, NumberOperand, dst); // shift target addr
  a.addl(no_reg, (int32)&byte_map_base, VMAddressOperand, dst); // add start of map to dst
  a.movb(0, dst, 0, VMAddressOperand);
}
  
  
Label* CodeGen::selfCall(RegisterState* s, LookupType lookupType,
                         Location receiver, Location self,
                         oop selector, oop delegatee, fint argc) {

  a.Comment("selfCall");
  Unused(self);

  move( ReceiverReg, receiver, Temp1);

  // WARNING: following code sequences are known to get_target_of_Self_call_site
  // and set_target_of_Self_call_site
  
  // must align nmln to follow
  // There are a call and two jumps coming, each 5 bytes, so want pc to be 1 mod 4.
  // See align * in asmDefs_gcc_i386.[sS]
  fint x = ((fint)a.offset() - 1) & 3;
  fint num_nops = (4 - x) & 3;
  for (fint i = 0;  i < num_nops; ++i) a.nop();
  a.call((int32)SendMessage_stub, BPVMAddressOperand);
  Label* l = SendDesc(s, lookupType, selector,
                      delegatee ? oop(delegatee) : oop(badOop));
  return l;
}
  
Label* CodeGen::perform(RegisterState* s, LookupType lookupType,
                        Location receiver, Location self, fint argc,
                        oop delegatee) {
  a.Comment("_Perform Primitive");
  return selfCall(s, lookupType, receiver, self,
                  as_smiOop(argc), delegatee, argc);
}


void CodeGen::assignment(Location receiver, slotDesc* s, Location val) {
  // this one called for impl self, arg on stack
  a.Comment("Begin Simple Assignment");
  assert(!isRegister(receiver) && !isRegister(val), 
    "called from genReceiverDataAccess, rcvr is self, val is arg or local, check this so I can use Temp[12]");
  move(Temp2, receiver);
  int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
  a.leal(Temp2, offset, NumberOperand, Temp2); // store object data slot contents
  move(Temp1, val);
  a.movl(Temp1, Temp2, 0, NumberOperand); // store object data slot contents
  recordStore(Temp2); // NB: recordStore will clobber Temp2 till I put in ByteMapBaseReg
  a.Comment("End Simple Assignment");
}


void CodeGen::assignment(Location receiver,
                         realSlotRef* path, Location val,
                         bool isMem) {
  a.Comment("Begin Assignment");
  // called from assignmentCode; real assignment method
  int32 offset = smiOop(path->desc->data)->byte_count() - Mem_Tag;
  if (path->holder->is_object_or_map()) {
    Location tmp = val == Temp1 ? Temp2 : Temp1;
    bool yipes = tmp == receiver;
    if (yipes) a.pushl(val);
    
    Location tr = loadPath(tmp, path->holder, receiver, tmp);
    a.leal(tr, offset, NumberOperand, tr);

    Location r; int32 d; OperandType t;
    Location other_temp = tr == Temp1 ? Temp2 : Temp1;
    if (yipes) a.popl(other_temp);
    else if (val != other_temp) {
      reg_disp_type_of_loc(&r, &d, &t, val);
      a.movl(r, d, t, other_temp);
    }
    a.movl(other_temp, tr, 0, NumberOperand);

    if (isMem) {
      recordStore(tr);
    }
  } else {
    fatal("don't support vframe lookups yet");
  }
  a.Comment("End Assignment");
}

void CodeGen::loadBlockParent(Location block, Location dst) {
  a.Comment("loadBlockParent");
  // load [block+scope_offset], dst
  block = moveToReg(block, dst);
  a.movl(block, scope_offset(), NumberOperand, dst);
}


void CodeGen::loadSaved(Location dest, Location src,
                        Location frames_bp, compiled_vframe* src_vf) {
  a.Comment("loadSaved");
  // Load location from a frame on the stack; sp is that frame's 
  // block home (BP on Intel) and frameSz its size.
  assert(isRegister(frames_bp), "???");
  fint frameSz = src_vf->code->frameSize();
  Location r; int32 d; OperandType t;
  reg_disp_type_of_loc(&r, &d, &t, src);
  assert(t == NumberOperand, "???");
  Location tr = isRegister(dest) ? dest : Temp1;
  // I don't think the following untested case would ever happen, but put the code in anyway. -- dmu 5/06
  # if GENERATE_DEBUGGING_AIDS
    if (r != ebp  &&  CheckAssertions) warning("untested loadSaved from outgoing arg");
  # endif
  a.movl(frames_bp, d - (r == ebp ? 0 : (frameSz - linkage_area_size) * oopSize), NumberOperand,  tr);
  move(dest, tr);
}

  
void CodeGen::storeSaved(Location         dst,    Location frames_bp,
                              compiled_vframe* dst_vf, Location src) {
  a.Comment("storeSaved");
  assert(isRegister(frames_bp), "???");
  fint frameSz = dst_vf->code->frameSize();
  Location r;  int32 d;  OperandType t;
  reg_disp_type_of_loc(&r, &d, &t, dst);
  assert(t == NumberOperand, "???");
  Location tr = isRegister(src) ? src 
             :  isRegister(dst) ? dst
             : frames_bp == Temp1 ? Temp2 : Temp1;
  if (tr == src) a.Untested("storeSaved1");
  move(tr, src);
  // I don't think the following untested case would ever happen, but put the code in anyway. -- dmu 5/06
  # if GENERATE_DEBUGGING_AIDS
    if (r != ebp  &&  CheckAssertions) warning("untested loadSaved from outgoing arg");
  # endif
  a.movl(tr, frames_bp,  r == ebp ?  d  :  d - (frameSz - linkage_area_size) * oopSize, NumberOperand);
}

#ifdef UNUSED
void CodeGen::loadSender(Location dest, Location sp) {
  // load frame's sender
  a.movl(ebp, dest);
}
#endif

  
void CodeGen::lookup(Location dest, realSlotRef* path, Location receiver) {
  a.Comment("lookup");
  // <loadPath dest, path, receiver>
  // load receiver/dest/t, offset - Mem_Tag, dest/t
  // <move t, dest>
  
  // Since called from CodeGen::loadPath, cannot clobber Temp2
    
  assert(isRegister(dest), "is always a register");

  if (path->holder->is_object_or_map()) {
    Location t1 = loadPath(dest, path->holder, receiver, Temp1);
    // load data slot
    a.movl( t1, smiOop(path->desc->data)->byte_count() - Mem_Tag, NumberOperand, dest);
  } else {
    fatal("don't support vframe lookups");
  }
}
  
      
// load NLR registers
void CodeGen::prepareNLR(Location result, Location scope, smi homeID) {
  // <move scope, NLRHomeReg>
  // <loadImmediate homeID, NLRHomeIDReg>
  // <move result, NLRResultReg>
  a.Comment("prepareNLR");
  assert(result != NLRHomeReg, "checking clobbering");
  move(NLRHomeReg, scope);
  move(NLRResultReg, result);
  a.movl(homeID, NumberOperand, NLRHomeIDReg);
}


void CodeGen::testAndContinueNLR(smi homeID) {
  a.Comment("testAndContinueNLR");
  Label cont(a.printing);
  Label doNLR(a.printing);
  if (homeID) {       // note: will be 0 if no inlining
    a.Untested("CodeGen::testAndContinueNLR1"); // need SIC to test this
    a.cmpl( homeID, NumberOperand, NLRHomeIDReg);
    a.jne(&doNLR);
  }
  # if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      // frames actually quad word + 8 aligned, so use xor trick
      a.testl(NLRHomeReg, NLRHomeReg);
      Label ok(a.printing);
      a.jz(&ok);
      a.xorl(frame_alignment_offset * oopSize, NumberOperand, NLRHomeReg);
      a.testl(frame_word_alignment*oopSize - 1, NumberOperand, NLRHomeReg);
      a.je(&ok);
      a.hlt(); // bad NLRHome
      a.Data((int32)"bad NLRHome", true);
      ok.define();
      a.xorl(frame_alignment_offset * oopSize, NumberOperand, NLRHomeReg);
    }
  # endif
  a.cmpl(ebp, NLRHomeReg);
  a.je(&cont);
  doNLR.define();
  continueNonLocalReturn();
  cont.define();
  epilogue(NLRResultReg);
}

// continue NLR (return through caller's inline cache)
void CodeGen::continueNonLocalReturn() {
  a.Comment("continueNonLocalReturn");
  assert(haveStackFrame, "should have stack frame");
  restore_frame_and_return(sendDesc::non_local_return_offset);
}


void CodeGen::zapBlock(Location block, bool memoized) {
  // <move block, t>
  // *if memoized
  //    subcc block/t, 0, g0
  //    bne,a _done           // do work in delay slot
  // *end
  // store block/t, scopeOffset, G0
  // _done: ...
  
  Location t;
  a.Comment("zap block");
  if (isRegister(block)) {
    t = block;
  } else {
    t = NLRTempReg;
    assert(t != block  &&  t != NLRResultReg  && t != NLRHomeReg && t != NLRHomeIDReg, "");
  }
  move(t, block, no_reg, memoized); // set cc if memoized: was block generated?
  Label done;
  if (memoized)
    a.je(&done);     // br if no block, otherwize zap, the no block case probably likely
  a.movl(0, NumberOperand, t, scope_offset(), NumberOperand);    // zap the block
  done.define();
}

  
Label* CodeGen::SendDesc(RegisterState* s, LookupType lookupType,
                         oop selector, oop delegatee) {
  a.Comment("begin SendDesc");
  Label past_send_desc(a.printing);
  a.jmp(&past_send_desc);
  s->genMask();                               // mask of used regs
  Label* l = new Label(a.printing);
  a.jmp(l);                      // non-local return code
  assert((a.offset() & Tag_Mask) == 0, "must be aligned");
  a.Zero();   // nmlns
  a.Zero();
  if (selector != badOop) {
    if (isPerformLookupType(lookupType)) {
      assert_smi(selector, "should be an integer argcount");
      a.Data(smiOop(selector)->value(), true);      // really arg count
    } else {
      assert_string(selector, "should be a string constant");
      a.Data(selector, true);                       // constant selector
    }
  }
  
#   ifdef SIC_COMPILER
  if (theCompiler->containsLoop) {
    // need counters for the sends to know how often the loop executes
    a.Data(withCountBits(lookupType, Counting), true);
  } 
  else {
    a.Data(lookupType, true);
  }
#   else
    a.Data(lookupType, true);
#   endif
  
#   if GENERATE_DEBUGGING_AIDS
  if (CheckAssertions)
    switch (lookupType) {
     case DirectedResendLookupType:
      assert(lookupType & DelegateeStaticBit, "should have static delegatee");
      assert_string(delegatee, "should be a string");
      // fall through
     case ImplicitSelfLookupType:
     case ResendLookupType:
     case StaticNormalLookupType:
     case NormalLookupType:
      assert(!isPerformLookupType(lookupType),
             "should have a static selector");
      assert_string(selector, "should be a string");
      break;
     default: break;
    }
#   endif
  if (delegatee != badOop) {
    assert(needsDelegatee(lookupType), "shouldn't have a delegatee");
    a.Data(delegatee, true);
  }
  past_send_desc.define();
  a.Comment("end SendDesc");
  return l;
}


Label* CodeGen::branch() {
  a.Comment("branch");
  Label* L = new Label(a.printing);
  a.jmp(L);
  return L;
}


Label* CodeGen::unconditionalBranchCode( Label* dst,  
                                         bool allowPreemption, 
                                         RegisterState* s) {
  //    if allowPreemption <cPrimCall interruptCheck>
  //    bra,a dst
  
  // (this code lifted from what used to be restart)
  
  a.Comment("unconditionalBranchCode");
  Label* nlr = NULL;
  if ( !allowPreemption) { 
    nlr = NULL;
  } 
  else {
    Label* dst1 = NULL;
    testStackOverflowForLoop(dst1, nlr, s); // allocs label and sets dst1
    dst->unify( dst1 );
  }
  a.jmp(dst);
  return nlr;
}


Label* CodeGen::conditionalBranchCode( Location testMe, 
                                       oop target, 
                                       Label* dst, 
                                       bool allowPreemption,  
                                       RegisterState* s) {
  //   <loadOop target_oop t1>
  //   <move    testMe     t2>
  //   cmp  t1, t2
  //   if no allowPreemption:
  //     beq  dst
  //     nop
  //   if allowPreemption:
  //     bne  end
  //     <unconditionalBranch dst>
  //     end:
   
  a.Comment("conditionalBranchCode");
  move(Temp1, testMe);
  a.cmpl((int32)target, OopOperand, Temp1);

  if ( !allowPreemption ) {
    a.je(dst);
    return NULL; // no nlr
  }
  Label end;
  a.jne(&end);
  Label* nlr = unconditionalBranchCode( dst, true, s);
  end.define();
  return nlr;
}


Label* CodeGen::indexedBranchCode( Location        testMe, 
                                   LabelList*      labels,
                                   bool            allowPreemption,
                                   RegisterState*  s) {
  // if allowPreemption
  //   <testStackOverflowForLoop( afterTest ) >
  // afterTest:
  // 
  //   <move testMe, t1>
  //   <loadOop smiOop of label length, t2>
  //   tcmp t2, t1
  //   bvs end
  //   nop
  //   bleu end
  //   nop
  //   call .+8
  //   add t1, o7, t1
  //   jump t1, 12, g0
  //    bra L1
  //    bra L2
  //    ...
  // end:
  
  a.Comment("indexedBranchCode");
  Label* nlr= NULL;
  if (allowPreemption) {
    // Ideally would only do this in the arms that actually do
    // branch back.
    // Beware: the stack overflow test could call other code that
    // could clobber Temp1 and Temp2, so it cannot be in the
    // middle somewhere. -- dmu
    Label* afterTest = NULL;
    testStackOverflowForLoop( afterTest, nlr, s ); // sets afterTest
    afterTest->define();
  }
  
  move(Temp1, testMe);
  
  a.testl( Tag_Mask, NumberOperand, Temp1);
  Label end;
  a.jne( &end ); // goto end if not int
  a.cmpl((int32)as_smiOop(labels->length()), OopOperand, Temp1);   // use unsigned comp to catch negative number
  a.jnb(&end); // goto end if index is out of bounds, if Temp1 is not below Temp2
  
  Label L(a.printing);
  a.call(&L); // get pc of next inst onto stack
  pc_t link_reg_value= a.addr();
  L.define();
  
  const int32 indexShift = 2 - Tag_Size;
  assert(indexShift == 0, "no shift needed");
  
  a.popl(Temp2);
  const int32 bytes_from_link_value_to_jumps = 7;
  // indexing is by_two because index is ALREADY 4x value because of tagging
  a.leal(Temp2,  bytes_from_link_value_to_jumps, NumberOperand, Temp1, Assembler::by_two, Temp2);

  a.jmp(Temp2);

  pc_t start_of_jumps= a.addr();
  assert( bytes_from_link_value_to_jumps == start_of_jumps - link_reg_value, "recount");

  for (fint i = 0;  i < labels->length(); ++i) {
    a.jmp(labels->nth(i)); a.hlt(); a.hlt(); a.hlt(); // 8 bytes
  }
  pc_t end_of_jumps= a.addr();
  assert(end_of_jumps - start_of_jumps == 8 * labels->length(), "8?");
  end.define();
  return nlr;
}


void CodeGen::loadImmediate(Location dest, int32 value) {
  a.Comment("loadImmediate");
  Location r; int32 d; OperandType t;
  reg_disp_type_of_loc(&r, &d, &t, dest);
  a.movl(value, NumberOperand, r, d, t);
}


// -1 = recvr, 0 = first arg
// move data from "from" to outgoing arg location for argNo
void CodeGen::loadArg(fint argNo, Location from, bool isPrimCall) {
  a.Comment("loadArg");
  Unused(isPrimCall); // no diff
  move(ArgLocation(argNo), from, Temp1);
}


void CodeGen::loadOop(Location dest, Location src, slotDesc* s) {
  a.Comment("loadOop1");
  int32 offset = smiOop(s->data)->byte_count() - Mem_Tag;
  Location t  = isRegister(src)  ? src  :  dest != Temp1 ? Temp1 : Temp2;
  Location tt = isRegister(dest) ? dest :  src  != Temp1 ? Temp1 : Temp2;
  move(t, src);
  a.movl(t, offset, NumberOperand, tt);
  move(dest, tt);
}


void CodeGen::loadOop(Location dest, oop p) {
  a.Comment("loadOop2");
  Location r; int32 d; OperandType t;
  reg_disp_type_of_loc(&r, &d, &t, dest);
  a.movl((int32)p, OopOperand, r, d, t);
}


void CodeGen::loadBlockOop(Location dest, slotsOop p, RegisterState* s) {

  //   <loadOop p, rr>
  //   <move sp, arg1>
  //   <cPrimCall BlockClone>
  //   <move rr, dest>
  
  a.Comment("Begin Block Cloning");
  s->allocateArgs(1, true);                   // track args for frame construction    

  move(ArgLocation(0), ebp);                 // load frame oop for block
  loadOop(ReceiverReg, p);                    // load block to clone
  PrimDesc* pd = blockClone();
  assert(! pd->needsNLRCode(), "rewrite this - must backpatch NLR code");
  Label* l = cPrimCall(pd, s, false, true, 2 /* 1 arg + rcvr */);
  assert(l == NULL, "shouldn't need a label");
  move(dest, CResultReg);
}


void CodeGen::nonLifoTrap(RegisterState* s) {
  a.Comment("nonLifoTrap");
  static PrimDesc* non_lifo_abort = NULL;
  if (non_lifo_abort == NULL)
    non_lifo_abort = getPrimDescOfFunction(
                       fntype(&NLRSupport::non_lifo_abort), 
                       true);
      
  Label next(a.printing);
  a.call(&next); // prim needs some PC in this method
  next.define();
  a.popl(Temp1);
  move(ReceiverReg, Temp1);
  s->allocateArgs(0, true); // for receiver

  Label* nlr_dest = cPrimCall(non_lifo_abort, s, true, true, 1);
  assert(nlr_dest == NULL, "should not need a label");
}


void CodeGen::initialize_for_platform() {
  
  // _max_no_of_outgoing_args_and_rcvr        = 0;
  _number_of_memory_locals                 = 0;
  
  reg_disp_type_of_loc(&leaf_rcvr_base, &leaf_rcvr_disp, &leaf_rcvr_type, LReceiverReg);
}

  

void CodeGen::assignmentCode(realSlotRef* dataRef) {
  prologue(true, 0 );
  move(Temp1, LArgLocation(0));
  assert(!haveStackFrame, "LReceiverReg only for no frame");
  assignment(LReceiverReg, dataRef, Temp1);
  epilogue(LReceiverReg);
}  



void CodeGen::move(Location dest, Location src, Location tempReg, bool setcc) {
  Location b;  int32 d; OperandType t;    
  if (dest == src) {
    if (setcc) {
      reg_disp_type_of_loc(&b, &d, &t, src);
      a.testl(AllBits, NumberOperand, b, d, t);
    }
  }
  else if (isRegister(dest)) {
    reg_disp_type_of_loc(&b, &d, &t, src);
    a.movl(b,d, t, dest);
    if (setcc)  a.testl(dest, dest);
  }
  else if (isRegister(src)) {
    reg_disp_type_of_loc(&b, &d, &t, dest);
    if (setcc)  a.testl(src, src);
    a.movl(src, b,d, t);
  }
  else if (!isRegister(tempReg)) {
    reg_disp_type_of_loc(&b, &d, &t, src);
    a.pushl(b, d, t);
    reg_disp_type_of_loc(&b, &d, &t, dest);
    a.popl(b, d, t);
    if (setcc) a.testl(AllBits, NumberOperand, b, d, t);
  }
  else {
    reg_disp_type_of_loc(&b, &d, &t, src);
    a.movl(b, d, t, tempReg);
    reg_disp_type_of_loc(&b, &d, &t, dest);
    if (setcc) a.testl(tempReg, tempReg);
    a.movl(tempReg, b, d, t);
  }
}


inline Location CodeGen::moveToReg(Location what, Location reg) {
  Location t;
  if (isRegister(what)) {
    t = what;
  } else {
    t = reg;
    move(t, what);
  }
  return t;
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerState_i386.hh"
# include "_registerState_i386.cpp.incl"

# ifdef FAST_COMPILER

# undef  MAX
# define MAX ((maxDepth >> LogBitsPerWord) + 1)


void RegisterState::initialize_for_platform(fint /*maxTemps*/) {
  rcvrDepth = 0;
}


void RegisterState::genMask() {
  RegisterString m = mask();
  theAssembler->Data(m);
}


RegisterString RegisterState::mask() {
  assert(sizeof(allocated) == oopSize, "");
  return stackAllocs[0];
}


Location RegisterState::pickLocal() {
  // do not currently use registers, just go to stack
  Location r = pickStackTemp();
  allocate(r);
  return r;
}


void RegisterState::allocateArgs(fint nargs, bool isPrimCall) {
  Unused(isPrimCall);
  argDepth = max(argDepth, nargs);
  rcvrDepth = 1;
}


void RegisterState::allocate(Location r) {
  if (isRegister(r)) {
    ::allocateRegister(allocated, r);
  } 
  else if (is_StackLocation(r)) {
    assert(!isSet(stackAllocs[whichMask(r)], whichBit(r)),
           "already allocated");
    setNth(stackAllocs[whichMask(r)], whichBit(r));
    fint tempNo = index_for_StackLocation(r) + 1;
    if (tempNo >= stackDepth) 
      stackDepth = tempNo;
    curDepth++;
    assert(curDepth <= stackDepth, "curDepth too big");
  } 
}
  

void RegisterState::deallocate(Location r) {
  if (isRegister(r)) {
    ::deallocateRegister(allocated, r);
    allocated = ::allocate(allocated, permanent); // ensure we do not deallocate a permanent reg
  } 
  else if (is_StackLocation(r)) {
    if (isSet(stackPerms[whichMask(r)], whichBit(r))) {
      // permanent -- don't deallocate
    } 
    else {
      assert(isSet(stackAllocs[whichMask(r)], whichBit(r)), "not allocated");
      clearNth(stackAllocs[whichMask(r)], whichBit(r));
      --curDepth;
      assert(curDepth >= 0, "negative depth");
    }
  } 
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# pragma implementation "fcompiler_i386.hh"
# include "_fcompiler_i386.cpp.incl"

# ifdef FAST_COMPILER
 

fint   FCompiler::number_of_memory_locals() {
  return codeGen->number_of_memory_locals();
}


# endif // FAST_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerString_i386.hh"

# include "_registerString_i386.cpp.incl"

# ifdef FAST_COMPILER

void printRegister(Location r) {
  fatal("Unused Intel -- see Assembler::printR");
}


RegisterString registerMaskBit(Location l, fint stackLocs, fint nonRegisterArgs) {
  return is_StackLocation(l)  ?  nthBit(index_for_StackLocation(l)) : 0;
}

# endif // FAST_COMPILER
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation  "runtime_i386.hh"

# include "_runtime_i386.cpp.incl"

// Follows is a list of potential improvements to my I386 port of Self as of 5/16/06:
// - Could use the SIMD instructions to speed searches for senders, implementors, etc. (search_i386.cpp).
// - Could implement deferred compilation of uncommon branches for more speed.
// - Could implement interpreter glue, to allow experiments with the interpreter.
// - Could optimize the SIC better for this platform; it seems to generate unneeded moves:
//    e.g. desktop close top-level method moves eax to esi and back to eax.
// Could implement compiled_vframe::sendeeOrNULL_for_get_expr_stack to improve recompilation accuracy.
// Could make a universal binary for convenience.
// Could get the Carbon UI to work in the debug version (cannot pull UI window to front right now).
//   (But the X version does work.)
// Could add explicit info in the Snapshot so would not have to heuristally determin endian-ness.
//
// There is a very intermittent bug when doing "comparatorTests doTests testShortLines".
// I have seen this once in the debug version when running "tests testVMSuite. tests testSelfSuite".




bool check_saved_byte_map_base() {
  // nothing for intel
  return true;
} 


// cannot used simple DiscardStack (as in runtime_sparc.[sS]) because of restoring
// nonvol registers
void volatile DiscardStack() {
  currentProcess->abort();
}


char* adjust_initial_SP(char* init_SP) {
  int r = (int)init_SP - oopSize; // incase we want to push a parameter
  r -= frame_alignment_offset * BytesPerWord;
  return (char*)(r & ~(frame_word_alignment * BytesPerWord  -  1)) + frame_alignment_offset * BytesPerWord; // apple docs says 16-byte alignment
}


// define byte_map_base if not in a register

char* byte_map_base; // also use a register, too



// define SPLimit if not kept in a register:

char* SPLimit = NULL;
extern "C" {void set_SPLimitReg(char*); }
void  setSPLimit(char* m)        { 
  SPLimit = m; 
}

char* currentSPLimit()           { return SPLimit; }

// some machines do not need the following in asm:
extern "C" oop breakpoint_prim(oop rcvr) { 
  return rcvr; 
}


// This should be an assembly function, similar to sparc/runtime/runtime_asm_gcc_sparc.[sS].
// I haven't implemented it yet, so I just put the stub here. -mabdelmalek 10/02.
void  HandleUncommonTrap()
{
  fatal("uncommon traps unimplemented on i386");
}



// Not implemented on Intel:
char* DIRecompile_stub_returnPC = NULL;



oop failure_oop_for_restarting_uncommon_prim() {
  assert(SaveOutgoingArgumentsOfPatchedFrames, "I386 needs this");
  if (SpendTimeForDebugging)
    warning("untested: failure_oop_for_restarting_uncommon_prim");
  return OutgoingArgsOfReturnTrapOrRecompileFrame->obj_at(0);
}


void fillRegisterValue(Location loc, oop b) {
  if (SpendTimeForDebugging)
    warning("untested: fillRegisterValue");
  OutgoingArgsOfReturnTrapOrRecompileFrame->obj_at_put(loc - ReceiverReg, b);
}




void set_flags_for_platform() {
  LogVMMessages                        = true;   lprintf("for I386:  LogVMMessages = true\n");
  PrintScriptName                      = true;   lprintf("for I386:  PrintScriptName  = true\n");
  Inline                               = true;   lprintf("for I386:  Inline = true\n");
  # ifdef SIC_COMPILER
    SICDeferUncommonBranches             = false;  lprintf("for I386:  SICDeferUncommonBranches = false (not implemented)\n"); 
    SICReplaceOnStack                    = false;  lprintf("for I386:  SICReplaceOnStack = false (not implemented)\n");
  # endif
  SaveOutgoingArgumentsOfPatchedFrames = true;   lprintf("for I386:  SaveOutgoingArgumentsOfPatchedFrames = true\n");
}

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.3 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "aCompiler_i386.hh"
# include "_aCompiler_i386.cpp.incl"

# if  defined(FAST_COMPILER) || defined(SIC_COMPILER)



# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "framePieces_i386.hh"

# include "_framePieces_i386.cpp.incl"


i386_sp* i386_sp::push_new_sp( char* pc,
                             fint size_in_oops,
                             bool zapAlways ) {
  assert((size_in_oops & (frame_word_alignment - 1)) == 0, "alignment");
  assert( ((frame*)this)->is_aligned(), "alignment");
  i386_sp* sp = (i386_sp*)( ((oop*)this) - size_in_oops );
# if GENERATE_DEBUGGING_AIDS
    if (CheckAssertions) {
      zapAlways = true;
    }
# endif
  if (zapAlways)
    // If errorObj starts showing up where it shouldn't, I recommend
    // substituting 0xfffffffe for it below: (dmu)
    set_oops(sp->as_oops(), size_in_oops, (oop)Memory->errorObj);
  sp->set_link(this);
  sp->set_return_addr(pc);
  assert( ((frame*)sp)->is_aligned(), "alignment");
  return sp;
}
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "registerLocator_i386.hh"

# include "_registerLocator_i386.cpp.incl"


void RegisterLocator::update_addresses_from_self_frame() {}


void RegisterLocator::update_addresses_from_VM_frame() {
}




RegisterLocator* RegisterLocator::for_frame(frame* f) {
  RegisterLocator* r = new RegisterLocator;
  r->my_frame = f;
  return r;
}
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "uncommonBranch_i386.hh"

# include "_uncommonBranch_i386.cpp.incl" 


# if defined(SIC_COMPILER)

bool shouldRestartSend(int32* instp) {
  Unused(instp);
  fatal("unimp intel"); // do uncommon branches?
  return false;
}

unsigned trapCount(int32* instp) {
  Unused(instp);
  fatal("unimp intel");
  return 0;
}

void setTrapCount(int32* instp, unsigned count) {
  Unused(instp); Unused(count);
  fatal("unimp intel");
}

# endif


bool isMapLoad(int* instp) {
  fatal("Unused Intel");  return 0; 
}


void handleMapLoadTrap(InterruptedContext* c) {
  Unused(c);
  fatal("Unused Intel");
}
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.6 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include "_interruptedCtx_i386.cpp.incl"

char* InterruptedContext::next_pc(){ return NULL; }
void InterruptedContext::set_next_pc(void *) {}


void InterruptedContext::set_continuation_address(char *addr, bool mustWork, bool setSema) {
  assert(!continuePC, "continuePC already set");
  if (setSema) processSemaphore = true; 
  Unused(mustWork);
  the_interrupted_context->set_pc(addr);
}


bool InterruptedContext::in_system_trap() {
  if (scp == &dummy_scp)
    return false;
  // system call, sysenter instruction
  return pc()[0] == '\x0f'  &&  pc()[1] == '\x34';
}


int InterruptedContext::system_trap() {
  return int(NULL); // unimp for now
}


// used to be named patchMaxSP, but it seems to do what setupPreemption does,
// only with extra SPARC stuff and it's called within signal handling -- dmu 1/96

void InterruptedContext::setupPreemptionFromSignal() {
  // SPLimit is a global, not a register
  currentProcess->setupPreemption();
}


# if TARGET_OS_VERSION == MACOSX_VERSION  &&  OSX_RELEASE == TIGER_RELEASE
  # define __ss  ss // after TIGER use __ss

  # define __eip eip

  # define __esp esp
  # define __ebp ebp

  # define __eax eax
  # define __ebx ebx
  # define __ecx ecx
  # define __edx edx

  # define __esi esi
  # define __edi edi

  # define __eflags eflags

  # define __cs cs
  # define __ds ds
  # define __es es
  # define __fs fs
  # define __gs gs
# endif

# if TARGET_OS_VERSION == MACOSX_VERSION

  char** InterruptedContext::pc_addr() {
  return  (char**) &scp->uc_mcontext->__ss.__eip; // see /usr/include/mach/i386/thread_status.h
  }
  int* InterruptedContext::sp_addr() {
  return  (int*) &scp->uc_mcontext->__ss.__esp;
  }
  int* InterruptedContext::ebp_addr() {
  return  (int*) &scp->uc_mcontext->__ss.__ebp;
  }

# elif TARGET_OS_VERSION == LINUX_VERSION

  char** InterruptedContext::pc_addr() {
    return  (char**) &scp->uc_mcontext.gregs[REG_EIP]; // see /usr/include/asm-i386/sigcontext.h
  }
  int* InterruptedContext::sp_addr() {
    return  (int*) &scp->uc_mcontext.gregs[REG_ESP];
  }
  int* InterruptedContext::ebp_addr() {
    return  (int*) &scp->uc_mcontext.gregs[REG_EBP];
  }

# elif TARGET_OS_VERSION == SOLARIS_VERSION

char** InterruptedContext::pc_addr() {
  return (char**) &((ucontext_t*) scp)->uc_mcontext.gregs[REG_PC];
}
int* InterruptedContext::sp_addr() {
  return (int*) &((ucontext_t*) scp)->uc_mcontext.gregs[REG_SP];
}
int* InterruptedContext::ebp_addr() {
  return (int*) &((ucontext_t*) scp)->uc_mcontext.gregs[REG_FP];
}

# else
  # error What OS version?
# endif

void InterruptedContext::print_registers() {
  InterruptedContext* ic = 
    the_interrupted_context && the_interrupted_context->is_set()  ?  the_interrupted_context  :
   (AbortContext.is_set() ? &AbortContext : NULL);
   
  if (ic == NULL) {
    lprintf("context is not set\n");
    return;
  }
  # if TARGET_OS_VERSION == MACOSX_VERSION

  i386_thread_state_t* ssp = &ic->scp->uc_mcontext->__ss;
  lprintf("eax       = 0x%x\n", ssp->__eax);
  lprintf("ebx       = 0x%x\n", ssp->__ebx);
  lprintf("ecx       = 0x%x\n", ssp->__ecx);
  lprintf("edx       = 0x%x\n", ssp->__edx);
  lprintf("esi       = 0x%x\n", ssp->__esi);
  lprintf("edi       = 0x%x\n", ssp->__edi);
  lprintf("ebp       = 0x%x\n", ssp->__ebp);
  lprintf("esp       = 0x%x\n", ssp->__esp);
  lprintf("ss        = 0x%x\n", ssp->__ss);
  lprintf("eflags    = 0x%x\n", ssp->__eflags);
  lprintf("eip       = 0x%x\n", ssp->__eip);
  lprintf("cs       = 0x%x\n", ssp->__cs);
  lprintf("ds       = 0x%x\n", ssp->__ds);
  lprintf("es       = 0x%x\n", ssp->__es);
  lprintf("fs       = 0x%x\n", ssp->__fs);
  lprintf("gs       = 0x%x\n", ssp->__gs);

  # elif TARGET_OS_VERSION == LINUX_VERSION

    greg_t *gregs = ic->scp->uc_mcontext.gregs;
    lprintf("eax       = 0x%x\n", gregs[REG_EAX]);
    lprintf("ebx       = 0x%x\n", gregs[REG_EBX]);
    lprintf("ecx       = 0x%x\n", gregs[REG_ECX]);
    lprintf("edx       = 0x%x\n", gregs[REG_EDX]);
    lprintf("esi       = 0x%x\n", gregs[REG_ESI]);
    lprintf("edi       = 0x%x\n", gregs[REG_EDI]);
    lprintf("ebp       = 0x%x\n", gregs[REG_EBP]);
    lprintf("esp       = 0x%x\n", gregs[REG_ESP]);
    lprintf("ss        = 0x%x\n", gregs[REG_SS]);
    lprintf("efl       = 0x%x\n", gregs[REG_EFL]);
    lprintf("eip       = 0x%x\n", gregs[REG_EIP]);
    lprintf("cs        = 0x%x\n", gregs[REG_CS]);
    lprintf("ds        = 0x%x\n", gregs[REG_DS]);
    lprintf("es        = 0x%x\n", gregs[REG_ES]);
    lprintf("fs        = 0x%x\n", gregs[REG_FS]);
    lprintf("gs        = 0x%x\n", gregs[REG_GS]);
    lprintf("trapno    = 0x%x\n", gregs[REG_TRAPNO]);
    lprintf("err       = 0x%x\n", gregs[REG_ERR]);
    lprintf("uesp      = 0x%x\n", gregs[REG_UESP]);
  
# elif TARGET_OS_VERSION == SOLARIS_VERSION
  
  greg_t *gregs = ((ucontext_t*) ic->scp)->uc_mcontext.gregs;
  lprintf("eax       = 0x%x\n", gregs[EAX]);
  lprintf("ebx       = 0x%x\n", gregs[EBX]);
  lprintf("ecx       = 0x%x\n", gregs[ECX]);
  lprintf("edx       = 0x%x\n", gregs[EDX]);
  lprintf("esi       = 0x%x\n", gregs[ESI]);
  lprintf("edi       = 0x%x\n", gregs[EDI]);
  lprintf("ebp       = 0x%x\n", gregs[EBP]);
  lprintf("esp       = 0x%x\n", gregs[ESP]);
  lprintf("ss        = 0x%x\n", gregs[SS]);
  lprintf("efl       = 0x%x\n", gregs[EFL]);
  lprintf("eip       = 0x%x\n", gregs[EIP]);
  lprintf("cs        = 0x%x\n", gregs[CS]);
  lprintf("ds        = 0x%x\n", gregs[DS]);
  lprintf("es        = 0x%x\n", gregs[ES]);
  lprintf("fs        = 0x%x\n", gregs[FS]);
  lprintf("gs        = 0x%x\n", gregs[GS]);
  lprintf("trapno    = 0x%x\n", gregs[TRAPNO]);
  lprintf("err       = 0x%x\n", gregs[ERR]);
  lprintf("uesp      = 0x%x\n", gregs[UESP]);

  # else
	# error What OS?
  # endif
  lprintf("\n");
}

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "conversion_i386.hh"
# include "_conversion_i386.cpp.incl"


  
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


  void Conversion::fix_new_vfs() { }
  
  
  void Conversion::fixConversionStack_for_vframe_conversion() {
    fixConversionStack(sd->return_pc(), sp);
  }


  bool Conversion::createFrame(fint i, nmethod *newNM) {    
    // create new frame, store it in newFr and initialize it
    // XXXX what if fr is interp frame?

    // this routine seems to use sd and sp,
    //  and to set sd, sp, and newFr

    // figure out new value for sd and if isInInterruptCheck

    sendDesc* prev_sd= vf[i]->fr->send_desc();

    // Have to be careful when converting a frame at the bottom of the
    // stack which has just returned from interruptCheck, after a process
    // switch, or while single stepping -- otherwise the next send is
    // omitted.  MIW 6/8/94

    bool isInInterruptCheck=
              prev_sd // eliminate uncommon branches
          && prev_sd->isPrimCall()
          && prev_sd->jump_addr() == first_inst_addr(interruptCheck);

    assert(vf[i]->fr->is_aligned(), "frame alignment check");
    sendDesc* sd_of_created_frame = newNM->sendDescFor(vf[i], isInInterruptCheck);    
  
    assert( i == 1   ||  newVF[i-1],  "Mac has no null check");
    
    // Intel frames contain their own return addresses, so use sd_of_created_frame
    i386_sp*   newSP = ((i386_sp*)  sp)->push_new_sp( sd_of_created_frame->return_pc(),
                                                    newNM->frameSize(),
                                                    true );
    if (stk->isStackOverflow(newSP)) {
      fatal("stack overflow while converting stack frame");
    }
    sp = (char*)newSP;
    
    newFr = (frame*)newSP;
    rlFr  = NULL;
    newFrRl = RegisterLocator::for_frame(newFr);    
    sd = sd_of_created_frame;    
    return isInInterruptCheck;
  }


  frame* Conversion::fixConversionStack_for_returning_to_self( 
           char* self_sparc_fp_or_ppc_sp, 
           sendDesc* self_sd ) {
    // need frame below real last frame:
    fixConversionStack((char*)self_sd, self_sparc_fp_or_ppc_sp);
    return (frame*)self_sparc_fp_or_ppc_sp;
  }


  void Conversion::continue_after_return_trap_with_result( 
                     oop res, 
                     char* continuationPC, 
                     char* self_sparc_fp_or_ppc_sp ) {
    OutgoingArgsOfReturnTrapOrRecompileFrame = NULL; // done with this                                           
    ContinueAfterReturnTrap(res, continuationPC, self_sparc_fp_or_ppc_sp);
  }
  
  
  oop Conversion::get_result() {
    fatal("Unimplemented"); return NULL;
  }
  
# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_i386.hh"

# include "_frame_i386.cpp.incl"


frame* frame::sender() { return my_sp()->link()->as_frame();  }

char**  frame::real_return_addr_addr() { return my_sp()->return_addr_addr();  }

char* frame::return_addr() {
  if (!InterruptedContext::the_interrupted_context->is_set())
    return platform_independent_return_addr();
    
  // Profiler uses this from interrupts, but last return address may not be right,
  // so return null.
  if ((frame*)InterruptedContext::the_interrupted_context->sp() == this)
    return NULL;
    
  // C compiler moves sp, but does not save pc for leaves
  if ( InterruptedContext::the_interrupted_context->sp()->sender()  ==  this
  &&   !Memory->code->contains(InterruptedContext::the_interrupted_context->pc()))
    return NULL;
    
  return platform_independent_return_addr();
}

  
// Return the address I was entered at, assuming I am a C++ frame

char* frame::c_entry_point() {
  frame* s = sender();
  if ( s == NULL ) return NULL;
  
  char* r = s->real_return_addr(); // where sender will return into
  
  if ( Memory->code->contains(r))   
    return NULL;
  char* callp = r - 1;
  if (callp == NULL  ||  !isImmediateCall((inst_t*)callp)) return NULL;
  return  get_target_of_C_call_site((inst_t*)callp);
}

# pragma warn_unusedarg off

frame** frame::nmethod_frame_chain_addr(nmethod* nm) { 
  return (frame**)my_bp() + nmethod_frame_chain_offset;
}


objVectorOop* frame::patched_frame_saved_outgoing_args_addr(nmethod* nm) { 
  return (objVectorOop*)my_bp() + patched_frame_saved_outgoing_args_offset; 
}


char**  frame::currentPC_addr() { 
  return (char**)my_bp() + current_pc_offset;
}
 
 
// this may not be a self frame ????
oop*  frame::location_addr_of_incoming_argument(Location r, RegisterLocator* rl) { 
  oop* bp = (oop*)my_bp();
  if (r == IReceiverReg)
    return bp + ircvr_offset;
  assert(is_IArgLocation(r), "???");
  fint i = index_for_IArgLocation(r);
  return bp + first_iarg_offset + i;
} 

 
void**  frame::location_addr(Location r, RegisterLocator* rl) {
  Location base; int32 d; OperandType t;
  reg_disp_type_of_loc(&base, &d, &t, r);
  assert(t != RegisterOperand, "???");
  return (base == esp ? (void**)my_sp() : (void**)my_bp())  +  d / oopSize;
}
  

// called from recompile.c:
// copy the receiver to the new place and adjust all block homes
// (the receiver must contain references to every live block belonging
// to this frame for zapping purposes)

void frame::copy_to( char* sp,
                     char* caller,
                     char* pc,
                     bool adjust) {

  frame* new_f = (frame*)sp;
  if (SpendTimeForDebugging)
    warning("untested frame::copy_to:"); // need SIC to test this

  if (adjust) {
    // make sure all memoized blocks exist, then adjust their scope
    abstract_vframe* callee = NULL;
    OopOopTable* dummy = EMPTY;
    for ( abstract_vframe* vf = new_vframe(this);
          vf  &&  vf->fr == this;
          callee = vf,  vf = vf->sender()) {
      vf->createBlocks(callee, dummy);
    }
    ResourceMark rm; // for RegisterLocators
    adjust_blocks(block_scope_of_home_frame(), new_f, RegisterLocator::for_frame(new_f) );
  }
  copy_oops( (oop*)this, (oop*)new_f, frame_size());

  my_sp()->set_link( ((frame*)caller)->my_sp() );
  set_return_addr( pc );
}
  

// adjust fp of copied frames so they don't refer to the original stack

void frame::adjust_frame_links_of_copied_frames( frame* last_frame_to_copy,
                                                 frame* first_copied_frame) {
  
  i386_sp* osp =                     my_sp(); // this frame
  i386_sp* nsp = first_copied_frame->my_sp(); // copied frame's this
  int32 diff = (char*)nsp - (char*)osp;
  
  while ( osp  <  last_frame_to_copy->my_sp() ) {

    nsp->adjust_link(diff);

    osp = osp->link();
    nsp = nsp->link();
  }
  // note last copied link should just be what it was so don't do anything
}

// end of copy() helpers


// From Sparc: Note that the lastFrame frame will never be returned to, so we can
// bash its registers.  But a dummy frame is needed to make the stack
// look right (to serve as the frame of the last sparc_sp).
// (used to be fixStack)


void frame::fix_frame(char* pc, char* sp) { 
  // Although conversion happens in VM process, 
  // currentProcess still says orig Self process.
  // So, unchain_frames which does Stack::first_VM_frame (sp?)
  // will go from current (conversion) frame looking for a self frame.
  // Thus, zap a conversion frame (which we never return from anyway)
  // to lead traversal back to self frame.

  assert(isOnVMStack(this), "will be overwritten");
  my_sp()->set_link((i386_sp*)sp);
  set_real_return_addr(pc);
}
 
// I386 uses bp for block home, but sp for frame*
/* Here's the deal:
   SPARC uses bp cause sp is not constant.
   This makes the VM work harder to find the block_scope (home method's frame)
   from what is stored in the block (home method's caller's frame).
   See, the home method's frame is the sp of the frame but the sp can vary, while the frame pointer cannot.
   So we store the frame pointer of the home frame in the block and crawl the stack to recover the stack pointer
   when needed by the VM.
   PPC uses the sp, which saves a lot of work.
   Intel COULD use the sp, because it doesn't change.
   BUT, at block creation time (see loadBlockOop in codeGen_i388.cpp), we have the sp BEFORE
   pushing the pc and old base pointer, so would have to subtract 8 from what we have to create the block.
   That isn't so bad, but then at NLR time, would have to compare what is in the NLRHomeReg to sp-8, would be slower.
   So for Intel, use the base pointer (frame pointer) as SPARC does.
   Could revisit this, use the sp, and just add 8 when setting the NLRHomeReg.
   Cannot use the sp+8 value in the block because the VM expects this to be a frame in various places.
   -- dmu 4/25/06
*/   

frame* frame::block_scope_of_home_frame() {
   return sender();
}

frame* frame::home_frame_of_block_scope(frame* currentFrameHint) {
  return sendee(currentFrameHint);
}


frame* frame::home_frame_of_vfo_locals(frame* currentFrameHint) {
  return this; }

frame* frame::vfo_locals_of_home_frame() { return this; }

int32 frame::frame_size_of_uncopied_frame() { return (oop*)sender() - (oop*)this; }

char* frame::c_return_pc() {  
  return return_addr(); 
}

frame* frame::make_full_frame(char* pc)            {  
  return this; 
}
frame* frame::make_full_frame_after_trap(char* pc) {  
  return (frame*)
       (int32(this)
    -   ((int32(this) - frame_alignment_offset*oopSize) & (frame_word_alignment*oopSize - 1)));
}
frame* frame::make_full_frame_on_user_stack()      {  return this; }



void frame::print_compiled() {
  if (is_compiled_self_frame())
    lprintf(" chain = %#lx;", (nmethod_frame_chain(code())));
  lprintf("\n\tlocals = [%#lx, %#lx], currPC = %#lx, size = %ld words\n",
         (long unsigned)(my_sp()),
         (long unsigned)(currentPC()),
         long(frame_size()));
}


// 0 = first arg
// basically used for printing (tracing)
// "this" is the LOOKUP FRAME, doing the sending
oop frame::get_lookup_arg(fint index) { 
  return ((oop*)my_sp())[first_iarg_offset + index];
}


void frame::printRegs() {
}


void frame::printVerbose_on_this_platform() {
  warning("unimp intel (debugging only, anyway)");
}


oop* frame::first_incoming_arg_addr() {
  return (oop*)my_bp() + first_iarg_offset;
}

oop* frame::first_local_addr() {
  return (oop*)my_bp() + first_local_offset;
}



sendDesc* frame::send_desc() { 
  return sendDesc::sendDesc_from_return_PC( return_addr());  // it is always the same here
}



// used for scavenging, must return allocated location
//  for every interpreter state class that may exist
//  return IllegalLocation for non-interp frame

Location frame::location_of_interpreter_of_block_scope(void* entry_point) {
  return  entry_point == first_inst_addr(interpret)  
            ?  Location(IArgLocation(0))  
            :  IllegalLocation;
}


frame* frame::get_patched_self_frame(char* sp_of_patched_frame) {
  if (Interpret)
    warning("next line may be wrong for interpreter, was currentFrame()->sender()");
  return (frame*)sp_of_patched_frame;
}


int32  frame::copy_through_oop_count(frame* last_frame_to_copy) {
  return (oop*)last_frame_to_copy + last_frame_to_copy->frame_size() - (oop*)this;
}


void frame::fix_current_return_address(char* ) {  }

oop frame::perform_selector_of_SendMessage_stub_frame() {
  return  ((oop*)sender()->my_sp())[PerformSelectorLoc_sp_offset / oopSize];
}  

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.3 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "stubs_i386.hh"
# include "_stubs_i386.cpp.incl"

# pragma warn_unusedarg off


// asm routines (only needed for SIC:)



extern "C" {
  oop UncommonBranch(...) { fatal("unimp intel");  return NULL; }
  // Note: do not have a stack frame yet; link is link of caller
  // Also, they must preserve Temp1 and Temp2
  }

// Only needed for interpreter/compiler interoperation:
extern "C" {  oop ReturnResult_stub(...) { fatal("unimp intel");  return NULL; } }
oop ReturnResult_stub_result;

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_iterator_i386.hh"

# include "_frame_iterator_i386.cpp.incl"


void FrameIterator::do_vm_frame() {
  assert(!processSemaphore, "oopClosure will not be called on resultOop in HandleReturnTrap");
  assert(SaveOutgoingArgumentsOfPatchedFrames, "always true for I386");
  if (!SaveOutgoingArgumentsOfPatchedFrames)
    return;
  do_incoming_arguments_of_vm_frame_called_from_self();
}


void FrameIterator::do_incoming_arguments_of_vm_frame_called_from_self() {
  frame* s = f->sender();
  if (s == NULL  ||  !s->is_self_frame())
    return;
  // hit the outgoing args of the self frame in case it is later patched
  // (when a frame is patched we grab its outgoing args)
  fint n = s->outgoing_arg_count(f);
  for (fint i = 0;  i < n + 1 /*rcvr*/;  ++i)
    oop_closure->do_oop(
      f->location_addr_of_incoming_argument(LocationOfSavedOutgoingArgInSendee(i-1), NULL));
}


# if defined(FAST_COMPILER) || defined(SIC_COMPILER)
            
  void FrameIterator::do_incoming_arguments() {
    fint nargs = nm->incoming_arg_count();
    oop* p = f->first_incoming_arg_addr() - 1 /* rcvr */;
    for (fint i = -1 /* rcvr */;  i < nargs;  ++i, ++p) {
      assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, i), "");
      oop_closure->do_oop(p);
    }
  }


  void FrameIterator::do_compiled() {
    nm = f->code();

    do_incoming_arguments();
    do_memory_locals();
    do_patched_frame_saved_outgoing_args();
  }
  
  
  void FrameIterator::do_memory_locals() {
    fint n = min(nm->number_of_memory_locals(),  sizeof(RegisterString) * BitsPerByte);
    oop* p = f->first_local_addr();
    fint i;
    for (i = 0;  i < n;  ++i, --p) {
      assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, i), "");

           if ( isSet(mask, i) )  oop_closure->do_oop(p);
      else if ( zap            )  *p = badOop;
    }      
    # if GENERATE_DEBUGGING_AIDS
      if (CheckAssertions)
        for ( fint j = i; j  <  sizeof(RegisterString) * BitsPerByte;  ++j)  {
          assert( !isSet(mask, j), "nonsensical bit set in mask");
        }
    # endif
    
    for (  ;  i < nm->number_of_memory_locals();  ++i, --p ) {
      assert(check_for_overwriting_patched_frame_saved_outgoing_args(p, i), "");
      oop_closure->do_oop(p);
    }
  }
  

  bool FrameIterator::check_for_overwriting_patched_frame_saved_outgoing_args(oop* p, fint idx) {
    if (!f->is_patched())                                         return true;
    if (p != (oop*)f->patched_frame_saved_outgoing_args_addr(nm)) return true;
    WizardMode = true;
    lprintf("****** about to die at index %d, frame = 0x%x, next_frame = 0x%x, frame size = 0x%x (%d)\n",
            idx, f, f->sender(), f->frame_size(), f->frame_size());
    if (nm != f->code()) {
      lprintf("****** nm (0x%x) != f->code() (0x%x)\n", nm, f->code());
      nm = f->code();
    }
    if (!GCInProgress && !ScavengeInProgress) { // cannot do these when the heap is all weird
      lprintf("nmethod of offending frame is:\n");
      nm->verify();
      nm->print(); 
      lprintf("\n\n\ncode: \n");
      nm->printCode();
    }
    return false;
  }

  
# endif // either compiler
# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// # pragma implementation "vframe_i386.hh"
# include "_vframe_i386.cpp.incl"

    
# if defined(FAST_COMPILER) || defined(SIC_COMPILER)


oop* dummy_vframe::register_contents_addr(Location r) {
  fint i = index_for_IArgLocation(r);
  assert(SaveOutgoingArgumentsOfPatchedFrames, "I386 needs this");
  assert(i + 1  <  OutgoingArgsOfReturnTrapOrRecompileFrame->length(), "bounds");
  oop* argp = OutgoingArgsOfReturnTrapOrRecompileFrame->objs(i + 1);
  assert((*argp)->verify(), "checking arg");
  return argp;
}


int32 compiled_vframe::register_offset(Location r) {
  fatal("Unused Intel; don't save registers in frames"); 
  return 0;
}


# if defined(SIC_COMPILER)
oop* compiled_vframe::special_register_contents_addr(Location r) {
  return NULL; // for SPARC
}
# endif


oop* compiled_vframe::register_contents_addr(Location r) {
  return (oop*) fr->location_addr(r, rl);
}


oop* compiled_vframe::register_contents_secondary_addr(Location r) {
  return NULL; // only in one place on I386
}
  
  
void compiled_vframe::print_code(fint curFrame) {
  lprintf("#%ld", (long) curFrame);
  if (!WizardMode) return;

  lprintf(" <%#lx%c@ %#lx%c%s # %ld",
         (long unsigned)fr,
         fr->return_addr() != fr->real_return_addr() ? '*' : ' ',
         (long unsigned)code,
         code->isInvalid() ? '!' : ' ',
         code->isDebug()
           ? "deb" : VMString[code->compiler()]->copy_null_terminated(),
         long(code->scopes->offsetTo(desc)));

  lprintf(">");
}


void compiled_vframe::fix_frame(frame* ) { } // for sparc


void compiled_vframe::get_search_locations_for_liveness_check(NameDesc*, frame*& fr_to_search, RegisterLocator*& rl_to_search) {
  fr_to_search = fr;
  rl_to_search = rl;
}


void compiled_vframe::copy_outgoing_arg(fint argNo, NameDesc* nd2, compiled_vframe* vf, dummy_vframe* dummy, 
                                        NameDesc* nd, frame* oldBlkHome, OopOopTable* blkValues) {

  // we're restarting a send (e.g. after recompilation that left most
  // recent frame invalid); outgoing args were saved by assembly glue

  // On I386, ReturnTrap asm glue does not save the outgoing arguments.
  
  // Used to be:
  // if (!nd2->isIllegal())
  //   copyValue(nd, vf, nd2, oldBlkHome, blkValues); // get value from caller if possible
  // else
  //   copyValueTo(nd, new_string("<unknown outgoing argument>")); 

  Location loc= LocationOfSavedOutgoingArgInSendee(argNo);
  NameDesc* fromNd = new LocationNameDesc(loc, 0);
  if (!nd2->isIllegal()) {
    // verify that args were saved correctly
    oop val1 = vf->get_contents(nd2, false);
    oop val2 = dummy->get_contents(fromNd);
    if (val1 != val2)
      fatal3("inconsistent outgoing arg %d: %#lx vs. %#lx",
             argNo, val1, val2);
  }
  copyValue(nd, dummy, fromNd, oldBlkHome, blkValues);
}


compiled_vframe* compiled_vframe::sendeeOrNULL_for_get_expr_stack() {
  // From PPC: Cannot find a register locator when this makes a dummy_vframe, so live
  // with less info for recompilation. -- dmu 2/03
  //
  // Could maybe fix this for Intel.
  return NULL;
}

# endif // defined(FAST_COMPILER) || defined(SIC_COMPILER)

# endif // TARGET_ARCH == I386_ARCH
# if  TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 1.4 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# pragma implementation "frame_format_i386.hh"

# include "_frame_format_i386.cpp.incl"



void reg_disp_type_of_loc(Location* basep, int32* offsetp, OperandType* tp, Location loc) {
  Location b = IllegalLocation;  int32 d = 0;  OperandType t = NumberOperand;
  
       if (isRegister(loc))  b = loc, d = 0, t = RegisterOperand;
       
  else if (loc == IReceiverReg  || is_IArgLocation(loc))  b = ebp,  d =  index_for_IArgLocation(loc) + 1 +     ircvr_offset;
  else if (loc == LReceiverReg  || is_LArgLocation(loc))  b = esp,  d =  index_for_LArgLocation(loc) + 1 + leaf_rcvr_offset;
  else if (loc ==  ReceiverReg  ||  is_ArgLocation(loc))  b = esp,  d =  index_for_ArgLocation(loc)  + 1 +   rcvr_offset;
  else if (                       is_StackLocation(loc))  b = ebp,  d = -index_for_StackLocation(loc) + first_local_offset;
  else fatal1("don't know how to do %s\n", locationName(loc));

  if (basep) *basep = b;
  if (offsetp) *offsetp = d * oopSize;
  if (tp) *tp = t;
}
# endif // TARGET_ARCH == I386_ARCH
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include <stdio.h>
# include <stdlib.h>
# include <string.h> // jaguar

static char* program;

void usage() {
  fprintf(stderr, "Usage: %s file...\n", program);
  fprintf(stderr, "    or %s -h (to get this message)\n", program);
}

struct File {
  char* path;

  File(char* s) { path = (char*) malloc(strlen(s) + 1); strcpy(path, s); }
  ~File() { free(path); }

  int addTrailingNewline();
};

int File::addTrailingNewline() {
  FILE* file;
  if (!(file = fopen(path, "r+"))) {
    fprintf(stderr, "Couldn't open %s for writing\n", path);
    return -1;
  }
  
  // seek to end of file
  if (fseek(file, -1, 2)) {
    fprintf(stderr, "Couldn't seek to end of %s\n", path);
    if (fclose(file)) {
      fprintf(stderr, "Couldn't close %s\n", path);
    }
    return -1;
  }
  
  // read last character
  int ch = getc(file);
  if (ch == EOF) {
    fprintf(stderr, "Internal error: didn't really seek to end of %s\n",
	    path);
    if (fclose(file)) {
      fprintf(stderr, "Couldn't close %s\n", path);
    }
    return -1;
  }
  // just test to make sure
  if (getc(file) != EOF) {
    fprintf(stderr, "Internal error: didn't really seek to end of %s\n",
	    path);
    if (fclose(file)) {
      fprintf(stderr, "Couldn't close %s\n", path);
    }
    return -1;
  }
  
  // append newline if not already there
  if (ch != '\n') {
    if (putc('\n', file) != '\n') {
      fprintf(stderr, "Couldn't append newline to %s\n", path);
      if (fclose(file)) {
	fprintf(stderr, "Couldn't close %s\n", path);
      }
      return -1;
    } else {
      fprintf(stderr, "Added newline to %s\n", path);
    }
  }
  
  if (fclose(file)) {
    fprintf(stderr, "Couldn't close %s\n", path);
    return -1;
  }

  return 0;
}

int main(int argc, char** argv) {
  program = argv[0];

  while (--argc,++argv, argc>=1 && ((*argv)[0] == '-')) {
    switch ((*argv)[1]) {
     case 'h': usage(); exit(0);
     case '\0': goto done;
     default:
      fprintf(stderr, "Option %s unknown.\n", *argv);
      usage(); exit(-1);
    }
  }
 done: ;

  for (; argc>=1; --argc,++argv) {
    File f(*argv);
    f.addTrailingNewline();
  }

  exit(0);
}
# include <stdlib.h>
# include <fcntl.h>
# include <stdio.h>
# include <sys/mman.h>
# if defined(__APPLE__) \
  && defined(__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__) \
  && ((__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__-0) >= 1050)
  # include <libkern/OSCacheControl.h>
  # define DO_INVALIDATE_ICACHE
# endif
// $Revision: 30.2 $

int
main(int argc, char* argv[]) {
	if (argc <= 1) {
   		fprintf(stderr, "Usage: %s testFile test args...\n", argv[0]);
		exit(1);
	}
	FILE* f = fopen(argv[1], "r");
	if (f == NULL) {
		perror(argv[1]);
		exit(1);
	}
	if (fseek(f, 0, SEEK_END)) {
		perror("seek");
		exit(1);
	}
	int len = ftell(f);
	rewind(f);
	char* buf = new char[len];
	if (fread(buf, len, 1, f) != 1) {
		perror("fread");
		exit(1);
	}
  	mprotect(buf, len, PROT_EXEC | PROT_READ | PROT_WRITE);
#if defined(DO_INVALIDATE_ICACHE)
	sys_icache_invalidate(buf, len);
#endif
	typedef int (*fn_t)(...);
	int args[3];
	int argsToSkip = 2; // name of me, name of testfile
	for (int i = 0; i < argc - argsToSkip;  ++i)
		args[i] = atoi(argv[i + argsToSkip]);
	int r = ((fn_t)buf) (args[0], args[1], args[2]);
	printf("result = %d\n", r);
}
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

// zap sends signal SIGKILL to all processes owned by you 
// and which command line has argv[1] as prefix.

#include <stdio.h>
#include <string.h> // jaguar
#include <signal.h>
#include <sys/param.h>
#include <unistd.h>
#include <stdlib.h>

#if defined(sun) || defined(__sun)
# define MAXHOSTNAMELEN 257
#endif

const char *ps = "ps -gx";

char hostname[MAXHOSTNAMELEN];

void kill_if_match(int pid, char *pid_text, char *match_text) {
  int i;
  int ok = 1;

  for (i = 0; i < strlen(match_text); i++)
    ok = ok && match_text[i] == pid_text[i];

  if (ok && (pid_text[i] == ' ' || pid_text[i] == '\0')) {
    printf("Zapping \"%s\"(%d) on %s.\n", 
      pid_text, pid,
      gethostname(hostname, MAXHOSTNAMELEN) ? "unknown host" : hostname);
    kill( pid, SIGKILL);
  }
}

int main(int argc, char **argv) {
  FILE *fin;
  char buf[BUFSIZ];
  int pid, text_pos, i;
  char *pid_text;

  if (argc != 2) {
    fprintf( stderr, "usage: %s <name>\n", argv[0]);
    exit(1);
  }

  if ((fin = popen(ps, "r")) == NULL) {
    fprintf( stderr, "%s: can't run %s\n", argv[0], ps);
    exit(1);
  }

  if (fgets(buf, sizeof buf, fin) == NULL) {
    fprintf( stderr, "cant read from process\n" );
    exit(1);
  }

  for ( i=0; buf[i] != 'C' && buf[i] != '\0'; i++);
  if (buf[i] == 'C')
    text_pos = i;
  else{
    fprintf( stderr, "%s: can't find C in COMMAND column\n", argv[0]);
    exit(1);
  }

  while (fgets(buf, sizeof buf, fin) != NULL) {
    sscanf(buf, "%d", &pid);
    buf[strlen(buf)-1] = '\0';
    pid_text = &buf[text_pos];
    kill_if_match( pid, pid_text, argv[1]);
  }
}
/* Sun-$Revision: 30.6 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

#include <stdio.h>
#include <stdlib.h>
#include <sys/types.h>
#include <sys/stat.h>


int main(int argc, char** argv) {
  struct stat buf;

  if (argc == 2) {
    if (!lstat(argv[1], &buf)) {
      if ((buf.st_mode & S_IFLNK) == S_IFLNK) {
	exit(0);
      } else {
	exit(1);
      }
    } else {
      perror("lstat failed");
    }
  } else {
    printf("usage: %s filename        returns 0 if file is a link\n", argv[0]);
  }
}
/* Sun-$Revision: 30.6 $ */

/* Copyright 1995-2006 Sun Microsystems, Inc.
   See the LICENSE file for license information. */

# include <stdlib.h>
# include <unistd.h>
# include <sys/types.h>
# include <sys/socket.h>
# include <netinet/in.h>
# include <netdb.h>
# include <stdio.h>
# include <string.h>

short port_number    =  1275;
const int   debug          =     0;
const int   hostNameLength =   100;
const int   selfExpLength  =  1000;


void usage(char* name) {
  fprintf(stderr, "usage: %s [host [port]] \n", name);
  exit(-1);
}


inline void safeDo(int dofail, const char *s) {
  if (dofail) {
    fprintf(stderr, "rself: %s\n", s);
    exit(-1);
  }
}


inline void trace(const char *s) {
  if (debug)
    fprintf(stderr, "%s\n", s);
}

 
inline void safeWrite(int fd, void *buf, int len) {
  safeDo(write(fd, buf, len) < len, "write failed");
}


void parseArgs(int argc, char **argv, char *hostName) {
  if (argc >= 4)
    usage(argv[0]);
  safeDo(gethostname(hostName, hostNameLength) == -1, "gethostname failed");
  if (argc > 1)
      strcpy(hostName, argv[1]);
  if (argc > 2)
      port_number = atoi(argv[2]);
}


void relay(int sock) {
  /* Read from socket, echo to stdout. Read from input, echo to sock. 
     When eof(input) shutdown that direction of sock. If read on sock
     fails, assume it is because self process did a shutdown; then stop
     waiting for output from him and also stop relaying input to him. */
  trace("Starting to relay.\n");
  if (fork()) {
    char ch;
    for (;;) {
      char buf[BUFSIZ];
      int count = read(sock, buf, sizeof(buf));
      if (count <= 0)
	break;
      if (write(1, buf, count) == -1) {
	break; // this is bad.
      }
    }
    trace("read failed - assuming shutdown");
    /* Now exit, thereby terminating parent (and child). */
  } else {
    int ich;  /* int to be able to distinguish EOF. */
    while((ich = getchar()) != EOF) {
      char ch = ich;
      safeWrite(sock, &ch, sizeof(ch));
    }
    shutdown(sock, 1);
  }
}


int createConnection(char *hostName, short port) {
  trace("createConnection start");
  struct hostent *hp = gethostbyname(hostName);   
  struct sockaddr_in receiver;
  memset((char *)&receiver, '\0', sizeof(receiver));
  memcpy((char *)&receiver.sin_addr, hp->h_addr, hp->h_length);

  receiver.sin_family= hp->h_addrtype;
  receiver.sin_port= htons(port);
  int sock = socket(PF_INET, SOCK_STREAM, 0);
  safeDo(sock == -1, "socket create failed");

  safeDo(connect(sock, (struct sockaddr *)&receiver, sizeof(receiver)) == -1,
         "connect failed");         /* Something rotten about that cast?! */
  trace("Got a connection");
  return sock;
}


int main(int argc, char **argv) {
  trace("main start");
  char hostName[hostNameLength];
  parseArgs(argc, argv, hostName);
  int sock = createConnection(hostName, port_number);
  relay(sock);
  close(sock);
}
/* Sun-$Revision: 30.13 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */


# if defined(MACOSX_VERSION) && (TARGET_OS_VERSION == MACOSX_VERSION)
  typedef SelfX11Cursor Cursor;
# endif

# include "_glueDefs.cpp.incl"

# define Status int // cause I had to undef it earlier

# include <X11/Xlib.h>
# include <X11/Xutil.h>
# include <X11/Xutil.h>
# include <X11/extensions/shape.h>
# include "xlib.primMaker.hh"

// Display_seal is declared here
# include "xlibPrims.hh"

#ifdef DYNAMIC
VERIFYCHECKSUM
#endif

// Display_seal is defined in the VM
# define xTypeSealsDo(template)						      \
    template(Foo)							      \
    template(Atom)							      \
    template(Colormap)							      \
    template(Cursor)							      \
    template(Drawable)							      \
    template(Font)							      \
    template(GC)							      \
    template(Pixmap)							      \
    template(Screen)							      \
    template(Visual)							      \
    template(Window)							      \
    template(XCharStruct)						      \
    template(XColor)							      \
    template(XEvent)							      \
    template(XFontStruct)						      \
    template(XGCValues)							      \
    template(XImage)							      \
    template(XSizeHints)						      \
    template(XTextProperty)						      \
    template(XTextProperty_value)					      \
    template(XVisualInfo)						      \
    template(XWindowAttributes)						      \
    template(XWMHints)							      \
    template(XSetWindowAttributesWrap)					      \
    template(Region)

# define defineXTypeSeals(stem)						      \
    const char* CONC(stem,_seal) = STR(stem);

xTypeSealsDo(defineXTypeSeals)

// All events need to have the same type seal. For convenience, define
// extra names for them, so that we can write the templates more concisely.
// E.g., can use 'XAnyEvent' instead of 'proxy {XAnyEvent *} XEvent_seal'.
const char* XAnyEvent_seal            = XEvent_seal;
const char* XButtonEvent_seal         = XEvent_seal;
const char* XClientMessageEvent_seal  = XEvent_seal;
const char* XColormapEvent_seal       = XEvent_seal;
const char* XConfigureEvent_seal      = XEvent_seal;
const char* XCrossingEvent_seal       = XEvent_seal;
const char* XEnterWindowEvent_seal    = XEvent_seal;
const char* XExposeEvent_seal         = XEvent_seal;
const char* XFocusChangeEvent_seal    = XEvent_seal;
const char* XGraphicsExposeEvent_seal = XEvent_seal;
const char* XKeyEvent_seal            = XEvent_seal;
const char* XLeaveWindowEvent_seal    = XEvent_seal;
const char* XMapEvent_seal            = XEvent_seal;
const char* XMappingEvent_seal        = XEvent_seal;
const char* XMotionEvent_seal         = XEvent_seal;
const char* XNoExposeEvent_seal       = XEvent_seal;
const char* XReparentEvent_seal       = XEvent_seal;
const char* XUnmapEvent_seal          = XEvent_seal;
const char* XVisibilityEvent_seal     = XEvent_seal;


// This class is simply a wrapper around the XSetWindowAttributes structure
// and the mask used by X to determine which fields to care about.  It only
// provides setter methods since it is meant to be a structure used to set
// window properties - thus it assumes you know what you are putting in and
// you won't need to get it out later.
// I created this class because I was doing this in Self anyway (less cleanly)
// and the X function that uses it took too many arguments.  So this is the
// somewhat over-blown solution.  But I thought it was the easiest :)
// RCD - 7/20/94
class XSetWindowAttributesWrap {
public:
  XSetWindowAttributesWrap  () : attrMask(0L), winAttrs(new XSetWindowAttributes) {}
  ~XSetWindowAttributesWrap () { delete winAttrs; }

  void backgroundPixmap (Pixmap pixmap) {
    winAttrs->background_pixmap = pixmap;
    attrMask                  |= CWBackPixmap;
  }

  void backgroundPixel (unsigned long pixel) {
    winAttrs->background_pixel = pixel;
    attrMask                 |= CWBackPixel;
  }

  void borderPixmap (Pixmap pixmap) {
    winAttrs->border_pixmap = pixmap;
    attrMask              |= CWBorderPixmap;
  }

  void borderPixel (unsigned long pixel) {
    winAttrs->border_pixel = pixel;
    attrMask             |= CWBorderPixel;
  }

  void bitGravity (int gravity) {
    winAttrs->bit_gravity = gravity;
    attrMask            |= CWBitGravity;
  }

  void winGravity (int gravity) {
    winAttrs->win_gravity = gravity;
    attrMask            |= CWWinGravity;
  }

  void backingStore (int when) {
    winAttrs->backing_store = when;
    attrMask              |= CWBackingStore;
  }

  void backingPlanes (unsigned long planes) {
    winAttrs->backing_planes = planes;
    attrMask               |= CWBackingPlanes;
  }

  void backingPixel (unsigned long pixel) {
    winAttrs->backing_pixel = pixel;
     attrMask             |= CWBackingPixel;
  }

  void saveUnder (Bool yesOrNo) {
    winAttrs->save_under = yesOrNo;
    attrMask           |= CWSaveUnder;
  }

  void eventMask (long mask) {
    winAttrs->event_mask = mask;
    attrMask           |= CWEventMask;
  }

  void doNotPropagateMask (long mask) {
    winAttrs->do_not_propagate_mask = mask;
    attrMask                      |= CWDontPropagate;
  }

  void overrideRedirect (Bool yesOrNo) {
    winAttrs->override_redirect = yesOrNo;
    attrMask                  |= CWOverrideRedirect;
  }

  void colormap (Colormap cmap) {
    winAttrs->colormap = cmap;
    attrMask         |= CWColormap;
  }

  void cursor (Cursor cursor) {
    winAttrs->cursor = cursor;
    attrMask       |= CWCursor;
  }

  unsigned long          mask     () { return attrMask; }
  XSetWindowAttributes * rawAttrs () { return winAttrs; }

private:
  unsigned long 	attrMask;
  XSetWindowAttributes *winAttrs;
};


oop XNextEvent_wrap(Display *display, bool peek,
                    objVectorOop eventProtos, void *FH) {
  XEvent *evt = new XEvent();
  if (peek)
    XPeekEvent(display, evt);
  else
    XNextEvent(display, evt);
  int type = evt->type;
  if (type < 0 || type >= eventProtos->length()) {
    char err[50];
    sprintf(err, "unknown X event, type = %d", type);
    failure(FH, err);
    delete evt;
    return NULL;
  }
  oop proto = eventProtos->obj_at(type);
  if (!proto->is_proxy()) {
    prim_failure(FH, BADTYPEERROR);
    delete evt;
    return NULL;
  }
  proxyOop res = proxyOop(proto)->clone();
  res->set_pointer(evt);
  res->set_type_seal(XEvent_seal);
  return res;
}


void XGetGCValues_wrap(Display* display, GC gc, unsigned long valuemask,
		       XGCValues *values_return, void* FH) {
  int status = XGetGCValues(display, gc, valuemask, values_return);
  if (status == 0) {
    failure(FH, "XGetGCValues failed--valuemask may be invalid");
  }
}

Window XCreateIOWindow_wrap(Display* display, Window win,
                            int x, int y,
                            unsigned int width, unsigned int height,
                            int depth, Visual* visual,
                            XSetWindowAttributesWrap* winAttrs) {
  return XCreateWindow(display, win, x, y, width, height, 0, depth,
                       InputOutput,
                       visual, winAttrs->mask(), winAttrs->rawAttrs());
}

void XChangeWindowAttributes_wrap(Display *display, Window win,
                                  XSetWindowAttributesWrap *winAttrs) {
  XChangeWindowAttributes(display, win,
                          winAttrs->mask(), winAttrs->rawAttrs());
}

XWindowAttributes *XGetWindowAttributes_wrap(Display *display, 
                                             Window win, void *FH) {
  XWindowAttributes *attrs = new XWindowAttributes();
  if (!XGetWindowAttributes(display, win, attrs)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    delete attrs;
    return NULL;
  }
  return attrs;
}




void XMoveWindowBy_wrap(Display* display, Window win,
                            int x, int y) {
  int old_x, old_y;
  unsigned int w, h, b, d;
  Window pw;

  if (!XGetGeometry(display, win, &pw, &old_x, &old_y, &w, &h, &b, &d)) {
    return;
  }
  XMoveWindow(display, win, x + old_x, y + old_y);
}


int XSetWMProtocol_wrap(Display* display,  Window window,  Atom protocol) {
  Atom ps[1];
  ps[0] = protocol;
  return XSetWMProtocols(display, window, ps, 1);
}

void XQueryColors_wrap(Display* display,
		       Colormap colormap,
		       objVectorOop colors_oop,
		       void *FH) {
  XColor** pcolors = (XColor**)colors_oop->convertProxyArray(XColor_seal);
  if (!pcolors) {
    prim_failure(FH, BADTYPEERROR);
    return;
  }
  int32 count = colors_oop->length();
  XColor* colors = NEW_RESOURCE_ARRAY(XColor, count);
  int32 i;
  for (i = 0;  i < count;  i++) {
    colors[i] = *pcolors[i];
  }
  XQueryColors(display, colormap, colors, count);
  for (i = 0; i < count; i++) {
    *pcolors[i] = colors[i];
  }
}

void XStoreColors_wrap(Display* display, Colormap colormap,
		       objVectorOop colors_oop, void *FH) {
  XColor** pcolors = (XColor**)colors_oop->convertProxyArray(XColor_seal);
  if (!pcolors) {
    prim_failure(FH, BADTYPEERROR);
    return;
  }
  int32 count = colors_oop->length();
  XColor* colors = NEW_RESOURCE_ARRAY(XColor, count);
  for (int32 i = 0; i < count; i++) {
    colors[i] = *pcolors[i];
  }
  XStoreColors(display, colormap, colors, count);
}

void XAllocColorCells_wrap(Display* display, Colormap colormap, bool contig,
			   objVectorOop plane_masks_return_oop,
			   objVectorOop pixels_return_oop, void *FH) {

  unsigned int nplanes = plane_masks_return_oop->length();
  unsigned int npixels = pixels_return_oop->length();
  int i;

  ResourceMark rm;

  if (npixels <= 0) {
    prim_failure(FH, BADSIZEERROR);
    return;
  }

  unsigned long* plane_masks_return = NEW_RESOURCE_ARRAY(unsigned long, nplanes);
  unsigned long* pixels_return      = NEW_RESOURCE_ARRAY(unsigned long, npixels);

  if (!XAllocColorCells(display, colormap, contig,
                        plane_masks_return, nplanes,
			pixels_return,      npixels)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return;
  }

  for (i = 0; i < nplanes; i++)
    plane_masks_return_oop->obj_at_put(i, as_smiOop(plane_masks_return[i]), 
                                       false);
  for (i = 0; i < npixels; i++)
    pixels_return_oop->obj_at_put(i, as_smiOop(pixels_return[i]), false);
}

void XFreeColorCells_wrap(Display* display, Colormap colormap,
		      objVectorOop pixels_oop, unsigned long planes, void *FH) {
  int npixels = pixels_oop->length();
  unsigned long* pixels = (unsigned long*)pixels_oop->convertIntArray();
  if (!pixels) {
    prim_failure(FH, BADTYPEERROR);
    return;
  }

  XFreeColors(display, colormap, pixels, npixels, planes);
}

void XFillPolygon_wrap(Display* display, Drawable d, GC gc,
		       objVectorOop xsOop, objVectorOop ysOop,
		       int shape, int mode, void* FH) {
  if (!xlib_semaphore)  fatal("xlib_semaphore should be set");
  int32* xs = xsOop->convertIntArray();
  int32* ys = ysOop->convertIntArray();
  if (!xs || !ys) {
    prim_failure(FH, BADTYPEERROR);
    return;
  }
  int n = xsOop->length();
  if (n != ysOop->length()) {
    failure(FH, "different number of x and y coordinates");
    return;
  }
  XPoint *points= new XPoint[n];

  for (int i = 0; i < n; i++) {
    points[i].x = xs[i];
    points[i].y = ys[i];
  }

  XFillPolygon(display, d, gc, points, n, shape, mode);
  delete [] points;
}

void XDrawLines_wrap(Display* display, Drawable d, GC gc,
		     objVectorOop xsOop, objVectorOop ysOop,
		     int mode, void* FH) {
  int32* xs = xsOop->convertIntArray();
  int32* ys = ysOop->convertIntArray();
  if (!xs || !ys) {
    prim_failure(FH, BADTYPEERROR);
    return;
  }
  int n = xsOop->length();
  if (n != ysOop->length()) {
    failure(FH, "different number of x and y coordinates");
    return;
  }
  XPoint *points= new XPoint[n];

  for (int i = 0; i < n; i++) {
    points[i].x = xs[i];
    points[i].y = ys[i];
  }

  XDrawLines(display, d, gc, points, n, mode);
  delete [] points;
}


// Note: str is an object vector containing 16-bit unsigned integer
// values. Each such integer represents a 2-byte X char.

void XDrawString16_wrap(Display *display, Drawable drawable,
                        GC gc, int x, int y, objVectorOop str, void *FH) {
  if (!xlib_semaphore)  fatal("xlib_semaphore should be set");
  int len = str->length();
  assert(sizeof(XChar2b) == sizeof(unsigned short),
         "XDrawString16_wrap: bad int sizes");
  XChar2b *xstr = (XChar2b *)str->convertUnsignedShortArray();
  if (xstr == NULL)
    prim_failure(FH, BADTYPEERROR);
  else
    XDrawString16(display, drawable, gc, x, y, xstr, str->length());
}


XFontStruct* XLoadQueryFont_wrap(Display* display, const char* name, void* FH) {
  XFontStruct* font_struct = XLoadQueryFont(display, name);
  if (font_struct == NULL) {
    failure(FH, "font does not exist");
    return NULL;
  }
  return font_struct;
}

void XSetClipRectangle_wrap(Display* display, GC gc,
			    int  x,    int y,
			    int width, int height) {
  XRectangle rect;
  rect.x = x; rect.y = y; rect.width = width; rect.height = height;
  XSetClipRectangles(display, gc, 0, 0, &rect, 1, Unsorted);
}

Atom XClientMessageEvent_atomAt_wrap(XClientMessageEvent* rcvr,
				     unsigned int index, void* FH) {
  if (    sizeof(rcvr->data.l[0]) * index
      >=  sizeof(rcvr->data.l)) {
    prim_failure(FH, BADINDEXERROR);
    return 0;
  }
  return rcvr->data.l[index];
}

int XStringToTextProperty_wrap(XTextProperty* textProperty, char* string) {
  return XStringListToTextProperty(&string, 1, textProperty);
}

int XLookupString_wrap(XKeyEvent* event, char* string, int len,
                       objVectorOop keySym) {
  KeySym ks;
  int n = XLookupString(event, string, len, &ks, NULL);
  if (keySym->length() >= 1) keySym->obj_at_put(0, as_smiOop(ks), false);
  return n;
}

unsigned int XQueryBestStippleWidth(Display* display, Drawable which_screen,
				    unsigned int width, unsigned int height,
				    void* FH) {
  unsigned int width_return, height_return;
  if (!XQueryBestStipple(display, which_screen, width, height, &width_return,
			 &height_return)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return 0;
  }
  return width_return;
}

unsigned int XQueryBestStippleHeight(Display* display, Drawable which_screen,
				     unsigned int width, unsigned int height,
				     void* FH) {
  unsigned int width_return, height_return;
  if (!XQueryBestStipple(display, which_screen, width, height, &width_return,
			 &height_return)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return 0;
  }
  return height_return;
}

unsigned int XQueryBestTileWidth(Display* display, Drawable which_screen,
				 unsigned int width, unsigned int height,
				 void* FH) {
  unsigned int width_return, height_return;
  if (!XQueryBestTile(display, which_screen, width, height, &width_return,
		      &height_return)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return 0;
  }
  return width_return;
}

unsigned int XQueryBestTileHeight(Display* display, Drawable which_screen,
				     unsigned int width, unsigned int height,
				     void* FH) {
  unsigned int width_return, height_return;
  if (!XQueryBestTile(display, which_screen, width, height, &width_return,
		      &height_return)) {
    prim_failure(FH, PRIMITIVEFAILEDERROR);
    return 0;
  }
  return height_return;
}

XVisualInfo* XMatchVisualInfo_wrap(Display* display, int screen, int depth,
				   int vclass, void *FH) {
  XVisualInfo* vinfo_return = new XVisualInfo;
  if (!XMatchVisualInfo(display, screen, depth, vclass, vinfo_return)) {
    delete vinfo_return;
    failure(FH, "no matching visual found");
    return NULL;
  }
  return vinfo_return;
}


int maxAscent(XFontStruct* font_struct) {
  return font_struct->max_bounds.ascent;
}

int maxDescent(XFontStruct* font_struct) {
  return font_struct->max_bounds.descent;
}

int maxCharWidth(XFontStruct* font_struct) {
  return font_struct->max_bounds.width;
}

int perCharWidth(XFontStruct* font_struct, int ch) {
  return font_struct->per_char[ch].width;
}

inline void XFree_XSizeHints_wrap(XSizeHints *p) {XFree((char*)p);}
inline void XFree_XWMHints_wrap  (XWMHints   *p) {XFree((char*)p);}

inline XEvent* asXEvent(XEvent* e) {return e;}


void XStoreBytes_wrap(Display* display, char* buffer, void *FH) {
  if (!XStoreBytes(display, buffer, strlen(buffer))) {
    failure(FH, "xStoreBytes failed");
  }
}

char *XFetchBytes_wrap(Display* display, void *FH) {
  char* result;
  int   size;
  char* buffer;
  if ((result = XFetchBytes(display, &size)) == 0) {
    failure(FH, "xFetchBytes failed");
    return NULL;
  }

  // null terminate the string before returning.
  buffer = (char*) malloc(size + 1);
  strncpy(buffer, result, size);
  buffer[size] = '\0';

  XFree(result);
  return buffer;
}


XImage* XCreateImage_wrap(Display* display, Visual* visual,
                          unsigned int depth, int format,
                          unsigned int width, unsigned int height,
                          int bitmapPadding) {

  // Doesn't work without this. -- dmu 1/2000
  unsigned int pixel_size = depth == 24  ?  32  :  depth;
  unsigned int bit_width = width * pixel_size;

  assert(bitmapPadding == 8 || bitmapPadding == 16 || bitmapPadding == 32,
         "XCreateImage_wrap assumes padding is power of two");

  // round up by padding
  unsigned int padded_bit_width =  (bit_width + bitmapPadding - 1) 
                                & ~(bitmapPadding - 1);

  static const unsigned int bits_per_byte = 8;
  char* data = (char*) malloc((padded_bit_width * height) / bits_per_byte);

  return XCreateImage(
    display, visual, depth, format, 0, data, width, height, bitmapPadding, 0);
}


void XImagePutData_wrap(XImage* image, char* pixels, int pixelsLen, objVectorOop map, void* FH) {
  int mapLen = map->length();
  for (int i = 0;  i < mapLen;  ++i)
    if (!map->obj_at(i)->is_smi()) {
      prim_failure(FH, BADTYPEERROR);
      return;
    }
  
  int w = image->width;
  int h = image->height;
  int x, y;

  if (pixelsLen < (w * h)) return;
  for (y = 0; y < h; y++) {
    int rowOffset = y * w;
    for (x = 0; x < w; x++) {
      unsigned int mapIndex = (unsigned char)pixels[rowOffset + x];
      if (mapIndex < mapLen) {
        oop p = map->obj_at(mapIndex);
        XPutPixel(image, x, y, smiOop(p)->value());
      } else {
        XPutPixel(image, x, y, 0);
      }
    }
  }
}

void XImageGetData_wrap(XImage* image, char* pixels, int pixelsLen) {
  int w = image->width;
  int h = image->height;
  int x, y;

  if (pixelsLen < (w * h)) return;
  for (y = 0; y < h; y++) {
    int rowOffset = y * w;
    for (x = 0; x < w; x++) {
      pixels[rowOffset + x] = (char)XGetPixel(image, x, y);
    }
  }
}

Window XTranslateCoordinates_wrap(Display *display, 
                                  Window src_w, Window dst_w,
                                  objVectorOop src_coords, void *FH) {
  if (src_coords->length() < 3)  {   // Yes, 3. We need a place for the bool
                                     // result of XTranslateCoordinates.
    prim_failure(FH, BADSIZEERROR);
    return NULL;
  }
  oop ox, oy, ret;
  int newX, newY;
  Window child;
  ox = src_coords->obj_at(0);
  oy = src_coords->obj_at(1);
  if (!ox->is_smi() || !oy->is_smi()) {
    prim_failure(FH, BADTYPEERROR);
    return NULL;
  }
  if (XTranslateCoordinates(display, src_w, dst_w, 
                            smiOop(ox)->value(), smiOop(oy)->value(),
                            &newX, &newY, &child))
    ret = Memory->trueObj;
  else
    ret = Memory->falseObj;
  src_coords->obj_at_put(0, as_smiOop(newX), false);
  src_coords->obj_at_put(1, as_smiOop(newY), false);
  src_coords->obj_at_put(2, ret);    // This one needs storecheck!
  return child;
}



static oop timeAsVector(Time time) {
  const unsigned long ms_per_day = 1000 * 60 * 60 * 24;
  objVectorOop result = Memory->objVectorObj->cloneSize(2);
  result->obj_at_put(0, as_smiOop(time/ms_per_day), false);
  result->obj_at_put(1, as_smiOop(time%ms_per_day), false);
  return result;
}

// Extracting time stamp for events
oop xButtonEvent_time(XButtonEvent* evt) {
 return timeAsVector(evt->time); }
oop xCrossingEvent_time(XCrossingEvent* evt) {
 return timeAsVector(evt->time); }
oop xKeyEvent_time(XKeyEvent* evt) {
 return timeAsVector(evt->time); }
oop xMotionEvent_time(XMotionEvent* evt) {
 return timeAsVector(evt->time); }


//
//	X Shape wrappers
//
Bool XShapeQueryExtension_wrap(Display* display) {
	int	ev, err;
  return XShapeQueryExtension(display, &ev, &err);
}

// -- Not really necessary to define this wrapper, can call directly
void XShapeCombineRegion_wrap(Display* display, Drawable d, int dkind,
			      int x, int y, Region r, int op) {
  XShapeCombineRegion(display, d, dkind, x, y, r, op);
}

// NOTE: Pixmap m MUST have a depth of 1
// -- Not really necessary to define this wrapper, can call directly
void XShapeCombineMask_wrap(Display* display, Drawable d, int dkind,
			    int x, int y, Pixmap m, int op) {
  XShapeCombineMask(display, d, dkind, x, y, m, op);
}

// -- Not really necessary to define this wrapper, can call directly
void XShapeCombineShape_wrap(Display* display, Drawable d, int dkind,
			     int x, int y, Pixmap m, int skind, int op) {
  XShapeCombineShape(display, d, dkind, x, y, m, skind, op);
}

#ifdef WHAT_TO_DO
// r is an array of rectangles, nr is number of rectangles in the array.
// No clue how to pass this array from self--not defined for now.
void XShapeCombineRectangles_wrap(Display* display, Drawable d, int dkind,
				  int x, int y, XRectangle *r, int nr,
				  int op, int order) {
  XShapeCombineRectangles(display, d, dkind, x, y, r, nr, op, order);
}
#endif

// special wrapper to handle a single rectangle at a time
void XShapeCombineRectangle_wrap(Display* display, Drawable d, int dkind,
				  int x, int y,
				  unsigned int width, unsigned int height,
				  int op) {
  XRectangle rect;
  rect.x = x; rect.y = y; rect.width = width; rect.height = height;
  XShapeCombineRectangles(display, d, dkind, 0, 0, &rect, 1, op, Unsorted);
}

//
//	X Region wrappers
//
//	The destination region is the first parameter in most calls
//	so the calls are sensible in Self. Function calls generated
//	by primitiveMaker pass the first parameter by reference.
Region XCreateRegion_wrap() {
  Region r;
  r = XCreateRegion();
  return (r);
}

void XDestroyRegion_wrap(Region r) {
  XDestroyRegion(r);
}

Bool XEmptyRegion_wrap(Region r) {
  return XEmptyRegion(r);
}

Bool XEqualRegion_wrap(Region r1, Region r2) {
  return XEqualRegion(r1, r2);
}

void XUnionRegion_wrap(Region d, Region s1, Region s2) {
  XUnionRegion(s1, s2, d);
}

// Reversed source region with rectangle in calling parameters
// to make it look better in the Self template.
void XUnionRectWithRegion_wrap(Region d, Region s,
			       int x, int y, int width, int height) {
  XRectangle rect;
  rect.x = x; rect.y = y; rect.width = width; rect.height = height;
  XUnionRectWithRegion(&rect, s, d);
}

void XIntersectRegion_wrap(Region d, Region s1, Region s2) {
  XIntersectRegion(s1, s2, d);
}

void XXorRegion_wrap(Region d, Region s1, Region s2) {
  XXorRegion(s1, s2, d);
}

void XOffsetRegion_wrap(Region r, int x, int y) {
  XOffsetRegion(r, x, y);
}

void XSubtractRegion_wrap(Region d, Region s1, Region s2) {
  XSubtractRegion(s1, s2, d);
}



# define XEvent_member_wraps_Do(template)				      \
    template(XAnyEvent,xany)						      \
    template(XButtonEvent,xbutton)					      \
    template(XCirculateEvent,xcirculate)				      \
    template(XCirculateRequestEvent,xcirculaterequest)			      \
    template(XClientMessageEvent,xclient)				      \
    template(XColormapEvent,xcolormap)					      \
    template(XConfigureEvent,xconfigure)				      \
    template(XConfigureRequestEvent,xconfigurerequest)			      \
    template(XCreateWindowEvent,xcreatewindow)				      \
    template(XCrossingEvent,xcrossing)					      \
    template(XDestroyWindowEvent,xdestroywindow)			      \
    template(XErrorEvent,xerror)					      \
    template(XExposeEvent,xexpose)					      \
    template(XFocusChangeEvent,xfocus)					      \
    template(XGraphicsExposeEvent,xgraphicsexpose)			      \
    template(XGravityEvent,xgravity)					      \
    template(XKeyEvent,xkey)						      \
    template(XKeymapEvent,xkeymap)					      \
    template(XMapEvent,xmap)						      \
    template(XMapRequestEvent,xmaprequest)				      \
    template(XMappingEvent,xmapping)					      \
    template(XMotionEvent,xmotion)					      \
    template(XNoExposeEvent,xnoexpose)					      \
    template(XPropertyEvent,xproperty)					      \
    template(XReparentEvent,xreparent)					      \
    template(XResizeRequestEvent,xresizerequest)			      \
    template(XSelectionClearEvent,xselectionclear)			      \
    template(XSelectionEvent,xselection)				      \
    template(XSelectionRequestEvent,xselectionrequest)			      \
    template(XUnmapEvent,xunmap)					      \
    template(XVisibilityEvent,xvisibility)

# define define_XEvent_member_wrap(type,member)				      \
    type* CONC3(get_,member,_wrap) (XEvent* e) {return & e->member;}

XEvent_member_wraps_Do(define_XEvent_member_wrap)


# define WHAT_GLUE FUNCTIONS
# undef  PRIMITIVE_GLUE_FLAG_CODE
# define PRIMITIVE_GLUE_FLAG_CODE BlockGlueFlag gf(xlib_semaphore); // must be right before xlib_glue
    xlib_glue
# undef WHAT_GLUE
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

#include "_glueDefs.cpp.incl"
#include "transcendental.primMaker.hh"

#include <math.h>

#ifdef DYNAMIC
VERIFYCHECKSUM
#endif
 
double log2(double x) {
  return log(x) / log(2.0);
}

#define WHAT_GLUE FUNCTIONS
  transcendental_glue
#undef WHAT_GLUE
/* Sun-$Revision: 30.5 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include "_glueDefs.cpp.incl"
# include "stat.primMaker.hh"

#ifdef DYNAMIC
VERIFYCHECKSUM
#endif

// also declared in unixPrims.h
// (but unixPrims.h cannot be included here without including other files)
extern const char *UnixFile_seal;

const char* stat_type_seal = "stat_type_seal";

typedef struct stat stat_type;

stat_type* stat_wrapper(char* path, void* FH) {
  stat_type *buf;
  buf = new stat_type;
  if (stat(path, buf) == -1) {
    unix_failure(FH);
    return NULL;
  }
  return buf;
}

stat_type* lstat_wrapper(char* path, void* FH) {
  stat_type *buf;
  buf = new stat_type;
  if (lstat(path, buf) == -1) {
    unix_failure(FH);
    return NULL;
  }
  return buf;
}

stat_type* fstat_wrapper(int fd, void* FH) {
  stat_type *buf;
  buf = new stat_type;
  if (fstat(fd, buf) == -1) {
    unix_failure(FH);
    return NULL;
  }
  return buf;
}

# include <time.h>
oop time_to_vector(time_t t) {
  const int seconds_per_day = 60 * 60 * 24;
  objVectorOop res = Memory->objVectorObj->cloneSize(2);
  res->obj_at_put(0, as_smiOop(t / seconds_per_day), false);         //days
  res->obj_at_put(1, as_smiOop((t % seconds_per_day) * 1000), false);//msecs
  return res;
}

bool s_isdir_wrapper(stat_type *buf)  { return S_ISDIR(buf->st_mode); }
bool s_ischr_wrapper(stat_type *buf)  { return S_ISCHR(buf->st_mode); }
bool s_isblk_wrapper(stat_type *buf)  { return S_ISBLK(buf->st_mode); }
bool s_isreg_wrapper(stat_type *buf)  { return S_ISREG(buf->st_mode); }
bool s_islnk_wrapper(stat_type *buf)  { return S_ISLNK(buf->st_mode); }
bool s_issock_wrapper(stat_type *buf) { return S_ISSOCK(buf->st_mode); }
bool s_isfifo_wrapper(stat_type *buf) { return S_ISFIFO(buf->st_mode); }

oop stat_st_atime_wrapper(stat_type* s) { return time_to_vector(s->st_atime); }
oop stat_st_mtime_wrapper(stat_type* s) { return time_to_vector(s->st_mtime); }
oop stat_st_ctime_wrapper(stat_type* s) { return time_to_vector(s->st_ctime); }

# define WHAT_GLUE FUNCTIONS
    stat_glue
# undef WHAT_GLUE
// This file was generated from the xgl header file and contains
// C++ wrapper functions around xgl structure accesses from Self.
// Do NOT edit this file by hand!

// Copyright 1992-2012 AUTHORS.
// See the LICENSE file for license information.

#include <_glueDefs.c.incl> // for glue preprocessor macros
#include <X11/Xlib.h>             // for Window and Display definitions
#include <xgl/xgl.h>              // duh
#include "xgl.primMaker.h"        // definitions generated by primitiveMaker

VERIFYCHECKSUM

#define WHAT_GLUE C_DECLS
   // xgl_entries (apparently this prints the C comments in your templates)
#undef WHAT_GLUE

// apparently the C_DECLS preprocessor macro is supposed to do this
extern char * Display_seal;
char * Window_seal = "Window_seal";  // this seal will not actually match 
                                     // Window_seal defined in xlib


// these are simple here to make it easier to template these things 
// - should match those in <xgl/xgl.h>
typedef void Xgl_object_obj;
typedef Xgl_object_obj Xgl_sys_state_obj;

// these are the seals for the previous types - names must match
char* Xgl_object_seal = "Xgl_object_seal";
char* Xgl_object_obj_seal = "Xgl_object_obj_seal";
char* Xgl_sys_state_obj_seal = "Xgl_sys_state_obj_seal";


// these are convenience typedefs so we can have c-style vectors
// - they are not as powerful as byteVectors, but they remove one 
// layer of extra copying
typedef int Int_vec_proxy;
typedef long Long_vec_proxy;
typedef float Float_vec_proxy;
typedef double Double_vec_proxy;
typedef unsigned char Unsigned_char_vec_proxy;

// These are seals for the previous types - names must match
char* Int_vec_proxy_seal = "Int_vec_proxy_seal";
char* Long_vec_proxy_seal = "Long_vec_proxy_seal";
char* Float_vec_proxy_seal = "Float_vec_proxy_seal";
char* Double_vec_proxy_seal = "Double_vec_proxy_seal";
char* Unsigned_char_vec_proxy_seal = "Unsigned_char_vec_proxy_seal";


// this is a hack, but I do not know what stream info xgl is expecting here
char * Xgl_stream_info_seal = "Xgl_stream_info_seal";

// These are seals for the Xgl types that will become proxies.
char* Xgl_X_window_seal = "Xgl_X_window_seal";
char* Xgl_arc_ad3d_seal = "Xgl_arc_ad3d_seal";
char* Xgl_arc_af3d_seal = "Xgl_arc_af3d_seal";
char* Xgl_arc_d2d_seal = "Xgl_arc_d2d_seal";
char* Xgl_arc_d3d_seal = "Xgl_arc_d3d_seal";
char* Xgl_arc_f2d_seal = "Xgl_arc_f2d_seal";
char* Xgl_arc_f3d_seal = "Xgl_arc_f3d_seal";
char* Xgl_arc_i2d_seal = "Xgl_arc_i2d_seal";
char* Xgl_arc_list_seal = "Xgl_arc_list_seal";
char* Xgl_bbox_seal = "Xgl_bbox_seal";
char* Xgl_bbox_d2d_seal = "Xgl_bbox_d2d_seal";
char* Xgl_bbox_d3d_seal = "Xgl_bbox_d3d_seal";
char* Xgl_bbox_f2d_seal = "Xgl_bbox_f2d_seal";
char* Xgl_bbox_f3d_seal = "Xgl_bbox_f3d_seal";
char* Xgl_bbox_i2d_seal = "Xgl_bbox_i2d_seal";
char* Xgl_bbox_status_seal = "Xgl_bbox_status_seal";
char* Xgl_bounds_d1d_seal = "Xgl_bounds_d1d_seal";
char* Xgl_bounds_d2d_seal = "Xgl_bounds_d2d_seal";
char* Xgl_bounds_d3d_seal = "Xgl_bounds_d3d_seal";
char* Xgl_bounds_f1d_seal = "Xgl_bounds_f1d_seal";
char* Xgl_bounds_f2d_seal = "Xgl_bounds_f2d_seal";
char* Xgl_bounds_f3d_seal = "Xgl_bounds_f3d_seal";
char* Xgl_bounds_i2d_seal = "Xgl_bounds_i2d_seal";
char* Xgl_circle_ad3d_seal = "Xgl_circle_ad3d_seal";
char* Xgl_circle_af3d_seal = "Xgl_circle_af3d_seal";
char* Xgl_circle_d2d_seal = "Xgl_circle_d2d_seal";
char* Xgl_circle_d3d_seal = "Xgl_circle_d3d_seal";
char* Xgl_circle_f2d_seal = "Xgl_circle_f2d_seal";
char* Xgl_circle_f3d_seal = "Xgl_circle_f3d_seal";
char* Xgl_circle_i2d_seal = "Xgl_circle_i2d_seal";
char* Xgl_circle_list_seal = "Xgl_circle_list_seal";
char* Xgl_color_seal = "Xgl_color_seal";
char* Xgl_color_facet_seal = "Xgl_color_facet_seal";
char* Xgl_color_homogeneous_seal = "Xgl_color_homogeneous_seal";
char* Xgl_color_list_seal = "Xgl_color_list_seal";
char* Xgl_color_normal_facet_seal = "Xgl_color_normal_facet_seal";
char* Xgl_color_rgb_seal = "Xgl_color_rgb_seal";
char* Xgl_color_rgbw_seal = "Xgl_color_rgbw_seal";
char* Xgl_color_type_supported_seal = "Xgl_color_type_supported_seal";
char* Xgl_curve_color_spline_seal = "Xgl_curve_color_spline_seal";
char* Xgl_ell_ad3d_seal = "Xgl_ell_ad3d_seal";
char* Xgl_ell_af3d_seal = "Xgl_ell_af3d_seal";
char* Xgl_ell_d3d_seal = "Xgl_ell_d3d_seal";
char* Xgl_ell_f3d_seal = "Xgl_ell_f3d_seal";
char* Xgl_ell_list_seal = "Xgl_ell_list_seal";
char* Xgl_error_info_seal = "Xgl_error_info_seal";
char* Xgl_facet_seal = "Xgl_facet_seal";
char* Xgl_facet_list_seal = "Xgl_facet_list_seal";
char* Xgl_facet_list_list_seal = "Xgl_facet_list_list_seal";
char* Xgl_inquire_seal = "Xgl_inquire_seal";
char* Xgl_irect_seal = "Xgl_irect_seal";
char* Xgl_irect_list_seal = "Xgl_irect_list_seal";
char* Xgl_matrix_d2d_seal = "Xgl_matrix_d2d_seal";
char* Xgl_matrix_d3d_seal = "Xgl_matrix_d3d_seal";
char* Xgl_matrix_f2d_seal = "Xgl_matrix_f2d_seal";
char* Xgl_matrix_f3d_seal = "Xgl_matrix_f3d_seal";
char* Xgl_matrix_i2d_seal = "Xgl_matrix_i2d_seal";
char* Xgl_mono_text_seal = "Xgl_mono_text_seal";
char* Xgl_mono_text_list_seal = "Xgl_mono_text_list_seal";
char* Xgl_normal_facet_seal = "Xgl_normal_facet_seal";
char* Xgl_nu_bspline_curve_seal = "Xgl_nu_bspline_curve_seal";
char* Xgl_nurbs_curve_seal = "Xgl_nurbs_curve_seal";
char* Xgl_nurbs_surf_seal = "Xgl_nurbs_surf_seal";
char* Xgl_nurbs_surf_simple_geom_seal = "Xgl_nurbs_surf_simple_geom_seal";
char* Xgl_obj_desc_seal = "Xgl_obj_desc_seal";
char* Xgl_pick_info_seal = "Xgl_pick_info_seal";
char* Xgl_plane_seal = "Xgl_plane_seal";
char* Xgl_plane_list_seal = "Xgl_plane_list_seal";
char* Xgl_pt_seal = "Xgl_pt_seal";
char* Xgl_pt_color_d2d_seal = "Xgl_pt_color_d2d_seal";
char* Xgl_pt_color_d3d_seal = "Xgl_pt_color_d3d_seal";
char* Xgl_pt_color_data_f3d_seal = "Xgl_pt_color_data_f3d_seal";
char* Xgl_pt_color_f2d_seal = "Xgl_pt_color_f2d_seal";
char* Xgl_pt_color_f3d_seal = "Xgl_pt_color_f3d_seal";
char* Xgl_pt_color_flag_d3d_seal = "Xgl_pt_color_flag_d3d_seal";
char* Xgl_pt_color_flag_data_f3d_seal = "Xgl_pt_color_flag_data_f3d_seal";
char* Xgl_pt_color_flag_f3d_seal = "Xgl_pt_color_flag_f3d_seal";
char* Xgl_pt_color_i2d_seal = "Xgl_pt_color_i2d_seal";
char* Xgl_pt_color_normal_d3d_seal = "Xgl_pt_color_normal_d3d_seal";
char* Xgl_pt_color_normal_data_f3d_seal = "Xgl_pt_color_normal_data_f3d_seal";
char* Xgl_pt_color_normal_f3d_seal = "Xgl_pt_color_normal_f3d_seal";
char* Xgl_pt_color_normal_flag_d3d_seal = "Xgl_pt_color_normal_flag_d3d_seal";
char* Xgl_pt_color_normal_flag_data_f3d_seal = "Xgl_pt_color_normal_flag_data_f3d_seal";
char* Xgl_pt_color_normal_flag_f3d_seal = "Xgl_pt_color_normal_flag_f3d_seal";
char* Xgl_pt_d2d_seal = "Xgl_pt_d2d_seal";
char* Xgl_pt_d2h_seal = "Xgl_pt_d2h_seal";
char* Xgl_pt_d3d_seal = "Xgl_pt_d3d_seal";
char* Xgl_pt_d3h_seal = "Xgl_pt_d3h_seal";
char* Xgl_pt_data_f3d_seal = "Xgl_pt_data_f3d_seal";
char* Xgl_pt_f2d_seal = "Xgl_pt_f2d_seal";
char* Xgl_pt_f2h_seal = "Xgl_pt_f2h_seal";
char* Xgl_pt_f3d_seal = "Xgl_pt_f3d_seal";
char* Xgl_pt_f3h_seal = "Xgl_pt_f3h_seal";
char* Xgl_pt_flag_d2d_seal = "Xgl_pt_flag_d2d_seal";
char* Xgl_pt_flag_d3d_seal = "Xgl_pt_flag_d3d_seal";
char* Xgl_pt_flag_data_f3d_seal = "Xgl_pt_flag_data_f3d_seal";
char* Xgl_pt_flag_f2d_seal = "Xgl_pt_flag_f2d_seal";
char* Xgl_pt_flag_f3d_seal = "Xgl_pt_flag_f3d_seal";
char* Xgl_pt_flag_i2d_seal = "Xgl_pt_flag_i2d_seal";
char* Xgl_pt_i2d_seal = "Xgl_pt_i2d_seal";
char* Xgl_pt_i2h_seal = "Xgl_pt_i2h_seal";
char* Xgl_pt_list_seal = "Xgl_pt_list_seal";
char* Xgl_pt_list_list_seal = "Xgl_pt_list_list_seal";
char* Xgl_pt_normal_d3d_seal = "Xgl_pt_normal_d3d_seal";
char* Xgl_pt_normal_data_f3d_seal = "Xgl_pt_normal_data_f3d_seal";
char* Xgl_pt_normal_f3d_seal = "Xgl_pt_normal_f3d_seal";
char* Xgl_pt_normal_flag_d3d_seal = "Xgl_pt_normal_flag_d3d_seal";
char* Xgl_pt_normal_flag_data_f3d_seal = "Xgl_pt_normal_flag_data_f3d_seal";
char* Xgl_pt_normal_flag_f3d_seal = "Xgl_pt_normal_flag_f3d_seal";
char* Xgl_pt_type_supported_seal = "Xgl_pt_type_supported_seal";
char* Xgl_rect_ad3d_seal = "Xgl_rect_ad3d_seal";
char* Xgl_rect_af3d_seal = "Xgl_rect_af3d_seal";
char* Xgl_rect_d2d_seal = "Xgl_rect_d2d_seal";
char* Xgl_rect_d3d_seal = "Xgl_rect_d3d_seal";
char* Xgl_rect_f2d_seal = "Xgl_rect_f2d_seal";
char* Xgl_rect_f3d_seal = "Xgl_rect_f3d_seal";
char* Xgl_rect_i2d_seal = "Xgl_rect_i2d_seal";
char* Xgl_rect_list_seal = "Xgl_rect_list_seal";
char* Xgl_render_component_desc_seal = "Xgl_render_component_desc_seal";
char* Xgl_segment_seal = "Xgl_segment_seal";
char* Xgl_spline_data_seal = "Xgl_spline_data_seal";
char* Xgl_surf_color_spline_seal = "Xgl_surf_color_spline_seal";
char* Xgl_surf_data_spline_seal = "Xgl_surf_data_spline_seal";
char* Xgl_surf_data_spline_list_seal = "Xgl_surf_data_spline_list_seal";
char* Xgl_texture_blend_rgb_seal = "Xgl_texture_blend_rgb_seal";
char* Xgl_texture_color_comp_info_seal = "Xgl_texture_color_comp_info_seal";
char* Xgl_texture_decal_rgb_seal = "Xgl_texture_decal_rgb_seal";
char* Xgl_texture_desc_seal = "Xgl_texture_desc_seal";
char* Xgl_texture_interp_info_seal = "Xgl_texture_interp_info_seal";
char* Xgl_texture_mipmap_desc_seal = "Xgl_texture_mipmap_desc_seal";
char* Xgl_threshold_seal = "Xgl_threshold_seal";
char* Xgl_trim_curve_seal = "Xgl_trim_curve_seal";
char* Xgl_trim_loop_seal = "Xgl_trim_loop_seal";
char* Xgl_trim_loop_list_seal = "Xgl_trim_loop_list_seal";


// accessor wrappers for various xgl structures
// for Xgl_X_window
Xgl_X_window* xgl_x_window_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_X_window, count);
}
void xgl_x_window_delete (Xgl_X_window* rcvr) {
  delete [] rcvr;
}

Xgl_X_window* xgl_x_window_at (Xgl_X_window* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_x_window_at_put (Xgl_X_window* rcvr, fint which, Xgl_X_window* xWindow) {
  rcvr[which] = *xWindow;
}

void xgl_x_window_set (Xgl_X_window* rcvr,
  void* xDisplay,
  int xScreen,
  unsigned long xWindow) {
  rcvr->X_display = xDisplay;
  rcvr->X_screen = xScreen;
  rcvr->X_window = xWindow;
}


// for Xgl_arc_ad3d
Xgl_arc_ad3d* xgl_arc_ad3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_ad3d, count);
}
void xgl_arc_ad3d_delete (Xgl_arc_ad3d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_ad3d* xgl_arc_ad3d_at (Xgl_arc_ad3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_ad3d_at_put (Xgl_arc_ad3d* rcvr, fint which, Xgl_arc_ad3d* arcAd3d) {
  rcvr[which] = *arcAd3d;
}

Xgl_pt_flag_d3d* xgl_arc_ad3d_center (Xgl_arc_ad3d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_ad3d_center (Xgl_arc_ad3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}

void xgl_arc_ad3d_set (Xgl_arc_ad3d* rcvr,
                      Xgl_pt_flag_d3d* center,
                      double radius,
                      double startAngle,
                      double stopAngle) {
  rcvr->center = *center;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_af3d
Xgl_arc_af3d* xgl_arc_af3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_af3d, count);
}
void xgl_arc_af3d_delete (Xgl_arc_af3d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_af3d* xgl_arc_af3d_at (Xgl_arc_af3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_af3d_at_put (Xgl_arc_af3d* rcvr, fint which, Xgl_arc_af3d* arcAf3d) {
  rcvr[which] = *arcAf3d;
}

Xgl_pt_flag_f3d* xgl_arc_af3d_center (Xgl_arc_af3d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_af3d_center (Xgl_arc_af3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}

void xgl_arc_af3d_set (Xgl_arc_af3d* rcvr,
	Xgl_pt_flag_f3d* center,
	float radius,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_d2d
Xgl_arc_d2d* xgl_arc_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_d2d, count);
}
void xgl_arc_d2d_delete (Xgl_arc_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_d2d* xgl_arc_d2d_at (Xgl_arc_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_d2d_at_put (Xgl_arc_d2d* rcvr, fint which, Xgl_arc_d2d* arcD2d) {
  rcvr[which] = *arcD2d;
}

Xgl_pt_flag_d2d* xgl_arc_d2d_center (Xgl_arc_d2d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_d2d_center (Xgl_arc_d2d* rcvr, Xgl_pt_flag_d2d* center) {
   rcvr->center = *center;
}

void xgl_arc_d2d_set (Xgl_arc_d2d* rcvr,
	Xgl_pt_flag_d2d* center,
	double radius,
	double startAngle,
	double stopAngle) {
  rcvr->center = *center;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_d3d
Xgl_arc_d3d* xgl_arc_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_d3d, count);
}
void xgl_arc_d3d_delete (Xgl_arc_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_d3d* xgl_arc_d3d_at (Xgl_arc_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_d3d_at_put (Xgl_arc_d3d* rcvr, fint which, Xgl_arc_d3d* arcD3d) {
  rcvr[which] = *arcD3d;
}

Xgl_pt_flag_d3d* xgl_arc_d3d_center (Xgl_arc_d3d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_d3d_center (Xgl_arc_d3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}

Xgl_pt_d3d* xgl_arc_d3d_dir (Xgl_arc_d3d* rcvr) {
   return rcvr->dir;
}
void xgl_arc_d3d_set (Xgl_arc_d3d* rcvr,
	Xgl_pt_flag_d3d* center,
	bool dirNormal,
	bool dirNormalized,
	double radius,
	double startAngle,
	double stopAngle) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_f2d
Xgl_arc_f2d* xgl_arc_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_f2d, count);
}
void xgl_arc_f2d_delete (Xgl_arc_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_f2d* xgl_arc_f2d_at (Xgl_arc_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_f2d_at_put (Xgl_arc_f2d* rcvr, fint which, Xgl_arc_f2d* arcF2d) {
  rcvr[which] = *arcF2d;
}

Xgl_pt_flag_f2d* xgl_arc_f2d_center (Xgl_arc_f2d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_f2d_center (Xgl_arc_f2d* rcvr, Xgl_pt_flag_f2d* center) {
   rcvr->center = *center;
}

void xgl_arc_f2d_set (Xgl_arc_f2d* rcvr,
	Xgl_pt_flag_f2d* center,
	float radius,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_f3d
Xgl_arc_f3d* xgl_arc_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_f3d, count);
}
void xgl_arc_f3d_delete (Xgl_arc_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_f3d* xgl_arc_f3d_at (Xgl_arc_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_f3d_at_put (Xgl_arc_f3d* rcvr, fint which, Xgl_arc_f3d* arcF3d) {
  rcvr[which] = *arcF3d;
}

Xgl_pt_flag_f3d* xgl_arc_f3d_center (Xgl_arc_f3d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_f3d_center (Xgl_arc_f3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}

Xgl_pt_f3d* xgl_arc_f3d_dir (Xgl_arc_f3d* rcvr) {
   return rcvr->dir;
}
void xgl_arc_f3d_set (Xgl_arc_f3d* rcvr,
	Xgl_pt_flag_f3d* center,
	bool dirNormal,
	bool dirNormalized,
	float radius,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_i2d
Xgl_arc_i2d* xgl_arc_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_i2d, count);
}
void xgl_arc_i2d_delete (Xgl_arc_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_arc_i2d* xgl_arc_i2d_at (Xgl_arc_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_i2d_at_put (Xgl_arc_i2d* rcvr, fint which, Xgl_arc_i2d* arcI2d) {
  rcvr[which] = *arcI2d;
}

Xgl_pt_flag_i2d* xgl_arc_i2d_center (Xgl_arc_i2d* rcvr) {
   return &(rcvr->center);
}
void xgl_arc_i2d_center (Xgl_arc_i2d* rcvr, Xgl_pt_flag_i2d* center) {
   rcvr->center = *center;
}

void xgl_arc_i2d_set (Xgl_arc_i2d* rcvr,
	Xgl_pt_flag_i2d* center,
	unsigned long radius,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->radius = radius;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_arc_list
Xgl_arc_list* xgl_arc_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_arc_list, count);
}
void xgl_arc_list_delete (Xgl_arc_list* rcvr) {
  delete [] rcvr;
}

Xgl_arc_list* xgl_arc_list_at (Xgl_arc_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_arc_list_at_put (Xgl_arc_list* rcvr, fint which, Xgl_arc_list* arcList) {
  rcvr[which] = *arcList;
}

Xgl_arc_ad3d* xgl_arc_list_arcs_ad3d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.ad3d;
}
void xgl_arc_list_arcs_ad3d (Xgl_arc_list* rcvr, Xgl_arc_ad3d* ad3d) {
   rcvr->arcs.ad3d = ad3d;
}

Xgl_arc_af3d* xgl_arc_list_arcs_af3d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.af3d;
}
void xgl_arc_list_arcs_af3d (Xgl_arc_list* rcvr, Xgl_arc_af3d* af3d) {
   rcvr->arcs.af3d = af3d;
}

Xgl_arc_d2d* xgl_arc_list_arcs_d2d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.d2d;
}
void xgl_arc_list_arcs_d2d (Xgl_arc_list* rcvr, Xgl_arc_d2d* d2d) {
   rcvr->arcs.d2d = d2d;
}

Xgl_arc_d3d* xgl_arc_list_arcs_d3d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.d3d;
}
void xgl_arc_list_arcs_d3d (Xgl_arc_list* rcvr, Xgl_arc_d3d* d3d) {
   rcvr->arcs.d3d = d3d;
}

Xgl_arc_f2d* xgl_arc_list_arcs_f2d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.f2d;
}
void xgl_arc_list_arcs_f2d (Xgl_arc_list* rcvr, Xgl_arc_f2d* f2d) {
   rcvr->arcs.f2d = f2d;
}

Xgl_arc_f3d* xgl_arc_list_arcs_f3d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.f3d;
}
void xgl_arc_list_arcs_f3d (Xgl_arc_list* rcvr, Xgl_arc_f3d* f3d) {
   rcvr->arcs.f3d = f3d;
}

Xgl_arc_i2d* xgl_arc_list_arcs_i2d (Xgl_arc_list* rcvr) {
   return rcvr->arcs.i2d;
}
void xgl_arc_list_arcs_i2d (Xgl_arc_list* rcvr, Xgl_arc_i2d* i2d) {
   rcvr->arcs.i2d = i2d;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_ad3d* ad3d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.ad3d = ad3d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_af3d* af3d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.af3d = af3d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_d2d* d2d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.d2d = d2d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_d3d* d3d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.d3d = d3d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_f2d* f2d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.f2d = f2d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_f3d* f3d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.f3d = f3d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}

void xgl_arc_list_set (Xgl_arc_list* rcvr,
	Xgl_arc_i2d* i2d,
	Xgl_bbox* bbox,
	unsigned long numArcs,
	Xgl_multiarc_type type) {
  rcvr->arcs.i2d = i2d;
  rcvr->bbox = bbox;
  rcvr->num_arcs = numArcs;
  rcvr->type = type;
}


// for Xgl_bbox
Xgl_bbox* xgl_bbox_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox, count);
}
void xgl_bbox_delete (Xgl_bbox* rcvr) {
  delete [] rcvr;
}

Xgl_bbox* xgl_bbox_at (Xgl_bbox* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_at_put (Xgl_bbox* rcvr, fint which, Xgl_bbox* bbox) {
  rcvr[which] = *bbox;
}

Xgl_bounds_f2d* xgl_bbox_box_f2d (Xgl_bbox* rcvr) {
   return &(rcvr->box.f2d);
}
void xgl_bbox_box_f2d (Xgl_bbox* rcvr, Xgl_bounds_f2d* f2d) {
   rcvr->box.f2d = *f2d;
}

Xgl_bounds_f3d* xgl_bbox_box_f3d (Xgl_bbox* rcvr) {
   return &(rcvr->box.f3d);
}
void xgl_bbox_box_f3d (Xgl_bbox* rcvr, Xgl_bounds_f3d* f3d) {
   rcvr->box.f3d = *f3d;
}

Xgl_bounds_i2d* xgl_bbox_box_i2d (Xgl_bbox* rcvr) {
   return &(rcvr->box.i2d);
}
void xgl_bbox_box_i2d (Xgl_bbox* rcvr, Xgl_bounds_i2d* i2d) {
   rcvr->box.i2d = *i2d;
}

void xgl_bbox_set (Xgl_bbox* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_f2d* box_f2d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.f2d = *box_f2d;
}

void xgl_bbox_set (Xgl_bbox* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_f3d* box_f3d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.f3d = *box_f3d;
}

void xgl_bbox_set (Xgl_bbox* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_i2d* box_i2d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.i2d = *box_i2d;
}


// for Xgl_bbox_d2d
Xgl_bbox_d2d* xgl_bbox_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_d2d, count);
}
void xgl_bbox_d2d_delete (Xgl_bbox_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_d2d* xgl_bbox_d2d_at (Xgl_bbox_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_d2d_at_put (Xgl_bbox_d2d* rcvr, fint which, Xgl_bbox_d2d* bboxD2d) {
  rcvr[which] = *bboxD2d;
}

Xgl_bounds_d2d* xgl_bbox_d2d_box_d2d (Xgl_bbox_d2d* rcvr) {
   return &(rcvr->box.d2d);
}
void xgl_bbox_d2d_box_d2d (Xgl_bbox_d2d* rcvr, Xgl_bounds_d2d* d2d) {
   rcvr->box.d2d = *d2d;
}

void xgl_bbox_d2d_set (Xgl_bbox_d2d* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_d2d* box_d2d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.d2d = *box_d2d;
}


// for Xgl_bbox_d3d
Xgl_bbox_d3d* xgl_bbox_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_d3d, count);
}
void xgl_bbox_d3d_delete (Xgl_bbox_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_d3d* xgl_bbox_d3d_at (Xgl_bbox_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_d3d_at_put (Xgl_bbox_d3d* rcvr, fint which, Xgl_bbox_d3d* bboxD3d) {
  rcvr[which] = *bboxD3d;
}

Xgl_bounds_d3d* xgl_bbox_d3d_box_d3d (Xgl_bbox_d3d* rcvr) {
   return &(rcvr->box.d3d);
}
void xgl_bbox_d3d_box_d3d (Xgl_bbox_d3d* rcvr, Xgl_bounds_d3d* d3d) {
   rcvr->box.d3d = *d3d;
}

void xgl_bbox_d3d_set (Xgl_bbox_d3d* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_d3d* box_d3d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.d3d = *box_d3d;
}


// for Xgl_bbox_f2d
Xgl_bbox_f2d* xgl_bbox_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_f2d, count);
}
void xgl_bbox_f2d_delete (Xgl_bbox_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_f2d* xgl_bbox_f2d_at (Xgl_bbox_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_f2d_at_put (Xgl_bbox_f2d* rcvr, fint which, Xgl_bbox_f2d* bboxF2d) {
  rcvr[which] = *bboxF2d;
}

Xgl_bounds_f2d* xgl_bbox_f2d_box_f2d (Xgl_bbox_f2d* rcvr) {
   return &(rcvr->box.f2d);
}
void xgl_bbox_f2d_box_f2d (Xgl_bbox_f2d* rcvr, Xgl_bounds_f2d* f2d) {
   rcvr->box.f2d = *f2d;
}

void xgl_bbox_f2d_set (Xgl_bbox_f2d* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_f2d* box_f2d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.f2d = *box_f2d;
}


// for Xgl_bbox_f3d
Xgl_bbox_f3d* xgl_bbox_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_f3d, count);
}
void xgl_bbox_f3d_delete (Xgl_bbox_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_f3d* xgl_bbox_f3d_at (Xgl_bbox_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_f3d_at_put (Xgl_bbox_f3d* rcvr, fint which, Xgl_bbox_f3d* bboxF3d) {
  rcvr[which] = *bboxF3d;
}

Xgl_bounds_f3d* xgl_bbox_f3d_box_f3d (Xgl_bbox_f3d* rcvr) {
   return &(rcvr->box.f3d);
}
void xgl_bbox_f3d_box_f3d (Xgl_bbox_f3d* rcvr, Xgl_bounds_f3d* f3d) {
   rcvr->box.f3d = *f3d;
}

void xgl_bbox_f3d_set (Xgl_bbox_f3d* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_f3d* box_f3d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.f3d = *box_f3d;
}


// for Xgl_bbox_i2d
Xgl_bbox_i2d* xgl_bbox_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_i2d, count);
}
void xgl_bbox_i2d_delete (Xgl_bbox_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_i2d* xgl_bbox_i2d_at (Xgl_bbox_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_i2d_at_put (Xgl_bbox_i2d* rcvr, fint which, Xgl_bbox_i2d* bboxI2d) {
  rcvr[which] = *bboxI2d;
}

Xgl_bounds_i2d* xgl_bbox_i2d_box_i2d (Xgl_bbox_i2d* rcvr) {
   return &(rcvr->box.i2d);
}
void xgl_bbox_i2d_box_i2d (Xgl_bbox_i2d* rcvr, Xgl_bounds_i2d* i2d) {
   rcvr->box.i2d = *i2d;
}

void xgl_bbox_i2d_set (Xgl_bbox_i2d* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_bounds_i2d* box_i2d) {
  rcvr->bbox_type = bboxType;
  rcvr->box.i2d = *box_i2d;
}


// for Xgl_bbox_status
Xgl_bbox_status* xgl_bbox_status_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bbox_status, count);
}
void xgl_bbox_status_delete (Xgl_bbox_status* rcvr) {
  delete [] rcvr;
}

Xgl_bbox_status* xgl_bbox_status_at (Xgl_bbox_status* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bbox_status_at_put (Xgl_bbox_status* rcvr, fint which, Xgl_bbox_status* bboxStatus) {
  rcvr[which] = *bboxStatus;
}

Xgl_geom_status xgl_bbox_status_box_status (Xgl_bbox_status* rcvr) {
   return rcvr->box.status;
}
void xgl_bbox_status_box_status (Xgl_bbox_status* rcvr, Xgl_geom_status status) {
   rcvr->box.status = status;
}

void xgl_bbox_status_set (Xgl_bbox_status* rcvr,
	Xgl_bbox_type bboxType,
	Xgl_geom_status box_status) {
  rcvr->bbox_type = bboxType;
  rcvr->box.status = box_status;
}


// for Xgl_bounds_d1d
Xgl_bounds_d1d* xgl_bounds_d1d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_d1d, count);
}
void xgl_bounds_d1d_delete (Xgl_bounds_d1d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_d1d* xgl_bounds_d1d_at (Xgl_bounds_d1d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_d1d_at_put (Xgl_bounds_d1d* rcvr, fint which, Xgl_bounds_d1d* boundsD1d) {
  rcvr[which] = *boundsD1d;
}


// for Xgl_bounds_d2d
Xgl_bounds_d2d* xgl_bounds_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_d2d, count);
}
void xgl_bounds_d2d_delete (Xgl_bounds_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_d2d* xgl_bounds_d2d_at (Xgl_bounds_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_d2d_at_put (Xgl_bounds_d2d* rcvr, fint which, Xgl_bounds_d2d* boundsD2d) {
  rcvr[which] = *boundsD2d;
}

void xgl_bounds_d2d_set (Xgl_bounds_d2d* rcvr,
	double xmax,
	double xmin,
	double ymax,
	double ymin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
}


// for Xgl_bounds_d3d
Xgl_bounds_d3d* xgl_bounds_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_d3d, count);
}
void xgl_bounds_d3d_delete (Xgl_bounds_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_d3d* xgl_bounds_d3d_at (Xgl_bounds_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_d3d_at_put (Xgl_bounds_d3d* rcvr, fint which, Xgl_bounds_d3d* boundsD3d) {
  rcvr[which] = *boundsD3d;
}

void xgl_bounds_d3d_set (Xgl_bounds_d3d* rcvr,
	double xmax,
	double xmin,
	double ymax,
	double ymin,
	double zmax,
	double zmin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
  rcvr->zmax = zmax;
  rcvr->zmin = zmin;
}


// for Xgl_bounds_f1d
Xgl_bounds_f1d* xgl_bounds_f1d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_f1d, count);
}
void xgl_bounds_f1d_delete (Xgl_bounds_f1d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_f1d* xgl_bounds_f1d_at (Xgl_bounds_f1d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_f1d_at_put (Xgl_bounds_f1d* rcvr, fint which, Xgl_bounds_f1d* boundsF1d) {
  rcvr[which] = *boundsF1d;
}


// for Xgl_bounds_f2d
Xgl_bounds_f2d* xgl_bounds_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_f2d, count);
}
void xgl_bounds_f2d_delete (Xgl_bounds_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_f2d* xgl_bounds_f2d_at (Xgl_bounds_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_f2d_at_put (Xgl_bounds_f2d* rcvr, fint which, Xgl_bounds_f2d* boundsF2d) {
  rcvr[which] = *boundsF2d;
}

void xgl_bounds_f2d_set (Xgl_bounds_f2d* rcvr,
	float xmax,
	float xmin,
	float ymax,
	float ymin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
}


// for Xgl_bounds_f3d
Xgl_bounds_f3d* xgl_bounds_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_f3d, count);
}
void xgl_bounds_f3d_delete (Xgl_bounds_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_f3d* xgl_bounds_f3d_at (Xgl_bounds_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_f3d_at_put (Xgl_bounds_f3d* rcvr, fint which, Xgl_bounds_f3d* boundsF3d) {
  rcvr[which] = *boundsF3d;
}

void xgl_bounds_f3d_set (Xgl_bounds_f3d* rcvr,
	float xmax,
	float xmin,
	float ymax,
	float ymin,
	float zmax,
	float zmin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
  rcvr->zmax = zmax;
  rcvr->zmin = zmin;
}


// for Xgl_bounds_i2d
Xgl_bounds_i2d* xgl_bounds_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_bounds_i2d, count);
}
void xgl_bounds_i2d_delete (Xgl_bounds_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_bounds_i2d* xgl_bounds_i2d_at (Xgl_bounds_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_bounds_i2d_at_put (Xgl_bounds_i2d* rcvr, fint which, Xgl_bounds_i2d* boundsI2d) {
  rcvr[which] = *boundsI2d;
}

void xgl_bounds_i2d_set (Xgl_bounds_i2d* rcvr,
	long xmax,
	long xmin,
	long ymax,
	long ymin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
}


// for Xgl_circle_ad3d
Xgl_circle_ad3d* xgl_circle_ad3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_ad3d, count);
}
void xgl_circle_ad3d_delete (Xgl_circle_ad3d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_ad3d* xgl_circle_ad3d_at (Xgl_circle_ad3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_ad3d_at_put (Xgl_circle_ad3d* rcvr, fint which, Xgl_circle_ad3d* circleAd3d) {
  rcvr[which] = *circleAd3d;
}

Xgl_pt_flag_d3d* xgl_circle_ad3d_center (Xgl_circle_ad3d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_ad3d_center (Xgl_circle_ad3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}


// for Xgl_circle_af3d
Xgl_circle_af3d* xgl_circle_af3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_af3d, count);
}
void xgl_circle_af3d_delete (Xgl_circle_af3d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_af3d* xgl_circle_af3d_at (Xgl_circle_af3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_af3d_at_put (Xgl_circle_af3d* rcvr, fint which, Xgl_circle_af3d* circleAf3d) {
  rcvr[which] = *circleAf3d;
}

Xgl_pt_flag_f3d* xgl_circle_af3d_center (Xgl_circle_af3d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_af3d_center (Xgl_circle_af3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}


// for Xgl_circle_d2d
Xgl_circle_d2d* xgl_circle_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_d2d, count);
}
void xgl_circle_d2d_delete (Xgl_circle_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_d2d* xgl_circle_d2d_at (Xgl_circle_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_d2d_at_put (Xgl_circle_d2d* rcvr, fint which, Xgl_circle_d2d* circleD2d) {
  rcvr[which] = *circleD2d;
}

Xgl_pt_flag_d2d* xgl_circle_d2d_center (Xgl_circle_d2d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_d2d_center (Xgl_circle_d2d* rcvr, Xgl_pt_flag_d2d* center) {
   rcvr->center = *center;
}


// for Xgl_circle_d3d
Xgl_circle_d3d* xgl_circle_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_d3d, count);
}
void xgl_circle_d3d_delete (Xgl_circle_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_d3d* xgl_circle_d3d_at (Xgl_circle_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_d3d_at_put (Xgl_circle_d3d* rcvr, fint which, Xgl_circle_d3d* circleD3d) {
  rcvr[which] = *circleD3d;
}

Xgl_pt_flag_d3d* xgl_circle_d3d_center (Xgl_circle_d3d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_d3d_center (Xgl_circle_d3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}

Xgl_pt_d3d* xgl_circle_d3d_dir (Xgl_circle_d3d* rcvr) {
   return rcvr->dir;
}
void xgl_circle_d3d_set (Xgl_circle_d3d* rcvr,
	Xgl_pt_flag_d3d* center,
	bool dirNormal,
	bool dirNormalized,
	double radius) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->radius = radius;
}


// for Xgl_circle_f2d
Xgl_circle_f2d* xgl_circle_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_f2d, count);
}
void xgl_circle_f2d_delete (Xgl_circle_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_f2d* xgl_circle_f2d_at (Xgl_circle_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_f2d_at_put (Xgl_circle_f2d* rcvr, fint which, Xgl_circle_f2d* circleF2d) {
  rcvr[which] = *circleF2d;
}

Xgl_pt_flag_f2d* xgl_circle_f2d_center (Xgl_circle_f2d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_f2d_center (Xgl_circle_f2d* rcvr, Xgl_pt_flag_f2d* center) {
   rcvr->center = *center;
}


// for Xgl_circle_f3d
Xgl_circle_f3d* xgl_circle_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_f3d, count);
}
void xgl_circle_f3d_delete (Xgl_circle_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_f3d* xgl_circle_f3d_at (Xgl_circle_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_f3d_at_put (Xgl_circle_f3d* rcvr, fint which, Xgl_circle_f3d* circleF3d) {
  rcvr[which] = *circleF3d;
}

Xgl_pt_flag_f3d* xgl_circle_f3d_center (Xgl_circle_f3d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_f3d_center (Xgl_circle_f3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}

Xgl_pt_f3d* xgl_circle_f3d_dir (Xgl_circle_f3d* rcvr) {
   return rcvr->dir;
}
void xgl_circle_f3d_set (Xgl_circle_f3d* rcvr,
	Xgl_pt_flag_f3d* center,
	bool dirNormal,
	bool dirNormalized,
	float radius) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->radius = radius;
}


// for Xgl_circle_i2d
Xgl_circle_i2d* xgl_circle_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_i2d, count);
}
void xgl_circle_i2d_delete (Xgl_circle_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_circle_i2d* xgl_circle_i2d_at (Xgl_circle_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_i2d_at_put (Xgl_circle_i2d* rcvr, fint which, Xgl_circle_i2d* circleI2d) {
  rcvr[which] = *circleI2d;
}

Xgl_pt_flag_i2d* xgl_circle_i2d_center (Xgl_circle_i2d* rcvr) {
   return &(rcvr->center);
}
void xgl_circle_i2d_center (Xgl_circle_i2d* rcvr, Xgl_pt_flag_i2d* center) {
   rcvr->center = *center;
}


// for Xgl_circle_list
Xgl_circle_list* xgl_circle_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_circle_list, count);
}
void xgl_circle_list_delete (Xgl_circle_list* rcvr) {
  delete [] rcvr;
}

Xgl_circle_list* xgl_circle_list_at (Xgl_circle_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_circle_list_at_put (Xgl_circle_list* rcvr, fint which, Xgl_circle_list* circleList) {
  rcvr[which] = *circleList;
}

Xgl_circle_ad3d* xgl_circle_list_circles_ad3d (Xgl_circle_list* rcvr) {
   return rcvr->circles.ad3d;
}
void xgl_circle_list_circles_ad3d (Xgl_circle_list* rcvr, Xgl_circle_ad3d* ad3d) {
   rcvr->circles.ad3d = ad3d;
}

Xgl_circle_af3d* xgl_circle_list_circles_af3d (Xgl_circle_list* rcvr) {
   return rcvr->circles.af3d;
}
void xgl_circle_list_circles_af3d (Xgl_circle_list* rcvr, Xgl_circle_af3d* af3d) {
   rcvr->circles.af3d = af3d;
}

Xgl_circle_d2d* xgl_circle_list_circles_d2d (Xgl_circle_list* rcvr) {
   return rcvr->circles.d2d;
}
void xgl_circle_list_circles_d2d (Xgl_circle_list* rcvr, Xgl_circle_d2d* d2d) {
   rcvr->circles.d2d = d2d;
}

Xgl_circle_d3d* xgl_circle_list_circles_d3d (Xgl_circle_list* rcvr) {
   return rcvr->circles.d3d;
}
void xgl_circle_list_circles_d3d (Xgl_circle_list* rcvr, Xgl_circle_d3d* d3d) {
   rcvr->circles.d3d = d3d;
}

Xgl_circle_f2d* xgl_circle_list_circles_f2d (Xgl_circle_list* rcvr) {
   return rcvr->circles.f2d;
}
void xgl_circle_list_circles_f2d (Xgl_circle_list* rcvr, Xgl_circle_f2d* f2d) {
   rcvr->circles.f2d = f2d;
}

Xgl_circle_f3d* xgl_circle_list_circles_f3d (Xgl_circle_list* rcvr) {
   return rcvr->circles.f3d;
}
void xgl_circle_list_circles_f3d (Xgl_circle_list* rcvr, Xgl_circle_f3d* f3d) {
   rcvr->circles.f3d = f3d;
}

Xgl_circle_i2d* xgl_circle_list_circles_i2d (Xgl_circle_list* rcvr) {
   return rcvr->circles.i2d;
}
void xgl_circle_list_circles_i2d (Xgl_circle_list* rcvr, Xgl_circle_i2d* i2d) {
   rcvr->circles.i2d = i2d;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_ad3d* ad3d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.ad3d = ad3d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_af3d* af3d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.af3d = af3d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_d2d* d2d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.d2d = d2d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_d3d* d3d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.d3d = d3d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_f2d* f2d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.f2d = f2d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_f3d* f3d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.f3d = f3d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}

void xgl_circle_list_set (Xgl_circle_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_circle_i2d* i2d,
	unsigned long numCircles,
	Xgl_multicircle_type type) {
  rcvr->bbox = bbox;
  rcvr->circles.i2d = i2d;
  rcvr->num_circles = numCircles;
  rcvr->type = type;
}


// for Xgl_color
Xgl_color* xgl_color_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color, count);
}
void xgl_color_delete (Xgl_color* rcvr) {
  delete [] rcvr;
}

Xgl_color* xgl_color_at (Xgl_color* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_at_put (Xgl_color* rcvr, fint which, Xgl_color* color) {
  rcvr[which] = *color;
}

Xgl_color_rgb* xgl_color_rgb (Xgl_color* rcvr) {
   return &(rcvr->rgb);
}
void xgl_color_rgb (Xgl_color* rcvr, Xgl_color_rgb* rgb) {
   rcvr->rgb = *rgb;
}


// for Xgl_color_facet
Xgl_color_facet* xgl_color_facet_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_facet, count);
}
void xgl_color_facet_delete (Xgl_color_facet* rcvr) {
  delete [] rcvr;
}

Xgl_color_facet* xgl_color_facet_at (Xgl_color_facet* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_facet_at_put (Xgl_color_facet* rcvr, fint which, Xgl_color_facet* colorFacet) {
  rcvr[which] = *colorFacet;
}

float xgl_color_facet_color_gray (Xgl_color_facet* rcvr) {
   return rcvr->color.gray;
}
void xgl_color_facet_color_gray (Xgl_color_facet* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_color_facet_color_index (Xgl_color_facet* rcvr) {
   return rcvr->color.index;
}
void xgl_color_facet_color_index (Xgl_color_facet* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_color_facet_color_rgb (Xgl_color_facet* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_color_facet_color_rgb (Xgl_color_facet* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_color_facet_color_z (Xgl_color_facet* rcvr) {
   return rcvr->color.z;
}
void xgl_color_facet_color_z (Xgl_color_facet* rcvr, unsigned long z) {
   rcvr->color.z = z;
}


// for Xgl_color_homogeneous
Xgl_color_homogeneous* xgl_color_homogeneous_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_homogeneous, count);
}
void xgl_color_homogeneous_delete (Xgl_color_homogeneous* rcvr) {
  delete [] rcvr;
}

Xgl_color_homogeneous* xgl_color_homogeneous_at (Xgl_color_homogeneous* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_homogeneous_at_put (Xgl_color_homogeneous* rcvr, fint which, Xgl_color_homogeneous* colorHomogeneous) {
  rcvr[which] = *colorHomogeneous;
}

Xgl_color_rgbw* xgl_color_homogeneous_rgbw (Xgl_color_homogeneous* rcvr) {
   return &(rcvr->rgbw);
}
void xgl_color_homogeneous_rgbw (Xgl_color_homogeneous* rcvr, Xgl_color_rgbw* rgbw) {
   rcvr->rgbw = *rgbw;
}


// for Xgl_color_list
Xgl_color_list* xgl_color_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_list, count);
}
void xgl_color_list_delete (Xgl_color_list* rcvr) {
  delete [] rcvr;
}

Xgl_color_list* xgl_color_list_at (Xgl_color_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_list_at_put (Xgl_color_list* rcvr, fint which, Xgl_color_list* colorList) {
  rcvr[which] = *colorList;
}

void xgl_color_list_set (Xgl_color_list* rcvr,
	Xgl_color* colors,
	unsigned long length,
	unsigned long startIndex) {
  rcvr->colors = colors;
  rcvr->length = length;
  rcvr->start_index = startIndex;
}


// for Xgl_color_normal_facet
Xgl_color_normal_facet* xgl_color_normal_facet_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_normal_facet, count);
}
void xgl_color_normal_facet_delete (Xgl_color_normal_facet* rcvr) {
  delete [] rcvr;
}

Xgl_color_normal_facet* xgl_color_normal_facet_at (Xgl_color_normal_facet* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_normal_facet_at_put (Xgl_color_normal_facet* rcvr, fint which, Xgl_color_normal_facet* colorNormalFacet) {
  rcvr[which] = *colorNormalFacet;
}

float xgl_color_normal_facet_color_gray (Xgl_color_normal_facet* rcvr) {
   return rcvr->color.gray;
}
void xgl_color_normal_facet_color_gray (Xgl_color_normal_facet* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_color_normal_facet_color_index (Xgl_color_normal_facet* rcvr) {
   return rcvr->color.index;
}
void xgl_color_normal_facet_color_index (Xgl_color_normal_facet* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_color_normal_facet_color_rgb (Xgl_color_normal_facet* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_color_normal_facet_color_rgb (Xgl_color_normal_facet* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_color_normal_facet_color_z (Xgl_color_normal_facet* rcvr) {
   return rcvr->color.z;
}
void xgl_color_normal_facet_color_z (Xgl_color_normal_facet* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

Xgl_pt_f3d* xgl_color_normal_facet_normal (Xgl_color_normal_facet* rcvr) {
   return &(rcvr->normal);
}
void xgl_color_normal_facet_normal (Xgl_color_normal_facet* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}


// for Xgl_color_rgb
Xgl_color_rgb* xgl_color_rgb_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_rgb, count);
}
void xgl_color_rgb_delete (Xgl_color_rgb* rcvr) {
  delete [] rcvr;
}

Xgl_color_rgb* xgl_color_rgb_at (Xgl_color_rgb* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_rgb_at_put (Xgl_color_rgb* rcvr, fint which, Xgl_color_rgb* colorRgb) {
  rcvr[which] = *colorRgb;
}

void xgl_color_rgb_set (Xgl_color_rgb* rcvr,
	float b,
	float g,
	float r) {
  rcvr->b = b;
  rcvr->g = g;
  rcvr->r = r;
}


// for Xgl_color_rgbw
Xgl_color_rgbw* xgl_color_rgbw_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_rgbw, count);
}
void xgl_color_rgbw_delete (Xgl_color_rgbw* rcvr) {
  delete [] rcvr;
}

Xgl_color_rgbw* xgl_color_rgbw_at (Xgl_color_rgbw* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_rgbw_at_put (Xgl_color_rgbw* rcvr, fint which, Xgl_color_rgbw* colorRgbw) {
  rcvr[which] = *colorRgbw;
}

void xgl_color_rgbw_set (Xgl_color_rgbw* rcvr,
	float b,
	float g,
	float r,
	float w) {
  rcvr->b = b;
  rcvr->g = g;
  rcvr->r = r;
  rcvr->w = w;
}


// for Xgl_color_type_supported
Xgl_color_type_supported* xgl_color_type_supported_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_color_type_supported, count);
}
void xgl_color_type_supported_delete (Xgl_color_type_supported* rcvr) {
  delete [] rcvr;
}

Xgl_color_type_supported* xgl_color_type_supported_at (Xgl_color_type_supported* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_color_type_supported_at_put (Xgl_color_type_supported* rcvr, fint which, Xgl_color_type_supported* colorTypeSupported) {
  rcvr[which] = *colorTypeSupported;
}


// for Xgl_curve_color_spline
Xgl_curve_color_spline* xgl_curve_color_spline_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_curve_color_spline, count);
}
void xgl_curve_color_spline_delete (Xgl_curve_color_spline* rcvr) {
  delete [] rcvr;
}

Xgl_curve_color_spline* xgl_curve_color_spline_at (Xgl_curve_color_spline* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_curve_color_spline_at_put (Xgl_curve_color_spline* rcvr, fint which, Xgl_curve_color_spline* curveColorSpline) {
  rcvr[which] = *curveColorSpline;
}

void xgl_curve_color_spline_set (Xgl_curve_color_spline* rcvr,
	Xgl_color_homogeneous* colors,
	float* knotVector,
	unsigned long numKnots,
	unsigned long order) {
  rcvr->colors = colors;
  rcvr->knot_vector = knotVector;
  rcvr->num_knots = numKnots;
  rcvr->order = order;
}


// for Xgl_ell_ad3d
Xgl_ell_ad3d* xgl_ell_ad3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_ell_ad3d, count);
}
void xgl_ell_ad3d_delete (Xgl_ell_ad3d* rcvr) {
  delete [] rcvr;
}

Xgl_ell_ad3d* xgl_ell_ad3d_at (Xgl_ell_ad3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_ell_ad3d_at_put (Xgl_ell_ad3d* rcvr, fint which, Xgl_ell_ad3d* ellAd3d) {
  rcvr[which] = *ellAd3d;
}

Xgl_pt_flag_d3d* xgl_ell_ad3d_center (Xgl_ell_ad3d* rcvr) {
   return &(rcvr->center);
}
void xgl_ell_ad3d_center (Xgl_ell_ad3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}

void xgl_ell_ad3d_set (Xgl_ell_ad3d* rcvr,
	Xgl_pt_flag_d3d* center,
	double majorAxis,
	double minorAxis,
	double rotAngle,
	double startAngle,
	double stopAngle) {
  rcvr->center = *center;
  rcvr->major_axis = majorAxis;
  rcvr->minor_axis = minorAxis;
  rcvr->rot_angle = rotAngle;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_ell_af3d
Xgl_ell_af3d* xgl_ell_af3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_ell_af3d, count);
}
void xgl_ell_af3d_delete (Xgl_ell_af3d* rcvr) {
  delete [] rcvr;
}

Xgl_ell_af3d* xgl_ell_af3d_at (Xgl_ell_af3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_ell_af3d_at_put (Xgl_ell_af3d* rcvr, fint which, Xgl_ell_af3d* ellAf3d) {
  rcvr[which] = *ellAf3d;
}

Xgl_pt_flag_f3d* xgl_ell_af3d_center (Xgl_ell_af3d* rcvr) {
   return &(rcvr->center);
}
void xgl_ell_af3d_center (Xgl_ell_af3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}

void xgl_ell_af3d_set (Xgl_ell_af3d* rcvr,
	Xgl_pt_flag_f3d* center,
	float majorAxis,
	float minorAxis,
	float rotAngle,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->major_axis = majorAxis;
  rcvr->minor_axis = minorAxis;
  rcvr->rot_angle = rotAngle;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_ell_d3d
Xgl_ell_d3d* xgl_ell_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_ell_d3d, count);
}
void xgl_ell_d3d_delete (Xgl_ell_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_ell_d3d* xgl_ell_d3d_at (Xgl_ell_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_ell_d3d_at_put (Xgl_ell_d3d* rcvr, fint which, Xgl_ell_d3d* ellD3d) {
  rcvr[which] = *ellD3d;
}

Xgl_pt_flag_d3d* xgl_ell_d3d_center (Xgl_ell_d3d* rcvr) {
   return &(rcvr->center);
}
void xgl_ell_d3d_center (Xgl_ell_d3d* rcvr, Xgl_pt_flag_d3d* center) {
   rcvr->center = *center;
}

Xgl_pt_d3d* xgl_ell_d3d_dir (Xgl_ell_d3d* rcvr) {
   return rcvr->dir;
}
void xgl_ell_d3d_set (Xgl_ell_d3d* rcvr,
	Xgl_pt_flag_d3d* center,
	bool dirNormal,
	bool dirNormalized,
	double majorAxis,
	double minorAxis,
	double rotAngle,
	double startAngle,
	double stopAngle) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->major_axis = majorAxis;
  rcvr->minor_axis = minorAxis;
  rcvr->rot_angle = rotAngle;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_ell_f3d
Xgl_ell_f3d* xgl_ell_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_ell_f3d, count);
}
void xgl_ell_f3d_delete (Xgl_ell_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_ell_f3d* xgl_ell_f3d_at (Xgl_ell_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_ell_f3d_at_put (Xgl_ell_f3d* rcvr, fint which, Xgl_ell_f3d* ellF3d) {
  rcvr[which] = *ellF3d;
}

Xgl_pt_flag_f3d* xgl_ell_f3d_center (Xgl_ell_f3d* rcvr) {
   return &(rcvr->center);
}
void xgl_ell_f3d_center (Xgl_ell_f3d* rcvr, Xgl_pt_flag_f3d* center) {
   rcvr->center = *center;
}

Xgl_pt_f3d* xgl_ell_f3d_dir (Xgl_ell_f3d* rcvr) {
   return rcvr->dir;
}
void xgl_ell_f3d_set (Xgl_ell_f3d* rcvr,
	Xgl_pt_flag_f3d* center,
	bool dirNormal,
	bool dirNormalized,
	float majorAxis,
	float minorAxis,
	float rotAngle,
	float startAngle,
	float stopAngle) {
  rcvr->center = *center;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
  rcvr->major_axis = majorAxis;
  rcvr->minor_axis = minorAxis;
  rcvr->rot_angle = rotAngle;
  rcvr->start_angle = startAngle;
  rcvr->stop_angle = stopAngle;
}


// for Xgl_ell_list
Xgl_ell_list* xgl_ell_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_ell_list, count);
}
void xgl_ell_list_delete (Xgl_ell_list* rcvr) {
  delete [] rcvr;
}

Xgl_ell_list* xgl_ell_list_at (Xgl_ell_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_ell_list_at_put (Xgl_ell_list* rcvr, fint which, Xgl_ell_list* ellList) {
  rcvr[which] = *ellList;
}

Xgl_ell_ad3d* xgl_ell_list_ells_ad3d (Xgl_ell_list* rcvr) {
   return rcvr->ells.ad3d;
}
void xgl_ell_list_ells_ad3d (Xgl_ell_list* rcvr, Xgl_ell_ad3d* ad3d) {
   rcvr->ells.ad3d = ad3d;
}

Xgl_ell_af3d* xgl_ell_list_ells_af3d (Xgl_ell_list* rcvr) {
   return rcvr->ells.af3d;
}
void xgl_ell_list_ells_af3d (Xgl_ell_list* rcvr, Xgl_ell_af3d* af3d) {
   rcvr->ells.af3d = af3d;
}

Xgl_ell_d3d* xgl_ell_list_ells_d3d (Xgl_ell_list* rcvr) {
   return rcvr->ells.d3d;
}
void xgl_ell_list_ells_d3d (Xgl_ell_list* rcvr, Xgl_ell_d3d* d3d) {
   rcvr->ells.d3d = d3d;
}

Xgl_ell_f3d* xgl_ell_list_ells_f3d (Xgl_ell_list* rcvr) {
   return rcvr->ells.f3d;
}
void xgl_ell_list_ells_f3d (Xgl_ell_list* rcvr, Xgl_ell_f3d* f3d) {
   rcvr->ells.f3d = f3d;
}

void xgl_ell_list_set (Xgl_ell_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_ell_ad3d* ad3d,
	unsigned long numElls,
	Xgl_multiell_type type) {
  rcvr->bbox = bbox;
  rcvr->ells.ad3d = ad3d;
  rcvr->num_ells = numElls;
  rcvr->type = type;
}

void xgl_ell_list_set (Xgl_ell_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_ell_af3d* af3d,
	unsigned long numElls,
	Xgl_multiell_type type) {
  rcvr->bbox = bbox;
  rcvr->ells.af3d = af3d;
  rcvr->num_ells = numElls;
  rcvr->type = type;
}

void xgl_ell_list_set (Xgl_ell_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_ell_d3d* d3d,
	unsigned long numElls,
	Xgl_multiell_type type) {
  rcvr->bbox = bbox;
  rcvr->ells.d3d = d3d;
  rcvr->num_ells = numElls;
  rcvr->type = type;
}

void xgl_ell_list_set (Xgl_ell_list* rcvr,
	Xgl_bbox* bbox,
	Xgl_ell_f3d* f3d,
	unsigned long numElls,
	Xgl_multiell_type type) {
  rcvr->bbox = bbox;
  rcvr->ells.f3d = f3d;
  rcvr->num_ells = numElls;
  rcvr->type = type;
}


// for Xgl_error_info
Xgl_error_info* xgl_error_info_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_error_info, count);
}
void xgl_error_info_delete (Xgl_error_info* rcvr) {
  delete [] rcvr;
}

Xgl_error_info* xgl_error_info_at (Xgl_error_info* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_error_info_at_put (Xgl_error_info* rcvr, fint which, Xgl_error_info* errorInfo) {
  rcvr[which] = *errorInfo;
}


// for Xgl_facet
Xgl_facet* xgl_facet_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_facet, count);
}
void xgl_facet_delete (Xgl_facet* rcvr) {
  delete [] rcvr;
}

Xgl_facet* xgl_facet_at (Xgl_facet* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_facet_at_put (Xgl_facet* rcvr, fint which, Xgl_facet* facet) {
  rcvr[which] = *facet;
}

Xgl_color_facet* xgl_facet_color_facet (Xgl_facet* rcvr) {
   return &(rcvr->color_facet);
}
void xgl_facet_color_facet (Xgl_facet* rcvr, Xgl_color_facet* colorFacet) {
   rcvr->color_facet = *colorFacet;
}

Xgl_color_normal_facet* xgl_facet_color_normal_facet (Xgl_facet* rcvr) {
   return &(rcvr->color_normal_facet);
}
void xgl_facet_color_normal_facet (Xgl_facet* rcvr, Xgl_color_normal_facet* colorNormalFacet) {
   rcvr->color_normal_facet = *colorNormalFacet;
}

Xgl_normal_facet* xgl_facet_normal_facet (Xgl_facet* rcvr) {
   return &(rcvr->normal_facet);
}
void xgl_facet_normal_facet (Xgl_facet* rcvr, Xgl_normal_facet* normalFacet) {
   rcvr->normal_facet = *normalFacet;
}


// for Xgl_facet_list
Xgl_facet_list* xgl_facet_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_facet_list, count);
}
void xgl_facet_list_delete (Xgl_facet_list* rcvr) {
  delete [] rcvr;
}

Xgl_facet_list* xgl_facet_list_at (Xgl_facet_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_facet_list_at_put (Xgl_facet_list* rcvr, fint which, Xgl_facet_list* facetList) {
  rcvr[which] = *facetList;
}

Xgl_color_facet* xgl_facet_list_facets_color_facets (Xgl_facet_list* rcvr) {
   return rcvr->facets.color_facets;
}
void xgl_facet_list_facets_color_facets (Xgl_facet_list* rcvr, Xgl_color_facet* colorFacets) {
   rcvr->facets.color_facets = colorFacets;
}

Xgl_color_normal_facet* xgl_facet_list_facets_color_normal_facets (Xgl_facet_list* rcvr) {
   return rcvr->facets.color_normal_facets;
}
void xgl_facet_list_facets_color_normal_facets (Xgl_facet_list* rcvr, Xgl_color_normal_facet* colorNormalFacets) {
   rcvr->facets.color_normal_facets = colorNormalFacets;
}

Xgl_normal_facet* xgl_facet_list_facets_normal_facets (Xgl_facet_list* rcvr) {
   return rcvr->facets.normal_facets;
}
void xgl_facet_list_facets_normal_facets (Xgl_facet_list* rcvr, Xgl_normal_facet* normalFacets) {
   rcvr->facets.normal_facets = normalFacets;
}

void xgl_facet_list_set (Xgl_facet_list* rcvr,
	Xgl_facet_type facetType,
	Xgl_color_facet* colorFacets,
	unsigned long numFacets) {
  rcvr->facet_type = facetType;
  rcvr->facets.color_facets = colorFacets;
  rcvr->num_facets = numFacets;
}

void xgl_facet_list_set (Xgl_facet_list* rcvr,
	Xgl_facet_type facetType,
	Xgl_color_normal_facet* colorNormalFacets,
	unsigned long numFacets) {
  rcvr->facet_type = facetType;
  rcvr->facets.color_normal_facets = colorNormalFacets;
  rcvr->num_facets = numFacets;
}

void xgl_facet_list_set (Xgl_facet_list* rcvr,
	Xgl_facet_type facetType,
	Xgl_normal_facet* normalFacets,
	unsigned long numFacets) {
  rcvr->facet_type = facetType;
  rcvr->facets.normal_facets = normalFacets;
  rcvr->num_facets = numFacets;
}


// for Xgl_facet_list_list
Xgl_facet_list_list* xgl_facet_list_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_facet_list_list, count);
}
void xgl_facet_list_list_delete (Xgl_facet_list_list* rcvr) {
  delete [] rcvr;
}

Xgl_facet_list_list* xgl_facet_list_list_at (Xgl_facet_list_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_facet_list_list_at_put (Xgl_facet_list_list* rcvr, fint which, Xgl_facet_list_list* facetListList) {
  rcvr[which] = *facetListList;
}


// for Xgl_inquire
Xgl_inquire* xgl_inquire_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_inquire, count);
}
void xgl_inquire_delete (Xgl_inquire* rcvr) {
  delete [] rcvr;
}

Xgl_inquire* xgl_inquire_at (Xgl_inquire* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_inquire_at_put (Xgl_inquire* rcvr, fint which, Xgl_inquire* inquire) {
  rcvr[which] = *inquire;
}

Xgl_color_type_supported* xgl_inquire_color_type (Xgl_inquire* rcvr) {
   return &(rcvr->color_type);
}
void xgl_inquire_color_type (Xgl_inquire* rcvr, Xgl_color_type_supported* colorType) {
   rcvr->color_type = *colorType;
}

Xgl_pt_type_supported* xgl_inquire_pt_type (Xgl_inquire* rcvr) {
   return &(rcvr->pt_type);
}
void xgl_inquire_pt_type (Xgl_inquire* rcvr, Xgl_pt_type_supported* ptType) {
   rcvr->pt_type = *ptType;
}


// for Xgl_irect
Xgl_irect* xgl_irect_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_irect, count);
}
void xgl_irect_delete (Xgl_irect* rcvr) {
  delete [] rcvr;
}

Xgl_irect* xgl_irect_at (Xgl_irect* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_irect_at_put (Xgl_irect* rcvr, fint which, Xgl_irect* irect) {
  rcvr[which] = *irect;
}

void xgl_irect_set (Xgl_irect* rcvr,
	long xmax,
	long xmin,
	long ymax,
	long ymin) {
  rcvr->xmax = xmax;
  rcvr->xmin = xmin;
  rcvr->ymax = ymax;
  rcvr->ymin = ymin;
}


// for Xgl_irect_list
Xgl_irect_list* xgl_irect_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_irect_list, count);
}
void xgl_irect_list_delete (Xgl_irect_list* rcvr) {
  delete [] rcvr;
}

Xgl_irect_list* xgl_irect_list_at (Xgl_irect_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_irect_list_at_put (Xgl_irect_list* rcvr, fint which, Xgl_irect_list* irectList) {
  rcvr[which] = *irectList;
}


// for Xgl_matrix_d2d
Xgl_matrix_d2d* xgl_matrix_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_matrix_d2d, count);
}
void xgl_matrix_d2d_delete (Xgl_matrix_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_matrix_d2d* xgl_matrix_d2d_at (Xgl_matrix_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_matrix_d2d_at_put (Xgl_matrix_d2d* rcvr, fint which, Xgl_matrix_d2d* matrixD2d) {
  rcvr[which] = *matrixD2d;
}

double xgl_matrix_d2d_row_column (Xgl_matrix_d2d* matrixD2d, fint row, fint column) {
  return (*matrixD2d)[row][column];
}
void xgl_matrix_d2d_row_column_put (Xgl_matrix_d2d* matrixD2d, fint row, fint column, double data) {
  (*matrixD2d)[row][column] = data;
}


// for Xgl_matrix_d3d
Xgl_matrix_d3d* xgl_matrix_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_matrix_d3d, count);
}
void xgl_matrix_d3d_delete (Xgl_matrix_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_matrix_d3d* xgl_matrix_d3d_at (Xgl_matrix_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_matrix_d3d_at_put (Xgl_matrix_d3d* rcvr, fint which, Xgl_matrix_d3d* matrixD3d) {
  rcvr[which] = *matrixD3d;
}

double xgl_matrix_d3d_row_column (Xgl_matrix_d3d* matrixD3d, fint row, fint column) {
  return (*matrixD3d)[row][column];
}
void xgl_matrix_d3d_row_column_put (Xgl_matrix_d3d* matrixD3d, fint row, fint column, double data) {
  (*matrixD3d)[row][column] = data;
}


// for Xgl_matrix_f2d
Xgl_matrix_f2d* xgl_matrix_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_matrix_f2d, count);
}
void xgl_matrix_f2d_delete (Xgl_matrix_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_matrix_f2d* xgl_matrix_f2d_at (Xgl_matrix_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_matrix_f2d_at_put (Xgl_matrix_f2d* rcvr, fint which, Xgl_matrix_f2d* matrixF2d) {
  rcvr[which] = *matrixF2d;
}

float xgl_matrix_f2d_row_column (Xgl_matrix_f2d* matrixF2d, fint row, fint column) {
  return (*matrixF2d)[row][column];
}
void xgl_matrix_f2d_row_column_put (Xgl_matrix_f2d* matrixF2d, fint row, fint column, float data) {
  (*matrixF2d)[row][column] = data;
}


// for Xgl_matrix_f3d
Xgl_matrix_f3d* xgl_matrix_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_matrix_f3d, count);
}
void xgl_matrix_f3d_delete (Xgl_matrix_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_matrix_f3d* xgl_matrix_f3d_at (Xgl_matrix_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_matrix_f3d_at_put (Xgl_matrix_f3d* rcvr, fint which, Xgl_matrix_f3d* matrixF3d) {
  rcvr[which] = *matrixF3d;
}

float xgl_matrix_f3d_row_column (Xgl_matrix_f3d* matrixF3d, fint row, fint column) {
  return (*matrixF3d)[row][column];
}
void xgl_matrix_f3d_row_column_put (Xgl_matrix_f3d* matrixF3d, fint row, fint column, float data) {
  (*matrixF3d)[row][column] = data;
}


// for Xgl_matrix_i2d
Xgl_matrix_i2d* xgl_matrix_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_matrix_i2d, count);
}
void xgl_matrix_i2d_delete (Xgl_matrix_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_matrix_i2d* xgl_matrix_i2d_at (Xgl_matrix_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_matrix_i2d_at_put (Xgl_matrix_i2d* rcvr, fint which, Xgl_matrix_i2d* matrixI2d) {
  rcvr[which] = *matrixI2d;
}

long xgl_matrix_i2d_row_column (Xgl_matrix_i2d* matrixI2d, fint row, fint column) {
  return (*matrixI2d)[row][column];
}
void xgl_matrix_i2d_row_column_put (Xgl_matrix_i2d* matrixI2d, fint row, fint column, long data) {
  (*matrixI2d)[row][column] = data;
}


// for Xgl_mono_text
Xgl_mono_text* xgl_mono_text_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_mono_text, count);
}
void xgl_mono_text_delete (Xgl_mono_text* rcvr) {
  delete [] rcvr;
}

Xgl_mono_text* xgl_mono_text_at (Xgl_mono_text* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_mono_text_at_put (Xgl_mono_text* rcvr, fint which, Xgl_mono_text* monoText) {
  rcvr[which] = *monoText;
}


// for Xgl_mono_text_list
Xgl_mono_text_list* xgl_mono_text_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_mono_text_list, count);
}
void xgl_mono_text_list_delete (Xgl_mono_text_list* rcvr) {
  delete [] rcvr;
}

Xgl_mono_text_list* xgl_mono_text_list_at (Xgl_mono_text_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_mono_text_list_at_put (Xgl_mono_text_list* rcvr, fint which, Xgl_mono_text_list* monoTextList) {
  rcvr[which] = *monoTextList;
}


// for Xgl_normal_facet
Xgl_normal_facet* xgl_normal_facet_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_normal_facet, count);
}
void xgl_normal_facet_delete (Xgl_normal_facet* rcvr) {
  delete [] rcvr;
}

Xgl_normal_facet* xgl_normal_facet_at (Xgl_normal_facet* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_normal_facet_at_put (Xgl_normal_facet* rcvr, fint which, Xgl_normal_facet* normalFacet) {
  rcvr[which] = *normalFacet;
}

Xgl_pt_f3d* xgl_normal_facet_normal (Xgl_normal_facet* rcvr) {
   return &(rcvr->normal);
}
void xgl_normal_facet_normal (Xgl_normal_facet* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}


// for Xgl_nu_bspline_curve
Xgl_nu_bspline_curve* xgl_nu_bspline_curve_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_nu_bspline_curve, count);
}
void xgl_nu_bspline_curve_delete (Xgl_nu_bspline_curve* rcvr) {
  delete [] rcvr;
}

Xgl_nu_bspline_curve* xgl_nu_bspline_curve_at (Xgl_nu_bspline_curve* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_nu_bspline_curve_at_put (Xgl_nu_bspline_curve* rcvr, fint which, Xgl_nu_bspline_curve* nuBsplineCurve) {
  rcvr[which] = *nuBsplineCurve;
}

Xgl_pt_list* xgl_nu_bspline_curve_ctrl_pts (Xgl_nu_bspline_curve* rcvr) {
   return &(rcvr->ctrl_pts);
}
void xgl_nu_bspline_curve_ctrl_pts (Xgl_nu_bspline_curve* rcvr, Xgl_pt_list* ctrlPts) {
   rcvr->ctrl_pts = *ctrlPts;
}

Xgl_bounds_f1d* xgl_nu_bspline_curve_range (Xgl_nu_bspline_curve* rcvr) {
   return &(rcvr->range);
}
void xgl_nu_bspline_curve_range (Xgl_nu_bspline_curve* rcvr, Xgl_bounds_f1d* range) {
   rcvr->range = *range;
}

void xgl_nu_bspline_curve_set (Xgl_nu_bspline_curve* rcvr,
	Xgl_pt_list* ctrlPts,
	float* knotVector,
	unsigned long numKnots,
	unsigned long order,
	Xgl_bounds_f1d* range,
	bool trimCurveVis) {
  rcvr->ctrl_pts = *ctrlPts;
  rcvr->knot_vector = knotVector;
  rcvr->num_knots = numKnots;
  rcvr->order = order;
  rcvr->range = *range;
  rcvr->trim_curve_vis = trimCurveVis;
}


// for Xgl_nurbs_curve
Xgl_nurbs_curve* xgl_nurbs_curve_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_nurbs_curve, count);
}
void xgl_nurbs_curve_delete (Xgl_nurbs_curve* rcvr) {
  delete [] rcvr;
}

Xgl_nurbs_curve* xgl_nurbs_curve_at (Xgl_nurbs_curve* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_nurbs_curve_at_put (Xgl_nurbs_curve* rcvr, fint which, Xgl_nurbs_curve* nurbsCurve) {
  rcvr[which] = *nurbsCurve;
}

Xgl_pt_list* xgl_nurbs_curve_ctrl_pts (Xgl_nurbs_curve* rcvr) {
   return &(rcvr->ctrl_pts);
}
void xgl_nurbs_curve_ctrl_pts (Xgl_nurbs_curve* rcvr, Xgl_pt_list* ctrlPts) {
   rcvr->ctrl_pts = *ctrlPts;
}

void xgl_nurbs_curve_set (Xgl_nurbs_curve* rcvr,
	Xgl_pt_list* ctrlPts,
	float* knotVector,
	unsigned long numKnots,
	unsigned long order) {
  rcvr->ctrl_pts = *ctrlPts;
  rcvr->knot_vector = knotVector;
  rcvr->num_knots = numKnots;
  rcvr->order = order;
}


// for Xgl_nurbs_surf
Xgl_nurbs_surf* xgl_nurbs_surf_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_nurbs_surf, count);
}
void xgl_nurbs_surf_delete (Xgl_nurbs_surf* rcvr) {
  delete [] rcvr;
}

Xgl_nurbs_surf* xgl_nurbs_surf_at (Xgl_nurbs_surf* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_nurbs_surf_at_put (Xgl_nurbs_surf* rcvr, fint which, Xgl_nurbs_surf* nurbsSurf) {
  rcvr[which] = *nurbsSurf;
}

Xgl_pt_list* xgl_nurbs_surf_ctrl_pts (Xgl_nurbs_surf* rcvr) {
   return &(rcvr->ctrl_pts);
}
void xgl_nurbs_surf_ctrl_pts (Xgl_nurbs_surf* rcvr, Xgl_pt_list* ctrlPts) {
   rcvr->ctrl_pts = *ctrlPts;
}

void xgl_nurbs_surf_set (Xgl_nurbs_surf* rcvr,
	Xgl_pt_list* ctrlPts,
	float* knotVectorU,
	float* knotVectorV,
	unsigned long numKnotsU,
	unsigned long numKnotsV,
	unsigned long orderU,
	unsigned long orderV) {
  rcvr->ctrl_pts = *ctrlPts;
  rcvr->knot_vector_u = knotVectorU;
  rcvr->knot_vector_v = knotVectorV;
  rcvr->num_knots_u = numKnotsU;
  rcvr->num_knots_v = numKnotsV;
  rcvr->order_u = orderU;
  rcvr->order_v = orderV;
}


// for Xgl_nurbs_surf_simple_geom
Xgl_nurbs_surf_simple_geom* xgl_nurbs_surf_simple_geom_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_nurbs_surf_simple_geom, count);
}
void xgl_nurbs_surf_simple_geom_delete (Xgl_nurbs_surf_simple_geom* rcvr) {
  delete [] rcvr;
}

Xgl_nurbs_surf_simple_geom* xgl_nurbs_surf_simple_geom_at (Xgl_nurbs_surf_simple_geom* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_nurbs_surf_simple_geom_at_put (Xgl_nurbs_surf_simple_geom* rcvr, fint which, Xgl_nurbs_surf_simple_geom* nurbsSurfSimpleGeom) {
  rcvr[which] = *nurbsSurfSimpleGeom;
}

Xgl_pt_f3d* xgl_nurbs_surf_simple_geom_geom_desc_conical_apex (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.conical.apex);
}
void xgl_nurbs_surf_simple_geom_geom_desc_conical_apex (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_pt_f3d* apex) {
   rcvr->geom_desc.conical.apex = *apex;
}

Xgl_pt_f3d* xgl_nurbs_surf_simple_geom_geom_desc_conical_axis_dir (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.conical.axis_dir);
}
void xgl_nurbs_surf_simple_geom_geom_desc_conical_axis_dir (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_pt_f3d* axisDir) {
   rcvr->geom_desc.conical.axis_dir = *axisDir;
}

float xgl_nurbs_surf_simple_geom_geom_desc_conical_cone_angle (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.conical.cone_angle;
}
void xgl_nurbs_surf_simple_geom_geom_desc_conical_cone_angle (Xgl_nurbs_surf_simple_geom* rcvr, float coneAngle) {
   rcvr->geom_desc.conical.cone_angle = coneAngle;
}

bool xgl_nurbs_surf_simple_geom_geom_desc_conical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.conical.norm_flag;
}
void xgl_nurbs_surf_simple_geom_geom_desc_conical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr, bool normFlag) {
   rcvr->geom_desc.conical.norm_flag = normFlag;
}

Xgl_pt_f3d* xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_axial_pt (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.cylindrical.axial_pt);
}
void xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_axial_pt (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_pt_f3d* axialPt) {
   rcvr->geom_desc.cylindrical.axial_pt = *axialPt;
}

Xgl_pt_f3d* xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_axis_dir (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.cylindrical.axis_dir);
}
void xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_axis_dir (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_pt_f3d* axisDir) {
   rcvr->geom_desc.cylindrical.axis_dir = *axisDir;
}

bool xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.cylindrical.norm_flag;
}
void xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr, bool normFlag) {
   rcvr->geom_desc.cylindrical.norm_flag = normFlag;
}

float xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_radius (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.cylindrical.radius;
}
void xgl_nurbs_surf_simple_geom_geom_desc_cylindrical_radius (Xgl_nurbs_surf_simple_geom* rcvr, float radius) {
   rcvr->geom_desc.cylindrical.radius = radius;
}

Xgl_plane* xgl_nurbs_surf_simple_geom_geom_desc_planar (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.planar);
}
void xgl_nurbs_surf_simple_geom_geom_desc_planar (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_plane* planar) {
   rcvr->geom_desc.planar = *planar;
}

Xgl_pt_f3d* xgl_nurbs_surf_simple_geom_geom_desc_spherical_center (Xgl_nurbs_surf_simple_geom* rcvr) {
   return &(rcvr->geom_desc.spherical.center);
}
void xgl_nurbs_surf_simple_geom_geom_desc_spherical_center (Xgl_nurbs_surf_simple_geom* rcvr, Xgl_pt_f3d* center) {
   rcvr->geom_desc.spherical.center = *center;
}

bool xgl_nurbs_surf_simple_geom_geom_desc_spherical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.spherical.norm_flag;
}
void xgl_nurbs_surf_simple_geom_geom_desc_spherical_norm_flag (Xgl_nurbs_surf_simple_geom* rcvr, bool normFlag) {
   rcvr->geom_desc.spherical.norm_flag = normFlag;
}

float xgl_nurbs_surf_simple_geom_geom_desc_spherical_radius (Xgl_nurbs_surf_simple_geom* rcvr) {
   return rcvr->geom_desc.spherical.radius;
}
void xgl_nurbs_surf_simple_geom_geom_desc_spherical_radius (Xgl_nurbs_surf_simple_geom* rcvr, float radius) {
   rcvr->geom_desc.spherical.radius = radius;
}

void xgl_nurbs_surf_simple_geom_set (Xgl_nurbs_surf_simple_geom* rcvr,
	Xgl_pt_f3d* conical_apex,
	Xgl_pt_f3d* conical_axisDir,
	float conical_coneAngle,
	bool conical_normFlag,
	Xgl_nurbs_surf_type surfType) {
  rcvr->geom_desc.conical.apex = *conical_apex;
  rcvr->geom_desc.conical.axis_dir = *conical_axisDir;
  rcvr->geom_desc.conical.cone_angle = conical_coneAngle;
  rcvr->geom_desc.conical.norm_flag = conical_normFlag;
  rcvr->surf_type = surfType;
}

void xgl_nurbs_surf_simple_geom_set (Xgl_nurbs_surf_simple_geom* rcvr,
	Xgl_pt_f3d* cylindrical_axialPt,
	Xgl_pt_f3d* cylindrical_axisDir,
	bool cylindrical_normFlag,
	float cylindrical_radius,
	Xgl_nurbs_surf_type surfType) {
  rcvr->geom_desc.cylindrical.axial_pt = *cylindrical_axialPt;
  rcvr->geom_desc.cylindrical.axis_dir = *cylindrical_axisDir;
  rcvr->geom_desc.cylindrical.norm_flag = cylindrical_normFlag;
  rcvr->geom_desc.cylindrical.radius = cylindrical_radius;
  rcvr->surf_type = surfType;
}

void xgl_nurbs_surf_simple_geom_set (Xgl_nurbs_surf_simple_geom* rcvr,
	Xgl_plane* geom_desc_planar,
	Xgl_nurbs_surf_type surfType) {
  rcvr->geom_desc.planar = *geom_desc_planar;
  rcvr->surf_type = surfType;
}

void xgl_nurbs_surf_simple_geom_set (Xgl_nurbs_surf_simple_geom* rcvr,
	Xgl_pt_f3d* spherical_center,
	bool spherical_normFlag,
	float spherical_radius,
	Xgl_nurbs_surf_type surfType) {
  rcvr->geom_desc.spherical.center = *spherical_center;
  rcvr->geom_desc.spherical.norm_flag = spherical_normFlag;
  rcvr->geom_desc.spherical.radius = spherical_radius;
  rcvr->surf_type = surfType;
}


// for Xgl_obj_desc
Xgl_obj_desc* xgl_obj_desc_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_obj_desc, count);
}
void xgl_obj_desc_delete (Xgl_obj_desc* rcvr) {
  delete [] rcvr;
}

Xgl_obj_desc* xgl_obj_desc_at (Xgl_obj_desc* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_obj_desc_at_put (Xgl_obj_desc* rcvr, fint which, Xgl_obj_desc* objDesc) {
  rcvr[which] = *objDesc;
}

Xgl_object_obj* xgl_obj_desc_accum_buf_raster (Xgl_obj_desc* rcvr) {
   return rcvr->accum_buf.raster;
}
void xgl_obj_desc_accum_buf_raster (Xgl_obj_desc* rcvr, Xgl_object_obj* raster) {
   rcvr->accum_buf.raster = raster;
}

void* xgl_obj_desc_stream_desc (Xgl_obj_desc* rcvr) {
   return rcvr->stream.desc;
}
void xgl_obj_desc_stream_desc (Xgl_obj_desc* rcvr, void* desc) {
   rcvr->stream.desc = desc;
}

char* xgl_obj_desc_stream_name (Xgl_obj_desc* rcvr) {
   return rcvr->stream.name;
}
void xgl_obj_desc_stream_name (Xgl_obj_desc* rcvr, char* name) {
   rcvr->stream.name = name;
}

void* xgl_obj_desc_win_ras_desc (Xgl_obj_desc* rcvr) {
   return rcvr->win_ras.desc;
}
void xgl_obj_desc_win_ras_desc (Xgl_obj_desc* rcvr, void* desc) {
   rcvr->win_ras.desc = desc;
}

Xgl_window_type xgl_obj_desc_win_ras_type (Xgl_obj_desc* rcvr) {
   return rcvr->win_ras.type;
}
void xgl_obj_desc_win_ras_type (Xgl_obj_desc* rcvr, Xgl_window_type type) {
   rcvr->win_ras.type = type;
}


// for Xgl_pick_info
Xgl_pick_info* xgl_pick_info_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pick_info, count);
}
void xgl_pick_info_delete (Xgl_pick_info* rcvr) {
  delete [] rcvr;
}

Xgl_pick_info* xgl_pick_info_at (Xgl_pick_info* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pick_info_at_put (Xgl_pick_info* rcvr, fint which, Xgl_pick_info* pickInfo) {
  rcvr[which] = *pickInfo;
}

void xgl_pick_info_set (Xgl_pick_info* rcvr,
	unsigned long id1,
	unsigned long id2,
	unsigned long vertexFlag) {
  rcvr->id1 = id1;
  rcvr->id2 = id2;
  rcvr->vertex_flag = vertexFlag;
}


// for Xgl_plane
Xgl_plane* xgl_plane_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_plane, count);
}
void xgl_plane_delete (Xgl_plane* rcvr) {
  delete [] rcvr;
}

Xgl_plane* xgl_plane_at (Xgl_plane* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_plane_at_put (Xgl_plane* rcvr, fint which, Xgl_plane* plane) {
  rcvr[which] = *plane;
}

Xgl_pt_d3d* xgl_plane_normal (Xgl_plane* rcvr) {
   return &(rcvr->normal);
}
void xgl_plane_normal (Xgl_plane* rcvr, Xgl_pt_d3d* normal) {
   rcvr->normal = *normal;
}

Xgl_pt_d3d* xgl_plane_pt (Xgl_plane* rcvr) {
   return &(rcvr->pt);
}
void xgl_plane_pt (Xgl_plane* rcvr, Xgl_pt_d3d* pt) {
   rcvr->pt = *pt;
}


// for Xgl_plane_list
Xgl_plane_list* xgl_plane_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_plane_list, count);
}
void xgl_plane_list_delete (Xgl_plane_list* rcvr) {
  delete [] rcvr;
}

Xgl_plane_list* xgl_plane_list_at (Xgl_plane_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_plane_list_at_put (Xgl_plane_list* rcvr, fint which, Xgl_plane_list* planeList) {
  rcvr[which] = *planeList;
}


// for Xgl_pt
Xgl_pt* xgl_pt_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt, count);
}
void xgl_pt_delete (Xgl_pt* rcvr) {
  delete [] rcvr;
}

Xgl_pt* xgl_pt_at (Xgl_pt* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_at_put (Xgl_pt* rcvr, fint which, Xgl_pt* pt) {
  rcvr[which] = *pt;
}

Xgl_pt_d2d* xgl_pt_pt_d2d (Xgl_pt* rcvr) {
   return rcvr->pt.d2d;
}
void xgl_pt_pt_d2d (Xgl_pt* rcvr, Xgl_pt_d2d* d2d) {
   rcvr->pt.d2d = d2d;
}

Xgl_pt_d3d* xgl_pt_pt_d3d (Xgl_pt* rcvr) {
   return rcvr->pt.d3d;
}
void xgl_pt_pt_d3d (Xgl_pt* rcvr, Xgl_pt_d3d* d3d) {
   rcvr->pt.d3d = d3d;
}

Xgl_pt_f2d* xgl_pt_pt_f2d (Xgl_pt* rcvr) {
   return rcvr->pt.f2d;
}
void xgl_pt_pt_f2d (Xgl_pt* rcvr, Xgl_pt_f2d* f2d) {
   rcvr->pt.f2d = f2d;
}

Xgl_pt_f3d* xgl_pt_pt_f3d (Xgl_pt* rcvr) {
   return rcvr->pt.f3d;
}
void xgl_pt_pt_f3d (Xgl_pt* rcvr, Xgl_pt_f3d* f3d) {
   rcvr->pt.f3d = f3d;
}

Xgl_pt_i2d* xgl_pt_pt_i2d (Xgl_pt* rcvr) {
   return rcvr->pt.i2d;
}
void xgl_pt_pt_i2d (Xgl_pt* rcvr, Xgl_pt_i2d* i2d) {
   rcvr->pt.i2d = i2d;
}

void xgl_pt_set (Xgl_pt* rcvr,
	Xgl_pt_d2d* d2d,
	Xgl_pt_type ptType) {
  rcvr->pt.d2d = d2d;
  rcvr->pt_type = ptType;
}

void xgl_pt_set (Xgl_pt* rcvr,
	Xgl_pt_d3d* d3d,
	Xgl_pt_type ptType) {
  rcvr->pt.d3d = d3d;
  rcvr->pt_type = ptType;
}

void xgl_pt_set (Xgl_pt* rcvr,
	Xgl_pt_f2d* f2d,
	Xgl_pt_type ptType) {
  rcvr->pt.f2d = f2d;
  rcvr->pt_type = ptType;
}

void xgl_pt_set (Xgl_pt* rcvr,
	Xgl_pt_f3d* f3d,
	Xgl_pt_type ptType) {
  rcvr->pt.f3d = f3d;
  rcvr->pt_type = ptType;
}

void xgl_pt_set (Xgl_pt* rcvr,
	Xgl_pt_i2d* i2d,
	Xgl_pt_type ptType) {
  rcvr->pt.i2d = i2d;
  rcvr->pt_type = ptType;
}


// for Xgl_pt_color_d2d
Xgl_pt_color_d2d* xgl_pt_color_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_d2d, count);
}
void xgl_pt_color_d2d_delete (Xgl_pt_color_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_d2d* xgl_pt_color_d2d_at (Xgl_pt_color_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_d2d_at_put (Xgl_pt_color_d2d* rcvr, fint which, Xgl_pt_color_d2d* ptColorD2d) {
  rcvr[which] = *ptColorD2d;
}

float xgl_pt_color_d2d_color_gray (Xgl_pt_color_d2d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_d2d_color_gray (Xgl_pt_color_d2d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_d2d_color_index (Xgl_pt_color_d2d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_d2d_color_index (Xgl_pt_color_d2d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_d2d_color_rgb (Xgl_pt_color_d2d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_d2d_color_rgb (Xgl_pt_color_d2d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_d2d_color_z (Xgl_pt_color_d2d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_d2d_color_z (Xgl_pt_color_d2d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_d2d_set (Xgl_pt_color_d2d* rcvr,
	float color_gray,
	double x,
	double y) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_d2d_set (Xgl_pt_color_d2d* rcvr,
	unsigned long color_index,
	double x,
	double y) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_d2d_set (Xgl_pt_color_d2d* rcvr,
	Xgl_color_rgb* color_rgb,
	double x,
	double y) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_color_d3d
Xgl_pt_color_d3d* xgl_pt_color_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_d3d, count);
}
void xgl_pt_color_d3d_delete (Xgl_pt_color_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_d3d* xgl_pt_color_d3d_at (Xgl_pt_color_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_d3d_at_put (Xgl_pt_color_d3d* rcvr, fint which, Xgl_pt_color_d3d* ptColorD3d) {
  rcvr[which] = *ptColorD3d;
}

float xgl_pt_color_d3d_color_gray (Xgl_pt_color_d3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_d3d_color_gray (Xgl_pt_color_d3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_d3d_color_index (Xgl_pt_color_d3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_d3d_color_index (Xgl_pt_color_d3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_d3d_color_rgb (Xgl_pt_color_d3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_d3d_color_rgb (Xgl_pt_color_d3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_d3d_color_z (Xgl_pt_color_d3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_d3d_color_z (Xgl_pt_color_d3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_d3d_set (Xgl_pt_color_d3d* rcvr,
	float color_gray,
	double x,
	double y,
	double z) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_d3d_set (Xgl_pt_color_d3d* rcvr,
	unsigned long color_index,
	double x,
	double y,
	double z) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_d3d_set (Xgl_pt_color_d3d* rcvr,
	Xgl_color_rgb* color_rgb,
	double x,
	double y,
	double z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_data_f3d
Xgl_pt_color_data_f3d* xgl_pt_color_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_data_f3d, count);
}
void xgl_pt_color_data_f3d_delete (Xgl_pt_color_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_data_f3d* xgl_pt_color_data_f3d_at (Xgl_pt_color_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_data_f3d_at_put (Xgl_pt_color_data_f3d* rcvr, fint which, Xgl_pt_color_data_f3d* ptColorDataF3d) {
  rcvr[which] = *ptColorDataF3d;
}

float xgl_pt_color_data_f3d_color_gray (Xgl_pt_color_data_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_data_f3d_color_gray (Xgl_pt_color_data_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_data_f3d_color_index (Xgl_pt_color_data_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_data_f3d_color_index (Xgl_pt_color_data_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_data_f3d_color_rgb (Xgl_pt_color_data_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_data_f3d_color_rgb (Xgl_pt_color_data_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_data_f3d_color_z (Xgl_pt_color_data_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_data_f3d_color_z (Xgl_pt_color_data_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

float* xgl_pt_color_data_f3d_data (Xgl_pt_color_data_f3d* rcvr) {
   return rcvr->data;
}
void xgl_pt_color_data_f3d_set (Xgl_pt_color_data_f3d* rcvr,
	float color_gray,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_data_f3d_set (Xgl_pt_color_data_f3d* rcvr,
	unsigned long color_index,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_data_f3d_set (Xgl_pt_color_data_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_f2d
Xgl_pt_color_f2d* xgl_pt_color_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_f2d, count);
}
void xgl_pt_color_f2d_delete (Xgl_pt_color_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_f2d* xgl_pt_color_f2d_at (Xgl_pt_color_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_f2d_at_put (Xgl_pt_color_f2d* rcvr, fint which, Xgl_pt_color_f2d* ptColorF2d) {
  rcvr[which] = *ptColorF2d;
}

float xgl_pt_color_f2d_color_gray (Xgl_pt_color_f2d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_f2d_color_gray (Xgl_pt_color_f2d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_f2d_color_index (Xgl_pt_color_f2d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_f2d_color_index (Xgl_pt_color_f2d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_f2d_color_rgb (Xgl_pt_color_f2d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_f2d_color_rgb (Xgl_pt_color_f2d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_f2d_color_z (Xgl_pt_color_f2d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_f2d_color_z (Xgl_pt_color_f2d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_f2d_set (Xgl_pt_color_f2d* rcvr,
	float color_gray,
	float x,
	float y) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_f2d_set (Xgl_pt_color_f2d* rcvr,
	unsigned long color_index,
	float x,
	float y) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_f2d_set (Xgl_pt_color_f2d* rcvr,
	Xgl_color_rgb* color_rgb,
	float x,
	float y) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_color_f3d
Xgl_pt_color_f3d* xgl_pt_color_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_f3d, count);
}
void xgl_pt_color_f3d_delete (Xgl_pt_color_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_f3d* xgl_pt_color_f3d_at (Xgl_pt_color_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_f3d_at_put (Xgl_pt_color_f3d* rcvr, fint which, Xgl_pt_color_f3d* ptColorF3d) {
  rcvr[which] = *ptColorF3d;
}

float xgl_pt_color_f3d_color_gray (Xgl_pt_color_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_f3d_color_gray (Xgl_pt_color_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_f3d_color_index (Xgl_pt_color_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_f3d_color_index (Xgl_pt_color_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_f3d_color_rgb (Xgl_pt_color_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_f3d_color_rgb (Xgl_pt_color_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_f3d_color_z (Xgl_pt_color_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_f3d_color_z (Xgl_pt_color_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_f3d_set (Xgl_pt_color_f3d* rcvr,
	float color_gray,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_f3d_set (Xgl_pt_color_f3d* rcvr,
	unsigned long color_index,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_f3d_set (Xgl_pt_color_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_flag_d3d
Xgl_pt_color_flag_d3d* xgl_pt_color_flag_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_flag_d3d, count);
}
void xgl_pt_color_flag_d3d_delete (Xgl_pt_color_flag_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_flag_d3d* xgl_pt_color_flag_d3d_at (Xgl_pt_color_flag_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_flag_d3d_at_put (Xgl_pt_color_flag_d3d* rcvr, fint which, Xgl_pt_color_flag_d3d* ptColorFlagD3d) {
  rcvr[which] = *ptColorFlagD3d;
}

float xgl_pt_color_flag_d3d_color_gray (Xgl_pt_color_flag_d3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_flag_d3d_color_gray (Xgl_pt_color_flag_d3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_flag_d3d_color_index (Xgl_pt_color_flag_d3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_flag_d3d_color_index (Xgl_pt_color_flag_d3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_flag_d3d_color_rgb (Xgl_pt_color_flag_d3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_flag_d3d_color_rgb (Xgl_pt_color_flag_d3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_flag_d3d_color_z (Xgl_pt_color_flag_d3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_flag_d3d_color_z (Xgl_pt_color_flag_d3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_flag_d3d_set (Xgl_pt_color_flag_d3d* rcvr,
	float color_gray,
	unsigned long flag,
	double x,
	double y,
	double z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_d3d_set (Xgl_pt_color_flag_d3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	double x,
	double y,
	double z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_d3d_set (Xgl_pt_color_flag_d3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	double x,
	double y,
	double z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_flag_data_f3d
Xgl_pt_color_flag_data_f3d* xgl_pt_color_flag_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_flag_data_f3d, count);
}
void xgl_pt_color_flag_data_f3d_delete (Xgl_pt_color_flag_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_flag_data_f3d* xgl_pt_color_flag_data_f3d_at (Xgl_pt_color_flag_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_flag_data_f3d_at_put (Xgl_pt_color_flag_data_f3d* rcvr, fint which, Xgl_pt_color_flag_data_f3d* ptColorFlagDataF3d) {
  rcvr[which] = *ptColorFlagDataF3d;
}

float xgl_pt_color_flag_data_f3d_color_gray (Xgl_pt_color_flag_data_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_flag_data_f3d_color_gray (Xgl_pt_color_flag_data_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_flag_data_f3d_color_index (Xgl_pt_color_flag_data_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_flag_data_f3d_color_index (Xgl_pt_color_flag_data_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_flag_data_f3d_color_rgb (Xgl_pt_color_flag_data_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_flag_data_f3d_color_rgb (Xgl_pt_color_flag_data_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_flag_data_f3d_color_z (Xgl_pt_color_flag_data_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_flag_data_f3d_color_z (Xgl_pt_color_flag_data_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

float* xgl_pt_color_flag_data_f3d_data (Xgl_pt_color_flag_data_f3d* rcvr) {
   return rcvr->data;
}
void xgl_pt_color_flag_data_f3d_set (Xgl_pt_color_flag_data_f3d* rcvr,
	float color_gray,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_data_f3d_set (Xgl_pt_color_flag_data_f3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_data_f3d_set (Xgl_pt_color_flag_data_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_flag_f3d
Xgl_pt_color_flag_f3d* xgl_pt_color_flag_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_flag_f3d, count);
}
void xgl_pt_color_flag_f3d_delete (Xgl_pt_color_flag_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_flag_f3d* xgl_pt_color_flag_f3d_at (Xgl_pt_color_flag_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_flag_f3d_at_put (Xgl_pt_color_flag_f3d* rcvr, fint which, Xgl_pt_color_flag_f3d* ptColorFlagF3d) {
  rcvr[which] = *ptColorFlagF3d;
}

float xgl_pt_color_flag_f3d_color_gray (Xgl_pt_color_flag_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_flag_f3d_color_gray (Xgl_pt_color_flag_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_flag_f3d_color_index (Xgl_pt_color_flag_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_flag_f3d_color_index (Xgl_pt_color_flag_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_flag_f3d_color_rgb (Xgl_pt_color_flag_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_flag_f3d_color_rgb (Xgl_pt_color_flag_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_flag_f3d_color_z (Xgl_pt_color_flag_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_flag_f3d_color_z (Xgl_pt_color_flag_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_flag_f3d_set (Xgl_pt_color_flag_f3d* rcvr,
	float color_gray,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_f3d_set (Xgl_pt_color_flag_f3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_flag_f3d_set (Xgl_pt_color_flag_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_i2d
Xgl_pt_color_i2d* xgl_pt_color_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_i2d, count);
}
void xgl_pt_color_i2d_delete (Xgl_pt_color_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_i2d* xgl_pt_color_i2d_at (Xgl_pt_color_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_i2d_at_put (Xgl_pt_color_i2d* rcvr, fint which, Xgl_pt_color_i2d* ptColorI2d) {
  rcvr[which] = *ptColorI2d;
}

float xgl_pt_color_i2d_color_gray (Xgl_pt_color_i2d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_i2d_color_gray (Xgl_pt_color_i2d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_i2d_color_index (Xgl_pt_color_i2d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_i2d_color_index (Xgl_pt_color_i2d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_i2d_color_rgb (Xgl_pt_color_i2d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_i2d_color_rgb (Xgl_pt_color_i2d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_i2d_color_z (Xgl_pt_color_i2d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_i2d_color_z (Xgl_pt_color_i2d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

void xgl_pt_color_i2d_set (Xgl_pt_color_i2d* rcvr,
	float color_gray,
	long x,
	long y) {
  rcvr->color.gray = color_gray;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_i2d_set (Xgl_pt_color_i2d* rcvr,
	unsigned long color_index,
	long x,
	long y) {
  rcvr->color.index = color_index;
  rcvr->x = x;
  rcvr->y = y;
}

void xgl_pt_color_i2d_set (Xgl_pt_color_i2d* rcvr,
	Xgl_color_rgb* color_rgb,
	long x,
	long y) {
  rcvr->color.rgb = *color_rgb;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_color_normal_d3d
Xgl_pt_color_normal_d3d* xgl_pt_color_normal_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_d3d, count);
}
void xgl_pt_color_normal_d3d_delete (Xgl_pt_color_normal_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_d3d* xgl_pt_color_normal_d3d_at (Xgl_pt_color_normal_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_d3d_at_put (Xgl_pt_color_normal_d3d* rcvr, fint which, Xgl_pt_color_normal_d3d* ptColorNormalD3d) {
  rcvr[which] = *ptColorNormalD3d;
}

float xgl_pt_color_normal_d3d_color_gray (Xgl_pt_color_normal_d3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_d3d_color_gray (Xgl_pt_color_normal_d3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_d3d_color_index (Xgl_pt_color_normal_d3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_d3d_color_index (Xgl_pt_color_normal_d3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_d3d_color_rgb (Xgl_pt_color_normal_d3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_d3d_color_rgb (Xgl_pt_color_normal_d3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_d3d_color_z (Xgl_pt_color_normal_d3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_d3d_color_z (Xgl_pt_color_normal_d3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

Xgl_pt_f3d* xgl_pt_color_normal_d3d_normal (Xgl_pt_color_normal_d3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_d3d_normal (Xgl_pt_color_normal_d3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_d3d_set (Xgl_pt_color_normal_d3d* rcvr,
	float color_gray,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.gray = color_gray;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_d3d_set (Xgl_pt_color_normal_d3d* rcvr,
	unsigned long color_index,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.index = color_index;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_d3d_set (Xgl_pt_color_normal_d3d* rcvr,
	Xgl_color_rgb* color_rgb,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_normal_data_f3d
Xgl_pt_color_normal_data_f3d* xgl_pt_color_normal_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_data_f3d, count);
}
void xgl_pt_color_normal_data_f3d_delete (Xgl_pt_color_normal_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_data_f3d* xgl_pt_color_normal_data_f3d_at (Xgl_pt_color_normal_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_data_f3d_at_put (Xgl_pt_color_normal_data_f3d* rcvr, fint which, Xgl_pt_color_normal_data_f3d* ptColorNormalDataF3d) {
  rcvr[which] = *ptColorNormalDataF3d;
}

float xgl_pt_color_normal_data_f3d_color_gray (Xgl_pt_color_normal_data_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_data_f3d_color_gray (Xgl_pt_color_normal_data_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_data_f3d_color_index (Xgl_pt_color_normal_data_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_data_f3d_color_index (Xgl_pt_color_normal_data_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_data_f3d_color_rgb (Xgl_pt_color_normal_data_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_data_f3d_color_rgb (Xgl_pt_color_normal_data_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_data_f3d_color_z (Xgl_pt_color_normal_data_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_data_f3d_color_z (Xgl_pt_color_normal_data_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

float* xgl_pt_color_normal_data_f3d_data (Xgl_pt_color_normal_data_f3d* rcvr) {
   return rcvr->data;
}
Xgl_pt_f3d* xgl_pt_color_normal_data_f3d_normal (Xgl_pt_color_normal_data_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_data_f3d_normal (Xgl_pt_color_normal_data_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_data_f3d_set (Xgl_pt_color_normal_data_f3d* rcvr,
	float color_gray,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_data_f3d_set (Xgl_pt_color_normal_data_f3d* rcvr,
	unsigned long color_index,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_data_f3d_set (Xgl_pt_color_normal_data_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_normal_f3d
Xgl_pt_color_normal_f3d* xgl_pt_color_normal_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_f3d, count);
}
void xgl_pt_color_normal_f3d_delete (Xgl_pt_color_normal_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_f3d* xgl_pt_color_normal_f3d_at (Xgl_pt_color_normal_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_f3d_at_put (Xgl_pt_color_normal_f3d* rcvr, fint which, Xgl_pt_color_normal_f3d* ptColorNormalF3d) {
  rcvr[which] = *ptColorNormalF3d;
}

float xgl_pt_color_normal_f3d_color_gray (Xgl_pt_color_normal_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_f3d_color_gray (Xgl_pt_color_normal_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_f3d_color_index (Xgl_pt_color_normal_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_f3d_color_index (Xgl_pt_color_normal_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_f3d_color_rgb (Xgl_pt_color_normal_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_f3d_color_rgb (Xgl_pt_color_normal_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_f3d_color_z (Xgl_pt_color_normal_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_f3d_color_z (Xgl_pt_color_normal_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

Xgl_pt_f3d* xgl_pt_color_normal_f3d_normal (Xgl_pt_color_normal_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_f3d_normal (Xgl_pt_color_normal_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_f3d_set (Xgl_pt_color_normal_f3d* rcvr,
	float color_gray,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_f3d_set (Xgl_pt_color_normal_f3d* rcvr,
	unsigned long color_index,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_f3d_set (Xgl_pt_color_normal_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_normal_flag_d3d
Xgl_pt_color_normal_flag_d3d* xgl_pt_color_normal_flag_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_flag_d3d, count);
}
void xgl_pt_color_normal_flag_d3d_delete (Xgl_pt_color_normal_flag_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_flag_d3d* xgl_pt_color_normal_flag_d3d_at (Xgl_pt_color_normal_flag_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_flag_d3d_at_put (Xgl_pt_color_normal_flag_d3d* rcvr, fint which, Xgl_pt_color_normal_flag_d3d* ptColorNormalFlagD3d) {
  rcvr[which] = *ptColorNormalFlagD3d;
}

float xgl_pt_color_normal_flag_d3d_color_gray (Xgl_pt_color_normal_flag_d3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_flag_d3d_color_gray (Xgl_pt_color_normal_flag_d3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_flag_d3d_color_index (Xgl_pt_color_normal_flag_d3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_flag_d3d_color_index (Xgl_pt_color_normal_flag_d3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_flag_d3d_color_rgb (Xgl_pt_color_normal_flag_d3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_flag_d3d_color_rgb (Xgl_pt_color_normal_flag_d3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_flag_d3d_color_z (Xgl_pt_color_normal_flag_d3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_flag_d3d_color_z (Xgl_pt_color_normal_flag_d3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

Xgl_pt_f3d* xgl_pt_color_normal_flag_d3d_normal (Xgl_pt_color_normal_flag_d3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_flag_d3d_normal (Xgl_pt_color_normal_flag_d3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_flag_d3d_set (Xgl_pt_color_normal_flag_d3d* rcvr,
	float color_gray,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_d3d_set (Xgl_pt_color_normal_flag_d3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_d3d_set (Xgl_pt_color_normal_flag_d3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_normal_flag_data_f3d
Xgl_pt_color_normal_flag_data_f3d* xgl_pt_color_normal_flag_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_flag_data_f3d, count);
}
void xgl_pt_color_normal_flag_data_f3d_delete (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_flag_data_f3d* xgl_pt_color_normal_flag_data_f3d_at (Xgl_pt_color_normal_flag_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_flag_data_f3d_at_put (Xgl_pt_color_normal_flag_data_f3d* rcvr, fint which, Xgl_pt_color_normal_flag_data_f3d* ptColorNormalFlagDataF3d) {
  rcvr[which] = *ptColorNormalFlagDataF3d;
}

float xgl_pt_color_normal_flag_data_f3d_color_gray (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_flag_data_f3d_color_gray (Xgl_pt_color_normal_flag_data_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_flag_data_f3d_color_index (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_flag_data_f3d_color_index (Xgl_pt_color_normal_flag_data_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_flag_data_f3d_color_rgb (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_flag_data_f3d_color_rgb (Xgl_pt_color_normal_flag_data_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_flag_data_f3d_color_z (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_flag_data_f3d_color_z (Xgl_pt_color_normal_flag_data_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

float* xgl_pt_color_normal_flag_data_f3d_data (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return rcvr->data;
}
Xgl_pt_f3d* xgl_pt_color_normal_flag_data_f3d_normal (Xgl_pt_color_normal_flag_data_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_flag_data_f3d_normal (Xgl_pt_color_normal_flag_data_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_flag_data_f3d_set (Xgl_pt_color_normal_flag_data_f3d* rcvr,
	float color_gray,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_data_f3d_set (Xgl_pt_color_normal_flag_data_f3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_data_f3d_set (Xgl_pt_color_normal_flag_data_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_color_normal_flag_f3d
Xgl_pt_color_normal_flag_f3d* xgl_pt_color_normal_flag_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_color_normal_flag_f3d, count);
}
void xgl_pt_color_normal_flag_f3d_delete (Xgl_pt_color_normal_flag_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_color_normal_flag_f3d* xgl_pt_color_normal_flag_f3d_at (Xgl_pt_color_normal_flag_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_color_normal_flag_f3d_at_put (Xgl_pt_color_normal_flag_f3d* rcvr, fint which, Xgl_pt_color_normal_flag_f3d* ptColorNormalFlagF3d) {
  rcvr[which] = *ptColorNormalFlagF3d;
}

float xgl_pt_color_normal_flag_f3d_color_gray (Xgl_pt_color_normal_flag_f3d* rcvr) {
   return rcvr->color.gray;
}
void xgl_pt_color_normal_flag_f3d_color_gray (Xgl_pt_color_normal_flag_f3d* rcvr, float gray) {
   rcvr->color.gray = gray;
}

unsigned long xgl_pt_color_normal_flag_f3d_color_index (Xgl_pt_color_normal_flag_f3d* rcvr) {
   return rcvr->color.index;
}
void xgl_pt_color_normal_flag_f3d_color_index (Xgl_pt_color_normal_flag_f3d* rcvr, unsigned long index) {
   rcvr->color.index = index;
}

Xgl_color_rgb* xgl_pt_color_normal_flag_f3d_color_rgb (Xgl_pt_color_normal_flag_f3d* rcvr) {
   return &(rcvr->color.rgb);
}
void xgl_pt_color_normal_flag_f3d_color_rgb (Xgl_pt_color_normal_flag_f3d* rcvr, Xgl_color_rgb* rgb) {
   rcvr->color.rgb = *rgb;
}

unsigned long xgl_pt_color_normal_flag_f3d_color_z (Xgl_pt_color_normal_flag_f3d* rcvr) {
   return rcvr->color.z;
}
void xgl_pt_color_normal_flag_f3d_color_z (Xgl_pt_color_normal_flag_f3d* rcvr, unsigned long z) {
   rcvr->color.z = z;
}

Xgl_pt_f3d* xgl_pt_color_normal_flag_f3d_normal (Xgl_pt_color_normal_flag_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_color_normal_flag_f3d_normal (Xgl_pt_color_normal_flag_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_color_normal_flag_f3d_set (Xgl_pt_color_normal_flag_f3d* rcvr,
	float color_gray,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.gray = color_gray;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_f3d_set (Xgl_pt_color_normal_flag_f3d* rcvr,
	unsigned long color_index,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.index = color_index;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}

void xgl_pt_color_normal_flag_f3d_set (Xgl_pt_color_normal_flag_f3d* rcvr,
	Xgl_color_rgb* color_rgb,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->color.rgb = *color_rgb;
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_d2d
Xgl_pt_d2d* xgl_pt_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_d2d, count);
}
void xgl_pt_d2d_delete (Xgl_pt_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_d2d* xgl_pt_d2d_at (Xgl_pt_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_d2d_at_put (Xgl_pt_d2d* rcvr, fint which, Xgl_pt_d2d* ptD2d) {
  rcvr[which] = *ptD2d;
}


// for Xgl_pt_d2h
Xgl_pt_d2h* xgl_pt_d2h_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_d2h, count);
}
void xgl_pt_d2h_delete (Xgl_pt_d2h* rcvr) {
  delete [] rcvr;
}

Xgl_pt_d2h* xgl_pt_d2h_at (Xgl_pt_d2h* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_d2h_at_put (Xgl_pt_d2h* rcvr, fint which, Xgl_pt_d2h* ptD2h) {
  rcvr[which] = *ptD2h;
}

void xgl_pt_d2h_set (Xgl_pt_d2h* rcvr,
	double w,
	double x,
	double y) {
  rcvr->w = w;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_d3d
Xgl_pt_d3d* xgl_pt_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_d3d, count);
}
void xgl_pt_d3d_delete (Xgl_pt_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_d3d* xgl_pt_d3d_at (Xgl_pt_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_d3d_at_put (Xgl_pt_d3d* rcvr, fint which, Xgl_pt_d3d* ptD3d) {
  rcvr[which] = *ptD3d;
}

void xgl_pt_d3d_set (Xgl_pt_d3d* rcvr,
	double x,
	double y,
	double z) {
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_d3h
Xgl_pt_d3h* xgl_pt_d3h_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_d3h, count);
}
void xgl_pt_d3h_delete (Xgl_pt_d3h* rcvr) {
  delete [] rcvr;
}

Xgl_pt_d3h* xgl_pt_d3h_at (Xgl_pt_d3h* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_d3h_at_put (Xgl_pt_d3h* rcvr, fint which, Xgl_pt_d3h* ptD3h) {
  rcvr[which] = *ptD3h;
}

void xgl_pt_d3h_set (Xgl_pt_d3h* rcvr,
	double w,
	double x,
	double y,
	double z) {
  rcvr->w = w;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_data_f3d
Xgl_pt_data_f3d* xgl_pt_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_data_f3d, count);
}
void xgl_pt_data_f3d_delete (Xgl_pt_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_data_f3d* xgl_pt_data_f3d_at (Xgl_pt_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_data_f3d_at_put (Xgl_pt_data_f3d* rcvr, fint which, Xgl_pt_data_f3d* ptDataF3d) {
  rcvr[which] = *ptDataF3d;
}

float* xgl_pt_data_f3d_data (Xgl_pt_data_f3d* rcvr) {
   return rcvr->data;
}
void xgl_pt_data_f3d_set (Xgl_pt_data_f3d* rcvr,
	float x,
	float y,
	float z) {
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_f2d
Xgl_pt_f2d* xgl_pt_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_f2d, count);
}
void xgl_pt_f2d_delete (Xgl_pt_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_f2d* xgl_pt_f2d_at (Xgl_pt_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_f2d_at_put (Xgl_pt_f2d* rcvr, fint which, Xgl_pt_f2d* ptF2d) {
  rcvr[which] = *ptF2d;
}


// for Xgl_pt_f2h
Xgl_pt_f2h* xgl_pt_f2h_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_f2h, count);
}
void xgl_pt_f2h_delete (Xgl_pt_f2h* rcvr) {
  delete [] rcvr;
}

Xgl_pt_f2h* xgl_pt_f2h_at (Xgl_pt_f2h* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_f2h_at_put (Xgl_pt_f2h* rcvr, fint which, Xgl_pt_f2h* ptF2h) {
  rcvr[which] = *ptF2h;
}

void xgl_pt_f2h_set (Xgl_pt_f2h* rcvr,
	float w,
	float x,
	float y) {
  rcvr->w = w;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_f3d
Xgl_pt_f3d* xgl_pt_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_f3d, count);
}
void xgl_pt_f3d_delete (Xgl_pt_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_f3d* xgl_pt_f3d_at (Xgl_pt_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_f3d_at_put (Xgl_pt_f3d* rcvr, fint which, Xgl_pt_f3d* ptF3d) {
  rcvr[which] = *ptF3d;
}

void xgl_pt_f3d_set (Xgl_pt_f3d* rcvr,
	float x,
	float y,
	float z) {
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_f3h
Xgl_pt_f3h* xgl_pt_f3h_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_f3h, count);
}
void xgl_pt_f3h_delete (Xgl_pt_f3h* rcvr) {
  delete [] rcvr;
}

Xgl_pt_f3h* xgl_pt_f3h_at (Xgl_pt_f3h* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_f3h_at_put (Xgl_pt_f3h* rcvr, fint which, Xgl_pt_f3h* ptF3h) {
  rcvr[which] = *ptF3h;
}

void xgl_pt_f3h_set (Xgl_pt_f3h* rcvr,
	float w,
	float x,
	float y,
	float z) {
  rcvr->w = w;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_flag_d2d
Xgl_pt_flag_d2d* xgl_pt_flag_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_d2d, count);
}
void xgl_pt_flag_d2d_delete (Xgl_pt_flag_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_d2d* xgl_pt_flag_d2d_at (Xgl_pt_flag_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_d2d_at_put (Xgl_pt_flag_d2d* rcvr, fint which, Xgl_pt_flag_d2d* ptFlagD2d) {
  rcvr[which] = *ptFlagD2d;
}

void xgl_pt_flag_d2d_set (Xgl_pt_flag_d2d* rcvr,
	unsigned long flag,
	double x,
	double y) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_flag_d3d
Xgl_pt_flag_d3d* xgl_pt_flag_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_d3d, count);
}
void xgl_pt_flag_d3d_delete (Xgl_pt_flag_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_d3d* xgl_pt_flag_d3d_at (Xgl_pt_flag_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_d3d_at_put (Xgl_pt_flag_d3d* rcvr, fint which, Xgl_pt_flag_d3d* ptFlagD3d) {
  rcvr[which] = *ptFlagD3d;
}

void xgl_pt_flag_d3d_set (Xgl_pt_flag_d3d* rcvr,
	unsigned long flag,
	double x,
	double y,
	double z) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_flag_data_f3d
Xgl_pt_flag_data_f3d* xgl_pt_flag_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_data_f3d, count);
}
void xgl_pt_flag_data_f3d_delete (Xgl_pt_flag_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_data_f3d* xgl_pt_flag_data_f3d_at (Xgl_pt_flag_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_data_f3d_at_put (Xgl_pt_flag_data_f3d* rcvr, fint which, Xgl_pt_flag_data_f3d* ptFlagDataF3d) {
  rcvr[which] = *ptFlagDataF3d;
}

float* xgl_pt_flag_data_f3d_data (Xgl_pt_flag_data_f3d* rcvr) {
   return rcvr->data;
}
void xgl_pt_flag_data_f3d_set (Xgl_pt_flag_data_f3d* rcvr,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_flag_f2d
Xgl_pt_flag_f2d* xgl_pt_flag_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_f2d, count);
}
void xgl_pt_flag_f2d_delete (Xgl_pt_flag_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_f2d* xgl_pt_flag_f2d_at (Xgl_pt_flag_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_f2d_at_put (Xgl_pt_flag_f2d* rcvr, fint which, Xgl_pt_flag_f2d* ptFlagF2d) {
  rcvr[which] = *ptFlagF2d;
}

void xgl_pt_flag_f2d_set (Xgl_pt_flag_f2d* rcvr,
	unsigned long flag,
	float x,
	float y) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_flag_f3d
Xgl_pt_flag_f3d* xgl_pt_flag_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_f3d, count);
}
void xgl_pt_flag_f3d_delete (Xgl_pt_flag_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_f3d* xgl_pt_flag_f3d_at (Xgl_pt_flag_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_f3d_at_put (Xgl_pt_flag_f3d* rcvr, fint which, Xgl_pt_flag_f3d* ptFlagF3d) {
  rcvr[which] = *ptFlagF3d;
}

void xgl_pt_flag_f3d_set (Xgl_pt_flag_f3d* rcvr,
	unsigned long flag,
	float x,
	float y,
	float z) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_flag_i2d
Xgl_pt_flag_i2d* xgl_pt_flag_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_flag_i2d, count);
}
void xgl_pt_flag_i2d_delete (Xgl_pt_flag_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_flag_i2d* xgl_pt_flag_i2d_at (Xgl_pt_flag_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_flag_i2d_at_put (Xgl_pt_flag_i2d* rcvr, fint which, Xgl_pt_flag_i2d* ptFlagI2d) {
  rcvr[which] = *ptFlagI2d;
}

void xgl_pt_flag_i2d_set (Xgl_pt_flag_i2d* rcvr,
	unsigned long flag,
	long x,
	long y) {
  rcvr->flag = flag;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_i2d
Xgl_pt_i2d* xgl_pt_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_i2d, count);
}
void xgl_pt_i2d_delete (Xgl_pt_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_i2d* xgl_pt_i2d_at (Xgl_pt_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_i2d_at_put (Xgl_pt_i2d* rcvr, fint which, Xgl_pt_i2d* ptI2d) {
  rcvr[which] = *ptI2d;
}


// for Xgl_pt_i2h
Xgl_pt_i2h* xgl_pt_i2h_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_i2h, count);
}
void xgl_pt_i2h_delete (Xgl_pt_i2h* rcvr) {
  delete [] rcvr;
}

Xgl_pt_i2h* xgl_pt_i2h_at (Xgl_pt_i2h* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_i2h_at_put (Xgl_pt_i2h* rcvr, fint which, Xgl_pt_i2h* ptI2h) {
  rcvr[which] = *ptI2h;
}

void xgl_pt_i2h_set (Xgl_pt_i2h* rcvr,
	long w,
	long x,
	long y) {
  rcvr->w = w;
  rcvr->x = x;
  rcvr->y = y;
}


// for Xgl_pt_list
Xgl_pt_list* xgl_pt_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_list, count);
}
void xgl_pt_list_delete (Xgl_pt_list* rcvr) {
  delete [] rcvr;
}

Xgl_pt_list* xgl_pt_list_at (Xgl_pt_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_list_at_put (Xgl_pt_list* rcvr, fint which, Xgl_pt_list* ptList) {
  rcvr[which] = *ptList;
}

Xgl_pt_color_d2d* xgl_pt_list_pts_color_d2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_d2d;
}
void xgl_pt_list_pts_color_d2d (Xgl_pt_list* rcvr, Xgl_pt_color_d2d* colorD2d) {
   rcvr->pts.color_d2d = colorD2d;
}

Xgl_pt_color_d3d* xgl_pt_list_pts_color_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_d3d;
}
void xgl_pt_list_pts_color_d3d (Xgl_pt_list* rcvr, Xgl_pt_color_d3d* colorD3d) {
   rcvr->pts.color_d3d = colorD3d;
}

Xgl_pt_color_data_f3d* xgl_pt_list_pts_color_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_data_f3d;
}
void xgl_pt_list_pts_color_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_data_f3d* colorDataF3d) {
   rcvr->pts.color_data_f3d = colorDataF3d;
}

Xgl_pt_color_f2d* xgl_pt_list_pts_color_f2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_f2d;
}
void xgl_pt_list_pts_color_f2d (Xgl_pt_list* rcvr, Xgl_pt_color_f2d* colorF2d) {
   rcvr->pts.color_f2d = colorF2d;
}

Xgl_pt_color_f3d* xgl_pt_list_pts_color_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_f3d;
}
void xgl_pt_list_pts_color_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_f3d* colorF3d) {
   rcvr->pts.color_f3d = colorF3d;
}

Xgl_pt_color_flag_d3d* xgl_pt_list_pts_color_flag_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_flag_d3d;
}
void xgl_pt_list_pts_color_flag_d3d (Xgl_pt_list* rcvr, Xgl_pt_color_flag_d3d* colorFlagD3d) {
   rcvr->pts.color_flag_d3d = colorFlagD3d;
}

Xgl_pt_color_flag_data_f3d* xgl_pt_list_pts_color_flag_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_flag_data_f3d;
}
void xgl_pt_list_pts_color_flag_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_flag_data_f3d* colorFlagDataF3d) {
   rcvr->pts.color_flag_data_f3d = colorFlagDataF3d;
}

Xgl_pt_color_flag_f3d* xgl_pt_list_pts_color_flag_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_flag_f3d;
}
void xgl_pt_list_pts_color_flag_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_flag_f3d* colorFlagF3d) {
   rcvr->pts.color_flag_f3d = colorFlagF3d;
}

Xgl_pt_color_i2d* xgl_pt_list_pts_color_i2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_i2d;
}
void xgl_pt_list_pts_color_i2d (Xgl_pt_list* rcvr, Xgl_pt_color_i2d* colorI2d) {
   rcvr->pts.color_i2d = colorI2d;
}

Xgl_pt_color_normal_d3d* xgl_pt_list_pts_color_normal_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_d3d;
}
void xgl_pt_list_pts_color_normal_d3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_d3d* colorNormalD3d) {
   rcvr->pts.color_normal_d3d = colorNormalD3d;
}

Xgl_pt_color_normal_data_f3d* xgl_pt_list_pts_color_normal_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_data_f3d;
}
void xgl_pt_list_pts_color_normal_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_data_f3d* colorNormalDataF3d) {
   rcvr->pts.color_normal_data_f3d = colorNormalDataF3d;
}

Xgl_pt_color_normal_f3d* xgl_pt_list_pts_color_normal_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_f3d;
}
void xgl_pt_list_pts_color_normal_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_f3d* colorNormalF3d) {
   rcvr->pts.color_normal_f3d = colorNormalF3d;
}

Xgl_pt_color_normal_flag_d3d* xgl_pt_list_pts_color_normal_flag_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_flag_d3d;
}
void xgl_pt_list_pts_color_normal_flag_d3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_flag_d3d* colorNormalFlagD3d) {
   rcvr->pts.color_normal_flag_d3d = colorNormalFlagD3d;
}

Xgl_pt_color_normal_flag_data_f3d* xgl_pt_list_pts_color_normal_flag_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_flag_data_f3d;
}
void xgl_pt_list_pts_color_normal_flag_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_flag_data_f3d* colorNormalFlagDataF3d) {
   rcvr->pts.color_normal_flag_data_f3d = colorNormalFlagDataF3d;
}

Xgl_pt_color_normal_flag_f3d* xgl_pt_list_pts_color_normal_flag_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.color_normal_flag_f3d;
}
void xgl_pt_list_pts_color_normal_flag_f3d (Xgl_pt_list* rcvr, Xgl_pt_color_normal_flag_f3d* colorNormalFlagF3d) {
   rcvr->pts.color_normal_flag_f3d = colorNormalFlagF3d;
}

Xgl_pt_d2d* xgl_pt_list_pts_d2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.d2d;
}
void xgl_pt_list_pts_d2d (Xgl_pt_list* rcvr, Xgl_pt_d2d* d2d) {
   rcvr->pts.d2d = d2d;
}

Xgl_pt_d2h* xgl_pt_list_pts_d2h (Xgl_pt_list* rcvr) {
   return rcvr->pts.d2h;
}
void xgl_pt_list_pts_d2h (Xgl_pt_list* rcvr, Xgl_pt_d2h* d2h) {
   rcvr->pts.d2h = d2h;
}

Xgl_pt_d3d* xgl_pt_list_pts_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.d3d;
}
void xgl_pt_list_pts_d3d (Xgl_pt_list* rcvr, Xgl_pt_d3d* d3d) {
   rcvr->pts.d3d = d3d;
}

Xgl_pt_d3h* xgl_pt_list_pts_d3h (Xgl_pt_list* rcvr) {
   return rcvr->pts.d3h;
}
void xgl_pt_list_pts_d3h (Xgl_pt_list* rcvr, Xgl_pt_d3h* d3h) {
   rcvr->pts.d3h = d3h;
}

Xgl_pt_data_f3d* xgl_pt_list_pts_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.data_f3d;
}
void xgl_pt_list_pts_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_data_f3d* dataF3d) {
   rcvr->pts.data_f3d = dataF3d;
}

Xgl_pt_f2d* xgl_pt_list_pts_f2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.f2d;
}
void xgl_pt_list_pts_f2d (Xgl_pt_list* rcvr, Xgl_pt_f2d* f2d) {
   rcvr->pts.f2d = f2d;
}

Xgl_pt_f2h* xgl_pt_list_pts_f2h (Xgl_pt_list* rcvr) {
   return rcvr->pts.f2h;
}
void xgl_pt_list_pts_f2h (Xgl_pt_list* rcvr, Xgl_pt_f2h* f2h) {
   rcvr->pts.f2h = f2h;
}

Xgl_pt_f3d* xgl_pt_list_pts_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.f3d;
}
void xgl_pt_list_pts_f3d (Xgl_pt_list* rcvr, Xgl_pt_f3d* f3d) {
   rcvr->pts.f3d = f3d;
}

Xgl_pt_f3h* xgl_pt_list_pts_f3h (Xgl_pt_list* rcvr) {
   return rcvr->pts.f3h;
}
void xgl_pt_list_pts_f3h (Xgl_pt_list* rcvr, Xgl_pt_f3h* f3h) {
   rcvr->pts.f3h = f3h;
}

Xgl_pt_flag_d2d* xgl_pt_list_pts_flag_d2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_d2d;
}
void xgl_pt_list_pts_flag_d2d (Xgl_pt_list* rcvr, Xgl_pt_flag_d2d* flagD2d) {
   rcvr->pts.flag_d2d = flagD2d;
}

Xgl_pt_flag_d3d* xgl_pt_list_pts_flag_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_d3d;
}
void xgl_pt_list_pts_flag_d3d (Xgl_pt_list* rcvr, Xgl_pt_flag_d3d* flagD3d) {
   rcvr->pts.flag_d3d = flagD3d;
}

Xgl_pt_flag_data_f3d* xgl_pt_list_pts_flag_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_data_f3d;
}
void xgl_pt_list_pts_flag_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_flag_data_f3d* flagDataF3d) {
   rcvr->pts.flag_data_f3d = flagDataF3d;
}

Xgl_pt_flag_f2d* xgl_pt_list_pts_flag_f2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_f2d;
}
void xgl_pt_list_pts_flag_f2d (Xgl_pt_list* rcvr, Xgl_pt_flag_f2d* flagF2d) {
   rcvr->pts.flag_f2d = flagF2d;
}

Xgl_pt_flag_f3d* xgl_pt_list_pts_flag_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_f3d;
}
void xgl_pt_list_pts_flag_f3d (Xgl_pt_list* rcvr, Xgl_pt_flag_f3d* flagF3d) {
   rcvr->pts.flag_f3d = flagF3d;
}

Xgl_pt_flag_i2d* xgl_pt_list_pts_flag_i2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.flag_i2d;
}
void xgl_pt_list_pts_flag_i2d (Xgl_pt_list* rcvr, Xgl_pt_flag_i2d* flagI2d) {
   rcvr->pts.flag_i2d = flagI2d;
}

Xgl_pt_i2d* xgl_pt_list_pts_i2d (Xgl_pt_list* rcvr) {
   return rcvr->pts.i2d;
}
void xgl_pt_list_pts_i2d (Xgl_pt_list* rcvr, Xgl_pt_i2d* i2d) {
   rcvr->pts.i2d = i2d;
}

Xgl_pt_i2h* xgl_pt_list_pts_i2h (Xgl_pt_list* rcvr) {
   return rcvr->pts.i2h;
}
void xgl_pt_list_pts_i2h (Xgl_pt_list* rcvr, Xgl_pt_i2h* i2h) {
   rcvr->pts.i2h = i2h;
}

Xgl_pt_normal_d3d* xgl_pt_list_pts_normal_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_d3d;
}
void xgl_pt_list_pts_normal_d3d (Xgl_pt_list* rcvr, Xgl_pt_normal_d3d* normalD3d) {
   rcvr->pts.normal_d3d = normalD3d;
}

Xgl_pt_normal_data_f3d* xgl_pt_list_pts_normal_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_data_f3d;
}
void xgl_pt_list_pts_normal_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_normal_data_f3d* normalDataF3d) {
   rcvr->pts.normal_data_f3d = normalDataF3d;
}

Xgl_pt_normal_f3d* xgl_pt_list_pts_normal_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_f3d;
}
void xgl_pt_list_pts_normal_f3d (Xgl_pt_list* rcvr, Xgl_pt_normal_f3d* normalF3d) {
   rcvr->pts.normal_f3d = normalF3d;
}

Xgl_pt_normal_flag_d3d* xgl_pt_list_pts_normal_flag_d3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_flag_d3d;
}
void xgl_pt_list_pts_normal_flag_d3d (Xgl_pt_list* rcvr, Xgl_pt_normal_flag_d3d* normalFlagD3d) {
   rcvr->pts.normal_flag_d3d = normalFlagD3d;
}

Xgl_pt_normal_flag_data_f3d* xgl_pt_list_pts_normal_flag_data_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_flag_data_f3d;
}
void xgl_pt_list_pts_normal_flag_data_f3d (Xgl_pt_list* rcvr, Xgl_pt_normal_flag_data_f3d* normalFlagDataF3d) {
   rcvr->pts.normal_flag_data_f3d = normalFlagDataF3d;
}

Xgl_pt_normal_flag_f3d* xgl_pt_list_pts_normal_flag_f3d (Xgl_pt_list* rcvr) {
   return rcvr->pts.normal_flag_f3d;
}
void xgl_pt_list_pts_normal_flag_f3d (Xgl_pt_list* rcvr, Xgl_pt_normal_flag_f3d* normalFlagF3d) {
   rcvr->pts.normal_flag_f3d = normalFlagF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_d2d* colorD2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_d2d = colorD2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_d3d* colorD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_d3d = colorD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_data_f3d* colorDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_data_f3d = colorDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_f2d* colorF2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_f2d = colorF2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_f3d* colorF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_f3d = colorF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_flag_d3d* colorFlagD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_flag_d3d = colorFlagD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_flag_data_f3d* colorFlagDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_flag_data_f3d = colorFlagDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_flag_f3d* colorFlagF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_flag_f3d = colorFlagF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_i2d* colorI2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_i2d = colorI2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_d3d* colorNormalD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_d3d = colorNormalD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_data_f3d* colorNormalDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_data_f3d = colorNormalDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_f3d* colorNormalF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_f3d = colorNormalF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_flag_d3d* colorNormalFlagD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_flag_d3d = colorNormalFlagD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_flag_data_f3d* colorNormalFlagDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_flag_data_f3d = colorNormalFlagDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_color_normal_flag_f3d* colorNormalFlagF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.color_normal_flag_f3d = colorNormalFlagF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_d2d* d2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.d2d = d2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_d2h* d2h) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.d2h = d2h;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_d3d* d3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.d3d = d3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_d3h* d3h) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.d3h = d3h;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_data_f3d* dataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.data_f3d = dataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_f2d* f2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.f2d = f2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_f2h* f2h) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.f2h = f2h;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_f3d* f3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.f3d = f3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_f3h* f3h) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.f3h = f3h;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_d2d* flagD2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_d2d = flagD2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_d3d* flagD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_d3d = flagD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_data_f3d* flagDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_data_f3d = flagDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_f2d* flagF2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_f2d = flagF2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_f3d* flagF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_f3d = flagF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_flag_i2d* flagI2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.flag_i2d = flagI2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_i2d* i2d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.i2d = i2d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_i2h* i2h) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.i2h = i2h;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_d3d* normalD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_d3d = normalD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_data_f3d* normalDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_data_f3d = normalDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_f3d* normalF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_f3d = normalF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_flag_d3d* normalFlagD3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_flag_d3d = normalFlagD3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_flag_data_f3d* normalFlagDataF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_flag_data_f3d = normalFlagDataF3d;
}

void xgl_pt_list_set (Xgl_pt_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numDataValues,
	unsigned long numPts,
	Xgl_pt_type ptType,
	Xgl_pt_normal_flag_f3d* normalFlagF3d) {
  rcvr->bbox = bbox;
  rcvr->num_data_values = numDataValues;
  rcvr->num_pts = numPts;
  rcvr->pt_type = ptType;
  rcvr->pts.normal_flag_f3d = normalFlagF3d;
}


// for Xgl_pt_list_list
Xgl_pt_list_list* xgl_pt_list_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_list_list, count);
}
void xgl_pt_list_list_delete (Xgl_pt_list_list* rcvr) {
  delete [] rcvr;
}

Xgl_pt_list_list* xgl_pt_list_list_at (Xgl_pt_list_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_list_list_at_put (Xgl_pt_list_list* rcvr, fint which, Xgl_pt_list_list* ptListList) {
  rcvr[which] = *ptListList;
}

void xgl_pt_list_list_set (Xgl_pt_list_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numPtLists,
	Xgl_pt_list* ptLists) {
  rcvr->bbox = bbox;
  rcvr->num_pt_lists = numPtLists;
  rcvr->pt_lists = ptLists;
}


// for Xgl_pt_normal_d3d
Xgl_pt_normal_d3d* xgl_pt_normal_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_d3d, count);
}
void xgl_pt_normal_d3d_delete (Xgl_pt_normal_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_d3d* xgl_pt_normal_d3d_at (Xgl_pt_normal_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_d3d_at_put (Xgl_pt_normal_d3d* rcvr, fint which, Xgl_pt_normal_d3d* ptNormalD3d) {
  rcvr[which] = *ptNormalD3d;
}

Xgl_pt_f3d* xgl_pt_normal_d3d_normal (Xgl_pt_normal_d3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_d3d_normal (Xgl_pt_normal_d3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_d3d_set (Xgl_pt_normal_d3d* rcvr,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_normal_data_f3d
Xgl_pt_normal_data_f3d* xgl_pt_normal_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_data_f3d, count);
}
void xgl_pt_normal_data_f3d_delete (Xgl_pt_normal_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_data_f3d* xgl_pt_normal_data_f3d_at (Xgl_pt_normal_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_data_f3d_at_put (Xgl_pt_normal_data_f3d* rcvr, fint which, Xgl_pt_normal_data_f3d* ptNormalDataF3d) {
  rcvr[which] = *ptNormalDataF3d;
}

float* xgl_pt_normal_data_f3d_data (Xgl_pt_normal_data_f3d* rcvr) {
   return rcvr->data;
}
Xgl_pt_f3d* xgl_pt_normal_data_f3d_normal (Xgl_pt_normal_data_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_data_f3d_normal (Xgl_pt_normal_data_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_data_f3d_set (Xgl_pt_normal_data_f3d* rcvr,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_normal_f3d
Xgl_pt_normal_f3d* xgl_pt_normal_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_f3d, count);
}
void xgl_pt_normal_f3d_delete (Xgl_pt_normal_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_f3d* xgl_pt_normal_f3d_at (Xgl_pt_normal_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_f3d_at_put (Xgl_pt_normal_f3d* rcvr, fint which, Xgl_pt_normal_f3d* ptNormalF3d) {
  rcvr[which] = *ptNormalF3d;
}

Xgl_pt_f3d* xgl_pt_normal_f3d_normal (Xgl_pt_normal_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_f3d_normal (Xgl_pt_normal_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_f3d_set (Xgl_pt_normal_f3d* rcvr,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_normal_flag_d3d
Xgl_pt_normal_flag_d3d* xgl_pt_normal_flag_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_flag_d3d, count);
}
void xgl_pt_normal_flag_d3d_delete (Xgl_pt_normal_flag_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_flag_d3d* xgl_pt_normal_flag_d3d_at (Xgl_pt_normal_flag_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_flag_d3d_at_put (Xgl_pt_normal_flag_d3d* rcvr, fint which, Xgl_pt_normal_flag_d3d* ptNormalFlagD3d) {
  rcvr[which] = *ptNormalFlagD3d;
}

Xgl_pt_f3d* xgl_pt_normal_flag_d3d_normal (Xgl_pt_normal_flag_d3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_flag_d3d_normal (Xgl_pt_normal_flag_d3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_flag_d3d_set (Xgl_pt_normal_flag_d3d* rcvr,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	double x,
	double y,
	double z) {
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_normal_flag_data_f3d
Xgl_pt_normal_flag_data_f3d* xgl_pt_normal_flag_data_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_flag_data_f3d, count);
}
void xgl_pt_normal_flag_data_f3d_delete (Xgl_pt_normal_flag_data_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_flag_data_f3d* xgl_pt_normal_flag_data_f3d_at (Xgl_pt_normal_flag_data_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_flag_data_f3d_at_put (Xgl_pt_normal_flag_data_f3d* rcvr, fint which, Xgl_pt_normal_flag_data_f3d* ptNormalFlagDataF3d) {
  rcvr[which] = *ptNormalFlagDataF3d;
}

float* xgl_pt_normal_flag_data_f3d_data (Xgl_pt_normal_flag_data_f3d* rcvr) {
   return rcvr->data;
}
Xgl_pt_f3d* xgl_pt_normal_flag_data_f3d_normal (Xgl_pt_normal_flag_data_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_flag_data_f3d_normal (Xgl_pt_normal_flag_data_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_flag_data_f3d_set (Xgl_pt_normal_flag_data_f3d* rcvr,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_normal_flag_f3d
Xgl_pt_normal_flag_f3d* xgl_pt_normal_flag_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_normal_flag_f3d, count);
}
void xgl_pt_normal_flag_f3d_delete (Xgl_pt_normal_flag_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_pt_normal_flag_f3d* xgl_pt_normal_flag_f3d_at (Xgl_pt_normal_flag_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_normal_flag_f3d_at_put (Xgl_pt_normal_flag_f3d* rcvr, fint which, Xgl_pt_normal_flag_f3d* ptNormalFlagF3d) {
  rcvr[which] = *ptNormalFlagF3d;
}

Xgl_pt_f3d* xgl_pt_normal_flag_f3d_normal (Xgl_pt_normal_flag_f3d* rcvr) {
   return &(rcvr->normal);
}
void xgl_pt_normal_flag_f3d_normal (Xgl_pt_normal_flag_f3d* rcvr, Xgl_pt_f3d* normal) {
   rcvr->normal = *normal;
}

void xgl_pt_normal_flag_f3d_set (Xgl_pt_normal_flag_f3d* rcvr,
	unsigned long flag,
	Xgl_pt_f3d* normal,
	float x,
	float y,
	float z) {
  rcvr->flag = flag;
  rcvr->normal = *normal;
  rcvr->x = x;
  rcvr->y = y;
  rcvr->z = z;
}


// for Xgl_pt_type_supported
Xgl_pt_type_supported* xgl_pt_type_supported_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_pt_type_supported, count);
}
void xgl_pt_type_supported_delete (Xgl_pt_type_supported* rcvr) {
  delete [] rcvr;
}

Xgl_pt_type_supported* xgl_pt_type_supported_at (Xgl_pt_type_supported* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_pt_type_supported_at_put (Xgl_pt_type_supported* rcvr, fint which, Xgl_pt_type_supported* ptTypeSupported) {
  rcvr[which] = *ptTypeSupported;
}

void xgl_pt_type_supported_set (Xgl_pt_type_supported* rcvr,
	unsigned int ptDim2d,
	unsigned int ptDim3d,
	unsigned int ptTypeDouble,
	unsigned int ptTypeFloat,
	unsigned int ptTypeInt) {
  rcvr->pt_dim_2d = ptDim2d;
  rcvr->pt_dim_3d = ptDim3d;
  rcvr->pt_type_double = ptTypeDouble;
  rcvr->pt_type_float = ptTypeFloat;
  rcvr->pt_type_int = ptTypeInt;
}


// for Xgl_rect_ad3d
Xgl_rect_ad3d* xgl_rect_ad3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_ad3d, count);
}
void xgl_rect_ad3d_delete (Xgl_rect_ad3d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_ad3d* xgl_rect_ad3d_at (Xgl_rect_ad3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_ad3d_at_put (Xgl_rect_ad3d* rcvr, fint which, Xgl_rect_ad3d* rectAd3d) {
  rcvr[which] = *rectAd3d;
}

Xgl_pt_d2d* xgl_rect_ad3d_corner_max (Xgl_rect_ad3d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_ad3d_corner_max (Xgl_rect_ad3d* rcvr, Xgl_pt_d2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_d3d* xgl_rect_ad3d_corner_min (Xgl_rect_ad3d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_ad3d_corner_min (Xgl_rect_ad3d* rcvr, Xgl_pt_flag_d3d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}


// for Xgl_rect_af3d
Xgl_rect_af3d* xgl_rect_af3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_af3d, count);
}
void xgl_rect_af3d_delete (Xgl_rect_af3d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_af3d* xgl_rect_af3d_at (Xgl_rect_af3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_af3d_at_put (Xgl_rect_af3d* rcvr, fint which, Xgl_rect_af3d* rectAf3d) {
  rcvr[which] = *rectAf3d;
}

Xgl_pt_f2d* xgl_rect_af3d_corner_max (Xgl_rect_af3d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_af3d_corner_max (Xgl_rect_af3d* rcvr, Xgl_pt_f2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_f3d* xgl_rect_af3d_corner_min (Xgl_rect_af3d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_af3d_corner_min (Xgl_rect_af3d* rcvr, Xgl_pt_flag_f3d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}


// for Xgl_rect_d2d
Xgl_rect_d2d* xgl_rect_d2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_d2d, count);
}
void xgl_rect_d2d_delete (Xgl_rect_d2d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_d2d* xgl_rect_d2d_at (Xgl_rect_d2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_d2d_at_put (Xgl_rect_d2d* rcvr, fint which, Xgl_rect_d2d* rectD2d) {
  rcvr[which] = *rectD2d;
}

Xgl_pt_d2d* xgl_rect_d2d_corner_max (Xgl_rect_d2d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_d2d_corner_max (Xgl_rect_d2d* rcvr, Xgl_pt_d2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_d2d* xgl_rect_d2d_corner_min (Xgl_rect_d2d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_d2d_corner_min (Xgl_rect_d2d* rcvr, Xgl_pt_flag_d2d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}


// for Xgl_rect_d3d
Xgl_rect_d3d* xgl_rect_d3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_d3d, count);
}
void xgl_rect_d3d_delete (Xgl_rect_d3d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_d3d* xgl_rect_d3d_at (Xgl_rect_d3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_d3d_at_put (Xgl_rect_d3d* rcvr, fint which, Xgl_rect_d3d* rectD3d) {
  rcvr[which] = *rectD3d;
}

Xgl_pt_d2d* xgl_rect_d3d_corner_max (Xgl_rect_d3d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_d3d_corner_max (Xgl_rect_d3d* rcvr, Xgl_pt_d2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_d3d* xgl_rect_d3d_corner_min (Xgl_rect_d3d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_d3d_corner_min (Xgl_rect_d3d* rcvr, Xgl_pt_flag_d3d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}

Xgl_pt_d3d* xgl_rect_d3d_dir (Xgl_rect_d3d* rcvr) {
   return rcvr->dir;
}
void xgl_rect_d3d_set (Xgl_rect_d3d* rcvr,
	Xgl_pt_d2d* cornerMax,
	Xgl_pt_flag_d3d* cornerMin,
	bool dirNormal,
	bool dirNormalized) {
  rcvr->corner_max = *cornerMax;
  rcvr->corner_min = *cornerMin;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
}


// for Xgl_rect_f2d
Xgl_rect_f2d* xgl_rect_f2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_f2d, count);
}
void xgl_rect_f2d_delete (Xgl_rect_f2d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_f2d* xgl_rect_f2d_at (Xgl_rect_f2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_f2d_at_put (Xgl_rect_f2d* rcvr, fint which, Xgl_rect_f2d* rectF2d) {
  rcvr[which] = *rectF2d;
}

Xgl_pt_f2d* xgl_rect_f2d_corner_max (Xgl_rect_f2d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_f2d_corner_max (Xgl_rect_f2d* rcvr, Xgl_pt_f2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_f2d* xgl_rect_f2d_corner_min (Xgl_rect_f2d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_f2d_corner_min (Xgl_rect_f2d* rcvr, Xgl_pt_flag_f2d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}


// for Xgl_rect_f3d
Xgl_rect_f3d* xgl_rect_f3d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_f3d, count);
}
void xgl_rect_f3d_delete (Xgl_rect_f3d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_f3d* xgl_rect_f3d_at (Xgl_rect_f3d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_f3d_at_put (Xgl_rect_f3d* rcvr, fint which, Xgl_rect_f3d* rectF3d) {
  rcvr[which] = *rectF3d;
}

Xgl_pt_f2d* xgl_rect_f3d_corner_max (Xgl_rect_f3d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_f3d_corner_max (Xgl_rect_f3d* rcvr, Xgl_pt_f2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_f3d* xgl_rect_f3d_corner_min (Xgl_rect_f3d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_f3d_corner_min (Xgl_rect_f3d* rcvr, Xgl_pt_flag_f3d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}

Xgl_pt_f3d* xgl_rect_f3d_dir (Xgl_rect_f3d* rcvr) {
   return rcvr->dir;
}
void xgl_rect_f3d_set (Xgl_rect_f3d* rcvr,
	Xgl_pt_f2d* cornerMax,
	Xgl_pt_flag_f3d* cornerMin,
	bool dirNormal,
	bool dirNormalized) {
  rcvr->corner_max = *cornerMax;
  rcvr->corner_min = *cornerMin;
  rcvr->dir_normal = dirNormal;
  rcvr->dir_normalized = dirNormalized;
}


// for Xgl_rect_i2d
Xgl_rect_i2d* xgl_rect_i2d_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_i2d, count);
}
void xgl_rect_i2d_delete (Xgl_rect_i2d* rcvr) {
  delete [] rcvr;
}

Xgl_rect_i2d* xgl_rect_i2d_at (Xgl_rect_i2d* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_i2d_at_put (Xgl_rect_i2d* rcvr, fint which, Xgl_rect_i2d* rectI2d) {
  rcvr[which] = *rectI2d;
}

Xgl_pt_i2d* xgl_rect_i2d_corner_max (Xgl_rect_i2d* rcvr) {
   return &(rcvr->corner_max);
}
void xgl_rect_i2d_corner_max (Xgl_rect_i2d* rcvr, Xgl_pt_i2d* cornerMax) {
   rcvr->corner_max = *cornerMax;
}

Xgl_pt_flag_i2d* xgl_rect_i2d_corner_min (Xgl_rect_i2d* rcvr) {
   return &(rcvr->corner_min);
}
void xgl_rect_i2d_corner_min (Xgl_rect_i2d* rcvr, Xgl_pt_flag_i2d* cornerMin) {
   rcvr->corner_min = *cornerMin;
}


// for Xgl_rect_list
Xgl_rect_list* xgl_rect_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_rect_list, count);
}
void xgl_rect_list_delete (Xgl_rect_list* rcvr) {
  delete [] rcvr;
}

Xgl_rect_list* xgl_rect_list_at (Xgl_rect_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_rect_list_at_put (Xgl_rect_list* rcvr, fint which, Xgl_rect_list* rectList) {
  rcvr[which] = *rectList;
}

Xgl_rect_ad3d* xgl_rect_list_rects_ad3d (Xgl_rect_list* rcvr) {
   return rcvr->rects.ad3d;
}
void xgl_rect_list_rects_ad3d (Xgl_rect_list* rcvr, Xgl_rect_ad3d* ad3d) {
   rcvr->rects.ad3d = ad3d;
}

Xgl_rect_af3d* xgl_rect_list_rects_af3d (Xgl_rect_list* rcvr) {
   return rcvr->rects.af3d;
}
void xgl_rect_list_rects_af3d (Xgl_rect_list* rcvr, Xgl_rect_af3d* af3d) {
   rcvr->rects.af3d = af3d;
}

Xgl_rect_d2d* xgl_rect_list_rects_d2d (Xgl_rect_list* rcvr) {
   return rcvr->rects.d2d;
}
void xgl_rect_list_rects_d2d (Xgl_rect_list* rcvr, Xgl_rect_d2d* d2d) {
   rcvr->rects.d2d = d2d;
}

Xgl_rect_d3d* xgl_rect_list_rects_d3d (Xgl_rect_list* rcvr) {
   return rcvr->rects.d3d;
}
void xgl_rect_list_rects_d3d (Xgl_rect_list* rcvr, Xgl_rect_d3d* d3d) {
   rcvr->rects.d3d = d3d;
}

Xgl_rect_f2d* xgl_rect_list_rects_f2d (Xgl_rect_list* rcvr) {
   return rcvr->rects.f2d;
}
void xgl_rect_list_rects_f2d (Xgl_rect_list* rcvr, Xgl_rect_f2d* f2d) {
   rcvr->rects.f2d = f2d;
}

Xgl_rect_f3d* xgl_rect_list_rects_f3d (Xgl_rect_list* rcvr) {
   return rcvr->rects.f3d;
}
void xgl_rect_list_rects_f3d (Xgl_rect_list* rcvr, Xgl_rect_f3d* f3d) {
   rcvr->rects.f3d = f3d;
}

Xgl_rect_i2d* xgl_rect_list_rects_i2d (Xgl_rect_list* rcvr) {
   return rcvr->rects.i2d;
}
void xgl_rect_list_rects_i2d (Xgl_rect_list* rcvr, Xgl_rect_i2d* i2d) {
   rcvr->rects.i2d = i2d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_ad3d* ad3d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.ad3d = ad3d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_af3d* af3d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.af3d = af3d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_d2d* d2d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.d2d = d2d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_d3d* d3d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.d3d = d3d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_f2d* f2d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.f2d = f2d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_f3d* f3d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.f3d = f3d;
}

void xgl_rect_list_set (Xgl_rect_list* rcvr,
	Xgl_bbox* bbox,
	unsigned long numRects,
	Xgl_multirect_type rectType,
	Xgl_rect_i2d* i2d) {
  rcvr->bbox = bbox;
  rcvr->num_rects = numRects;
  rcvr->rect_type = rectType;
  rcvr->rects.i2d = i2d;
}


// for Xgl_render_component_desc
Xgl_render_component_desc* xgl_render_component_desc_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_render_component_desc, count);
}
void xgl_render_component_desc_delete (Xgl_render_component_desc* rcvr) {
  delete [] rcvr;
}

Xgl_render_component_desc* xgl_render_component_desc_at (Xgl_render_component_desc* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_render_component_desc_at_put (Xgl_render_component_desc* rcvr, fint which, Xgl_render_component_desc* renderComponentDesc) {
  rcvr[which] = *renderComponentDesc;
}

Xgl_texture_blend_rgb* xgl_render_component_desc_op_blend_rgb (Xgl_render_component_desc* rcvr) {
   return &(rcvr->op.blend.rgb);
}
void xgl_render_component_desc_op_blend_rgb (Xgl_render_component_desc* rcvr, Xgl_texture_blend_rgb* rgb) {
   rcvr->op.blend.rgb = *rgb;
}

Xgl_texture_decal_rgb* xgl_render_component_desc_op_decal_rgb (Xgl_render_component_desc* rcvr) {
   return &(rcvr->op.decal.rgb);
}
void xgl_render_component_desc_op_decal_rgb (Xgl_render_component_desc* rcvr, Xgl_texture_decal_rgb* rgb) {
   rcvr->op.decal.rgb = *rgb;
}

void xgl_render_component_desc_set (Xgl_render_component_desc* rcvr,
	Xgl_render_component comp,
	Xgl_texture_blend_rgb* blend_rgb,
	Xgl_texture_op textureOp) {
  rcvr->comp = comp;
  rcvr->op.blend.rgb = *blend_rgb;
  rcvr->texture_op = textureOp;
}

void xgl_render_component_desc_set (Xgl_render_component_desc* rcvr,
	Xgl_render_component comp,
	Xgl_texture_decal_rgb* decal_rgb,
	Xgl_texture_op textureOp) {
  rcvr->comp = comp;
  rcvr->op.decal.rgb = *decal_rgb;
  rcvr->texture_op = textureOp;
}


// for Xgl_segment
Xgl_segment* xgl_segment_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_segment, count);
}
void xgl_segment_delete (Xgl_segment* rcvr) {
  delete [] rcvr;
}

Xgl_segment* xgl_segment_at (Xgl_segment* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_segment_at_put (Xgl_segment* rcvr, fint which, Xgl_segment* segment) {
  rcvr[which] = *segment;
}


// for Xgl_spline_data
Xgl_spline_data* xgl_spline_data_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_spline_data, count);
}
void xgl_spline_data_delete (Xgl_spline_data* rcvr) {
  delete [] rcvr;
}

Xgl_spline_data* xgl_spline_data_at (Xgl_spline_data* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_spline_data_at_put (Xgl_spline_data* rcvr, fint which, Xgl_spline_data* splineData) {
  rcvr[which] = *splineData;
}

float* xgl_spline_data_data (Xgl_spline_data* rcvr) {
   return rcvr->data;
}

// for Xgl_surf_color_spline
Xgl_surf_color_spline* xgl_surf_color_spline_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_surf_color_spline, count);
}
void xgl_surf_color_spline_delete (Xgl_surf_color_spline* rcvr) {
  delete [] rcvr;
}

Xgl_surf_color_spline* xgl_surf_color_spline_at (Xgl_surf_color_spline* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_surf_color_spline_at_put (Xgl_surf_color_spline* rcvr, fint which, Xgl_surf_color_spline* surfColorSpline) {
  rcvr[which] = *surfColorSpline;
}

void xgl_surf_color_spline_set (Xgl_surf_color_spline* rcvr,
	Xgl_color_homogeneous* colors,
	float* knotVectorU,
	float* knotVectorV,
	unsigned long numKnotsU,
	unsigned long numKnotsV,
	unsigned long orderU,
	unsigned long orderV) {
  rcvr->colors = colors;
  rcvr->knot_vector_u = knotVectorU;
  rcvr->knot_vector_v = knotVectorV;
  rcvr->num_knots_u = numKnotsU;
  rcvr->num_knots_v = numKnotsV;
  rcvr->order_u = orderU;
  rcvr->order_v = orderV;
}


// for Xgl_surf_data_spline
Xgl_surf_data_spline* xgl_surf_data_spline_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_surf_data_spline, count);
}
void xgl_surf_data_spline_delete (Xgl_surf_data_spline* rcvr) {
  delete [] rcvr;
}

Xgl_surf_data_spline* xgl_surf_data_spline_at (Xgl_surf_data_spline* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_surf_data_spline_at_put (Xgl_surf_data_spline* rcvr, fint which, Xgl_surf_data_spline* surfDataSpline) {
  rcvr[which] = *surfDataSpline;
}

void xgl_surf_data_spline_set (Xgl_surf_data_spline* rcvr,
	Xgl_spline_data* data,
	float* knotVectorU,
	float* knotVectorV,
	unsigned long numKnotsU,
	unsigned long numKnotsV,
	unsigned long orderU,
	unsigned long orderV) {
  rcvr->data = data;
  rcvr->knot_vector_u = knotVectorU;
  rcvr->knot_vector_v = knotVectorV;
  rcvr->num_knots_u = numKnotsU;
  rcvr->num_knots_v = numKnotsV;
  rcvr->order_u = orderU;
  rcvr->order_v = orderV;
}


// for Xgl_surf_data_spline_list
Xgl_surf_data_spline_list* xgl_surf_data_spline_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_surf_data_spline_list, count);
}
void xgl_surf_data_spline_list_delete (Xgl_surf_data_spline_list* rcvr) {
  delete [] rcvr;
}

Xgl_surf_data_spline_list* xgl_surf_data_spline_list_at (Xgl_surf_data_spline_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_surf_data_spline_list_at_put (Xgl_surf_data_spline_list* rcvr, fint which, Xgl_surf_data_spline_list* surfDataSplineList) {
  rcvr[which] = *surfDataSplineList;
}


// for Xgl_texture_blend_rgb
Xgl_texture_blend_rgb* xgl_texture_blend_rgb_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_blend_rgb, count);
}
void xgl_texture_blend_rgb_delete (Xgl_texture_blend_rgb* rcvr) {
  delete [] rcvr;
}

Xgl_texture_blend_rgb* xgl_texture_blend_rgb_at (Xgl_texture_blend_rgb* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_blend_rgb_at_put (Xgl_texture_blend_rgb* rcvr, fint which, Xgl_texture_blend_rgb* textureBlendRgb) {
  rcvr[which] = *textureBlendRgb;
}

Xgl_color_rgb* xgl_texture_blend_rgb_c1 (Xgl_texture_blend_rgb* rcvr) {
   return &(rcvr->c1);
}
void xgl_texture_blend_rgb_c1 (Xgl_texture_blend_rgb* rcvr, Xgl_color_rgb* c1) {
   rcvr->c1 = *c1;
}

Xgl_color_rgb* xgl_texture_blend_rgb_c2 (Xgl_texture_blend_rgb* rcvr) {
   return &(rcvr->c2);
}
void xgl_texture_blend_rgb_c2 (Xgl_texture_blend_rgb* rcvr, Xgl_color_rgb* c2) {
   rcvr->c2 = *c2;
}


// for Xgl_texture_color_comp_info
Xgl_texture_color_comp_info* xgl_texture_color_comp_info_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_color_comp_info, count);
}
void xgl_texture_color_comp_info_delete (Xgl_texture_color_comp_info* rcvr) {
  delete [] rcvr;
}

Xgl_texture_color_comp_info* xgl_texture_color_comp_info_at (Xgl_texture_color_comp_info* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_color_comp_info_at_put (Xgl_texture_color_comp_info* rcvr, fint which, Xgl_texture_color_comp_info* textureColorCompInfo) {
  rcvr[which] = *textureColorCompInfo;
}

Xgl_render_component_desc* xgl_texture_color_comp_info_render_component_desc (Xgl_texture_color_comp_info* rcvr) {
   return rcvr->render_component_desc;
}

// for Xgl_texture_decal_rgb
Xgl_texture_decal_rgb* xgl_texture_decal_rgb_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_decal_rgb, count);
}
void xgl_texture_decal_rgb_delete (Xgl_texture_decal_rgb* rcvr) {
  delete [] rcvr;
}

Xgl_texture_decal_rgb* xgl_texture_decal_rgb_at (Xgl_texture_decal_rgb* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_decal_rgb_at_put (Xgl_texture_decal_rgb* rcvr, fint which, Xgl_texture_decal_rgb* textureDecalRgb) {
  rcvr[which] = *textureDecalRgb;
}

Xgl_color_rgb* xgl_texture_decal_rgb_c (Xgl_texture_decal_rgb* rcvr) {
   return &(rcvr->c);
}
void xgl_texture_decal_rgb_c (Xgl_texture_decal_rgb* rcvr, Xgl_color_rgb* c) {
   rcvr->c = *c;
}


// for Xgl_texture_desc
Xgl_texture_desc* xgl_texture_desc_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_desc, count);
}
void xgl_texture_desc_delete (Xgl_texture_desc* rcvr) {
  delete [] rcvr;
}

Xgl_texture_desc* xgl_texture_desc_at (Xgl_texture_desc* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_desc_at_put (Xgl_texture_desc* rcvr, fint which, Xgl_texture_desc* textureDesc) {
  rcvr[which] = *textureDesc;
}

Xgl_texture_color_comp_info* xgl_texture_desc_comp_info (Xgl_texture_desc* rcvr) {
   return &(rcvr->comp_info);
}
void xgl_texture_desc_comp_info (Xgl_texture_desc* rcvr, Xgl_texture_color_comp_info* compInfo) {
   rcvr->comp_info = *compInfo;
}

Xgl_texture_mipmap_desc* xgl_texture_desc_info_mipmap (Xgl_texture_desc* rcvr) {
   return &(rcvr->info.mipmap);
}
void xgl_texture_desc_info_mipmap (Xgl_texture_desc* rcvr, Xgl_texture_mipmap_desc* mipmap) {
   rcvr->info.mipmap = *mipmap;
}

Xgl_texture_interp_info* xgl_texture_desc_interp_info (Xgl_texture_desc* rcvr) {
   return &(rcvr->interp_info);
}
void xgl_texture_desc_interp_info (Xgl_texture_desc* rcvr, Xgl_texture_interp_info* interpInfo) {
   rcvr->interp_info = *interpInfo;
}

void xgl_texture_desc_set (Xgl_texture_desc* rcvr,
	Xgl_texture_color_comp_info* compInfo,
	Xgl_texture_mipmap_desc* info_mipmap,
	Xgl_texture_interp_info* interpInfo,
	Xgl_texture_type textureType) {
  rcvr->comp_info = *compInfo;
  rcvr->info.mipmap = *info_mipmap;
  rcvr->interp_info = *interpInfo;
  rcvr->texture_type = textureType;
}


// for Xgl_texture_interp_info
Xgl_texture_interp_info* xgl_texture_interp_info_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_interp_info, count);
}
void xgl_texture_interp_info_delete (Xgl_texture_interp_info* rcvr) {
  delete [] rcvr;
}

Xgl_texture_interp_info* xgl_texture_interp_info_at (Xgl_texture_interp_info* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_interp_info_at_put (Xgl_texture_interp_info* rcvr, fint which, Xgl_texture_interp_info* textureInterpInfo) {
  rcvr[which] = *textureInterpInfo;
}

void xgl_texture_interp_info_set (Xgl_texture_interp_info* rcvr,
	Xgl_texture_interp_method filter1,
	Xgl_texture_interp_method filter2) {
  rcvr->filter1 = filter1;
  rcvr->filter2 = filter2;
}


// for Xgl_texture_mipmap_desc
Xgl_texture_mipmap_desc* xgl_texture_mipmap_desc_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_texture_mipmap_desc, count);
}
void xgl_texture_mipmap_desc_delete (Xgl_texture_mipmap_desc* rcvr) {
  delete [] rcvr;
}

Xgl_texture_mipmap_desc* xgl_texture_mipmap_desc_at (Xgl_texture_mipmap_desc* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_texture_mipmap_desc_at_put (Xgl_texture_mipmap_desc* rcvr, fint which, Xgl_texture_mipmap_desc* textureMipmapDesc) {
  rcvr[which] = *textureMipmapDesc;
}

unsigned char* xgl_texture_mipmap_desc_boundary_values (Xgl_texture_mipmap_desc* rcvr) {
   return rcvr->boundary_values;
}
void xgl_texture_mipmap_desc_set (Xgl_texture_mipmap_desc* rcvr,
	float depthInterpFactor,
	float maxUFreq,
	float maxVFreq,
	Xgl_object_obj* textureMap,
	Xgl_texture_boundary uBoundary,
	Xgl_texture_boundary vBoundary) {
  rcvr->depth_interp_factor = depthInterpFactor;
  rcvr->max_u_freq = maxUFreq;
  rcvr->max_v_freq = maxVFreq;
  rcvr->texture_map = textureMap;
  rcvr->u_boundary = uBoundary;
  rcvr->v_boundary = vBoundary;
}


// for Xgl_threshold
Xgl_threshold* xgl_threshold_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_threshold, count);
}
void xgl_threshold_delete (Xgl_threshold* rcvr) {
  delete [] rcvr;
}

Xgl_threshold* xgl_threshold_at (Xgl_threshold* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_threshold_at_put (Xgl_threshold* rcvr, fint which, Xgl_threshold* threshold) {
  rcvr[which] = *threshold;
}

void xgl_threshold_set (Xgl_threshold* rcvr,
	unsigned long circles,
	unsigned long polygons,
	unsigned long vectors) {
  rcvr->circles = circles;
  rcvr->polygons = polygons;
  rcvr->vectors = vectors;
}


// for Xgl_trim_curve
Xgl_trim_curve* xgl_trim_curve_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_trim_curve, count);
}
void xgl_trim_curve_delete (Xgl_trim_curve* rcvr) {
  delete [] rcvr;
}

Xgl_trim_curve* xgl_trim_curve_at (Xgl_trim_curve* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_trim_curve_at_put (Xgl_trim_curve* rcvr, fint which, Xgl_trim_curve* trimCurve) {
  rcvr[which] = *trimCurve;
}

Xgl_pt_list* xgl_trim_curve_ctrl_pts (Xgl_trim_curve* rcvr) {
   return &(rcvr->ctrl_pts);
}
void xgl_trim_curve_ctrl_pts (Xgl_trim_curve* rcvr, Xgl_pt_list* ctrlPts) {
   rcvr->ctrl_pts = *ctrlPts;
}

Xgl_bounds_f1d* xgl_trim_curve_range (Xgl_trim_curve* rcvr) {
   return &(rcvr->range);
}
void xgl_trim_curve_range (Xgl_trim_curve* rcvr, Xgl_bounds_f1d* range) {
   rcvr->range = *range;
}


// for Xgl_trim_loop
Xgl_trim_loop* xgl_trim_loop_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_trim_loop, count);
}
void xgl_trim_loop_delete (Xgl_trim_loop* rcvr) {
  delete [] rcvr;
}

Xgl_trim_loop* xgl_trim_loop_at (Xgl_trim_loop* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_trim_loop_at_put (Xgl_trim_loop* rcvr, fint which, Xgl_trim_loop* trimLoop) {
  rcvr[which] = *trimLoop;
}


// for Xgl_trim_loop_list
Xgl_trim_loop_list* xgl_trim_loop_list_new (fint count) {
  return NEW_C_HEAP_ARRAY(Xgl_trim_loop_list, count);
}
void xgl_trim_loop_list_delete (Xgl_trim_loop_list* rcvr) {
  delete [] rcvr;
}

Xgl_trim_loop_list* xgl_trim_loop_list_at (Xgl_trim_loop_list* rcvr, fint which) {
  return &(rcvr[which]);
}
void xgl_trim_loop_list_at_put (Xgl_trim_loop_list* rcvr, fint which, Xgl_trim_loop_list* trimLoopList) {
  rcvr[which] = *trimLoopList;
}


// accessors for c-style vector proxies
// for Int_vec_proxy
int* int_vec_proxy_new (fint count) {
  return new int[count];
}
void int_vec_proxy_delete (int* intVec) {
  delete [] intVec;
}

int int_vec_proxy_at (int* intVec, fint which) {
  return intVec[which];
}
void int_vec_proxy_at_put (int* intVec, fint which, int data) {	
    intVec[which] = data;
}


// for Long_vec_proxy
long* long_vec_proxy_new (fint count) {
  return new long[count];
}
void long_vec_proxy_delete (long* longVec) {
  delete [] longVec;
}

long long_vec_proxy_at (long* longVec, fint which) {
  return longVec[which];
}
void long_vec_proxy_at_put (long* longVec, fint which, long data) {	
    longVec[which] = data;
}


// for Float_vec_proxy
float* float_vec_proxy_new (fint count) {
  return new float[count];
}
void float_vec_proxy_delete (float* floatVec) {
  delete [] floatVec;
}

float float_vec_proxy_at (float* floatVec, fint which) {
  return floatVec[which];
}
void float_vec_proxy_at_put (float* floatVec, fint which, float data) {	
    floatVec[which] = data;
}


// for Double_vec_proxy
double* double_vec_proxy_new (fint count) {
  return new double[count];
}
void double_vec_proxy_delete (double* doubleVec) {
  delete [] doubleVec;
}

double double_vec_proxy_at (double* doubleVec, fint which) {
  return doubleVec[which];
}
void double_vec_proxy_at_put (double* doubleVec, fint which, double data) {	
    doubleVec[which] = data;
}


// for Unsigned_char_vec_proxy
unsigned char* unsigned_char_vec_proxy_new (fint count) {
  return new unsigned char[count];
}
void unsigned_char_vec_proxy_delete (unsigned char* unsignedCharVec) {
  delete [] unsignedCharVec;
}

unsigned char unsigned_char_vec_proxy_at (unsigned char* unsignedCharVec, fint which) {
  return unsignedCharVec[which];
}
void unsigned_char_vec_proxy_at_put (unsigned char* unsignedCharVec, fint which, unsigned char data) {	
    unsignedCharVec[which] = data;
}


// for Xgl_objects
Xgl_object* xgl_object_new (fint count) {
  return new Xgl_object[count];
}
void xgl_object_delete (Xgl_object* objectVec, fint count) {
  for (int i = 0; i < count; i++) xgl_object_destroy(objectVec[i]);
}

Xgl_object xgl_object_at (Xgl_object* objectVec, fint which) {
  return objectVec[which];
}
void xgl_object_at_put (Xgl_object* objectVec, fint which, Xgl_object object) {	
    objectVec[which] = object;
}


// This is a silly wrapper - but I hate having to pass a trailing 0.
Xgl_sys_state xgl_open_no_args () { return xgl_open(XGL_UNUSED); }


#define WHAT_GLUE FUNCTIONS
   xgl_glue
#undef WHAT_GLUE
/* Sun-$Revision: 30.7 $ */

/* Copyright 1992-2012 AUTHORS.
   See the LICENSE file for license information. */

# include "_glueDefs.cpp.incl"
# include "termcap.primMaker.hh"

//# define WHAT_GLUE C_DECLS
//    termcap_glue
//# undef WHAT_GLUE

#ifdef DYNAMIC
VERIFYCHECKSUM
#endif

// Interface to termcap
extern "C" {
int   tgetnum(char *id);
int   tgetflag(char *id);
int   tgetent(char *bp, char *name);
char *tgetstr(char id[2], char **area);
char *tgoto(char *cap, int col, int row);
}

static char termcap_buffer[1024];
static char termcap_decode_area[100];

char *tgetenv_wrapper(char *name, void *FH) {
  if( tgetent( termcap_buffer, name) != 1 ) unix_failure(FH);
  return termcap_buffer;
}

char *tgetstr_wrapper(char *id, void *FH) {
  char *area = termcap_decode_area;
  if( tgetstr( id, &area) == 0 ) unix_failure(FH);
  return termcap_decode_area;
}

char *tgoto_wrapper(char *cm, int destcol, int destline, void *FH) {
  char *goto_string;
  if( (goto_string = tgoto( cm, destcol, destline)) == 0 ) unix_failure(FH);
  return goto_string;
}

# define WHAT_GLUE FUNCTIONS
    termcap_glue
# undef WHAT_GLUE
/* 
$Revision: 1.1 $

A proxy server for debugging.

This server serves requests from remote clients
that specify debugging calls to the servers local OS environment.

Addressing:          server host IP, well-known port (see # define below)
Transport:           TCP sockets
Transmission format: proprietary


*/

# include "kleinDebugServer.hh"

# if defined(LLVM)
  # include "llvmDebugServer.hh"
# elif defined(__APPLE__)
  # include "machDebugServer.hh"
# elif defined(__linux__)
  # include "linuxDebugServer.hh"
# endif

# include "smallSelfDebugServer.hh"


const char* RequestTypes[] = {
  "zero",
  "terminate",
  "ptrace",
  "ping",
  "miniping",
  "waitStatus",
  "signal",
  "getReturnHandler",
  "getBaseAndLength",
  "setVMOop"
//  "startInterpreter"
};

const char* string_for_request_type(request_type_t t) {
  return (unsigned int)t  < sizeof(RequestTypes)/sizeof(RequestTypes[0])   ?  RequestTypes[t]  :  NULL;
}


// Debug flags:
//  Printing:
bool verbose             = false,
     print_request_types = false,
     print_children      = false;
    

// Debugging:
    

bool shift_base = false;
bool isClient    = false; // for testing
int  client_reps = 100;   // for testing


int BaseAndLengthGetter::fixed_address_buffer[fixed_address_buffer_length / sizeof(int)];

void breakpoint() {
   //since the debug server spawns child processes to do its work,
   //setting breakpoings inside of it is generally speaking not useful -> this 
   //breakpoint dumps out the pid of the child process. Use gdb to attach to it and debug.
   // - Ausch & Ungar, Nov/04
   printf_and_flush("at breakpoint");
   while(1);
 }


class ServerSocketOpener {
 public:
  void open_on_port(int p) {
    port = p;
    open_socket();
    make_socket_reusable();
    bind_socket();
    listen_for_connection();
  }
  
  int serverSocketFd() { return fd; }
    
 private:
  struct sockaddr_in serverAddress;
  int fd;
  int port;

  void open_socket() {
    fd = socket(AF_INET, SOCK_STREAM, 0);
    if (fd < 0)  { perror("socket() failed"); exit(1); }
    if (verbose) printf_and_flush("opened server socket\n");
  }
  
  // Avoids the problem of not being able to reopen socket right away
  // after a crash. Original by Jake, revised by dmu, 12/03.
  void make_socket_reusable() {
    int opt_val = true;
    if ( setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, (void *)&opt_val, sizeof(opt_val)) < 0) {
        perror("SERVER WARNING: Could not make socket reusable.\n");
     }
     else if (verbose) printf_and_flush("make socket reusable\n");
  }
  
  void bind_socket() {
     bzero(&serverAddress, sizeof(serverAddress));
     serverAddress.sin_family = AF_INET;
     serverAddress.sin_port = htons(port);
     serverAddress.sin_addr.s_addr = htonl(INADDR_ANY);
	 
     if (bind(fd, (struct sockaddr *) &serverAddress, sizeof(serverAddress)) < 0) {
        char buf[1024];
        sprintf(buf, "SERVER ERROR: Could not bind server socket.\n"
                     "Is port %d already in use?\n", port);
        perror(buf);
        exit(1);
       
        if (verbose) printf_and_flush("bound socket\n");
     }
  }
  
  void listen_for_connection() {
    if (listen(fd, /*backlog*/ 100) < 0) {
        perror("SERVER ERROR: listen() failed");
        exit(2);  
    }
   if (verbose) printf_and_flush("listening on port %d\n", port);
  }
};


class ClientSocketOpener {

 public:
  int open_on_port(const char* hostName, int port) {
    get_host_entry(hostName);
    open_socket();
    connect_socket(port);
    return socket_fd;
  }
 private:
  struct hostent* host_entry;
  int socket_fd;
  
  void get_host_entry(const char* hostName) {
    host_entry = gethostbyname(hostName);
    if (host_entry == NULL) {
      perror("gethostbyname");
      error_printf_and_flush( "get_host_entry: gethostbyname failed host %s, h_errno %d",
              hostName, h_errno);
      exit(1);
    }
  }

  void open_socket() {
    socket_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (socket_fd == -1) {
      perror("socket");
      error_printf_and_flush( "open_socket: " "socket failed, errno = %d", errno);
      exit(1);
    }
  }

  void connect_socket(int port) {
    struct sockaddr_in a;
    a.sin_family = AF_INET;
    a.sin_port = htons(port);
    long aLong;
    memcpy((char*)&aLong, host_entry->h_addr_list[0], sizeof(aLong));
    memset(a.sin_zero, 0, sizeof(a.sin_zero));
    a.sin_addr.s_addr = htonl(aLong);
    int res = connect(socket_fd, (struct sockaddr*)&a, sizeof(a));
    if (res) {
      perror("connect");
      error_printf_and_flush( "connect_socket: " "connect failed: errno = %d", errno);
      exit(1);
    }
  }
};



class ConnectionAcceptor {
public:
  ConnectionAcceptor(int fd) {
    serverSocketFd = fd;
    clientSocketFd = -1;
  }

  // initiate connection; fork, return -1 if parent, clientSocketFd if child

  int accept_and_fork() {
    for (;;) {
      collect_zombies();
      accept_a_connection();
      if (clientSocketFd < 0)
        continue;
        
      int childPid = fork();
      if (childPid == 0)
        break; // is child
      else if (childPid < 0)
        perror("SERVER WARNING: service creation (fork) failed");
      else if (print_children)
        printf_and_flush("forked child, pid = %d\n", childPid);
      close(clientSocketFd);
    }
    /*child*/
    close(serverSocketFd);
    if (verbose  ||  print_children)
      printf_and_flush("child returning %d\n", clientSocketFd);
    setpgid(0, getpid());
    return clientSocketFd;
 }
          
 private:
  int serverSocketFd;
  int clientSocketFd;

  static void collect_zombies() {
    int status;
    if (verbose) printf_and_flush("collecting zomebies\n");
    for (int i = 0;  i < 10;  ++i) {
       pid_t child_pid = waitpid(-1, &status, WNOHANG);
       if (!print_children)
         ;
       else if (child_pid == 0)
         ; // none
       else if (child_pid == -1)
         ; // error
       else
         printf_and_flush("collected child %d\n", child_pid);
    }
    if (verbose) perror("waitpid");
  }

  void accept_a_connection() {
    struct sockaddr clientAddress;
    socklen_t nAddressBytes = sizeof(clientAddress);
    if (verbose)   printf_and_flush("accepting\n");
    clientSocketFd = accept(serverSocketFd,  &clientAddress, &nAddressBytes);
         if (clientSocketFd < 0)  perror("SERVER WARNING: client connect failed"); 
    else if (verbose)             printf_and_flush("accepted connection\n");
  }
};


class ServerInitiator: public SocketUser {
 private:

  // For SPARC, was: static const char arch[] = "sparc";
  const char*  arch;
  char   error_or_null[1000];

  void set_arch() {
    #ifdef LLVM
    arch = "llvm";
    #else
    static struct utsname my_utsname;
    if ( uname(&my_utsname) ) {
      perror("set_arch: uname");
      exit(1);
    }
    arch = my_utsname.machine;
    if (0 == strcmp( arch, "Power Macintosh") )
      arch = "ppc";
    if (verbose) printf_and_flush("set_arch() setting arch to: %s\n", arch);
    #endif
  }

  char* read_request(int& len) { return s.read_bytes(len, "receiving initiation request"); }    
  

 public:
   ServerInitiator(BufferedSocket& ss) : SocketUser(ss) {set_arch(); }
    /* 
      How initiation works:

      Client sends INITIATION_REQUEST as ('\0'-terminated) string.
      Server responds with INITIATION_RESPONSE plus architecture string and my pid.
    */
    
  void initiate() {
    if (verbose) printf_and_flush("initiating\n"); 
    int len = -1;
    char* req = read_request(len);
    validate_request(req, len);
    delete req;
    send_response();
    send_arch();
    send_pid();
    s.flush_output();

    if (error_or_null[0])  exit(1);
  }
      
    
 private:

  
  void validate_request(char* req, int len) {
    error_or_null[0] = '\0';
    
    if (len != (int)strlen(initiation_request)) 
      sprintf(error_or_null, "Client Error: invalid initiation request length: "
                              "should have been: %lu, was %d",
                              strlen(initiation_request), len);

    else if (strncmp(req, initiation_request, len) != 0)
      sprintf(error_or_null, "Client Error: invalid initiation request: should have been: %s, was %s",
                              initiation_request, req);
                              
   if (strlen(error_or_null) >= sizeof(error_or_null))  error_printf_and_flush("oops");

   if (error_or_null[0])
      error_printf_and_flush( "validate_request: %s", error_or_null);
   else if ( verbose )
      printf_and_flush("accepted initiation request\n");
  }    
  void send_response() { s.write_string(error_or_null[0] ? error_or_null : initiation_response,  "sending initiation response"); }
  void send_arch()     { s.write_string(arch,  "sending arch"); }
  void send_pid()      { s.write_int(getpid(), "sending pid"); }
};



class ClientInitiator: public SocketUser {
public:
  ClientInitiator(BufferedSocket& ss) : SocketUser(ss) {}
  /*
  How initiation works:
         Client sends INITIATION_REQUEST as ('\0'-terminated) string.
   Server responds with INITIATION_RESPONSE plus architecture string and my pid.
   */
  void initiate() {
    if (verbose) printf_and_flush("initiating\n");
    send_request();
    s.flush_output();
    read_response();
    read_arch();
    read_pid();
  }

private:
  void send_request() {
    s.write_string(initiation_request, "sending initiation request");
  }
  void read_response() {
    char* resp = s.read_string("getting initiation response");
    printf_and_flush("resp = %s\n", resp);
    delete resp;
  }
  void read_arch() {
    char* arch = s.read_string("getting arch");
    printf_and_flush("arch = %s\n", arch);
    delete arch;
  }
  void read_pid()      { s.read_int("getting pid"); }
};




class RequestServer: public SocketUser {
public:
  RequestServer(BufferedSocket& ss) : SocketUser(ss) {}
    
  bool serve_requests() {
      // return false when no more to serve
      read_request_type();
      if (verbose) printf_and_flush("servicing request type: %d\n", request_type);
      if (print_request_types && string_for_request_type(request_type))
        printf_and_flush("request type: %s  pid: %d...\n",
         string_for_request_type(request_type), getpid());

      bool r = false;
      switch (request_type) {
        case request_terminate:                                                                              break;
        case request_ptrace:                        { PTracer                x(s);  x.do_it(); }  r = true;  break;
        case request_ping:                          { Pinger                 x(s);  x.do_it(); }  r = true;  break;
        case request_miniping:                      { MiniPinger             x(s);  x.do_it(); }  r = true;  break;
        case request_waitStatus:                    { Waiter                 x(s);  x.do_it(); }  r = true;  break;
        case request_signal:                        { Signaller              x(s);  x.do_it(); }  r = true;  break;
        case request_getReturnHandler:              { ReturnHandlerGetter    x(s);  x.do_it(); }  r = true;  break;
        case request_getBaseAndLength:              { BaseAndLengthGetter    x(s);  x.do_it(); }  r = true;  break;
        
        #ifndef LLVM
        case request_mach:                          { MachRequestServer      x(s);  r = x.do_it(); break; }
        case request_linux:		            { LinuxRequestServer     x(s);  r = x.do_it(); break; }
        #endif
        case request_smallSelf:		            { SmallSelfRequestServer x(s);  r = x.do_it(); break; }

        
        default:  
        error_printf_and_flush( "serve_requests: " "bad request type: %d,  pid: %d", request_type, getpid()); 
        exit(1);
        return false; /* for compiler */
      }
      s.flush_output();
      if (print_request_types && string_for_request_type(request_type))
       printf_and_flush("request type: %s, pid: %d done\n",
        string_for_request_type(request_type), getpid());

      return r;
    }

 private:
  request_type_t request_type;

  void read_request_type() { request_type = (request_type_t)s.read_byte("reading request type"); }
};




class RequestSender: public SocketUser {
public:
  RequestSender(BufferedSocket& ss) : SocketUser(ss) {}
  void send_requests() {
    for (int i = 0; i < client_reps; ++i) {
      send_ping(); if (verbose) printf_and_flush("%d\n", i);
    }
  }
 private:
  void send_ping() {
    send_request_type(request_ping);     if (verbose)  printf_and_flush("sent rt\n");
    s.write_int(17, "fake pid");         if (verbose)  printf_and_flush("sent pid\n");
    s.flush_output();
    s.read_int("reading result");        if (verbose)  printf_and_flush("read res\n");
    char* err = s.read_string("error");  if (verbose)  printf_and_flush("read err\n");
    if (err[0]) {
      error_printf_and_flush( "ping failed: %s", err);
      exit(1);
    }
    delete err;
  }
  void send_miniping() {
    send_request_type(request_miniping);   if (verbose) printf_and_flush("sent rt\n");
    s.flush_output();
    s.read_int("reading result");       if (verbose)  printf_and_flush("read res\n");
  }
  void send_request_type(request_type_t rt) { s.write_byte(rt, "sending request type"); }
};


void error_printf_and_flush(const char* msg , ...) {
  // Darned child process printf_and_flush doesn't work without the flush. -- dmu, jp 12/03
  va_list ap;
  va_start(ap, msg);
  fprintf(stderr, "kleinDebugServer[%d]: ", getpid());
  vfprintf(stderr, msg, ap);
  va_end(ap);
  fprintf(stderr, "\n");
  fflush(stderr);
}


void printf_and_flush(const char* msg , ...) {
  // Darned child process printf_and_flush doesn't work without the flush. -- dmu, jp 12/03
  va_list ap;
  va_start(ap, msg);
  printf ("[%d]: ", getpid());
  vprintf(msg, ap);
  va_end(ap);
  fflush(stdout);
}


void process_arguments(int argc, char *argv[], int& serverPort) {
  char* commandName = argv[0];
  for (;;) {
    int nargs = argc;
    if (argc > 1   &&  strcmp(argv[1], "-v") == 0) {
      verbose = true;
      --argc, ++argv;
    }
    if (argc > 1   &&  strcmp(argv[1], "-printRequestTypes") == 0) {
      print_request_types = true;
      --argc, ++argv;
    }
    if (argc > 1   &&  strcmp(argv[1], "-printChildren") == 0) {
      print_children = true;
      --argc, ++argv;
    }
    if (argc > 1   &&  strcmp(argv[1], "-printBufferLength") == 0) {
      printf_and_flush( "%d\n", fixed_address_buffer_length );
      --argc, ++argv;
    }
    if (argc > 1   &&  strcmp(argv[1], "-shiftBase") == 0) {
      shift_base = true;
      --argc, ++argv;
    }
    if (argc > 1   &&  strcmp(argv[1], "-c") == 0) {
      isClient = true;
      --argc, ++argv;
      if (argc > 1  &&  isdigit(argv[1][0])) {
        client_reps = atoi(argv[1]);
        --argc, ++argv;
      }
    }
    if (argc > 2  &&  strcmp(argv[1], "-p") == 0) {
      serverPort = atoi(argv[2]);
      argc -= 2;  argv += 2;
      if (verbose) printf_and_flush("setting serverPort to %d\n", serverPort);
    }
    if (argc > 1   &&  strcmp(argv[1], "-runAllTests") == 0) {
      runAllTests();
      --argc, ++argv;
    }
    if (nargs == argc)
      break;
  }
  if (argc > 1) {
    printf_and_flush("usage: %s [-c [client_reps]] [-v] [-printRequestTypes] [-printChildren] [-printBufferLength] [-runAllTests] "
                     "[-p portNum]\n",
                     commandName);
    exit(1);
  }
}


char* build_errno_status(int e, const char* why ) {
  if (e == 0) {
    char* empty = new char[1];
    empty[0] = '\0';
    return empty;
  }
  int whylen = strlen(why);
  char sep[] = ": ";
  int seplen = strlen(sep);
  char* err = strerror(e);
  int errlen = strlen(err);
  char* buf = new char[whylen + seplen + errlen + 1];
  strcpy(buf, why);
  strcpy(buf + whylen,  sep);
  strcpy(buf + whylen + seplen, err);
  return buf;
}

int doServer(int serverPort) {
  ServerSocketOpener so;
  so.open_on_port(serverPort);
  ConnectionAcceptor ca(so.serverSocketFd());
  int clientSocketFd = ca.accept_and_fork();
  // At this point, I am the child process
  BufferedSocket s(clientSocketFd);
  ServerInitiator in(s);
  in.initiate();
  RequestServer rs(s);
  while (rs.serve_requests()) {}
  return 0;
}


int doClient(int serverPort) {
  // for performance testing; a simple C client
  ClientSocketOpener co;
  printf_and_flush("opening client\n");
  int fd = co.open_on_port("localhost", serverPort);
  BufferedSocket s(fd);
  printf_and_flush("initiating client\n");
  ClientInitiator ci(s); 
  ci.initiate();
  RequestSender rs(s);
  printf_and_flush("sending requests\n");
  rs.send_requests();
  return 0;
}


void check_stdios() {
  for (int fd = 0;  fd <= 2;  ++fd) {
    int r = fcntl(fd, F_GETFL, 0);
    if (r == -1)  fprintf(stderr, "Warning: fcntl failed, fd: %d\n", fd);
    else if (r & (FASYNC | FNONBLOCK))  
      fprintf(stderr, "Warning: fd %d is in async or nonblocking mode.\nThis may interfere with running Klein.\n"
                      "Run kleinDebugServer in its own terminal window to fix this.\n", fd);
  }                      
}



int main(int argc, char* argv[]) {
    check_stdios();
  
    int serverPort = default_port;
    process_arguments(argc, argv, serverPort);

    return isClient ? doClient(serverPort) : doServer(serverPort);
}

# include "prims.hh"
# include "errorCodes.hh"
# include "lookup.hh"
# include "stringObj.hh"
# include "activation.hh"
# include "utils.hh"
# include "memory.hh"
# include <alloca.h>
# include <unistd.h>
# include <signal.h>

const char* Primitives::          failure_suffix = "IfFail:";
const int   Primitives::length_of_failure_suffix = 7;

# define smi_check_unary() {      \
  if (!is_smi(rcvr))             \
    return primitive_fail_string(BADTYPEERROR);}
    
# define smi_check_binary() {      \
  smi_check_unary();               \
  if (!is_smi(argsp[0])) {         \
    return primitive_fail_string(BADTYPEERROR);}}
    
oop_t primitive_fail_string (error_t errorID) {
  oop_t error_string = ErrorCodes::stringForError(errorID);
  return change_tag(error_string, mark_tag);
}

oop_t primitive_fail ( oop_t selfStringForPrimitive, oop_t rcvr_for_fail, oop_t error_result, oop_t failBlock, oop_t current_activation, oop_t* activation){
  assert(is_mark(error_result));
  oop_t error_string = change_tag(error_result, mem_tag);
  oop_t argsp[2] = {error_string, selfStringForPrimitive};  
  
  oop_t selfStringForMessage;
  
  if (failBlock == badOop) {
    selfStringForMessage = StringObj::intern("primitiveFailedError:Name:");
    *activation = ActivationObj::clone_for_failure(rcvr_for_fail, selfStringForMessage, (oop_t*) argsp, 2, current_activation);
  }
  else {
    rcvr_for_fail = failBlock;
    selfStringForMessage = StringObj::intern("value:With:");
    *activation = ActivationObj::clone_for_failure(rcvr_for_fail, selfStringForMessage, (oop_t*) argsp, 2, current_activation);
  }  
  
  return error_result;
}

oop_t string_print_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {  
  if (!is_byteVector(rcvr))
    return primitive_fail_string(BADTYPEERROR);
  
  ByteVectorObj::from(rcvr)->string_print();
  return rcvr;
}

oop_t debug_print_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {  
  MemObj::debug_print(rcvr);
  return rcvr;
}

oop_t print_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {  
  MemObj::print(rcvr);
  return rcvr;
}

oop_t string_canonicalize_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if (!is_byteVector(rcvr))
    return primitive_fail_string(BADTYPEERROR);
  
  ByteVectorObj* bv = ByteVectorObj::from(rcvr);
  return StringObj::intern(bv->bytes(), bv->indexableSize());
}


oop_t smi_complement_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_unary();
  return smiOop_for_value ( -1 - value_of_smiOop(rcvr)); 
}

oop_t smi_and_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary(); 
  return smiOop_for_value( value_of_smiOop(rcvr) & value_of_smiOop(argsp[0]) );   
}

oop_t smi_or_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  return smiOop_for_value( value_of_smiOop(rcvr) | value_of_smiOop(argsp[0]) );   
}

oop_t smi_xor_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  return smiOop_for_value( value_of_smiOop(rcvr) ^ value_of_smiOop(argsp[0]) );   
}

oop_t smi_arleftshift_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi r = value_of_smiOop(rcvr);
  smi a = value_of_smiOop(argsp[0]);
  smi result = r << a;
  if (result > MAXIMUM_SMI_VALUE || result < MINIMUM_SMI_VALUE)
    return primitive_fail_string(OVERFLOWERROR); //overflow
  return smiOop_for_value( result );   
}

oop_t smi_loleftshift_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  return smi_arleftshift_prim( rcvr, argsp, current_activation, new_actp); //todo primitives test Is this correct? ~ Ausch   
}

smi abs_value (smi arg) {
  return arg < 0 ? -arg : arg;
}

smi arrightshift(smi r, smi a) {
  smi result = r >> a; // aaa According to the C spec, the implementation of >> is platform specific !! 
                       // (for -'ve numbers, at least) ~ Ausch

  // keep the sign
  if (result > 0 && r < 0)
    result = -1 * result;

  return result;
}

oop_t smi_arrightshift_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi r = value_of_smiOop(rcvr), a = value_of_smiOop(argsp[0]);
  return smiOop_for_value( arrightshift(r, a) );
}

oop_t smi_lorightshift_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  //logical right shift does not keep the sign
  smi_check_binary();
  smi a = value_of_smiOop(argsp[0]);
  oop_t shifted = ((u_int32)rcvr) >> a;
  return change_tag( shifted, smi_tag );
  //todo primitives add assertion to verify platform-dependant shifting ~ Ausch
}

oop_t smi_add_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi r = value_of_smiOop(rcvr), a = value_of_smiOop(argsp[0]);
  smi sum = r + a;
  // todo optimize time: There's gotta be a better way to check for overflow
  //                     (in this and all the other arithmetic prims). -- Adam, 5/06
  if (sum > MAXIMUM_SMI_VALUE || sum < MINIMUM_SMI_VALUE)
    return primitive_fail_string(OVERFLOWERROR);
  return smiOop_for_value(sum);
}

oop_t smi_sub_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi r = value_of_smiOop(rcvr), a = value_of_smiOop(argsp[0]);
  smi difference = r - a;
  if (difference > MAXIMUM_SMI_VALUE || difference < MINIMUM_SMI_VALUE)
    return primitive_fail_string(OVERFLOWERROR);
  return smiOop_for_value(difference);    
}

oop_t smi_mul_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi a = value_of_smiOop(argsp[0]); if (a == 0) return smiOop_for_value(0);
  smi r = value_of_smiOop(rcvr    );
  if (a > 0) {
    if (r > MAXIMUM_SMI_VALUE / a || r < MINIMUM_SMI_VALUE / a)
      return primitive_fail_string(OVERFLOWERROR);
  } else {
    if (r < MAXIMUM_SMI_VALUE / a || r > MINIMUM_SMI_VALUE / a)
      return primitive_fail_string(OVERFLOWERROR);
  }
  return smiOop_for_value(r * a);
}

oop_t smi_div_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi a = value_of_smiOop(argsp[0]);
  if (a == 0)
    return primitive_fail_string(DIVISIONBYZEROERROR);
  smi r = value_of_smiOop(rcvr);
  if (r == MINIMUM_SMI_VALUE && a == -1)
    return primitive_fail_string(OVERFLOWERROR);
  int32 quo = (r / a);
  //assert( abs(quo) == abs(r) / abs(a), "smi_div_prim is wrong on this platform");
  return smiOop_for_value(quo);
}

oop_t smi_mod_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  smi_check_binary();
  smi a = value_of_smiOop(argsp[0]);
  if (a == 0)
    return primitive_fail_string(DIVISIONBYZEROERROR);
  smi r = value_of_smiOop(rcvr);
  if (r == MINIMUM_SMI_VALUE && a == -1)
    return primitive_fail_string(OVERFLOWERROR);
  smi mod = (r % a);
  return smiOop_for_value(mod);
}



# define condition_test_init()        \
  smi_check_binary();                 \
  smi r = value_of_smiOop (rcvr    ); \
  smi a = value_of_smiOop (argsp[0]);

oop_t smi_eq_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( (r == a) ? The::true_object : The::false_object);
}


oop_t smi_ne_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( (r != a ) ? The::true_object : The::false_object);
}

oop_t smi_lt_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( ( r < a ) ? The::true_object : The::false_object);
}

oop_t smi_le_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( ( r <= a ) ? The::true_object : The::false_object);
}

oop_t smi_gt_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( ( r > a ) ? The::true_object : The::false_object);
}

oop_t smi_ge_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  condition_test_init();
  return The::oop_of ( ( r >= a ) ? The::true_object : The::false_object);
}

# undef condition_test_init()


# define int32_check(x, var)                            \
  if (is_smi(x)) {                                      \
    var = value_of_smiOop(x);                           \
  } else {                                              \
    if (! is_mem(x))                                    \
      return primitive_fail_string(BADTYPEERROR);       \
    MemObj* m = MemObj::from(x);                        \
    if (! m->is_byteVector())                           \
      return primitive_fail_string(BADTYPEERROR);       \
    var = *((int32*) ((ByteVectorObj*)m)->bytes());     \
  }

# define int32_binary_check()                           \
  int32 r, a;                                           \
  int32_check(rcvr, r);                                 \
  oop_t a_oop = argsp[0];                               \
  int32_check(a_oop, a);                                

# define return_int32(v)                                \
  if (MINIMUM_SMI_VALUE <= v && v <= MAXIMUM_SMI_VALUE) \
    return smiOop_for_value(v);                         \
  else                                                  \
    return                                              \
      ((ByteVectorObj*) The::addr_of(The::int32_proto)) \
          ->clone_for_int32(v);

# define int32_binary_prim(func, valueExpr)   \
  oop_t func (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {   \
    int32_binary_check();                                                              \
    int32 result = (valueExpr);                                                        \
    return_int32( result );                                                            \
  }

int32_binary_prim( int32_add_prim,  r + a );
int32_binary_prim( int32_sub_prim,  r - a );
int32_binary_prim( int32_mul_prim,  r * a );
int32_binary_prim( int32_cmp_prim,  r == a ? 0 : (r < a ? -1 : 1) );
int32_binary_prim( int32_and_prim,  r & a );
int32_binary_prim( int32_or_prim,   r | a );
int32_binary_prim( int32_xor_prim,  r ^ a );
int32_binary_prim( int32_shl_prim,  r << a );
int32_binary_prim( int32_shr_prim,  arrightshift(r, a) );
int32_binary_prim( int32_ushr_prim, r >> a );

oop_t int32_div_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  int32_binary_check();
  if (a == 0)
    return primitive_fail_string(DIVISIONBYZEROERROR);
  int32 result = r / a;
  return_int32( result );
}

oop_t int32_rem_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  int32_binary_check();
  if (a == 0)
    return primitive_fail_string(DIVISIONBYZEROERROR);
  int32 result = r % a;
  return_int32( result );
}


oop_t eq_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  return The::oop_of(rcvr == argsp[0] ? The::true_object : The::false_object);
}

oop_t at_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_objVector(     rcvr )  
           &&  is_smi      ( argsp[0] ) ))
    return primitive_fail_string(BADTYPEERROR);
  
  ObjVectorObj* r = ObjVectorObj::from(rcvr);
  smi a = value_of_smiOop (argsp[0]);
  
  if ( a < 0 || a >= r->indexableSize())
    return primitive_fail_string(BADINDEXERROR);
    
  return r->indexable_at(a);
}

oop_t at_put_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_objVector(     rcvr )  
           &&  is_smi      ( argsp[0] ) ))
    return primitive_fail_string(BADTYPEERROR);
  
  ObjVectorObj* r = ObjVectorObj::from(rcvr);
  smi a = value_of_smiOop (argsp[0]);
  
  if ( a < 0 || a >= r->indexableSize())
    return primitive_fail_string(BADINDEXERROR);

  r->write_indexable_at(a, argsp[1]);
  return rcvr;
}

oop_t size_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if (!is_objVector(rcvr))
    return primitive_fail_string(BADTYPEERROR);
  return smiOop_for_value(ObjVectorObj::from(rcvr)->indexableSize());
}

oop_t byteat_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_byteVector(     rcvr )  
           &&  is_smi       ( argsp[0] ) ))
    return primitive_fail_string(BADTYPEERROR);

  ByteVectorObj* r = ByteVectorObj::from(rcvr);
  smi a = value_of_smiOop (argsp[0]);
  
  if ( a < 0 || a >= r->indexableSize())
    return primitive_fail_string(BADINDEXERROR);
  return r -> byteAt(a);
}

oop_t byteat_put_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_byteVector(     rcvr )  
           &&  is_smi       ( argsp[0] ) 
           &&  is_smi       ( argsp[1] ) ))
    return primitive_fail_string(BADTYPEERROR);
  

  ByteVectorObj* r = ByteVectorObj::from(rcvr);
  smi a = value_of_smiOop (argsp[0]);
  
  if ( a < 0 || a >= r->indexableSize() )
    return primitive_fail_string(BADINDEXERROR);
  
  r -> byteAt_Put( a, argsp[1] );
  return rcvr;
}

oop_t bytesize_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( !is_byteVector( rcvr ))
    return primitive_fail_string(BADTYPEERROR);
    
  return smiOop_for_value(ByteVectorObj::from(rcvr)->indexableSize());
}

oop_t byteVectorConcatenate_Prototype_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_byteVector(     rcvr )  
           &&  is_byteVector( argsp[0] ) 
           &&  is_byteVector( argsp[1] ) ))
    return primitive_fail_string(BADTYPEERROR);
 // if ( is_canonical_string(argsp[1])) //todo: implement is_canonical_string(oop_t oop)
 //   return primitive_fail_string(BADTYPEERROR); //can't use canonical strings as prototypes for concatenation
  
  ByteVectorObj *result_bv, *rcvr_bv, *arg_bv;
  rcvr_bv  = ByteVectorObj::from(     rcvr);
  arg_bv   = ByteVectorObj::from( argsp[0]);
  smi rcvr_size = rcvr_bv->indexableSize();
  smi  arg_size =  arg_bv->indexableSize();

  oop_t result_oop = ByteVectorObj::from( argsp[1]) -> clone_and_resize( rcvr_size + arg_size, 0, &result_bv); 
  
  if (result_oop == badOop)
    unimplemented ("failure: out of memory?"); //todo gc OOM checks are GC dependant?
  
  result_bv->range_set(  0       , rcvr_size, rcvr_bv->bytes());
  result_bv->range_set( rcvr_size,  arg_size,  arg_bv->bytes());
  
  return result_oop;
}


// aaa Does _CloneBytes:Filler: only fail when out of memory or when filler is out of bounds?
oop_t bv_clone_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_byteVector(rcvr)  
           &&  is_smi       (argsp[0]) 
           &&  is_smi       (argsp[1]) ))
    return primitive_fail_string(BADTYPEERROR);
  
  smi size = value_of_smiOop(argsp[0]);
  smi fill = value_of_smiOop(argsp[1]);
  if (! (0 <= fill  &&  fill <= 255))
    return primitive_fail_string(BADTYPEERROR);
  
  oop_t r = ByteVectorObj::from(rcvr)->clone_and_resize( size, fill );
  if (r == badOop)
    unimplemented ("failure: out of memory?"); //todo gc OOM checks are GC dependant?
  
  return r;
}

oop_t ov_clone_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if ( ! (     is_objVector(rcvr)  
           &&  is_smi      (argsp[0]) ))
    return primitive_fail_string(BADTYPEERROR);
  
  smi   size = value_of_smiOop(argsp[0]);
  oop_t fill = argsp[1];
  
  oop_t r = ObjVectorObj::from(rcvr)->clone_and_resize( size, fill );
  if (r == badOop)
    unimplemented ("failure: out of memory?"); //todo gc OOM checks are GC dependant?
  
  return r;
}

// aaa Does _Clone only fail when out of memory?
oop_t clone_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  oop_t r;
  if (is_mark(rcvr))
    return primitive_fail_string(BADTYPEERROR);
    
  else if (is_smi(rcvr) || is_float(rcvr)) // todo primitives cleanup Also check if it is a canonical string
    return rcvr;
  else if (is_byteVector(rcvr))
    r = bv_clone_prim (rcvr, argsp, current_activation, new_actp);
  else if  (is_objVector(rcvr))
    r = ObjVectorObj::from(rcvr) -> clone();
//  else if  (is_block(rcvr))
//    r = clone_block(rcvr, activation);
  else
    r = MemObj::from(rcvr) -> clone();
  
  if (r == badOop) 
    unimplemented ("failure: out of memory?"); //todo gc OOM checks are GC dependant?
  
  return r;
}

oop_t identity_hash_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  if (is_smi(rcvr))
    return rcvr;
  if (is_float(rcvr))
    return primitive_fail_string(PRIMITIVENOTDEFINEDERROR); // unimplemented floats
  assert(is_mem(rcvr));
  return smiOop_for_value(MemObj::from(rcvr)->oid());
}

oop_t asObject_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
 if (!is_smi(rcvr))
   return primitive_fail_string(BADTYPEERROR);
 //todo primitives cleanup failure Handle the case where an oid is not in the table ~Ausch
 return Object_Table::oop_for_int(value_of_smiOop(rcvr));
}

oop_t objectId_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
 if (!is_mem(rcvr))
   return primitive_fail_string(BADTYPEERROR);
 //todo primitives cleanup failure Handle the case where an object is not in the table ~Ausch
 return smiOop_for_value(Object_Table::index_for_oop(rcvr));
}


oop_t tagPartOfObjectReference_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
 return smiOop_for_value(tag(rcvr));
}


oop_t map_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
 return mapOop(rcvr);
}


oop_t wizard_mode_prim(oop_t rcvr, oop_t* , oop_t , oop_t* ) {
  return The::addr_of(The::vm)->contents_of_slot(StringObj::intern("wizardMode"));
}


oop_t set_wizard_mode_prim(oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  oop_t new_value = argsp[0];
  if (! is_boolean(new_value))
    return primitive_fail_string(BADTYPEERROR);
  oop_t wizard_mode_str = StringObj::intern("wizardMode");
  oop_t old_value = The::addr_of(The::vm)->contents_of_slot(wizard_mode_str);
  The::addr_of(The::vm)->set_contents_of_slot(wizard_mode_str, new_value);
  return old_value;
}


oop_t scavenge_prim(oop_t rcvr, oop_t* , oop_t , oop_t* ) {
  Memory::scavenge();
  return rcvr;
}


oop_t breakpoint_prim(oop_t rcvr, oop_t* , oop_t , oop_t* ) {
  printf_and_flush("breakpoint: ");
  if (is_byteVector(rcvr)) ByteVectorObj::from(rcvr)->string_print();
  else                     printf_and_flush("rcvr not a string");
  printf_and_flush("\n");
  
  if (false) { // for debugging
    printf_and_flush("ac oop %d, ac addr 0x%x, map oop %d, map addr 0x%x, ts %d\n", 
                      The::oop_of(The::active_context),
                      The::addr_of(The::active_context),
                                   The::addr_of(The::active_context)->map_oop(),
                      MemObj::from(The::addr_of(The::active_context)->map_oop()),
                      *(int*)Object_Table::get_timestamp_address()
                      );
  }
  kill(getpid(), SIGTRAP);
  return rcvr;
}


oop_t perform_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  oop_t sel = argsp[0];
  oop_t* start_of_args = argsp + 1;
  fint arg_count = 0;
  bool isImplicitSelf = false; // todo adam broken: how can I find out whether it isImplicitSelf or not?
  bool isUndirected = false;
  oop_t delegatee = NULL;
  
  ActivationObj::from(current_activation)
     -> non_primitive_send(rcvr,
                           start_of_args,
                           arg_count,
                           isImplicitSelf,
                           sel,
                           isUndirected,
                           delegatee,
                           current_activation,
                           new_actp);

  return badOop;
}


// todo garbageCollection verify primitive
oop_t verify_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  printf_and_flush("Warning!! call to unimplemented primitive stub: _Verify.");
  return rcvr;
}


oop_t theVM_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  return The::oop_of(The::vm);
}

oop_t set_theVM_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  // Here for compatibility with Klein. Hopefully we'll never try to set it to a
  // different VM than the one that's already running. (I suppose we could, but we'd
  // have to fix up a lot of stuff in The and so on, and it'd be a big pain.) -- Adam, 5/06
  assert(argsp[0] == The::oop_of(The::vm));
  return rcvr;
}

oop_t thisProcess_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  // todo processes: Once we have multiple processes, this shouldn't return the prototypical process. :) -- Adam, 5/06
  return The::oop_of(The::process_proto);
}

oop_t noMapTest_prim (oop_t rcvr, oop_t* argsp, oop_t current_activation, oop_t* new_actp) {
  // Here for compatibility with Klein; doesn't need to do anything. -- Adam, 5/06
  return rcvr;
}


oop_t unimp_prim (oop_t rcvr, oop_t* argsp, smi arg_count, oop_t current_activation, oop_t* new_actp) {
  return primitive_fail_string(PRIMITIVENOTDEFINEDERROR);
}
# undef smi_check_unary
# undef smi_check_binary


PrimitiveTableEntry Primitives::entries[] = {
//Format: 
//{ "_PrimName",      function_name, nargsCache, selfNameCache} -- last two are caches.

  { "_Verify", verify_prim, 0, 0},

  { "_TheVM", theVM_prim, 0, 0},
  { "_TheVM:", set_theVM_prim, 0, 0},
  { "_ThisProcess", thisProcess_prim, 0, 0},

  { "_NoMapTest", noMapTest_prim, 0, 0},

  { "_StringPrint", string_print_prim, 0, 0 },
  { "_DebugPrint",   debug_print_prim, 0, 0 },
  { "_Print",              print_prim, 0, 0 },
  { "_StringCanonicalize", string_canonicalize_prim, -1, 0 },
//{ "_Quit", unimp_prim, -1, 0 },
  
// Cloning

  { "_Clone",               clone_prim, 0, 0 },
  { "_Clone:Filler:",       ov_clone_prim, 0, 0 },
  { "_CloneBytes:Filler:",  bv_clone_prim, 0, 0 },
//{ "_CloneVector",         unimp_prim, 0, 0 },
//{ "_Define:",             unimp_prim, 0, 0 },

//  { "_ErrorMessage",        unimp_prim, 0, 0 },  //returns the description string for an error message coming out of a primitive
  
//  { "_Restart",             restart_prim, 0, 0 },  // no need - implemented in the interpreter loop

  { "_IdentityHash",        identity_hash_prim, 0, 0 }, 

  { "_AsObject",            asObject_prim, 0, 0 }, //returns an object for an OID
  { "_ObjectId",            objectId_prim, 0, 0 }, //returns an OID for an object

//  { "_MethodPointer",       unimp_prim, 0, 0 }, 
  
  { "_TagPartOfObjectReference",  tagPartOfObjectReference_prim, 0, 0 },
  { "_Map",  map_prim, 0, 0 },
  
  { "_Eq:",                   eq_prim, 0, 0 },
  { "_At:",                   at_prim, 0, 0 },
  { "_At:Put:",           at_put_prim, 0, 0 },
  { "_Size",                size_prim, 0, 0 },
  { "_ByteAt:",           byteat_prim, 0, 0 },
  { "_ByteAt:Put:",   byteat_put_prim, 0, 0 },
  { "_ByteSize",        bytesize_prim, 0, 0 },
  
//  { "_ByteVectorCompare:",          unimp_prim, 0, 0 },
  { "_ByteVectorConcatenate:Prototype:",          byteVectorConcatenate_Prototype_prim, 0, 0 },

  
  
//Integer Arithmetic
//  { "_FloatAsInt",        unimp_prim, 0, 0 }, 
//  { "_IntAsFloat",        unimp_prim, 0, 0 }, 

  { "_IntAdd:",          smi_add_prim, 0, 0 },
  { "_IntSub:",          smi_sub_prim, 0, 0 },
  { "_IntMul:",          smi_mul_prim, 0, 0 },
  { "_IntDiv:",          smi_div_prim, 0, 0 },
  { "_IntMod:",          smi_mod_prim, 0, 0 },

  { "_IntComplement",              smi_complement_prim, 0, 0 },  
  { "_IntAnd:",                           smi_and_prim, 0, 0 },
  { "_IntOr:",                             smi_or_prim, 0, 0 },
  { "_IntXor:",                           smi_xor_prim, 0, 0 },
  { "_IntArithmeticShiftLeft:",  smi_arleftshift_prim , 0, 0 },
  { "_IntLogicalShiftLeft:",     smi_loleftshift_prim , 0, 0 },
  { "_IntArithmeticShiftRight:", smi_arrightshift_prim , 0, 0 },
  { "_IntLogicalShiftRight:",    smi_lorightshift_prim , 0, 0 },
  
  { "_IntEQ:",          smi_eq_prim, 0, 0 },
  { "_IntNE:",          smi_ne_prim, 0, 0 },
  { "_IntLT:",          smi_lt_prim, 0, 0 },
  { "_IntLE:",          smi_le_prim, 0, 0 },
  { "_IntGE:",          smi_ge_prim, 0, 0 },
  { "_IntGT:",          smi_gt_prim, 0, 0 },

  { "_Int32:Add:",          int32_add_prim,  0, 0 },
  { "_Int32:Sub:",          int32_sub_prim,  0, 0 },
  { "_Int32:Mul:",          int32_mul_prim,  0, 0 },
  { "_Int32:Div:",          int32_div_prim,  0, 0 },
  { "_Int32:Rem:",          int32_rem_prim,  0, 0 },
  { "_Int32:Cmp:",          int32_cmp_prim,  0, 0 },
  { "_Int32:And:",          int32_and_prim,  0, 0 },
  { "_Int32:Or:" ,          int32_or_prim,   0, 0 },
  { "_Int32:Xor:",          int32_xor_prim,  0, 0 },
  { "_Int32:Shl:",          int32_shl_prim,  0, 0 },
  { "_Int32:Shr:",          int32_shr_prim,  0, 0 },
  { "_Int32:Ushr:",         int32_ushr_prim, 0, 0 },



// Mirrors
/*  { "_Mirror",                                             mirror_prim, 0, 0 }, 
  { "_MirrorAnnotation",                                   unimp_prim, 0, 0 }, 
  { "_MirrorAnnotationAt:",                                unimp_prim, 0, 0 }, 
  { "_MirrorByteCodePosition",                             unimp_prim, 0, 0 },
  { "_MirrorCodes",                                        unimp_prim, 0, 0 },
  { "_MirrorContentsAt:",                                  unimp_prim, 0, 0 },
  { "_MirrorCopyAt:Put:IsParent:IsArgument:Annotation:",   unimp_prim, 0, 0 },
  { "_MirrorCopyAnnotation:",            unimp_prim, 0, 0 },
  { "_MirrorCopyRemoveSlot:",            unimp_prim, 0, 0 },
  { "_MirrorCreateBlock",            unimp_prim, 0, 0 },
  { "_MirrorDefine:",            unimp_prim, 0, 0 },
  { "_MirrorEvaluate:",            unimp_prim, 0, 0 },
  { "_MirrorIsArgumentAt:",            unimp_prim, 0, 0 },
  { "_MirrorIsAssignableAt:",            unimp_prim, 0, 0 },
  { "_MirrorIsParentAt:",            unimp_prim, 0, 0 },
  { "_MirrorLiterals",            unimp_prim, 0, 0 },
  { "_MirrorMethodHolder",            unimp_prim, 0, 0 },
  { "_MirrorNames",            unimp_prim, 0, 0 },
  { "_MirrorNameAt:",            unimp_prim, 0, 0 },
  { "_MirrorReceiver",            unimp_prim, 0, 0 },
  { "_MirrorReflectee",            unimp_prim, 0, 0 },
  { "_MirrorReflecteeEq:",            unimp_prim, 0, 0 },
  { "_MirrorReflecteeIdentityHash",            unimp_prim, 0, 0 },
  { "_MirrorSelector",            unimp_prim, 0, 0 },
  { "_MirrorSender",            unimp_prim, 0, 0 },
  { "_MirrorSize",            unimp_prim, 0, 0 },
  { "_MirrorSource",            unimp_prim, 0, 0 },
  { "_MirrorSourceOffset",            unimp_prim, 0, 0 },
  { "_MirrorSourceLength",            unimp_prim, 0, 0 },*/
  
  
// Others

/* 
  { "_GarbageCollect",    unimp_prim, 0, 0 }, 
  { "_CurrentTimeString",   unimp_prim, 0, 0 },
  { "_DateTime:",           unimp_prim, 0, 0 },

  { "_AbortProcess",          unimp_prim, 0, 0 },
  { "_ActivationAt:",          unimp_prim, 0, 0 },
  { "_ActivationStack",          unimp_prim, 0, 0 },
  { "_AddSlots:",          unimp_prim, 0, 0 },
  { "_AddSlotsIfAbsent:",          unimp_prim, 0, 0 },
  { "_AddressAsObject",          unimp_prim, 0, 0 },
  { "_AnnotateSpyLog",          unimp_prim, 0, 0 },
  { "_BitSize",          unimp_prim, 0, 0 },
  { "_BlockSignals",          unimp_prim, 0, 0 },
  */
  
  { "_Breakpoint",         breakpoint_prim, 0, 0 },
  
  /*
  { "_CopyByteRangeDstPos:Src:SrcPos:Length:",          unimp_prim, 0, 0 },
  { "_CopyRangeDstPos:Src:SrcPos:Length:",          unimp_prim, 0, 0 },
  { "_CFloatDouble:At:",          unimp_prim, 0, 0 },
  { "_CFloatDouble:At:Put:",          unimp_prim, 0, 0 },
  { "_CSignedIntSize:At:",          unimp_prim, 0, 0 },
  { "_CSignedIntSize:At:Put:",          unimp_prim, 0, 0 },
  { "_CUnsignedIntSize:At:",          unimp_prim, 0, 0 },
  { "_CUnsignedIntSize:At:Put:",          unimp_prim, 0, 0 },
  { "_OnNonLocalReturn:",          unimp_prim, 0, 0 },
*/
  { "_Perform:",          perform_prim, 0, 0 }, // todo adam perform: do the rest of these - probably need a more general mechanism, though
/*  { "_Perform:With:",          unimp_prim, 0, 0 },
  { "_Perform:With:With:",          unimp_prim, 0, 0 },
  { "_PerformResend:",          unimp_prim, 0, 0 },
  { "_PerformResend:With:",          unimp_prim, 0, 0 },
  { "_PerformResend:With:With:",          unimp_prim, 0, 0 },
  { "_Perform:DelegatingTo:",          unimp_prim, 0, 0 },
  { "_PrimitiveDocumentation",  unimp_prim, 0, 0 },
  { "_PrimitiveList",          unimp_prim, 0, 0 },
*/
  { "_Scavenge",                  scavenge_prim, 0, 0 },
  { "_WizardMode",             wizard_mode_prim, 0, 0 },
  { "_WizardMode:",        set_wizard_mode_prim, 0, 0 },
  
   
  { "", 0, 0, 0 }
};


oop_t PrimitiveTableEntry::invoke(oop_t rcvr, oop_t* argsp, smi nargs, oop_t current_actp, oop_t* new_actp) {
  if ( arg_count_sans_IfFail != nargs )  {
    printf_and_flush("%d, %d, \n", arg_count_sans_IfFail, nargs);
    unimplemented("arg count mismatch");
  }
  return fn(rcvr, argsp, current_actp, new_actp);
}



PrimitiveTableEntry* Primitives::lookup(oop_t sel, bool* has_IfFail) {
  // todo optimize time, use a hash table, dmu 1/06
  
  for (PrimitiveTableEntry *e = entries;  !e->is_null();  ++e) {
    if (e->matches_sans_IfFail(sel)) {
      *has_IfFail = false;
      return e;
    }
    if (e->matches_with_IfFail(sel)) {
      *has_IfFail = true;
      return e;
    }
  }

  // If the primitive isn't found, we figure out whether it has
  // a fail block  the slow way. This is needed for compatibility
  // with the original Self vm ~ Ausch Mar/06
  
  // todo optimize primitive failure
  *has_IfFail = ByteVectorObj::from(sel)->ends_with_C_string((char*)failure_suffix, length_of_failure_suffix);
  return NULL;
}


oop_t Primitives::invoke( oop_t sel, oop_t rcvr, oop_t* argsp, smi arg_count, oop_t current_activation, oop_t* new_actp ) {
  assert(length_of_C_string((char*)failure_suffix) == length_of_failure_suffix);
  
  bool has_IfFail = false;
  
  PrimitiveTableEntry* pte = lookup(sel, &has_IfFail);
  
  oop_t result =  (pte != NULL)      
                ? pte->invoke( rcvr, argsp, pte->arg_count_sans_IfFail, current_activation, new_actp )
                : unimp_prim ( rcvr, argsp, arg_count,                  current_activation, new_actp );
                
  if (!is_mark(result)) // normal primitive returned successfully
    return result;
  
  if (result == badOop) // the primitive wants to call a method; for now, only the _Perform: primitives do this
    return result;
  
  oop_t rcvr_for_fail = ( !rcvr || rcvr == badOop) ? The::oop_of(The::lobby) : rcvr;

  return primitive_fail ( sel,
                          rcvr_for_fail,
                          result, 
                          has_IfFail ? argsp[arg_count-1] : badOop, 
                          current_activation,
                          new_actp);
}


void Primitives::initialize() {
  for ( PrimitiveTableEntry *e = entries;  !e->is_null();  ++e) {

    int cName_sans_IfFail_length  = length_of_C_string(e->cName_sans_IfFail);
    int cName_with_IfFail_length  = cName_sans_IfFail_length + length_of_failure_suffix;
    
    char* cName_with_IfFail = (char*) alloca(cName_with_IfFail_length + 1 /* for the null char */);
    sprintf(cName_with_IfFail, "%s%s", e->cName_sans_IfFail, failure_suffix);
    
    e->selfName_sans_IfFail = StringObj::intern( e->cName_sans_IfFail,  cName_sans_IfFail_length );
    e->selfName_with_IfFail = StringObj::intern(    cName_with_IfFail,  cName_with_IfFail_length );

    e->arg_count_sans_IfFail = arg_count_of_string(e->cName_sans_IfFail, cName_sans_IfFail_length);
  }   
}

# include "errorCodes.hh"
# include "stringObj.hh"




ErrorCodeEntry ErrorCodes::errorTable[] = {
  { PRIMITIVENOTDEFINEDERROR, "primitiveNotDefinedError" },
  { PRIMITIVEFAILEDERROR,         "primitiveFailedError" },
  
  { BADTYPEERROR,               "badTypeError" },
  { DIVISIONBYZEROERROR, "divisionByZeroError" },
  { OVERFLOWERROR,             "overflowError" },
  { BADSIGNERROR,               "badSignError" },
  { ALIGNMENTERROR,           "alignmentError" },
  { BADINDEXERROR,             "badIndexError" },
  { BADSIZEERROR,               "badSizeError" },
  { REFLECTTYPEERROR,       "reflectTypeError" },
  
  { OUTOFMEMORYERROR,     "outOfMemoryError" },
  { STACKOVERFLOWERROR, "stackOverflowError" },
  
  { SLOTNAMEERROR,                         "slotNameError" },
  { BADSLOTNAMEERROR,                   "badSlotNameError" },
  { ARGUMENTCOUNTERROR,               "argumentCountError" },
  { PARENTERROR,                             "parentError" }, //illegal parent priority
  { UNASSIGNABLESLOTERROR,         "unassignableSlotError" },
  { LONELYASSIGNMENTSLOTERROR, "lonelyAssignmentSlotError" },
  
  
  //activation errors
  { NOACTIVATIONERROR,  "noActivationError" }, //dead activation
  { NORECEIVERERROR,      "noReceiverError" }, //activation has no receiver
  { NOPARENTERROR,          "noParentError" }, //  ''              parent
  { NOSENDERERROR,          "noSenderError" }, //  ''              sender
  
  //invalid branch bytecode
  { BADBRANCHERROR, "badBranchError" },
  
  //marks end of errorTable
  { ENDMARKER , "" }
};

oop_t getSelfString(ErrorCodeEntry* e) {
   return e->selfString ? e->selfString : StringObj::intern(e->cString);
}

void ErrorCodes::initialize() {
  for (ErrorCodeEntry *e = errorTable;  !e->end_marker();  ++e) {
    e->selfString = StringObj::intern(e->cString);
  }
}

// todo optimize time 
oop_t ErrorCodes::stringForError (error_t errorID) {
  for (ErrorCodeEntry *e = errorTable; !e->end_marker(); ++e)
    if (e->ID == errorID) return getSelfString(e);
  unimplemented("missing error string error");
  return 0;
}# include "memory.hh"
# include "stringObj.hh"
# include "activation.hh"
# include "interpreter.hh"
# include "freeLists.hh"


// todo optimize time: This is pretty ridiculous. :) So many lookups. Hard-code the offset?
// todo optimize time: Maybe have objsTop, etc. be the actual address, rather than a tagged smi with the address?
inline oop_t*     top_of_space( MemObj* space_addr ) { return (oop_t*) value_of_smiOop( space_addr->contents_of_slot(StringObj::intern("top"       )) ); }
inline oop_t* objsTop_of_space( MemObj* space_addr ) { return (oop_t*) value_of_smiOop( space_addr->contents_of_slot(StringObj::intern("objsTop"   )) ); }
inline oop_t*  bottom_of_space( MemObj* space_addr ) { return (oop_t*) value_of_smiOop( space_addr->contents_of_slot(StringObj::intern("objsBottom")) ); }

inline void set_objsTop_of_space( MemObj* space_addr, oop_t* p ) { space_addr->set_contents_of_slot( StringObj::intern("objsTop"), smiOop_for_value((smi) p) ); }


bool Memory::is_address_in_space( oop_t* addr, MemObj* space_addr ) {
  return bottom_of_space(space_addr) <= addr  &&  addr < top_of_space(space_addr);
}


# include "smallVMDeclarationsOrStubs.hh"
// todo adam gc: What's the right way to allocate this mark stack? And how big should it be?
const int mark_stack_limit = fixed_address_buffer_length / (sizeof(int) * 32);
static oop_t mark_stack[mark_stack_limit];
static int top_of_mark_stack;
static int number_of_objects_marked;

static MemObj** current_sweep_OT_entry_addr;

// todo adam gc: how big should the remembered set be?
const int remembered_set_size = 200;
static oop_t remembered_set[remembered_set_size];
static int number_of_remembered_objects;

void Memory::add_to_remembered_set(oop_t o) {
  remembered_set[number_of_remembered_objects++] = o;
  if (number_of_remembered_objects == remembered_set_size) {
    // todo adam gc: do a scavenge? That would mean that any pointer write could end up moving objects around.
    unimplemented("just filled up the remembered set; now what?");
  }
}


enum gc_phase {
    gc_mark_phase,
    gc_sweep_phase,
    gc_idling_phase
} current_gc_phase;


// todo adam gc: this feels like a hack.
static bool is_safe_to_do_gc = false;
void Memory::not_safe_to_do_gc_now() { is_safe_to_do_gc = false; }
void Memory::    safe_to_do_gc_now() {
 is_safe_to_do_gc = true;
 current_gc_phase = gc_idling_phase;
 top_of_mark_stack = 0;
 current_sweep_OT_entry_addr = Object_Table::base_addr();
 number_of_remembered_objects = 0;
}


void push_on_mark_stack(oop_t o, MemObj* addr) {
  if (top_of_mark_stack >= mark_stack_limit)
    unimplemented("cannot yet recover from mark stack overflow");

  addr->gc_mark_grey();
  mark_stack[top_of_mark_stack++] = o;
}


void push_on_mark_stack_if_necessary(oop_t x) {
  if (! is_mem(x))
    return;
  
  MemObj* addr = MemObj::from(x);
  if (addr->gc_is_marked_as_live())
    return;

  push_on_mark_stack(x, addr);
}


void mark_one_object() {
    oop_t obj = mark_stack[--top_of_mark_stack];
    MemObj* addr = MemObj::from(obj);
    addr->gc_mark_as_off_mark_stack();
    oop_t* p = (oop_t*)addr;
    if (addr->is_activation()) {
      oop_t* end = ((ActivationObj*)addr)->end_of_live_oops();
      while ( ++p != end ) {
        push_on_mark_stack_if_necessary(*p);
      }
    } else if (addr->is_byteVector()) {
      oop_t* end = (oop_t*) ((ByteVectorObj*)addr)->bytes();
      while ( ++p != end ) {
        push_on_mark_stack_if_necessary(*p);
      }
    } else {
      oop_t o;
      while ( !is_mark(o = *++p) ) {
        push_on_mark_stack_if_necessary(o);
      }
    }
}


void mark_n_objects(fint n) {
  fint i = 0;
  while (i < n && top_of_mark_stack > 0) {
    ++i;
    mark_one_object();
  }
  number_of_objects_marked += i;
}


void emergency_full_marking_phase() {
  while (top_of_mark_stack > 0) {
    mark_one_object();
  }
}


inline ObjVectorObj* freeOopsLists_in_space(MemObj* space_addr) {
  // todo optimize time: shouldn't need to do a lookup - hard-code the offset?
  return ObjVectorObj::from( space_addr->contents_of_slot(StringObj::intern("freeOopsLists")) );
}


inline void sweep_one_object(ObjVectorObj* freeOopsLists) {
  MemObj* addr = *current_sweep_OT_entry_addr;
  if (Object_Table::is_valid(addr)) {
    if (addr->gc_is_marked_as_live()) {
      addr->gc_mark_white();
    } else {
      fint nOops = addr->total_size_in_oops();
      FreeOopsLists::add(freeOopsLists, nOops, (oop_t*)addr);
      Object_Table::invalidate(current_sweep_OT_entry_addr);
    }
  }
  ++current_sweep_OT_entry_addr;
}


void sweep_n_objects(fint n) {
  ObjVectorObj* freeOopsLists = freeOopsLists_in_space( The::addr_of(The::tenuredSpace) );
  MemObj** end_addr = Object_Table::end_addr();
  fint i = 0;
  while (i++ < n && current_sweep_OT_entry_addr < end_addr) {
    sweep_one_object(freeOopsLists);
  }
}


void emergency_full_sweeping_phase() {
  ObjVectorObj* freeOopsLists = freeOopsLists_in_space( The::addr_of(The::tenuredSpace) );
  MemObj** end_addr = Object_Table::end_addr();
  while (current_sweep_OT_entry_addr < end_addr) {
    sweep_one_object(freeOopsLists);
  }
}


void reset_gc() {
  current_gc_phase = gc_mark_phase;
  top_of_mark_stack = 0;
  number_of_objects_marked = 0;
  current_sweep_OT_entry_addr = Object_Table::base_addr();
  // Must stay in sync with startingPoints in the exportPolicy in the Self world.
  push_on_mark_stack_if_necessary( Interpreter::current_activation() ); // todo adam gc: I'd like to have a way to start from the root of the stack, since it's less volatile
  push_on_mark_stack_if_necessary( The::oop_of(The::vm   )           );
  push_on_mark_stack_if_necessary( The::oop_of(The::lobby)           );
}


bool should_start_collecting_again() {
  ObjVectorObj* freeOopsLists = freeOopsLists_in_space( The::addr_of(The::tenuredSpace) );
  return ! FreeOopsLists::has_a_fair_amount_of_free_space_left(freeOopsLists);
}


void do_a_little_bit_of_gc() {
  switch (current_gc_phase) {
    case gc_idling_phase:
      if (should_start_collecting_again())
        reset_gc();
      break;
    case gc_mark_phase:
      if (top_of_mark_stack > 0)
        mark_n_objects(300); // todo adam gc magic number
      else
        current_gc_phase = gc_sweep_phase;
      break;
    case gc_sweep_phase:
      sweep_n_objects(300); // todo adam gc magic number
      if (current_sweep_OT_entry_addr >= Object_Table::end_addr())
        current_gc_phase = gc_idling_phase;
      break;
    default:
      fatal("what phase are we in???");
  }
}


// I don't know if we want to leave this functionality in, but it was
// easy to write, and might be useful as a last-resort recovery mechanism
// (though hopefully we can write the incremental GC in a way that this
// will never be necessary). -- Adam, 4/06
void emergency_full_gc() {
  switch (current_gc_phase) {
    case gc_idling_phase:
      reset_gc();
      // fall through
    case gc_mark_phase:
      emergency_full_marking_phase();
      current_gc_phase = gc_sweep_phase;
      // fall through
    case gc_sweep_phase:
      emergency_full_sweeping_phase();
      current_gc_phase = gc_idling_phase;
      break;
    default:
      fatal("what phase are we in???");
  }
}


oop_t Memory::allocate_oops(fint nOops, MemObj** addrp) {
  return Memory::allocate_oops_and_bytes(nOops, NO_BYTES_PART, addrp);
}


inline fint oops_needed_to_hold_bytes(fint bytes_needed) {
  return divide_and_round_up(bytes_needed, sizeof(oop_t));
}


FreeOops* allocate_oops_from_freeOopsLists( MemObj* space_addr, fint nOops ) {
  ObjVectorObj* freeOopsLists = freeOopsLists_in_space( space_addr );
  FreeOops* f = FreeOopsLists::find_freeOops(freeOopsLists, nOops);
  if (!f) {
    printf_and_flush("ran out of memory allocating %i oops; invoking emergency stop-the-world GC...\n", nOops);
    FreeOopsLists::print_freeOops_left(freeOopsLists);
    emergency_full_gc();
    f = FreeOopsLists::find_freeOops(freeOopsLists, nOops);
    if (!f) {
      unimplemented("OK, we're REALLY out of memory. Now what? Compaction?");
    }
  }
  return f;
}


void promote( oop_t o, MemObj* addr ) {
  // todo optimize time: pass the tenured_space_addr in so we don't have to keep finding it again (but watch out for it moving)
  MemObj* tenured_space_addr = The::addr_of(The::tenuredSpace);
  fint nOops = addr->total_size_in_oops();
  FreeOops* f = allocate_oops_from_freeOopsLists( tenured_space_addr, nOops );
  oop_t* new_addr = (oop_t*)f;
  oop_t *src = (oop_t*)addr, *dst = new_addr, *end = new_addr + nOops;
  while (dst != end)
    *dst++ = *src++;
  if (! is_mark(*end))
    *end = trailingMarkOop;
  Object_Table::at_oop_put( o, (MemObj*)new_addr );
}


// todo adam gc: do a non-recursive version of this?
// todo adam gc: duplication with the marking algorithm, but I'm reluctant to make a macro or use a function pointer.
void recursive_scavenge(oop_t o);
void recursive_scavenge_young_objects_in(MemObj* addr) {
  oop_t* p = (oop_t*)addr;
  if (addr->is_activation()) {
    oop_t* end = ((ActivationObj*)addr)->end_of_live_oops();
    while ( ++p != end ) {
      recursive_scavenge(*p);
    }
  } else if (addr->is_byteVector()) {
    oop_t* end = (oop_t*) ((ByteVectorObj*)addr)->bytes();
    while ( ++p != end ) {
      recursive_scavenge(*p);
    }
  } else {
    oop_t o;
    while ( !is_mark(o = *++p) ) {
      recursive_scavenge(o);
    }
  }
}


// todo adam gc: do a non-recursive version of this?
void recursive_scavenge(oop_t o) {
  if (! is_mem(o) )
    return;
  
  MemObj* addr = MemObj::from(o);

  if (! addr->is_young())
    return;
  
  promote( o, addr );
  recursive_scavenge_young_objects_in( addr );
}


void recycle_oops_of_new_objects( MemObj* eden_space_addr ) {
  oop_t* objsBottom =  bottom_of_space(eden_space_addr);
  oop_t* objsTop    = objsTop_of_space(eden_space_addr);
  oop_t* p          = objsBottom;
  while (p < objsTop) {
    MemObj* addr = (MemObj*)p;
    oop_t o = addr->oop();
    MemObj* recorded_addr = MemObj::from(o);
    if (recorded_addr == addr) { // has not been scavenged
      Object_Table::recycle_oop(o);
    }
    p += addr->total_size_in_oops();
  }
  set_objsTop_of_space( eden_space_addr, objsBottom );
}


void do_full_scavenge( MemObj* eden_space_addr ) {
  printf("invoking scavenger...\n");
  for (fint i = 0; i < number_of_remembered_objects; ++i) {
    oop_t   remembered_obj      = remembered_set[i];
    MemObj* remembered_obj_addr = MemObj::from(remembered_obj);
    remembered_obj_addr->gc_mark_as_not_being_in_remembered_set();
    recursive_scavenge_young_objects_in( remembered_obj_addr );
  }
  number_of_remembered_objects = 0;
  recursive_scavenge( Interpreter::current_activation() );
  
  The::moved_some_objects();
  
  recycle_oops_of_new_objects(eden_space_addr);
  
  printf("done scavenging.\n");
}


void Memory::scavenge() {
  do_full_scavenge(The::addr_of(The::edenSpace));
}


oop_t* allocate_oops_in_eden( MemObj* eden_space_addr, fint nOops ) {
  oop_t* old_objsTop = objsTop_of_space(eden_space_addr);
  oop_t*         top =     top_of_space(eden_space_addr);

  oop_t* new_addr    = old_objsTop;
  oop_t* new_objsTop = new_addr + nOops;

  if (new_objsTop >= top) { // >= so as to leave room for the trailing mark
    do_full_scavenge(eden_space_addr);
    return allocate_oops_in_eden(The::addr_of(The::edenSpace), nOops);
  }
  
  *new_objsTop = trailingMarkOop;
  set_objsTop_of_space( eden_space_addr, new_objsTop );
  return new_addr;
}


oop_t Memory::allocate_oops_and_bytes(fint nOops, fint nBytes, MemObj** addrp, char** bytesp) {
  if (is_safe_to_do_gc)
    do_a_little_bit_of_gc();
  
  MemObj* eden_addr = The::addr_of(The::edenSpace);
  oop_t* new_addr = allocate_oops_in_eden( eden_addr, nOops + oops_needed_to_hold_bytes(nBytes) );
  MemObj* a = (MemObj*) new_addr;
  
  if (addrp )  *addrp = a;
  if (bytesp)  *bytesp = (char*) (new_addr + nOops);
  oop_t x = a->alloc_oop_for_me();
  
  return x;
}


void  Memory::remember_to_revisit(MemObj* addr) {
  push_on_mark_stack(addr->oop(), addr);
}

inline bool has_not_swept_yet(oop_t obj) {
  return current_sweep_OT_entry_addr <= Object_Table::address_of_entry_for_oop(obj);
}

oop_t  Memory::adjust_markOop_for_clone(oop_t mark_oop, oop_t new_obj) {
  // todo adam gc: do something less conservative? is this even correct?
  if (    current_gc_phase == gc_mark_phase
      || (current_gc_phase == gc_sweep_phase && has_not_swept_yet(new_obj)))
    return gc_make_markOop_black(mark_oop);
  else
    return mark_oop;
}
# include "freeLists.hh"
# include "utils.hh"
# include "objVector.hh"


FreeOops* FreeOopsLists::get_first_entry(ObjVectorObj* freeOopsLists, fint i) {
  assert(smi_tag == 0 && tag_shift == 2); // an untagged address looks like a tagged smi
  return (FreeOops*) freeOopsLists->indexable_at(i);
}


void FreeOopsLists::set_first_entry(ObjVectorObj* freeOopsLists, fint i, FreeOops* f) {
  assert(smi_tag == 0 && tag_shift == 2); // an untagged address looks like a tagged smi
  freeOopsLists->write_indexable_at(i, (oop_t) f);
}


inline fint index_for_freeOopsList_containing_entries_of_size(fint i) {
  return min(i, FreeOopsLists::last_index);
}


void FreeOopsLists::add(ObjVectorObj* freeOopsLists, fint nOops, oop_t* addr) {
  assert(nOops >= FreeOops::oop_size_of_an_entry());
  FreeOops* f = (FreeOops*)addr;
  fint i = index_for_freeOopsList_containing_entries_of_size(nOops);
  f->set_next_freeOops( get_first_entry(freeOopsLists, i) );
  f->set_size(nOops);
  set_first_entry(freeOopsLists, i, f);
}


// todo cleanup: Break up this function? The problem is that all the different sections
//               have to pass more than one thing on to the later sections. -- Adam, 5/06
FreeOops* FreeOopsLists::find_freeOops(ObjVectorObj* freeOopsLists, fint nOops) {
  // find the index of the freeOopsList we need   
  fint i = nOops;
  FreeOops* f;
  while (! (f = get_first_entry(freeOopsLists, i = index_for_freeOopsList_containing_entries_of_size(i)))) {
    if (i == last_index)
      return NULL;
    
    i += (i == nOops
             ? FreeOops::oop_size_of_an_entry()  // don't leave holes too small for a whole freelist entry
             : 1);
  }
  
  smi s = f->size();
  FreeOops* old_f = NULL;
  if (i == last_index) {
    // the last freeOopsList contains all entries of that size or higher,
    // so we need to find an entry of an appropriate size
    while (! FreeOops::is_acceptable_size(nOops, s)) {
      old_f = f;
      f = f->next_freeOops();
      if (!f)
        return NULL;
      s = f->size();
    }
  }
  
  // remove the FreeOops from the list
  if (old_f == NULL)
    set_first_entry(freeOopsLists, i, f->next_freeOops());
  else
    old_f->set_next_freeOops(f->next_freeOops());
  
  // if we used a chunk that was too big, put the extra oops back on the appropriate list
  smi extra_oops = s - nOops;
  if (extra_oops != 0)
    add( freeOopsLists, extra_oops, ((oop_t*) f) + nOops );
  
  return f;
}


void FreeOopsLists::print_freeOops_left(ObjVectorObj* freeOopsLists) {
  for (fint i = FreeOops::oop_size_of_an_entry(); i < last_index; ++i) {
    fint n = 0;
    FreeOops* f = get_first_entry(freeOopsLists, i);
    while (f) {
      ++n;
      f = f->next_freeOops();
    }
    printf("Entries of size %i: %i\n", i, n);
  }

  printf("Sizes of entries of size %i or higher:", last_index);
  FreeOops* f = get_first_entry(freeOopsLists, last_index);
  while (f) {
    printf(" %i,", f->size());
    f = f->next_freeOops();
  }
  printf("\n");
}


// todo adam gc: This code really feels like a hack. It just sorta makes sure that there
//               are a bunch of entries of each size (or higher). What's a better way to
//               decide whether to start a new GC cycle?
const fint DESIRED_ENTRIES_OF_EACH_SIZE = 25; // todo adam gc magic number
bool FreeOopsLists::has_a_fair_amount_of_free_space_left(ObjVectorObj* freeOopsLists) {
  fint entries_needed_of_previous_size = 0;
  for (fint i = FreeOops::oop_size_of_an_entry(); i < last_index; ++i) {
    FreeOops* f = get_first_entry(freeOopsLists, i);
    fint entries_needed_of_this_size = DESIRED_ENTRIES_OF_EACH_SIZE + entries_needed_of_previous_size;
    while (entries_needed_of_this_size && f) {
      --entries_needed_of_this_size;
      f = f->next_freeOops();
    }
    entries_needed_of_previous_size = entries_needed_of_this_size;
  }
  
  FreeOops* f = get_first_entry(freeOopsLists, last_index);
  fint oops_available_in_last_list = 0;
  fint oops_desired = (DESIRED_ENTRIES_OF_EACH_SIZE + entries_needed_of_previous_size) * last_index;
  while (f) {
    oops_available_in_last_list += f->size();
    if (oops_available_in_last_list >= oops_desired)
      return true;
    f = f->next_freeOops();
  }
  return false;
}
# include "lookup.hh"
# include "map.hh"
# include "stringObj.hh"
# include "activation.hh"


// todo optimize space Should we save map space by not having separate SDs for assignment slots? -- dmu 12/06

Lookup::Result* Lookup::findSlotsIn( oop_t rcvr, oop_t selector, LookupType lt ) {
  static bool reentered = false;
  if (reentered) fatal("reentered");
  reentered = true;
  
  static Lookup lp;
  lp.init(selector, lt);
  if (baseLookupType(lt) == ResendBaseLookupType)   lp.findInParentsOf(rcvr, MapObj::from(mapOop(rcvr)));
  else                                              lp.findInObject(rcvr);
  
  reentered = false;
  return &lp.result;
}

void Lookup::selectorAndSourceForLookupError (Result::result_type rt, oop_t& sel, const char*& source) {
 switch (rt) {
  case Result::foundNone: 
    sel = StringObj::intern( "undefinedSelector:Receiver:Type:Delegatee:MethodHolder:Arguments:");
    source = "\n\"undefined selector error;\nthis method was automatically generated by the VM.\"\n";
    break;
  
  case Result::foundOne: 
    fatal ("Should not reach here");
    break;

  case Result::foundTwo: 
    sel = StringObj::intern( "ambiguousSelector:Receiver:Type:Delegatee:MethodHolder:Arguments:");
    source = "\n\"undefined selector error;\nthis method was automatically generated by the VM.\"\n";
    break;

    
/*
   case delegateeNotFound:
    sel = VMString[MISSINGPARENTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"missing parent error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case foundAssignableParent:
   case resendUndecidable:
    ShouldNotReachHere(); // run-time lookup failing w/ a compile-time condition

   case mismatchedArgCount:
    sel = VMString[MISMATCHEDARGUMENTCOUNTSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"mismatched argument count error;\nthis method was automatically generated by the VM.\"\n";
    break;

   case performTypeError:
    sel = VMString[PERFORMTYPEERRORSELECTOR_RECEIVER_TYPE_DELEGATEE_METHODHOLDER_ARGUMENTS_];
    source = "\n\"perform type error;\nthis method was automatically generated by the VM.\"\n";
    break;
*/
   default:
    fatal ("should not reach here"); // unknown status
  }    
  
}


oop_t Lookup::messageTypeForLookupError(LookupType lookupType) {
  // todo optimize time: put these strings in The. -- Adam, 5/06
  
  switch (baseLookupType(lookupType)) {
   case NormalBaseLookupType:
     if (isImplicitSelfLookupType(lookupType))
       return StringObj::intern("implicitSelf");
     else
       return StringObj::intern("normal");
   case ResendBaseLookupType:
     return StringObj::intern("undirectedResend");
   case DirectedResendBaseLookupType:
     return StringObj::intern("directedResend");
   case DelegatedBaseLookupType:
     return StringObj::intern("delegated");
   default:
     fatal("should not reach here");
     return badOop;
  }
}

const fint MAX_ARGS = 100;
fint Lookup::argCountForLookupError(oop_t selector, fint perform_arg_count, LookupType lookupType) {
  fint argc;

  if (isPerformLookupType(lookupType)) {
    assert(perform_arg_count >= 0); // should have a static selector or a perform arg count
    argc = perform_arg_count;
  } else {
    assert(is_byteVector(selector)); // should be a string if static
    argc = ByteVectorObj::from(selector)->arg_count();
  }
  assert(argc >= 0 && argc < MAX_ARGS);
  return argc;
}

oop_t Lookup::lookupFailed(oop_t rcvr, oop_t sel, oop_t del, oop_t holder_of_sender_method, oop_t sender_act, LookupType lookupType, Result::result_type rt) {
  oop_t selForError;
  const char* sourceForError;
  selectorAndSourceForLookupError(rt, selForError, sourceForError);

  oop_t holder_of_error_method;
  oop_t rcvr_for_error_method = The::oop_of(The::process_proto); // todo processes need to get the current process
  oop_t error_method = findObject( rcvr_for_error_method, selForError, &holder_of_error_method );
  if (error_method == badOop) {
    printf("Lookup failed - "); StringObj::from(sel)->string_print(); printf("\n");
    printf("And looking up error selector failed - "); StringObj::from(selForError)->string_print(); printf("\n");
    unimplemented("lookup failure: could not find original selector and could not find error selector either");
    return badOop;
  }
  
  assert( holder_of_sender_method != badOop );
  oop_t msgType = messageTypeForLookupError(lookupType);
  oop_t vector_of_original_args = ActivationObj::from(sender_act)->vector_of_outgoing_arguments(ByteVectorObj::from(sel)->arg_count(), isImplicitSelfLookupType(lookupType));

  assert(ByteVectorObj::from(selForError)->arg_count() == 6); // Selector:Receiver:Type:Delegatee:MethodHolder:Arguments:
  oop_t args_to_lookup_error_method[6];
  args_to_lookup_error_method[0] = selForError;
  args_to_lookup_error_method[1] = rcvr;
  args_to_lookup_error_method[2] = msgType;
  args_to_lookup_error_method[3] = del;
  args_to_lookup_error_method[4] = holder_of_sender_method;
  args_to_lookup_error_method[5] = vector_of_original_args;
  
  // todo should really create a new method sending the error message
  return ActivationObj::clone_for(error_method, holder_of_error_method, rcvr_for_error_method, args_to_lookup_error_method, 6, sender_act);  
  
/*
  // Create a method sending the lookup error message to the
  // current process.
  ByteCode b(true);
  slotList* slots = EMPTY;

  // push the receiver of the method send
  b.GenSendByteCode(0, 0, new_string("_ThisProcess"), true, false, NULL);

  // push the arguments
  
  // First push the selector. Must put it in a slot because
  // for a perform type error, it might be a block with an uplevel access.
  // That confuses the fixup code, which expects block literals to be lexically part of this method.
  // So, replaced   b.GenLiteralByteCode(0, 0, selector()); 
  // with: (dmu 1/03)
  stringOop attemptedSelector = new_string("selector");
  slots = slots->add( attemptedSelector, map_slotType, selector());
  b.GenSendByteCode(0, 0, attemptedSelector, true, false, NULL);

  b.GenSelfByteCode(0, 0);
  
  b.GenLiteralByteCode(0, 0, msgType);
  
  b.GenLiteralByteCode(0, 0,
                       delegatee() == NULL ? Memory->nilObj : delegatee());
  
  assert_methodHolder_is_object();
  assert(!methodHolder_or_map()->has_code(),
         "method holder shouldn't have code");
  
  stringOop del = new_string("delegatee");
  slots = slots->add( del, map_slotType,
                      isResendLookupType(lookupType())
                      ? methodHolder_or_map() : Memory->nilObj);
  b.GenSendByteCode(0, 0, del, true, false, NULL);
  
  // create vector to hold args: "vector _Clone: arc _FillingWith: nil"
  b.GenLiteralByteCode(0, 0, Memory->objVectorObj);
  b.GenLiteralByteCode(0, 0, as_smiOop(argc));
  b.GenLiteralByteCode(0, 0, Memory->nilObj);
  b.GenSendByteCode(0, 0, VMString[_CLONE_FILLER_], false, false, NULL);
  
  // also cons up arg names, add arg slots, get args into stack
  for (fint i = 0; i < argc; i++) {
    // make argNameOop
    char argName[20];
    sprintf(argName, "arg%ld", (long)i + 1);
    stringOop arg = new_string(argName);
    
    // make slot and store arg in vector
    slots = slots->add(arg, arg_slotType, as_smiOop(i));
    
    // <vector> at: i Put: <arg>
    b.GenLiteralByteCode(0, 0, as_smiOop(i));
    b.GenSendByteCode(0, 0, arg, true, false, NULL);
    b.GenSendByteCode(0, 0, VMString[_AT_PUT_], false, false, NULL);
  }
  
  // send the message to the receiver.
  b.GenSendByteCode(0, 0, sel, false, false, NULL);
  
  // NB: don't change <error> below without consulting recompile.c;
  // the code there uses it to recognize these weird methods.  Should
  // really be a flag somewhere
  bool ok = b.Finish("<error>", source);
  assert(ok, "no errors here");

  slotsOop method= create_outerMethod(slots, &b);
  setResult(method);*/
}

oop_t Lookup::findObject( oop_t rcvr,
                          oop_t sel,
                          oop_t* holderp,
                          SlotDesc** sdp,
                          oop_t* lookup_failed_act_addr,
                          oop_t sender_act,
                          bool isUndirectedResend,
                          oop_t delegatee,
                          oop_t holder_of_sender_method,
                          bool isSelfImplicit ) {
                          
  BaseLookupType blt = isUndirectedResend   ?         ResendBaseLookupType
                     : delegatee            ? DirectedResendBaseLookupType
                                            :         NormalBaseLookupType;
  LookupType lt = blt | (isSelfImplicit ? ImplicitSelfBit : 0);
  
  oop_t lookup_start = isUndirectedResend ?              holder_of_sender_method
                     : delegatee          ? MemObj::from(holder_of_sender_method)->contents_of_slot(delegatee)
                     : rcvr;

  Result* r = findSlotsIn(lookup_start, sel, lt);
  if (r->rt != Lookup::Result::foundOne) {
    if (lookup_failed_act_addr == NULL) {
      printf("Lookup failed - "); StringObj::from(sel)->string_print(); printf("\n");
      fatal("Lookup failed that was not expected to fail");
    } else {
      *lookup_failed_act_addr = lookupFailed(rcvr, sel, delegatee, holder_of_sender_method, sender_act, lt, r->rt);
    }
    return badOop;
  }
  oop_t holder = r->refs[0].holder;

  if (holderp) {
    *holderp = holder;
    if (sdp)
      *sdp = r->refs[0].sd;
  }
  return r->refs[0].sd->contents(holder);
}


void Lookup::findInObject(oop_t r) {
  MapObj* m_addr = MapObj::from(mapOop(r));
  if (m_addr->lookup_is_marked())
    return;
  m_addr->lookup_mark();
  
  SlotDesc* sd = m_addr->find_slot(selector);
  if (sd != NULL)
    result.add_slot(r, sd);
  else
    findInParentsOf(r, m_addr);
  m_addr->lookup_unmark();
}


void Lookup::findInParentsOf(oop_t r, MapObj* m_addr) {
  // todo optimize time should put parent slots in front -- dmu 1/06
  FOR_EACH_PARENT_SLOT_DESC(m_addr, sd) {
    oop_t parent = sd->contents(r);
    findInObject(parent);
    if (result.rt == Result::foundTwo)
      return;
  }
}


void Lookup::Result::add_slot(oop_t holder, SlotDesc* sd) {
  switch (rt) {
   case foundNone:
    refs[0].sd = sd;
    refs[0].holder = holder;
    rt = foundOne;
    break;
    
   case foundOne:
    if (refs[0].sd == sd)
      break;
    refs[1].sd = sd;
    refs[1].holder = holder;
    rt = foundTwo;
    break;
    
   default:
    fatal("impossible");
  }
}

# include "abstract_interpreter.hh"
# include "abstract_interpreter_inline.hh"

# include "objVectorLayout.hh"
# include "byteVectorLayout.hh"
# include "immediateLayout.hh"
# include "maps.hh"


abstract_interpreter::abstract_interpreter(Oop methodOop)
 : mi(MemoryObjectLayout().mapOf(methodOop)) {
  pc= mi.firstBCI();
  error_msg= NULL;
}


void abstract_interpreter_method_info::init(ByteVectorOop c, ObjVectorOop l) {
     codes_object = c;
  literals_object = l;
  codes           = (unsigned char*) ByteVectorLayout().for_AddressOfIndexableAt(codes_object, 0);
  literals        =  ObjVectorLayout().for_AddressOfIndexableAt(literals_object, 0);
  length_codes    = ByteVectorLayout().indexableSizeOf(   codes_object);
  length_literals =  ObjVectorLayout().indexableSizeOf(literals_object);
  
  if (length_codes == 0) {
    instruction_set = TWENTIETH_CENTURY_INSTRUCTION_SET;
  }
  else {
    char first_code = ByteVectorLayout().for_IndexableAt(codes_object, 0);
    instruction_set =     getOp((u_char)first_code) == INSTRUCTION_SET_SELECTION_CODE
                        ? (InstructionSetKind) getIndex(first_code)
                        : TWENTIETH_CENTURY_INSTRUCTION_SET;
    always_assert( instruction_set == TWENTIETH_CENTURY_INSTRUCTION_SET
                  ||  0 <= instruction_set  &&  instruction_set <= LAST_INSTRUCTION_SET,
                 "bad instruction set");
  }    
}


abstract_interpreter_method_info::abstract_interpreter_method_info(
                                                              MethodMapOop m)  {
  // TODO: I don't know how to check whether it hasCode or not. always_assert(MethodMapLayout().hasCode(m), "cannot interpret data");
  _map_oop       = MethodMapLayout().enclosingMapOopOf(m);
  init(MethodMapLayout().codesOf(m), MethodMapLayout().literalsOf(m));
}    


/*
TODO AAA
void abstract_interpreter_method_info::print_short() {
  lprintf("method_map %#lx, "
         "codes %#lx, length %d,  literals %#lx, length %d\n",
         map(), 
         codes,         length_codes,
         literals,      length_literals);
}


void abstract_interpreter_bytecode_info::print_short() {
  lprintf( "code %d, op %d, index %d\n", code, op, x);
}


void abstract_interpreter_interbytecode_state::print_short() {
  lprintf( "lexical_level %d, index %d,"
          " delegatee %#lx, is_undirected_resend %s,"
          " argument_count %d\n", 
          lexical_level, index, 
          (unsigned long)delegatee, 
          is_undirected_resend ? "true" : "false",
          argument_count);
}




void abstract_interpreter::print_short() {
  lprintf( "pc %d\n", pc);
  mi.print_short();
  bc.print_short();
  is.print_short();
}
*/


void abstract_interpreter::interpret_method() {
  for ( ;  pc < mi.length_codes;  ++pc ) {
    interpret_bytecode();
    if ( get_error_msg() )
      return;
  }
}

void abstract_interpreter::fetch_and_decode_bytecode() {
  bc.decode(mi.codes[pc]);

  // when asserts are turned on, it is illegal to carry
  // a non-zero index during NO_OPERAND_CODE's.
  // So the following predicate is technically not needed.
  // But I don't want illegal programs doing strange things
  // with asserts off, so put it in anyway -- dmu.
  if ( bc.op != NO_OPERAND_CODE ) 
    is.index = (is.index << INDEXWIDTH) | bc.x;
}

#   define interpret(opExpr) \
      pre_ ## opExpr (); \
       do_ ## opExpr (); \
     post_ ## opExpr ();

# define case_op(opName) \
  case opName: interpret(opName)
  
void abstract_interpreter::dispatch_bytecode() {
  switch (bc.op) {
   default: interpret(illegal_code);               break;
   case_op(LEXICAL_LEVEL_CODE);                    break;
   case_op(READ_LOCAL_CODE);                       break;
   case_op(WRITE_LOCAL_CODE);                      break;
   case_op(INDEX_CODE);                            break;
   case_op(LITERAL_CODE);                          break;
   case_op(DELEGATEE_CODE);                        break;
   case_op(ARGUMENT_COUNT_CODE);                   break;
   case_op(SEND_CODE);                             break;
   case_op(IMPLICIT_SEND_CODE);                    break;
   
   case_op(BRANCH_CODE);                           break;
   case_op(BRANCH_TRUE_CODE);                      break;
   case_op(BRANCH_FALSE_CODE);                     break;
   case_op(BRANCH_INDEXED_CODE);                   break;
   
   case     NO_OPERAND_CODE:
    switch (bc.x) {
     default: interpret(illegal_no_operand_code);  break;
     case_op(SELF_CODE);                           break;
     case_op(POP_CODE);                            break;
     case_op(UNDIRECTED_RESEND_CODE);              break;
     case_op(NONLOCAL_RETURN_CODE);                break;
    }
    break;
  }
}


fint abstract_interpreter::get_argument_count() {
  # ifdef GENERATE_ASSERTIONS
  if ( CheckAssertions   &&   mi.instruction_set == TWENTIETH_CENTURY_PLUS_ARGUMENT_COUNT_INSTRUCTION_SET
  &&   is.argument_count != get_selector()->arg_count()) {
    error("argument_count does not match selector's argument count", is.argument_count, get_selector()->arg_count());
    // TODO AAA : fatal2("argument_count %d does not match selector's argument count %d", is.argument_count, get_selector()->arg_count());
  }
  # endif
  return mi.instruction_set == TWENTIETH_CENTURY_PLUS_ARGUMENT_COUNT_INSTRUCTION_SET ? is.argument_count : ByteVectorLayout().argCountOf(get_selector());
}


StringOop abstract_interpreter::get_selector() { 
  Oop s = get_literal();
  return check(check_selector_string, s)  
    ?  s  
    :  ByteVectorLayout().newString("Error: cannot find selector");
}



void abstract_interpreter::do_LITERAL_CODE() { 
 do_literal_code( get_literal()); 
}



bool abstract_interpreter_method_info::verify() {
/* TODO AAA
  if (_map_oop->verify_oop()) {
  } else {
    error1("bad oop in abstract_interpreter_method_info 0x%x", this);
    return false;
  }
  if ( codes           == (unsigned char*) map()->codes()->bytes()
  &&   length_codes    == map()->codes()->length()
  &&   literals        == map()->literals()->objs()
  &&   length_literals == map()->literals()->length() ) {
  } else {
    error1("inconsistency in abstract_interpreter_method_info 0x%x", this);
    return false;
  }
  if (! ObjVectorLayout().isObjVector(literals_object)) {
	error("literals not an objVector");
    // TODO AAA error2("literals_object 0x%x in "
    //       "abstract_interpreter_method_info 0x%x not objVector",
    //       literals_object,
    //       this);
    return false;
  }
  if (! ByteVectorLayout().isByteVector(codes_object)) {
	error("codes not a byteVector");
    // TODO AAA error2("codes_object 0x%x in "
    //       "abstract_interpreter_method_info 0x%x not byteVector",
    //       codes_object,
    //       this);
    return false;
  }
*/
  return true;
}
  

bool abstract_interpreter::verify() {
  return mi.verify();
}


void abstract_interpreter::check_branch_target(Oop p) {
  if (! SmiLayout().hasMyTag(p)) {
    set_error_msg( "branch target must be smallInt");
  }
  else if (   0 <= SmiLayout().valueOf(p)
           &&      SmiLayout().valueOf(p) <= mi.length_codes ) { // == length_codes means return
  }
  else {
    set_error_msg( "bad branch target");
  }
}


intNN abstract_interpreter::get_branch_pc() {
  Oop p = get_literal();
  return check(::check_branch_target, p)  ?  SmiLayout().valueOf(p)  :  0;
}


ObjVectorOop abstract_interpreter::get_branch_vector() {
  Oop p = get_literal();
  return check(check_branch_vector, p)
    ?  p  :  VECTOR_OOP;
}


void check_index_range(abstract_interpreter *ai, Oop) {
  if ( ai->is.index < ai->mi.length_literals ) return;
  ai->set_error_msg( "index out of bounds");
}

void check_selector_string(abstract_interpreter *ai, Oop s) {
  if ( ByteVectorLayout().isString(s) ) return;
  ai->set_error_msg( "selector must be a string");
}

void check_branch_target(abstract_interpreter *ai, Oop p) {
  ai->check_branch_target(p);
}

void check_no_send_modifiers(abstract_interpreter *ai, Oop) {
  ai->set_error_msg( ai->is.check_no_send_modifiers());
}

void check_no_lexical_level(abstract_interpreter *ai, Oop) {
  ai->set_error_msg( ai->is.check_no_lexical_level());
}

void check_no_two_send_modifiers(abstract_interpreter *ai, Oop) {
  ai->set_error_msg( ai->is.check_no_two_send_modifiers());
}

void check_no_operand(abstract_interpreter *ai, Oop) {
  ai->set_error_msg( ai->is.check_no_operand());
}

void check_delegatee(abstract_interpreter *ai, Oop) {
  Oop p = ai->get_literal();
  if ( !ai->error_msg  &&  ! ByteVectorLayout().isString(p))
    ai->set_error_msg( "delegatee must be string"); 
}

void check_no_argument_count(abstract_interpreter *ai, Oop) {
  if (ai->is.argument_count != 0)
    ai->set_error_msg( "should not have argument count before argument count setter");
}

void check_branch_vector(abstract_interpreter *ai, Oop) {
  Oop p= ai->get_literal();
  if (ai->error_msg)  return;
  if (! ObjVectorLayout().isObjVector(p)) {
    ai->set_error_msg( "branch vector must be object vector");
    return;
  }
  for (int i = 0, n = ObjVectorLayout().indexableSizeOf(p); i < n;  ++i) {
    Oop x = ObjVectorLayout().for_IndexableAt(p, i);
    check_branch_target(ai, x);
    if (ai->error_msg) return;
  }
}

void check_for_pop(abstract_interpreter *ai, Oop n) {
  ai->check_for_pop(n);
}

# include "byteCodes.hh"# include "interpreter.hh"
# include "lookup.hh"
# include "prims.hh"
# include "activation.hh"
# include "errorCodes.hh"
# include <stdlib.h>


void runAllTests() {
}

void startSmallSelf() {
  Interpreter::start( The::oop_of(The::vm), The::oop_of(The::start_selector) );
  printf_and_flush("Interpreter exited normally\n");
  exit(0);
}

oop_t Interpreter::current_activation() { return The::oop_of(The::active_context); }

void Interpreter::start(oop_t rcvr, oop_t sel) {
    printf_and_flush("starting interpreter\n");

    Primitives::initialize();
    ErrorCodes::initialize();

    oop_t holder;
    oop_t contents = Lookup::findObject( rcvr, sel, &holder );

    if (!is_method(contents))
      fatal("start method?");
      
    oop_t new_activation, a;
    for ( a = ActivationObj::clone_for(contents, holder, rcvr, NULL, 0, NULL);
          a;
          a = new_activation )
      new_activation = ActivationObj::from(a)->loop(a);
}
# include "activation.hh"
# include "interpreter.hh"
# include "lookup.hh"
# include "byteCodes.hh"
# include "stringObj.hh"
# include "prims.hh"
# include "asserts.hh"
# include "blockObj.hh"





// todo optimize time by including io in sp -- dmu 1/06

# define DECLARE_STACK \
  fint io          = indexableOrigin(); \
  fint sp          = get_sp_quickly(io);  \
  fint sp_limit    = indexableSize(); 

# define push(x)  if (sp >= sp_limit) { \
                    printf("sp is %i, sp_limit is %i, activationMap oop is %i\n", sp, sp_limit, map_oop()); \
                    unimplemented("stack overflow, suggest cloning and ``becoming'' activation"); \
                  } \
                  else write_oop( io +   sp++,  x )
                  
# define pop()    read_oop( io + --sp ) 


oop_t ActivationObj::loop(oop_t this_activation) {

  The::set_active_context( this_activation, this);
  
  DECLARE_STACK;
  smi         bci = get_pc_quickly(io);
  
  ActivationMapObj* m_addr = map_addr();
  
  oop_t           codes_oop    = m_addr->codes();
  ByteVectorObj*  codes_addr   = ByteVectorObj::from(codes_oop);
  char*           codes        = codes_addr->bytes();
  fint            codes_length = codes_addr->indexableSize();



  oop_t         literals       = m_addr->literals();
  ObjVectorObj* literals_addr  = ObjVectorObj::from(literals);
  fint          literals_io    = literals_addr->indexableOrigin();
  
 
  fint index = 0, temp_index;
  # define UC_index ((temp_index = index << INDEXWIDTH), (index = 0), temp_index | bc_index)
  bool undirected_resend = false;
  # define UC_undirected_resend (undirected_resend ? (undirected_resend = false, true) : false)
  
  fint lexical_level = 0;
  
  # define use_lit (literals_addr->read_oop(literals_io + UC_index))
  
  oop_t delegatee = 0, temp_del;
  # define UC_del  ((temp_del = delegatee), (delegatee = 0), temp_del)
  
  fint arg_count = 0, temp_arg_count;
  # define UC_arg_count ((temp_arg_count = arg_count), (arg_count = 0), temp_arg_count)
  
  fint temp_bci;
  // for process pre-emption, stop on backward branches
  // todo optimize should probably just stop every 10 or 100 backward branches, or even just every N bytecodes
  # define set_bci(bci_oop) (temp_bci = value_of_smiOop(assert_smi(bci_oop)), stop = temp_bci < bci, bci = temp_bci)
  
  oop_t self = get_self_quickly(io);
  oop_t rcvr = get_rcvr_quickly(io);
  for ( bool stop = false; !stop; ) {
    if (bci >= codes_length) {
      oop_t r = pop();
      oop_t s = get_sender_quickly(io);
      if ( s ) // it'll be NULL if we're returning from the start method
        ActivationObj::from(s)->remote_push(r);
      // todo optimize time slow; quits this routine just for a return -- dmu 1/06
      return s;
    }
    unsigned char bc = codes[bci++];
    ByteCodeKind kind  = getOp(bc);
    fint         bc_index = getIndex(bc);
    // printf("interpreting a bytecode in activationMap %i, bc is %i, kind is %i, bc_index is %i\n", map_oop(), bc, kind, bc_index);
    switch (kind) {
     default:   fatal("unknown kind of bytecode"); break;
     
     case                   INDEX_CODE:          index = UC_index;     break;
     case           LEXICAL_LEVEL_CODE:  lexical_level = UC_index;     break;
     case          ARGUMENT_COUNT_CODE:      arg_count = UC_index;     break;
  
     case           READ_LOCAL_CODE:   push(local_obj_addr(lexical_level)-> read_arg_or_local(UC_index)      );  lexical_level = 0;               break;
     case          WRITE_LOCAL_CODE:        local_obj_addr(lexical_level)->write_arg_or_local(UC_index, pop());  lexical_level = 0;  push(self);  break;
     
     case          BRANCH_CODE:                                                          set_bci(use_lit);                   break;
     case          BRANCH_TRUE_CODE:     if ( pop() == The::oop_of(The:: true_object))   set_bci(use_lit);  else index = 0;  break;
     case          BRANCH_FALSE_CODE:    if ( pop() == The::oop_of(The::false_object))   set_bci(use_lit);  else index = 0;  break;
     case          BRANCH_INDEXED_CODE:
                                        {
                                         ObjVectorObj* branch_vector_addr = ObjVectorObj::from(assert_objVector(use_lit));
                                         oop_t branch_index_oop = pop();
                                         if ( is_smi(branch_index_oop) ) {
                                            smi branch_index = value_of_smiOop(branch_index_oop);
                                            if (  0 <= branch_index  &&  branch_index < branch_vector_addr->indexableSize()  )   {
                                              oop_t dest_oop = branch_vector_addr->indexable_at(branch_index);
                                              set_bci(dest_oop);
                                            }
                                         }
                                        }
                                        break;
       
     
     case      DELEGATEE_CODE:               delegatee = use_lit;                                     break;


     case LITERAL_CODE:
      {
       oop_t lit = use_lit;
       if (::is_block(lit)) {
         put_sp_quickly(io, sp); // make sure that the sp is stored correctly, because an allocation could trigger a GC
         oop_t cloned_block = BlockObj::clone_block(lit, this_activation);
         ActivationObj* possibly_moved_act_addr = ActivationObj::from(this_activation); // mightHaveScavengedTheActivation
         if (possibly_moved_act_addr != this) {
           possibly_moved_act_addr->remote_push(cloned_block);
           possibly_moved_act_addr->put_pc_quickly( io, bci );
           return this_activation;
         } else {
           push(cloned_block);
         }
       } else {
         push(lit);
       }
      }
      break;
     
     case IMPLICIT_SEND_CODE:
      // fall through
     case SEND_CODE:
     {
      oop_t selector = use_lit;
      if (selector == The::oop_of(The::restart_selector)) {
        put_sp_quickly( io,  sp  = first_stack_offset               );
        put_pc_quickly( io,  bci = get_pc_after_endInit_quickly(io) );
        break;
      }
      put_sp_quickly( io, sp );
      // todo optimize dmu 3/6. This is here for the _Breakpoint primitve to help debugging by storing the PC.
      // But it slows every primitive, sigh.
      put_pc_quickly( io, bci);

      oop_t a = send(kind == IMPLICIT_SEND_CODE, selector, UC_undirected_resend, UC_del, UC_arg_count, this_activation); 
      if (a != this_activation || ActivationObj::from(a) != this) { // mightHaveScavengedTheActivation
        // put_pc_quickly( io, bci); // commented out after I added the put_pc_quickly above, dmu 3/6
        return a;
      }
      sp = get_sp_quickly(io);
     }
     break;
      
     case NO_OPERAND_CODE:
      switch(bc_index) {
       default: fatal("???"); break;
        case               POP_CODE:      pop();                                  break;
        case              SELF_CODE:      push(self);                             break;
        case          END_INIT_CODE:      put_pc_after_endInit_quickly(io, bci);  break;

        case   NONLOCAL_RETURN_CODE:      return nonlocal_return(pop(), rcvr);    break;
        case UNDIRECTED_RESEND_CODE:      undirected_resend = true;               break;
       }
       break;

    }
  }
  put_sp_quickly( io, sp  );
  put_pc_quickly( io, bci );
  return this_activation;
}


void ActivationObj::remote_push(oop_t x) {
  DECLARE_STACK;
  push(x);
  put_sp_quickly( io, sp );
}


oop_t ActivationObj::remote_pop() {
  DECLARE_STACK;
  sp_limit; // unused
  oop_t r = pop();
  put_sp_quickly( io, sp );
  return r;
}


inline static oop_t assert_block(oop_t rcvr) { assert(is_block(rcvr)); return rcvr; }


static oop_t home_frame(oop_t rcvr) {
  return BlockObj::from(assert_block(rcvr))->homeFramePointer();
}


oop_t ActivationObj::send(bool isImplicitSelf, oop_t sel, bool isUndirected, oop_t delegatee, fint arg_count, oop_t this_act) {
  DECLARE_STACK;
  sp_limit; // unused
  sp -= arg_count;
  smi args_offset = sp;
  oop_t* argsp = oop_addr(io + args_offset);
  oop_t  rcvr = isImplicitSelf  ?  get_self_quickly(io)  :  pop();
  put_sp_quickly(io, sp);

  // todo optimize time slow? -- dmu 1/06
  for (smi i = 0;  i < arg_count;  ++i )
    read_barrier(io + args_offset + i);
  
  oop_t new_act = NULL;

  StringObj* sel_addr = StringObj::from(sel);
  
  // I use this printf so often that I'd like to leave it here (commented out). -- Adam, 5/06
  // printf("sending "); sel_addr->string_print(); printf("\n");
  
  if (sel_addr->bytes()[0] == '_')  {
 
    oop_t result = primitive_send(sel, rcvr, argsp, arg_count, this_act, &new_act);
    // todo optimize time mightHaveScavengedTheActivation:
    // Use a boolean flag to tell if the activation has moved?
    // Look for other places tagged mightHaveScavengedTheActivation. -- Adam, 5/06
    if (!is_mark(result)) {
      ActivationObj* possibly_moved_act_addr = ActivationObj::from(this_act);
      possibly_moved_act_addr->remote_push(result);
    }
  }
  else {
    non_primitive_send(rcvr, argsp, arg_count, isImplicitSelf, sel, isUndirected, delegatee, this_act, &new_act);
  }
  return new_act ? new_act : this_act;
}


void ActivationObj::non_primitive_send(oop_t rcvr, oop_t* argsp, fint arg_count, bool isImplicitSelf, oop_t sel, bool isUndirected, oop_t delegatee, oop_t this_act, oop_t* new_actp) {
    // todo optimize time  For block method activations, could cache the result of outermost_lexical_frame()->get_methodHolder()
    //                     (or just outermost_lexical_frame(), so that it could be useful for NLRs too) in the methodHolder indexable. -- Adam, 3/06
    oop_t holder_of_sender_method = (isUndirected || delegatee) ? outermost_lexical_frame()->get_methodHolder() : The::oop_of(The::nil_object);

    oop_t holder;
    SlotDesc* sd;

    oop_t contents = Lookup::findObject( rcvr, sel, &holder, &sd, new_actp, this_act, isUndirected, delegatee, holder_of_sender_method, isImplicitSelf );
    if (contents == badOop) { // lookup failure
    }
    else {
      if (sd->is_assignment()) {
        assert(arg_count == 1);
        sd->set_assignable_contents(holder, argsp[0]);
        remote_push(rcvr);
      }
      else if (is_mem(contents)  &&  MemObj::from(contents)->is_activationMap()) {
        // todo optimize time: pass in the contents address, too, since we've already got it. -- Adam, 5/06
        *new_actp = ActivationObj::clone_for(contents, holder, rcvr, argsp, arg_count, this_act);
      }
      else {
        assert(arg_count == 0);
        remote_push(contents);
      }
    }
}


// todo optimize time finding primtives someday, also optimized fail block creation -- dmu 1/06

oop_t ActivationObj::primitive_send( oop_t sel, oop_t rcvr, oop_t* argsp, fint arg_count, oop_t this_act, oop_t* new_actp) {
  oop_t result = Primitives::invoke(sel, rcvr, argsp, arg_count, this_act, new_actp);
  return result;
}


// shortcut for use with primitive failure
oop_t ActivationObj::clone_for_failure(oop_t rcvr_for_fail, oop_t self_string_for_message, oop_t* argsp, fint arg_count, oop_t sender_act ) {
  oop_t holder, lookup_failed_act;
  oop_t contents = Lookup::findObject(rcvr_for_fail, self_string_for_message, &holder, NULL, &lookup_failed_act, sender_act, false, NULL, The::oop_of(The::nil_object));
  if (contents == badOop)
    return lookup_failed_act;
  return clone_for(contents, holder, rcvr_for_fail, argsp, 2, sender_act);
}

static oop_t self_for_new_activation(ActivationMapObj* contents_addr, oop_t rcvr) {
  return contents_addr->is_outer_activation_map()
             ? rcvr
             : ActivationObj::from(home_frame(rcvr))->get_self();
}

oop_t ActivationObj::clone_for(oop_t method, oop_t method_holder, oop_t rcvr, oop_t* args, fint arg_count, oop_t sender_act) {
  assert(is_mem(method));
  ActivationMapObj* m_addr = ActivationMapObj::from(method);
  assert(m_addr->is_activationMap());
  oop_t self = self_for_new_activation(m_addr, rcvr);
  
  fint partSizes = m_addr->get_activationPartSizes();
  assert( ActivationMapObj::get_argCount(partSizes) == arg_count );
  fint assignable_local_count = ActivationMapObj::get_assignableLocalCount( partSizes );
  fint max_stack_size         = ActivationMapObj::get_maxStackSize(         partSizes );
  smi new_indexable_origin = first_arg_offset + arg_count + assignable_local_count;
  smi new_indexable_size   = first_stack_offset + max_stack_size;
  ActivationObj* addr;
  oop_t a =  ((ObjVectorObj*) The::addr_of( The::vector_proto ))
                  ->clone_and_resize( new_indexable_size, badOop, (ObjVectorObj**)&addr, new_indexable_origin );
  addr->initialize(m_addr, method, method_holder, self, rcvr, args, arg_count, assignable_local_count, max_stack_size, sender_act);
  return a;
}


void ActivationObj::initialize(ActivationMapObj* m_addr,
                               oop_t             activationMap,
                               oop_t             method_holder,
                               oop_t             self,
                               oop_t             rcvr,
                               oop_t*            args,
                               fint              arg_count,
                               fint              assignable_local_count,
                               fint              max_stack_size,
                               oop_t             sender_act) {

  set_mark_bit_for_activation();
  set_map( activationMap );
  fint io = indexableOrigin();
  
  put_sp_quickly(             io,  first_stack_offset     );
  put_pc_quickly(             io,  m_addr->starting_pc()  ); 
  put_self_quickly(           io,  self                   );
  put_rcvr_quickly(           io,  rcvr                   );
  put_sender_quickly(         io,  sender_act             );
  put_methodHolder_quickly(   io,  method_holder          );

  // set args
  for (fint i = 0;  i < arg_count;  ++i)
    write_arg_or_local( first_arg_offset + i,  args[i] );
    
  // set locals
  oop_t nilOop = The::oop_of(The::nil_object);
  for (fint i = 0;  i < assignable_local_count;  ++i)
    write_arg_or_local( first_arg_offset + arg_count + i,  nilOop );
}


ActivationObj* ActivationObj::outermost_lexical_frame() {
  return map_addr()->is_outer_activation_map()
             ? this
             : ActivationObj::from(home_frame(get_rcvr()))->outermost_lexical_frame();
}


oop_t ActivationObj::nonlocal_return(oop_t result, oop_t rcvr) {
  ActivationObj* outermost_frame;
  if (! ::is_block(rcvr)) {
    // todo optimize transmogrify away NLR bytecodes in outer methods
    assert(map_addr()->is_outer_activation_map());
    outermost_frame = this;
  } else {
    // todo unimplemented _OnNonLocalReturn:
    outermost_frame = ActivationObj::from(home_frame(rcvr))->outermost_lexical_frame();
  }
  oop_t sender_frame = outermost_frame->get_sender();
  ActivationObj::from(sender_frame)->remote_push(result);
  return sender_frame;
}


oop_t ActivationObj::vector_of_outgoing_arguments(fint argc, bool isImplicitSelf) {
  ObjVectorObj* vaddr;
  oop_t v = ((ObjVectorObj*) The::addr_of(The::vector_proto))->clone_and_resize(argc, The::oop_of(The::nil_object), &vaddr);
  fint beginning_of_args_offset = indexableOrigin() + get_sp() + (isImplicitSelf ? 0 : 1); // sp has already been cut back
  fint       end_of_args_offset = beginning_of_args_offset + argc;
  for (fint i = beginning_of_args_offset; i < end_of_args_offset; ++i) {
    vaddr->write_indexable_at( i, read_oop(i) );
  }
  return v;
}
# include "objectTable.hh"
# include "objVector.hh"

ObjVectorObj*          Object_Table::ot_addr;
MemObj**               Object_Table::base;
fint                   Object_Table::size;
int32                  Object_Table::timestamp = 0;

void Object_Table::initialize( ObjVectorObj* a ) {
  ot_addr = a;
  base    = (MemObj**) a->indexable_addr(0);
  size    =            a->indexableSize();
}

// todo optimize time: Could just keep lastInvalidEntry as a C++ variable, but then we'd have to make sure to update the slot
//                     in the objectLocator object whenever necessary. -- Adam, 5/06
// optimization: bypass the write barrier
oop_t Object_Table::       lastInvalidEntry(       ) { return          ot_addr->read_oop(lastInvalidEntry_offset);      }
void  Object_Table::record_lastInvalidEntry(oop_t o) { ++timestamp;  *(ot_addr->oop_addr(lastInvalidEntry_offset)) = o; }
# include "map.hh"
# include "stringObj.hh"


SlotDesc*   MapObj::find_slot_with_C_name(const char* n) {
  FOR_EACH_SLOT_DESC(this, sd)
    if (sd->name_addr()->is_equal_to_C_string(n))
      return sd;
  return NULL;
}

// todo optimize time should do either binary search or move to front -- dmu 1/06

SlotDesc*   MapObj::find_slot(oop_t selector) {
  FOR_EACH_SLOT_DESC(this, sd) {
    // error_printf_and_flush("find_slot "); StringObj::from(sd->name())->error_print();
    if (sd->name() == selector)
      return sd;
  }
  return NULL;
}


oop_t  SlotDesc::write_map_slot(MemObj*, oop_t) {
  unimplemented("write_map_slot error?");
  return 0; 
}  


void SlotDesc::print() {
  printf_and_flush("Slot Desc 0x%x: %*s, type 0x%x, value 0x%x, is_obj %d, is_map %d, is_parent %d\n", 
                   this,
                   name_addr()->indexableSize(),
                   name_addr()->indexableOrigin(),
                   type(),
                   value(),
                   is_object(),
                   is_map(),
                   is_parent());
}

void SlotDesc::printAugmentedName() {
  if (is_argument()) printf(":");
  ByteVectorObj::from(name())->string_print();
  if (is_parent())   printf("*");
}

void printMark(oop_t oop) {
  assert(tag(oop) == mark_tag);
  printf("a Mark#%#lx", long(oop)); 
}

void printObjectId( oop_t oop ) {
  assert(is_mem(oop));
  //printf("<id: %ld oop: (0x%lx)>", Object_Table::index_for_oop(oop), oop);
  printf("%ld", (long)Object_Table::index_for_oop(oop));
}

void MapObj::print( oop_t oopForObj ) {
  MapObj* map;
  bool printingAMap = false;
  switch (tag(oopForObj)) {
    case float_tag: unimplemented("floats"); return;
    case  mark_tag: printMark(oopForObj);    return;
    case   smi_tag: 
      printf("%d: ", (value_of_smiOop(oopForObj)));
      map = MapObj::from(The::oop_of(The::smi_map)); 
      break;
    case   mem_tag: 
      printf("<"); printObjectId (oopForObj); printf(">: ");
      MemObj* obj = MemObj::from(oopForObj);
      map = obj->map_addr();
      if (map->is_mapMap()) { printf("map "); printingAMap = true;}
      break;
  }
  
  printf("( ");
  fint length = map->slotDesc_count();
  if (length || map->is_block()) {
    printf("| ");
    FOR_EACH_SLOT_DESC(map, slot) {
      if (!slot->is_assignment()) { //don't print assignment slot names
        slot->printAugmentedName();
        if (slot->is_argument()) 
          printf(" = <arg %ld>", (long)slot->contents(oopForObj));
        else if (slot->is_map()) {
          printf(" = ");
          print_oop(slot->contents(oopForObj));
        } else if (slot->is_assignment()) {
          //do nothing
        } else {
          assert(slot->is_object());
          if (printingAMap) {
            // just printing a map; there isn't an object to print
            printf(" = <offset %ld>", (long)slot->value());
          } else {
            // printing a real object; fetch its slot
            printf(" <- ");
            print_oop(slot->contents(oopForObj));
          }
        }
        printf(". ");
      }
    }
    printf("| ");
  }
  if ( ::is_byteVector( oopForObj ))  ByteVectorObj::print_byteVector(oopForObj);
  if ( ::is_objVector ( oopForObj ))   ObjVectorObj::print_objVector( oopForObj);
//  if ( obj -> is_method()     ) printCode(obj);
  
  printf(")\n");
}

 
void MapObj::print_oop ( oop_t objOop ) {
  switch(tag (objOop)) {
   case   smi_tag:  printf("<a smi, value: %d>", (value_of_smiOop(objOop))); return;
   case float_tag:  printf("<a float  ???>");                                return;// yoda can't handle floats yet
   case  mark_tag:  printMark( objOop );                                     return;
   case   mem_tag:  break;
  }  
  //printf ("[object table index: %d] ", Object_Table::index_for_oop(debugee));
  printf("<");
  if      (objOop ==  The::oop_of (The::true_object))          printf("true");
  else if (objOop ==  The::oop_of (The::false_object))         printf("false");
  else if (objOop ==  The::oop_of (The::nil_object))           printf("nil");
  else if (objOop ==  The::oop_of (The::vector_proto))         printf("vector prototype");
  else if (objOop ==  The::oop_of (The::string_proto))         printf("string prototype");
  else if (objOop ==  The::oop_of (The::set_emptyMarker))      printf("set empty marker");
  else if (objOop ==  The::oop_of (The::set_removedMarker))    printf("set removed marker"); 
  
  else if (objOop ==  The::oop_of (The::blockMap_mapType))                printf("block map");
  else if (objOop ==  The::oop_of (The::objectVectorMap_mapType))         printf("object vector map");
  else if (objOop ==  The::oop_of (The::outerActivationMap_mapType))      printf("outer activation map");
  else if (objOop ==  The::oop_of (The::blockActivationMap_mapType))      printf("block activation map");
    
  else if (objOop ==  The::oop_of (The::smi_map))         printf("smi map");
  else if (objOop ==  The::oop_of (The::float_map))       printf("float map");

  else if (::is_method ( objOop ))        printf("a method");
  else if (::is_block  ( objOop ))        printf("a block");
  
  
 // else if (::is_string ( objOop ))       ByteVectorObj::from(debugee)->string_print(); //todo implement case for immutable strings
 // else if (::is_map    ( objOop ))        printf(" a map "); //

  else if (objOop ==  The::oop_of (The::vm))       printf("the vm");

  else printObjectId(objOop);
  printf(">");
}# include "wellKnownObjects.hh"
# include "stringObj.hh"
# include "lookup.hh"


The::Well_Known_Object The::wks[The::last];

void The::Well_Known_Object::reset_addr() {           addr = MemObj::from(oop); }
void The::Well_Known_Object::set(oop_t x) { oop = x;  addr = MemObj::from( x ); }

void setBootstrapInfo(int oop, int addr) {
  Memory::not_safe_to_do_gc_now();
  Object_Table::initialize( (ObjVectorObj*)addr );
  The::initialize_for_vm(oop_t(oop));
  Memory::safe_to_do_gc_now();
}

void The::set_from(ID dst, ID src, const char* slotName) {
  // printf_and_flush("setting %s\n", slotName);
  set_oop_and_addr_of( dst, addr_of(src)->contents_of_slot_for_bootstrapping(slotName) );
} 


void The::initialize_for_vm(oop_t vm_oop) {
  set_oop_and_addr_of( vm, oop_t(vm_oop));

  set_from( universe,             vm,             "universe");
  set_from( objectLocator,        vm,             "objectLocator");
  set_from( start_selector,       vm,             "startSelector");
  set_from( newGeneration,        universe,       "newGeneration");
  set_from( oldGeneration,        universe,       "oldGeneration");
  set_from( edenSpace,            newGeneration,  "edenSpace");
  set_from( tenuredSpace,         oldGeneration,  "tenuredSpace");
  set_from( canonicalizedStrings, universe,       "canonicalizedStrings");
  set_from( float_map,            universe,       "floatMap");
  set_from( smi_map,              universe,       "smiMap");

  set_from( canonicalizedStrings,      universe,             "canonicalizedStrings");
  set_from( canonicalizedStringVector, canonicalizedStrings, "myKeys");
  
  set_oop_and_addr_of(       true_object, Lookup::findObject( vm_oop, StringObj::slow_intern("true"   ) ) );
  set_oop_and_addr_of(      false_object, Lookup::findObject( vm_oop, StringObj::slow_intern("false"  ) ) );
  set_oop_and_addr_of(        nil_object, Lookup::findObject( vm_oop, StringObj::slow_intern("nil"    ) ) );
  set_oop_and_addr_of(      vector_proto, Lookup::findObject( vm_oop, StringObj::slow_intern("vector" ) ) );
  set_oop_and_addr_of(      string_proto, Lookup::findObject( vm_oop, StringObj::slow_intern("string" ) ) );
  set_oop_and_addr_of(             lobby, Lookup::findObject( vm_oop, StringObj::slow_intern("lobby"  ) ) );

  
  set_oop_and_addr_of( set_emptyMarker,   Lookup::findObject( oop_of(canonicalizedStrings), StringObj::slow_intern(  "emptyMarker") ) );
  set_oop_and_addr_of( set_removedMarker, Lookup::findObject( oop_of(canonicalizedStrings), StringObj::slow_intern("removedMarker") ) );
  
  set_oop_and_addr_of( size_string,  StringObj::slow_intern("size") );
  
  // It is safe to use StringObj::intern after this point, because the well-known objects
  // needed for the correct functioning of the string table have been found. -- Adam 2/06
  // todo: Have a little flag indicating this, so that if someone calls intern before
  // this point it'll just call slow_intern?
  set_oop_and_addr_of(                int32_proto, Lookup::findObject( vm_oop, StringObj::intern("int32"  ) ) );
  set_oop_and_addr_of(              process_proto, Lookup::findObject( vm_oop, StringObj::intern("process") ) );
  
  set_oop_and_addr_of(    objectVectorMap_mapType, StringObj::intern(      "objVectorMap") );
  set_oop_and_addr_of(           blockMap_mapType, StringObj::intern(          "blockMap") ); 
  set_oop_and_addr_of( outerActivationMap_mapType, StringObj::intern("outerActivationMap") ); 
  set_oop_and_addr_of( blockActivationMap_mapType, StringObj::intern("blockActivationMap") );
  set_oop_and_addr_of(             mapMap_mapType, StringObj::intern(            "mapMap") );



  set_oop_and_addr_of(           restart_selector, StringObj::intern("_Restart") );

  set_oop_and_addr_of( mirrors_namespace, Lookup::findObject( Lookup::findObject( vm_oop, StringObj::intern("yoda") ), StringObj::intern("mirrors") ) );
  
  // XXXX unimplemented("set assignment primitive");

  assert_all_valid();
}


void The::assert_all_valid() {
  for (ID i = vm;  i < last;  i = ID(int(i) + 1))
    wks[i].addr->assert_valid();
}


void The::moved_some_objects() {
  for (ID i = vm;  i < last;  i = ID(int(i) + 1))
    reset_addr_of(i);
}
# include "objVector.hh"
# include "utils.hh"

oop_t ObjVectorObj::clone() {
  return clone_and_resize(indexableSize(), 0, NULL);
}


void ObjVectorObj::print_objVector(oop_t x) {
  printf("object vector: {");
  if (MemObj::from(x)->is_map()) {
    printf("...");
  } else {
    ObjVectorObj* obj = ObjVectorObj::from(x);
    bool first = true;
    oop_t* p    =     (oop_t*)(obj -> indexableOrigin());
    oop_t* end  = p + obj -> indexableSize();
    oop_t* end2 = p + VectorPrintLimit < end ? p + VectorPrintLimit : end;
    for (; p < end2; p ++) {
      if (first) first = false;
      else printf(", ");
      MemObj::print_oop(*p);
    }
    if (end != end2) {
      printf(", ... (%d more elements) ", end - end2);
    }
  }
  printf("} ");
}

oop_t ObjVectorObj::clone_and_resize(smi new_obj_indexable_size, oop_t fill_if_not_badOop, ObjVectorObj** addrp, smi new_obj_indexable_origin_or_zero) {
  smi  io = indexableOrigin();
  smi  new_obj_indexable_origin  =   new_obj_indexable_origin_or_zero == 0  ?  io  :  new_obj_indexable_origin_or_zero;

  fint nOops = new_obj_indexable_origin + new_obj_indexable_size;
  
  ObjVectorObj* a;
  oop_t new_obj = Memory::allocate_oops(nOops, (MemObj**) &a);
  
  if (new_obj == badOop)
    unimplemented("out of memory");
  
  oop_t* new_obj_addr = (oop_t*)a;
  
  smi  sz = indexableSize();
  
  oop_t*  start_of_new_obj_indexables = new_obj_addr                +         new_obj_indexable_origin ;
  oop_t*  end_of_indexables_copy      = start_of_new_obj_indexables + min(sz, new_obj_indexable_size  );
  oop_t*  past_new_obj                = start_of_new_obj_indexables +         new_obj_indexable_size   ;
  
  assert(mark_offset == 0);
  a->set_mark(markOop_for_clone(mark_oop(), new_obj));
  
  {
    bool shouldFill = fill_if_not_badOop != badOop;
    assert(shouldFill = true); // skip badOop fill if no asserts

    oop_t *srcp = ((oop_t*)this) + mark_offset + 1,
          *dstp = new_obj_addr   + mark_offset + 1;

    if (io != new_obj_indexable_origin) {
      oop_t*  end_of_named_slots_copy = new_obj_addr + min(io, new_obj_indexable_origin);
      
                       while ( dstp < end_of_named_slots_copy     )  *dstp++ = *srcp++;
      if (shouldFill)  while ( dstp < start_of_new_obj_indexables )  *dstp++ = fill_if_not_badOop;
      
      a->set_indexableOrigin(new_obj_indexable_origin);
      srcp = ((oop_t*)this) + io;
    }

                     while ( dstp < end_of_indexables_copy      )  *dstp++ = *srcp++;
    if (shouldFill)  while ( dstp < past_new_obj                )  *dstp++ = fill_if_not_badOop;
  }
  
  a->set_indexableSize(new_obj_indexable_size);
  if (addrp)  *addrp = a;
  return new_obj;
}
# include "string.h"
# include "utils.hh"
# include "byteVector.hh"
# include "stringObj.hh"



void ByteVectorObj::range_set(smi from, smi length, char* new_values) {
    if ( length == 0 )
      return;

    assert( length > 0 );
    assert( range_check( from              ));
    assert( range_check( from + length - 1 ));
//  todo alex somehow ensure that the length of new values is at least length in size        
    for ( char *oc = &(bytes()[from]),  *nc = new_values; 
              oc < &(bytes()[from + length]); 
             *oc++ = *nc++) {}
}



bool ByteVectorObj::is_equal_to_C_string(const char* cString) {
  // There's no '\0' at the end of the Self string, so be careful.
  smi   n = indexableSize();
  char* p = bytes();
  
  const char* cp = cString;
  
  for (  ;  n--;  ++p, ++cp ) {
    char c = *cp;
    if ( c == '\0'  ||  c != *p )
      return false;
  }
  return *cp == '\0';
}


bool ByteVectorObj::is_equal_to_bytes_at(const char* other_bytes, fint other_bytes_size) {
  smi   n = indexableSize();
  if (n != other_bytes_size)
    return false;
  
  const char* p = bytes();
  
  const char* cp = other_bytes;
  
  while (n--)
    if ( *p++ != *cp++ )
      return false;

  return true;
}


//aaa todo check if can factor this and is_equal_to_C_string
bool ByteVectorObj::ends_with_C_string(const char* cString, int size) {
  // passing in a size is just an optimization
  // size should not include null character
  if (size == -1)            size  = length_of_C_string(cString);
  else               assert( size == length_of_C_string(cString));
    
  if (size > indexableSize())  return false; //optimization
  
  for (const char *sp = bytes() + indexableSize(),
                  *cp = cString +          size;
             cString <= cp; )  {
    if ( *cp-- != *sp-- )
      return false;
  }
  return true;
}


void ByteVectorObj::copy_to_C_string(char* s, unsigned int size) {
  unsigned int n = min(indexableSize(), size - 1);
  strncpy(s, bytes(), n);
  s[n] = '\0';
}


oop_t ByteVectorObj::clone_and_resize(smi new_indexable_size, char fill, ByteVectorObj** addrp, bool shouldFill) {
  ByteVectorObj* new_addr;
  char* new_bytes;
  oop_t new_bv = clone_oops_and_allocate_bytes(indexableOrigin(), new_indexable_size, (MemObj**) &new_addr, &new_bytes);
  
  new_addr->set_indexableSize(new_indexable_size);
  copy_bytes_to(new_bytes, new_indexable_size, fill, shouldFill);
  if (addrp) *addrp = new_addr;
  
  assert(new_addr->is_byteVector());
  return new_bv;
}

void ByteVectorObj::copy_bytes_to(char* new_bytes, smi new_indexable_size, char fill, bool shouldFill) {
  char  *srcp = bytes();
  char  *dstp = new_bytes;

  char* end_before_fill = dstp + min(indexableSize(), new_indexable_size);
  char* end_for_fill    = dstp + new_indexable_size ;
  
                    while ( dstp <  end_before_fill )  *dstp++ = *srcp++;
  if (shouldFill)   while ( dstp <  end_for_fill    )  *dstp++ = fill;
}


oop_t ByteVectorObj::clone_for_C_string(const char* cString, ByteVectorObj** addrp, fint length) {
  fint n = (length == -1) ? length_of_C_string(cString)
                          : length;
  
  ByteVectorObj* addr;
  oop_t new_bv = clone_and_resize(n, 0, &addr, false);
  if (addrp) *addrp = addr;
  
  const char *srcp = cString, *endp = cString + n;
  char *dstp = addr->bytes();
  while (srcp != endp)
    *dstp++ = *srcp++;
  
  return new_bv;
}

oop_t ByteVectorObj::clone_for_int32(int32 i, ByteVectorObj** addrp) {
  ByteVectorObj* addr;
  oop_t new_bv = clone_and_resize(sizeof(int32), 0, &addr, false);
  if (addrp) *addrp = addr;

  *((int32*) bytes()) = i;

  return new_bv;
}

void ByteVectorObj::print_byteVector(oop_t oop) {
  printf("byte array: {");
  if (MemObj::from(oop)->is_map()) {
    printf("...");
  } else {
    ByteVectorObj* obj = ByteVectorObj::from(oop);
    bool first = true;
    char* p =         obj->bytes();
    char* end = p +   obj->indexableSize();
    char* end2 = p + VectorPrintLimit < end ? p + VectorPrintLimit : end;
    for (; p < end2; p ++) {
      if (first) first = false;
      else printf(", ");
      printf("%ld", long(*p));
    }
    if (end != end2) {
      printf(", ... (%d more elements) ", end - end2);
    }
  }
  printf("} ");
}
# include "object.hh"
# include "map.hh"
# include "stringObj.hh"
# include "activation.hh"


MapObj* MemObj::map_addr() {
  return MapObj::from(map_oop());
}


oop_t MemObj::contents_of_slot(oop_t n) {
  SlotDesc* sd = map_addr()->find_slot(n);
  assert(sd);
  return sd->contents(this);
}


oop_t MemObj::contents_of_slot_for_bootstrapping(const char* n) {
  SlotDesc* sd = map_addr()->find_slot_with_C_name(n);
  assert(sd);
  return sd->contents(this);
}  


void MemObj::set_contents_of_slot(oop_t n, oop_t x) {
  SlotDesc* sd = map_addr()->find_slot(n);
  assert(sd);
  sd->set_contents(this, x);
}


fint MemObj::total_size_in_oops() {
  if (is_activation()) {
    return ((ActivationObj*)this)->total_size_in_oops();
  } else if (is_byteVector()) {
    return ((ByteVectorObj*)this)->total_size_in_oops();
  } else {
    oop_t* start = (oop_t*)this;
    oop_t*   end = start;
    while ( !is_mark(*++end) ) {}
    return end - start;
  }
}


bool MemObj::is_objVector()  { oop_t mt = map_addr() -> mapType();
                               return mt == The::oop_of(The::objectVectorMap_mapType)
                                   || mt == The::oop_of(The::mapMap_mapType)
                                   || mt == The::oop_of(The::blockActivationMap_mapType)
                                   || mt == The::oop_of(The::outerActivationMap_mapType); }
bool MemObj::is_block()      { return (map_addr() -> mapType()) == The::oop_of(The::blockMap_mapType);        }

bool MemObj::is_map()        { return  map_addr() -> is_mapMap(); }


bool MemObj::is_in_space(MemObj* space_addr) { return Memory::is_address_in_space( (oop_t*)this, space_addr ); }


oop_t MemObj::clone(MemObj** addrp) {
  if (is_byteVector()) {
    untested("calling regular clone() on a byte vector");
    ByteVectorObj* this_bv = (ByteVectorObj*) this;
    return this_bv->clone_and_resize(this_bv->indexableSize(), 0, (ByteVectorObj**)addrp);
  }
  
  return clone_oops_and_allocate_bytes(total_size_in_oops(), NO_BYTES_PART, addrp);
}


oop_t  MemObj::markOop_for_clone( oop_t orig_mark, oop_t new_obj ) {
  return adjust_mark_oop_to_encode_oid( Memory::adjust_markOop_for_clone(orig_mark, new_obj), value_of_markOop(new_obj) );
}


// todo cleanup refactor this and the ObjVectorObj clone
oop_t MemObj::clone_oops_and_allocate_bytes(smi nOops, smi nBytes, MemObj** addrp, char** bytesp) {
  oop_t* start_of_copy = (oop_t*)this;
  oop_t*   end_of_copy = start_of_copy + nOops;
  
  MemObj* a;
  oop_t new_obj = Memory::allocate_oops_and_bytes(nOops, nBytes, &a, bytesp);
  
  if (new_obj == badOop)
    unimplemented("out of memory");
  
  oop_t* new_obj_addr = (oop_t*)a;
  
  assert(mark_offset == 0);
  a->set_mark(markOop_for_clone(mark_oop(), new_obj));
  
  for (oop_t *dstp =  new_obj_addr + mark_offset + 1,
             *srcp = start_of_copy + mark_offset + 1;
       srcp < end_of_copy;
       *dstp++ = *srcp++)  {}
  
  assert(a->oop() == new_obj);
  if (addrp)  *addrp = a;
  return new_obj;
}


bool is_method(oop_t x) {
  return  is_mem(x)  &&  MemObj::from(x)->is_activationMap();
}

// todo optimize time should be fast -- dmu 1/06
bool is_blockMethod(oop_t x) {
  unimplemented("is_blockMethod");
  return false;
}  

bool is_boolean(oop_t x) {
  return x == The::oop_of(The:: true_object)
      || x == The::oop_of(The::false_object);
}

bool is_byteVector(oop_t x) {
  return is_mem(x)  &&  MemObj::from(x)->is_byteVector();
}

bool is_objVector(oop_t x) {
  return is_mem(x)  &&  MemObj::from(x)->is_objVector();
}

bool is_block(oop_t x) {
  return is_mem(x)  &&  MemObj::from(x)->is_block();
}


// todo cleanup _Print get printing to work right. Dispose of debug_print

void print(oop_t x) {
  char buf[1000];
  get_print_string(x, buf, sizeof(buf));
  printf_and_flush("%s\n", buf);
} 


void MemObj::debug_print(oop_t debugee) {
  const int MaxLen = 30;
  fint t = tag(debugee);
  char s[MaxLen]; // = NEW_RESOURCE_ARRAY(char, MaxLen);
  if (t == smi_tag) {
    sprintf(s, "a smi%ld", (long)(value_of_smiOop(debugee)));
  } else if (t == mem_tag) {
    if      (debugee ==  The::oop_of (The::true_object))          sprintf(s, "true");
    else if (debugee ==  The::oop_of (The::false_object))         sprintf(s, "false");
    else if (debugee ==  The::oop_of (The::nil_object))           sprintf(s, "nil");
    else if (debugee ==  The::oop_of (The::vector_proto))         sprintf(s, "vector prototype");
    else if (debugee ==  The::oop_of (The::string_proto))         sprintf(s, "string prototype");
    else if (debugee ==  The::oop_of (The::set_emptyMarker))      sprintf(s, "set empty marker");
    else if (debugee ==  The::oop_of (The::set_removedMarker))    sprintf(s, "set removed marker"); 
    
    else if (debugee ==  The::oop_of (The::blockMap_mapType))
      sprintf(s, "block map");
    else if (debugee ==  The::oop_of (The::objectVectorMap_mapType))
      sprintf(s, "object vector map");
    else if (debugee ==  The::oop_of (The::outerActivationMap_mapType))
      sprintf(s, "outer activation map");
    else if (debugee ==  The::oop_of (The::blockActivationMap_mapType))
      sprintf(s, "block activation map");
      
    else if (debugee ==  The::oop_of (The::smi_map))   sprintf(s, "smi map");
    else if (debugee ==  The::oop_of (The::float_map))  sprintf(s, "float map");
    
    else if (MemObj::from(debugee) -> is_byteVector()) 
      ByteVectorObj::from(debugee)->string_print();
    else sprintf(s, "??? %#lx", long(debugee));
  } else if (t == float_tag) {
    sprintf(s, "yoda can't handle floats yet");
  } else {
    assert(t == mark_tag);
    sprintf(s, "Mark#%#lx", long(debugee));
  }
  printf_and_flush("%s", s);
}

void MemObj::print(oop_t x) {
  MapObj::print(x);
}


void MemObj::print_oop( oop_t x) {
  if (x == badOop)  
    printf("badOop");
  else 
    MapObj::print_oop(x);
}

void get_print_string(oop_t x, char* s, int size) {
  // toto unimplemented incomplete -- dmu 1/06
  switch (tag(x)) {
    default: fatal("???");
    case float_tag:  sprintf(s, "a float 0x%x", x);                return;
    case   smi_tag:  sprintf(s, "a smi %d", value_of_smiOop(x));   return;
    case  mark_tag:  sprintf(s, "a markOop 0x%x", x);              return;
    case   mem_tag:  break;
  }
  if (size < length_of_C_string(s))  fatal("string overflow");
  size -= length_of_C_string(s);
  if (is_byteVector(x)) {
    ByteVectorObj::from(x)->copy_to_C_string(s, size);
  }
  else if (is_method(x)) {
   printf_and_flush("object vector printing not implemented\n");
//  }else if (is_blockMethod(x)) {
//   printf_and_flush("block method printing not implemented\n");
  } else if (is_block(x)) {
   printf_and_flush("block printing not implemented\n");
  } else if (is_objVector(x)) {
    smi id =  Object_Table::index_for_oop(x);
    ObjVectorObj* x_ov = ObjVectorObj::from(x);
    printf_and_flush("object vector [ID: %d, size: %d], contents:\n", id , x_ov->indexableSize());
    //x_ov->print();
    printf_and_flush("done contents [ID: %d].\n", id);
  } else {
    printf_and_flush("printing not implemented for ??? object type\n");
  }
   
  
  if (size < length_of_C_string(s))  fatal("string overflow");
}

# include <stdio.h>
# include "stringObj.hh"
# include "utils.hh"
# include "objVector.hh"

// must be kept in sync with traits smallInt hash
inline smi hash_of_smi(smi i) { return i; }

// must be kept in sync with klein virtualMachines abstractVM stringComparisonMixin hashElement:
smi hash_of_string(const char* str, smi size) {
  smi h = size;
  const char *x = str, *end = str + size, *end_minus_three = end - 3;
  while (x <= end_minus_three) {
    h = h ^ hash_of_smi( *x++       )
          ^ hash_of_smi( *x++ <<  8 )
          ^ hash_of_smi( *x++ << 16 );
  }
  if (! (x < end)) return h;  h = h ^ hash_of_smi( *x++      );
  if (! (x < end)) return h;  h = h ^ hash_of_smi( *x++ <<  8);
  
  return h;
}


smi StringObj::hash_for_comparison() {
  return hash_of_string( bytes(), indexableSize() );
}


// must be kept in sync with traits hashTableSetOrDictionary indexOf:IfPresent:IfAbsent:FirstRM:
smi find_string_in_table_past_first_RM(const char* str, smi str_size, smi firstRM, smi* indexp, ObjVectorObj* selfStrings, smi io, smi sz, oop_t emptyMarker, oop_t removedMarker) {
  bool firstWrap = true;
  smi i = firstRM + 1;
  
  while (true) {
    oop_t k;
    if (i < sz) {
      k = selfStrings->read_oop(io + i);
    } else {
      if (firstWrap) {
        firstWrap = false;
        i = 0;
        k = selfStrings->read_oop(io + 0);
      } else {
        *indexp = firstRM;
        return false;
      }
    }
    
    if (  emptyMarker == k)  { *indexp = firstRM;  return false; }
    
    if (removedMarker != k  &&  ::is_byteVector(k)  &&  ByteVectorObj::from(k)->is_equal_to_bytes_at(str, str_size)) {
      selfStrings->write_oop(firstRM, k                           );
      selfStrings->write_oop(i,       The::oop_of(The::nil_object));
      *indexp = firstRM;
      return true;
    }
    
    ++i;
  }
}


// must be kept in sync with traits hashTableSetOrDictionary unsafe_indexOf:IfPresent:IfAbsent:
bool find_string_in_table(const char* str, smi str_size, smi* indexp) {
  ObjVectorObj* selfStrings = (ObjVectorObj*) The::addr_of(The::canonicalizedStringVector);
  smi io   =  selfStrings->indexableOrigin();
  smi sz   =  selfStrings->indexableSize();
  
  smi i = (sz - 1) & hash_of_string(str, str_size);

  oop_t   emptyMarker = The::oop_of(The::set_emptyMarker  );
  oop_t removedMarker = The::oop_of(The::set_removedMarker);
  bool firstWrap = true;
  while (true) {
    oop_t k;
    if (i < sz) {
      k = selfStrings->read_oop(io + i);
    } else {
      if (firstWrap) {
        firstWrap = false;
        i = 0;
        k = selfStrings->read_oop(io + 0);
      } else {
        fatal("table should never get this full!");
      }
    }
    
    if (  emptyMarker == k)  { *indexp = i;  return false; }
    if (removedMarker == k)  return find_string_in_table_past_first_RM(str, str_size, i, indexp, selfStrings, io, sz, emptyMarker, removedMarker);
    
    if (::is_byteVector(k) && ByteVectorObj::from(k)->is_equal_to_bytes_at(str, str_size)) { *indexp = i;  return true; }
    
    ++i;
  }
}



oop_t StringObj::slow_intern(const char* cString) {
  ObjVectorObj* selfStrings = (ObjVectorObj*) The::addr_of(The::canonicalizedStringVector);
  smi i =      selfStrings->indexableOrigin();
  smi n =  i + selfStrings->indexableSize();
  for ( ; i < n;  ++i ) {
    oop_t x = selfStrings->read_oop(i);
    if ( ::is_byteVector(x) && ByteVectorObj::from(x)->is_equal_to_C_string(cString) ) {

      return x;
    }
  }
  error_printf_and_flush("could not find \'%s\' in the string table; slow_intern cannot add it\n", cString);
  return 0; // shut up the compiler
}


oop_t StringObj::intern(const char* cString, fint length) {
  ObjVectorObj* selfStrings = (ObjVectorObj*) The::addr_of(The::canonicalizedStringVector);
  smi io =      selfStrings->indexableOrigin();
  
  smi i;
  fint n = (length == -1) ? length_of_C_string(cString)
                          : length;
  
  bool found = find_string_in_table(cString, n, &i);
  
  if (found)
    return selfStrings->read_oop(io + i);
  
  // printf("adding a new string \'%*s\' at index %i\n", n, cString, i);
    
  ByteVectorObj* new_string_addr;
  oop_t new_string = ((StringObj*) The::addr_of(The::string_proto))->clone_for_C_string(cString, &new_string_addr, n); // could use any canonical string as the prototype
  
  selfStrings->write_oop(io + i, new_string);
  
  smi old_size = value_of_smiOop(assert_smi(The::addr_of(The::canonicalizedStrings)->contents_of_slot(The::oop_of(The::size_string))));
  smi new_size = old_size + 1;
  The::addr_of(The::canonicalizedStrings)->set_contents_of_slot(The::oop_of(The::size_string), smiOop_for_value(new_size));
  
  return new_string;
}
# include "utils.hh"
# include "small_self_types.hh"

// todo cleanup use standard libaries from here 

bool is_id_alpha( char c ) {
  // todo optimize could use a character table lookup -- dmu
  return  'a' <= c  &&  c <= 'z'
      ||  'A' <= c  &&  c <= 'Z'
      ||   c == '_';
}

int length_of_C_string(const char* s ) {
  int i = 0;
  for (const char* c = s;  *c != '\0';  ++c)
    ++i;
  return i;
}


int arg_count_of_string(const char* s, int len )  {
  char c = *s;

  if ( !is_id_alpha(c) )        return 1;

  if ( s[ len - 1 ]  !=  ':' )  return 0; // an optimization

  fint argc = 1;
  for ( const char* ss  =  s + len - 3;  // last is :, next-to-last is alpha
		    ss > s;              // do not need to look at first one
		  --ss )
         if (*ss == ':')  ++argc;

  return argc;
}# include <stdlib.h>
# include "asserts.hh"

void fatal_handler(const char* file, int line, const char* msg) {
  error_printf_and_flush("fatal %s:%d %s", file, line, msg);
  abort();
}  


void untested_handler(const char* file, int line, const char* msg) {
  error_printf_and_flush("untested %s:%d %s", file, line, msg);
}  


void unimplemented_handler(const char* file, int line, const char* msg) {
  error_printf_and_flush("unimplemented %s:%d %s", file, line, msg);
  abort();
}  # include "universe.hh"
# include "generation.hh"
# include "space.hh"

NewGeneration* Universe::newGeneration() {
  return new NewGeneration(contentsOfSlotNamed("newGeneration"));
}

# define GENERATIONS_DO(x, body) NewGeneration* x = newGeneration(); body
# define SPACES_DO(x, body) EdenSpace* x = newGeneration()->edenSpace(); body

BPRef Universe::allocateBytes(int nBytes) {
  return newGeneration()->allocateBytes(nBytes);
}

Address Universe::allocateOops(int nOops) {
  return newGeneration()->allocateOops(nOops);
}

void Universe::verify(Verifier* aVerifier) {
  verifyGenerations         (aVerifier);
  verifyCanonicalizedStrings(aVerifier);
}

void Universe::verifyCanonicalizedStrings(Verifier* aVerifier) {
  // TODO: implement this.
}

void Universe::verifyGenerations(Verifier* aVerifier) {
  GENERATIONS_DO(g,
    g->verify(aVerifier);
  )
}

bool Universe::isBytesPartAddressInBounds(Address bpAddr) {
  SPACES_DO(s,
    if ( s->isBytesPartAddressInBounds(bpAddr) ) return true;
  )
  return false;
}

bool Universe::isObjectAddressInBounds(Address addr) {
  SPACES_DO(s,
    if ( s->isObjectAddressInBounds(addr) ) return true;
  )
  return false;
}
# include "maps.hh"

# include "allNonMapLayouts.hh"
# include "verifier.hh"
# include "slotType.hh"

# include "maps.incl.impl.hh"

int  MapLayout::slotDescSize    () {return 3;} //TODO: generate this from Self
int  MapLayout::scalarValueCount() {return 2;} //TODO: generate this from Self
Oop  MapLayout::basicAt    (Oop mapOop, int i       ) {return ObjVectorLayout().for_IndexableAt(mapOop, i);}
void MapLayout::basicAt_Put(Oop mapOop, int i, Oop x) {ObjVectorLayout().for_IndexableAt_Put(mapOop, i, x);}
int  MapLayout::basicSizeOf(Oop mapOop) {return ObjVectorLayout().indexableSizeOf(mapOop);}
int  MapLayout::     sizeOf(Oop mapOop) {return (basicSizeOf(mapOop) - scalarValueCount()) / slotDescSize();}

const static int mapTypeIndex = 0; //TODO: generate this from Self
Oop MapLayout::mapTypeOopOf(Oop mapOop) {return for_IndexableAt(mapOop, mapTypeIndex);}

//TODO: generate this from Self?
int MapLayout::nameIndexAt(int i) {return scalarValueCount() + (i * slotDescSize())    ;}
int MapLayout::typeIndexAt(int i) {return scalarValueCount() + (i * slotDescSize()) + 1;}
int MapLayout::dataIndexAt(int i) {return scalarValueCount() + (i * slotDescSize()) + 2;}

Oop MapLayout::nameOopAt(Oop mapOop, int i) {return basicAt(mapOop, nameIndexAt(i));}
Oop MapLayout::typeOopAt(Oop mapOop, int i) {return basicAt(mapOop, typeIndexAt(i));}
Oop MapLayout::dataOopAt(Oop mapOop, int i) {return basicAt(mapOop, dataIndexAt(i));}

int MapLayout::typeAt(Oop mapOop, int i) { return SmiLayout().valueOf(typeOopAt(mapOop, i)); }

void MapLayout::at_PutNameOop(Oop mapOop, int i, Oop v) {basicAt_Put(mapOop, nameIndexAt(i), v);}
void MapLayout::at_PutTypeOop(Oop mapOop, int i, Oop v) {basicAt_Put(mapOop, typeIndexAt(i), v);}
void MapLayout::at_PutDataOop(Oop mapOop, int i, Oop v) {basicAt_Put(mapOop, dataIndexAt(i), v);}

int MapLayout::indexOfSlotWithSelector(Oop mapOop, StringOop desiredSlotNameOop) {
  if (mapOop == 0) return INVALID_OOP;
  int s = sizeOf(mapOop);
  for (int i = 0; i < s; i++) {
    Oop slotNameOop = nameOopAt(mapOop, i);
    if (desiredSlotNameOop == slotNameOop) {return i;}
  }
  return INVALID_MAP_INDEX;
}

int MapLayout::indexOfSlotNamed(Oop mapOop, char* n) {
  if (mapOop == 0) return INVALID_OOP;
  int s = sizeOf(mapOop);
  int nSize = lengthOfCString(n);
  for (int i = 0; i < s; i++) {
    Oop slotNameOop = nameOopAt(mapOop, i);
    if (isCStringEqualToKleinString(nSize, n, slotNameOop)) {return i;}
  }
  return INVALID_MAP_INDEX;
}

Oop MapLayout::contentsOfSlotAt(Oop mapOop, Oop oop, int i) {
  if (i == INVALID_MAP_INDEX) return INVALID_OOP;
  Oop mapData = dataOopAt(mapOop, i);
  if (! isObjectSlot(typeAt(mapOop, i))) {
    return mapData;
  } else {
    return MemoryObjectLayout().for_At(oop, SmiLayout().valueOf(mapData));
  }
}

Oop MapLayout::contentsOfSlotNamed(Oop mapOop, Oop oop, char* n) {
  int i = indexOfSlotNamed(mapOop, n);
  return contentsOfSlotAt(mapOop, oop, i);
}

FailureCode MapLayout::setContentsOfSlotAt(Oop mapOop, Oop oop, Oop newValueOop, int i) {
  if (i == INVALID_MAP_INDEX) return FAILED;
  if (! isObjectSlot(typeAt(mapOop, i))) {
    at_PutDataOop(mapOop, i, newValueOop);
  } else {
    MemoryObjectLayout().for_At_Put(oop, SmiLayout().valueOf(dataOopAt(mapOop, i)), newValueOop);
  }
  return SUCCEEDED;
}

FailureCode MapLayout::setContentsOfSlotNamed(Oop mapOop, Oop oop, Oop newValueOop, char* n) {
  int i = indexOfSlotNamed(mapOop, n);
  return setContentsOfSlotAt(mapOop, oop, newValueOop, i);
}

void MapLayout::verifyObject(Oop mapOop, Oop o, int nextIndex, Verifier* aVerifier) {
  myLayout()->verifyObject_UpTo_With(mapOop, o, nextIndex, aVerifier);
}
# include "objectLayout.hh"
# include "tag.hh"
# include "memoryObjectLayout.hh"


Oop ObjectLayout::mapOf( Oop o ) {
  TagValue t = Tag::tagOfOop(o);
  switch (t) {
    case Tag::mem  : { return MemoryObjectLayout().mapOf(o); }
    case Tag::smi  : error("smi"              ); break;
    case Tag::flt  : error("float"            ); break;
    case Tag::mark : error("smi"              ); break;
    default:         error("unknown tag value");
  }
  return 0;
}
# include "layout.hh"
# include "blockLayout.hh"
# include "immediateLayout.hh"
# include "headerFields.hh"

intNN BlockLayout ::     homeFramePointerOf( Oop o           )  { return homeFramePointerField()->   valueFor( o,     this ); }
void  BlockLayout :: set_homeFramePointerOf( Oop o, intNN fp )  {        homeFramePointerField()->setValueFor( o, fp, this ); }

void  BlockLayout :: zap_homeFramePointerOf( Oop o           )  { set_homeFramePointerOf( o, SmiLayout().encode(0) ); }

AbstractHeaderField* BlockLayout :: lastField()  { return homeFramePointerField(); }
# include "tag.hh"

# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "bigHeader.hh"
# include "byteVectorLayout.hh"
# include "headerFields.hh"
# include "bytesPartLayout.hh"
# include "verifier.hh"
# include "universe.hh"
# include "theVM.hh"
# include "immediateLayout.hh"
# include "markLayout.hh"
# include "smallVMDeclarationsOrStubs.hh"

AbstractHeaderField* ByteVectorLayout::lastField() { return bytesPartRefField(); }

BPRef ByteVectorLayout ::     bytesPartRefOf( Oop o              ) { return bytesPartRefField()->   valueFor(o,        this); }
void  ByteVectorLayout :: set_bytesPartRefOf( Oop o, BPRef bpRef ) {        bytesPartRefField()->setValueFor(o, bpRef, this); }

int  ByteVectorLayout ::     indexableSizeOf(Oop o       ) { return BytesPartLayout().    indexableSizeOfBytesPart(bytesPartRefOf(o)   ); }
void ByteVectorLayout :: set_indexableSizeOf(Oop o, int n) {        BytesPartLayout().set_indexableSizeOfBytesPart(bytesPartRefOf(o), n); }

Byte* ByteVectorLayout :: for_AddressOfIndexableAt(Oop o, int i        ) { return BytesPartLayout().forBytesPart_AddressAt( bytesPartRefOf(o), i    ); }
Byte  ByteVectorLayout :: for_IndexableAt         (Oop o, int i        ) { return BytesPartLayout().forBytesPart_At       ( bytesPartRefOf(o), i    ); }
void  ByteVectorLayout :: for_IndexableAt_Put     (Oop o, int i, Byte b) {        BytesPartLayout().forBytesPart_At_Put   ( bytesPartRefOf(o), i, b ); }

bool ByteVectorLayout :: isByteVector(Oop o) {
  return MarkLayout().isMarkValueForByteVector( markValueOf(o) );
}

bool ByteVectorLayout :: isString    (Oop o)  {
  error("TODO: isString(o): should this be different? Check the map, maybe?");
  return isByteVector(o);
}

void ByteVectorLayout :: verifyBytesPartRefOf(Oop o, Verifier* aVerifier) {
  Oop bpRefAsSmi = SmiLayout().oopForValue( for_VerifySmiAt_With(o, bytesPartRefField()->indexFor(o, this), aVerifier) );
  aVerifier->always_assert(theVM()->universe()->isBytesPartAddressInBounds( (Address) bpRefAsSmi ),  "bad bytes part ref");
}

void ByteVectorLayout :: verifyObject_UpTo_With(Oop mapOop, Oop o, int nextIndex, Verifier* aVerifier) {
  verifyBytesPartRefOf( o, aVerifier );
  AbstractVectorLayout :: verifyObject_UpTo_With(mapOop, o, emptyObjectSizeFor(o), aVerifier);
  //TODO: if it's a canonical string, check it against the canonicalized strings table?
}

int ByteVectorLayout :: argCountOf(Oop o) {
  int len = indexableSizeOf(o);
  always_assert( len > 0,  "should have a positive length" );
  Byte* s = BytesPartLayout().addressOfFirstByteInBytesPart( bytesPartRefOf(o) );
  return argCountOfString(s, len);
}

Oop ByteVectorLayout :: newString(char* s) {
  error("newString not implemented yet");
  return INVALID_OOP;
}
# include "headerFields.hh"
# include "memoryObjectLayout.hh"
# include "wordLayout.hh"
# include "immediateLayout.hh"

# define VALUE_ACCESSORS( className, valueType, basicGetter, basicSetter )                             \
valueType className :: valueFor( Oop o, MemoryObjectLayout* aLayout )  {                              \
  if ( _markField != NULL ) {                                                                         \
    MarkValue  mv = aLayout->markValueOf(o);                                                          \
    FieldValue fv = _markField->valueOfWord(mv);                                                      \
    if ( fv != _markField->valueMeaningUnencodable() ) {                                              \
      int n = _markField->numberForValue(fv);                                                         \
      return valueForEncodedNumberInMark(n);                                                          \
    }                                                                                                 \
  }                                                                                                   \
  return aLayout-> basicGetter (o, indexFor(o, aLayout));                                             \
}                                                                                                     \
                                                                                                      \
void className :: setValueFor( Oop o, valueType v, MemoryObjectLayout* aLayout )  {                   \
  if ( _markField != NULL ) {                                                                         \
    MarkValue  mv = aLayout->markValueOf(o);                                                          \
    FieldValue fv = _markField->valueOfWord(mv);                                                      \
    if ( fv != _markField->valueMeaningUnencodable() ) {                                              \
      error( "this is difficult - we would have to create a whole new object with the extra field" ); \
    }                                                                                                 \
  }                                                                                                   \
  return aLayout-> basicSetter (o, indexFor(o, aLayout), v);                                          \
}

VALUE_ACCESSORS(  OopValueHeaderField, Oop      , for_At         , for_At_Put          )
VALUE_ACCESSORS(  SmiValueHeaderField, int      , for_SmiValueAt , for_At_PutSmiValue  )
VALUE_ACCESSORS( MarkValueHeaderField, MarkValue, for_MarkValueAt, for_At_PutMarkValue )

Oop OopValueHeaderField::valueForEncodedNumberInMark(int n) { return SmiLayout().encode(n); }

Oop MapHeaderField::valueForEncodedNumberInMark(int n) {
  error("not implemented yet, but should look up the map at that index in the compact map table");
  return 0;
}

int AbstractHeaderField::indexFor(Oop o, MemoryObjectLayout* aLayout) {
  if (_precedingField == NULL) {return 0;}
  int i = _precedingField->indexAfterMeFor(o, aLayout);
  return i;
}

int AbstractHeaderField::indexAfterMeFor(Oop o, MemoryObjectLayout* aLayout) {
  return indexFor(o, aLayout) + (isIncludedIn(o, aLayout) ? 1 : 0);
}

bool AbstractHeaderField::isIncludedIn(Oop o, MemoryObjectLayout* aLayout) {
  if (_markField != NULL) {
    MarkValue mv = aLayout->markValueOf(o);
    FieldValue fv = _markField->valueOfWord(mv);
    if (fv != _markField->valueMeaningUnencodable()) {
      return false;
    }
  }
  return true;
}

int SmiValueHeaderField::verifyObject(Oop o, MemoryObjectLayout* aLayout, Verifier* aVerifier) {
  if (_markField != NULL) {
    MarkValue mv = aLayout->markValueOf(o);
    FieldValue fv = _markField->valueOfWord(mv);
    if (fv != _markField->valueMeaningUnencodable()) {
      int n = _markField->numberForValue(fv);
      return valueForEncodedNumberInMark(n);
    }
  }
  return aLayout->for_VerifySmiAt_With( o, indexFor(o, aLayout), aVerifier );
}
# include "base.hh"
# include <stdio.h>
# include "bytesPartLayout.hh"
# include "byteVectorLayout.hh"

// TODO: These obviously need to be better.
void error         ( const char* msg ) { error_printf_and_flush("error: %s", msg); for(;;) pause(); }
void throwException( const char* msg ) { error(msg); }

void always_assert( bool x, const char* msg ) {
  if (!x)
    error(msg);
}

int roundUpTo( int n, int modulus ) {
  return   (n + modulus - 1)  /  modulus   *   modulus;
}

int lengthOfCString( char* s ) {
  int i = 0;
  for ( char* c = s;  *c != '\0';  ++c)
    ++i;
  return i;
}


bool areCStringsEqual( char* a, char* b ) {
  char* c = a;
  char* d = b;
  while ( *c != '\0' ) {
    if ( *d != *c ) return false;
    ++c;
    ++d;
  }
  return  *d == '\0';
}


bool is_klein_string_equal_to_C_string(int kleinStringSize, Byte* kleinString, char* cString) {
  // There's no '\0' at the end of the kleinString, so be careful.

  for ( int i = 0;  i < kleinStringSize;  ++i) {
    char c = cString[i];
    if ( c == '\0'  ||  c != kleinString[i] ) 
      return false;
  }
  return  cString[kleinStringSize] == '\0';
}


bool isCStringEqualToKleinString( int cStringSize, char* cString, Oop kleinString ) {
  // There's no '\0' at the end of the kleinString, so be careful.
  
  BPRef bpRef = ByteVectorLayout().bytesPartRefOf(kleinString);
  int kleinStringSize = BytesPartLayout().indexableSizeOfBytesPart(bpRef);
  Byte* kleinStringBytes = BytesPartLayout().addressOfFirstByteInBytesPart(bpRef);
  
  if (cStringSize != kleinStringSize) return false;
  
  for (int i = 0; i < cStringSize; i++) {
    if (cString[i] != kleinStringBytes[i]) return false;
  }
  return true;
}

bool is_id_alpha( char c ) {
  return  'a' <= c  &&  c <= 'z'
      ||  'A' <= c  &&  c <= 'Z'
      ||   c == '_';
}


int argCountOfString( char* s, int len )  {
  char c = *s;

  if ( !is_id_alpha(c) )        return 1;

  if ( s[ len - 1 ]  !=  ':' )  return 0; // an optimization

  fint argc = 1;
  for ( const char* ss  =  s + len - 3;  // last is :, next-to-last is alpha
		    ss > s;              // do not need to look at first one
		  --ss )
         if (*ss == ':')  ++argc;

  return argc;
}
# include "bytesPartLayout.hh"
# include "immediateLayout.hh"
# include "universe.hh"
# include "theVM.hh"
# include "smallVMDeclarationsOrStubs.hh"


const int bytesPartIndexableSizeFieldOffsetInBytes = 0;
const int lastBytesHeaderWordOffset                = bytesPartIndexableSizeFieldOffsetInBytes;
const int bytesPartHeaderSizeInBytes               = (lastBytesHeaderWordOffset + 1) * oopSize;
const int     firstByteOffsetInBytes               = bytesPartHeaderSizeInBytes;

Address addressAtByteOffsetFromBytesPart(BPRef bpRef, int offsetInBytes) {
  return (Address) (((Byte*) bpRef) + offsetInBytes);
}

Address addressOfIndexableSizeFieldInBytesPart(BPRef bpRef) {
  return addressAtByteOffsetFromBytesPart(bpRef, bytesPartIndexableSizeFieldOffsetInBytes);
}

Byte* BytesPartLayout::addressOfFirstByteInBytesPart(BPRef bpRef) {
  return (Byte*) addressAtByteOffsetFromBytesPart(bpRef, firstByteOffsetInBytes);
}

int BytesPartLayout::indexableSizeOfBytesPart(BPRef bpRef) {
  Address indexableSizeAddress = addressOfIndexableSizeFieldInBytesPart(bpRef);
  return SmiLayout().valueOf(*indexableSizeAddress);
}

void BytesPartLayout::set_indexableSizeOfBytesPart(BPRef bpRef, int s) {
  Address indexableSizeAddress = addressOfIndexableSizeFieldInBytesPart(bpRef);
  *indexableSizeAddress = SmiLayout().oopForValue(s);
}

BPRef BytesPartLayout::allocateBytesPartWithIndexableSize(int nBytes) {
  BPRef bpRef = theVM()->universe()->allocateBytes(nBytes + bytesPartHeaderSizeInBytes);
  set_indexableSizeOfBytesPart(bpRef, nBytes);
  return bpRef;
}

bool isByteIndexOutOfBounds(BPRef bpRef, int i) {
  return (i < 0)  ||  (i >= BytesPartLayout().indexableSizeOfBytesPart(bpRef));
}

void bytesPartBoundsCheck(BPRef bpRef, int i) {
  if (isByteIndexOutOfBounds(bpRef, i)) {throwException("bytes part index out of bounds: %i");}
}	

Byte* forBytesPart_UncheckedAddressAt(BPRef bpRef, int i) {
  return (Byte*) addressAtByteOffsetFromBytesPart(bpRef, firstByteOffsetInBytes + i);
}

Byte* BytesPartLayout::forBytesPart_AddressAt(BPRef bpRef, int i) {
  bytesPartBoundsCheck(bpRef, i);
  return forBytesPart_UncheckedAddressAt(bpRef, i);
}

Byte BytesPartLayout::forBytesPart_At(BPRef bpRef, int i) {
  return *(forBytesPart_AddressAt(bpRef, i));
}

void BytesPartLayout::forBytesPart_At_Put(BPRef bpRef, int i, Byte b) {
  *(forBytesPart_AddressAt(bpRef, i)) = b;
}

BPRef BytesPartLayout::nextBytesPartAfter(BPRef bpRef) {
  int s = indexableSizeOfBytesPart(bpRef);
  s = roundUpTo(s, oopSize);
  return (BPRef) forBytesPart_UncheckedAddressAt(bpRef, s);
}
# include "abstractVectorLayout.hh"

bool AbstractVectorLayout :: isIndexOutOfBounds( Oop o, int i )  {
       return  !(  0 <= i  &&  i <  indexableSizeOf(o)  );
}
# include "kleinObject.hh"
# include "maps.hh"
# include "immediateLayout.hh"


Oop KleinObject::contentsOfSlotNamed( char* n ) {
  return MemoryObjectLayout().contentsOfSlotInObject( _oop, n );
}

int KleinObject::smiContentsOfSlotNamed( char* n ) {
  // TODO: This method does not properly handle the case
  // of an improper oop. Need to fix to handle illegal oops.
  return SmiLayout().valueOf( contentsOfSlotNamed(n) );
}

FailureCode KleinObject::setContentsOfSlotNamed( char* n, Oop o ) {
  return MemoryObjectLayout().setContentsOfSlotInObject( _oop, o, n );
}

FailureCode KleinObject::setSmiContentsOfSlotNamed( char* n, int i ) {
  return setContentsOfSlotNamed( n, SmiLayout().oopForValue(i) );
}
# include "theVM.hh"
# include "universe.hh"
# include "stdio.h" //TODO: take this out
# include "verifier.hh"

static Oop oopForTheVM = 0;

void setBootstrapInfo(Oop oop) {
  oopForTheVM = oop;
}

TheVM* theVM() {
  return new TheVM(oopForTheVM);
}

TheVM* setTheVM(int vmOop) {
  oopForTheVM = vmOop;
  return theVM();
}

Universe* TheVM::universe() {
  return new Universe( contentsOfSlotNamed("universe") );
}

bool notyet = true;

void startSmallSelf() {
  printf("Haha! We started smallSelf.\n");
  Verifier v;
  while (notyet) {}
  theVM()->universe()->verify(&v);
  //TODO: get the Self method to run and run it.
}
# include "verifier.hh"

void Verifier::always_assert(bool b, char* msg) {
  if (!b)  throwException(msg);
}
# include "generation.hh"
# include "space.hh"

EdenSpace* NewGeneration::edenSpace() {
  return new EdenSpace(contentsOfSlotNamed("edenSpace"));
}


BPRef NewGeneration::allocateBytes(int nBytes) {
  return edenSpace()->allocateBytes(nBytes);
}

Address NewGeneration::allocateOops(int nOops) {
  return edenSpace()->allocateOops(nOops);
}

# define SPACES_DO_WITH_NAME(x, xName, body) {Space* x = edenSpace(); char* xName = "eden"; body}

void NewGeneration::verify(Verifier* aVerifier) {
  SPACES_DO_WITH_NAME(s, sName,
    s->verifySpace(sName, aVerifier);
  )
}
# include "immediateLayout.hh"

void ImmediateLayout :: verifyObject_UpTo_With( Oop mapOop, Oop o, int nextIndex, Verifier* aVerifier )  {
  error("should never get here");
}

bool ImmediateLayout :: hasMyTag( Oop o )  {
  return Tag::tagOfOop(o) == myTag();
}
# include "wordLayout.hh"
# include "bytesPartLayout.hh"
# include "markLayout.hh"
# include "byteVectorLayout.hh"
# include "objVectorLayout.hh"
# include "tag.hh"
# include "slotType.hh"
# include "slotTypeFields.incl.hh"
# include "theVM.hh"
# include "universe.hh"
# include "headerFields.hh"
# include "stdio.h"

// A lot of these tests are very brittle and will break if we make any changes to
// the layouts of objects. When that happens, we can either make these tests less
// brittle, or else just throw them away. I mostly just want them for the early
// stages of the project, to give me confidence that this code I'm writing actually
// works. Once we have better end-to-end tests, these tests here will become less
// useful (though maybe still useful enough to be worth debrittlifying them).
// -- Adam, 11/05

void always_assert_equal(int a, int b, char* aMsg, char* bMsg) {
  if ( a != b ) {
    printf("%i != %i\n", a,    b   );
    printf("%s != %s\n", aMsg, bMsg);
    error("assertion failure!");
  }
}

# define ASSERT_EQUAL(a, b) always_assert_equal(a, b, #a, #b)

# define ASSERT_FAIL(x) { try {x  ASSERT(false); } catch (...) {} }

void testTag() {
  ASSERT( Tag::tagOfOop( (Oop)  0 ) == 0 );
  ASSERT( Tag::tagOfOop( (Oop)  1 ) == 1 );
  ASSERT( Tag::tagOfOop( (Oop)  2 ) == 2 );
  ASSERT( Tag::tagOfOop( (Oop)  3 ) == 3 );
  ASSERT( Tag::tagOfOop( (Oop) 16 ) == 0 );
  ASSERT( Tag::tagOfOop( (Oop) 57 ) == 1 );
  ASSERT( Tag::tagOfOop( (Oop) 94 ) == 2 );
  ASSERT( Tag::tagOfOop( (Oop)  7 ) == 3 );
}

void testSlotType() {
  ASSERT( isObjectSlot  ( slotType_slotTypeField_objectSlotValue   << slotType_slotTypeField_shift ) );
  ASSERT( isMapSlot     ( slotType_slotTypeField_mapSlotValue      << slotType_slotTypeField_shift ) );
  ASSERT( isArgumentSlot( slotType_slotTypeField_argumentSlotValue << slotType_slotTypeField_shift ) );

  ASSERT(  isParent(    1 << slotType_isParentField_shift     ) );
  ASSERT( !isParent(    0 << slotType_isParentField_shift     ) );

  ASSERT(  isAssignable(1 << slotType_isAssignableField_shift ) );
  ASSERT( !isAssignable(0 << slotType_isAssignableField_shift ) );

  SlotType assignableObjectSlotType = (1 << slotType_isAssignableField_shift) | (slotType_slotTypeField_objectSlotValue   << slotType_slotTypeField_shift );
  ASSERT( isAssignable(assignableObjectSlotType) );
  ASSERT( isObjectSlot(assignableObjectSlotType) );
}

void testBytesPart() {
  BytesPartLayout bpLayout;
  BPRef bpRef = bpLayout.allocateBytesPartWithIndexableSize(4);
  ASSERT(bpLayout.indexableSizeOfBytesPart(bpRef) == 4);

              bpLayout.forBytesPart_At_Put( bpRef, 0,    'A');
              bpLayout.forBytesPart_At_Put( bpRef, 1,    'd');
              bpLayout.forBytesPart_At_Put( bpRef, 2,    'a');
              bpLayout.forBytesPart_At_Put( bpRef, 3,    'm');

  ASSERT     (bpLayout.forBytesPart_At    ( bpRef, 0) == 'A');
  ASSERT     (bpLayout.forBytesPart_At    ( bpRef, 1) == 'd');
  ASSERT     (bpLayout.forBytesPart_At    ( bpRef, 2) == 'a');
  ASSERT     (bpLayout.forBytesPart_At    ( bpRef, 3) == 'm');

  ASSERT_FAIL(bpLayout.forBytesPart_At    ( bpRef, -1);     );
  ASSERT_FAIL(bpLayout.forBytesPart_At    ( bpRef,  4);     );
}

void testMarkLayout() {
  MarkLayout markLayout;
  ASSERT(markLayout.encode(5) == 23);
  ASSERT(markLayout.decode(99) == 24);
  ASSERT(markLayout.valueOf(79) == 19);
  ASSERT(markLayout.trailingMark() == 3);
  ASSERT(markLayout.hashOfMarkValue(markLayout.set_hashOfMarkValue(17, 56)) == 56);
  ASSERT(markLayout.isMarkValueForByteVector(1));
}

void testMemoryObjectLayout() {
  MemoryObjectLayout moLayout;
  ASSERT(moLayout.memForAddress((Address) 4) == 5);
  ASSERT(moLayout.addressOfMem(9) == (Address) 8);

  Oop o = moLayout.memForAddress(theVM()->universe()->allocateOops(9));

                moLayout.setMarkValueOf(o,   7 );
  ASSERT_EQUAL( moLayout.   markValueOf(o),  7 );
  ASSERT_EQUAL( moLayout.for_At(o, 0)     , 31 );

                moLayout.setOIDOf(o,   13 );
  ASSERT_EQUAL( moLayout.   oidOf(o),  13 );
  ASSERT_EQUAL( moLayout.for_At(o, 1), 52 );
}

void testObjVectorLayout() {
  ObjVectorLayout ovLayout;
  Oop o = ovLayout.memForAddress(theVM()->universe()->allocateOops(15));
  ovLayout.setMarkValueOf( o,   ovLayout.indexableSizeField  ()->markField()->wordMeaningUnencodable()
                              | ovLayout.indexableOriginField()->markField()->wordMeaningUnencodable() );

          ovLayout.set_indexableSizeOf(o,    10);
  ASSERT( ovLayout.    indexableSizeOf(o) == 10);

          ovLayout.set_indexableOriginOf(o,    5);
  ASSERT( ovLayout.    indexableOriginOf(o) == 5);

  ASSERT_EQUAL((int)  ovLayout.for_AddressOfIndexableAt(o, 0),
               (int) (ovLayout.addressOfMem(o) + 5));

                ovLayout.for_IndexableAt_Put( o,  0,   101);
                ovLayout.for_IndexableAt_Put( o,  1,   201);
                ovLayout.for_IndexableAt_Put( o,  2,   301);
                ovLayout.for_IndexableAt_Put( o,  9,  1001);

  ASSERT_EQUAL( ovLayout.for_IndexableAt    ( o,  0),  101);
  ASSERT_EQUAL( ovLayout.for_IndexableAt    ( o,  1),  201);
  ASSERT_EQUAL( ovLayout.for_IndexableAt    ( o,  2),  301);
  ASSERT_EQUAL( ovLayout.for_IndexableAt    ( o,  9), 1001);

  ASSERT_FAIL ( ovLayout.for_IndexableAt    ( o, -1);     );
  ASSERT_FAIL ( ovLayout.for_IndexableAt    ( o, 10);     );
}

void testByteVectorLayout() {
  ByteVectorLayout bvLayout;
  BytesPartLayout bpLayout;
  BPRef bpRef = bpLayout.allocateBytesPartWithIndexableSize(10);
  Oop o = bvLayout.memForAddress(theVM()->universe()->allocateOops(4));

         bvLayout.setMarkValueOf(o,    7);
  ASSERT(bvLayout.   markValueOf(o) == 7);

         bvLayout.set_bytesPartRefOf(o,    bpRef);
  ASSERT(bvLayout.    bytesPartRefOf(o) == bpRef);

                bvLayout.for_IndexableAt_Put( o,  0,  'b' );
                bvLayout.for_IndexableAt_Put( o,  1,  'l' );
                bvLayout.for_IndexableAt_Put( o,  2,  'a' );
                bvLayout.for_IndexableAt_Put( o,  9,  'h' );

  ASSERT_EQUAL( bvLayout.for_IndexableAt    ( o,  0), 'b' );
  ASSERT_EQUAL( bvLayout.for_IndexableAt    ( o,  1), 'l' );
  ASSERT_EQUAL( bvLayout.for_IndexableAt    ( o,  2), 'a' );
  ASSERT_EQUAL( bvLayout.for_IndexableAt    ( o,  9), 'h' );

  ASSERT_FAIL ( bvLayout.for_IndexableAt    ( o, 10);     );
}

int argCountOfCString(char* s) {
  return argCountOfString(s, lengthOfCString(s));
}

void testArgCount() {
  ASSERT_EQUAL( argCountOfCString( "x"          ), 0 );
  ASSERT_EQUAL( argCountOfCString( "x:"         ), 1 );
  ASSERT_EQUAL( argCountOfCString( "blah:"      ), 1 );
  ASSERT_EQUAL( argCountOfCString( "isAardvark" ), 0 );
  ASSERT_EQUAL( argCountOfCString( "to:By:Do:"  ), 3 );
}

void runAllTests() {
  testTag();
  testSlotType();
//  testBytesPart();
//  testMarkLayout();
//  testMemoryObjectLayout();
//  testObjVectorLayout();
//  testByteVectorLayout();
}
# include "markLayout.hh"
# include "wordLayout.hh"

# include "markFields.incl.hh"

static       BitField* mark_isByteVectorField             = new       BitField(mark_isByteVectorField_width,             mark_isByteVectorField_shift  );
static NumberBitField* mark_compactMapIndexField          = new NumberBitField(mark_compactMapIndexField_width,          mark_compactMapIndexField_shift,          mark_compactMapIndexField_lowestEncodableNumber         );
static NumberBitField* mark_objVectorIndexableOriginField = new NumberBitField(mark_objVectorIndexableOriginField_width, mark_objVectorIndexableOriginField_shift, mark_objVectorIndexableOriginField_lowestEncodableNumber);
static NumberBitField* mark_objVectorIndexableSizeField   = new NumberBitField(mark_objVectorIndexableSizeField_width,   mark_objVectorIndexableSizeField_shift,   mark_objVectorIndexableSizeField_lowestEncodableNumber  );
static       BitField* mark_hasBeenVisitedField           = new       BitField(mark_hasBeenVisitedField_width,           mark_hasBeenVisitedField_shift);
static       BitField* mark_hashField                     = new       BitField(mark_hashField_width,                     mark_hashField_shift          );

BitField*       MarkLayout ::             isByteVectorField() { return mark_isByteVectorField; }
NumberBitField* MarkLayout ::          compactMapIndexField() { return mark_compactMapIndexField; }
NumberBitField* MarkLayout :: objVectorIndexableOriginField() { return mark_objVectorIndexableOriginField; }
NumberBitField* MarkLayout ::   objVectorIndexableSizeField() { return mark_objVectorIndexableSizeField; }
BitField*       MarkLayout ::           hasBeenVisitedField() { return mark_hasBeenVisitedField; }
BitField*       MarkLayout ::                     hashField() { return mark_hashField; }

Oop MarkLayout::trailingMark() {return oopForValue(0);}

const HashValue    noHashValue = 0;
const HashValue firstHashValue = 1;

HashValue MarkLayout::hashOfMarkValue(MarkValue mv) {
  return mark_hashField->valueOfWord(mv);
}

MarkValue MarkLayout::set_hashOfMarkValue(MarkValue mv, HashValue h) {
  HashValue newHashValue = (h == noHashValue) ? firstHashValue : h;
  MarkValue newMarkValue = mark_hashField->setValueOfWord(mv, newHashValue);
  ASSERT(hashOfMarkValue(newMarkValue) == newHashValue);
  return newMarkValue;
}

bool MarkLayout::isMarkValueForByteVector(MarkValue mv) {
  return mark_isByteVectorField->doesWordHaveValue(mv, 1);
}
# include "objVectorLayout.hh"
# include "headerFields.hh"
# include "byteVectorLayout.hh"
# include "bytesPartLayout.hh"
# include "maps.hh"
# include "verifier.hh"


AbstractHeaderField* ObjVectorLayout::lastField() {return indexableSizeField();}

int  ObjVectorLayout ::     indexableOriginOf( Oop o        ) { return indexableOriginField()->   valueFor( o,    this ); }
void ObjVectorLayout :: set_indexableOriginOf( Oop o, int n ) {        indexableOriginField()->setValueFor( o, n, this ); }

int  ObjVectorLayout ::     indexableSizeOf  ( Oop o        ) { return indexableSizeField()  ->   valueFor( o,    this ); }
void ObjVectorLayout :: set_indexableSizeOf  ( Oop o, int n ) {        indexableSizeField()  ->setValueFor( o, n, this ); }

Address ObjVectorLayout :: for_AddressOfIndexableAt( Oop o, int i )  {
  FailureCode c = boundsCheck(o, i);
  if ( c != SUCCEEDED )  return INVALID_ADDRESS;
  
  return for_AddressAt( o,  indexableOriginOf(o) + i );
}

Oop ObjVectorLayout::for_IndexableAt(Oop o, int i) {
  FailureCode c = boundsCheck(o, i);
  if ( c != SUCCEEDED )  return INVALID_OOP;

  return for_At( o, indexableOriginOf(o) + i );
}

FailureCode ObjVectorLayout :: for_IndexableAt_Put(Oop o, int i, Oop x) {
  FailureCode c = boundsCheck(o, i);
  if ( c != SUCCEEDED )  return c;

  for_At_Put( o, indexableOriginOf(o) + i, x );
  return SUCCEEDED;
}

bool ObjVectorLayout::isObjVector(Oop o) {
  return mapLayoutFor( mapOf(o) ) -> isVector();
}

int ObjVectorLayout :: verifyIndexableSizeOf  ( Oop o, Verifier* aVerifier )  { return indexableSizeField  ()->verifyObject( o, this, aVerifier ); }
int ObjVectorLayout :: verifyIndexableOriginOf( Oop o, Verifier* aVerifier )  { return indexableOriginField()->verifyObject( o, this, aVerifier ); }

void ObjVectorLayout :: verifyObject_UpTo_With( Oop mapOop, Oop o, int nextIndex, Verifier* aVerifier ) {
  // TODO: If o happens to be a map, maybe do extra verification?
  int s  = verifyIndexableSizeOf  (o, aVerifier);
  int io = verifyIndexableOriginOf(o, aVerifier);
  aVerifier->always_assert(io + s <= nextIndex, "indexables go past this object");
  verifyObjectSlotBounds(mapOop, emptyObjectSizeFor(o), io, aVerifier);
}


MapLayout* ObjVectorLayout :: mapLayoutFor(Oop mapOop) {
  // TODO: Put a pointer to a vtable in the map object, figure out at the right offset for a
  // pointer so that the vtable is in the place where the C++ compiler expects it, and then
  // do a simple dispatch.
  Oop   mapTypeOop  =        MapLayout().mapTypeOopOf(mapOop);
  BPRef bpRef       = ByteVectorLayout().bytesPartRefOf(mapTypeOop);
  Byte* mapType     =  BytesPartLayout().addressOfFirstByteInBytesPart(bpRef);
  int   mapTypeSize =  BytesPartLayout().     indexableSizeOfBytesPart(bpRef);

  # include "mapTypeTesters.incl.impl.hh"

  error("unknown map type");
  return NULL;
}
# include "memoryObjectLayout.hh"
# include "immediateLayout.hh"
# include "headerFields.hh"
# include "verifier.hh"
# include "maps.hh"
# include "universe.hh"
# include "space.hh"
# include "theVM.hh"
# include "markLayout.hh"
# include "slotType.hh"


Address   MemoryObjectLayout :: for_AddressAt      ( Oop o, int i               ) { return addressOfMem(o) + i; }
Oop       MemoryObjectLayout :: for_At             ( Oop o, int i               ) { return *((Oop*) for_AddressAt(o, i))    ; }
void      MemoryObjectLayout :: for_At_Put         ( Oop o, int i, Oop       x  ) {        *((Oop*) for_AddressAt(o, i)) = x; }

int       MemoryObjectLayout :: for_SmiValueAt     ( Oop o, int i               ) { return SmiLayout().valueOf(for_At(o, i));     }
void      MemoryObjectLayout :: for_At_PutSmiValue ( Oop o, int i, int       x  ) { for_At_Put(o, i, SmiLayout().oopForValue(x)); }

MarkValue MemoryObjectLayout :: for_MarkValueAt    ( Oop o, int i               ) { return MarkLayout().valueOf(for_At(o, i));      }
void      MemoryObjectLayout :: for_At_PutMarkValue( Oop o, int i, MarkValue mv ) { for_At_Put(o, i, MarkLayout().oopForValue(mv)); }


# include "headerFields.incl.hh"

AbstractHeaderField* MemoryObjectLayout :: lastHeaderField() { return memoryObject_mapField; }
AbstractHeaderField* MemoryObjectLayout :: lastField      () { return lastHeaderField(); }

int MemoryObjectLayout :: emptyObjectSizeFor( Oop o ) { return lastField()->indexAfterMeFor(o, this); }

// TODO: Get this stuff into blockLayout.cpp or wherever it belongs.
# include "blockLayout.hh"
OopValueHeaderField*      BlockLayout::homeFramePointerField() { return block_homeFramePointerField   ; }
# include "objVectorLayout.hh"
SmiValueHeaderField*  ObjVectorLayout:: indexableOriginField() { return objVector_indexableOriginField; }
SmiValueHeaderField*  ObjVectorLayout::   indexableSizeField() { return objVector_indexableSizeField  ; }
# include "byteVectorLayout.hh"
OopValueHeaderField* ByteVectorLayout::    bytesPartRefField() { return byteVector_bytesPartRefField  ; }


MarkValue MemoryObjectLayout ::    markValueOf( Oop o                   ) { return memoryObject_markField->   valueFor(o,         this); }
void      MemoryObjectLayout :: setMarkValueOf( Oop o, MarkValue mv     ) {        memoryObject_markField->setValueFor(o, mv,     this); }

int       MemoryObjectLayout ::          oidOf( Oop o                   ) { return memoryObject_oidField ->   valueFor(o,         this); }
void      MemoryObjectLayout ::       setOIDOf( Oop o, int       oid    ) {        memoryObject_oidField ->setValueFor(o, oid,    this); }

Oop       MemoryObjectLayout ::          mapOf( Oop o                   ) { Oop moop = memoryObject_mapField ->   valueFor(o, this); 
                                                                            ASSERT( moop != 0 );
                                                                            return moop;                                                 }
void      MemoryObjectLayout ::       setMapOf( Oop o, Oop       mapOop ) {        memoryObject_mapField ->setValueFor(o, mapOop, this); }


Oop         MemoryObjectLayout ::    contentsOfSlotInObject( Oop oop,                  char* n ) { return MapLayout().   contentsOfSlotNamed( mapOf(oop), oop,              n ); }
FailureCode MemoryObjectLayout :: setContentsOfSlotInObject( Oop oop, Oop newValueOop, char* n ) { return MapLayout().setContentsOfSlotNamed( mapOf(oop), oop, newValueOop, n ); }


int MemoryObjectLayout :: verifyOopsOf( Oop o, Verifier* aVerifier ) {
  ASSERT( memoryObject_markField->isFirst() );
  Oop* originalAddr = decode(o);
  Oop* addr = originalAddr;
  while (true) {
    ++addr;
    Oop oop = *addr;
    TagValue t = Tag::tagOfOop(oop);
    if (t == Tag::mark) break;
    if (t == Tag::mem ) {
      Address addr = MemoryObjectLayout().addressOfMem(oop);
      aVerifier->always_assert(theVM()->universe()->isObjectAddressInBounds(addr), "mem oop does not point inside object space");
      Oop* targetAddr = decode(oop);
      TagValue targetTag = Tag::tagOfOop(*targetAddr);
      aVerifier->always_assert(targetTag == Tag::mark, "mem oop does not point to a mark");
    }
  }
  return addr - originalAddr; //TODO: did I get the pointer arithmetic right here?
}

Oop MemoryObjectLayout :: verifyContentsOfOop( Oop o, Verifier* aVerifier ) {
  int nextIndex = verifyOopsOf(o, aVerifier);
  Oop mapOop = MemoryObjectLayout().mapOf(o);
  MapLayout* mapLayout = ObjVectorLayout().mapLayoutFor(mapOop);
  MarkValue mv;
  //TODO: fix this verifier failblock thingy.
  try {mv = ((MemoryObjectLayout*) (mapLayout->myLayout()))->markValueOf(o);} catch (...) {mv = 0;}
  //TODO: put this back in in a way that works.
  //bool markIsBV = MarkLayout().isMarkValueForByteVector(mv);
  //bool  mapIsBV = mapLayout->isByteVector();
  //aVerifier->always_assert(markIsBV == mapIsBV, "byte vector bit in mark is wrong");
  mapLayout->verifyObject(mapOop, o, nextIndex, aVerifier);
  return memForAddress(for_AddressAt(o, nextIndex));
}

int MemoryObjectLayout :: for_VerifySmiAt_With( Oop o, int i, Verifier* aVerifier ) {
  Oop oop = for_At(o, i);
   
  aVerifier->always_assert( Tag::tagOfOop(oop) == Tag::smi, "not a smi" );
  return SmiLayout().valueOf(oop);
}

void MemoryObjectLayout :: verifyObject_UpTo_With( Oop mapOop, Oop o, int nextIndex, Verifier* aVerifier ) {
  verifyObjectSlotBounds( mapOop, emptyObjectSizeFor(o), nextIndex, aVerifier );
}

void MemoryObjectLayout :: verifyObjectSlotBounds( Oop mapOop, int lowest, int pastHighest, Verifier* aVerifier ) {
  for (int i = 0, n = MapLayout().sizeOf(mapOop); i < n; i++) {
    if (isObjectSlot( MapLayout().typeAt(mapOop, i))) {
      int dataOffset = SmiLayout().valueOf( MapLayout().dataOopAt(mapOop, i));
      aVerifier->always_assert(lowest     <= dataOffset , "slot offset is too small");
      aVerifier->always_assert(dataOffset <  pastHighest, "slot offset is too big"  );
    }
  }
}
# include "space.hh"
# include "verifier.hh"
# include "bytesPartLayout.hh"
# include "memoryObjectLayout.hh"

void Space::verifySpace( char* n, Verifier* aVerifier ) {
  verifyName      (n, aVerifier);
  verifyBoundaries(   aVerifier);
  verifyBytes     (   aVerifier);
  verifyObjs      (   aVerifier);
  verifySentinel  (   aVerifier);
}

Address Space::   objsBottom () {return (Address) smiContentsOfSlotNamed( "objsBottom"  ); }
Address Space::   objsTop    () {return (Address) smiContentsOfSlotNamed( "objsTop"     ); }
Byte*   Space::   bytesBottom() {return (Byte*)   smiContentsOfSlotNamed( "bytesBottom" ); }
Byte*   Space::   bytesTop   () {return (Byte*)   smiContentsOfSlotNamed( "bytesTop"    ); }

void    Space::setObjsBottom ( Address v ) {   setSmiContentsOfSlotNamed( "objsBottom"  , (int) v ); }
void    Space::setObjsTop    ( Address v ) {   setSmiContentsOfSlotNamed( "objsTop"     , (int) v ); }
void    Space::setBytesBottom( Byte*   v ) {   setSmiContentsOfSlotNamed( "bytesBottom" , (int) v ); }
void    Space::setBytesTop   ( Byte*   v ) {   setSmiContentsOfSlotNamed( "bytesTop"    , (int) v ); }

void Space::verifyName(char* n, Verifier* aVerifier) {
  aVerifier->always_assert(areCStringsEqual(_name, n), "space has wrong name");
}

void Space::verifyBoundaries(Verifier* aVerifier) {
  aVerifier->always_assert(objsBottom() <= objsTop() && objsTop() <= ((Address) bytesBottom()) && bytesBottom() <= bytesTop(), "space bounds");
}

void Space::verifyBytes(Verifier* aVerifier) {
  BPRef bp = (BPRef) bytesBottom();
  BPRef bt = (BPRef) bytesTop   ();
  while (bp < bt) {
    bp = BytesPartLayout().nextBytesPartAfter(bp);
  }
  aVerifier->always_assert(bp == bt, "bytes part of space");
}

void Space::verifyObjs(Verifier* aVerifier) {
  Oop ob = MemoryObjectLayout().memForAddress((Address) objsBottom());
  Oop ot = MemoryObjectLayout().memForAddress((Address) objsTop   ());
  Oop o = ob;
  Oop last = 0, last1 = 0, last2 = 0;
  while (o < ot) {
    last2 = last1;  last1 = last; last = o;
    o = MemoryObjectLayout().verifyContentsOfOop(o, aVerifier);
  }
  aVerifier->always_assert(o == ot, "last object");
}

void Space::verifySentinel(Verifier* aVerifier) {
  Oop s = * ((Oop*) objsTop());
  aVerifier->always_assert( Tag::tagOfOop(s) == Tag::mark,  "not a sentinel" );
}

bool Space::isBytesPartAddressInBounds(Address bpAddr) {
  // TODO: It bothers me that we're doing all these stupid casts between Oop* and Byte* and so on.
  return bytesBottom() <= ((Byte*) bpAddr) && ((Byte*) bpAddr) < bytesTop();
}

bool Space::isObjectAddressInBounds(Address addr) {
  return objsBottom() <= addr && addr < objsTop();
}

BPRef Space::allocateBytes(int nBytes) {
  Byte* newBytesBottom = ((Byte*) bytesBottom()) - roundUpTo(nBytes, oopSize);
  
  // Check for out of memory: using '<=' to reserve space for the mark following
  // the last word in the space.
  if (newBytesBottom <= ((Byte*) objsTop())) {
    error("space full");
  }
  
  setBytesBottom(newBytesBottom);
  return (BPRef) newBytesBottom;
}

Address Space::allocateOops(int nOops) {
  Address oldObjsTop = (Address) objsTop();
  Address newObjsTop = oldObjsTop + nOops;
  
  // Check for out of memory: using '<=' to reserve space for the mark following
  // the last word in the space.
  if (bytesBottom() <= ((Byte*) newObjsTop)) {
    error("space full");
  }
  
  setObjsTop(newObjsTop);
  return newObjsTop;
}

oop Space::clone( oop original ) {
  oop mapOop = MemoryObjectLayout().mapOf(original);
  int oldBytesBottom = bytesBottom;
  int oldOopsTop     = oopsTop;
  allocateOops (oriMap -> oopSize);
  allocateBytes(oriMap -> byteSize);
  copyOopPart  (oriMap -> oopPart()  , oldOopsTop    , oopsTop    );
  copyBytesPart(oriMap -> bytesPart(), oldBytesBottom, bytesBottom);
  Map* newMap = mapForOop(oldOopsTop);
  newMap -> adjustForNewOop       (oldOopsTop);
  newMap -> adjustForNewBytesPart (oldBytesBottom);
  return oldOopsTop;
}# include "wordLayout.hh"

FieldValue BitField :: allOnesValue() { return (1 << width()) - 1; }
BitCount   BitField :: shiftPast   () { return shift() + width(); }
intNN      BitField :: mask        () { return wordForValue( allOnesValue() ); }

intNN      BitField :: wordForValue(FieldValue v) { return v << shift(); }

intNN      BitField :: inPlaceValueOfWord(intNN w              ) { return w & mask(); }
FieldValue BitField ::        valueOfWord(intNN w              ) { return inPlaceValueOfWord(w) >> shift(); }
intNN      BitField ::     setValueOfWord(intNN w, FieldValue v) { return (w & (-1 - mask())) | wordForValue(v); }

bool       BitField ::  doesWordHaveValue(intNN w, FieldValue v) { return valueOfWord(w) == v; }

int        BitField :: encodableNumberCount()    { return (1 << width()) - 1; }
FieldValue BitField :: valueMeaningUnencodable() { return encodableNumberCount(); }
#define INSIDE_DYBASE

#include "database.h"
#include "btree.h"
#include "dybase.h"

dybase_storage_t dybase_open(char const* file_path, int page_pool_size, dybase_error_handler_t hnd)
{
    try {
        if (page_pool_size == 0) {
            page_pool_size = dbDefaultPagePoolSize;
        }
        dbDatabase* db = new dbDatabase(dbDatabase::dbAllAccess, (dbDatabase::dbErrorHandler)hnd, page_pool_size/dbPageSize);
        if (db->open(file_path)) {
            return db;
        } else {
            delete db;
            return NULL;
        }
    } catch (dbException&) {
        return NULL;
    }
}

void dybase_close(dybase_storage_t storage)
{
    dbDatabase* db = (dbDatabase*)storage;
    try {
        db->close();
        delete db;
    } catch (dbException&) {
        delete db;
    }
}


void dybase_commit(dybase_storage_t storage)
{
    try {
        ((dbDatabase*)storage)->commit();
    } catch (dbException&) {}
}

void dybase_rollback(dybase_storage_t storage)
{
    try {
        ((dbDatabase*)storage)->rollback();
    } catch (dbException&) {}
}

dybase_oid_t dybase_get_root_object(dybase_storage_t storage)
{
    try {
        return ((dbDatabase*)storage)->getRoot();
    } catch (dbException&) {
        return 0;
    }
}

void dybase_set_root_object(dybase_storage_t storage, dybase_oid_t oid)
{
    try {
        ((dbDatabase*)storage)->setRoot(oid);
    } catch (dbException&) {}
}

dybase_oid_t dybase_allocate_object(dybase_storage_t storage)
{
    try {
        return ((dbDatabase*)storage)->allocate();
    } catch (dbException&) {
        return 0;
    }
}

void dybase_deallocate_object(dybase_storage_t storage, dybase_oid_t oid)
{
    try {
        ((dbDatabase*)storage)->freeObject(oid);
    } catch (dbException&) {}
}

dybase_handle_t dybase_begin_store_object(dybase_storage_t storage, dybase_oid_t oid, char const* class_name)
{
    try {
        return ((dbDatabase*)storage)->getStoreHandle(oid, class_name);
    } catch (dbException&) {
        return NULL;
    }
}

void dybase_store_object_field(dybase_handle_t handle, char const* field_name, int field_type,
                               void* value_ptr, int value_length)
{
    try {
        ((dbStoreHandle*)handle)->setFieldValue(field_name, field_type, value_ptr, value_length);
    } catch (dbException&) {}
}

void dybase_store_array_element(dybase_handle_t handle, int elem_type,
                                void* value_ptr, int value_length)
{
    try {
        ((dbStoreHandle*)handle)->setElement(elem_type, value_ptr, value_length);
    } catch (dbException&) {}
}

void dybase_store_map_entry(dybase_handle_t handle, int key_type, void* key_ptr, int key_length,
			    int value_type, void* value_ptr, int value_length)
{
    try {
        ((dbStoreHandle*)handle)->setElement(key_type, key_ptr, key_length);
        ((dbStoreHandle*)handle)->setElement(value_type, value_ptr, value_length);
    } catch (dbException&) {}
}

void dybase_end_store_object(dybase_handle_t handle)
{
    dbStoreHandle* hnd = (dbStoreHandle*)handle;
    try {
        hnd->db->storeObject(hnd);
        delete hnd;
    } catch (dbException&) {
        delete hnd;
    }
}

dybase_handle_t dybase_begin_load_object(dybase_storage_t storage, dybase_oid_t oid)
{
    try {
        return ((dbDatabase*)storage)->getLoadHandle(oid);
    } catch (dbException&) {
        return NULL;
    }
}

char* dybase_get_class_name(dybase_handle_t handle)
{
    return ((dbLoadHandle*)handle)->getClassName();
}

char* dybase_next_field(dybase_handle_t handle)
{
    dbLoadHandle* hnd = (dbLoadHandle*)handle;
    if (!hnd->hasNextField()) {
        delete hnd;
        return NULL;
    } else {
        return hnd->getFieldName();
    }
}

void dybase_next_element(dybase_handle_t handle)
{
    bool hasNext = ((dbLoadHandle*)handle)->hasNext();
    assert(hasNext);
}


void dybase_get_value(dybase_handle_t handle, int* type, void** value_ptr, int* value_length)
{
    dbLoadHandle* hnd = (dbLoadHandle*)handle;
    *type = hnd->getType();
    *value_ptr = hnd->getValue();
    *value_length = hnd->getLength();
}


dybase_oid_t dybase_create_index(dybase_storage_t storage, int key_type, int unique)
{
    try {
        return dbBtree::allocate((dbDatabase*)storage, key_type, (bool)unique);
    } catch (dbException&) {
        return 0;
    }
}

int dybase_insert_in_index(dybase_storage_t storage, dybase_oid_t index, void* key, int key_type, int key_size, dybase_oid_t obj, int replace)
{
    try {
        return dbBtree::insert((dbDatabase*)storage, (oid_t)index, key, key_type, key_size, (oid_t)obj, (bool)replace);
    } catch (dbException&) {
        return 0;
    }
}

int dybase_remove_from_index(dybase_storage_t storage, dybase_oid_t index, void* key, int key_type, int key_size, dybase_oid_t obj)
{
    try {
        return dbBtree::remove((dbDatabase*)storage, (oid_t)index, key, key_type, key_size, (oid_t)obj);
    } catch (dbException&) {
        return 0;
    }
}

int dybase_index_search(dybase_storage_t storage, dybase_oid_t index, int key_type,
                        void* min_key, int min_key_size, int min_key_inclusive,
                        void* max_key, int max_key_size, int max_key_inclusive,
                        dybase_oid_t** selected_objects)
{
    try {
        dbSearchContext ctx;
        ctx.low = min_key;
        ctx.lowSize = min_key_size;
        ctx.lowInclusive = min_key_inclusive;
        ctx.high = max_key;
        ctx.highSize = max_key_size;
        ctx.highInclusive = max_key_inclusive;
        ctx.keyType = key_type;
        dbBtree::find((dbDatabase*)storage, (oid_t)index, ctx);
        *selected_objects = ctx.selection.grab();
        return ctx.selection.size();
    } catch (dbException&) {
        return 0;
    }
}

void dybase_free_selection(dybase_storage_t storage, dybase_oid_t* selected_objects, int n_selected)
{
    try {
        delete[] selected_objects;
    } catch (dbException&) {}
}

void dybase_drop_index(dybase_storage_t storage, dybase_oid_t index)
{
    try {
        dbBtree::drop((dbDatabase*)storage, (oid_t)index);
    } catch (dbException&) {}
}

void dybase_clear_index(dybase_storage_t storage, dybase_oid_t index)
{
    try {
        dbBtree::clear((dbDatabase*)storage, (oid_t)index);
    } catch (dbException&) {}
}

dybase_iterator_t dybase_create_index_iterator(dybase_storage_t storage, dybase_oid_t index, int key_type,
                                                                void* min_key, int min_key_size, int min_key_inclusive,
                                                                void* max_key, int max_key_size, int max_key_inclusive,
                                                                int ascent)
{
    try {
        return (dybase_iterator_t)new dbBtreeIterator((dbDatabase*)storage, (oid_t)index, key_type, min_key, min_key_size,
                                                  min_key_inclusive, max_key, max_key_size, max_key_inclusive, (bool)ascent);
    } catch (dbException&) {
        return NULL;
    }
}

dybase_oid_t dybase_index_iterator_next(dybase_iterator_t iterator)
{
    try {
        return((dbBtreeIterator*)iterator)->next();
    } catch (dbException&) {
        return 0;
    }
}

void dybase_free_index_iterator(dybase_iterator_t iterator)
{
    delete (dbBtreeIterator*)iterator;
}


void dybase_set_gc_threshold(dybase_storage_t storage, long allocated_delta)
{
    ((dbDatabase*)storage)->setGcThreshold(allocated_delta);
}

void dybase_gc(dybase_storage_t storage)
{
    ((dbDatabase*)storage)->gc();
}
//-< BTREE.CPP >-----------------------------------------------------*--------*
// GigaBASE                  Version 1.0         (c) 1999  GARRET    *     ?  *
// (Post Relational Database Management System)                      *   /\|  *
//                                                                   *  /  \  *
//                          Created:      1-Jan-99    K.A. Knizhnik  * / [] \ *
//                          Last update: 25-Oct-99    K.A. Knizhnik  * GARRET *
//-------------------------------------------------------------------*--------*
// B-Tree implementation
//-------------------------------------------------------------------*--------*

#include "database.h"
#include "btree.h"

#if defined(__sun) || defined(__SVR4)
    #define NO_LARGE_LOCAL_ARRAYS
#endif


void dbBtree::find(dbDatabase* db, oid_t treeId, dbSearchContext& sc)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return;
    }
    dbGetTie tie;
    dbBtree* tree = (dbBtree*)db->getObject(tie, treeId);

    if (sc.keyType != tree->type) {
        if (sc.low != NULL || sc.high != NULL) {
            db->handleError(dybase_bad_key_type, "Type of the key doesn't match index type");
            return;
        }
        sc.keyType = tree->type;
    }

    oid_t rootId = tree->root;
    int   height = tree->height;
    if (rootId != 0) {
        dbBtreePage* page = (dbBtreePage*)db->get(rootId);
        page->find(db, sc, height);
        db->pool.unfix(page);
    }
}

oid_t dbBtree::allocate(dbDatabase* db, int type, bool unique)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return 0;
    }
    dbBtree* tree = new dbBtree();
    tree->size = sizeof(dbBtree);
    tree->root = 0;
    tree->height = 0;
    tree->type = type;
    tree->unique = unique;
    tree->cid = dbBtreeId;
    return db->allocateObject(tree);
}

bool dbBtree::packItem(dbDatabase* db, dbBtree* tree, dbBtreePage::item& it, void* key, int keyType, size_t keySize, oid_t oid)
{

    if (keyType != tree->type) {
        db->handleError(dybase_bad_key_type, "Type of the key doesn't match index type");
        return false;
    }
    it.oid = oid;
    switch (keyType) {
      case dybase_object_type:
        it.keyLen = sizeof(oid_t);
        it.refKey = *(oid_t*)key;
        break;
      case dybase_bool_type:
        it.keyLen = sizeof(db_int1);
        it.boolKey = *(db_int1*)key;
        break;
      case dybase_int_type:
        it.keyLen = sizeof(db_int4);
        it.intKey = *(db_int4*)key;
        break;
      case dybase_long_type:
        it.keyLen = sizeof(db_int8);
        it.longKey = *(db_int8*)key;
        break;
      case dybase_real_type:
        it.keyLen = sizeof(db_real8);
        it.realKey = *(db_real8*)key;
        break;
      case dybase_string_type:
        if (keySize > dbBtreePage::dbMaxKeyLen) {
            db->handleError(dybase_bad_key_type, "Size of string key is too large");
            return false;
        }
        it.keyLen = keySize;
        memcpy(it.charKey, key, keySize);
        break;
    }
    return true;
}

bool dbBtree::insert(dbDatabase* db, oid_t treeId, void* key, int keyType, size_t keySize, oid_t oid, bool replace)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return false;
    }
    dbGetTie treeTie;
    dbBtreePage::item ins;
    dbBtree* tree = (dbBtree*)db->getObject(treeTie, treeId);

    if (!packItem(db, tree, ins, key, keyType, keySize, oid)) {
        return false;
    }

    oid_t rootId = tree->root;
    int   height = tree->height;

    if (rootId == 0) {
        dbPutTie tie;
        dbBtree* t = (dbBtree*)db->putObject(tie, treeId);
        t->root = dbBtreePage::allocate(db, 0, tree->type, ins);
        t->height = 1;
        return true;
    } else {
        int result =  dbBtreePage::insert(db, rootId, tree->type, ins, tree->unique, replace, height);
        assert(result != not_found);
        if (result == overflow) {
            dbPutTie tie;
            dbBtree* t = (dbBtree*)db->putObject(tie, treeId);
            t->root = dbBtreePage::allocate(db, rootId, tree->type, ins);
            t->height += 1;
        }
        return result != duplicate;
    }
}

bool dbBtree::remove(dbDatabase* db, oid_t treeId, void* key, int keyType, size_t keySize, oid_t oid)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return false;
    }
    dbGetTie treeTie;
    dbBtreePage::item rem;
    dbBtree* tree = (dbBtree*)db->getObject(treeTie, treeId);

    if (oid == 0 && !tree->unique) {
        db->handleError(dybase_bad_key_type, "Associated object should be specified to perform remove from non-unique index");
        return false;
    }

    if (!packItem(db, tree, rem, key, keyType, keySize, oid)) {
        return false;
    }

    oid_t rootId = tree->root;
    int   height = tree->height;

    if (rootId == 0) {
        return false;
    }

    int result = dbBtreePage::remove(db, rootId, tree->type, rem, height);
    if (result == underflow) {
        dbBtreePage* page = (dbBtreePage*)db->get(rootId);
        if (page->nItems == 0) {
            dbPutTie tie;
            dbBtree* t = (dbBtree*)db->putObject(tie, treeId);
            if (height == 1) {
                t->height = 0;
                t->root = 0;
            } else {
                if (tree->type == dybase_string_type) {
                    t->root = page->strKey[0].oid;
                } else {
                    t->root = page->record[dbBtreePage::maxItems-1];
                }
                t->height -= 1;
            }
            db->freePage(rootId);
        }
        db->pool.unfix(page);
    } else if (result == dbBtree::overflow) {
        dbPutTie tie;
        dbBtree* t = (dbBtree*)db->putObject(tie, treeId);
        t->root = dbBtreePage::allocate(db, rootId, tree->type, rem);
        t->height += 1;
    }
    return result != not_found;
}

void dbBtree::clear(dbDatabase* db, oid_t treeId)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return;
    }
    _clear(db, treeId);
}

void dbBtree::_clear(dbDatabase* db, oid_t treeId)
{
    dbPutTie tie;
    dbBtree* tree = (dbBtree*)db->putObject(tie, treeId);
    if (tree->root != 0) {
        dbBtreePage::purge(db, tree->root, tree->type, tree->height);
        tree->root = 0;
        tree->height = 0;
    }
}

void dbBtree::drop(dbDatabase* db, oid_t treeId)
{
    dbCriticalSection cs(db->mutex);
    if (!db->opened) {
        db->handleError(dybase_not_opened, "Database not opened");
        return;
    }
    _drop(db, treeId);
}

void dbBtree::_drop(dbDatabase* db, oid_t treeId)
{
    _clear(db, treeId);
    db->free(db->getPos(treeId) & ~dbFlagsMask, sizeof(dbBtree));
    db->freeId(treeId);
}


#define CHECK(a, b, inclusion) (a > b || (a == b && !inclusion))

#define FIND(KEY, TYPE)                                                           \
        if (sc.low != NULL) {                                                     \
            TYPE key = *(TYPE*)sc.low;                                            \
            while (l < r)  {                                                      \
                 int i = (l+r) >> 1;                                              \
                 if (CHECK(key, KEY[i], lowInclusion)) {                          \
                     l = i+1;                                                     \
                 } else {                                                         \
                     r = i;                                                       \
                 }                                                                \
            }                                                                     \
            assert(r == l);                                                       \
        }                                                                         \
        if (sc.high != NULL) {                                                    \
            TYPE key = *(TYPE*)sc.high;                                           \
            if (height == 0) {                                                    \
                while (l < n) {                                                   \
                    if (CHECK(KEY[l], key, highInclusion)) {                      \
                        return false;                                             \
                    }                                                             \
                    sc.selection.add(record[maxItems-1-l]);                       \
                    l += 1;                                                       \
                }                                                                 \
                return true;                                                      \
            } else {                                                              \
                do {                                                              \
                    dbBtreePage* pg = (dbBtreePage*)db->get(record[maxItems-1-l]);\
                    if (!pg->find(db, sc, height)) {                              \
                        db->pool.unfix(pg);                                       \
                        return false;                                             \
                    }                                                             \
                    db->pool.unfix(pg);                                           \
                    if (l == n) {                                                 \
                        return true;                                              \
                    }                                                             \
                } while(KEY[l++] <= key);                                         \
                return false;                                                     \
            }                                                                     \
        }                                                                         \
        break

inline int compareStrings(void* s1, size_t n1, void* s2, size_t n2) {
    int len = n1 < n2 ? n1 : n2;
    int diff = memcmp(s1, s2, len);
    return diff != 0 ? diff : n1 - n2;
}


bool dbBtreePage::find(dbDatabase* db, dbSearchContext& sc, int height)
{
    int l = 0, n = nItems, r = n;
    height -= 1;
    int lowInclusion = sc.lowInclusive;
    int highInclusion = sc.highInclusive;

    switch (sc.keyType) {
      case dybase_object_type:
        FIND(refKey, oid_t);
      case dybase_bool_type:
        FIND(boolKey, db_int1);
      case dybase_int_type:
        FIND(intKey, db_int4);
      case dybase_long_type:
        FIND(longKey, db_int8);
      case dybase_real_type:
        FIND(realKey, db_real8);
      case dybase_string_type:
        if (sc.low != NULL) {
            while (l < r)  {
                int i = (l+r) >> 1;
                if (compareStrings(sc.low, sc.lowSize, &charKey[strKey[i].offs], strKey[i].size) >= lowInclusion)
                {
                    l = i + 1;
                } else {
                    r = i;
                }
            }
            assert(r == l);
        }
        if (sc.high != NULL) {
            if (height == 0) {
                while (l < n) {
                    if (compareStrings(&charKey[strKey[l].offs], strKey[l].size, sc.high, sc.highSize) >= highInclusion)
                    {
                        return false;
                    }
                    sc.selection.add(strKey[l].oid);
                    l += 1;
                }
            } else {
                do {
                    dbBtreePage* pg = (dbBtreePage*)db->get(strKey[l].oid);
                    if (!pg->find(db, sc, height)) {
                        db->pool.unfix(pg);
                        return false;
                    }
                    db->pool.unfix(pg);
                    if (l == n) {
                        return true;
                    }
                    l += 1;
                } while (compareStrings(&charKey[strKey[l-1].offs], strKey[l-1].size, sc.high, sc.highSize) <= 0);
                return false;
            }
        } else {
            if (height == 0) {
                while (l < n) {
                    sc.selection.add(strKey[l].oid);
                    l += 1;
                }
            } else {
                do {
                    dbBtreePage* pg = (dbBtreePage*)db->get(strKey[l].oid);
                    if (!pg->find(db, sc, height)) {
                        db->pool.unfix(pg);
                        return false;
                    }
                    db->pool.unfix(pg);
                } while (++l <= n);
            }
        }
        return true;
    }
    if (height == 0) {
        while (l < n) {
            sc.selection.add(record[maxItems-1-l]);
            l += 1;
        }
    } else {
        do {
            dbBtreePage* pg = (dbBtreePage*)db->get(record[maxItems-1-l]);
            if (!pg->find(db, sc, height)) {
                db->pool.unfix(pg);
                return false;
            }
            db->pool.unfix(pg);
        } while (++l <= n);
    }
    return true;
}


oid_t dbBtreePage::allocate(dbDatabase* db, oid_t root, int type, item& ins)
{
    oid_t pageId = db->allocatePage();
    dbBtreePage* page = (dbBtreePage*)db->put(pageId);
    page->nItems = 1;
    if (type == dybase_string_type) {
        page->size = ins.keyLen*sizeof(char);
        page->strKey[0].offs = sizeof(page->charKey) - ins.keyLen*sizeof(char);
        page->strKey[0].size = ins.keyLen;
        page->strKey[0].oid = ins.oid;
        page->strKey[1].oid = root;
        memcpy(&page->charKey[page->strKey[0].offs], ins.charKey, ins.keyLen);
    } else {
        memcpy(page->charKey, ins.charKey, dbSizeofType[type]);
        page->record[maxItems-1] = ins.oid;
        page->record[maxItems-2] = root;
    }
    db->pool.unfix(page);
    return pageId;
}


#define INSERT(KEY, TYPE) {                                                 \
    TYPE key = ins.KEY;                                                     \
    while (l < r)  {                                                        \
        int i = (l+r) >> 1;                                                 \
        if (key > pg->KEY[i]) l = i+1; else r = i;                          \
    }                                                                       \
    assert(l == r);                                                         \
    /* insert before e[r] */                                                \
    if (--height != 0) {                                                    \
        result = insert(db, pg->record[maxItems-r-1], type, ins, unique, replace, height);   \
        if (result != dbBtree::overflow) {                                  \
            db->pool.unfix(pg);                                             \
            return result;                                                  \
        }                                                                   \
        n += 1;                                                             \
    } else if (r < n && key == pg->KEY[r]) {                                \
        if (replace) {                                                      \
            db->pool.unfix(pg);                                             \
            pg = (dbBtreePage*)db->put(tie, pageId);                        \
            pg->record[maxItems-r-1] = ins.oid;                             \
            return dbBtree::done;                                           \
        } else if (unique) {                                                \
            db->pool.unfix(pg);                                             \
            return dbBtree::duplicate;                                      \
        }                                                                   \
    }                                                                       \
    db->pool.unfix(pg);                                                     \
    pg = (dbBtreePage*)db->put(tie, pageId);                                \
    const int max = sizeof(pg->KEY) / (sizeof(oid_t) + sizeof(TYPE));       \
    if (n < max) {                                                          \
        memmove(&pg->KEY[r+1], &pg->KEY[r], (n - r)*sizeof(TYPE));          \
        memcpy(&pg->record[maxItems-n-1], &pg->record[maxItems-n],          \
               (n-r)*sizeof(oid_t));                                        \
        pg->KEY[r] = ins.KEY;                                               \
        pg->record[maxItems-r-1] = ins.oid;                                 \
        pg->nItems += 1;                                                    \
        return dbBtree::done;                                               \
    } else { /* page is full then divide page */                            \
        oid_t pageId = db->allocatePage();                                  \
        dbBtreePage* b = (dbBtreePage*)db->put(pageId);                     \
        assert(n == max);                                                   \
        const int m = max/2;                                                \
        if (r < m) {                                                        \
            memcpy(b->KEY, pg->KEY, r*sizeof(TYPE));                        \
            b->KEY[r] = ins.KEY;                                            \
            memcpy(&b->KEY[r+1], &pg->KEY[r], (m-r-1)*sizeof(TYPE));        \
            memcpy(pg->KEY, &pg->KEY[m-1], (max-m+1)*sizeof(TYPE));         \
            memcpy(&b->record[maxItems-r], &pg->record[maxItems-r],         \
                   r*sizeof(oid_t));                                        \
            b->record[maxItems-r-1] = ins.oid;                              \
            memcpy(&b->record[maxItems-m], &pg->record[maxItems-m+1],       \
                   (m-r-1)*sizeof(oid_t));                                  \
            memmove(&pg->record[maxItems-max+m-1],&pg->record[maxItems-max],\
                    (max-m+1)*sizeof(oid_t));                               \
        } else {                                                            \
            memcpy(b->KEY, pg->KEY, m*sizeof(TYPE));                        \
            memcpy(pg->KEY, &pg->KEY[m], (r-m)*sizeof(TYPE));               \
            pg->KEY[r-m] = ins.KEY;                                         \
            memcpy(&pg->KEY[r-m+1], &pg->KEY[r], (max-r)*sizeof(TYPE));     \
            memcpy(&b->record[maxItems-m], &pg->record[maxItems-m],         \
                   m*sizeof(oid_t));                                        \
            memmove(&pg->record[maxItems-r+m], &pg->record[maxItems-r],     \
                    (r-m)*sizeof(oid_t));                                   \
            pg->record[maxItems-r+m-1] = ins.oid;                           \
            memmove(&pg->record[maxItems-max+m-1],&pg->record[maxItems-max],\
                    (max-r)*sizeof(oid_t));                                 \
        }                                                                   \
        ins.oid = pageId;                                                   \
        ins.KEY = b->KEY[m-1];                                              \
        if (height == 0) {                                                  \
            pg->nItems = max - m + 1;                                       \
            b->nItems = m;                                                  \
        } else {                                                            \
            pg->nItems = max - m;                                           \
            b->nItems = m - 1;                                              \
        }                                                                   \
        db->pool.unfix(b);                                                  \
        return dbBtree::overflow;                                           \
    }                                                                       \
}


int dbBtreePage::insert(dbDatabase* db, oid_t pageId, int type, item& ins, bool unique, bool replace, int height)
{
    dbPutTie tie;
    dbBtreePage* pg = (dbBtreePage*)db->get(pageId);
    int result;
    int l = 0, n = pg->nItems, r = n;

    switch (type) {
      case dybase_object_type:
        INSERT(refKey, oid_t);
      case dybase_bool_type:
        INSERT(boolKey, db_int1);
      case dybase_int_type:
        INSERT(intKey, db_int4);
      case dybase_long_type:
        INSERT(longKey, db_int8);
      case dybase_real_type:
        INSERT(realKey, db_real8);
      case dybase_string_type:
      {
        while (l < r)  {
            int i = (l+r) >> 1;
            if (compareStrings(ins.charKey, ins.keyLen, &pg->charKey[pg->strKey[i].offs], pg->strKey[i].size) > 0) {
                l = i+1;
            } else {
                r = i;
            }
        }
        if (--height != 0) {
            result = insert(db, pg->strKey[r].oid, type, ins, unique, replace, height);
            assert (result != dbBtree::not_found);
            if (result != dbBtree::overflow) {
                db->pool.unfix(pg);
                return result;
            }
        } else if (r < n
                   && compareStrings(ins.charKey, ins.keyLen, &pg->charKey[pg->strKey[r].offs], pg->strKey[r].size) == 0)
        {
            if (replace) {
                db->pool.unfix(pg);
                pg = (dbBtreePage*)db->put(tie, pageId);
                pg->strKey[r].oid = ins.oid;
                return dbBtree::done;
            } else if (unique) {
                db->pool.unfix(pg);             \
                return dbBtree::duplicate;
            }
        }
        db->pool.unfix(pg);
        pg = (dbBtreePage*)db->put(tie, pageId);
        return pg->insertStrKey(db, r, ins, height);
      }
    }
    return dbBtree::done;
}

int dbBtreePage::insertStrKey(dbDatabase* db, int r, item& ins, int height)
{
    int n = height != 0 ? nItems+1 : nItems;
    // insert before e[r]
    int len = ins.keyLen;
    if (size + len*sizeof(char) + (n+1)*sizeof(str) <= sizeof(charKey)) {
        memmove(&strKey[r+1], &strKey[r], (n-r)*sizeof(str));
        size += len*sizeof(char);
        strKey[r].offs = sizeof(charKey) - size;
        strKey[r].size = len;
        strKey[r].oid = ins.oid;
        memcpy(&charKey[sizeof(charKey) - size], ins.charKey, len*sizeof(char));
        nItems += 1;
    } else { // page is full then divide page
        oid_t pageId = db->allocatePage();
        dbBtreePage* b = (dbBtreePage*)db->put(pageId);
        size_t moved = 0;
        size_t inserted = len*sizeof(char) + sizeof(str);
        long prevDelta = (1L << (sizeof(long)*8-1)) + 1;
        for (int bn = 0, i = 0; ; bn += 1) {
            size_t addSize, subSize;
            int j = nItems - i - 1;
            size_t keyLen = strKey[i].size;
            if (bn == r) {
                keyLen = len;
                inserted = 0;
                addSize = len;
                if (height == 0) {
                    subSize = 0;
                    j += 1;
                } else {
                    subSize = strKey[i].size;
                }
            } else {
                addSize = subSize = keyLen;
                if (height != 0) {
                    if (i + 1 != r) {
                        subSize += strKey[i+1].size;
                        j -= 1;
                    } else {
                        inserted = 0;
                    }
                }
            }
            long delta = (moved + addSize*sizeof(char) + (bn+1)*sizeof(str))
                - (j*sizeof(str) + size - subSize*sizeof(char) + inserted);
            if (delta >= -prevDelta) {
                char insKey[dbBtreePage::dbMaxKeyLen];
                if (bn <= r) {
                    memcpy(insKey, ins.charKey, len*sizeof(char));
                }
                if (height == 0) {
                    memcpy(ins.charKey, (char*)&b->charKey[b->strKey[bn-1].offs], b->strKey[bn-1].size);
                    ins.keyLen = b->strKey[bn-1].size;
                } else {
                    assert(((void)"String fits in the B-Tree page",
                            moved + (bn+1)*sizeof(str) <= sizeof(charKey)));
                    if (bn != r) {
                        ins.keyLen = keyLen;
                        memcpy(ins.charKey, &charKey[strKey[i].offs],
                               keyLen*sizeof(char));
                        b->strKey[bn].oid = strKey[i].oid;
                        size -= keyLen*sizeof(char);
                        i += 1;
                    } else {
                        b->strKey[bn].oid = ins.oid;
                    }
                }
                compactify(i);
                if (bn < r || (bn == r && height == 0)) {
                    memmove(&strKey[r-i+1], &strKey[r-i],
                            (n - r)*sizeof(str));
                    size += len*sizeof(char);
                    nItems += 1;
                    assert(((void)"String fits in the B-Tree page",
                            size + (n-i+1)*sizeof(str) <= sizeof(charKey)));
                    strKey[r-i].offs = sizeof(charKey) - size;
                    strKey[r-i].size = len;
                    strKey[r-i].oid = ins.oid;
                    memcpy(&charKey[strKey[r-i].offs], insKey, len*sizeof(char));
                }
                b->nItems = bn;
                b->size = moved;
                ins.oid = pageId;
                db->pool.unfix(b);
                assert(nItems > 0 && b->nItems > 0);
                return dbBtree::overflow;
            }
            prevDelta = delta;
            moved += keyLen*sizeof(char);
            assert(((void)"String fits in the B-Tree page",
                    moved + (bn+1)*sizeof(str) <= sizeof(charKey)));
            b->strKey[bn].size = keyLen;
            b->strKey[bn].offs = sizeof(charKey) - moved;
            if (bn == r) {
                b->strKey[bn].oid = ins.oid;
                memcpy(&b->charKey[b->strKey[bn].offs], ins.charKey, keyLen*sizeof(char));
            } else {
                b->strKey[bn].oid = strKey[i].oid;
                memcpy(&b->charKey[b->strKey[bn].offs],
                       &charKey[strKey[i].offs], keyLen*sizeof(char));
                size -= keyLen*sizeof(char);
                i += 1;
            }
        }
    }
    return size + sizeof(str)*(nItems+1) < sizeof(charKey)/2
        ? dbBtree::underflow : dbBtree::done;
}

void dbBtreePage::compactify(int m)
{
    int i, j, offs, len, n = nItems;
#ifdef NO_LARGE_LOCAL_ARRAYS
    int *size = new int[dbPageSize];
    int *index = new int[dbPageSize];
#else
    int size[dbPageSize];
    int index[dbPageSize];
#endif
    if (m == 0) {
        return;
    }
    if (m < 0) {
        m = -m;
        for (i = 0; i < n-m; i++) {
            len = strKey[i].size;
            size[strKey[i].offs + len*sizeof(char)] = len;
            index[strKey[i].offs + len*sizeof(char)] = i;
        }
        for (; i < n; i++) {
            len = strKey[i].size;
            size[strKey[i].offs + len*sizeof(char)] = len;
            index[strKey[i].offs + len*sizeof(char)] = -1;
        }
    } else {
        for (i = 0; i < m; i++) {
            len = strKey[i].size;
            size[strKey[i].offs + len*sizeof(char)] = len;
            index[strKey[i].offs + len*sizeof(char)] = -1;
        }
        for (; i < n; i++) {
            len = strKey[i].size;
            size[strKey[i].offs + len*sizeof(char)] = len;
            index[strKey[i].offs + len*sizeof(char)] = i - m;
            strKey[i-m].oid = strKey[i].oid;
            strKey[i-m].size = len;
        }
        strKey[i-m].oid = strKey[i].oid;
    }
    nItems = n -= m;
    for (offs = sizeof(charKey), i = offs; n != 0; i -= len) {
        len = size[i]*sizeof(char);
        j = index[i];
        if (j >= 0) {
            offs -= len;
            n -= 1;
            strKey[j].offs = offs;
            if (offs != i - len) {
                memmove(&charKey[offs], &charKey[(i - len)], len);
            }
        }
    }
#ifdef NO_LARGE_LOCAL_ARRAYS
    delete[] size;
    delete[] index;
#endif
}

int dbBtreePage::removeStrKey(int r)
{
    int len = strKey[r].size;
    int offs = strKey[r].offs;
    memmove(charKey + sizeof(charKey) - size + len*sizeof(char),
            charKey + sizeof(charKey) - size,
            size - sizeof(charKey) + offs);
    memcpy(&strKey[r], &strKey[r+1], (nItems-r)*sizeof(str));
    nItems -= 1;
    size -= len*sizeof(char);
    for (int i = nItems; --i >= 0; ) {
        if (strKey[i].offs < offs) {
            strKey[i].offs += len*sizeof(char);
        }
    }
    return size + sizeof(str)*(nItems+1) < sizeof(charKey)/2
        ? dbBtree::underflow : dbBtree::done;
}

int dbBtreePage::replaceStrKey(dbDatabase* db, int r, item& ins, int height)
{
    ins.oid = strKey[r].oid;
    removeStrKey(r);
    return insertStrKey(db, r, ins, height);
}

int dbBtreePage::handlePageUnderflow(dbDatabase* db, int r, int type, item& rem, int height)
{
    dbPutTie tie;
    if (type == dybase_string_type) {
        dbBtreePage* a = (dbBtreePage*)db->put(tie, strKey[r].oid);
        int an = a->nItems;
        if (r < (int)nItems) { // exists greater page
            dbBtreePage* b = (dbBtreePage*)db->get(strKey[r+1].oid);
            int bn = b->nItems;
            size_t merged_size = (an+bn)*sizeof(str) + a->size + b->size;
            if (height != 1) {
                merged_size += strKey[r].size*sizeof(char) + sizeof(str)*2;
            }
            if (merged_size > sizeof(charKey)) {
                // reallocation of nodes between pages a and b
                int i, j, k;
                dbPutTie tie;
                db->pool.unfix(b);
                b = (dbBtreePage*)db->put(tie, strKey[r+1].oid);
                size_t size_a = a->size;
                size_t size_b = b->size;
                size_t addSize, subSize;
                if (height != 1) {
                    addSize = strKey[r].size;
                    subSize = b->strKey[0].size;
                } else {
                    addSize = subSize = b->strKey[0].size;
                }
                i = 0;
                long prevDelta = (an*sizeof(str) + size_a) - (bn*sizeof(str) + size_b);
                while (true) {
                    i += 1;
                    long delta = ((an+i)*sizeof(str) + size_a + addSize*sizeof(char))
                         - ((bn-i)*sizeof(str) + size_b - subSize*sizeof(char));
                    if (delta >= 0) {
                        if (delta >= -prevDelta) {
                            i -= 1;
                        }
                        break;
                    }
                    size_a += addSize*sizeof(char);
                    size_b -= subSize*sizeof(char);
                    prevDelta = delta;
                    if (height != 1) {
                        addSize = subSize;
                        subSize = b->strKey[i].size;
                    } else {
                        addSize = subSize = b->strKey[i].size;
                    }
                }
                int result = dbBtree::done;
                if (i > 0) {
                    k = i;
                    if (height != 1) {
                        int len = strKey[r].size;
                        a->size += len*sizeof(char);
                        a->strKey[an].offs = sizeof(a->charKey) - a->size;
                        a->strKey[an].size = len;
                        memcpy(a->charKey + a->strKey[an].offs,
                               charKey + strKey[r].offs, len*sizeof(char));
                        k -= 1;
                        an += 1;
                        a->strKey[an+k].oid = b->strKey[k].oid;
                        b->size -= b->strKey[k].size*sizeof(char);
                    }
                    for (j = 0; j < k; j++) {
                        int len = b->strKey[j].size;
                        a->size += len*sizeof(char);
                        b->size -= len*sizeof(char);
                        a->strKey[an].offs = sizeof(a->charKey) - a->size;
                        a->strKey[an].size = len;
                        a->strKey[an].oid = b->strKey[j].oid;
                        memcpy(a->charKey + a->strKey[an].offs,
                               b->charKey + b->strKey[j].offs, len*sizeof(char));
                        an += 1;
                    }
                    memcpy(rem.charKey, b->charKey + b->strKey[i-1].offs,
                           b->strKey[i-1].size*sizeof(char));
                    rem.keyLen = b->strKey[i-1].size;
                    result = replaceStrKey(db, r, rem, height);
                    a->nItems = an;
                    b->compactify(i);
                }
                assert(a->nItems > 0 && b->nItems > 0);
                return result;
            } else { // merge page b to a
                if (height != 1) {
                    a->size += (a->strKey[an].size = strKey[r].size)*sizeof(char);
                    a->strKey[an].offs = sizeof(charKey) - a->size;
                    memcpy(&a->charKey[a->strKey[an].offs],
                           &charKey[strKey[r].offs], strKey[r].size*sizeof(char));
                    an += 1;
                    a->strKey[an+bn].oid = b->strKey[bn].oid;
                }
                for (int i = 0; i < bn; i++, an++) {
                    a->strKey[an] = b->strKey[i];
                    a->strKey[an].offs -= a->size;
                }
                a->size += b->size;
                a->nItems = an;
                memcpy(a->charKey + sizeof(charKey) - a->size,
                       b->charKey + sizeof(charKey) - b->size,
                       b->size);
                db->pool.unfix(b);
                db->freePage(strKey[r+1].oid);
                strKey[r+1].oid = strKey[r].oid;
                return removeStrKey(r);
            }
        } else { // page b is before a
            dbBtreePage* b = (dbBtreePage*)db->get(strKey[r-1].oid);
            int bn = b->nItems;
            size_t merged_size = (an+bn)*sizeof(str) + a->size + b->size;
            if (height != 1) {
                merged_size += strKey[r-1].size*sizeof(char) + sizeof(str)*2;
            }
            if (merged_size > sizeof(charKey)) {
                // reallocation of nodes between pages a and b
                dbPutTie tie;
                int i, j, k;
                db->pool.unfix(b);
                b = (dbBtreePage*)db->put(tie, strKey[r-1].oid);
                size_t size_a = a->size;
                size_t size_b = b->size;
                size_t addSize, subSize;
                if (height != 1) {
                    addSize = strKey[r-1].size;
                    subSize = b->strKey[bn-1].size;
                } else {
                    addSize = subSize = b->strKey[bn-1].size;
                }
                i = 0;
                long prevDelta = (an*sizeof(str) + size_a) - (bn*sizeof(str) + size_b);
                while (true) {
                    i += 1;
                    long delta = ((an+i)*sizeof(str) + size_a + addSize*sizeof(char))
                         - ((bn-i)*sizeof(str) + size_b - subSize*sizeof(char));
                    if (delta >= 0) {
                        if (delta >= -prevDelta) {
                            i -= 1;
                        }
                        break;
                    }
                    prevDelta = delta;
                    size_a += addSize*sizeof(char);
                    size_b -= subSize*sizeof(char);
                    if (height != 1) {
                        addSize = subSize;
                        subSize = b->strKey[bn-i-1].size;
                    } else {
                        addSize = subSize = b->strKey[bn-i-1].size;
                    }
                }
                int result = dbBtree::done;
                if (i > 0) {
                    k = i;
                    assert(i < bn);
                    if (height != 1) {
                        memmove(&a->strKey[i], a->strKey, (an+1)*sizeof(str));
                        b->size -= b->strKey[bn-k].size*sizeof(char);
                        k -= 1;
                        a->strKey[k].oid = b->strKey[bn].oid;
                        int len = strKey[r-1].size;
                        a->strKey[k].size = len;
                        a->size += len*sizeof(char);
                        a->strKey[k].offs = sizeof(charKey) - a->size;
                        memcpy(&a->charKey[a->strKey[k].offs],
                               &charKey[strKey[r-1].offs], len*sizeof(char));
                    } else {
                        memmove(&a->strKey[i], a->strKey, an*sizeof(str));
                    }
                    for (j = 0; j < k; j++) {
                        int len = b->strKey[bn-k+j].size;
                        a->size += len*sizeof(char);
                        b->size -= len*sizeof(char);
                        a->strKey[j].offs = sizeof(a->charKey) - a->size;
                        a->strKey[j].size = len;
                        a->strKey[j].oid = b->strKey[bn-k+j].oid;
                        memcpy(a->charKey + a->strKey[j].offs,
                               b->charKey + b->strKey[bn-k+j].offs, len*sizeof(char));
                    }
                    an += i;
                    a->nItems = an;
                    memcpy(rem.charKey, b->charKey + b->strKey[bn-k-1].offs,
                           b->strKey[bn-k-1].size*sizeof(char));
                    rem.keyLen =  b->strKey[bn-k-1].size;
                    result = replaceStrKey(db, r-1, rem, height);
                    b->compactify(-i);
                }
                assert(a->nItems > 0 && b->nItems > 0);
                return result;
            } else { // merge page b to a
                if (height != 1) {
                    memmove(a->strKey + bn + 1, a->strKey, (an+1)*sizeof(str));
                    a->size += (a->strKey[bn].size = strKey[r-1].size)*sizeof(char);
                    a->strKey[bn].offs = sizeof(charKey) - a->size;
                    a->strKey[bn].oid = b->strKey[bn].oid;
                    memcpy(&a->charKey[a->strKey[bn].offs],
                           &charKey[strKey[r-1].offs], strKey[r-1].size*sizeof(char));
                    an += 1;
                } else {
                    memmove(a->strKey + bn, a->strKey, an*sizeof(str));
                }
                for (int i = 0; i < bn; i++) {
                    a->strKey[i] = b->strKey[i];
                    a->strKey[i].offs -= a->size;
                }
                an += bn;
                a->nItems = an;
                a->size += b->size;
                memcpy(a->charKey + sizeof(charKey) - a->size,
                       b->charKey + sizeof(charKey) - b->size,
                       b->size);
                db->pool.unfix(b);
                db->freePage(strKey[r-1].oid);
                return removeStrKey(r-1);
            }
        }
    } else {
        dbBtreePage* a = (dbBtreePage*)db->put(tie, record[maxItems-r-1]);
        size_t sizeofType = dbSizeofType[type];
        int an = a->nItems;
        if (r < int(nItems)) { // exists greater page
            dbBtreePage* b = (dbBtreePage*)db->get(record[maxItems-r-2]);
            int bn = b->nItems;
            assert(bn >= an);
            if (height != 1) {
                memcpy(a->charKey + an*sizeofType, charKey + r*sizeofType, sizeofType);
                an += 1;
                bn += 1;
            }
            size_t merged_size = (an+bn)*(sizeof(oid_t) + sizeofType);
            if (merged_size > sizeof(charKey)) {
                // reallocation of nodes between pages a and b
                int i = bn - ((an + bn) >> 1);
                dbPutTie tie;
                db->pool.unfix(b);
                b = (dbBtreePage*)db->put(tie, record[maxItems-r-2]);
                memcpy(a->charKey + an*sizeofType, b->charKey, i*sizeofType);
                memcpy(b->charKey, b->charKey + i*sizeofType, (bn-i)*sizeofType);
                memcpy(&a->record[maxItems-an-i], &b->record[maxItems-i],
                       i*sizeof(oid_t));
                memmove(&b->record[maxItems-bn+i], &b->record[maxItems-bn],
                        (bn-i)*sizeof(oid_t));
                memcpy(charKey + r*sizeofType,  a->charKey + (an+i-1)*sizeofType,
                       sizeofType);
                b->nItems -= i;
                a->nItems += i;
                return dbBtree::done;
            } else { // merge page b to a
                memcpy(a->charKey + an*sizeofType, b->charKey, bn*sizeofType);
                memcpy(&a->record[maxItems-an-bn], &b->record[maxItems-bn],
                       bn*sizeof(oid_t));
                db->pool.unfix(b);
                db->freePage(record[maxItems-r-2]);
                memmove(&record[maxItems-nItems], &record[maxItems-nItems-1],
                        (nItems - r - 1)*sizeof(oid_t));
                memcpy(charKey + r*sizeofType, charKey + (r+1)*sizeofType,
                       (nItems - r - 1)*sizeofType);
                a->nItems += bn;
                nItems -= 1;
                return (nItems+1)*(sizeofType + sizeof(oid_t)) < sizeof(charKey)/2
                    ? dbBtree::underflow : dbBtree::done;
            }
        } else { // page b is before a
            dbBtreePage* b = (dbBtreePage*)db->get(record[maxItems-r]);
            int bn = b->nItems;
            assert(bn >= an);
            if (height != 1) {
                an += 1;
                bn += 1;
            }
            size_t merged_size = (an+bn)*(sizeof(oid_t) + sizeofType);
            if (merged_size > sizeof(charKey)) {
                // reallocation of nodes between pages a and b
                int i = bn - ((an + bn) >> 1);
                dbPutTie tie;
                db->pool.unfix(b);
                b = (dbBtreePage*)db->put(tie, record[maxItems-r]);
                memmove(a->charKey + i*sizeofType, a->charKey, an*sizeofType);
                memcpy(a->charKey, b->charKey + (bn-i)*sizeofType, i*sizeofType);
                memcpy(&a->record[maxItems-an-i], &a->record[maxItems-an],
                       an*sizeof(oid_t));
                memcpy(&a->record[maxItems-i], &b->record[maxItems-bn],
                       i*sizeof(oid_t));
                if (height != 1) {
                    memcpy(a->charKey + (i-1)*sizeofType, charKey + (r-1)*sizeofType,
                           sizeofType);
                }
                memcpy(charKey + (r-1)*sizeofType, b->charKey + (bn-i-1)*sizeofType,
                       sizeofType);
                b->nItems -= i;
                a->nItems += i;
                return dbBtree::done;
            } else { // merge page b to a
                memmove(a->charKey + bn*sizeofType, a->charKey, an*sizeofType);
                memcpy(a->charKey, b->charKey, bn*sizeofType);
                memcpy(&a->record[maxItems-an-bn], &a->record[maxItems-an],
                       an*sizeof(oid_t));
                memcpy(&a->record[maxItems-bn], &b->record[maxItems-bn],
                       bn*sizeof(oid_t));
                if (height != 1) {
                    memcpy(a->charKey + (bn-1)*sizeofType, charKey + (r-1)*sizeofType,
                           sizeofType);
                }
                db->pool.unfix(b);
                db->freePage(record[maxItems-r]);
                record[maxItems-r] = record[maxItems-r-1];
                a->nItems += bn;
                nItems -= 1;
                return (nItems+1)*(sizeofType + sizeof(oid_t)) < sizeof(charKey)/2
                    ? dbBtree::underflow : dbBtree::done;
            }
        }
    }
}

#define REMOVE(KEY,TYPE) {                                                        \
    TYPE key = rem.KEY;                                                           \
    while (l < r)  {                                                              \
        i = (l+r) >> 1;                                                           \
        if (key > pg->KEY[i]) l = i+1; else r = i;                                \
    }                                                                             \
    if (--height == 0) {                                                          \
        oid_t oid = rem.oid;                                                      \
        while (r < n) {                                                           \
            if (key == pg->KEY[r]) {                                              \
                if (pg->record[maxItems-r-1] == oid || oid == 0) {                \
                    db->pool.unfix(pg);                                           \
                    pg = (dbBtreePage*)db->put(tie, pageId);                      \
                    memcpy(&pg->KEY[r], &pg->KEY[r+1], (n - r - 1)*sizeof(TYPE)); \
                    memmove(&pg->record[maxItems-n+1], &pg->record[maxItems-n],   \
                            (n - r - 1)*sizeof(oid_t));                           \
                    pg->nItems = --n;                                             \
                    return n*(sizeof(TYPE)+sizeof(oid_t)) < sizeof(pg->charKey)/2 \
                        ? dbBtree::underflow : dbBtree::done;                     \
                }                                                                 \
            } else {                                                              \
                break;                                                            \
            }                                                                     \
            r += 1;                                                               \
        }                                                                         \
        db->pool.unfix(pg);                                                       \
        return dbBtree::not_found;                                                \
    }                                                                             \
    break;                                                                        \
}

int dbBtreePage::remove(dbDatabase* db, oid_t pageId, int type, item& rem,  int height)
{
    dbBtreePage* pg = (dbBtreePage*)db->get(pageId);
    dbPutTie tie;
    int i, n = pg->nItems, l = 0, r = n;

    switch (type) {
      case dybase_object_type:
        REMOVE(refKey, oid_t);
      case dybase_bool_type:
        REMOVE(boolKey, db_int1);
      case dybase_int_type:
        REMOVE(intKey, db_int4);
      case dybase_long_type:
        REMOVE(longKey, db_int8);
      case dybase_real_type:
        REMOVE(realKey, db_real8);
      case dybase_string_type:
      {
        while (l < r)  {
            i = (l+r) >> 1;
            if (compareStrings(rem.charKey, rem.keyLen, &pg->charKey[pg->strKey[i].offs], pg->strKey[i].size) > 0) {
                l = i+1;
            } else {
                r = i;
            }
        }
        if (--height != 0) {
            do {
                switch (remove(db, pg->strKey[r].oid, type, rem, height)) {
                  case dbBtree::underflow:
                    db->pool.unfix(pg);
                    pg = (dbBtreePage*)db->put(tie, pageId);
                    return pg->handlePageUnderflow(db, r, type, rem, height);
                  case dbBtree::done:
                    db->pool.unfix(pg);
                    return dbBtree::done;
                  case dbBtree::overflow:
                    db->pool.unfix(pg);
                    pg = (dbBtreePage*)db->put(tie, pageId);
                    return pg->insertStrKey(db, r, rem, height);
                }
            } while (++r <= n);
        } else {
            while (r < n) {
                if (compareStrings(rem.charKey, rem.keyLen, &pg->charKey[pg->strKey[r].offs], pg->strKey[r].size) == 0) {
                    if (pg->strKey[r].oid == rem.oid || rem.oid == 0) {
                        db->pool.unfix(pg);
                        pg = (dbBtreePage*)db->put(tie, pageId);
                        return pg->removeStrKey(r);
                    }
                } else {
                    break;
                }
                r += 1;
            }
        }
        db->pool.unfix(pg);
        return dbBtree::not_found;
      }
      default:
        assert(false);
    }
    do {
        switch (remove(db, pg->record[maxItems-r-1], type, rem, height)) {
          case dbBtree::underflow:
            db->pool.unfix(pg);
            pg = (dbBtreePage*)db->put(tie, pageId);
            return pg->handlePageUnderflow(db, r, type, rem, height);
          case dbBtree::done:
            db->pool.unfix(pg);
            return dbBtree::done;
        }
    } while (++r <= n);

    db->pool.unfix(pg);
    return dbBtree::not_found;
}


void dbBtreePage::purge(dbDatabase* db, oid_t pageId, int type, int height)
{
    if (--height != 0) {
        dbBtreePage* pg = (dbBtreePage*)db->get(pageId);
        int n = pg->nItems+1;
        if (type == dybase_string_type) { // page of strings
            while (--n >= 0) {
                purge(db, pg->strKey[n].oid, type, height);
            }
        } else {
            while (--n >= 0) {
                purge(db, pg->record[maxItems-n-1], type, height);
            }
        }
        db->pool.unfix(pg);
    }
    db->freePage(pageId);
}

void dbBtreePage::markPage(dbDatabase* db, oid_t pageId, int type, int height)
{
    dbBtreePage* pg = (dbBtreePage*)db->pool.get(db->getGCPos(pageId) & ~dbPageObjectFlag);
    int i, n = pg->nItems;
    if (--height != 0) {
        if (type == dybase_string_type) { // page of strings
            for (i = 0; i <= n; i++) {
                markPage(db, pg->strKey[i].oid, type, height);
            }
        } else {
            for (i = 0; i <= n; i++) {
                markPage(db, pg->record[maxItems-i-1], type, height);
            }
        }
    } else {
        if (type != dybase_string_type) { // page of scalars
            for (i = 0; i < n; i++) {
                db->markOid(pg->record[maxItems-i-1]);
            }
        } else { // page of strings
            for (i = 0; i < n; i++) {
                db->markOid(pg->strKey[i].oid);
            }
        }
    }
    db->pool.unfix(pg);
}

int dbBtreeIterator::compare(void* key, int keyType, dbBtreePage* pg, int pos)
{
    switch (keyType) {
      case dybase_bool_type:
        return *(db_int1*)key - pg->boolKey[pos];
      case dybase_int_type:
        return *(db_int4*)key - pg->intKey[pos];
      case dybase_long_type:
        return *(db_int8*)key < pg->longKey[pos] ? -1 : *(db_int8*)key == pg->longKey[pos] ? 0 : 1;
      case dybase_real_type:
        return *(db_real8*)key < pg->realKey[pos] ? -1 : *(db_real8*)key == pg->realKey[pos] ? 0 : 1;
      case dybase_object_type:
        return *(oid_t*)key - pg->refKey[pos];
    }
    return 0;
}

int dbBtreeIterator::compareStr(void* key, size_t keyLength, dbBtreePage* pg, int pos)
{
    return compareStrings(key, keyLength, &pg->charKey[pg->strKey[pos].offs], pg->strKey[pos].size);
}

dbBtreeIterator::dbBtreeIterator(dbDatabase* db, oid_t treeId, int type,
                                 void* from, size_t fromLength, int fromInclusion,
                                 void* till, size_t tillLength, int tillInclusion,
                                 bool ascent)
{
    int l, r, i;
    dbGetTie tie;
    dbBtree* tree = (dbBtree*)db->getObject(tie, treeId);
    sp = 0;

    if (tree->height == 0) {
        return;
    }

    if (type != tree->type) {
        if (from != NULL || till != NULL) {
            db->throwException(dybase_bad_key_type, "Type of the key doesn't match index type");
        } else {
            type = tree->type;
        }
    }
    dbBtreePage* pg;
    this->db = db;
    this->from = from;
    this->till = till;
    this->fromLength = fromLength;
    this->tillLength = tillLength;
    this->fromInclusion = fromInclusion;
    this->tillInclusion = tillInclusion;
    this->type = type;
    this->ascent = ascent;
    this->height = tree->height;
    int height = tree->height;

    if (from != NULL) {
        switch (type) {
          case dybase_object_type:
            from_val.refKey = *(oid_t*)from;
            this->from = &from_val.refKey;
            break;
          case dybase_bool_type:
            from_val.boolKey = *(db_int1*)from;
            this->from = &from_val.boolKey;
            break;
          case dybase_int_type:
            from_val.intKey = *(db_int4*)from;
            this->from = &from_val.intKey;
            break;
          case dybase_long_type:
            from_val.longKey = *(db_int8*)from;
            this->from = &from_val.longKey;
            break;
          case dybase_real_type:
            from_val.realKey = *(db_real8*)from;
            this->from = &from_val.realKey;
            break;
        }
    }
    if (till != NULL) {
        switch (type) {
          case dybase_object_type:
            till_val.refKey = *(oid_t*)till;
            this->till = &till_val.refKey;
            break;
          case dybase_bool_type:
            till_val.boolKey = *(db_int1*)till;
            this->till = &till_val.boolKey;
            break;
          case dybase_int_type:
            till_val.intKey = *(db_int4*)till;
            this->till = &till_val.intKey;
            break;
          case dybase_long_type:
            till_val.longKey = *(db_int8*)till;
            this->till = &till_val.longKey;
            break;
          case dybase_real_type:
            till_val.realKey = *(db_real8*)till;
            this->till = &till_val.realKey;
            break;
        }
    }


    int pageId = tree->root;

    if (type == dybase_string_type) {
        if (ascent) {
            if (from == NULL) {
                while (--height >= 0) {
                    posStack[sp] = 0;
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    pageId = pg->strKey[0].oid;
                    end = pg->nItems;
                    db->pool.unfix(pg);
                    sp += 1;
                }
            } else {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    l = 0;
                    r = pg->nItems;
                    while (l < r)  {
                        i = (l+r) >> 1;
                        if (compareStr(from, fromLength, pg, i) >= fromInclusion) {
                            l = i + 1;
                        } else {
                            r = i;
                        }
                    }
                    assert(r == l);
                    posStack[sp] = r;
                    pageId = pg->strKey[r].oid;
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                l = 0;
                end = r = pg->nItems;
                while (l < r)  {
                    i = (l+r) >> 1;
                    if (compareStr(from, fromLength, pg, i) >= fromInclusion) {
                        l = i + 1;
                    } else {
                        r = i;
                    }
                }
                assert(r == l);
                if (r == end) {
                    sp += 1;
                    gotoNextItem(pg, r-1);
                } else {
                    posStack[sp++] = r;
                    db->pool.unfix(pg);
                }
            }
            if (sp != 0 && till != NULL) {
                pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                if (-compareStr(till, tillLength, pg, posStack[sp-1]) >= tillInclusion) {
                    sp = 0;
                }
                db->pool.unfix(pg);
            }
        } else { // descent order
            if (till == NULL) {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    posStack[sp] = pg->nItems;
                    pageId = pg->strKey[posStack[sp]].oid;
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                posStack[sp++] = pg->nItems-1;
                db->pool.unfix(pg);
            } else {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    l = 0;
                    r = pg->nItems;
                    while (l < r)  {
                        i = (l+r) >> 1;
                        if (compareStr(till, tillLength, pg, i) >= 1-tillInclusion) {
                            l = i + 1;
                        } else {
                            r = i;
                        }
                    }
                    assert(r == l);
                    posStack[sp] = r;
                    pageId = pg->strKey[r].oid;
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                l = 0;
                r = pg->nItems;
                while (l < r)  {
                    i = (l+r) >> 1;
                    if (compareStr(till, tillLength, pg, i) >= 1-tillInclusion) {
                        l = i + 1;
                    } else {
                        r = i;
                    }
                }
                assert(r == l);
                if (r == 0) {
                    sp += 1;
                    gotoNextItem(pg, r);
                } else {
                    posStack[sp++] = r-1;
                    db->pool.unfix(pg);
                }
            }
            if (sp != 0 && from != NULL) {
                pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                if (compareStr(from, fromLength, pg, posStack[sp-1]) >= fromInclusion) {
                    sp = 0;
                }
                db->pool.unfix(pg);
            }
        }
    } else { // scalar type
        if (ascent) {
            if (from == NULL) {
                while (--height >= 0) {
                    posStack[sp] = 0;
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    pageId = pg->record[dbBtreePage::maxItems-1];
                    end = pg->nItems;
                    db->pool.unfix(pg);
                    sp += 1;
                }
            } else {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    l = 0;
                    r = pg->nItems;
                    while (l < r)  {
                        i = (l+r) >> 1;
                        if (compare(from, type, pg, i) >= fromInclusion) {
                            l = i + 1;
                        } else {
                            r = i;
                        }
                    }
                    assert(r == l);
                    posStack[sp] = r;
                    pageId = pg->record[dbBtreePage::maxItems-1-r];
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                l = 0;
                r = end = pg->nItems;
                while (l < r)  {
                    i = (l+r) >> 1;
                    if (compare(from, type, pg, i) >= fromInclusion) {
                        l = i + 1;
                    } else {
                        r = i;
                    }
                }
                assert(r == l);
                if (r == end) {
                    sp += 1;
                    gotoNextItem(pg, r-1);
                } else {
                    posStack[sp++] = r;
                    db->pool.unfix(pg);
                }
            }
            if (sp != 0 && till != NULL) {
                pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                if (-compare(till, type, pg, posStack[sp-1]) >= tillInclusion) {
                    sp = 0;
                }
                db->pool.unfix(pg);
            }
        } else { // descent order
            if (till == NULL) {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    posStack[sp] = pg->nItems;
                    pageId = pg->record[dbBtreePage::maxItems-1-posStack[sp]];
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                posStack[sp++] = pg->nItems-1;
                db->pool.unfix(pg);
            } else {
                while (--height > 0) {
                    pageStack[sp] = pageId;
                    pg = (dbBtreePage*)db->get(pageId);
                    l = 0;
                    r = pg->nItems;
                    while (l < r)  {
                        i = (l+r) >> 1;
                        if (compare(till, type, pg, i) >= 1-tillInclusion) {
                            l = i + 1;
                        } else {
                            r = i;
                        }
                    }
                    assert(r == l);
                    posStack[sp] = r;
                    pageId = pg->record[dbBtreePage::maxItems-1-r];
                    db->pool.unfix(pg);
                    sp += 1;
                }
                pageStack[sp] = pageId;
                pg = (dbBtreePage*)db->get(pageId);
                l = 0;
                r = pg->nItems;
                while (l < r)  {
                    i = (l+r) >> 1;
                    if (compare(till, type, pg, i) >= 1-tillInclusion) {
                        l = i + 1;
                    } else {
                        r = i;
                    }
                }
                assert(r == l);
                if (r == 0) {
                    sp += 1;
                    gotoNextItem(pg, r);
                } else {
                    posStack[sp++] = r-1;
                    db->pool.unfix(pg);
                }
            }
            if (sp != 0 && from != NULL) {
                pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                if (compare(from, type, pg, posStack[sp-1]) >= fromInclusion) {
                    sp = 0;
                }
                db->pool.unfix(pg);
            }
        }
    }
}

oid_t dbBtreeIterator::next()
{
    if (sp == 0) {
        return 0;
    }
    int pos = posStack[sp-1];
    dbBtreePage* pg = (dbBtreePage*)db->get(pageStack[sp-1]);
    oid_t oid = (type == dybase_string_type) ? pg->strKey[pos].oid : pg->record[dbBtreePage::maxItems-1-pos];
    gotoNextItem(pg, pos);
    return oid;
}

void dbBtreeIterator::gotoNextItem(dbBtreePage* pg, int pos)
{
    oid_t pageId;
    if (type == dybase_string_type) {
        if (ascent) {
            if (++pos == end) {
                while (--sp != 0) {
                    db->pool.unfix(pg);
                    pos = posStack[sp-1];
                    pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                    if (++pos <= (int)pg->nItems) {
                        posStack[sp-1] = pos;
                        do {
                            pageId = pg->strKey[pos].oid;
                            db->pool.unfix(pg);
                            pg = (dbBtreePage*)db->get(pageId);
                            end = pg->nItems;
                            pageStack[sp] = pageId;
                            posStack[sp] = pos = 0;
                        } while (++sp < height);
                        break;
                    }
                }
            } else {
                posStack[sp-1] = pos;
            }
            if (sp != 0 && till != NULL && -compareStr(till, tillLength,  pg, pos) >= tillInclusion) {
                sp = 0;
            }
        } else { // descent order
            if (--pos < 0) {
                while (--sp != 0) {
                    db->pool.unfix(pg);
                    pos = posStack[sp-1];
                    pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                    if (--pos >= 0) {
                        posStack[sp-1] = pos;
                        do {
                            pageId = pg->strKey[pos].oid;
                            db->pool.unfix(pg);
                            pg = (dbBtreePage*)db->get(pageId);
                            pageStack[sp] = pageId;
                            posStack[sp] = pos = pg->nItems;
                        } while (++sp < height);
                        posStack[sp-1] = --pos;
                        break;
                    }
                }
            } else {
                posStack[sp-1] = pos;
            }
            if (sp != 0 && from != NULL && compareStr(from, fromLength, pg, pos) >= fromInclusion) {
                sp = 0;
            }
        }
    } else { // scalar type
        if (ascent) {
            if (++pos == end) {
                while (--sp != 0) {
                    db->pool.unfix(pg);
                    pos = posStack[sp-1];
                    pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                    if (++pos <= (int)pg->nItems) {
                        posStack[sp-1] = pos;
                        do {
                            pageId = pg->record[dbBtreePage::maxItems-1-pos];
                            db->pool.unfix(pg);
                            pg = (dbBtreePage*)db->get(pageId);
                            end = pg->nItems;
                            pageStack[sp] = pageId;
                            posStack[sp] = pos = 0;
                        } while (++sp < height);
                        break;
                    }
                }
            } else {
                posStack[sp-1] = pos;
            }
            if (sp != 0 && till != NULL && -compare(till, type, pg, pos) >= tillInclusion) {
                sp = 0;
            }
        } else { // descent order
            if (--pos < 0) {
                while (--sp != 0) {
                    db->pool.unfix(pg);
                    pos = posStack[sp-1];
                    pg = (dbBtreePage*)db->get(pageStack[sp-1]);
                    if (--pos >= 0) {
                        posStack[sp-1] = pos;
                        do {
                            pageId = pg->record[dbBtreePage::maxItems-1-pos];
                            db->pool.unfix(pg);
                            pg = (dbBtreePage*)db->get(pageId);
                            pageStack[sp] = pageId;
                            posStack[sp] = pos = pg->nItems;
                        } while (++sp < height);
                        posStack[sp-1] = --pos;
                        break;
                    }
                }
            } else {
                posStack[sp-1] = pos;
            }
            if (sp != 0 && from != NULL && compare(from, type, pg, pos) >= fromInclusion) {
                sp = 0;
            }
        }
    }
    db->pool.unfix(pg);
}
//-< FILE.CPP >------------------------------------------------------*--------*
// GigaBASE                  Version 1.0         (c) 1999  GARRET    *     ?  *
// (Post Relational Database Management System)                      *   /\|  *
//                                                                   *  /  \  *
//                          Created:     20-Nov-98    K.A. Knizhnik  * / [] \ *
//                          Last update: 10-Dec-98    K.A. Knizhnik  * GARRET *
//-------------------------------------------------------------------*--------*
// System dependent implementation of mapped on memory file
//-------------------------------------------------------------------*--------*

#define _LARGEFILE64_SOURCE 1 // access to files greater than 2Gb in Solaris
#define _LARGE_FILE_API     1 // access to files greater than 2Gb in AIX

#include "database.h"

#ifndef O_LARGEFILE
#define O_LARGEFILE 0
#endif

dbFile::~dbFile()
{
    close();
}

#if defined(_WIN32)

class OS_info : public OSVERSIONINFO {
  public:
    OS_info() {
        dwOSVersionInfoSize = sizeof(OSVERSIONINFO);
        GetVersionEx(this);
    }
};

static OS_info osinfo;

#define BAD_POS 0xFFFFFFFF // returned by SetFilePointer and GetFileSize


dbFile::dbFile()
{
    fh = INVALID_HANDLE_VALUE;
}

int dbFile::open(char const* fileName, int attr)
{
    fh = CreateFile(fileName, (attr & read_only)
                    ? GENERIC_READ : GENERIC_READ|GENERIC_WRITE,
                    (attr & read_only) ? FILE_SHARE_READ : 0, NULL,
                    (attr & read_only) ? OPEN_EXISTING : (attr & truncate) ? CREATE_ALWAYS : OPEN_ALWAYS,
#ifdef _WINCE
                        FILE_ATTRIBUTE_NORMAL,
#else
                    ((attr & no_buffering) ? FILE_FLAG_NO_BUFFERING : 0)
                    | ((attr & sequential) ? FILE_FLAG_SEQUENTIAL_SCAN : FILE_FLAG_RANDOM_ACCESS),
#endif
                    NULL);
    if (fh == INVALID_HANDLE_VALUE) {
        return GetLastError();
    }
    return ok;
}

int dbFile::read(offs_t pos, void* buf, size_t size)
{
    DWORD readBytes;
    if (osinfo.dwPlatformId == VER_PLATFORM_WIN32_NT) {
        OVERLAPPED Overlapped;
        Overlapped.Offset = nat8_low_part(pos);
        Overlapped.OffsetHigh = nat8_high_part(pos);
        Overlapped.hEvent = NULL;
        if (ReadFile(fh, buf, size, &readBytes, &Overlapped)) {
            return readBytes == size ? ok : eof;
        } else {
            int rc = GetLastError();
            return (rc == ERROR_HANDLE_EOF) ? eof : rc;
        }
    } else {
        LONG high_pos = nat8_high_part(pos);
        LONG low_pos = nat8_low_part(pos);
        dbCriticalSection cs(mutex);
        if (SetFilePointer(fh, low_pos,
                           &high_pos, FILE_BEGIN) != BAD_POS
            && ReadFile(fh, buf, size, &readBytes, NULL))
        {
            return (readBytes == size) ? ok : eof;
        } else {
            int rc = GetLastError();
            return rc == ERROR_HANDLE_EOF ? eof : rc;
        }
    }
}

int dbFile::read(void* buf, size_t size)
{
    DWORD readBytes;
    if (ReadFile(fh, buf, size, &readBytes, NULL)) {
        return (readBytes == size) ? ok : eof;
    } else {
        int rc = GetLastError();
        return rc == ERROR_HANDLE_EOF ? eof : rc;
    }
}

int dbFile::setSize(offs_t size)
{
    LONG low_pos = nat8_low_part(size);
    LONG high_pos = nat8_high_part(size);
    if (SetFilePointer(fh, low_pos,
                       &high_pos, FILE_BEGIN) == BAD_POS
        || !SetEndOfFile(fh))
    {
        return GetLastError();
    }
    return ok;
}


int dbFile::write(void const* buf, size_t size)
{
    DWORD writtenBytes;
    return !WriteFile(fh, buf, size, &writtenBytes, NULL)
        ? GetLastError() : (writtenBytes == size) ? ok : eof;
}

int dbFile::write(offs_t pos, void const* buf, size_t size)
{
    DWORD writtenBytes;
    if (osinfo.dwPlatformId == VER_PLATFORM_WIN32_NT) {
        OVERLAPPED Overlapped;
        Overlapped.Offset = nat8_low_part(pos);
        Overlapped.OffsetHigh = nat8_high_part(pos);
        Overlapped.hEvent = NULL;
        return WriteFile(fh, buf, size, &writtenBytes, &Overlapped)
            ? writtenBytes == size ? ok : eof
            : GetLastError();
    } else {
        LONG high_pos = nat8_high_part(pos);
        LONG low_pos = nat8_low_part(pos);
        dbCriticalSection cs(mutex);
        return SetFilePointer(fh, low_pos, &high_pos, FILE_BEGIN)
            == BAD_POS ||
            !WriteFile(fh, buf, size, &writtenBytes, NULL)
            ? GetLastError()
            : (writtenBytes == size) ? ok : eof;
    }
}


int dbFile::flush()
{
    return FlushFileBuffers(fh) ? ok : GetLastError();
}

int dbFile::close()
{
    if (fh != INVALID_HANDLE_VALUE) {
        if (CloseHandle(fh)) {
            fh = INVALID_HANDLE_VALUE;
            return ok;
        } else {
            return GetLastError();
        }
    } else {
        return ok;
    }
}

void* dbFile::allocateBuffer(size_t size)
{
    return VirtualAlloc(NULL, size, MEM_RESERVE|MEM_COMMIT, PAGE_READWRITE);
}

void dbFile::protectBuffer(void* buf, size_t size, bool readonly)
{
    DWORD oldProt;
    VirtualProtect(buf, size, readonly ? PAGE_READONLY : PAGE_READWRITE, &oldProt);
}



void  dbFile::deallocateBuffer(void* buffer, size_t size)
{
    VirtualFree(buffer, 0, MEM_RELEASE);
}

char* dbFile::errorText(int code, char* buf, size_t bufSize)
{
    int len;

    switch (code) {
      case ok:
        strncpy(buf, "No error", bufSize);
        break;
      case eof:
        strncpy(buf, "Transfer less bytes than specified", bufSize);
        break;
      default:
        len = FormatMessage(FORMAT_MESSAGE_FROM_SYSTEM,
                            NULL,
                            code,
                            0,
                            buf,
                            bufSize,
                            NULL);
        if (len == 0) {
            char errcode[64];
            sprintf(errcode, "unknown error %u", code);
            strncpy(buf, errcode, bufSize);
        }
    }
    return buf;
}

#else // Unix

#include <unistd.h>
#include <fcntl.h>
#include <errno.h>

#ifdef __linux__
#define lseek(fd, offs, whence) lseek64(fd, offs, whence)
#endif

dbFile::dbFile()
{
    fd = -1;
}

int dbFile::open(char const* fileName, int attr)
{
    char* name = (char*)fileName;
#if defined(__sun)
    fd = ::open64(name, ((attr & read_only) ? O_RDONLY : O_CREAT|O_RDWR)
                  | ((attr & truncate) ? O_TRUNC : 0), 0666);
    if (fd >= 0) {
        directio(fd, DIRECTIO_ON);
    }
#elif defined(_AIX)
#if defined(_AIX43)
    fd = ::open64(name, ((attr & read_only) ? O_RDONLY : O_CREAT|O_RDWR|O_LARGEFILE|O_DSYNC|
                         ((attr & no_buffering) ? O_DIRECT : 0))
#else
     fd = ::open64(name, ((attr & read_only) ? O_RDONLY : O_CREAT|O_RDWR|O_LARGEFILE
#endif /* _AIX43 */
                   | ((attr & truncate) ? O_TRUNC : 0), 0666);
#else
    fd = ::open(name, O_LARGEFILE | ((attr & read_only) ? O_RDONLY : O_CREAT|O_RDWR)
                | ((attr & truncate) ? O_TRUNC : 0), 0666);
#endif
    if (fd < 0) {
        return errno;
    }
    return ok;
}

int dbFile::setSize(offs_t size)
{
    return ftruncate(fd, size);
}

int dbFile::read(offs_t pos, void* buf, size_t size)
{
    ssize_t rc;
#if defined(__sun) || defined(_AIX43)
    rc = pread64(fd, buf, size, pos);
#else
    {
        dbCriticalSection cs(mutex);
        if (offs_t(lseek(fd, pos, SEEK_SET)) != pos) {
            return errno;
        }
        rc = ::read(fd, buf, size);
    }
#endif
    if (rc == -1) {
        return errno;
    } else if (size_t(rc) != size) {
        return eof;
    } else {
        return ok;
    }
}

int dbFile::read(void* buf, size_t size)
{
    ssize_t rc = ::read(fd, buf, size);
    if (rc == -1) {
        return errno;
    } else if (size_t(rc) != size) {
        return eof;
    } else {
        return ok;
    }
}

int dbFile::write(void const* buf, size_t size)
{
    ssize_t rc = ::write(fd, buf, size);
    if (rc == -1) {
        return errno;
    } else if (size_t(rc) != size) {
        return eof;
    } else {
        return ok;
    }
}

int dbFile::write(offs_t pos, void const* buf, size_t size)
{
    ssize_t rc;
#if defined(__sun) || defined(_AIX43)
    rc = pwrite64(fd, buf, size, pos);
#else
    {
        dbCriticalSection cs(mutex);
        if (offs_t(lseek(fd, pos, SEEK_SET)) != pos) {
            return errno;
        }
        rc = ::write(fd, buf, size);
    }
#endif
    if (rc == -1) {
        return errno;
    } else if (size_t(rc) != size) {
        return eof;
    } else {
        return ok;
    }
}

int dbFile::flush()
{
#if defined(_AIX43)
    return ok; // direct IO is used: no need in flush
#else
    return fsync(fd) != ok ? errno : ok;
#endif
}

int dbFile::close()
{
    if (fd != -1) {
        if (::close(fd) == ok) {
            fd = -1;
            return ok;
        } else {
            return errno;
        }
    } else {
        return ok;
    }
}

void* dbFile::allocateBuffer(size_t size)
{
#if defined(__MINGW__) || defined(__CYGWIN__)
    return malloc(size);
#else
    return valloc(size);
#endif
}

void  dbFile::deallocateBuffer(void* buffer, size_t)
{
    free(buffer);
}

char* dbFile::errorText(int code, char* buf, size_t bufSize)
{
    switch (code) {
      case ok:
        strncpy(buf, "No error", bufSize);
        break;
      case eof:
        strncpy(buf, "Transfer less bytes than specified", bufSize);
        break;
      default:
        strncpy(buf, strerror(code), bufSize);
    }
    return buf;
}

#endif

int dbMultiFile::open(int n, dbSegment* seg, int attr)
{
    segment = new dbFileSegment[n];
    nSegments = n;
    while (--n >= 0) {
        segment[n].size = seg[n].size*dbPageSize;
        segment[n].offs = seg[n].offs;
        int rc = segment[n].open(seg[n].name, attr);
        if (rc != ok) {
            while (++n < nSegments) {
                segment[n].close();
            }
            return rc;
        }
    }
    return ok;
}

int dbMultiFile::close()
{
    if (segment != NULL) {
        for (int i = nSegments; --i >= 0;) {
            int rc = segment[i].close();
            if (rc != ok) {
                return rc;
            }
        }
        delete[] segment;
        segment = NULL;
    }
    return ok;
}

int dbMultiFile::setSize(offs_t)
{
    return ok;
}

int dbMultiFile::flush()
{
    for (int i = nSegments; --i >= 0;) {
        int rc = segment[i].flush();
        if (rc != ok) {
            return rc;
        }
    }
    return ok;
}


int dbMultiFile::write(offs_t pos, void const* ptr, size_t size)
{
    int n = nSegments-1;
    char const* src = (char const*)ptr;
    for (int i = 0; i < n; i++) {
        if (pos < segment[i].size) {
            if (pos + size > segment[i].size) {
                int rc = segment[i].write(segment[i].offs + pos, src, size_t(segment[i].size - pos));
                if (rc != ok) {
                    return rc;
                }
                size -= size_t(segment[i].size - pos);
                src += size_t(segment[i].size - pos);
                pos = 0;
            } else {
                return segment[i].write(segment[i].offs + pos, src, size);
            }
        } else {
            pos -= segment[i].size;
        }
    }
    return segment[n].write(segment[n].offs + pos, src, size);
}

int dbMultiFile::read(offs_t pos, void* ptr, size_t size)
{
    int n = nSegments-1;
    char* dst = (char*)ptr;
    for (int i = 0; i < n; i++) {
        if (pos < segment[i].size) {
            if (pos + size > segment[i].size) {
                int rc = segment[i].read(segment[i].offs + pos, dst, size_t(segment[i].size - pos));
                if (rc != ok) {
                    return rc;
                }
                size -= size_t(segment[i].size - pos);
                dst += size_t(segment[i].size - pos);
                pos = 0;
            } else {
                return segment[i].read(segment[i].offs + pos, dst, size);
            }
        } else {
            pos -= segment[i].size;
        }
    }
    return segment[n].read(segment[n].offs + pos, dst, size);
}


int dbRaidFile::setSize(offs_t)
{
    return ok;
}

int dbRaidFile::write(offs_t pos, void const* ptr, size_t size)
{
    char const* src = (char const*)ptr;
    while (true) {
        int i = (int)(pos / raidBlockSize % nSegments);
        int offs = (unsigned)pos % raidBlockSize;
        size_t available = raidBlockSize - offs;
        if (available >= size) {
            return segment[i].write(segment[i].offs + pos / (raidBlockSize*nSegments) * raidBlockSize + offs, src, size);
        }
        int rc = segment[i].write(segment[i].offs + pos / (raidBlockSize*nSegments) * raidBlockSize + offs, src, available);
        if (rc != ok) {
            return rc;
        }
        src += available;
        pos += available;
        size -= available;
    }
}


int dbRaidFile::read(offs_t pos, void* ptr, size_t size)
{
    char* dst = (char*)ptr;
    while (true) {
        int i = (int)(pos / raidBlockSize % nSegments);
        int offs = (unsigned)pos % raidBlockSize;
        size_t available = raidBlockSize - offs;
        if (available >= size) {
            return segment[i].read(segment[i].offs + pos / (raidBlockSize*nSegments) * raidBlockSize + offs, dst, size);
        }
        int rc = segment[i].read(segment[i].offs + pos / (raidBlockSize*nSegments) * raidBlockSize + offs, dst, available);
        if (rc != ok) {
            return rc;
        }
        dst += available;
        pos += available;
        size -= available;
    }
}
//-< DATABASE.CPP >--------------------------------------------------*--------*
// GigaBASE                  Version 1.0         (c) 1999  GARRET    *     ?  *
// (Post Relational Database Management System)                      *   /\|  *
//                                                                   *  /  \  *
//                          Created:     20-Nov-1998  K.A. Knizhnik  * / [] \ *
//                          Last update: 23-Nov-2001  K.A. Knizhnik  * GARRET *
//-------------------------------------------------------------------*--------*
// Database memory management, query execution, scheme evaluation
//-------------------------------------------------------------------*--------*

#include "database.h"
#include "btree.h"

void dbDatabase::handleError(int error, char const* msg)
{
    if (errorHandler != NULL) {
        (*errorHandler)(error, msg);
    } else {
        fprintf(stderr, "Error %d: %s\n", error, msg);
    }
}

void dbDatabase::throwException(int error, char const* msg)
{
    handleError(error, msg);
    throw dbException(error, msg);
}

bool dbDatabase::open(char const* name, int openAttr)
{
    dbCriticalSection cs(mutex);
    int rc;
    opened = false;

    size_t indexSize = initIndexSize < dbFirstUserId
        ? size_t(dbFirstUserId) : initIndexSize;
    indexSize = DOALIGN(indexSize, dbHandlesPerPage);

    memset(dirtyPagesMap, 0, dbDirtyPageBitmapSize+4);

    for (int i = dbBitmapId + dbBitmapPages; --i >= 0;) {
        bitmapPageAvailableSpace[i] = INT_MAX;
    }
    currRBitmapPage = currPBitmapPage = dbBitmapId;
    currRBitmapOffs = currPBitmapOffs = 0;
    reservedChain = NULL;
    classDescList = NULL;
    gcThreshold = 0;
    allocatedDelta = 0;
    gcDone = false;
    modified = false;

    if (accessType == dbReadOnly) {
        openAttr |= dbFile::read_only;
    }
    if (*name == '@') {
        FILE* f = fopen(name+1, "r");
        if (f == NULL) {
            handleError(dybase_open_error,
                        "Failed to open database configuration file");
            return false;
        }
        dbMultiFile::dbSegment segment[dbMaxFileSegments];
        const int maxFileNameLen = 1024;
        char fileName[maxFileNameLen];
        int i, n;
        db_int8 size;
        bool raid = false;
        size_t raidBlockSize = dbDefaultRaidBlockSize;
        for (i=0; (n=fscanf(f, "%s" INT8_FORMAT, fileName, &size)) >= 1; i++)
        {
            if (i == dbMaxFileSegments) {
                while (--i >= 0) delete[] segment[i].name;
                fclose(f);
                handleError(dybase_open_error, "Too much segments");
                return false;
            }

            if (n == 1) {
                if (i == 0) {
                    raid = true;
                } else if (!raid && segment[i-1].size == 0) {
                    while (--i >= 0) delete[] segment[i].name;
                    fclose(f);
                    handleError(dybase_open_error,
                                "Segment size was not specified");
                    return false;
                }
                size = 0;
            } else if (size == 0 || raid) {
                while (--i >= 0) delete[] segment[i].name;
                fclose(f);
                handleError(dybase_open_error, size == 0
                            ? "Invalid segment size"
                            : "segment size should not be specified for raid");
                return false;
            }

            if (strcmp(fileName, ".RaidBlockSize") == 0) {
                raidBlockSize = (size_t)size;
                raid = true;
                i -= 1;
                continue;
            }
            segment[i].size = offs_t(size);
            char* suffix = strchr(fileName, '[');
            db_int8 offs = 0;
            if (suffix != NULL) {
                *suffix = '\0';
                sscanf(suffix+1, INT8_FORMAT, &offs);
            }
            segment[i].name = new char[strlen(fileName) + 1];
            strcpy(segment[i].name, fileName);
            segment[i].offs = offs_t(offs);
        }
        fclose(f);
        if (i == 0) {
            fclose(f);
            handleError(dybase_open_error,
                        "File should have at least one segment");
            return false;
        }
        if (i == 1 && raid) {
            raid = false;
        }
        dbMultiFile* mfile;
        if (raid) {
            mfile = new dbRaidFile(raidBlockSize);
        } else {
            mfile = new dbMultiFile;
        }
        rc = mfile->open(i, segment, openAttr);
        while (--i >= 0) delete[] segment[i].name;
        if (rc != dbFile::ok) {
            delete mfile;
            handleError(dybase_open_error, "Failed to create database file");
            return false;
        }
        file = mfile;
    } else {
        file = new dbFile;
        if (file->open(name, openAttr) != dbFile::ok) {
            delete file;
            handleError(dybase_open_error, "Failed to create database file");
            return false;
        }
    }
    memset(header, 0, sizeof(dbHeader));
    rc = file->read(0, header, dbPageSize);
    if (rc != dbFile::ok && rc != dbFile::eof) {
        delete file;
        handleError(dybase_open_error, "Failed to read file header");
        return false;
    }

    if ((unsigned)header->curr > 1) {
        delete file;
        handleError(dybase_open_error, "Database file was corrupted: invalid root index");
        return false;
    }
    if (!header->isInitialized()) {
        if (accessType == dbReadOnly) {
            delete file;
            handleError(dybase_open_error, "Can not open uninitialized file in read only mode");
            return false;
        }
        curr = header->curr = 0;
        size_t used = dbPageSize;
        header->root[0].index = used;
        header->root[0].indexSize = indexSize;
        header->root[0].indexUsed = dbFirstUserId;
        header->root[0].freeList = 0;
        header->root[0].classDescList = 0;
        header->root[0].rootObject = 0;
        used += indexSize*sizeof(offs_t);
        header->root[1].index = used;
        header->root[1].indexSize = indexSize;
        header->root[1].indexUsed = dbFirstUserId;
        header->root[1].freeList = 0;
        header->root[1].classDescList = 0;
        header->root[1].rootObject = 0;
        used += indexSize*sizeof(offs_t);

        header->root[0].shadowIndex = header->root[1].index;
        header->root[1].shadowIndex = header->root[0].index;
        header->root[0].shadowIndexSize = indexSize;
        header->root[1].shadowIndexSize = indexSize;

        size_t bitmapPages =
            (used + dbPageSize*(dbAllocationQuantum*8-1) - 1)
            / (dbPageSize*(dbAllocationQuantum*8-1));
        size_t bitmapSize = bitmapPages*dbPageSize;
        size_t usedBitmapSize = (used + bitmapSize) / (dbAllocationQuantum*8);
        byte* bitmap = (byte*)dbFile::allocateBuffer(bitmapSize);
        memset(bitmap, 0xFF, usedBitmapSize);
        memset(bitmap + usedBitmapSize, 0, bitmapSize - usedBitmapSize);
        rc = file->write(used, bitmap, bitmapSize);
        dbFile::deallocateBuffer(bitmap);
        if (rc != dbFile::ok) {
            delete file;
            handleError(dybase_open_error, "Failed to write to the file");
            return false;
        }
        size_t bitmapIndexSize =
            DOALIGN((dbBitmapId + dbBitmapPages)*sizeof(offs_t), dbPageSize);
        offs_t* index = (offs_t*)dbFile::allocateBuffer(bitmapIndexSize);
        index[dbInvalidId] = dbFreeHandleFlag;
        size_t i;
        for (i = 0; i < bitmapPages; i++) {
            index[dbBitmapId + i] = used | dbPageObjectFlag | dbModifiedFlag;
            used += dbPageSize;
        }
        header->root[0].bitmapEnd = dbBitmapId + i;
        header->root[1].bitmapEnd = dbBitmapId + i;
        while (i < dbBitmapPages) {
            index[dbBitmapId+i] = dbFreeHandleFlag;
            i += 1;
        }
        rc = file->write(header->root[1].index, index, bitmapIndexSize);
        dbFile::deallocateBuffer(index);
        if (rc != dbFile::ok) {
            delete file;
            handleError(dybase_open_error, "Failed to write index to the file");
            return false;
        }
        header->root[0].size = used;
        header->root[1].size = used;
        currIndexSize = dbFirstUserId;
        if (!pool.open(file, used)) {
            delete file;
            handleError(dybase_open_error, "Failed to allocate page pool");
            return false;
        }
        if (dbFileExtensionQuantum != 0) {
            file->setSize(DOALIGN(used, dbFileExtensionQuantum));
        }
        offs_t indexPage = header->root[1].index;
        offs_t lastIndexPage =
            indexPage + header->root[1].bitmapEnd*sizeof(offs_t);
        while (indexPage < lastIndexPage) {
            offs_t* p = (offs_t*)pool.put(indexPage);
            for (i = 0; i < dbHandlesPerPage; i++) {
                p[i] &= ~dbModifiedFlag;
            }
            pool.unfix(p);
            indexPage += dbPageSize;
        }
        pool.copy(header->root[0].index, header->root[1].index,
                  currIndexSize*sizeof(offs_t));
        header->dirty = true;
        header->root[0].size = header->root[1].size;
        if (file->write(0, header, dbPageSize) != dbFile::ok) {
            pool.close();
            delete file;
            handleError(dybase_open_error, "Failed to write to the file");
            return false;
        }
        pool.flush();
        header->initialized = true;
        if (file->write(0, header, dbPageSize) != dbFile::ok ||
            file->flush() != dbFile::ok)
        {
            pool.close();
            delete file;
            handleError(dybase_open_error, "Failed to complete file initialization");
            return false;
        }
    } else {
        int curr = header->curr;
        this->curr = curr;
        if (header->root[curr].indexSize != header->root[curr].shadowIndexSize)
        {
            delete file;
            handleError(dybase_open_error, "Header of database file is corrupted");
            return false;
        }

        if (rc != dbFile::ok) {
            delete file;
            handleError(dybase_open_error, "Failed to read object index");
            return false;
        }
        pool.open(file, header->root[curr].size);
        if (header->dirty) {
            TRACE_MSG(("Database was not normally closed: start recovery\n"));
            if (accessType == dbReadOnly) {
                pool.close();
                delete file;
                handleError(dybase_open_error, "Can not open dirty file in read only mode");
                return false;
            }
            header->root[1-curr].size = header->root[curr].size;
            header->root[1-curr].indexUsed = header->root[curr].indexUsed;
            header->root[1-curr].freeList = header->root[curr].freeList;
            header->root[1-curr].index = header->root[curr].shadowIndex;
            header->root[1-curr].indexSize =
                header->root[curr].shadowIndexSize;
            header->root[1-curr].shadowIndex = header->root[curr].index;
            header->root[1-curr].shadowIndexSize =
                header->root[curr].indexSize;
            header->root[1-curr].bitmapEnd = header->root[curr].bitmapEnd;
            header->root[1-curr].rootObject = header->root[curr].rootObject;
            header->root[1-curr].classDescList = header->root[curr].classDescList;

            pool.copy(header->root[1-curr].index, header->root[curr].index,
                      DOALIGN(header->root[curr].indexUsed*sizeof(offs_t),
                              dbPageSize));
            TRACE_MSG(("Recovery completed\n"));
        }
        currIndexSize = header->root[1-curr].indexUsed;
    }
    committedIndexSize = currIndexSize;

    loadScheme();
    opened = true;
    return true;
}

void dbDatabase::loadScheme()
{
    dbGetTie tie;
    dbClassDescriptor** cpp = &classDescList;
    int cid = header->root[1-curr].classDescList;
    while (cid != 0) {
        dbClass* cls = ((dbClass*)getObject(tie, cid))->clone();
        dbClassDescriptor* desc = new dbClassDescriptor(cls, cid);
        classOidHash.put(&desc->oid, sizeof(desc->oid), desc);
        classSignatureHash.put(cls->signature, desc->signatureSize, desc);
        *cpp = desc;
        cpp = &desc->next;
        cid = cls->next;
    }
    *cpp = NULL;
}


void dbDatabase::close()
{
    dbCriticalSection cs(mutex);
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return;
    }
    if (modified) {
        commitTransaction();
    }
    dbClassDescriptor *desc, *next;
    for (desc = classDescList; desc != NULL; desc = next) {
        next = desc->next;
        delete desc;
    }
    classDescList = NULL;
    classOidHash.clear();
    classSignatureHash.clear();

    opened = false;
    if (header->dirty) {
        int rc = file->write(0, header, dbPageSize);
        if (rc != dbFile::ok) {
            throwException(dybase_file_error, "Failed to write header to the disk");
        }
        pool.flush();
        header->dirty = false;
        rc = file->write(0, header, dbPageSize);
        if (rc != dbFile::ok) {
            throwException(dybase_file_error, "Failed to write header to the disk");
        }
    }
    pool.close();
    file->close();
    delete file;
}


dbObject* dbDatabase::putObject(dbPutTie& tie, oid_t oid) {
    offs_t pos = getPos(oid);
    int offs = (int)pos & (dbPageSize-1);
    byte* p = pool.get(pos - offs);
    dbObject* obj = (dbObject*)(p + (offs & ~dbFlagsMask));
    if (!(offs & dbModifiedFlag)) {
        dirtyPagesMap[size_t(oid/dbHandlesPerPage/32)]
            |= 1 << int(oid/dbHandlesPerPage & 31);
        cloneBitmap(pos & ~dbFlagsMask, obj->size);
        allocate(obj->size, oid);
        pos = getPos(oid);
    }
    tie.set(pool, oid, pos & ~dbFlagsMask, obj->size);
    pool.unfix(p);
    return (dbObject*)tie.get();
}

byte* dbDatabase::put(dbPutTie& tie, oid_t oid) {
    offs_t pos = getPos(oid);
    if (!(pos & dbModifiedFlag)) {
        dirtyPagesMap[size_t(oid/dbHandlesPerPage/32)]
            |= 1 << int(oid/dbHandlesPerPage & 31);
        allocate(dbPageSize, oid);
        cloneBitmap(pos & ~dbFlagsMask, dbPageSize);
        pos = getPos(oid);
    }
    tie.set(pool, oid, pos & ~dbFlagsMask, dbPageSize);
    return tie.get();
}

oid_t dbDatabase::getRoot()
{
    return header->root[1-curr].rootObject;
}

void dbDatabase::setRoot(oid_t oid)
{
    header->root[1-curr].rootObject = oid;
    modified = true;
}

dbLoadHandle* dbDatabase::getLoadHandle(oid_t oid)
{
    dbCriticalSection cs(mutex);
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return NULL;
    }
    dbLoadHandle* hnd = new dbLoadHandle();
    dbObject* obj = getObject(hnd->tie, oid);
    hnd->curr = (byte*)(obj+1);
    hnd->end = (byte*)obj + obj->size;
    hnd->desc = (dbClassDescriptor*)classOidHash.get(&obj->cid, sizeof(obj->cid));
    assert(hnd->desc != NULL);
    return hnd;
}

void dbDatabase::storeObject(dbStoreHandle* handle)
{
    dbCriticalSection cs(mutex);
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return;
    }
    dbObject* obj = (dbObject*)handle->body.base();
    dbClassDescriptor* desc = (dbClassDescriptor*)classSignatureHash.get(handle->signature.base(), handle->signature.size());
    if (desc == NULL) {
        dbClass* cls = dbClass::create(handle->signature.base(), handle->signature.size());
        cls->next = header->root[1-curr].classDescList;
        desc = new dbClassDescriptor(cls, allocateObject(cls));
        header->root[1-curr].classDescList = desc->oid;
        classOidHash.put(&desc->oid, sizeof(desc->oid), desc);
        classSignatureHash.put(cls->signature, desc->signatureSize, desc);
        desc->next = classDescList;
        classDescList = desc;
    }
    obj->size = handle->body.size();
    obj->cid = desc->oid;
    oid_t oid = handle->oid;
    offs_t pos = getPos(oid);
    if (pos == 0) {
        pos = allocate(obj->size);
        setPos(oid, pos | dbModifiedFlag);
    } else {
        int offs = (int)pos & (dbPageSize-1);
        byte* p = pool.get(pos - offs);
        size_t oldSize = ((dbObject*)(p + (offs & ~dbFlagsMask)))->size;
        pool.unfix(p);
        if (!(offs & dbModifiedFlag)) {
            dirtyPagesMap[size_t(oid/dbHandlesPerPage/32)]
                |= 1 << int(oid/dbHandlesPerPage & 31);
            cloneBitmap(pos, oldSize);
            pos = allocate(obj->size);
            setPos(oid, pos | dbModifiedFlag);
        } else {
            if (DOALIGN(oldSize, dbAllocationQuantum) != DOALIGN(obj->size, dbAllocationQuantum)) {
                offs_t newPos = allocate(obj->size);
                cloneBitmap(pos & ~dbFlagsMask, oldSize);
                free(pos & ~dbFlagsMask, oldSize);
                pos = newPos;
                setPos(oid, pos | dbModifiedFlag);
            }
        }
    }
    pool.put(pos & ~dbFlagsMask, (byte*)obj, obj->size);
}

oid_t dbDatabase::allocateObject(dbObject* obj)
{
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return 0;
    }
    oid_t oid = allocateId();
    offs_t pos = allocate(obj->size);
    setPos(oid, pos | dbModifiedFlag);
    pool.put(pos, (byte*)obj, obj->size);
    return oid;
}


void dbDatabase::freeObject(oid_t oid)
{
    dbCriticalSection cs(mutex);
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return;
    }
    dbObject hdr;
    getHeader(hdr, oid);
    offs_t pos = getPos(oid);
    if (pos & dbModifiedFlag) {
        free(pos & ~dbFlagsMask, hdr.size);
    } else {
        cloneBitmap(pos, hdr.size);
    }
    freeId(oid);
}

void dbDatabase::freePage(oid_t oid)
{
    offs_t pos = getPos(oid);
    if (pos & dbModifiedFlag) {
        free(pos & ~dbFlagsMask, dbPageSize);
    } else {
        cloneBitmap(pos & ~dbFlagsMask, dbPageSize);
    }
    freeId(oid);
}


inline void dbDatabase::extend(offs_t size)
{
    if (size > header->root[1-curr].size) {
        if (dbFileExtensionQuantum != 0
            && DOALIGN(size, dbFileExtensionQuantum)
               != DOALIGN(header->root[1-curr].size, dbFileExtensionQuantum))
        {
            file->setSize(DOALIGN(size, dbFileExtensionQuantum));
        }
        header->root[1-curr].size = size;
    }
}

inline bool dbDatabase::wasReserved(offs_t pos, size_t size)
{
    for (dbLocation* location = reservedChain; location != NULL; location = location->next) {
        if (pos - location->pos < location->size || location->pos - pos < size) {
            return true;
        }
    }
    return false;
}

inline void dbDatabase::reserveLocation(dbLocation& location, offs_t pos, size_t size)
{
    location.pos = pos;
    location.size = size;
    location.next = reservedChain;
    reservedChain = &location;
}

inline void dbDatabase::commitLocation()
{
    reservedChain = reservedChain->next;
}


void dbDatabase::setDirty()
{
    modified = true;
    if (!header->dirty) {
        header->dirty = true;
        if (file->write(0, header, dbPageSize) != dbFile::ok) {
            throwException(dybase_file_error, "Failed to write header to the file");
        }
        pool.flush();
    }
}


offs_t dbDatabase::allocate(size_t size, oid_t oid)
{
    static byte const firstHoleSize [] = {
        8,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        6,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        7,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        6,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,
        5,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0,4,0,1,0,2,0,1,0,3,0,1,0,2,0,1,0
    };
    static byte const lastHoleSize [] = {
        8,7,6,6,5,5,5,5,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,
        2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,
        1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
        1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
        0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0
    };
    static byte const maxHoleSize [] = {
        8,7,6,6,5,5,5,5,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,
        5,4,3,3,2,2,2,2,3,2,2,2,2,2,2,2,4,3,2,2,2,2,2,2,3,2,2,2,2,2,2,2,
        6,5,4,4,3,3,3,3,3,2,2,2,2,2,2,2,4,3,2,2,2,1,1,1,3,2,1,1,2,1,1,1,
        5,4,3,3,2,2,2,2,3,2,1,1,2,1,1,1,4,3,2,2,2,1,1,1,3,2,1,1,2,1,1,1,
        7,6,5,5,4,4,4,4,3,3,3,3,3,3,3,3,4,3,2,2,2,2,2,2,3,2,2,2,2,2,2,2,
        5,4,3,3,2,2,2,2,3,2,1,1,2,1,1,1,4,3,2,2,2,1,1,1,3,2,1,1,2,1,1,1,
        6,5,4,4,3,3,3,3,3,2,2,2,2,2,2,2,4,3,2,2,2,1,1,1,3,2,1,1,2,1,1,1,
        5,4,3,3,2,2,2,2,3,2,1,1,2,1,1,1,4,3,2,2,2,1,1,1,3,2,1,1,2,1,1,0
    };
    static byte const maxHoleOffset [] = {
        0,1,2,2,3,3,3,3,4,4,4,4,4,4,4,4,0,1,5,5,5,5,5,5,0,5,5,5,5,5,5,5,
        0,1,2,2,0,3,3,3,0,1,6,6,0,6,6,6,0,1,2,2,0,6,6,6,0,1,6,6,0,6,6,6,
        0,1,2,2,3,3,3,3,0,1,4,4,0,4,4,4,0,1,2,2,0,1,0,3,0,1,0,2,0,1,0,5,
        0,1,2,2,0,3,3,3,0,1,0,2,0,1,0,4,0,1,2,2,0,1,0,3,0,1,0,2,0,1,0,7,
        0,1,2,2,3,3,3,3,0,4,4,4,4,4,4,4,0,1,2,2,0,5,5,5,0,1,5,5,0,5,5,5,
        0,1,2,2,0,3,3,3,0,1,0,2,0,1,0,4,0,1,2,2,0,1,0,3,0,1,0,2,0,1,0,6,
        0,1,2,2,3,3,3,3,0,1,4,4,0,4,4,4,0,1,2,2,0,1,0,3,0,1,0,2,0,1,0,5,
        0,1,2,2,0,3,3,3,0,1,0,2,0,1,0,4,0,1,2,2,0,1,0,3,0,1,0,2,0,1,0,0
    };

    setDirty();
    size = DOALIGN(size, dbAllocationQuantum);
    allocatedDelta += size;
    if (gcThreshold != 0 && allocatedDelta > gcThreshold && !gcDone) {
        startGC();
    }

    int objBitSize = size >> dbAllocationQuantumBits;
    offs_t pos;
    int holeBitSize = 0;
    register int alignment = size & (dbPageSize-1);
    register size_t offs;
    const int pageBits = dbPageSize*8;
    oid_t firstPage, lastPage;
    int   holeBeforeFreePage  = 0;
    oid_t freeBitmapPage = 0;
    dbLocation location;
    dbPutTie   tie;
    oid_t i;
    const size_t inc = dbPageSize/dbAllocationQuantum/8;

    lastPage = header->root[1-curr].bitmapEnd;
    if (alignment == 0) {
        firstPage = currPBitmapPage;
        offs = DOALIGN(currPBitmapOffs, inc);
    } else {
        firstPage = currRBitmapPage;
        offs = currRBitmapOffs;
    }


    while (true) {
        if (alignment == 0) {
            // allocate page object
            for (i = firstPage; i < lastPage; i++){
                int spaceNeeded = objBitSize - holeBitSize < pageBits
                    ? objBitSize - holeBitSize : pageBits;
                if (bitmapPageAvailableSpace[i] <= spaceNeeded) {
                    holeBitSize = 0;
                    offs = 0;
                    continue;
                }
                register byte* begin = get(i);
                size_t startOffs = offs;
                while (offs < dbPageSize) {
                    if (begin[offs++] != 0) {
                        offs = DOALIGN(offs, inc);
                        holeBitSize = 0;
                    } else if ((holeBitSize += 8) == objBitSize) {
                        pos = ((offs_t(i-dbBitmapId)*dbPageSize + offs)*8
                               - holeBitSize) << dbAllocationQuantumBits;
                        if (wasReserved(pos, size)) {
                            offs += objBitSize >> 3;
                            startOffs = offs = DOALIGN(offs, inc);
                            holeBitSize = 0;
                            continue;
                        }
                        reserveLocation(location, pos, size);
                        currPBitmapPage = i;
                        currPBitmapOffs = offs;
                        extend(pos + size);
                        if (oid != 0) {
                            offs_t prev = getPos(oid);
                            int marker = (int)prev & dbFlagsMask;
                            pool.copy(pos, prev - marker, size);
                            setPos(oid, pos | marker | dbModifiedFlag);
                        }
                        pool.unfix(begin);
                        begin = put(tie, i);
                        size_t holeBytes = holeBitSize >> 3;
                        if (holeBytes > offs) {
                            memset(begin, 0xFF, offs);
                            holeBytes -= offs;
                            begin = put(tie, --i);
                            offs = dbPageSize;
                        }
                        while (holeBytes > dbPageSize) {
                            memset(begin, 0xFF, dbPageSize);
                            holeBytes -= dbPageSize;
                            bitmapPageAvailableSpace[i] = 0;
                            begin = put(tie, --i);
                        }
                        memset(&begin[offs-holeBytes], 0xFF, holeBytes);
                        commitLocation();
                        return pos;
                    }
                }
                if (startOffs == 0 && holeBitSize == 0
                    && spaceNeeded < bitmapPageAvailableSpace[i])
                {
                    bitmapPageAvailableSpace[i] = spaceNeeded;
                }
                offs = 0;
                pool.unfix(begin);
            }
        } else {
            for (i = firstPage; i < lastPage; i++){
                int spaceNeeded = objBitSize - holeBitSize < pageBits
                    ? objBitSize - holeBitSize : pageBits;
                if (bitmapPageAvailableSpace[i] <= spaceNeeded) {
                    holeBitSize = 0;
                    offs = 0;
                    continue;
                }
                register byte* begin = get(i);
                size_t startOffs = offs;

                while (offs < dbPageSize) {
                    int mask = begin[offs];
                    if (holeBitSize + firstHoleSize[mask] >= objBitSize) {
                        pos = ((offs_t(i-dbBitmapId)*dbPageSize + offs)*8
                               - holeBitSize) << dbAllocationQuantumBits;
                        if (wasReserved(pos, size)) {
                            startOffs = offs += (objBitSize + 7) >> 3;
                            holeBitSize = 0;
                            continue;
                        }
                        reserveLocation(location, pos, size);
                        currRBitmapPage = i;
                        currRBitmapOffs = offs;
                        extend(pos + size);
                        if (oid != 0) {
                            offs_t prev = getPos(oid);
                            int marker = (int)prev & dbFlagsMask;
                            pool.copy(pos, prev - marker, size);
                            setPos(oid, pos | marker | dbModifiedFlag);
                        }
                        pool.unfix(begin);
                        begin = put(tie, i);
                        begin[offs] |= (1 << (objBitSize - holeBitSize)) - 1;
                        if (holeBitSize != 0) {
                            if (size_t(holeBitSize) > offs*8) {
                                memset(begin, 0xFF, offs);
                                holeBitSize -= offs*8;
                                begin = put(tie, --i);
                                offs = dbPageSize;
                            }
                            while (holeBitSize > pageBits) {
                                memset(begin, 0xFF, dbPageSize);
                                holeBitSize -= pageBits;
                                bitmapPageAvailableSpace[i] = 0;
                                begin = put(tie, --i);
                            }
                            while ((holeBitSize -= 8) > 0) {
                                begin[--offs] = 0xFF;
                            }
                            begin[offs-1] |= ~((1 << -holeBitSize) - 1);
                        }
                        commitLocation();
                        return pos;
                    } else if (maxHoleSize[mask] >= objBitSize) {
                        int holeBitOffset = maxHoleOffset[mask];
                        pos = ((offs_t(i-dbBitmapId)*dbPageSize + offs)*8 +
                               holeBitOffset) << dbAllocationQuantumBits;
                        if (wasReserved(pos, size)) {
                            startOffs = offs += (objBitSize + 7) >> 3;
                            holeBitSize = 0;
                            continue;
                        }
                        reserveLocation(location, pos, size);
                        currRBitmapPage = i;
                        currRBitmapOffs = offs;
                        extend(pos + size);
                        if (oid != 0) {
                            offs_t prev = getPos(oid);
                            int marker = (int)prev & dbFlagsMask;
                            pool.copy(pos, prev - marker, size);
                            setPos(oid, pos | marker | dbModifiedFlag);
                        }
                        pool.unfix(begin);
                        begin = put(tie, i);
                        begin[offs] |= ((1<<objBitSize) - 1) << holeBitOffset;
                        commitLocation();
                        return pos;
                    }
                    offs += 1;
                    if (lastHoleSize[mask] == 8) {
                        holeBitSize += 8;
                    } else {
                        holeBitSize = lastHoleSize[mask];
                    }
                }
                if (startOffs == 0 && holeBitSize == 0
                    && spaceNeeded < bitmapPageAvailableSpace[i])
                {
                    bitmapPageAvailableSpace[i] = spaceNeeded;
                }
                offs = 0;
                pool.unfix(begin);
            }
        }
        if (firstPage == dbBitmapId) {
            if (freeBitmapPage > i) {
                i = freeBitmapPage;
                holeBitSize = holeBeforeFreePage;
            }
            if (i == dbBitmapId + dbBitmapPages) {
                throwException(dybase_out_of_memory_error, "Out of memory");
            }
            size_t extension = (size > extensionQuantum)
                             ? size : extensionQuantum;
            int morePages =
                (extension + dbPageSize*(dbAllocationQuantum*8-1) - 1)
                / (dbPageSize*(dbAllocationQuantum*8-1));

            if (size_t(i + morePages) > dbBitmapId + dbBitmapPages) {
                morePages =
                    (size + dbPageSize*(dbAllocationQuantum*8-1) - 1)
                    / (dbPageSize*(dbAllocationQuantum*8-1));
                if (size_t(i + morePages) > dbBitmapId + dbBitmapPages) {
                    throwException(dybase_out_of_memory_error, "Out of memory");
                }
            }
            objBitSize -= holeBitSize;
            int skip = DOALIGN(objBitSize, dbPageSize/dbAllocationQuantum);
            pos = (offs_t(i-dbBitmapId)
                   << (dbPageBits+dbAllocationQuantumBits+3))
                  + (skip << dbAllocationQuantumBits);
            extend(pos + morePages*dbPageSize);
            size_t len = objBitSize >> 3;
            offs_t adr = pos;
            byte* p;
            while (len >= dbPageSize) {
                p = pool.put(adr);
                memset(p, 0xFF, dbPageSize);
                pool.unfix(p);
                adr += dbPageSize;
                len -= dbPageSize;
            }
            p = pool.put(adr);
            memset(p, 0xFF, len);
            p[len] = (1 << (objBitSize&7))-1;
            pool.unfix(p);
            adr = pos + (skip>>3);
            len = morePages * (dbPageSize/dbAllocationQuantum/8);
            while (true) {
                int off = (int)adr & (dbPageSize-1);
                p = pool.put(adr - off);
                if (dbPageSize - off >= len) {
                    memset(p + off, 0xFF, len);
                    pool.unfix(p);
                    break;
                } else {
                    memset(p + off, 0xFF, dbPageSize - off);
                    pool.unfix(p);
                    adr += dbPageSize - off;
                    len -= dbPageSize - off;
                }
            }
            oid_t j = i;
            while (--morePages >= 0) {
                dirtyPagesMap[size_t(j/dbHandlesPerPage/32)]
                    |= 1 << int(j/dbHandlesPerPage & 31);
                setPos(j++, pos | dbPageObjectFlag | dbModifiedFlag);
                pos += dbPageSize;
            }
            freeBitmapPage = header->root[1-curr].bitmapEnd = j;
            j = i + objBitSize / pageBits;
            if (alignment != 0) {
                currRBitmapPage = j;
                currRBitmapOffs = 0;
            } else {
                currPBitmapPage = j;
                currPBitmapOffs = 0;
            }
            while (j > i) {
                bitmapPageAvailableSpace[size_t(--j)] = 0;
            }

            pos = (offs_t(i-dbBitmapId)*dbPageSize*8 - holeBitSize)
                << dbAllocationQuantumBits;
            if (oid != 0) {
                offs_t prev = getPos(oid);
                int marker = (int)prev & dbFlagsMask;
                pool.copy(pos, prev - marker, size);
                setPos(oid, pos | marker | dbModifiedFlag);
            }
            if (holeBitSize != 0) {
                reserveLocation(location, pos, size);
                while (holeBitSize > pageBits) {
                    holeBitSize -= pageBits;
                    byte* p = put(tie, --i);
                    memset(p, 0xFF, dbPageSize);
                    bitmapPageAvailableSpace[i] = 0;
                }
                byte* cur = (byte*)put(tie, --i) + dbPageSize;
                while ((holeBitSize -= 8) > 0) {
                    *--cur = 0xFF;
                }
                *(cur-1) |= ~((1 << -holeBitSize) - 1);
                commitLocation();
            }
            return pos;
        }
        if (gcThreshold != 0 && !gcDone) {
            allocatedDelta -= size;
            startGC();
            currRBitmapPage = currPBitmapPage = dbBitmapId;
            currRBitmapOffs = currPBitmapOffs = 0;
            return allocate(size, oid);
        }
        freeBitmapPage = i;
        holeBeforeFreePage = holeBitSize;
        holeBitSize = 0;
        lastPage = firstPage + 1;
        firstPage = dbBitmapId;
        offs = 0;
    }
}

void dbDatabase::free(offs_t pos, size_t size)
{
    assert(pos != 0 && (pos & (dbAllocationQuantum-1)) == 0);
    dbPutTie tie;
    offs_t quantNo = pos / dbAllocationQuantum;
    int    objBitSize = (size+dbAllocationQuantum-1) / dbAllocationQuantum;
    oid_t  pageId = dbBitmapId + oid_t(quantNo / (dbPageSize*8));
    size_t offs = (size_t(quantNo) & (dbPageSize*8-1)) >> 3;
    byte*  p = put(tie, pageId) + offs;
    int    bitOffs = int(quantNo) & 7;

    allocatedDelta -= objBitSize*dbAllocationQuantum;

    if ((size_t(pos) & (dbPageSize-1)) == 0 && size >= dbPageSize) {
        if (pageId == currPBitmapPage && offs < currPBitmapOffs) {
            currPBitmapOffs = offs;
        }
    } else {
        if (pageId == currRBitmapPage && offs < currRBitmapOffs) {
            currRBitmapOffs = offs;
        }
    }

    bitmapPageAvailableSpace[pageId] = INT_MAX;

    if (objBitSize > 8 - bitOffs) {
        objBitSize -= 8 - bitOffs;
        *p++ &= (1 << bitOffs) - 1;
        offs += 1;
        while (objBitSize + offs*8 > dbPageSize*8) {
            memset(p, 0, dbPageSize - offs);
            p = put(tie, ++pageId);
            bitmapPageAvailableSpace[pageId] = INT_MAX;
            objBitSize -= (dbPageSize - offs)*8;
            offs = 0;
        }
        while ((objBitSize -= 8) > 0) {
            *p++ = 0;
        }
        *p &= ~((1 << (objBitSize + 8)) - 1);
    } else {
        *p &= ~(((1 << objBitSize) - 1) << bitOffs);
    }
}

void dbDatabase::gc()
{
    dbCriticalSection cs(mutex);
    if (gcDone) {
        return;
    }
    startGC();
}

void dbDatabase::startGC()
{
    int bitmapSize = (int)(header->root[curr].size >> (dbAllocationQuantumBits + 5)) + 1;
    bool existsNotMarkedObjects;
    offs_t pos;
    int  i, j;

    // mark
    greyBitmap = new db_int4[bitmapSize];
    blackBitmap = new db_int4[bitmapSize];
    memset(greyBitmap, 0, bitmapSize*sizeof(db_int4));
    memset(blackBitmap, 0, bitmapSize*sizeof(db_int4));
    int rootOid = header->root[curr].rootObject;
    if (rootOid != 0) {
        dbGetTie tie;
        markOid(rootOid);
        do {
            existsNotMarkedObjects = false;
            for (i = 0; i < bitmapSize; i++) {
                if (greyBitmap[i] != 0) {
                    existsNotMarkedObjects = true;
                    for (j = 0; j < 32; j++) {
                        if ((greyBitmap[i] & (1 << j)) != 0) {
                            pos = (((offs_t)i << 5) + j) << dbAllocationQuantumBits;
                            greyBitmap[i] &= ~(1 << j);
                            blackBitmap[i] |= 1 << j;
                            int offs = (int)pos & (dbPageSize-1);
                            byte* pg = pool.get(pos - offs);
                            dbObject* obj = (dbObject*)(pg + offs);
                            if (obj->cid == dbBtreeId) {
                                ((dbBtree*)obj)->markTree(this);
                            } else if (obj->cid >= dbFirstUserId) {
                                markOid(obj->cid);
                                tie.set(pool, pos);
                                markObject((dbObject*)tie.get());
                            }
                            pool.unfix(pg);
                        }
                    }
                }
            }
        } while (existsNotMarkedObjects);
    }

    // sweep
    gcDone = true;
    for (i = dbFirstUserId, j = committedIndexSize; i < j; i++) {
        pos = getGCPos(i);
        if (((int)pos & (dbPageObjectFlag|dbFreeHandleFlag)) == 0) {
            unsigned bit = (unsigned)(pos >> dbAllocationQuantumBits);
            if ((blackBitmap[bit >> 5] & (1 << (bit & 31))) == 0) {
                // object is not accessible
                assert(getPos(i) == pos);
                int offs = (int)pos & (dbPageSize-1);
                byte* pg = pool.get(pos - offs);
                dbObject* obj = (dbObject*)(pg + offs);
                if (obj->cid == dbBtreeId) {
                    dbBtree::_drop(this, i);
                } else if (obj->cid >= dbFirstUserId) {
                    freeId(i);
                    cloneBitmap(pos, obj->size);
                }
                pool.unfix(pg);
            }
        }
    }

    delete[] greyBitmap;
    delete[] blackBitmap;
    allocatedDelta = 0;
}


void dbDatabase::markObject(dbObject* obj)
{
    byte* p = (byte*)(obj + 1);
    byte* end = (byte*)obj + obj->size;
    while (p < end) {
        p = markField(p);
    }
}

byte* dbDatabase::markField(byte* p)
{
    int type = *p++;
    db_int4 len;
    oid_t oid;
    int i;

    switch (type & 7) {
      case dybase_object_type:
        memcpy(&oid, p, sizeof(oid_t));
        markOid(oid);
        p += sizeof(oid_t);
        break;
      case dybase_bool_type:
        p += 1;
        break;
      case dybase_int_type:
        p += sizeof(db_int4);
        break;
      case dybase_long_type:
      case dybase_real_type:
        p += sizeof(db_int8);
        break;
      case dybase_string_type:
        if (type != dybase_string_type) {
            // small string
            p += type >> 3;
        } else {
            memcpy(&len, p, sizeof(db_int4));
            p += sizeof(db_int4) + len;
        }
        break;
      case dybase_array_type:
        if (type != dybase_array_type) {
            // small array
            for (i = type >> 3; --i >= 0;) {
                p = markField(p);
            }
        } else {
            memcpy(&len, p, sizeof(db_int4));
            p += sizeof(db_int4);
            for (i = len; --i >= 0;) {
                p = markField(p);
            }
        }
	break;
      case dybase_map_type:
        if (type != dybase_map_type) {
            // small map
            for (i = (type >> 3) << 1; --i >= 0;) {
                p = markField(p);
            }
        } else {
            memcpy(&len, p, sizeof(db_int4));
            p += sizeof(db_int4);
            for (i = len << 1; --i >= 0;) {
                p = markField(p);
            }
        }
    }
    return p;
}


void dbDatabase::cloneBitmap(offs_t pos, size_t size)
{
    offs_t quantNo = pos / dbAllocationQuantum;
    int    objBitSize = (size+dbAllocationQuantum-1) / dbAllocationQuantum;
    oid_t  pageId = dbBitmapId + oid_t(quantNo / (dbPageSize*8));
    size_t offs = (size_t(quantNo) & (dbPageSize*8-1)) >> 3;
    int    bitOffs = int(quantNo) & 7;
    oid_t  oid = pageId;
    pos = getPos(oid);
    if (!(pos & dbModifiedFlag)) {
        dirtyPagesMap[size_t(oid / dbHandlesPerPage / 32)]
            |= 1 << (int(oid / dbHandlesPerPage) & 31);
        allocate(dbPageSize, oid);
        cloneBitmap(pos & ~dbFlagsMask, dbPageSize);
    }

    if (objBitSize > 8 - bitOffs) {
        objBitSize -= 8 - bitOffs;
        offs += 1;
        while (objBitSize + offs*8 > dbPageSize*8) {
            oid = ++pageId;
            pos = getPos(oid);
            if (!(pos & dbModifiedFlag)) {
                dirtyPagesMap[size_t(oid / dbHandlesPerPage / 32)]
                    |= 1 << (int(oid / dbHandlesPerPage) & 31);
                allocate(dbPageSize, oid);
                cloneBitmap(pos & ~dbFlagsMask, dbPageSize);
            }
            objBitSize -= (dbPageSize - offs)*8;
            offs = 0;
        }
    }
}


oid_t dbDatabase::allocate()
{
    dbCriticalSection cs(mutex);
    return allocateId();
}

oid_t dbDatabase::allocateId()
{
    oid_t oid;
    int curr = 1-this->curr;
    setDirty();
    if ((oid = header->root[curr].freeList) != 0) {
        header->root[curr].freeList = oid_t(getPos(oid) >> dbFlagsBits);
        dirtyPagesMap[size_t(oid / dbHandlesPerPage / 32)]
            |= 1 << (int(oid / dbHandlesPerPage) & 31);
    } else {
        if (currIndexSize + 1 > header->root[curr].indexSize) {
            size_t oldIndexSize = header->root[curr].indexSize;
            size_t newIndexSize = oldIndexSize * 2;
            while (newIndexSize < oldIndexSize + 1) {
                newIndexSize = newIndexSize*2;
            }
            TRACE_MSG(("Extend index size from %ld to %ld\n", oldIndexSize, newIndexSize));
            offs_t newIndex = allocate(newIndexSize * sizeof(offs_t));
            offs_t oldIndex = header->root[curr].index;
            pool.copy(newIndex, oldIndex, currIndexSize*sizeof(offs_t));
            header->root[curr].index = newIndex;
            header->root[curr].indexSize = newIndexSize;
            free(oldIndex, oldIndexSize*sizeof(offs_t));
        }
        oid = currIndexSize;
        header->root[curr].indexUsed = ++currIndexSize;
    }
    setPos(oid, 0);
    return oid;
}

void dbDatabase::freeId(oid_t oid)
{
    dirtyPagesMap[size_t(oid / dbHandlesPerPage / 32)]
        |= 1 << (int(oid / dbHandlesPerPage) & 31);
    setPos(oid, (offs_t(header->root[1-curr].freeList) << dbFlagsBits)
           | dbFreeHandleFlag);
    header->root[1-curr].freeList = oid;
}

void dbDatabase::commit()
{
    dbCriticalSection cs(mutex);
    commitTransaction();
}

void dbDatabase::commitTransaction()
{
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return;
    }
    if (!modified) {
        return;
    }
    //
    // Commit transaction
    //
    int rc;
    int n, curr = header->curr;
    oid_t i;
    db_int4* map = dirtyPagesMap;
    size_t currIndexSize = this->currIndexSize;
    size_t committedIndexSize = this->committedIndexSize;
    size_t oldIndexSize = header->root[curr].indexSize;
    size_t newIndexSize = header->root[1-curr].indexSize;
    size_t nPages = committedIndexSize / dbHandlesPerPage;
    if (newIndexSize > oldIndexSize) {
        offs_t newIndex = allocate(newIndexSize*sizeof(offs_t));
        header->root[1-curr].shadowIndex = newIndex;
        header->root[1-curr].shadowIndexSize = newIndexSize;
        cloneBitmap(header->root[curr].index, oldIndexSize*sizeof(offs_t));
        free(header->root[curr].index, oldIndexSize*sizeof(offs_t));
    }

    for (i = 0; i < nPages; i++) {
        if (map[size_t(i >> 5)] & (1 << int(i & 31))) {
            offs_t* srcIndex =
                (offs_t*)pool.get(header->root[1-curr].index+i*dbPageSize);
            offs_t* dstIndex =
                (offs_t*)pool.get(header->root[curr].index+i*dbPageSize);
            for (size_t j = 0; j < dbHandlesPerPage; j++) {
                offs_t pos = dstIndex[j];
                if (srcIndex[j] != pos) {
                    if (!(pos & dbFreeHandleFlag)) {
                        if (pos & dbPageObjectFlag) {
                            free(pos & ~dbFlagsMask, dbPageSize);
                        } else {
                            int offs = (int)pos & (dbPageSize-1);
                            dbObject* rec = (dbObject*)
                                (pool.get(pos-offs)+(offs & ~dbFlagsMask));
                            free(pos, rec->size);
                            pool.unfix(rec);
                        }
                    }
                }
            }
            pool.unfix(srcIndex);
            pool.unfix(dstIndex);
        }
    }
    if ((committedIndexSize % dbHandlesPerPage) != 0
        && (map[size_t(i >> 5)] & (1 << int(i & 31))))
    {
        offs_t* srcIndex =
            (offs_t*)pool.get(header->root[1-curr].index + i*dbPageSize);
        offs_t* dstIndex =
            (offs_t*)pool.get(header->root[curr].index + i*dbPageSize);
        n = committedIndexSize % dbHandlesPerPage;
        do {
            offs_t pos = *dstIndex;
            if (*srcIndex != pos) {
                if (!(pos & dbFreeHandleFlag)) {
                    if (pos & dbPageObjectFlag) {
                        free(pos & ~dbFlagsMask, dbPageSize);
                    } else {
                        int offs = (int)pos & (dbPageSize-1);
                        dbObject* rec = (dbObject*)
                            (pool.get(pos-offs) + (offs & ~dbFlagsMask));
                        free(pos, rec->size);
                        pool.unfix(rec);
                    }
                }
            }
            dstIndex += 1;
            srcIndex += 1;
        } while (--n != 0);

        pool.unfix(srcIndex);
        pool.unfix(dstIndex);
    }

    for (i = 0; i <= nPages; i++) {
        if (map[size_t(i >> 5)] & (1 << int(i & 31))) {
            offs_t* p =
                (offs_t*)pool.put(header->root[1-curr].index+i*dbPageSize);
            for (size_t j = 0; j < dbHandlesPerPage; j++) {
                p[j] &= ~dbModifiedFlag;
            }
            pool.unfix(p);
        }
    }
    if (currIndexSize > committedIndexSize) {
        offs_t page = (header->root[1-curr].index
                       + committedIndexSize*sizeof(offs_t)) & ~((offs_t)dbPageSize-1);
        offs_t end = (header->root[1-curr].index + dbPageSize - 1
                      + currIndexSize*sizeof(offs_t)) & ~((offs_t)dbPageSize-1);
        while (page < end) {
            offs_t* p = (offs_t*)pool.put(page);
            for (size_t h = 0; h < dbHandlesPerPage; h++) {
                p[h] &= ~dbModifiedFlag;
            }
            pool.unfix(p);
            page += dbPageSize;
        }
    }

    if ((rc = file->write(0, header, dbPageSize)) != dbFile::ok) {
        throwException(dybase_file_error, "Failed to write header");
    }

    pool.flush();

    header->curr = curr ^= 1;

    if ((rc = file->write(0, header, dbPageSize)) != dbFile::ok ||
        (rc = file->flush()) != dbFile::ok)
    {
        throwException(dybase_file_error, "Failed to flush changes to the disk");
    }

    header->root[1-curr].size = header->root[curr].size;
    header->root[1-curr].indexUsed = currIndexSize;
    header->root[1-curr].freeList  = header->root[curr].freeList;
    header->root[1-curr].bitmapEnd = header->root[curr].bitmapEnd;
    header->root[1-curr].rootObject  = header->root[curr].rootObject;
    header->root[1-curr].classDescList = header->root[curr].classDescList;

    if (newIndexSize != oldIndexSize) {
        header->root[1-curr].index=header->root[curr].shadowIndex;
        header->root[1-curr].indexSize=header->root[curr].shadowIndexSize;
        header->root[1-curr].shadowIndex=header->root[curr].index;
        header->root[1-curr].shadowIndexSize=header->root[curr].indexSize;
        pool.copy(header->root[1-curr].index, header->root[curr].index,
                  currIndexSize*sizeof(offs_t));
        memset(map, 0, 4*((currIndexSize+dbHandlesPerPage*32-1)
                          / (dbHandlesPerPage*32)));
    } else {
        for (i = 0; i < nPages; i++) {
            if (map[size_t(i >> 5)] & (1 << int(i & 31))) {
                map[size_t(i >> 5)] -= (1 << int(i & 31));
                pool.copy(header->root[1-curr].index + i*dbPageSize,
                          header->root[curr].index + i*dbPageSize,
                          dbPageSize);
            }
        }
        if (currIndexSize > i*dbHandlesPerPage &&
            ((map[size_t(i >> 5)] & (1 << int(i & 31))) != 0
             || currIndexSize != committedIndexSize))
        {
            pool.copy(header->root[1-curr].index + i*dbPageSize,
                      header->root[curr].index + i*dbPageSize,
                      size_t(sizeof(offs_t)*currIndexSize - i*dbPageSize));
            memset(map + size_t(i>>5), 0,
                   size_t(((currIndexSize + dbHandlesPerPage*32 - 1)
                           / (dbHandlesPerPage*32) - (i>>5))*4));
        }
    }
    this->curr = curr;
    this->committedIndexSize = currIndexSize;
    modified = false;
    gcDone = false;
}

void dbDatabase::rollback()
{
    dbCriticalSection cs(mutex);
    if (!opened) {
        handleError(dybase_not_opened, "Database not opened");
        return;
    }
    if (!modified) {
        return;
    }
    int curr = header->curr;
    size_t nPages =
        (committedIndexSize + dbHandlesPerPage - 1) / dbHandlesPerPage;
    db_int4 *map = dirtyPagesMap;
    if (header->root[1-curr].index != header->root[curr].shadowIndex) {
        pool.copy(header->root[curr].shadowIndex, header->root[curr].index,
                  dbPageSize*nPages);
    } else {
        for (oid_t i = 0; i < nPages; i++) {
            if (map[size_t(i >> 5)] & (1 << int(i & 31))) {
                pool.copy(header->root[curr].shadowIndex + i*dbPageSize,
                          header->root[curr].index + i*dbPageSize,
                          dbPageSize);
            }
        }
    }
    memset(map, 0,
           size_t((currIndexSize+dbHandlesPerPage*32-1) / (dbHandlesPerPage*32))*4);
    header->root[1-curr].indexSize = header->root[curr].shadowIndexSize;
    header->root[1-curr].indexUsed = header->root[curr].indexUsed;
    header->root[1-curr].freeList  = header->root[curr].freeList;
    header->root[1-curr].index = header->root[curr].shadowIndex;
    header->root[1-curr].bitmapEnd = header->root[curr].bitmapEnd;
    header->root[1-curr].size = header->root[curr].size;
    header->root[1-curr].rootObject = header->root[curr].rootObject;
    header->root[1-curr].classDescList = header->root[curr].classDescList;

    currRBitmapPage = currPBitmapPage = dbBitmapId;
    currRBitmapOffs = currPBitmapOffs = 0;
    currIndexSize = committedIndexSize;

    modified = false;

    oid_t cid = header->root[curr].classDescList;
    dbClassDescriptor* desc = classDescList;
    while (desc->oid != cid) {
        classOidHash.remove(&desc->oid, sizeof(desc->oid));
        classSignatureHash.remove(desc->cls->signature, desc->signatureSize);
        dbClassDescriptor* next = desc;
        delete desc;
        desc = next;
    }
    classDescList = desc;
}



dbDatabase::dbDatabase(dbAccessType   type,
                       dbErrorHandler hnd,
                       size_t         poolSize,
                       size_t         dbExtensionQuantum,
                       size_t         dbInitIndexSize
) : accessType(type),
    extensionQuantum(dbExtensionQuantum),
    initIndexSize(dbInitIndexSize),
    pool(this, poolSize),
    errorHandler(hnd)
{
    dirtyPagesMap = new db_int4[dbDirtyPageBitmapSize/4+1];
    bitmapPageAvailableSpace = new int[dbBitmapId + dbBitmapPages];
    classDescList = NULL;
    opened = false;
    header = (dbHeader*)dbFile::allocateBuffer(dbPageSize);
    dbFileExtensionQuantum = 0;
    dbFileSizeLimit = 0;
}

dbDatabase::~dbDatabase()
{
    delete[] dirtyPagesMap;
    delete[] bitmapPageAvailableSpace;
    dbFile::deallocateBuffer(header);
}


void dbTrace(char* message, ...)
{
    va_list args;
    va_start (args, message);
    vfprintf(stderr, message, args);
    va_end(args);
}
//-< PAGEPOOL.CPP >--------------------------------------------------*--------*
// GigaBASE                  Version 1.0         (c) 1999  GARRET    *     ?  *
// (Post Relational Database Management System)                      *   /\|  *
//                                                                   *  /  \  *
//                          Created:      6-Feb-98    K.A. Knizhnik  * / [] \ *
//                          Last update:  8-Feb-99    K.A. Knizhnik  * GARRET *
//-------------------------------------------------------------------*--------*
// Page pool implementation
//-------------------------------------------------------------------*--------*

#include "database.h"

byte* dbPagePool::find(offs_t addr, int state)
{
    dbPageHeader* ph;
    assert(((int)addr & (dbPageSize-1)) == 0);

    int hashCode = (unsigned(addr) >> dbPageBits) & hashBits;
    int i, rc;
    for (i = hashTable[hashCode]; i != 0; i = ph->collisionChain) {
        ph = &pages[i];
        if (ph->offs == addr) {
            if (ph->accessCount++ == 0) {
                pages[ph->next].prev = ph->prev;
                pages[ph->prev].next = ph->next;
            } else {
                assert((ph->state & dbPageHeader::psRaw) == 0);
            }
            if (!(ph->state & dbPageHeader::psDirty) &&
                (state & dbPageHeader::psDirty))
            {
                dirtyPages[nDirtyPages] = ph;
                ph->writeQueueIndex = nDirtyPages++;
            }
#ifdef PROTECT_PAGE_POOL
            if ((state & dbPageHeader::psDirty)) {
                dbFile::protectBuffer(buffer + (i-1)*dbPageSize, dbPageSize, false);
            }
#endif
            ph->state |= state;
            // printf("Find page %x, offs=%x\n", ph, addr);
            return buffer + (i-1)*dbPageSize;
        }
    }
    i = freePages;
    if (i == 0) {
        i = pages->prev;
        assert(((void)"unfixed page availabe", i != 0));
        ph = &pages[i];
        // printf("Throw page %p offs=%x\n", ph, ph->offs);
        if (ph->state & dbPageHeader::psDirty) {
            // printf("Write page " INT8_FORMAT "\n", ph->offs);
            rc = file->write(ph->offs, buffer + (i-1)*dbPageSize, dbPageSize);
            if (rc != dbFile::ok) {
                db->throwException(dybase_file_error, "Failed to write page");
            }
            if (!flushing) {
                dirtyPages[ph->writeQueueIndex] = dirtyPages[--nDirtyPages];
                dirtyPages[ph->writeQueueIndex]->writeQueueIndex = ph->writeQueueIndex;
            }
            if (ph->offs >= fileSize) {
                fileSize = ph->offs + dbPageSize;
            }
        }
        unsigned h = (unsigned(ph->offs) >> dbPageBits) & hashBits;
        int* np;
        for (np = &hashTable[h]; *np != i; np = &pages[*np].collisionChain);
        *np = ph->collisionChain;
        pages[ph->next].prev = ph->prev;
        pages[ph->prev].next = ph->next;
    } else {
        ph = &pages[i];
        freePages = ph->next;
        if (i >= nPages) {
            nPages = i+1;
        }
    }
    // printf("Use page %p offs=%x\n", ph, addr);
    ph->accessCount = 1;
    ph->state = 0;
    ph->offs = addr;
    ph->collisionChain = hashTable[hashCode];
    hashTable[hashCode] = i;

    if (state & dbPageHeader::psDirty) {
        dirtyPages[nDirtyPages] = ph;
        ph->writeQueueIndex = nDirtyPages++;
        ph->state |= dbPageHeader::psDirty;
    }

    byte* p = buffer + (i-1)*dbPageSize;
#ifdef PROTECT_PAGE_POOL
    dbFile::protectBuffer(p, dbPageSize, false);
#endif

    if (addr < fileSize) {
        ph->state |= dbPageHeader::psRaw;
        //printf("read addr=%x\n", addr);
        rc = file->read(addr, p, dbPageSize);
        if (rc == dbFile::eof) {
            memset(p, 0, dbPageSize);
        } else if (rc != dbFile::ok) {
            db->throwException(dybase_file_error, "Failed to read page");
        }
        ph->state &= ~(dbPageHeader::psWait|dbPageHeader::psRaw);
    } else {
        memset(p, 0, dbPageSize);
    }
#ifdef PROTECT_PAGE_POOL
    if (!(state & dbPageHeader::psDirty)) {
        dbFile::protectBuffer(p, dbPageSize, true);
    }
#endif
    return p;
}


void dbPagePool::copy(offs_t dst, offs_t src, size_t size)
{
    size_t dstOffs = (size_t)dst & (dbPageSize-1);
    size_t srcOffs = (size_t)src & (dbPageSize-1);
    dst -= dstOffs;
    src -= srcOffs;
    byte* dstPage = find(dst, dbPageHeader::psDirty);
    byte* srcPage = find(src, 0);
    size = (size + 3) >> 2;
    do {
        if (dstOffs == dbPageSize) {
            unfix(dstPage);
            dst += dbPageSize;
            dstPage = find(dst, dbPageHeader::psDirty);
            dstOffs = 0;
        }
        if (srcOffs == dbPageSize) {
            unfix(srcPage);
            src += dbPageSize;
            srcPage = find(src, 0);
            srcOffs = 0;
        }
        *(db_int4*)(dstPage + dstOffs) = *(db_int4*)(srcPage + srcOffs);
        dstOffs += 4;
        srcOffs += 4;
    } while (--size != 0);

    unfix(dstPage);
    unfix(srcPage);
}


bool dbPagePool::open(dbFile* file, offs_t fileSize)
{
    int i;

    this->file = file;
    this->fileSize = fileSize;

    size_t hashSize;
    for (hashSize = minHashSize; hashSize < poolSize; hashSize *= 2);
    hashTable = new int[hashSize];
    memset(hashTable, 0, sizeof(int)*hashSize);
    hashBits = hashSize-1;

    pages = new dbPageHeader[poolSize+1];
    pages->next = pages->prev = 0;
    for (i = poolSize+1; --i != 0;) {
        pages[i].state = 0;
        pages[i].next = i + 1;
    }
    pages[poolSize].next = 0;
    freePages = 1;

    flushing = false;
    nPages = 0;
    nDirtyPages = 0;
    dirtyPages = new dbPageHeader*[poolSize];

#if defined(__WATCOMC__)
    // reserve one more pages to allow access after end of page
    bufferSize = (poolSize+1)*dbPageSize;
#else
    bufferSize = poolSize*dbPageSize;
#endif
    buffer = (byte*)dbFile::allocateBuffer(bufferSize);
    return buffer != NULL;
}


void dbPagePool::close()
{
    delete[] hashTable;
    delete[] pages;
    delete[] dirtyPages;
    dbFile::deallocateBuffer(buffer, bufferSize);
    pages = NULL;
}


void dbPagePool::unfix(void* ptr)
{
    int i = (size_t((byte*)ptr - buffer) >> dbPageBits) + 1;
    dbPageHeader* ph = &pages[i];
    assert(ph->accessCount > 0);
    if (--ph->accessCount == 0) {
        ph->next = pages->next;
        ph->prev = 0;
        pages->next = pages[ph->next].prev = i;
#ifdef PROTECT_PAGE_POOL
        if (ph->state & dbPageHeader::psDirty) {
            dbFile::protectBuffer(buffer + (i-1)*dbPageSize, dbPageSize, true);
        }
#endif
    }
}


void dbPagePool::unfixLIFO(void* ptr)
{
    int i = (size_t((byte*)ptr - buffer) >> dbPageBits) + 1;
    dbPageHeader* ph = &pages[i];
    assert(ph->accessCount > 0);
    if (--ph->accessCount == 0) {
        ph->next = 0;
        ph->prev = pages->prev;
        pages->prev = pages[ph->prev].next = i;
    }
}


void dbPagePool::fix(void* ptr)
{
    int i = (size_t((byte*)ptr - buffer) >> dbPageBits) + 1;
    dbPageHeader* ph = &pages[i];
    assert(ph->accessCount != 0);
    ph->accessCount += 1;
}


void dbPagePool::modify(void* ptr)
{
    int i = (size_t((byte*)ptr - buffer) >> dbPageBits) + 1;
    dbPageHeader* ph = &pages[i];
    assert(ph->accessCount != 0);
    if (!(ph->state & dbPageHeader::psDirty)) {
        ph->state |= dbPageHeader::psDirty;
        dirtyPages[nDirtyPages] = ph;
        ph->writeQueueIndex = nDirtyPages++;
    }
}


void dbPagePool::put(offs_t pos, byte* obj, size_t size)
{
    int offs = (int)pos & (dbPageSize-1);
    byte* pg = find(pos - offs, dbPageHeader::psDirty);
    while (size > dbPageSize - offs) {
        memcpy(pg + offs, obj, dbPageSize - offs);
        unfix(pg);
        size -= dbPageSize - offs;
        pos += dbPageSize - offs;
        obj += dbPageSize - offs;
        pg = find(pos, dbPageHeader::psDirty);
        offs = 0;
    }
    memcpy(pg + offs, obj, size);
    unfix(pg);
}


static int __cdecl compareOffs(void const* a, void const* b)
{
     dbPageHeader* pa = *(dbPageHeader**)a;
     dbPageHeader* pb = *(dbPageHeader**)b;
     return pa->offs < pb->offs ? -1 : pa->offs == pb->offs ? 0 : 1;
}


void dbPagePool::flush()
{
    int rc;
    if (nDirtyPages != 0) {
        flushing = true;
        qsort(dirtyPages, nDirtyPages, sizeof(dbPageHeader*), compareOffs);
        for (int i = 0, n = nDirtyPages; i < n; i++) {
            dbPageHeader* ph = dirtyPages[i];
            if (ph->accessCount++ == 0) {
                pages[ph->next].prev = ph->prev;
                pages[ph->prev].next = ph->next;
            }
            if (ph->state & dbPageHeader::psDirty) {
                //printf("Flush page " INT8_FORMAT "\n", ph->offs);
                rc = file->write(ph->offs, buffer + (ph-pages-1)*dbPageSize,
                                 dbPageSize);
                if (rc != dbFile::ok) {
                    db->throwException(dybase_file_error, "Failed to write page");
                }
                ph->state &= ~dbPageHeader::psDirty;
                if (ph->offs >= fileSize) {
                    fileSize = ph->offs + dbPageSize;
                }
            }
            if (--ph->accessCount == 0) {
                ph->next = pages->next;
                ph->prev = 0;
                pages->next = pages[ph->next].prev = (ph - pages);
            }
        }
        flushing = false;
        nDirtyPages = 0;
    }
    rc = file->flush();
    if (rc != dbFile::ok) {
        db->throwException(dybase_file_error, "Failed to flush pages pool");
    }
}


void dbGetTie::set(dbPagePool& pool, offs_t pos)
{
    reset();
    int offs = (int)pos & (dbPageSize-1);
    byte* p = pool.get(pos - offs);
    size_t size = ((dbObject*)(p + offs))->size;
    if (offs + size > dbPageSize) {
        byte* dst = new byte[size];
        obj = dst;
        memcpy(dst, p + offs, dbPageSize - offs);
        pool.unfix(p);
        size -= dbPageSize - offs;
        pos += dbPageSize - offs;
        dst += dbPageSize - offs;
        while (size > dbPageSize) {
            p = pool.get(pos);
            memcpy(dst, p, dbPageSize);
            dst += dbPageSize;
            size -= dbPageSize;
            pos += dbPageSize;
            pool.unfix(p);
        }
        p = pool.get(pos);
        memcpy(dst, p, size);
        pool.unfix(p);
        page = NULL;
    } else {
        this->pool = &pool;
        page = p;
        obj = p + offs;
    }
}


void dbGetTie::reset()
{
    if (obj != NULL) {
        if (page != NULL) {
            assert(!pool->destructed()); // hack: page pool should not be
            // destructed before any reference to the storage
            // (cursors, references),...
            pool->unfix(page);
            page = NULL;
        } else {
            delete[] obj;
        }
        obj = NULL;
    }
}


void dbPutTie::set(dbPagePool& pool, oid_t oid, offs_t pos, size_t size)
{
    reset();

    this->oid = oid;
    this->pool = &pool;

    int offs = (int)pos & (dbPageSize-1);
    byte* p = pool.put(pos - offs);
    if (offs + size > dbPageSize) {
        this->size = size;
        this->pos = pos;
        byte* dst = new byte[size];
        obj = dst;
        memcpy(dst, p + offs, dbPageSize - offs);
        pool.unfix(p);
        size -= dbPageSize - offs;
        pos += dbPageSize - offs;
        dst += dbPageSize - offs;
        while (size > dbPageSize) {
            p = pool.get(pos);
            memcpy(dst, p, dbPageSize);
            dst += dbPageSize;
            size -= dbPageSize;
            pos += dbPageSize;
            pool.unfix(p);
        }
        p = pool.get(pos);
        memcpy(dst, p, size);
        pool.unfix(p);
        page = NULL;
    } else {
        page = p;
        obj = page + offs;
    }
}


void dbPutTie::reset()
{
    if (obj == NULL) {
        return;
    }
    if (page == NULL) {
        offs_t pos = this->pos;
        int offs = (int)pos & (dbPageSize-1);
        size_t size = this->size;
        assert(offs + size > dbPageSize);
        byte* p = pool->put(pos - offs);
        byte* src = obj;
        memcpy(p + offs, src, dbPageSize - offs);
        pool->unfix(p);
        src += dbPageSize - offs;
        size -= dbPageSize - offs;
        pos += dbPageSize - offs;
        while (size > dbPageSize) {
            p = pool->put(pos);
            memcpy(p, src, dbPageSize);
            pool->unfix(p);
            src += dbPageSize;
            pos += dbPageSize;
            size -= dbPageSize;
        }
        p = pool->put(pos);
        memcpy(p, src, size);
        pool->unfix(p);
        delete[] obj;
    } else {
        pool->unfix(page);
        page = NULL;
    }
    obj = NULL;
    oid = 0;
}
/**
 * 
 * 
 * Author:  Bystroushaak (bystrousak@kitakitsune.org)
 * Version: 0.0.1
 * Date:    .2013
 * 
 * Copyright: 
 *     This work is licensed under a CC BY.
 *     http://creativecommons.org/licenses/by/3.0/
*/
#include <iostream>



using namespace std;



int main(int argc, char *argv[]){
	
	
	return 0;
}
